<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.CL](#cs.CL) [Total: 74]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种从单目图像中恢复真实尺度3D食物模型的新方法，有效提升了食物摄入量估算的准确性。


<details>
  <summary>Details</summary>
Motivation: 饮食相关慢性病如肥胖和糖尿病的增加，促使精确的食物摄入量监测变得尤为重要。现有AI饮食评估虽然进步显著，但单目图像提取真实食物分量信息的问题未得到很好解决，限制了精准营养领域的发展。

Method: 作者提出一种结合大规模数据集预训练视觉特征的3D重建方法，通过学习尺度信息，将单视角重建结果转化为具备真实物理尺度的三维食物模型。

Result: 在两个公开数据集上的大量实验和消融研究显示，该方法相较现有方法，平均体积估算误差降低了近30%。

Conclusion: 该方法能够显著提升基于单目图像的食物分量估算精度，对精准营养领域具有重要推动作用，源码已开源。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 本文提出了一种新的可区分前景和背景的开放词汇语义分割方法DiSa。其核心是通过显著性提示模块和分层细化模块提升分割精度，实现对前景和背景特征的更好建模和边界清晰度。实验证明该方法优于现有主流方案。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言模型（如CLIP）在开放词汇分割任务中存在前景偏置和空间定位模糊的问题，尤其是忽视背景区域与对象边界模糊，影响分割质量。因此，提升模型的前景-背景解耦能力和空间细节捕捉能力成为该方向重要的研究动机。

Method: 文章提出DiSa框架，包括显著性感知解耦模块（SDM），通过显著性信息分别建模前景和背景特征，并采用分而治之的方法提升辨识能力。同时，引入分层细化模块（HRM），利用像素级空间上下文和通道级细化策略多层次优化特征表达。

Result: 在六个数据集上进行了广泛实验证实，DiSa在语义分割准确率等多个指标上均优于当前最先进的同类方法，表现出对复杂场景的更强适应性和泛化能力。

Conclusion: DiSa框架有效缓解了现有模型在前景偏置与边界模糊方面的不足，为开放词汇语义分割任务提供了更优的解决方案，具有较高的实际应用价值和推广前景。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: 提出了一种结合自监督和半监督的视觉Transformer训练方法（SSMAE），利用稀缺标注数据和大量无标注数据实现更高效的模型训练。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer需要大量标注数据才能训练良好，但现实中标注数据常常很少，如何高效利用大量无标签数据以提升模型表现是一个迫切问题。

Method: 提出Semi-Supervised Masked Autoencoder（SSMAE），将自监督的图像重建任务和半监督的分类任务联合优化。方法设计了基于模型预测稳定性的伪标签门控机制，仅在模型自信且预测稳定时才激活生成伪标签，并同时采用强、弱增强视图提升伪标签可靠性，从而减轻确认偏差。

Result: 在CIFAR-10和CIFAR-100数据集上，SSMAE在有限标注样本的条件下显著超过传统监督ViT和微调的MAE，尤其是在标签极少场景下提升明显（CIFAR-10 10%标注下相较ViT提升9.24%）。

Conclusion: 研究表明，在数据高效训练Transformer时，何时引入伪标签和如何生成伪标签同样重要，SSMAE在半监督场景下是一种更优的训练框架。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 本文提出了一种将稀疏性直接融入CLIP训练过程的方法，从而获得兼具可解释性和高性能的多模态表征。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然表现优异，但其稠密且不透明的隐空间表示难以解释。现有提升可解释性的稀疏方法多为后处理，通常牺牲性能和多模态能力。作者希望打破“可解释性与性能只能二选一”的常规假设。

Method: 在CLIP训练阶段引入稀疏机制，直接输出稀疏表征，而非通过如稀疏自编码器等事后处理。这一方案力求兼顾可解释性、性能和多模态能力。

Result: 所提出的稀疏CLIP训练方法获得了比SAE更优的可解释性，同时在下游任务中保持了强性能与多模态能力。实验显示其稀疏特征能便捷对齐语义概念，也揭示了模型跨模态知识的形成机制。

Conclusion: 实验表明可解释性与性能可以通过合适设计同时提升。该方法为未来多模态模型的开发提供了有价值的原则和方向。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 本论文对H&E染色图像中细胞核实例分割任务中常用的公开数据集进行了标准化、评价和融合，提出了新的基准测试集和训练集，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞核实例分割研究大多专注于分割算法的创新，但对所用数据集缺乏系统性梳理和标准化评价，且现有评测数据集数量有限，样本选择随意，影响方法比较和复现。

Method: 1）通过文献综述筛选并收集H&E染色图像中公开可用、包含人工标注的细胞核实例分割数据集；2）将这些数据集按统一格式标准化；3）基于CNN与CNN+ViT（视觉变换器）两种最新模型，对各数据集进行系统化性能评价和排名；4）提出融合各数据集的新测试集（NucFuse-test）和训练集（NucFuse-train）；5）融合实验、外部验证并公开代码。

Result: 标准化和融合后的数据集使得不同模型间的性能对比更为公平，NucFuse-test和NucFuse-train分别作为新的统一测试集与训练集，有效提升了模型的泛化能力和分割性能。对各数据集进行了系统的性能排名和分析。

Conclusion: 论文为H&E染色组织切片中细胞核实例分割任务的数据集选择和评价提供了新基准，统一了不同数据集格式和评测流程，提升了模型评估的科学性和可复现性，为后续相关研究奠定了数据基础。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 论文提出了一种新的无训练修剪方法（SAP），针对视觉-语言模型检索中索引向量过大问题，能在大幅压缩向量的同时保持高检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在视觉文档检索中，索引向量过大，影响实际部署；现有无训练修剪方法在高压缩场景下效果很差。此外，之前研究认为无训练修剪可行性存疑。

Method: 提出Structural Anchor Pruning（SAP）方法，从中间层识别重要视觉patch，无需训练即可进行有效修剪。同时，引入了Oracle Score Retention（OSR）协议评估各层信息对压缩效率的影响。

Result: 在ViDoRe基准测试上，SAP方法在压缩90%以上的同时保持了高检索准确率，优于传统方法。OSR分析显示中间层存在重要的语义结构锚patch，区别于最终层信号消散。

Conclusion: SAP方法为视觉RAG提供了高效、可扩展的索引向量压缩方案，证实了中间层结构锚点在高效压缩中的作用，挑战并补充了此前对无训练修剪可行性的质疑。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 本文提出了一种针对扩散式大规模多模态模型（如LLaDA-V）的结构化视觉token裁剪方法，在大幅降低计算量的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式多模态大模型（如LLaDA-V）由于其双向注意力和扩散式去噪过程，计算量极大，阻碍了高效推理和实际应用。现有的token裁剪方法仅针对浅层，未能结合模型的跨模态信息聚合特点做出更有效的优化。

Method: 首先对LLaDA-V的注意力机制进行深入分析，发现其跨模态信息多在中后层聚合，语义对齐较晚。基于此启发，借鉴FastV方法，设计了结构化视觉token裁剪策略，选择在去噪第一步过程的中后层有选择性地删除一部分视觉token，从而减少后续步骤的计算量，并尽可能保留关键信息。

Result: 在多个基准测试上，该方法能在保持95%任务表现的情况下，将计算开销最多削减65%。

Conclusion: 本文首次在人扩散式多模态大模型上探索结构化token裁剪，提出的中后层、首步裁剪策略有效提升了推理效率，并为多模态模型的高效推理开辟了新方向。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了TeleStyle，一个高效的内容保留风格迁移模型，支持图片和视频的风格化，能够在保持内容一致性的同时适应不同风格输入，并在多项指标上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 扩散式Transformer（DiTs）在内容和风格特征高度纠缠的情况下，难以实现精确的内容保留和风格迁移。因此，需要一种能够平衡内容保留和风格自适应的新方法。

Method: TeleStyle基于Qwen-Image-Edit模型，增强了内容保留与风格定制能力。作者收集了高质量风格数据集，合成多样化风格三元组，并采用课程式持续学习框架，在干净和带噪声的数据上训练模型。此外，作者还开发了视频风格化模块以增强时序一致性。

Result: TeleStyle在风格相似性、内容一致性、美学质量三个核心指标上均达到了最新水平，实现了对未见风格的良好泛化和优质视觉表现。

Conclusion: TeleStyle有效解决了DiTs在内容与风格分离上的难题，实现在静态图片与视频上的高质量内容保留风格迁移，适用于多种实际应用场景，代码和模型已开源。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 本研究提出自动化识别船体海洋生物附着严重程度的方法，并评估了多种计算机视觉模型与多模态大模型（LLMs）的表现，结果发现两者在不同类别上互补，混合方法有助于实现可扩展且可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 目前海洋生物附着的检测主要靠人工潜水员检查，存在危险且难以大规模应用，因此需要更安全、高效、可扩展的自动化评估方式。

Method: 本研究利用经专家标注的数据集，评估了卷积神经网络、基于Transformer的分割模型及无需训练的大型多模态语言模型（LLM）在生物附着严重度（LoF）分级自动判别中的表现，通过结构化提示和检索提升LLM准确性，并对不同模型进行对比。

Result: 计算机视觉模型在极端LoF等级上表现优异，但对中等等级区分较弱，主要受数据不平衡和图片构图影响。LLM通过结构化提示和检索，无需训练下取得了可竞争的表现，并能输出可解释理由。两类方法呈现互补特点。

Conclusion: 结合分割模型的覆盖能力与LLM的推理解释，有望实现更可扩展、可解释且高效的生物附着自动评估。该方法可为今后相关智能检测系统提供技术路线。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: 本文提出DenseGRPO框架，通过在文本到图像生成中的每个去噪步骤引入密集奖励，解决了现有稀疏奖励导致反馈信号与中间步骤贡献不匹配的问题，并通过时步自适应随机性调整策略优化探索空间。实验表明该方法能显著提升人类偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于GRPO的flow matching方法，在文本到图像生成的人类偏好对齐方面取得进展，但由于仅对最终结果进行奖励，中间过程贡献无法精细评估，导致学习效果受限。

Method: 1）为每个去噪步骤引入基于清晰中间图像的时步密集奖励，确保反馈信号与具体贡献匹配；2）基于密集奖励，提出自适应奖励感知探索空间校准方法，通过调整SDE采样器中各时刻的随机性注入，优化探索空间。

Result: 在多个标准基准上，DenseGRPO表现优异，显著提升了flow matching模型与人类偏好的对齐效果。

Conclusion: 比传统终极奖励更细致的密集奖励设计，有效促进了去噪过程各阶段的奖励对齐，并结合奖励自适应探索策略，共同推动文本到图像生成系统更优的人类偏好适应能力。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个新的方法FPL，有效地提升了CLIP这类视觉-语言预训练模型在下游任务中的适应能力，并且在准确率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等视觉-语言预训练模型很强大，但在迁移到下游任务时，现有高效微调方法要么效果不理想，要么参数和训练时间消耗过大。因此作者希望提出一种既高效、又效果优秀的新方法。

Method: 作者提出了特征投影学习（FPL）的方法：建立一个投影模型，将类别原型的特征投影到查询图像的特征空间内，并重构查询图像的特征图，再以负均方重建误差作为类别分数，将分类问题转化为特征投影问题。此外，输出结合了投影模型和原始CLIP的预测结果。

Result: 大量实验显示，FPL在准确率等指标上显著超过当前的其它SOTA方法。

Conclusion: FPL方法能高效、优雅地提升CLIP模型在下游任务上的表现，是适应VLP模型新方向上的有效进展。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 本文提出了一种名为Prompt-Agnostic Evolution（PAE）的方法，通过建模提示动态，提升视觉提示调优（VPT）在视觉Transformer上的效率和性能。PAE促进层间一致性，加速收敛，并提升下游任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的VPT方法训练不稳定，表现出梯度震荡：浅层提示停滞，深层提示振荡，层间失配，导致收敛慢且性能下降。因此，需要一种更有效、更稳定的方法来提升VPT表现。

Method: 1. 提出PAE方法，从频域视角初始化提示，引入任务相关的频率快捷模式。2. 利用共享的Koopman算子在所有层施加全局线性变换，增强层间的协同进化，避免单层独立更新带来的不一致。3. 借鉴Lyapunov稳定理论，设计正则项约束提示演化中的误差放大。

Result: 在25个下游任务数据集上的实验显示，PAE可使收敛速度平均加快1.41倍，并将准确率提升1-3%。该方法还能无缝集成到多种VPT变体，无需修改主干或推理过程。

Conclusion: PAE方法不仅提升了VPT的稳定性和适用性，还能通过简单、轻量的策略加速收敛并提高性能，适用于各种视觉任务和VPT模型。

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出了一种名为BLenDeR的扩散采样方法，通过集合理论启发的操作提升DML的数据多样性，并在主流数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型能够产生高质量的合成数据，但现有方法在提升类内多样性和可控性方面存在不足，限制了深度度量学习（DML）性能。作者希望通过改进采样方法，增强DML中类内样本的多样性，从而提升任务表现。

Method: 论文提出BLenDeR方法，基于扩散采样，通过集合理论启发的“并集”和“交集”操作对去噪残差进行处理。“并集”操作结合多个提示中的所有属性，“交集”则通过主成分分析提取共有特征，从而实现对类别内属性组合的可控多样化生成。

Result: 在CUB-200和Cars-196等标准DML基准测试上，BLenDeR与主流方法对比，Recall@1分别提升了3.7%和1.8%，且在多种数据集和骨干网络上表现稳定领先。

Conclusion: BLenDeR方法能够有效提升DML任务中的类内多样性，进而带来显著性能提升，优于现有主流生成增强方法。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的可逆高效扩散模型（RED），用于多模态图像融合，旨在提升细节保留与视觉保真度，克服传统扩散模型在融合任务中的噪声导致的细节丢失问题，并兼顾计算效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但直接应用于多模态图像融合时，因噪声累积问题导致结果细节丢失且一致性差。此外，引入显式监督训练虽然可提升性能，却带来计算效率难题。因此，亟需一种既能继承扩散模型优点，又能提升细节保留与效率的新方法。

Method: 本文提出了可逆高效扩散（RED）模型，将显式监督融入端到端训练框架，避免了传统的分布估计过程，以提升训练效率。同时设计结构以克服噪声累积带来的细节损失。

Result: RED模型在多模态图像融合任务中能够更好地保留细节，提高融合后图像的视觉质量，并在计算效率上优于传统扩散模型融合方法。

Conclusion: RED模型有效结合了扩散模型的生成能力和显式监督的优势，实现了高效能、高保真度的多模态图像融合，为相关应用提供了新的思路和方法。

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度感知的诊断框架（LVLMs-Saliency），用于检测并缓解大规模视觉-语言模型（LVLMs）中的幻觉（hallucination）。该框架通过融合注意力权重与其输入梯度，量化每个输出token的视觉支撑强度，并基于此提出了两种推理时防幻觉方法，有效减少了幻觉发生率。


<details>
  <summary>Details</summary>
Motivation: 目前用注意力分析检测LVLMs幻觉的方法只关注前向传播的注意力分布，忽略了梯度等能反映token影响传播的信号，导致难以准确分辨幻觉与事实性输出。为了解决这一不足，作者希望探索更有效、可解释的机制检测与缓解幻觉。

Method: 提出LVLMs-Saliency框架，将注意力权重与输入梯度结合以衡量输出token的视觉支撑强度。并据此设计：（1）显著性引导的拒绝采样（SGRS），在生成每个token时，筛除saliency低于动态阈值的候选项；（2）局部连贯性强化（LocoRE），通过增强当前token对最近token的关注，防止短时记忆丧失。

Result: 实验覆盖多种主流LVLMs，结果显示：提出的方法能显著降低模型幻觉率，同时保持输出流畅性和任务表现。

Conclusion: LVLMs-Saliency框架通过显著性建模和推理时干预，提升了视觉-语言模型输出的可靠性与可解释性，是一种有效的防幻觉方案。代码已开源，可复现。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了一种无需源域数据的领域自适应方法，通过多视角增强与潜在空间一致性学习，有效提升了目标域的特征泛化能力，并在多个数据集上取得了领先的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应（DA）方法大多需要源域数据参与、复杂的对抗训练或伪标签处理，带来计算成本高、源数据不可获得等实际问题。因此，作者希望提出一种既不依赖源域数据，又避免高昂训练代价的新方法。

Method: 该方法首创性地在仅有目标域数据的前提下，利用多视角数据增强和潜在空间一致性约束，学习域不变特征。通过生成多个目标域样本视图，最小化其潜在表示的距离，无需传统的源-目标对齐或伪标签优化。模型架构采用ConvNeXt编码器，并设计了融合分类与一致性目标的损失函数。

Result: 该方法在Office-31、Office-Home和Office-Caltech三个数据集上，达到了90.72%、84%、97.12%的平均分类准确率，相较现有方法平均提升1.23%、7.26%、1.77%。

Conclusion: 实验结果证明，该方法在提升领域适应能力的同时，大幅简化了训练流程，为源不可用的跨域应用场景提供了有效方案。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度的视频生成质量评估方法，包括新数据集GenVID和检测框架DVAR，用于高效识别与分类生成视频中的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成评估方法仅能给出粗略的质量评分，无法详细定位和分类视频生成过程中的常见伪影（如画面、运动、摄像相关问题），难以满足实际审核和质量控制需求。

Method: 作者首先提出围绕“外观、运动、摄像”三大维度构建伪影分类体系，将常见的生成伪影划分为10大类。接着，构建了GenVID数据集，涵盖来自多种先进模型的8万条生成视频并人工标注伪影类别。最后，利用该数据集，开发了DVAR（Dense Video Artifact Recognition）框架，实现对伪影的细粒度识别和分类。

Result: 实验证明，该方法在伪影检测准确率方面优于以往方法，能有效筛除低质量生成内容，实现高效且精细的视频内容评估。

Conclusion: 提出的GenVID数据集和DVAR框架提升了生成视频伪影的检测和分类能力，为视频生成内容的高质量筛选和审核提供了有力工具。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: 本文提出了Li-ViP3D++，一种基于query的多模态感知与轨迹预测框架，创新性地融合了摄像头与激光雷达数据，实现了更优的端到端检测与预测表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知和预测系统往往依赖模块化流程，信息流受限且容易因上游错误导致性能下降。虽然基于query的端到端模型有所改进，但相机和激光雷达的融合仍存在利用率低和偏差引入的问题。

Method: 提出Query-Gated Deformable Fusion（QGDF）机制，在query空间内融合多视角RGB图像与LiDAR数据：（1）通过多尺度masked attention汇聚图像特征，（2）通过可学习的query偏移实现BEV空间下的LiDAR特征提取，（3）针对每个目标自适应加权视觉和几何特征。整个架构实现检测、跟踪和轨迹预测的端到端联合优化。

Result: 在nuScenes数据集上，相较于前作，Li-ViP3D++提升了端到端行为和检测质量（EPA 0.335，mAP 0.502），大幅降低误报（FP ratio 0.147），同时推理速度更快（139.82 ms对145.91 ms）。

Conclusion: 在query空间中实现相机-LiDAR的可微融合，不仅提升了端到端感知与预测系统的鲁棒性，还兼顾了系统的部署效率。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [19] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: 作者提出了C-SAM框架，通过在剪枝过程中扰动mask，提高模型紧凑性的同时，显著提升对输入扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然SAM有助于增强DNN对输入变化的鲁棒性，但传统剪枝后的SAM模型或先剪枝再用SAM都会显著损失鲁棒性。如何同时实现高鲁棒性和高紧凑性是未解难题。

Method: 作者提出C-SAM，通过在训练时扰动剪枝mask（而非参数），促使剪枝模式本身在结构层面更加鲁棒，最终获得兼具紧凑性和鲁棒性的模型。

Result: 在CelebA-HQ、Flowers-102、CIFAR-10-C等数据集和多种主流结构上，C-SAM的认证鲁棒性优于现有方法，提升幅度最高可达42%，且准确率接近未剪枝模型。

Conclusion: C-SAM能够实现DNN模型结构的高压缩比和强鲁棒性的统一，证明了结构扰动下的锐度最小化在实际部署中的应用前景。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [20] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 本文针对医学图像分割中因协变量转移导致的性能下降问题，提出了融合不同分布样本的双域学习策略，在膀胱分割任务中验证了方法效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分割时，经常因为训练和实际应用数据分布不同（协变量转移）而性能大幅下降，尤其是在某些稀缺数据场景下（如带插管的CT图像）。作者希望探究是否能有效利用来源不同但相关的数据，提升目标领域分割的准确性。

Method: 提出了一种双域学习策略，将无插管（NA）和有插管（WA）的CT数据结合训练，通过在主要为NA数据的训练集加入少量WA数据，并利用多个深度学习架构在不同切面进行系统性实验，评估融合方式对模型泛化能力的提升。

Result: 实验显示，仅用NA数据难以覆盖WA图像的变形和伪影特征，但在训练集中掺入10%-30%的WA数据后，分割性能与只用WA训练得到的模型相当。具体性能指标如Dice系数可达0.94，IoU可达0.92。

Conclusion: 适量引入分布有差异但解剖结构相似的数据，有助于缓解数据稀缺，显著提升深度分割模型在放疗计划中的临床可靠性和适应性。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [21] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 本文提出了一种单张图像下的物体质量估计算法，将几何体积与材料密度等物理信息融合，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 直接通过RGB图像预测物体质量存在困难，因为质量由体积和密度共同决定，而这两者都无法仅通过外观直接获取。因此需要引入物理意义的表示约束解空间，从而提高估计的准确性。

Method: 方法包括：1) 利用单目深度估计算法恢复物体三维几何，从而推断体积信息；2) 使用视觉-语言模型提取物体的粗略材料语义信息，为密度判断提供依据；3) 利用实例自适应门控机制融合几何、语义和外观特征，通过两组独立的回归头分别预测与体积、密度相关的物理因子，同时只使用质量标注进行训练。

Result: 在image2mass与ABO-500数据集上的实验结果表明，本文方法在质量估计任务上优于当前最先进的方案。

Conclusion: 融合物理结构和多模态信息的单图质量估计算法能更好地解决歧义性，提升预测准确率，对视觉质量感知具有重要意义。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [22] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 本文提出了一种结构约束和语言指导的扩散模型（SLDM），用于从低剂量碘对比剂CT生成等效于正常剂量的影像，减少对比剂用量同时保证诊断质量。


<details>
  <summary>Details</summary>
Motivation: 碘对比剂虽然能提高CT诊断的敏感性和特异性，但过量使用会导致肾损伤和严重过敏。目前深度学习可以用较低剂量生成高剂量图像，但现有方法难以针对不完全成对的图像实现结构准确增强。

Method: 提出结构约束的语言指导扩散模型（SLDM），结合结构先验信息约束模型推理过程，引入带有空间智能的语义监督，增强模型对目标结构识别能力。此外，还设计了减影血管增强模块，提高目标区域的可观察性。

Result: 通过定性视觉对比分析和多项定量指标，验证该方法在低剂量对比剂CT血管成像增强中的有效性，重建效果优于现有方法。

Conclusion: SLDM方法能在减少对比剂用量的同时，稳定实现结构一致的图像增强，为低剂量CT血管成像提供了有力技术支持。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [23] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff是一种新颖的三重先验引导扩散网络，实现了通用图像复原，在多种退化场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图像复原模型在处理严重退化区域时重建效果不佳，即便结合语义信息，也易对空间结构造成破坏，如引入模糊等伪影。因此，亟需一种既能充分利用多源先验信息，又能兼顾空间结构与高层语义感知的统一模型。

Method: 作者提出Triple-Prior Guided Diffusion (TPGDiff)网络，在扩散过程全程引入退化先验，并在浅层引入结构先验、深层引入语义先验，实现分层和互补的先验信息引导。采用多源结构线索为结构先验，提升细节刻画能力，通过蒸馏驱动的语义提取器生成强鲁棒性的语义先验，并通过退化提取器学习退化感知先验，实现扩散过程的自适应控制。

Result: TPGDiff在多种单一及复合图像退化基准上均显著优于现有方法，展现了更好的性能与泛化能力。

Conclusion: TPGDiff通过三重分层先验引导的扩散架构，有效提升了图像复原质量与系统鲁棒性，为通用图像复原提供了可行的高效新范式。

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [24] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: 本文提出了OSDEnhancer，一种基于高效单步扩散模型的新型时空视频超分辨率（STVSR）方法，在面对复杂未知真实退化的视频时实现了细致、时序一致的高分重建，并取得了最优性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在视频超分辨率领域表现优异，但在同时提升分辨率与帧率的时空超分辨率任务（STVSR）中尚未充分挖掘其潜力，特别是在实际复杂退化情况下，现有方法难以保证高保真度与时序连贯性，因此亟需一个更健壮、灵活的STVSR框架。

Method: 提出了OSDEnhancer框架。该方法首先利用线性预插值初始化关键时空结构，然后通过专为时间一致性与空间细节设计的TR-SE MoE架构进行联合优化，并引入双向可变形VAE解码器进行时空聚合和序贯传播，实现高效单步扩散推理。

Result: OSDEnhancer在真实世界场景下展现出领先的STVSR性能，实验结果显示其在细节恢复和时序一致性方面优于现有方法，并具备更强的泛化能力。

Conclusion: OSDEnhancer突破了传统STVSR在复杂退化场景下的瓶颈，以单步扩散和专家混合结构实现了高效、可靠的视频超分辨率，是STVSR研究的重要进展。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [25] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi是一种通道排列不变的多元时间序列预测新框架，能够处理通道顺序变化和新增通道的问题，并在多个数据集上实现了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测方法若依赖通道顺序（channel-dependent），易过拟合于通道顺序，不适应通道变化；若忽略通道关系（channel-independent），又损失交互信息，导致性能受限。作者希望找到既能捕捉通道依赖，又具有顺序无关性的方案。

Method: CPiRi提出通道排列不变（CPI）的方法框架，包括空间-时间解耦结构和通道排列不变正则化。具体做法是：冻结并预训练时间编码器提取时序特征，小型空间模块学习通道之间的关系，并在训练中随机洗牌通道，强化模型对通道排序的鲁棒性。此外，还进行了理论上的排列等变性分析。

Result: CPiRi在多个公开基准测试中实现了最优表现。模型对通道顺序变化表现稳定，即使只在部分通道上训练，也能很好地泛化到未知通道，并且在大规模数据集上效率良好。

Conclusion: CPiRi有效解决了多元时间序列预测在通道顺序和结构变化下的泛化难题，兼顾准确性与灵活性，具有理论基础和实际应用价值。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [26] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 本论文提出了两种新方法，提升了基于3D高斯拟合的表面重建准确性：一种基于可见性的多视角几何一致性约束，以及一种递进式四叉树校准的单目深度约束，在DTU和TNT数据集上效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splatting在渲染和优化上表现良好，但表面重建精度仍受限。已有方法通过优化高斯深度估计提升重建，但多视角一致性在存在几何差异时不稳，单目深度又有尺度歧义和局部不一致问题，导致深度监督不准确。因此需要新的约束方法以提升监督稳定性和几何精度。

Method: 1）设计了一种基于高斯原语可见性聚合的多视角几何一致性约束，提升视角间一致性和几何监督的准确性。2）提出递进式四叉树分块校准的单目深度约束，实现分块仿射校准，从粗到细缓解尺度歧义，兼顾细节表达。方法有效结合视角信息和单目深度。

Result: 在DTU和TNT两个公开数据集上，提出方法在几何精度上相较现有高斯和隐式表面重建方法均有持续提升，实验结果显示新约束有效提高重建的准确性和细节表现。

Conclusion: 论文综合利用多视角和单目深度信息，通过创新的可见性一致性和尺度校准约束，显著提升Gaussion Splatting下的表面重建精度，并验证了方法在多个数据集上的有效性和优越性。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [27] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于拓扑数据分析（TDA）和最优传输（OT）的异常分割方法TopoOT，能够在分布变化下稳健检测2D和3D异常，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的异常分割方法依赖于阈值二值化，容易在数据分布变化时失效，且只能捕捉局部波动。作者希望通过TDA提升异常检测对于全局结构变化的敏感性，并使方法能够高效适应领域转移。

Method: TopoOT通过整合多层次滤波的持久性图和测试时自适应，将拓扑结构信息编码进异常检测流程。创新点在于“最优传输链式对齐”，跨不同阈值与滤波关联持久性图谱，形成鲁棒的地质稳定性得分，用于指导伪标签生成，并联合最优传输一致性和对比损失训练轻量化分割头，实现实时自适应。

Result: 在标准2D和3D异常检测基准上，TopoOT均取得了SOTA表现，相较于最优竞品，F1均值得分2D提升至24.1%，3D提升至10.2%。

Conclusion: TopoOT框架证明了拓扑结构与最优传输相结合、引入测试时自适应机制在异常分割任务中特别是分布转移时的优越性与鲁棒性，具有重要应用与研究价值。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [28] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMSF的多任务多模态监督框架，用于整合病理全视图图像与临床数据，实现更准确的肿瘤预后预测，并在多个数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 病理图像和临床数据分别包含肿瘤形态和患者相关背景信息，二者的有效整合对肿瘤预后分析至关重要。然而，这两类特征空间的统计特性和尺度差异使得多模态信息融合极具挑战。

Method: 作者提出基于线性复杂度的多示例学习（MIL）主干设计MMSF框架，核心包括四个模块：图结构特征提取（嵌入组织拓扑信息）、临床数据标准化嵌入、模态对齐特征融合和Mamba-MIL多任务预测。该框架显式分解融合共享和特异性表征。

Result: 在CAMELYON16与TCGA-NSCLC两个主流数据集上，MMSF相比强基线方法准确率提升2.1–6.6%，AUC提升2.2–6.9%；在五个TCGA生存分析队列中，相比单模态和其他多模态对比方法，C-index提升分别高达7.1–9.8%与5.6–7.1%。

Conclusion: MMSF能有效融合病理图像与临床数据，提升肿瘤预后预测能力，在多数据集、多任务上均显著优于传统和现有多模态方法。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [29] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: Palmprint 识别受异构部署条件影响，现有方法多对数据分布变化适应性差。文中提出 PalmBridge，通过特征空间对齐和向量量化，提高跨数据集泛化能力，减少真实场景性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有掌纹识别方法常假设训练和测试数据分布一致，实际应用中由于采集设备、环境等差异导致特征分布转移，导致识别准确率下降。常规的数据增强手段难以充分模拟目标环境分布，需求更有效的泛化方法。

Method: PalmBridge 框架为一种特征空间对齐方法。它通过向量量化从训练特征中学习一组紧凑的代表向量，并在注册和识别阶段将特征映射至最近代表向量，再与原始特征融合，从而抑制域偏差带来的干扰，但保留身份判别信息。代表向量与主干网络通过多重损失项联合优化。此外，作者通过 assignment consistency 和 collision rate 等指标分析模型表现。

Result: 在多个掌纹数据集和骨干网络上，PalmBridge 能在开放集评估下一致性地降低 EER（等错误率），并提升跨数据集泛化能力，同时不会显著增加计算开销。

Conclusion: PalmBridge 利用特征空间对齐与向量量化，有效缓解了因为实际环境差异导致的识别精度下降问题，是一种高效且适应性强的掌纹识别新方案。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [30] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了SpatialGenEval评测基准，系统性测试文本生成图像（T2I）模型的空间智能能力，并指出当前主流T2I模型在复杂空间推理方面仍存在显著瓶颈。研究还建立了SpatialT2I数据集，微调后的模型空间表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型虽然在生成高质量图片方面取得成功，但在处理复杂空间关系（如空间感知、推理与交互）上表现较差；这主要由于现有评测基准对空间信息关注不足，因此需要专门的数据集和评测方法系统性衡量空间智能。

Method: 作者设计了SpatialGenEval基准，包含1230个信息密集的长提示词，涵盖25种真实场景与10类空间子域，并配套选择题评估空间感知和推理能力。同时，作者构建了15,400组文本-图片对的SpatialT2I数据集，通过重写提示词保障信息的丰富性和图像一致性，并在主流模型上进行微调实验。

Result: 通过对21个主流T2I模型评测，空间推理能力被证实是主要瓶颈。利用SpatialT2I微调后的模型在空间关系表现上有明显提升，3个代表性模型性能分别提升了4.2%、5.7%、4.4%。

Conclusion: 信息密集型提示词与数据驱动方法对提升T2I模型空间智能具有重要促进作用，未来应注重构建更丰富的空间认知训练数据和评测体系。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [31] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了CURVE框架，结合因果推理和不确定性建模，有效提升场景图的泛化能力，减少对环境噪音的过度拟合。


<details>
  <summary>Details</summary>
Motivation: 现有场景图容易对训练数据中的伪相关性过拟合，导致在新环境中的表现较差，缺乏强泛化能力。

Method: 提出CURVE框架，融合变分不确定性建模和基于不确定性的结构正则化；通过基于原型的去偏方法，区分环境无关和环境相关的交互信息，从而得到更加稀疏且稳定的场景图结构。

Result: 在零样本迁移和小数据量的仿真到现实场景自适应实验中，CURVE展现了较强的领域泛化能力，并能提供可靠的不确定性估计，用以支持分布转移下的风险预测。

Conclusion: CURVE 能有效抑制环境相关的高方差关系，提升场景图在不同环境下的泛化和稳定性，是场景理解任务中的有力工具。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [32] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的RGB图像到RAW图像的重建方法RAW-Flow，通过生成式建模和潜空间流匹配，有效提升反向ISP任务的精度和细节恢复能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB到RAW的学习方法多视为直接回归任务，因逆ISP问题本身的不适定性及RGB量化过程中信息丢失，导致重建出的RAW图像细节和色彩精度不佳。作者希望创新性地解决这些难点。

Method: 提出RAW-Flow框架，将RGB到RAW重建建模为确定性的潜空间传输问题，采用流匹配方法学习潜空间中的向量场，并通过跨尺度上下文引导模块增强特征融合；设计双域自编码器及特征对齐约束，以联合编码RGB与RAW并促进高质量重建。

Result: 大量实验表明，RAW-Flow在重建精度和视觉效果两方面均超过当前主流方法。

Conclusion: 采用生成式潜空间建模和流匹配提升了RGB到RAW反向建模的重建能力，为相关领域提供了更强大有效的解决方案。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [33] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 本文提出了一种融合RFID门禁与多传感器环境安全监测的物联网一体化系统，采用云端架构提升互操作性与响应效率，并在实验中展示了高准确性、低成本和良好的容错性。


<details>
  <summary>Details</summary>
Motivation: 传统物理安全系统与环境安全监测通常各自独立，导致运维效率低、应急响应慢、管理复杂。为提升智慧基础设施的智能化管理，亟需一种高效整合的技术框架。

Method: 设计了双子系统：子系统1实现RFID身份认证、舵机门控和实时数据云记录，子系统2集成火焰检测、水流量监控、LCD状态显示与人员识别。两者均基于ESP32进行边缘计算与无线通信，通过智能本地缓存和统一云平台进行协调与管理。

Result: 系统在45天实验中实现99.2%的RFID认证精度、0.82秒响应、98.5%的火焰检测可靠性（5米范围内）、99.8%的云数据记录成功率，且断网时可本地缓存保持业务连续。成本仅5400 BDT（约48美元），比商业产品成本降低82%。

Conclusion: 通过架构优化和关键组件选型，所提出的融合安全-安全监测物联网系统在性能、成本和可靠性上均达到了专业级表现，具备广泛应用前景。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [34] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: 本文提出了一种名为RepSFNet的轻量级网络架构，能够在降低计算复杂度的同时实现准确且实时的人群计数。该网络在多个主流数据集上表现优异，并大幅减少推理延迟，适合实时和低功耗边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数方法在面对尺度变化、遮挡等问题时容易失效，并且模型参数和计算负担较重，难以满足实时与低功耗场景需求。因此需要一种结构简洁且有效的模型。

Method: 提出RepSFNet网络，采用RepLK-ViT主干以大卷积核进行多尺度特征提取，利用ASPP与CAN模块融合实现密度自适应的上下文建模，并通过拼接融合模块保留空间分辨率。损失函数结合均方误差与最优传输损失。该结构避免了注意力和多分支设计，显著减少了参数量和计算量。

Result: 在ShanghaiTech、NWPU和UCF-QNRF数据集上，RepSFNet在准确率上达到与最新方法媲美的水平，同时推理延迟最多降低34%。

Conclusion: RepSFNet兼具高精度与低延迟，适用于实时、低功耗的人群计数应用，在实际部署方面具有较大潜力。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [35] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出了一种名为HINT的层次化交互建模扩散式多人体动作生成框架，能灵活处理多人数、复杂交互和长时序文本引导的问题，实现了当前最佳的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有多人体动作生成方法大多局限于离线生成（固定长度、固定人数），难以处理可变文本长度和人数变化，且对长期序列和复杂交互的建模能力不足。

Method: HINT是首个采用层次化交互建模的多人体动作生成自回归框架。其一，HINT在规范化潜空间使用解耦姿态表示，分离本地动作语义和个体间交互，使系统可无需专门调整即支持人数变化。其二，结合滑窗策略，局部窗内细致建模、窗间全局聚合条件，有效记录历史、建模关系并与文本对齐，兼顾效率和时序一致性。

Result: 在多个公开评价基准上，HINT与最强离线模型表现相当，超越已有自回归方法。在InterHuman数据集上，HINT的FID分数由5.154提升至3.100，显示显著性能提升。

Conclusion: HINT有效突破了传统方法在人数和文本长度扩展性、长期一致性和交互复杂性方面的局限，为多人体动作生成提供了新思路和更强性能保障。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [36] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: 提出BiFTA方法，通过同时去除图像和文本中的冗余信息，有效提升了视觉-文本模型的零样本能力。


<details>
  <summary>Details</summary>
Motivation: 在现有视觉-语言模型中，细粒度文本描述和本地化图像patch虽然提升了零样本能力，但两者都存在冗余信息，影响了对齐效果。需要研究如何处理冗余以增强模型表现。

Method: 方法分为两步：（1）View refinement——通过IoU筛除重叠度高的图像patch，使视觉特征更具代表性；（2）Description refinement——根据文本描述余弦相似性移除重复或相似文本，确保文本多样性。二者结合称为BiFTA。

Result: 在6个基准数据集上，BiFTA显著提升了基于ViT和ResNet结构的CLIP模型零样本任务表现。

Conclusion: 去除视觉与语言模态中的冗余信息对提升文本-视觉对齐和零样本能力非常关键，BiFTA方法证明了这一点。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [37] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 本文提出了一种结构感知的点云生成框架——Quartet of Diffusions，通过显式建模部件组合和对称性，实现高质量、结构一致且可控的三维点云生成。


<details>
  <summary>Details</summary>
Motivation: 现有点云生成方法通常整体处理形状，或仅支持部件组合，无法同时保证对称性与部件先验的完整整合，限制了生成结果的多样性与结构合理性。

Method: 该方法协调四个扩散模型，分别学习全局形状隐变量、对称性、语义部件及其空间组合，从而实现结构解耦、可解释与可控的生成流程，并用全局隐变量进一步维持结构一致性。

Result: 实验表明Quartet方法在生成多样性、结构一致性及输出质量方面均达到了最新水平，优于已有方法。

Conclusion: Quartet是首个在点云生成全过程中同时集成且强制施加对称性与部件先验的3D点云生成框架，实现了更高水平的结构可控性和生成质量。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [38] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing是一种高效灵活的文档解析模型，在多个内容抽取任务中实现了更快速度和更优性能，对实际大规模文档处理应用价值突出。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析模型在速度和多样化文档内容处理上存在一定局限，难以满足大规模、多结构、多样内容的实际需求，亟需兼顾高性能与实用性的创新方案。

Method: Youtu-Parsing采用原生ViT作为视觉编码器，结合prompt引导的轻量型大语言模型Youtu-LLM-2B，实现文档特征提取、版面分析及结构化内容区域解码。提出Token并行和Query并行解码策略，分别在单步内并发生成多个token、同时预测多区域内容，大幅提升推理速度。

Result: 模型在OmniDocBench和olmOCR-bench基准上达到了最新SOTA表现，速度比传统自回归解码提升5-11倍，在表格等结构化文档场景表现优异，并具备处理稀有字符、多语种、手写内容的强泛化能力。

Conclusion: Youtu-Parsing在文档解析任务中展现出极高的实验价值与实际应用潜力，特别适合大规模文档智能处理场景。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [39] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型（VLMs）的多模态对齐与强化学习方法（MARE），以提升Deepfake检测的准确性、可靠性和可解释性，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测方法主要依赖分类或空间定位，但随着生成模型的快速进步，传统方法面临新的挑战，需要更高准确性、可解释性和与人类偏好一致的判别机制。

Method: 提出MARE框架，通过设计融合人类反馈的强化学习奖励函数，使模型生成与视觉空间一致且符合人类偏好的推理内容，同时加入伪造特征解耦模块以捕捉更本质的伪造线索。

Result: 在多个定量和定性实验中，MARE在准确性和可靠性指标上实现了当前最优表现。

Conclusion: 通过多模态对齐、强化学习与人类反馈、以及伪造特征解耦三大创新，MARE有效提升了视觉-语言模型在Deepfake检测中的性能和可解释性，具有广阔的应用前景。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [40] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 本论文提出了一种通过使用生成器的最后一层对真实图片进行“污染”，训练检测器区分原始真实图片与被污染图片的新方法，从而提升检测AI生成图片的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测器在遇到未见过的生成器生成的图片时，泛化能力较差。尽管最新的生成器架构多样，但其最终用于图像生成的部件常有共性，因此针对这一结构特征来提升检测效果。

Method: 作者首先提出用生成器最后一层“污染”真实图片，然后训练一个检测器区分原始图片与污染图片。此外，论文基于生成器的最终部件提出了分类方法，并据此对21种广泛使用的生成器进行了归类。方法使用DINOv3作为检测骨干，只在三个典型类别各用100张样本就进行微调训练。

Result: 在22个来自未见过的生成器的测试集上，所提出检测器平均准确率达到98.83%。

Conclusion: 通过关注生成器的共性部件，可以大幅提升检测AI生成图片模型对新型生成器的泛化能力。该方法有效且高效，在实际应用中有良好前景。

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [41] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dummy Forcing的方法，优化自回归视频扩散模型中自注意力机制的头部利用效率，大幅提升了推理速度且几乎不损失视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频扩散模型在自注意力机制中不同的头对历史帧的利用不足，大约有25%的头几乎只关注当前帧，这导致了记忆和计算资源的浪费。如何更高效地利用上下文信息，提高推理速度，是一个亟需解决的问题。

Method: 作者分析了现有模型中多头注意力历史信息利用的低效，提出了Dummy Forcing方法，动态限制不同头可访问的上下文，提高头间的异质性。其核心做法包括异构记忆分配、动态头分类，以及一种上下文压缩技术，无需额外训练即可实现。

Result: Dummy Forcing方法在无须重新训练的情况下，将缓存压缩并减少多余的KV（Key-Value）存储，实现了最高2倍的推理速度提升，视频生成速度可达24.3 FPS，同时视频质量下降不足0.5%。

Conclusion: 该方法能有效压缩推理时的缓存并提高速度，无需改变模型结构和训练过程，在保证近乎无损质量的基础上，显著提升自回归视频扩散模型在生成视频任务中的推理效率。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [42] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 该论文研究了如何用部分标注数据训练能够区分和分割脑白质高信号和缺血性卒中病灶的深度学习模型，并发现伪标签方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 脑小血管病相关的白质高信号和缺血性卒中病灶在MRI上难以区分，但对疾病诊断至关重要。由于数据标注困难，建立准确的自动分割模型具有挑战性。

Method: 作者收集了共2052份脑部MRI，包括完全和部分标注的私有及公开数据集，研究了六种结合脑白质高信号与缺血性卒中病灶分割的训练策略（包括伪标签方法）以充分利用部分标注数据。

Result: 多种利用部分标注数据的方法提高了模型表现，其中伪标签方法取得了最佳分割效果。

Conclusion: 充分利用部分标注数据（尤其是伪标签方法）能够提升深度学习模型对白质高信号和缺血性卒中病灶的分割与区分能力。这为实际临床应用提供了更有效的数据利用方式。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [43] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 针对动态视频生成中的时序一致性难题，提出了一种基于时域感知的loss加权方法Latent Temporal Discrepancy (LTD)，显著提升了运动区域的生成质量。


<details>
  <summary>Details</summary>
Motivation: 虽然视频生成模型在静态场景下取得了进展，但面对包含强烈动态变化的运动视频时，生成质量大幅下降，主要问题在于噪声干扰了时序一致性及动态区域的学习。现有扩散模型统一采用静态loss，对动态场景建模能力受限。

Method: 提出Latent Temporal Discrepancy (LTD)作为运动先验，通过在潜在空间中度量帧间变化幅度，对高差异区域加大loss权重，对稳定区域维持常规优化，实现loss的时域自适应加权，从而更好地引导模型捕捉复杂的动态特征。

Result: 在VBench和VMBench两套基准数据集上进行大量实验，结果显示新方法在总体性能上分别比强基线提升了3.31%和3.58%，在运动质量方面有显著提升。

Conclusion: 引入时域感知的LTD loss加权，有效提升了动态区域的生成质量和整体视频的时序一致性，对高动态场景的视频生成具有重要意义。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [44] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 该论文提出了肖像集合生成（PCG）这一新任务，并发布了大规模数据集CHEESE及相应的生成方法SCheese，实现了通过自然语言编辑肖像图片集并保持高度细节和身份一致性的能力。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的发展，用户对简单直观且能生成高质量多样化肖像图片集的需求日益增长。现有方法难以实现复杂多属性编辑和细节高度保真的肖像集合生成。

Method: 作者构建了CHEESE数据集，包含2.4万组肖像集合及57.3万注释样本，并提出SCheese框架。SCheese结合了分层式身份与细节保持，通过自适应特征融合确保身份一致性，并利用ConsistencyNet注入细粒度特征以保证细节的一致性。

Result: 实验证明，CHEESE数据集有助于推动PCG研究，SCheese方法在各种标准上达到了当前最优性能。

Conclusion: 论文为多样高质量肖像集合的生成开辟新方向，提出的数据集和方法显著提升了任务表现，推动了该领域的发展。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [45] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 本文发现现有扩散式多模态大模型在解码加速时常出现重复文本生成问题，即“Repeat Curse”。分析发现该问题源于语境信息流和不收敛的熵。提出了一种CoTA方法，通过增强语境注意力和引入罚项，显著缓解了重复问题。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大模型为减少推理延迟常用缓存加速，但这带来了重复文本输出，不仅影响生成质量，也阻碍了模型实际部署和用户体验。因此，研究其成因并提出有效缓解方法是重要问题。

Method: 首先作者从信息流视角分析了重复问题，揭示语境token作为语义锚点，随着网络加深，token熵收敛。发现重复通常和信息流中断及token熵不收敛有关。基于此，作者提出CoTA方法：增强语境token注意力以保持信息流通，且在解码时对不确定的token信心值加罚，抑制由不确定性导致的重复。

Result: CoTA方法在多项实验任务上均显著抑制了重复问题，并获得了一致的性能提升，表现出良好的通用性和有效性。

Conclusion: 本文系统分析了扩散式大模型重复文本生成的内在机制，并提出了简单有效的CoTA方法，实验证明其能提升模型输出的多样性和质量，为类似任务提供了新思路。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [46] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无监督异常检测框架AnomalyVFM，可将任何预训练视觉基础模型（VFM）转换为强大的零样本异常检测器，通过合成数据集生成与高效模型适配，实现比当前主流方法更优的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（如CLIP）的零样本异常检测方法效果较好，但纯视觉基础模型（如DINOv2）相关方法表现较差，主要归因于辅助数据集多样性不足与模型适配策略过于浅显。

Method: AnomalyVFM结合三阶段合成数据集生成方案与参数高效的模型适配机制（低秩特征适配器和置信加权像素损失），提升视觉基础模型在异常检测上的表现。

Result: 在以RADIO为骨干的条件下，AnomalyVFM在9个多样化数据集上平均图像级AUROC达94.1%，比最新方法高出3.3个百分点，展现出显著性能优势。

Conclusion: 通过创新的数据生成与模型适配技术，AnomalyVFM大幅提升了VFM在无监督异常检测任务上的表现，推动了相关领域的技术发展。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [47] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA框架结合数据驱动的Black Box与知识驱动的White Box模块，通过引入可解释的纠错知识引导模型下游适应，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高效参数微调方法将预训练模型视作黑盒，忽视了其内在知识，对下游任务适应能力有限。作者希望充分利用模型内在知识提升适应效果。

Method: 提出IOTA框架：包含数据驱动的Black Box模块和知识驱动的White Box模块。White Box对比正确与错误预测，抽取可解释纠错知识并生成人类可读的prompt，通过纠错知识引导Black Box模型调整预测。

Result: 在12个图像分类基准上，IOTA在少样本和不同难度的适应设置下，均显著优于现有SOTA方法，证明了纠错知识和知识引导机制的有效性。

Conclusion: IOTA通过知识和数据双重驱动，有效利用纠错知识引导下游任务适应，为参数高效微调提供了新的思路，提升了模型适应能力。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [48] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World 是一个开源世界模拟器，能够高保真、实时地生成多种风格的视频世界，并具有长期记忆和低延时交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前业界对于高质量、长时序、一致性强且可交互的世界模型需求巨大。但许多先进世界模型为闭源或功能有限，制约了广泛应用和技术创新。作者希望通过开源LingBot-World，缩小开源与闭源技术的差距，推动社区发展。

Method: LingBot-World以高效视频生成模型为基础，优化了环境多样性支持、长时序上下文一致性（即“长期记忆”）和实时交互。系统能覆盖逼真、科学、卡通等多种环境，并保证16帧/秒下低于1秒的延迟。作者将代码与模型全部公开。

Result: LingBot-World在环境多样性、长期一致性和实时响应等方面表现优异，满足内容创作、游戏和机器人学习等实际需求。

Conclusion: LingBot-World的开源发布为社区提供了高性能世界模拟工具，有助于促进内容创作、交互式应用和相关学科的发展，弥补了开源与闭源之间的差距。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [49] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: 本论文提出DeepSeek-OCR 2，通过全新的DeepEncoder V2模块，使视觉Token可基于图像语义进行动态重排，而非传统的固定顺序处理，从而提升模型对复杂布局图像的理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言模型在处理视觉信息时采用严格、固定的扫描顺序，这与人类灵活且受语义驱动的视觉感知机制不符，尤其限制了模型对复杂结构图像的理解。作者希望仿效人类的视觉认知和推理过程，改进机器的视觉处理流程。

Method: 作者设计了一种全新的编码器（DeepEncoder V2），为视觉编码过程引入因果推理能力，实现视觉Token在输入大模型之前的智能重排序。具体探索将2D图像理解转化为级联的1D因果推理结构方案。

Result: 实验结果表明，利用DeepEncoder V2动态重排视觉Token的新架构，有效增强了模型对复杂布局和语义结构图像的理解能力。模型及代码已公开。

Conclusion: 通过模拟人类的视觉因果推理，本研究证实动态重排视觉Token可提升VLMs理解复杂2D图像的能力，为后续相关视觉-语言模型设计提供了新的思路和架构基础。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [50] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 本文提出了DiffVC-RT，实现了实时扩散模型驱动的视频感知压缩，在信息损失、推理速度和时序一致性方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在神经视频压缩（NVC）中虽然生成质量较高，但面临信息丢失严重、推理延迟高和时序一致性差等实际部署难题。

Method: 1）提出高效且信息量丰富的模型结构，通过模块替换和裁剪降低计算复杂度，缓解信息损失；2）为了提升时序一致性，引入显式和隐式一致性建模，在U-Net中加入了零成本的在线时序偏移模块，并采用混合隐式一致性约束；3）提出异步并行解码管线和混合半精度实现，提高解码效率，实现帧级并行重建。

Result: 在HEVC数据集上，DiffVC-RT比VTM-17.0实现了80.1%的LPIPS码率节省，同时在NVIDIA H800 GPU上对720p视频可达到206 fps编码、30 fps解码的实时速度。

Conclusion: DiffVC-RT在扩散式视频压缩领域实现了感知质量提升和实时性能，显著改善了现有扩散模型的实用性，推动了相关技术的应用落地。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [51] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出StructAlign方法，通过结构化跨模态对齐，有效缓解CTVR（持续文本-视频检索）中的灾难性遗忘问题，并在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 持续文本-视频检索任务需模型在增量学习新类别的同时，保持已学类别的文本与视频的准确对齐，因此极易发生灾难性遗忘。主要挑战在于特征漂移：包括模态内特征漂移和跨模态对齐失效。

Method: 提出StructAlign方法，引入simplex Equiangular Tight Frame (ETF)几何结构作为统一先验，通过跨模态ETF对齐损失，推动文本与视频特征与类别级ETF原型对齐。同时，设计跨模态关系保持损失，利用多模态互补保持特征关系的稳定。两者联合以抑制模态漂移和灾难性遗忘。

Result: 在多个基准数据集上，StructAlign在所有评测指标上均超过当前最优的持续检索方法，表现出了更强的持续检索能力和抗遗忘性。

Conclusion: StructAlign 能有效解决CTVR中的跨模态与模态内特征漂移问题，显著提升持续文本-视频检索的效果，为多模态持续学习提供了新方法。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [52] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 本文对主流的行人再识别（ReID）模型和训练范式进行了对比分析，重点考察其在跨域应用场景下的表现，并探究基础模型在提升泛化能力中的作用。


<details>
  <summary>Details</summary>
Motivation: 行人再识别在实际应用中常面临跨域问题，现有模型的泛化能力有限，有必要系统比较不同训练范式（有监督、自监督、语言对齐模型）在跨域表现，同时分析基础模型在ReID中的应用价值。

Method: 对11个模型（涵盖有监督、自监督和语言对齐的基础模型）在9个数据集下进行系统性评测和对比分析，重点考察模型在原域和跨域下的鲁棒性，并特别关注诸如SigLIP2等基础模型的表现。

Result: 有监督模型在训练域内表现优异，但在跨域测试时效果显著下降；语言对齐的基础模型虽非专门为ReID设计，却在跨域任务中展现出出人意料的强大鲁棒性。

Conclusion: 基础模型（如语言对齐模型）在ReID跨域任务中较传统有监督模型更具泛化能力，有望成为提升实际应用ReID系统稳健性的关键方向。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [53] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 本文提出了CLEAR-Mamba框架，结合新颖的架构设计和训练策略，有效提升了眼科造影图像分类的泛化性和高置信度预测能力，并在多模态多疾病任务上获得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一模态的眼底造影图像分类方法存在泛化能力不足和高置信预测有限的问题，受限于设备之间差异和病灶特征的微妙差别。为此本研究旨在提升模型的跨域适应能力和对低置信样本的关注度，从而提升整体可靠性及实用性。

Method: 在MedMamba基础上，作者提出了CLEAR-Mamba框架，包括两个关键创新：（1）引入HaC超网络自适应条件层，能够根据输入特征动态生成参数，增强模型应对跨域场景的能力；（2）提出RaP可靠性感知预测机制，基于证据不确定性学习，重点优化原本低置信度样本，提高模型整体稳定性。

Result: 作者构建了大规模眼科造影（涵盖FFA和ICGA）数据集，用于多疾病分类任务。实验表明，CLEAR-Mamba在多项评价指标上均优于包括原始MedMamba在内的多种主流基线方法，特别是在多病种识别和可靠性评估方面表现突出。

Conclusion: CLEAR-Mamba有效平衡了眼科图像分类中的泛化能力和预测可靠性，为多模态医学影像特定分类任务提供了切实可行的新方案。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [54] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: 论文提出了一种用于多模态讽刺检测的新方法——生成式差异比较网络（GDCNet），通过引入由多模态大模型生成的客观图片描述作为语义锚点，提升了图片-文本讽刺识别的准确性，在多个公开数据集上达到了新的最好性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态讽刺检测方法主要依赖于跨模态嵌入不对齐来检测语义不一致，但面对图文相关性弱或语义间接时效果有限；而部分利用大模型生成讽刺线索的方法，由于生成内容的多样性和主观性，反而引入了噪声。

Method: 提出GDCNet网络，利用多模态大模型自动生成图片的客观描述，作为稳定的语义锚点。方法核心是计算生成描述与原有文本在语义和情感上的差异，以及图文间的一致性。然后这些差异特征与原始视觉和文本表示通过门控模块融合，实现跨模态特征的自适应平衡与讽刺检测。

Result: 在多个多模态讽刺检测基准（尤其是MMSD2.0）上，GDCNet展现出更高的准确率和鲁棒性，并刷新了当前最优结果。

Conclusion: GDCNet有效解决了现有方法在处理松散或间接相关的图文数据时的不足，通过引入稳定的、客观的描述锚点，提升了多模态讽刺检测的准确性和可靠性。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [55] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: 本文针对长周期、重复性任务缺乏评测基准的问题，提出了OS-Marathon数据集，并提出了一种低成本、多例举示范的方法以提升CUAs代理在此类任务上的表现。实验证明任务具有挑战性且方法有效。


<details>
  <summary>Details</summary>
Motivation: 在实际工作场景中如报销单处理、成绩录入等，长时间、重复性的任务让人类操作员感到枯燥繁重。同时，当前缺乏专门评估CUAs（计算机辅助代理）处理这类任务能力的基准数据集，成为领域发展的瓶颈。

Method: 文章构建了OS-Marathon评测集，涵盖242个在2个领域的长周期重复任务。并提出一种低成本的多示例（few-shot）学习方法，通过少量示例教会代理任务主逻辑，使其可以推广到更大、未知的数据集上高效执行。

Result: 实验分析显示这些任务本身极具挑战性，也说明了当前SOTA代理在长周期重复任务上的局限性。此外，提出的多示例方法能有效帮助代理学习和执行类似的长流程任务。

Conclusion: OS-Marathon为该领域提供了重要的评测基准和公开数据。提出的多示例教学法对提升专业场景下自动化代理效率具有实际价值，并推动了相关技术发展。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [56] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 本文提出了一种区域感知、基于频域的轻量级人脸变形攻击检测方法，在跨数据集和不同变形方式下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸变形攻击检测技术在跨数据集场景下性能大幅下降，尤其是在缺乏可信参考图像的单图像检测任务（S-MAD）中，这对电子身份验证和边境管控系统安全构成威胁，亟需提升检测系统的泛化能力。

Method: 方法包括：（1）在不同人脸区域的频域上分离真实图像与伪造图像；（2）提出残差频域建模，将信号频率与自然频谱衰减解耦，增强可分性；（3）通过马尔可夫随机场（MRF）融合不同面部区域的特征，实现全局、一致的检测决策。此外，方法仅用合成数据集SMDD进行训练。

Result: 该方法在FRLL-Morph数据集上的平均等错误率（EER）为1.85%，在MAD22数据集上排名第二，平均EER为6.12%。在低攻击分类错误率（APCER）条件下，也能保持较低的真实样本分类错误率（BPCER），仅用频谱特征即可取得良好效果。

Conclusion: 基于傅里叶频域残差建模和结构化区域特征融合的识别框架，在处理人脸变形攻击时表现出与深度学习架构相媲美甚至更优的性能，具有良好的跨数据集泛化能力和实用性。

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [57] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出了ProSkill，这是首个针对程序性任务的动作级技能评估基准数据集，引入绝对技能评估标注和高效注释协议，并揭示当前算法的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的技能评估研究多集中在体育领域，对于复杂的程序性（如制造、日常任务）活动没有大规模数据集，而且现有方法只限于有限动作、二元标签或成对评判，无法精准、广泛地评估技能水平。因此需要一个全面、可扩展的数据集与新评估协议。

Method: 作者提出了ProSkill数据集，首次为程序性任务提供了动作级别的技能评估，包括绝对标注和成对比较。他们设计了可扩展的新型注释协议，采用瑞士锦标赛（Swiss Tournament）机制高效实现成对比较，并用ELO评分系统将其聚合为一致且连续的全球分数。此外，用该数据集基准测试了当前主流算法，涵盖排名和成对范式。

Result: 基于ProSkill数据集的基准测试显示，当前主流技能评估算法在复杂程序性任务中表现不佳，凸显了技能评估的难度和ProSkill数据集的重要性。

Conclusion: ProSkill为程序性活动中的技能评估提供了第一套标准基准，推动了复杂技能评估算法的发展。公布的数据与代码将促进领域进步。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [58] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出了一种名为BiMoRS的轻量级双模态提示学习框架，用于提升遥感场景下视觉-语言模型的迁移与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然提示学习已在自然图像数据集上展现出较强泛化能力，但其在遥感影像领域中的迁移性尚未充分研究。遥感影像具有多标签场景、高类内差异和空间分辨率多样等独特挑战，导致现有提示学习方法难以直接应用于此领域。

Method: BiMoRS框架利用冻结的图像描述生成模型（如BLIP-2）为遥感影像生成文本摘要，将其用BERT分词器处理后，与来自CLIP编码器的高级视觉特征融合，再通过轻量级的交叉注意力模块实现基于文本-视觉融合表征的上下文化提示学习，全程无需改变CLIP主干网络。

Result: 在四个遥感数据集、三个领域泛化任务上进行了实验，BiMoRS无论在哪个任务上都展现出了优于强基线方法的性能提升，平均可提升2%。

Conclusion: BiMoRS有效克服了现有提示学习方法在遥感影像任务中的适用性问题，提升了视觉-语言模型在遥感领域的泛化能力。

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [59] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种名为LEAF的高效图像质量评价框架，可以大幅减少人工标注需求，同时保持与人主观评分高度一致的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在图像质量评价(IQA)中表现强，但适应这些大模型通常计算量大且依赖于大量人工主观评分(MOS)标注。作者认为制约关键并非模型感知能力，而是MOS的标尺校准。

Method: 提出LEAF框架：利用大模型作为教师，通过点对、对对偏好及决策可靠性等方式进行密集监督，将大模型的感知知识蒸馏给轻量学生模型；然后对学生进行少量MOS标注的校准，使其输出贴近人类评价。

Result: 在用户生成和AI生成的IQA基准上，LEAF方法显著减少了人工标注需求，并且与MOS的一致性依然很高。

Conclusion: 该方法使得在标注预算有限场景下，轻量化的IQA评估成为可能，兼顾效率与标注成本。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [60] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: 本文提出LEMON，一个专用于多模态大模型理解长时段教育视频的新评测基准，揭示了现有模型在多模态复杂推理上的局限。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型（MLLMs）在视觉、音频和语言任务上取得显著进展，但它们在需要复杂推理和跨模态整合的长篇、高知识密度教育内容上的表现尚未被系统评估。因此需要一个新的评测基准填补这一空白。

Method: 作者创建了LEMON基准，从5个学科29门课程收集了2277段平均约196秒的STEM讲座视频，生成4181个高质量问答对。LEMON涵盖六大任务和十二个子任务，突出多模态耦合、语义密集、时序与教学结构，并设计了多轮、不同认知层次的问题。

Result: 各类多模态大模型（如GPT-4o）在实验中不同任务上的表现存在明显差距，尤其在时序推理和教学内容预测方面表现不佳。

Conclusion: LEMON为长时段、复杂知识内容的多模态模型提供了新的挑战和测试平台，有望推动其在感知、推理与生成方面的进一步发展。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 本文全面回顾了视觉编解码和视觉token技术两大主流技术体系，提出了它们在优化压缩效率与模型性能之间权衡的统一看法，并展望了下一代视觉编解码与token技术的发展方向，对跨模态大模型及其应用有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型(LLMs/MLLMs)等人工智能技术的发展，压缩效率被发现与模型性能和智能能力密切相关。因此，有必要系统梳理并统一传统的视觉信息压缩（视觉编解码）与新兴视觉token技术之间的联系，为智能视觉系统提供理论指导。

Method: 首先，分别综述了视觉编解码和视觉token技术的原理及发展现状。其次，从优化视角统一这两类技术，提出压缩效率与模型性能的权衡分析框架。基于这一统一公式，融合二者的互通启发和潜力，进一步对未来技术走向进行预测。最后，通过实验验证了面向任务的token技术在多模态大模型、AIGC、具身智能等实际应用中的巨大潜力。

Result: 文章提出并验证了一种统一的分析框架，能够同时解释和指导视觉编解码与token技术在保持信息语义的前提下提升压缩与计算效率。实验结果表明，任务导向的token技术在多种AI实际任务中具有明显优势。

Conclusion: 视觉信息压缩的本质在于信息语义与计算成本间的优化，未来视觉token技术有望形成如传统编码标准那样统一高效的标准体系，为广泛的智能任务提供基础支撑。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: 本文提出了FairT2V，一个无需微调即能减少文本到视频生成模型中性别等人口统计偏见的去偏方法，通过中和文本编码器的嵌入向量，有效降低生成视频中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本到视频生成技术的迅速发展，相关的性别等人口统计学偏见问题未被充分研究和解决。在生成内容广泛应用的背景下，解决模型本身的结构性偏见对公平性和实际应用意义重大。

Method: 作者首先分析了T2V模型中的性别偏见来源，发现主要源于预训练文本编码器，并提出以锚点为基础的球面测地线变换中和嵌入（FairT2V）。该方法只在生成初期进行去偏，以兼顾语义保持与视频时序一致性。偏见度量方面引入性别偏向分数，并结合VideoLLM推理及人工验证进行评估。

Result: 在Open-Sora等先进T2V模型上的实验显示，FairT2V在不同职业类别下显著降低了性别等人口统计学偏见，同时对视频质量影响极小。

Conclusion: FairT2V方法无需额外训练便能有效减少T2V模型的性别偏见，为生成式AI模型的公平性和实际部署提供了有力支持。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为FunHSI的无训练、基于功能的3D人类与场景交互生成框架，通过任务提示生成语义和物理可行的3D人-场景交互，实现了比以往方法更真实、更具功能性的交互效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D人-场景交互方法缺乏对物体功能和人体与场景接触的显式推理，导致生成的交互常常不符合实际功能、物理上不合理，影响了在智能体AI、机器人、交互内容生成等领域的应用价值。

Method: FunHSI无需训练，直接根据任务语言提示进行功能驱动的场景元素识别和3D重建，并用接触图建模高级交互；结合视觉-语言模型生成执行任务的人体图像并估算3D姿势，最后通过分阶段优化确保人体姿势的物理可行性和功能正确性。

Result: FunHSI能生成高度合理、物理可行、符合任务功能的3D人-场景交互，无论是如“坐在沙发上”这类常规动作，还是如“调高房间温度”这样的细粒度交互，都表现优异。大量实验表明其在多样化的室内外场景中表现出优于现有方法的效果。

Conclusion: FunHSI突破了现有模型只能生成粗略或无功能交互的限制，实现了基于开放式语言任务精准、合理的3D人场景交互，为智能体AI、机器人等领域带来更高实用性。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 本文提出了一种结合图像与惯性测量单元（IMU）数据的多模态路面分类框架，并发布了一个全新的包含多模态和多样环境的大型数据集ROAD。该方法较现有技术在多个基准测试上取得了显著性能提升，尤其在复杂、多变环境下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有路面表面分类方法受限于单一传感器和环境变化不丰富的数据集，导致模型在新的或极端环境下难以泛化，实用性有限。解决这一问题有助于提升智能维护系统、自动驾驶等领域的稳定性和安全性。

Method: 作者提出了一种轻量级的双向交叉注意力模块，对图像与IMU数据进行融合，同时利用自适应门控层应对域转移情景下不同模态贡献变化。此外，构建了新的ROAD数据集，包含真实多模态、高异质视觉、以及仿真路面三大子集，涵盖各种环境、天气及照明条件。

Result: 在公开PVS基准上，提出方法比前沿方法提升1.4个百分点；在自建多模态ROAD子集上提升11.6个百分点，并显著提升少数类的F1分数。方法在夜间、暴雨、复杂路面转换等困难视觉场景下，表现出高度稳定性。

Conclusion: 结合廉价相机与IMU，并用多模态注意力机制进行融合，是实现经济、高鲁棒性路面理解的可行路径，尤其适合环境变化大、设备成本受限的应用场景。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出FreeFix方法，使用预训练的图像扩散模型提升新颖视角合成中的外推效果，无需微调，兼顾高保真和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF和3D高斯点云等方法在稠密输入下表现良好，但在新颖视角特别是外推任务中效果下降。即便结合扩散模型提升监督，现有方法也面临在泛化与保真度之间的权衡。

Method: 提出FreeFix，不需要对扩散模型进行微调，通过交错的2D-3D细化策略，用预训练图像扩散模型对渲染结果进行一致性细化，并引入逐像素置信度掩膜，针对不确定区域进行重点改进。

Result: 在多个公开数据集上实验表明，FreeFix能提升多帧一致性，其效果可与基于微调的方法媲美甚至超越，同时保持较强的泛化能力。

Conclusion: FreeFix无需微调即可充分利用预训练图像扩散模型，在新颖视角渲染任务中取得较好效果，为保持高泛化性和高保真度提供了解决思路。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于评分细则（rubric）的人类专家检测方法，能有效区分韩语人为文本和大语言模型（LLM）生成文本，其准确率通过结构化训练提升至100%，表现优于主流自动检测器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的流利韩语文本让连受过训练的读者都难以区分真伪。作者关注于能否通过结构化训练提升人类专家识别AI文本的能力，解决现有检测手段准确率不足的问题。

Method: 研究引入了基于韩国国家写作标准开发、针对微观特征（如标点用法、空格和语体变化等）的LREAD评分细则。通过三阶段长时盲测实验：1) 直觉判断；2) 用打分细则要求并阐明理由；3) 在新的小学生作文上考察领域精通度。

Result: 专家检测准确率由阶段一的60%提升至阶段三的100%，并且标注者间一致性显著增强（Fleiss’ kappa由-0.09增长到0.82）。人类专家借助细则，能捕捉到现有自动检测器难以识别的、与语言相关的细微诊断特征。

Conclusion: 结构化评分细则能极大提升人类专家区分韩语AI文本的能力，是现有自动检测器的有效、可解释补充。文中公布了评分细则及一套检测特征分类体系，为非英语环境下AI文本识别提供了新思路。

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [67] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的合成多轮工具调用对话数据的方法DiGiT-TC，可在无状态环境下生成仿真具备状态特征的对话，用于提升小型语言模型在多轮工具调用上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有合成多轮工具调用数据的方法大多假设存在一个能追踪状态的执行环境，这不适用于如数据安全要求高或工具规范多源合成的无状态实际应用场景。

Method: 提出DiGiT-TC数据生成方法，通过创新生成模式，使用户请求中隐式包含特定工具调用，仿真出类似状态环境下产生的对话数据。

Result: 在标准工具调用基准测试上验证了DiGiT-TC方法，即使在有状态问题环境下，也显著提升了模型表现。

Conclusion: DiGiT-TC能够在无状态环境下生成高质量具有状态特征的对话数据，有效弥补了此前方法的局限，并显著增强了模型在实际多轮工具调用场景中的能力。

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [68] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: 提出了Arrow Language Model，将直觉主义逻辑用于神经网络的下一个token预测，令序列处理对应于逻辑推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前主流的语言模型如Transformer用加法嵌入和注意力机制，但缺乏对序列顺序和逻辑结构的深入建模。希望用逻辑推理框架更自然、更结构化地理解序列预测任务。

Method: 用左嵌套蕴含链（左结合的逻辑蕴含）编码token序列前缀，通过非交换性组合保留顺序。将下一个token的预测形式化为蕴含消解（modus ponens）。用Prolog验证神经模型的基本属性，并与其他模型（如Transformer、状态空间模型）进行对比分析。

Result: 发现从证明论视角出发，可以自然导出现有的多重RNN结构，并且提出了一种可行的低秩神经模型实现。论证了序列处理的不同设计方案（如交换与非交换、单token与多token预测）的本质联系。

Conclusion: 基于直觉主义含义链的神经模型为序列建模和token预测提供了全新思路，在理论和实践上是对现有Transformer等主流模型的重要补充或替代。

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [69] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

TL;DR: 作者提出PaperAudit-Bench基准，包含一个涵盖多种错误类型的数据集和一个带有结构化错误检测的自动化审稿框架，通过实验表明该框架能更严格、更有效地发现和评价论文中的深层错误。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型能够流畅生成学术审稿，但在面对分布于全文、较为隐蔽的实质性问题时，审核往往缺乏足够的批判性和严格性。因此需要更系统、更细致的审稿工具与评价标准。

Method: 作者构建了PaperAudit-Bench基准，包括：1) PaperAudit-Dataset，涵盖需要段内和跨段推理的多类型错误，用来在长上下文场景下做可控测试；2) PaperAudit-Review，一个结合结构化错误检测和基于证据的评审自动化框架。作者还在该基准上进行了多模型和不同检测深度的实验，并训练小型LLM检测器。

Result: 实验发现，不同模型和检测深度对错误的检测能力差异大，凸显了复杂错误在长文本中被发现的难度。与主流自动化审稿基线方法相比，集成显式错误检测的方案表现出更严格且更具区分力的评价能力。数据集还用于训练低算力的小型LLM检测器，效果较好。

Conclusion: 引入结构化错误检测能显著提升自动化学术审稿的批判性和严格度，PaperAudit-Bench为后续开发高效、有力的自动化评审工具提供了数据和方法基础。

Abstract: Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [70] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

TL;DR: 提出PILOT框架，通过内部潜在引导提升紧凑型LLM的规划和多步推理能力，在数学与编程基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 紧凑型大语言模型在多步推理任务中容易因缺乏全局战略规划能力而导致错误积累。虽然大模型的外部指导可以激发其推理潜力，但实时依赖外部模型不切实际。为此，需要让小模型能自主进行有效规划与推理。

Method: 提出PILOT框架，不修改主模型参数，利用轻量级超网络根据输入生成潜在引导向量，作为内部引导，优化小模型的推理路径，实现对大模型战略的内化。

Result: 在数学和编程任务基准上，PILOT显著提高了推理质量，比如在MATH500数据集上准确率提升8.9%，且推理延迟几乎不增加。

Conclusion: PILOT框架能够高效地将大型模型的战略能力迁移到小模型，实现更稳定和准确的多步推理，对提升紧凑LLM的实际应用价值有重要意义。

Abstract: Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [71] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出了一种高效零样本指标LSC（Lowest Span Confidence）来检测大语言模型生成的幻觉，仅需单轮前馈和输出概率，无需复杂资源。该方法实验证明优于现有零样本基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM幻觉检测方法需昂贵的采样或内部状态访问，难以在实际API环境下应用。需要一种高效且资源需求低的新方法。

Method: 提出了LSC指标，通过滑动窗口机制评估语义连贯的span的联合概率，定位局部置信度最低的区域，以捕捉与事实不符的不确定模式。只需单次推理获得输出概率，无需内部权重信息或多次采样。

Result: LSC在多个先进LLM和多种基准上广泛实验，检测性能优于所有现有零样本基线，尤其在资源受限条件下表现突出。

Conclusion: LSC是一种鲁棒、资源友好且高效的LLM幻觉检测工具，适用于常见API场景，提升了幻觉检测的实用性和广泛应用可能性。

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [72] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 提出了一种自适应自知识蒸馏（ASKD）的新方法，通过减少学生对教师依赖，提升模型泛化能力，并将其应用于蒸馏Whisper模型，得到更快且准确率更高的FastWhisper。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法使学生模型可能继承教师模型的不足，导致泛化能力下降。作者希望解决学生对教师过度依赖的问题，从而提升小模型的性能表现。

Method: 提出自适应自知识蒸馏（ASKD），动态调整学生依赖教师的程度，让学生模型增加自我学习能力，并结合自蒸馏机制提升泛化效果。该方法应用于Whisper模型的蒸馏，开发出更轻量且高效的FastWhisper。

Result: FastWhisper在推理速度上提升5倍，识别准确率（字错误率）比教师Whisper低1.07%。

Conclusion: ASKD能有效提升蒸馏后学生模型的泛化能力和推理速度，在语音识别任务中有显著效果。

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [73] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出通过引入观点多样性与置信度交流机制，显著提升多智能体辩论（MAD）对大语言模型性能提升的有效性，优于以往的基础MAD和多数表决方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体辩论（MAD）在提升大语言模型推理与问答能力时，效果往往不如多数表决法且计算开销更大。研究发现原因在于，匿名同质智能体与均匀信念更新机制下，MAD难以提升集体决策正确率。为解决这一问题，作者借鉴了人类集体决策和协商中的有效机制，寻找提升自动化集体推断表现的新途径。

Method: 作者提出两项简单的改进措施：（1）‘多样性初始化’——在辩论开始阶段有意识地选择答案多样性更高的初始候选集，从而增加正确假设最初就被包含的概率；（2）‘置信度调节协议’——让智能体在辩论环节显式表达其置信度，并据此调整对他人观点的采纳程度。理论分析表明，这两种方法分别提升了辩论成功的先验概率和推动系统性向正确假设收敛。

Result: 在六个面向推理的QA数据集上进行实证评测，作者提出的方法（多样性初始化+置信度调节）始终优于基础MAD系统和简单多数表决，展现出持续且显著的性能提升。

Conclusion: 适当引入多样性和置信度表达这两个人类群体决策中的关键因素，能够显著提升基于大语言模型的多智能体辩论系统的有效性。研究进一步证明：简单与原则性的机制调整即可大幅增强自动化集体推理能力，并推动AI领域集体协作方法的发展。

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [74] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: 本文提出了HEART框架，首次实现了人类和大语言模型（LLM）在多轮情感支持对话场景下的直接对比评估，揭示了两者在同理心和一致性等维度的表现差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在语言理解和生成方面取得了很大进步，但关于其在人际沟通与情感支持等领域的表现还缺乏系统性评估方式。作者希望找出LLM在支持性对话方面与人类的差距、优势与局限。

Method: 作者设计了一个名为HEART的评估框架，对同一段多轮对话历史，分别由人类和LLM给出回应，随后由盲评人类评委和LLM作为裁判进行多维度评估。评价指标涵盖人类一致性、同理反应、调适感、共鸣和任务执行。

Result: 前沿的大模型在感知同理心和回应一致性等方面接近或超过了普通人类水平，但在人类独有的适应性重构、紧张命名和语气细微调整方面仍有劣势。人类和LLM评委在约80%的对比中偏好一致，且评价理由侧重相同的HEART维度。

Conclusion: HEART框架提升了对支持性对话的评估精度，把该能力独立于一般推理或流畅性来看，为理解模型与人类在社会性对话中的表现提供了统一的实证基础，并揭示了大模型在情感对话能力随规模上升的趋势与局限。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [75] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: 本论文提出了一种新的自动化评测方法Table-BiEval，用于无人工干预地衡量大语言模型（LLM）将自然语言转换为结构化格式时的准确性，尤其聚焦于复杂表格信息的机器可读性。实验评估了15个主流LLM，并发现结构效率存在显著差异，有时中等规模模型甚至优于更大模型。深层嵌套结构仍是通用难题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为自主代理时，面临着需要高质量自然语言到结构化数据的转换能力，尤其在代码、表格等场景。但现有评估方法依赖人工，成本高，且传统文本指标难以衡量结构性准确性，因此亟需自动化且高效的结构评测方法。

Method: 作者提出Table-BiEval框架，采用自监督方式，基于可判定的中间表示（Intermediate Representations），利用内容语义准确率和归一化树编辑距离，分别拆解结构和内容层面的评价，从而自动评估LLM对结构化信息的处理能力。实验中对15种SOTA LLM模型在层次结构和扁平表格两种拓扑上进行了全面测试。

Result: 实验结果显示，不同模型在结构化能力上有显著差异。某些中等规模（mid-sized）模型在结构效率上优于参数量更大的模型。同时，所有模型在深度递归嵌套的任务上表现不佳，表明该问题是现有技术的普遍瓶颈。

Conclusion: Table-BiEval能够有效、无人工地量化评估LLM结构化输出的准确性。实验结果推动对模型规模与结构处理效率之间关系的理解，并指出深嵌套结构的生成仍需进一步技术突破。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [76] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: 作者提出了OPT-ENGINE基准框架，系统评估大语言模型（LLM）在优化建模上的表现，揭示其纯文本推理能力的极限及当前最大瓶颈。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在优化建模领域表现突出，但其在自动建模和求解复杂现实任务中的边界尚不清晰。因此需要一个可扩展、分层难度的评测框架来细致分析其能力边界和发展方向。

Method: 作者设计了OPT-ENGINE评测框架，包括10类运筹优化任务（5个线性规划、5个混合整数规划），通过控制任务难度，对LLM在不同任务阶段的表现进行系统评估，并比较纯文本推理与接入外部求解器的工具集成推理方式。

Result: 实验表明：1）集成外部求解器的工具推理在任务复杂度提升时表现更鲁棒，纯文本推理能力达到瓶颈；2）约束条件的自动建模是LLM的主要性能瓶颈。

Conclusion: 研究为下一代优化建模专用LLM的开发指出了方向，即需重点提升自动建模（尤其是约束生成）能力，并增强与外部工具的深度集成。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [77] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: 本研究比较了三种大型语言模型（LLMs）——ChatGPT-5、Gemini-3-Pro和Claude-Sonnet-4.5——与人类评审在学术摘要评估任务中的一致性和可靠性，发现LLMs在客观评价标准上与人类有中等一致性，但在主观标准上一致性较弱，显示LLMs适合批量辅助评审但不能完全替代理性判断。


<details>
  <summary>Details</summary>
Motivation: 随着学术交流增多，评审负担日益加重，探索LLMs在辅助科学审稿中的可行性和表现成为热点。本研究动机在于评估三种主流LLMs在不同评价维度上与人类评审的一致性和稳定性，期望为其在科研领域应用提供依据。

Method: 选取160篇会议摘要，由14位人类评审和3种LLMs按统一评分标准独立打分。统计分析包括分数分布、组内相关系数（ICC）评估一致性、Bland-Altman图分析系统性偏差，具体从总体分、内容性及主观性维度细致考察AI与人类的评分一致性。

Result: 三种LLMs之间评分一致性好到非常好（ICC: 0.59-0.87）；ChatGPT和Claude与人工在整体质量和客观标准上达中等一致（ICC 约0.45-0.60），主观标准上一致性较低（ICC 0.23-0.38）。Gemini在一些维度表现一般或无一致性。三种LLMs整体评分结果与人工平均分差异较小。

Conclusion: LLMs能够为学术摘要批量评分提供可靠的客观评价，提升效率。然而，它们在主观判断上仍需依赖人类专家，因此当前最适合与人工评审结合使用，实现优势互补。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [78] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文系统性回顾了337篇关于基于Transformer的语言模型（TLM）句法能力的研究，发现当前研究方法多样但主要聚焦于英文、BERT模型及易测句法现象，模型在形式句法任务上表现好，而在句法-语义接口问题上表现不稳定。作者对今后研究作出改进建议。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现优异，但其句法能力评估的方法和覆盖范围仍有限。作者希望厘清当前TLM句法研究的全貌，找出研究中的局限与未来改进方向。

Method: 作者系统性梳理了337篇文献，统计并分析了1015个模型在不同句法现象和可解释性方法上的实验结果，对比现有数据、模型与评估手段。

Result: 现有研究方法与数据丰富，但过度集中于英语、BERT模型和较易监测的句法现象。TLM对形式句法（如词性、主谓一致）表现良好，但在语义相关或复杂句法结构（如指代、空缺结构）上的表现较为不稳定。

Conclusion: 针对现有不足，作者建议：报告完整的实验数据、加强理论与方法一致性、增加机理性方法、拓宽研究语言及现象范围，以更全面、深刻地评估TLM的句法能力。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [79] [Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey](https://arxiv.org/abs/2601.19927)
*Yuqing Zhao,Ziyao Liu,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文综述了RAG系统中的归因技术，系统梳理了幻觉类型及其应对方法，为未来研究提供借鉴。


<details>
  <summary>Details</summary>
Motivation: RAG系统结合外部检索提升LLM回答质量，但新的幻觉问题随之产生，目前缺乏系统的分类、正统一体的分析和比较，限制了归因技术的普及和优化。

Method: 文献综述方法，系统梳理了RAG系统中的幻觉类型，归纳整理归因技术的统一流程，根据不同幻觉类型分类分析现有技术，并比较其优缺点，总结出实际应用建议。

Result: 提出了RAG系统幻觉类型的详细分类法，梳理了归因防幻觉技术的统一流程，全面评测并分类对比了各类归因方法，对它们的优势和不足进行了总结，并给出了应用指引。

Conclusion: 归因类技术可有效缓解RAG系统幻觉问题，成果为选择和优化技术提供了理论支撑和实践指南，也为今后研究指明了方向。

Abstract: Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.

</details>


### [80] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 本文综述了大型推理模型（LRMs）内部机制的最新研究进展，梳理了训练动态、推理机制和非预期行为三大方向，并提出未来研究的挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在性能上取得突破，但其内部机制尚不明确。为推动模型的可解释性和透明度，作者动机在于系统梳理和总结现有的机制性理解研究成果。

Method: 采用综述方法，对近期关于LRMs内部机制的文献进行归纳整理，主要聚焦于训练动态、推理机制和非预期行为三方面，并对未来挑战提出展望。

Result: 明确总结了当前在训练动态、推理机制、非预期行为三方面的主要发现，提出了现有研究的不足和未被充分探索的领域。

Conclusion: 仅关注模型表面性能已远远不够，对机制性理解的研究应当持续推进。未来研究需重视可解释性、发展更完善的方法论，并构建统一的理论框架，以提升LRMs的透明度和安全性。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [81] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: 提出了一种名为Stingy Context的树形上下文压缩方案，能够极大减少大模型处理编程任务时所需的上下文长度，并依然保证任务表现。


<details>
  <summary>Details</summary>
Motivation: LLM在自动编程任务中受限于上下文窗口，海量代码输入时成本高且难以保留关键信息，需有效压缩代码而不丢失重要内容。

Method: 设计了基于树结构的代码碎片分解（TREEFRAG），将大规模代码库压缩进更小的上下文窗口，还与现有的平铺式（flat）方法做对比。

Result: 在12个主流前沿大模型上，对40个实际问题达到94%到97%的成功率，显著优于现有方法，并降低了处理成本。

Conclusion: Stingy Context有效压缩了代码上下文，提升了大模型在真实环境下的自动编程能力，并克服了长上下文带来的信息丢失问题。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [82] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 本论文提出了面向丹麦文化遗产的新基准数据集 Daisy，通过对丹麦文化经典中选定作品进行知识抽取和问答生成，形成741组高质量的封闭式问答对。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对丹麦文化遗产系统性、多层次的基准数据集，难以支持相关的AI与NLP模型评估与文化知识挖掘，因此作者希望建立一个权威且涵盖丰富历史文化主题的数据集。

Method: 作者基于丹麦文化经典（2006年）中每件文物或作品，对应的维基百科页面，利用大语言模型自动生成与作品相关的随机问题，问题涵盖主要和次要维度，并由人工审核与修正，最终形成包含741组封闭式问答的数据集，内容横跨从公元前1300年到当代的多种文化主题。

Result: 构建出涵盖741组高质量问答对的Daisy数据集，内容涉及考古、诗歌、音乐、设计、建筑等多个丹麦文化重要范畴，并经人工审核确保准确性。

Conclusion: Daisy数据集为丹麦文化遗产的知识挖掘和自动化问答系统提供了权威、系统性的基准，可用于文化遗产相关的人工智能研究，也推动了对丹麦文化的理解与传播。

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [83] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 本文提出了一种混合神经-符号系统，用于SemEval-2026任务中的叙事故事相似度判别，通过神经网络和符号方法的结合，提高了对模糊故事比较的判断能力，取得了81%的开发集准确率。


<details>
  <summary>Details</summary>
Motivation: 在叙事故事相似度任务中，单靠神经网络在面对模糊或接近的案例时常有不确定性。作者希望通过结合符号推理方法，弥补神经网络在一些难点案例上的不足，提高整体准确性。

Method: 系统采用大型语言模型，并行进行多次投票，决策信心不足时进入更多投票轮次。当投票遇到平局时，采用结合五种符号级叙事信号（词汇重叠、语义嵌入、故事语法结构、事件链对齐、叙事张力曲线）的符号集成方法作为最终裁决，实现级联式架构。

Result: 在SemEval-2026相关任务开发集上，该级联架构实现了81%的准确率，相比纯神经模型性能更优，特别在处理有歧义性的故事相似度比较上表现出色。

Conclusion: 混合神经-符号的方法能够有效提升叙事相似度判别任务中的准确性，尤其在面对复杂或模糊案例时，符号方法可以作为补充，从而提升整体系统的鲁棒性和可靠性。

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [84] ["Newspaper Eat" Means "Not Tasty": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews](https://arxiv.org/abs/2601.19932)
*Ruyuan Wan,Changye Li,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 本文提出了CodedLang数据集，系统研究了中文中被编码的语言，发现现有语言模型难以有效检测和理解这一现象。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在应对用户有意编码、表面文本与实际语义不符的“编码语言”时表现不佳，而缺乏真实数据集和清晰分类体系限制了相关研究进展。

Method: 作者构建了CodedLang数据集（含7744条中文Google地图评论及900条带编码语言标注），提出包含发音、拼写、跨语言等七类编码策略的分类体系，并对语言模型进行编码语言检测、分类与评论评分预测基准测试。同时，针对以发音为主的编码方式开展了声韵分析。

Result: 实验发现，即使是表现强劲的语言模型，在识别和理解编码语言方面仍存在较大困难。发音相关编码方式带来了显著挑战。

Conclusion: 编码语言是现实NLP系统中被忽视且重要的难题。本文的工作为相关研究提供了新的数据集、分类体系与任务基准，有助于推动该方向的发展。

Abstract: Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.

</details>


### [85] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 本文提出了一种从自然语言文本到非解析推理（NRR）数学状态空间的映射方法，使得语言中的歧义性在形式化表示中得以保留并量化。作者借助大语言模型作为解释生成器，实现了对歧义表达的高熵状态表示，相比传统方法更能维持语义歧义。


<details>
  <summary>Details</summary>
Motivation: NRR理论提供了保留语义歧义的形式框架，但缺乏具体算法将自然语言映射到NRR中的状态空间。作者旨在解决如何用算法将文本的歧义性形式化表示，并保持其在计算过程中的原始信息。

Method: 1. 设计了从文本到状态映射函数φ，将输入语言转为NRR框架下的叠加态。2. 正式提出了矛盾保留原则，要求歧义表达在状态表示中应有非零熵。3. 利用大语言模型（LLM）生成多种解释，作为状态的提取协议，并据此建立状态空间。

Result: 在68条涉及词汇、结构与语用歧义的测试句上，所提方法对歧义输入的平均香农熵H(S)为1.087比特，而基线单一解释法则为0。结果表明，本方法能够有效保留和表征语言歧义性。

Conclusion: 文本到NRR状态空间的映射方法填补了NRR理论与实际文本输入之间的算法空白，使得下游大模型推理过程中能延迟歧义消解，有助于更自然地模拟和推理人类语言中的不确定性。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [86] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在相同提示和固定解码参数下重复生成时仍会出现输出差异的问题。通过重复试验，量化LLM的输出漂移基线，并比较不同模型、提示类别、部署方式下的行为漂移情况。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，即使在温度和解码参数固定的情况下，LLM对相同提示也可能产生不同输出。这种不确定性影响了LLM的一致性和可重复性，因此需要系统地量化和分析其行为漂移现象，以为后续提升模型稳定性提供基线参考。

Method: 作者选取gpt-4o-mini和llama3.1-8b两个公开可用模型，对五类提示进行重复输入实验，包括完全相同的提示、输入有扰动和重用等模式，并在温度0.0与0.7下测试。通过独特输出比例、词汇相似度和词数等统计量度漂移程度，从而对不同模型与提示方式做直观对比。

Result: 实验发现，即便温度设为0.0，模型输出仍然存在不可忽略的非确定性。不同模型大小、部署方式和提示类型会展现出各自特有的变异模式，表现出行为漂移的普遍性和多样性。

Conclusion: 该研究为LLM行为漂移问题建立了系统的实证基线，有助于评估和对比未来漂移缓解和控制方法，并指出当前词汇度量存在局限，提出了采用语义度量方法的必要性，对LLM一致性提升具有参考价值。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [87] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: 本文提出了Mem2ActBench，一个专门评估大语言模型（LLM）代理在实际工具使用场景中主动运用长期记忆能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试代理在被动检索孤立事实方面的能力，未能考查其在复杂任务中主动利用长期记忆的实际能力，这在实际应用中更为关键。

Method: 该工作构建了Mem2ActBench数据集，通过自动化管道，整合多个异构来源的数据（如ToolACE、BFCL、Oasst1），并采用一致性建模解决冲突，生成了包含2029个会话的数据集。同时通过逆生成方法产生400个依赖记忆的工具使用任务，并通过人工评估验证其对记忆依赖性。

Result: 人工评估显示91.3%的任务确实高度依赖长期记忆。对七种现有记忆框架的实验发现，这些系统在根据长期记忆进行参数确定和工具选择时表现不足。

Conclusion: 现有记忆系统尚无法有效地将长期记忆主动应用于复杂工具任务中，未来亟需开发更强大的记忆利用及评测方法以提升大模型实际任务执行能力。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [88] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 本文构建了德语医学场景下含方言的对话数据集，并对29种ASR模型进行了评测，揭示了模型在词错误率等方面的显著差异。


<details>
  <summary>Details</summary>
Motivation: 虽然ASR（自动语音识别）能为医疗人员减负，但德语医学语境、尤其是方言覆盖下的ASR模型评测却很匮乏。

Method: 作者构建了包含医生与病人对话、包含方言的德语数据集，并评测了Whisper、Voxtral、Wav2Vec2等开源模型及商业API（AssemblyAI、Deepgram），采用WER、CER、BLEU等评价指标，部分涉及语义分析。

Result: 最佳模型在部分场景下词错误率（WER）低于3%，但部分模型在医学词汇、方言等方面错误率明显偏高，模型表现差异显著。

Conclusion: 部分ASR模型已在德语医学对话中表现优异，但对于医学专用词汇和方言处理仍存挑战，需进一步提升模型在专业和方言场景下的表现。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [89] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 本论文提出了一种基于大规模数据集和新颖训练策略的AI生成文本检测方法，并显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，AI生成文本越来越逼真，给教育、出版和数字安全带来了身份验证难题，因此亟需有效的检测技术。

Method: 作者构建了包含10亿人类文本和19亿AI文本的大型语料库，涵盖多领域，并通过多种LLM生成AI文本。在此基础上，开发和评估了多种检测模型，并提出了“针对单个LLM”及“针对LLM家族”的新型微调训练范式。

Result: 在覆盖21款大语言模型、含1亿token的基准测试上，最佳微调检测模型在token级别上实现了高达99.6%的准确率，远超现有开源基线。

Conclusion: 通过精细化大规模数据集和新训练方法，极大提升了AI文本检测能力，为技术和伦理领域提供了有力工具。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [90] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 该论文发现大语言模型在多语言任务特别是语言控制方面表现不足，提出了两种关键失效模式，并通过分层分析和选择性微调最终层，显著提升了多语言一致性，且仅需很少的参数更新与计算资源。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型经过多语言预训练，但在非英语任务中的语言控制（即用指定语言回答）的能力仍然薄弱，导致实际应用受限。因此，研究者希望系统性诊断并提升大模型的多语言控制能力。

Method: 作者设计了包含MMLU、MGSM、XQuAD基准在内的四种评测场景，识别并剖析了两大失效模式（多语言迁移瓶颈和语言一致性瓶颈）。为深入理解内部机制，论文将logit lens分析扩展到逐层追踪语言概率，并用跨语义相似性分析隐藏状态。最后提出只对负责编码语言控制的模型后几层做选择性微调。

Result: 在Qwen-3-32B和Bloom-7.1B两大模型上，仅微调3-5%参数即可让六种语言下的语言一致性超过98%，效果与全模型微调几乎相同，但计算资源消耗显著减少。

Conclusion: 首次证明了多语言任务中，利用层局部化的语言控制机制，仅微调末层即可高效提升多语言一致性，为多语言适应性提供了极具性价比的解决方案。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [91] [Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method](https://arxiv.org/abs/2601.20026)
*Pragatheeswaran Vipulanandan,Kamal Premaratne,Dilip Sarkar*

Main category: cs.CL

TL;DR: 本论文提出了一种基于量子物理和张量网络的新型不确定性评估方法，用于识别大语言模型（LLMs）的幻觉输出，并验证了其在多种模型和数据集上的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型尽管生成能力强，但易产生“幻觉”（即流畅但不可靠的输出），且在同样输入下结果可随机变化，缺乏有效和可解释的幻觉检测机制。

Method: 作者基于量子张量网络，提出了一种新颖的不确定性量化框架，通过分析不同token序列概率的不确定性（包含语义等价聚类），以检测幻觉输出，并通过最大熵策略筛选高置信度、语义连贯的输出。此外，框架能突出高熵区域，为需要人工介入的场景提供指导。方法适用于不同生成长度和量化级别，解决以往研究忽视的鲁棒性问题。

Result: 在TriviaQA、NQ、SVAMP、SQuAD等数据集和包括Mistral、Falcon、LLaMA系列8种架构的116组实验中，该方法在AUROC和AURAC指标上相较主流方案有持续提升，表现在资源受限下依然稳健。

Conclusion: 本文方法为LLMs幻觉检测提供了可解释且有效的技术手段，在多模型、多任务条件下均表现先进，为大语言模型的安全实用化提供强有力支持。

Abstract: Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.

</details>


### [92] [TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference](https://arxiv.org/abs/2601.20032)
*Nishanth Sridhar Nakshatri,Eylon Caplan,Rajkumar Pujari,Dan Goldwasser*

Main category: cs.CL

TL;DR: 本文提出了一种新的结构化分析框架TAIGR，更有效地验证健康领域网红在视频中的推荐和论证，超越以往基于单一事实核查方法。


<details>
  <summary>Details</summary>
Motivation: 传统事实核查方法以查找明确的真假声明为主，但健康网红的内容往往以对话性叙述和修辞策略呈现，不易被直接核查。为更有效地验证其内容，亟需一种能捕捉潜在主张及论证结构的新方法。

Method: TAIGR包括三步：1）识别网红传播内容的核心推荐（takeaway）；2）构建表现网红如何论证该结论的论证图；3）通过基于因子图的概率推理校验推荐的可靠性。

Result: 在健康领域网红视频转录文本上，TAIGR优于简单的事实核查，验证结果表明，建模语用和论证结构比仅处理单条虚实声明更准确。

Conclusion: 验证内容是否准确需要关注叙述中的主张及其论证关系，TAIGR为理解和分析复杂健康信息提供了更有效的技术思路。

Abstract: Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.

</details>


### [93] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）与SMT求解器的神经符号混合框架VERGE，通过验证驱动的迭代优化提升LLM在高风险领域推理任务中的逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在句法流畅性上表现突出，但在高风险领域确保输出的逻辑正确性仍是重大难题。传统方法难以提供形式化的验证与可解释的错误反馈，亟需融合AI推理与自动化定理证明的创新方法。

Method: 首先将LLM输出分解为原子性声明，并自动形式化为一阶逻辑，然后用定理证明工具验证逻辑一致性。方法创新包括：（1）各模型输出通过语义等价检查达成一致，减少表面形式偏差；（2）对不同类型声明采用语义路由，逻辑声明用符号求解器验证，常识声明用LLM集成判定；（3）通过最小修正子集（MCS）实现精确的逻辑错误定位，并将非通过式反馈转化为具体可操作建议。各声明按逻辑状态分类，多信号综合评分并加以方差惩罚，在迭代反馈指导下优化输出。

Result: 采用GPT-OSS-120B模型，VERGE在一套推理基准上相比单轮方法表现提升18.7%。

Conclusion: 该神经符号框架为AI推理任务提供了可形式化验证与多重共识保障，提升了AI在高风险任务中的可靠性与可解释性。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [94] [Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects](https://arxiv.org/abs/2601.20102)
*Amirhossein Haji Mohammad Rezaei,Zahra Shakeri*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估医学语言模型在面对与文化相关但与诊断无关的信息时，是否会出现诊断偏差的新基准测试，并在多个主流模型中发现了显著的准确性下降。


<details>
  <summary>Details</summary>
Motivation: 当前医学大模型有可能在处理文化相关信息时引入不必要的诊断偏差，进而影响医疗公平和可持续性。因此，迫切需要方法来量化并减少文化因素对诊断结果的不良影响。

Method: 作者将150道MedQA医学测试题扩展为1650个变体，通过插入文化标识、情境线索或两者结合，涵盖三个人群，并设置长度匹配的中性控制。所有变体均由临床医生验证正确答案不变，然后测试多款医学大模型在不同提示方式下的表现，并用人类验证的评判标准分析推理失误。

Result: 所有模型成绩均被文化线索显著影响，尤以标识与情境线索同时出现时表现最差（最多下降3-7个百分点）。与文化相关的解释中，超过一半最终导致错误结论，揭示文化参照推理与诊断失败之间的联系。

Conclusion: 医学语言模型对文化无关信息的敏感性可能导致临床诊断错误。本文提出的测试基准和公开数据可为将来设计更公平、稳健的医疗AI提供工具。

Abstract: Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.

</details>


### [95] [FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language](https://arxiv.org/abs/2601.20105)
*Faezeh Hosseini,Mohammadali Yousefzadeh,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本论文提出了 figurative hallucination（虚构型修辞幻觉）的概念，并针对波斯语推出首个综合性基准集 FFEHallu，用以评价大语言模型在处理固定修辞表达（如习语和谚语）中的表现，同时发现主流多语种模型在辨别真假修辞表达和跨语种翻译中存在显著短板。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在处理固定修辞表达（如习语和谚语）时面临很大挑战，这类表达具有文化依赖性、非组合性及表达固定性，易出现类似但并不存在的虚构修辞表达。作者希望揭示甚至量化此类“修辞幻觉”问题，并为低资源语言（如波斯语）相关研究提供工具和基线。

Method: 作者新提出了 FFEHallu 基准数据集，涵盖 600 个精心挑选的样本，通过三类任务来评估 LLM 的修辞幻觉表现：1）根据含义生成 FFE，2）在四种受控构造类别下检测虚构 FFE，3）将 FFE 从英语翻译到波斯语。随后，对六种当前最先进的多语种 LLM 进行了全面评测。

Result: 实验结果表明，尽管如 GPT4.1 这样的模型在拒绝虚构 FFE 及检索真实表达方面表现较好，大多数模型在区别真实修辞和高质量仿造表达时存在困难，且在跨语种翻译中经常出现修辞幻觉。

Conclusion: 当前多语种大模型在处理固定修辞表达方面仍有明显短板，尤其在文化根基和修辞能力上需要改进，迫切需要开发针对性的基准和方法来评估并减少修辞幻觉现象。

Abstract: Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.

</details>


### [96] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 本论文提出了一种新的训练范式——带可验证奖励的强化学习（RLVR），通过鼓励模型在不确定时选择“我不知道”以减少幻觉内容。实验证明适度的弃权奖励能有效提高大语言模型在事实任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生错误或无法验证的内容（幻觉），导致在事实领域应用时可信度下降。作者希望通过奖励模型在不确定时选择弃权，增强其理性与可信度。

Method: 采用带有三元奖励结构（-1、弃权奖励r_abs、1）的强化学习框架，对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct两款模型在MedMCQA和Hendrycks Math等基准任务上进行微调和评估。同时，结合监督学习进行弃权预训练以提升探索能力。

Result: 适度的弃权奖励（r_abs约为-0.25到0.3）能在不大幅降低准确率的前提下，显著减少错误回答。大模型对弃权激励更具鲁棒性。对于开放式问答，弃权效果受限于探索不足，可通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是一种有效且实用的语言模型幻觉缓解手段。论文提出的训练方法具备灵活性和可行性，并已开源训练代码。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [97] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了BengaliSent140，这是一个规模大、领域多样的孟加拉语二分类情感数据集，专为提升深度学习模型的表现和泛化能力而设计。


<details>
  <summary>Details</summary>
Motivation: 目前孟加拉语情感分析研究受到高质量大规模标注数据集的严重制约，现有数据集往往样本量小或局限于单一领域，难以满足深度学习模型的训练需求。

Method: 该研究整合了7个现有的孟加拉语文本数据集，通过统一注释标准，将它们归一为二分类情感标签（非仇恨与仇恨）。最终形成包含约14万条独特文本的数据集，数据来源跨越多个平台和领域，类别分布较为平衡。

Result: 数据集BengaliSent140共含139,792个样本，其中仇恨与非仇恨样本数量相当，代表性强。文中还给出了一组基线实验结果，验证了数据集的实用性。

Conclusion: BengaliSent140数据集扩展了孟加拉语情感数据的规模与多样性，有助于推动现代深度学习方法在孟加拉语情感分析和仇恨言论识别领域的发展。该数据集已公开，便于社区进一步研究和应用。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [98] [Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR](https://arxiv.org/abs/2601.20142)
*Zilai Wang,Natarajan Balaji Shankar,Kaiyuan Zhang,Zihan Wang,Abeer Alwan*

Main category: cs.CL

TL;DR: 该论文通过结合预训练与微调后SSL模型embedding的变化（delta embedding），提出了提升儿童语音识别（ASR）性能的新方法，并在MyST儿童语料库上实现了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 虽然自监督学习（SSL）模型在语音相关任务上表现优异，但因缺乏儿童语音数据和预训练域不匹配，儿童ASR依旧具挑战性。传统微调无法充分利用原预训练模型信息，研究动机在于探索如何最大化SSL模型在儿童ASR场景下的表现。

Method: 作者提出利用微调前后SSL模型embedding的差异（称为delta embedding），并将其与其它SSL模型的微调特征进行多种方式的特征融合，提升ASR性能。在MyST儿童语料库上，基于不同SSL模型（如WavLM、HuBERT、W2V2）进行了系统性的融合与对比实验。

Result: 融合delta embedding与WavLM模型特征，HuBERT获得了高达10%的WER相对下降，W2V2下降了4.4%。将WavLM与delta W2V2特征融合后，取得9.64的WER，在MyST语料库SSL模型上达到SOTA表现。

Conclusion: delta embedding在特征融合中能有效提升儿童ASR性能，特征融合为未来儿童语音识别系统提供了有前景的发展方向。

Abstract: Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.

</details>


### [99] [Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents](https://arxiv.org/abs/2601.20144)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Jiri Gesi,Xianfeng Tang,Chen Luo,Yisi Sang,Hanqing Lu,Manling Li,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种可验证的数据生成流程Trajectory2Task，用于模拟和评估工具调用智能体在现实复杂用户场景下的表现。通过针对三种常见但挑战性的用户意图（模糊、变化及不可行）构建任务，并在此基础上评测和微调语言模型，显示微调后模型在泛化能力和实际表现上有明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于工具调用智能体的研究只关注理想化、固定且规范的场景，而现实中的用户请求往往不明确、动态变化或受限于政策，但相关训练和评测数据严重不足。因此，亟需构建能反映现实复杂用户互动的研究和评测体系。

Method: 作者提出了Trajectory2Task流程：首先通过多轮探索生成有效的工具调用轨迹，然后将轨迹转化为带有可控意图变化的用户任务，形成可验证的、支持闭环训练与评测的数据集。以这些任务为基准，对七个先进大语言模型的表现进行评测。此外，将成功任务轨迹用于微调轻量化LLM，验证其提升效果。

Result: 评测结果显示，当前最先进的多款大语言模型在复杂用户场景任务中频繁失败。而经过成功任务微调的轻量化LLM，在三种现实意图条件下均取得了性能提升，并在未见过的工具使用领域表现出更好的泛化能力。

Conclusion: 该工作展示了现实复杂用户场景下工具调用智能体面临的挑战，Trajectory2Task流程填补了数据和评测的空白。微调后模型不仅在指定任务上表现更佳，也拓展了工具调用的泛化能力，为后续实际应用和研究提供了有力支撑。

Abstract: Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.

</details>


### [100] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Me-Agent的可学习、可记忆的个性化移动智能体，通过两级用户习惯学习提升个性化能力，并在新基准User FingerTip上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的移动代理在执行用户指令时，往往忽略了用户的个性化需求，难以解释含糊指令，也无法根据用户历史互动调整行为，因此亟需更智能、个性化的解决方案。

Method: Me-Agent采用了两级用户习惯学习：一是在提示（prompt）层面通过用户偏好学习和个人奖励模型增强个性化，二是在记忆层面设计了层次化偏好记忆，分别存储长期记忆和应用特定记忆。此外，作者还提出了新的评测基准User FingerTip，用于测试模型对日常含糊指令的理解和个性化能力。

Result: 在User FingerTip和其他通用基准上的大量实验表明，Me-Agent在个性化能力方面达到了最新水平，同时保持了优秀的指令执行性能。

Conclusion: Me-Agent显著提升了移动智能体的个性化服务能力，能够处理含糊与个性化指令，推动了智能体与用户真实需求的进一步贴合。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [101] [Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling](https://arxiv.org/abs/2601.20185)
*Husein Zolkepli*

Main category: cs.CL

TL;DR: 本文提出对X-Codec-2.0的简易修改，在降低特征速率的同时提升了音频采样率，显著提升神经音频压缩和多语言语音建模的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 现有X-Codec-2.0虽已表现优异，但受限于50 Hz特征速率和16 kHz采样率，影响了时域效率和音质表现。

Method: 通过引入额外的池化层并增大解码器跳变步长，将潜在特征速率从50 Hz降到25 Hz，输出采样率从16 kHz升至24 kHz，同时保持核心结构不变。

Result: 在多语言Common Voice 17测试集上，新方案的主观音质MOS（基于UTMOSv2）比原基线提升0.29，并成为25 Hz特征速率下最佳表现的编解码器。

Conclusion: 简单结构调整后，效率和感知质量均提升，为低特征速率高音质神经音频编解码器提供了新方案。

Abstract: X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.

</details>


### [102] [Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems](https://arxiv.org/abs/2601.20230)
*Haoyuan Yu,Yuxuan Chen,Minjie Cai*

Main category: cs.CL

TL;DR: 提出了一种通过将复杂对话分解为最小对话单元的新框架，实现了全双工语音交互系统，并在相关数据集和竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 实现更自然的人机全双工语音交互，解决当前全双工对话处理复杂、响应不及时等问题。

Method: 将复杂对话分解为最小会话单元，每个单元独立处理，并预测何时转换到下一个单元。具体实现为基于多模态大模型的半级联全双工对话系统，并集成VAD（语音活动检测）和TTS（语音合成）等辅助模块。该系统为免训练、即插即用。

Result: 在HumDial数据集上的实验表明该框架有效，在Human-like Spoken Dialogue Systems Challenge（全双工交互赛道）测试集排名第二。

Conclusion: 所提系统结构简洁、无需额外训练，且具备良好的全双工语音交互能力，实际应用潜力强。

Abstract: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.

</details>


### [103] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 本文提出了一种利用专家指南和Bloom认知分类法自动生成开放式问答基准的方法，有效解决实践型领域缺乏评测数据的问题，并揭示LLM在人类推理中的非直观表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的问答评测主要依赖于已有的人类考试数据，这些数据在需要专业判断和程序性知识的实践型领域常常缺失。为提升此类领域的模型评测能力，急需自动化、可扩展的高质量基准生成方法。

Method: 作者提出根据专家编写的实践指南，结合Bloom's Taxonomy，将隐含的违规场景转化为自动评分的多选题和多轮对话，覆盖不同认知层次。这一框架能实现可复现、可扩展的自动化基准生成。

Result: 在教学、营养和护理三个实际领域应用该框架后，发现LLM在高阶推理（如分析）上的表现有时优于低阶记忆题；显示出模型推理能力与人类之间的差异及一些非直观现象。

Conclusion: 该方法能够大规模地生成基于心理测量学的评测集合，有助于揭示LLM在现实情境中推理的优缺点，为后续模型改进与实际应用评估提供支持。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [104] [SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility](https://arxiv.org/abs/2601.20256)
*Xuanyu Su,Diana Inkpen,Nathalie Japkowicz*

Main category: cs.CL

TL;DR: 本文提出了一套新的基准（SoftHateBench），专门用于评测和生成“软仇恨”言论，并发现主流检测系统对这类隐蔽仇恨话语的识别能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体仇恨言论治理主要针对明显的有害言辞（如侮辱、威胁），而对更隐晦、带推理性质的“软仇恨”识别能力较弱，且缺乏有效的评测基准。

Method: 构建SoftHateBench基准，结合Argumentum Model of Topics（AMT）和Relevance Theory（RT），将明确的仇恨言论转化为逻辑自洽但貌似中立的软仇恨实例，涵盖7大社会文化领域和28个目标群体，共生成4745条软仇恨言论。

Result: 通过对主流编码器、通用大模型和安全模型的评测，发现所有系统在软仇恨检测上表现明显下降，许多系统无法识别带有推理和隐含立场的仇恨言论。

Conclusion: 现有的仇恨言论检测系统在应对“软仇恨”话语方面存在显著短板，SoftHateBench为今后改善和提升检测能力提供了重要工具和新标准。

Abstract: Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}

</details>


### [105] [RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis](https://arxiv.org/abs/2601.20275)
*Elina Sigdel,Anastasia Panfilova*

Main category: cs.CL

TL;DR: 本文提出了一种针对俄语文本的心理语言特征分析方法，改编自LIWC工具，并结合俄语的语言和文化特点。


<details>
  <summary>Details</summary>
Motivation: 目前心理语言学文本分析工具如LIWC多基于英语，直接翻译常常无法准确反映俄语的独特语法和文化内涵。因此，需要专门针对俄语开发心理语言特征分析方法和词典。

Method: 作者综合了俄语的句法、形态、词汇、统计特征，并结合预训练语言模型的预测结果，创建了包含96类特征的分析体系。词典构建没有采用直接翻译，而是根据多种俄语词典、语料库及语义辞典内容专门制定，将词元映射到42个心理语言学类别，并开发了RusLICA在线服务。

Result: 作者成功开发了面向俄语的心理语言分析工具和词典体系，将其集成到RusLICA网站中，实现了针对俄语文本的细致分析。

Conclusion: 该方法能够更好地适应俄语的语言和文化特点，有效提升俄语文本心理语言学分析的准确性和适用性，为非英语文本分析领域提供了有力工具。

Abstract: Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.

</details>


### [106] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 本文提出了一种新的针对长上下文大模型的评测基准EMB-S，用以严格测试模型在大规模语境中精准访问和利用证据的能力，发现当前模型的主要瓶颈在于语义区分能力而非上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有主流的NIAH（大海捞针）测试主要考察模型在比较简单环境下定位唯一答案的能力，实际对模型在复杂环境下的语义干扰与高难度证据检索弱化测试，不足以反映实际应用的挑战。

Method: 作者构建了EMB-S评测集，基于3.26亿token的MemoryBank，并设计有高难度近似干扰（hard negatives）和人工、LLM双重验证的标记证据，支持长上下文原生模型与检索式RAG模型在变量上下文规模下的公平对比，同时提出区分文档定位与QA能力的独立诊断协议。

Result: 实验发现，从64K小型隔离域到3.26亿共享大语料环境下，模型在普通NIAH任务中表现优异，但在面对大量语义干扰时，证据访问准确性大幅下降。也即上下文长度提升后，语义区分能力成为限制因素。

Conclusion: 长上下文大模型在扩展上下文窗口后，主要挑战是对复杂、多干扰环境下证据的语义甄别，单纯提升记忆长度无法解决实用场景中的主要瓶颈，对模型的语义区分和证据筛选能力需优先关注。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [107] [MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting](https://arxiv.org/abs/2601.20300)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: 提出MiLorE-SSL，一种结合LoRA与软MoE机制的轻量级多语自监督学习框架，可高效持续学习新语言，缓解遗忘，实验表现优异且参数量低。


<details>
  <summary>Details</summary>
Motivation: 当前多语种自监督学习模型在遇到新语言时受限于预训练语种，重新训练代价高昂，顺序训练又容易遗忘，迫切需要高效、低成本并能持续学习多语种的方法。

Method: 提出MiLorE-SSL框架，将低秩适配的LoRA模块与软混合专家机制结合，实现灵活的跨语种参数共享与高效持续训练；采用有限的历史数据回放进一步缓解遗忘，无需大规模历史语料。

Result: 实验表明，MiLorE-SSL在ML-SUPERB基准上的新语种表现优异，对现有语种能力也有提升，且拥有极低的（仅2.14%）可训练参数量。

Conclusion: MiLorE-SSL有效解决了多语种自监督学习中持续引入新语言的遗忘与效率问题，为低开销扩展多语种模型提供了实用方案。

Abstract: Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.

</details>


### [108] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种自适应过程优化方法（SAPO），用于小语言模型的自我进化，通过高效地缩小reasoner-verifier差距，提升模型在数学与代码任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化方法忽略了细粒度推理步骤的影响，导致推理器和验证器之间存在差距；而常用的蒙特卡洛过程监督又存在计算效率低下的问题。受到错误相关负波（ERN）现象启发，作者希望让推理器能及时感知和修正错误，提升整体推理能力。

Method: 提出Self-Adaptive Process Optimization（SAPO）方法，能自适应、高效地引入过程监督信号，通过主动最小化reasoner-verifier gap，而不是依赖低效的MC方法，从而改善自我进化过程。

Result: 大量实验表明，SAPO方法在数学和代码这两类具有挑战性的任务上优于大多数现有自我进化方法。同时，作者还构建了两个用于评测过程奖励模型效果的新基准。

Conclusion: SAPO突破了MC监督的效率瓶颈，通过精细化过程监督信号缩小reasoner-verifier差距，不仅提升了小语言模型的进化能力，还丰富了相关评测工具，有望促进该领域的发展。

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [109] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: 论文提出重用 KV cache 作为LLM推理阶段下游任务的高效表示方式，无需大幅计算或存储开销，效果优异，为表示重用提供了新方向。


<details>
  <summary>Details</summary>
Motivation: KV缓存通常仅被用于加速自回归解码，其实包含有丰富上下文信息。当前主流方法需重复计算或保存完整隐藏状态，资源消耗较大，如何以更低的代价利用这些已有缓存，是一个值得探索的问题。

Method: 作者提出用KV cache代替传统全隐藏层状态表示，作为下游任务输入，在两个关键应用场景：Chain-of-Embedding和快慢思维切换下验证其有效性，并在不同LLM模型（如Llama-3.1-8B-Instruct、Qwen2-7B-Instruct、DeepSeek等）上对比实验表现。

Result: （1）链式嵌入任务中，KV cache表示能达到与甚至优于传统嵌入方式的性能；（2）快慢思维切换场景中，可自适应调整推理速度，实现高达5.7倍推理token数减少且损失精度极小。

Conclusion: KV cache可作为“免费”的高效表示，应用于快速采样、推理等场景，没有精度显著损失，极大提升推理效率，并为LLM推理阶段表示重用开辟新方向。

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [110] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种名为CE-RM-4B的新型奖励模型，在使用少量高质量数据的情况下，在多种评价基准和实际下游任务中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 当前开放式自然语言生成的自动评价困难，很难用规则或传统指标解决。虽然基于大模型评审（LLM-as-a-Judge）方法有潜力，但它们在实际强化学习应用中效果不佳，原因在于主要采用对偶排序和评价标准优化不足。

Method: 提出了CE-RM-4B奖励模型，采用两阶段rollout训练方法和QBC统一评分标准，仅用约5700条高质量偏好数据进行训练。

Result: CE-RM-4B在多个奖励模型基准任务（尤其在Best-of-N场景下）表现优异，并且在实际强化学习任务中带来更有效的改进。

Conclusion: 通过新的点式奖励模型与优化训练流程，能够更好地自动化评价自然语言生成并提升强化学习效率，具有更好的实际应用前景。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [111] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过轨迹锚定的锦标赛框架（PsychePass）对大语言模型（LLM）在心理健康领域的治疗能力进行更加稳定和可信的评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估大语言模型治疗能力的方法存在不稳定（如目标偏离、评分波动）的问题，难以真实反映模型在心理咨询等复杂、长时序任务中的表现。

Method: 提出PsychePass评估框架：首先，通过让模拟的来访者精确控制咨询流程，将交互轨迹锚定下来；其次，在判断层面采用动态配对的瑞士锦标赛制，利用Elo分数得到稳健的能力排名。此外，锦标赛过程还可作为强化学习的奖励信号用于模型能力提升。

Result: 通过大量实验，验证了PsychePass能有效评估LLM的治疗能力，其评价结果与人类专家判断高度一致。

Conclusion: PsychePass框架能更加稳定、可信和高效地评估及提升大语言模型在心理健康领域中的治疗能力，优于现有的静态评分方法。

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [112] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: 本文提出了MobileBench-OL，这是一个面向移动GUI智能体的在线评测基准，涵盖了来自80个中文应用的1080个任务，能够多维度评估智能体的执行、推理和噪声鲁棒性。实验结果显示当前领先模型距离真实应用场景尚有差距。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体在线基准主要关注任务执行能力，较少评估推理、探索能力且未充分模拟现实世界中的环境扰动，导致基准测试与实际应用之间存在差距。

Method: 作者设计了MobileBench-OL，包含来自80个中文App的1080个真实任务，并划分为5个子集，分别考察执行、复杂推理和抗噪声能力，还提出了自动评测框架和复位机制以保证评测的稳定性和可重复性。

Result: 对12个领先GUI智能体在MobileBench-OL上的评测表明其综合能力亟需提升以适应真实环境任务。人工评估进一步验证了该基准在实际场景中对模型性能的可靠衡量能力。

Conclusion: MobileBench-OL更贴近实际应用环境，能全面评价GUI智能体的多维能力，为推动智能体真实场景落地提供了有力支撑，数据和代码将在论文接受后开放。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [113] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 本文提出了一种联合搜索生成顺序和token值的新解码方法(Order-Token Search)，提升了diffusion语言模型（DLMs）在多个推理和编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 目前DLMs的解码一般只沿一个生成轨迹进行，虽然DLMs本身支持无序生成，但现有方法未能有效探索多个生成路径，从而限制了DLMs的潜力。

Method: 作者提出Order-Token Search方法，通过联合搜索生成顺序和token值，并利用似然估计器为去噪动作打分，实现了稳定的剪枝和高效的多样轨迹探索。方法可应用于数学推理和编程等任务。

Result: 实验在GSM8K、MATH500、Countdown、HumanEval四个数据集上，Order-Token Search显著优于基线（分别提升3.1%、3.8%、7.9%、6.8%），并能媲美甚至优于已微调的强基线方法。

Conclusion: 联合顺序与token值的搜索是提升DLMs解码效果的关键，Order-Token Search为DLMs的研究和应用开辟了新方向。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [114] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 论文提出针对大语言模型（LLMs）使用外部工具时的能力边界评估新方法，不仅关注最终准确率，还引入认知负载理论对模型瓶颈进行诊断。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试仅报告最终准确率，无法揭示LLM在复杂任务中受限的真正“认知瓶颈”。如何更细致地刻画和量化模型能力的边界，是本研究的主要动机。

Method: 引入以认知负载理论为基础的分析框架，将任务复杂度分解为内在负载（解决路径的结构复杂性，通过工具交互图形式化）与外在负载（任务表述不明确导致的难度）。基于该框架，构建了ToolLoad-Bench基准，具备认知负载可参数化调节的特性，以支持可控实验。

Result: 实验证明，工具使用任务中随着认知负载提升，模型性能存在明显断崖式下跌；该框架对模型能力边界的预测高度契合真实实验结果。

Conclusion: 该方法为理解与映射LLM能力极限提供了定量、可诊断的工具基础，有助于未来更高效智能体系统的设计与开发。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [115] [SpeechMapper: Speech-to-text Embedding Projector for LLMs](https://arxiv.org/abs/2601.20417)
*Biswesh Mohapatra,Marcely Zanon Boito,Ioan Calapodescu*

Main category: cs.CL

TL;DR: SpeechMapper提出了一种高效、鲁棒的语音到LLM融合方法，实现低计算成本下更强泛化能力，减少了传统训练过程中的过拟合和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前主流的语音大模型（Speech LLM）通过投影层将语音基础模型连接到LLM，并在语音指令数据上联合训练。此策略计算开销大，且容易对任务及提示词过拟合，因此需要一种更高效并具备泛化能力的模型训练与融合方案。

Method: 提出SpeechMapper：先在无LLM条件下对模型预训练，降低硬件和数据消耗。随后只需简短的1K步指令微调，即可高效融合到目标LLM。实验涵盖了通用与任务特定条件，并采用了ASR适应等策略，无需在具体任务上训练即可支持多种任务。

Result: 在通用（task-agnostic）场景下，SpeechMapper在IWSLT25等数据集上几乎不逊于最优的语音LLM，即便未见过特定任务。任务特定的微调中，SpeechMapper则以更少的数据和计算超越了已有同类模型。

Conclusion: SpeechMapper为将语音基础模型与LLM高效融合提供了实用且易扩展的方法，能显著节省资源、减轻过拟合，并带来优秀的泛化和迁移能力，无需大规模任务特定微调。

Abstract: Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.

</details>


### [116] [Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020](https://arxiv.org/abs/2601.20424)
*Anna Ristilä,Otto Tarkka,Veronika Laippala,Kimmo Elo*

Main category: cs.CL

TL;DR: 该论文分析了芬兰议会发言中不同话题的情感表达，发现不同话题的情感色彩各异，且整体积极性有所上升。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将议会话语视为整体，忽视了不同话题在情感表达上的差异，关于各议题所关联的典型情感尚未有深入研究。

Method: 作者使用情感分析模型，对2000至2020年芬兰议会发言按议题进行情感分析，从共时和历时视角探讨情感和话题的关联。

Result: 结果显示议会发言中整体积极性增加，并揭示了具体议题下的情感表达特征。

Conclusion: 论文证明议会发言的情感表达具有话题相关性，并为议会辩论的话题化情感特征提供了新见解。

Abstract: Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.

</details>


### [117] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: 该论文提出PEARL，一种新颖的两阶段框架，提升大语言模型在复杂多轮工具调用中的计划与执行能力，在多个基准任务上取得了新的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂多步工具调用任务中存在计划薄弱、工具幻觉、参数生成错误、交互鲁棒性差等问题，限制了其在现实场景的实用性。

Method: PEARL框架包括两个阶段：离线阶段，Agent探索各类工具，学习有效的使用模式和失败条件；在线阶段，通过设计专用奖励函数，并用群体相对策略优化（GRPO）训练Planner，以获得更高质量的计划能力。

Result: 在ToolHop和T-Eval基准测试上，PEARL大幅优于现有方法，在ToolHop以56.5%的成功率刷新了SOTA，同时保持较低的工具调用错误率。

Conclusion: PEARL有效解决了LLM多工具调用中的计划与执行难题，为提升LLM智能体的健壮性和可靠性迈出关键一步。

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [118] [MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues](https://arxiv.org/abs/2601.20451)
*Diandian Guo,Fangfang Yuan,Cong Cao,Xixun Lin,Chuan Zhou,Hao Peng,Yanan Cao,Yanbing Liu*

Main category: cs.CL

TL;DR: 本论文提出了MuVaC，一个用于多模态讽刺检测与解释的变分因果推理框架，同时优化检测与解释任务，以提升对社交平台讽刺内容的理解能力。


<details>
  <summary>Details</summary>
Motivation: 在社交平台中，多模态对话中讽刺现象普遍且难以解读，而全面分析讽刺需要同时解决讽刺检测与解释任务。目前研究大多将两者割裂，忽视了检测与解释的因果依赖关系。本论文旨在弥合这一空白。

Method: 提出MuVaC框架，基于结构因果模型将多模态讽刺检测（MSD）和解释（MuSE）联合建模，建立变分因果路径以实现任务联合优化；设计了“对齐-再融合”的多模态特征整合方式，并在检测与解释间增强一致性以提升推理可信度。

Result: 在公开数据集上的实验显示，MuVaC在多模态讽刺检测和解释任务中性能优于现有方法。

Conclusion: MuVaC框架为理解多模态讽刺提供了更一致、更可信的新角度，有效促进了相关任务的表现提升，并为后续研究带来新启示。

Abstract: The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.

</details>


### [119] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: 本文提出BMAM（类脑多智能体记忆）架构，通过模拟大脑多功能记忆系统，提升大模型在长时间交互中的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的智能体在多轮、长时间交互中，容易遗失重要的时间信息和行为一致性，出现所谓“灵魂流失”问题。如何让智能体持续、稳定地保持知识和行为一致，是提升智能体能力的关键难题。

Method: 受认知科学启发，BMAM将记忆分为情节、语义、显著性和控制四个互补子系统，并设计各自功能和时间尺度。特别地，情节记忆以时间线组织，允许多信号融合检索长时推理证据。实验采用LoCoMo基准对架构效果进行评估及消融分析。

Result: BMAM在LoCoMo基准测试中，在长时序任务下取得了78.45%的准确率。消融实验发现，模拟海马体的情节记忆模块对时序推理尤为关键。

Conclusion: BMAM架构有效解决了智能体长时交互中的时间信息保持和一致性问题，为智能体内在“灵魂”稳定奠定基础，并证明多类型专用记忆子系统的合理性和必要性。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [120] [Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch](https://arxiv.org/abs/2601.20476)
*Evanfiya Logacheva,Arto Hellas,Tsvetomila Mihaylova,Juha Sorva,Ava Heinonen,Juho Leinonen*

Main category: cs.CL

TL;DR: 本研究提出一种基于修辞结构理论（RST）和情境示例的生成式AI图表代码生成方法，以减少AI幻觉并提升图表生成质量，并经实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI已广泛应用于计算机教育，其生成内容的质量及幻觉现象却引发关注，尤其在图表自动生成领域表现突出。因此，亟需提高自动化生成图表的可靠性与合理性。

Method: 作者提出基于修辞结构理论和情境示例的图表代码生成方法，通过引导大型语言模型（LLMs）更好地对齐用户预期来提升图表生成质量。实验邀请计算机教育者对150份生成图表从逻辑、连贯性、美观与AI幻觉等多方面进行评估，并进一步探讨用于自动化评估图表的可能性。

Result: 初步实验结果显示，该方法能有效减少幻觉现象，并提升生成图表与上下文一致性，但受制于LLM的随机性，图表整体质量仍有波动。同时，复杂语境更易造成幻觉现象，且LLM难以及时检测自身输出的错误。

Conclusion: 基于RST和情境示例的方法在降低生成式AI幻觉、提升图表信度方面具有前景，但LLM自身存在的不稳定性和高语境复杂度下的幻觉问题仍需进一步解决。

Abstract: Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.

</details>


### [121] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 该论文提出了一种新的大语言模型（LLM）创造力评估方法CDAT，结合新颖性与适切性，更贴近人类创造力理论，并发现主流LLM在创造力和适切性上存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估大语言模型（LLM）创造力的任务（如DAT）过于侧重新颖性，忽视了创造力同样核心的适切性。现有评分方法的有效性和理论基础不足。作者希望开发一种更符合人类创造力理论的评估方法，提高评价的科学性和解释性。

Method: 作者首先用DAT对主流LLM进行测试，发现一些没有创造力的基线模型表现反而更高。随后基于创造力理论，提出Conditional Divergent Association Task（CDAT），引入上下文相关的适切性，将新颖性与适切性结合用于评估，并与DAT比较效果。

Result: 实验表明，CDAT能更好地区分真实创造力与无意义的噪声。通过CDAT评估，不同规模和对齐程度的LLM在新颖性和适切性上表现出不同的权衡关系。有些小模型在新颖性得分高，大模型则偏向高适切性但新颖性降低。

Conclusion: CDAT方法更加科学、客观地评估了LLM的创造力，发现训练和对齐过程往往提升了模型的适切性但降低新颖性。该研究强调了创造力评估中同时关注新颖性与适切性的重要性，并开源了数据集和代码，以便社区进一步研究。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [122] [Single-Nodal Spontaneous Symmetry Breaking in NLP Models](https://arxiv.org/abs/2601.20582)
*Shalom Rosner,Ronit D. Gross,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 本文发现，在NLP模型的预训练和微调过程中，即使在有限的网络结构和确定性动态下，也会出现自发对称性破缺现象。该现象在注意力头、节点甚至单一节点的层面都有体现。通过分析BERT-6模型，作者展示了该现象在具体任务中的表现及其机制。


<details>
  <summary>Details</summary>
Motivation: 研究经典统计物理的自发对称性破缺现象是否与NLP神经网络中的学习机制相关，以及这种物理机制如何影响模型的性能与学习能力。

Method: 在BERT-6模型上进行实验，分别在Wikipedia数据集进行预训练，并在FewRel分类任务上微调，考察注意力头和节点层面对称性破缺的出现及其动力学机制。利用凸包分析方法对每个节点的作用进行理论上界分析。

Result: 实验证明，NLP模型在有限结构下依然会出现自发对称性破缺，且该破缺不仅存在于宏观的模型输出，还可以追踪到注意力头、个体节点甚至单一节点。节点数目增加会导致学习能力的变化，受制于输出空间扩大的错猜倾向与节点协作增强的平衡。

Conclusion: NLP模型中的自发对称性破缺现象与统计物理中的类似，但每个节点对整体任务有明确贡献并可定量分析。该机制揭示了模型内部学习能力形成的新视角，也为模型设计及优化提供了理论基础。

Abstract: Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.

</details>


### [123] [A Computational Approach to Language Contact -- A Case Study of Persian](https://arxiv.org/abs/2601.20592)
*Ali Basirat,Danial Namazifard,Navid Baradaran Hemmati*

Main category: cs.CL

TL;DR: 本文研究了单语语言模型中与语言接触相关的结构性痕迹，发现历史接触对语法特征影响有限，但对形态特征影响明显。


<details>
  <summary>Details</summary>
Motivation: 探索历经语言接触的波斯语，其单语模型内部表示是否及如何反映不同程度和类型的语言接触。

Method: 以波斯语训练的单语模型为对象，暴露于与波斯语有不同接触背景的其它语言，量化模型中间表示承载的语言信息，并分析其在模型不同组件和形态句法特征上的分布。

Result: 发现模型对普适句法信息的表征对历史语言接触基本不敏感，而形态特征，如格（Case）和性（Gender）则受到明显的语言特异性结构影响。

Conclusion: 单语模型中的语言接触效应具有选择性，主要受结构约束，形态特征更易受影响，而普适句法则较为稳定。

Abstract: We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.

</details>


### [124] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 本文提出了AgentIF-OneDay基准，关注AI代理在日常多样化任务中，普通用户使用自然语言指令与AI协作的能力评估。涵盖工作、生活和学习三大场景，考察AI真实解决实际问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估偏向增加任务难度，而忽视了日常多样化任务的代表性，导致一般用户对AI能力的感知有限。作者认为评测应更贴近真实生活任务场景。

Method: 提出AgentIF-OneDay基准，涵盖开放式流程执行、隐式指令理解和迭代完善三类任务。通过丰富任务和实例化评分细则，结合LLM验证与人工标注评测AI代理。

Result: AgentIF-OneDay包含104个任务、767个评分点。对四种主流AI代理展开基准测试，发现API产品与基于RL的Agent均表现突出。Gemini-3-Pro与人工评分一致率达80.1%。

Conclusion: 主流LLM及相关API已经内化了较强的Agent能力，为AI开发团队打造创新型智能代理提供坚实基础。该基准可更真实反映AI助理在日常多样任务中的实用性。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [125] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的自监督方法Probabilistic Process Supervision (P2S)，为大型语言模型在通用推理任务中的每一步提供详细奖励信号，并显著提升了模型在阅读理解和医疗问答等任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励可验证性的强化学习方法（如RLVR）主要在数学和编程等结构化领域有效，但在通用领域由于缺少可验证的奖励信号而效果有限。现有方法如RLPR侧重于结果奖励，忽视了推理过程中的每一步监督，因此亟需能为推理过程本身提供细致反馈的方法。

Method: 作者提出P2S自监督框架，在强化学习过程中自动生成并筛选高质量的推理链（gold-CoT），然后对每一步推理，根据模型当前生成前缀和gold-CoT后缀的条件概率，计算路径忠实奖励（PFR）。PFR可以与结果奖励灵活结合，为模型训练提供密集反馈，无需额外奖励模型或人工标注。

Result: 在阅读理解与医疗问答等基准任务上，P2S显著优于强基线方法，证明了其在模型推理能力提升上的有效性。

Conclusion: P2S方法能够为通用推理任务中的大型语言模型提供细粒度的过程奖励，有效缓解奖励稀疏问题，并在多个实际任务中明显提升了推理表现。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [126] [A Dialectic Pipeline for Improving LLM Robustness](https://arxiv.org/abs/2601.20659)
*Sara Candussio*

Main category: cs.CL

TL;DR: 本文提出了一种通过让大语言模型(LLM)进行自我对话（dialectic pipeline），减少幻觉现象并提升输出质量的方法，并证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前提升LLM输出质量的方法（如微调、训练校验器）计算资源需求高，且限制了模型通用性。因此，亟需一种能兼顾高效性和模型泛化能力的方法来减少“幻觉”并提升输出准确性。

Method: 作者提出了“辩证管道”，即令LLM进行自我对话，自我反思并修正可能的错误答案。该方法在不同数据集和模型上进行实验，并在各流程阶段加入了相关背景上下文（oracle-RAG设定），同时研究了背景摘要和过滤的影响。

Result: 实验结果显示，辩证管道在输出质量或准确性上显著优于标准答案和仅使用链式思维（CoT）提示的方法。

Conclusion: 辩证管道方法无需大量额外计算资源，能在保持模型泛化能力的同时显著提升输出质量，有望更好地支持LLM在不同场景下的大规模应用。

Abstract: Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.
  However, methods such as fine-tuning on domain-specific data or the training of a separate \textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.
  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.
  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.
  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.

</details>


### [127] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: 本研究探索大语言模型（LLMs）在电子健康记录（EHR）数据分析中的结构化数据查询与非结构化临床文本信息抽取的应用可行性。


<details>
  <summary>Details</summary>
Motivation: EHR数据既包含结构化又包含非结构化内容，传统方法处理存在局限，研究希望借助LLMs提升医疗数据查询与信息抽取的效率和准确性。

Method: 采用Python/Pandas进行结构化数据查询，利用检索增强生成（RAG）方法对临床文本信息进行抽取。搭建自动生成合成问答对的评估框架，基于MIMIC III数据集，对多种本地及API型LLMs进行实验。

Result: 基于精确匹配、语义相似性和人工评判的评估显示，LLMs具备较高的结构化查询和临床信息抽取能力。

Conclusion: LLMs在临床工作流中有望实现高效准确的数据查询和信息抽取，推动电子健康记录数据的智能化利用。

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [128] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 本文提出了一种多模态规划代理（agent）方法，通过动态分解和优化mRAG流程，有效提升了视觉问答（VQA）效率，并在多个数据集上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前VQA任务中的多模态检索增强生成（mRAG）方法虽能增强系统能力，但普遍采用多阶段串联流程，计算冗余高，效率低，尤其在知识密集型场景下问题更为突出。本文旨在解决效率与效果兼顾的mRAG VQA流程瓶颈。

Method: 作者训练了一个多模态规划代理，用于智能地动态拆分和控制mRAG流程的每一步，判断每一步是否必须执行，以减少冗余计算。该代理通过训练学习如何在保证准确性的同时优化检索与生成资源的花费。

Result: 实验显示，该代理能够在维持甚至提升准确率的前提下，将VQA检索和推理的总时间缩短超过60%，显著减少昂贵的工具调用次数，并在六个VQA数据集上全面超越了包括深度研究代理和精心设计的prompt方法等基线。

Conclusion: 提出的方法在保证VQA解答质量的同时，极大提升了计算和运行效率，展示了多模态任务中动态流程规划的实际应用价值。代码将公开，有望为后续相关研究提供支撑。

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [129] [ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code](https://arxiv.org/abs/2601.20679)
*Mingqiao Mo,Yunlong Tan,Hao Zhang,Heng Zhang,Yangfan He*

Main category: cs.CL

TL;DR: 本文提出了ShieldedCode，这是首个面向保护的虚拟机保护代码（VMP）表征学习框架，显著提升了模型在保护代码生成与分析中的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在代码生成领域有重大突破，但在软件安全保护方面的潜力没有被充分挖掘。现有虚拟机保护方法依赖固定规则，设计成本高且易被自动分析破解。因此，需要利用AI构建更有效、鲁棒的软件保护方案。

Method: 作者构建了大规模源代码与归一化虚拟机实现的配对数据集，并提出层次化依赖建模（涵盖指令内、前序、指令间）。联合优化语言建模、功能感知与保护感知的对比损失，以同时捕捉语义等价性与保护强度。还提出了衡量不同虚拟机保护变体有效性的任务，并采用两阶段的连续预训练和微调流程，使模型具备生成、比较和推理保护代码的能力。

Result: 实验显示，ShieldedCode在多种保护强度下显著提升了模型鲁棒性。在L0 VM代码生成上Pass@1为26.95%，高于GPT-4o的22.58%；在二进制相似性检测Recall@1上比jTrans等先进方法提升10%。

Conclusion: 本方法为基于学习的软件防护开辟了新方向，ShieldedCode在生成和分析受VMP保护代码方面取得优越性能，为软件安全带来更强的防护能力。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.

</details>


### [130] [Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin](https://arxiv.org/abs/2601.20680)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.CL

TL;DR: 本论文探索将社交媒体监控中的分批聚类（如HDBSCAN）替换为在线聚类方法，以应对大规模实时数据流处理的挑战，并在实际应用管道中进行比较评估。


<details>
  <summary>Details</summary>
Motivation: 现有批处理聚类算法在处理社交媒体连续、大规模、多语种数据流时面临扩展性、效率、内存与时效性难题，不能满足实时叙事检测的需求。

Method: 提出三阶段体系架构（数据收集、建模、仪表盘生成），系统性地比较多种在线聚类算法（含滑动窗口模拟），并结合经典聚类指标与叙事度量，对算法质量、效率与集成兼容性进行评估。

Result: 实验在乌克兰信息空间的历史数据集上实施，结果展示了不同在线聚类算法的质量、效率和集成权衡，为实际部署和选择算法提供了定量参考。

Conclusion: 研究弥补了批处理主题建模与社交媒体数据流需求间的鸿沟，为计算社会科学、危机信息学和叙事监控领域提供了高效、可扩展的解决方案和评估准则。

Abstract: Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.

</details>


### [131] [AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts](https://arxiv.org/abs/2601.20730)
*Shicheng Fang,Yuxin Wang,XiaoRan Liu,Jiahao Lu,Chuanyuan Tan,Xinchi Chen,Yining Zheng. Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文提出了AgentLongBench，用以评估大语言模型（LLM）作为自主智能体在复杂动态环境中的表现，揭示了它们在动态信息综合方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测主要为静态被动检索，难以反映智能体与环境互动中的动态推理和反馈能力。因此需要新的评测框架以更真实地反映LLM在实际代理任务中的表现。

Method: 作者设计了AgentLongBench，将侧重于横向思维谜题的环境模拟作为评测基础，自动生成知识密集型和知识无关型交互轨迹，通过真实交互流程考察智能体的动态推理与反馈处理能力。

Result: 实验发现，即便是SOTA模型和大规模记忆系统（32K到400万token）在静态检索上表现良好，但在动态信息整合和复杂工作流处理上显著失效。分析指出关键瓶颈是回答查询所需的最小token量，这直接导致高信息密度的tool响应比长轮对话中的记忆碎片化挑战更大。

Conclusion: 当前LLM虽擅长静态任务，但对环境动态交互和大信息量加工能力有限，AgentLongBench揭示了未来LLM代理系统需要在动态信息综合和高效处理密集响应上重点突破。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.

</details>


### [132] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）如何再现和传播社会规范（特别是异性恋顺性别规范）及其在文本生成中的可测量偏见。基于对不同性别与性取向标识类别的测试，量化分析了模型在情感、评价、有害性和多样性等维度的表现。


<details>
  <summary>Details</summary>
Motivation: 近年来，LLM在各种语言任务中广泛应用，但其反映和加剧社会偏见的问题备受关注。该研究旨在探究LLM在涉及性别和性取向时是否会再现偏见，以及具体表现在何处。

Method: 研究将被试主体划分为‘酷儿标记’、‘非酷儿标记’和‘未标记’三类，并对比MLM和ARLM两种主流模型在处理相关语句补全任务时，在情感、评价、有害性和生成多样性上的量化体现。

Result: MLM在面向酷儿标记主体时，生成的文本情感最不积极、有害性更高、评价更低。ARLM虽在一定程度上改善了上述问题，但封闭的ARLM对未标记主体反而更有害。总体上各种模型都不可避免地再现社会规范与偏见。

Conclusion: LLM会再现社会主流规范和歧视，表现为针对不同性别/性取向标记主体的生成文本差异。尽管部分模型特性有助于缓解部分偏见，但代表性伤害未被根除，只是被重新分配。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [133] [Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts](https://arxiv.org/abs/2601.20747)
*Elham Aghakhani,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文分析了Reddit上用户使用大语言模型（LLM）进行情感支持和心理健康相关互动的经验，重点考察用户的评价方式及其与系统的关系。结果揭示用户的信任、任务匹配等因素比情感纽带更影响使用态度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被越来越多地用于非临床场景中的情感支持和心理健康互动，研究者亟需了解用户日常实际使用和评价这些系统的方式。这有助于指导设计更符合用户需求的AI系统，提高应用效果与安全性。

Method: 作者收集了Reddit上47个心理健康社区中5,126篇关于AI情感支持体验的帖子，基于技术接受模型和治疗联盟理论构建了标注框架，采用LLM与人工相结合的分析流水线，对文本中的评价语言、采纳态度和关系匹配等内容进行大规模分析。

Result: 分析发现，用户对AI系统的参与度主要受实际结果、信任度和回应质量影响，而不仅仅是情感上的联结。正面评价更多出现在任务和目标高度一致的情境中；而寻求陪伴的用户则较常出现关系不匹配和依赖、症状加重等风险。

Conclusion: 该研究表明，理论驱动的结构可以有效用于大规模线上文本分析，并强调了在敏感、真实世界情景下理解用户如何诠释和接受语言技术的重要性。

Abstract: Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.

</details>


### [134] [Persona Prompting as a Lens on LLM Social Reasoning](https://arxiv.org/abs/2601.20757)
*Jing Yang,Moritz Hechtbauer,Elisabeth Khalilov,Evelyn Luise Brinkmann,Vera Schmitt,Nils Feldhus*

Main category: cs.CL

TL;DR: 本文研究了在仇恨言论检测等社会敏感任务中，使用“角色提示（Persona Prompting, PP）”对大型语言模型（LLM）生成理由解释的影响。作者发现，虽PP提升了部分任务的分类表现，但降低了解释理由的质量，且并未有效减少模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 在涉及社会敏感性的NLP任务中，如仇恨言论检测，模型生成的解释质量影响用户信任与模型可靠性。目前，角色提示作为一种定制解释的技术日益流行，但其对模型解释理由和偏见的具体影响仍缺少系统分析，因此需要深入研究。

Method: 作者利用带有词级解释注释的数据集，对比不同模拟人口角色下，LLM生成的解释和真实注释之间的一致性。同时，在多个LLM上评估PP对分类效果、模型偏见及人类对齐度的影响。

Result: （1）在主观性最强的仇恨言论检测任务中，PP提升了分类准确度但降低了解释理由质量；（2）模拟身份角色的解释与现实人口群体的解释无法很好对齐，且模型不同角色间的一致性较高，表明模型对角色提示整体不敏感；（3）模型存在稳定的群体性偏见，并普遍更倾向于将内容判为有害，即使采用PP也无法缓解。

Conclusion: 角色提示可提升一些主观社会任务的分类表现，但常常会牺牲解释理由的质量，也无法有效消除模型固有偏见。因此，其应用需谨慎权衡利弊，不能过度依赖。

Abstract: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

</details>


### [135] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: 本论文提出SERA方法，使开源编码智能体能够高效且廉价地专门化到私有代码库，并开源了全部代码、数据和集成工具。


<details>
  <summary>Details</summary>
Motivation: 开源编码智能体理论上能通过直接专门化到私有代码库，在准确性和适应性上超越闭源系统，但由于训练成本和复杂度，实际难以实现。这促使作者寻找高效、低成本的专门化训练方法。

Method: 提出了一种高效的监督微调方法（SFT），并开发出Soft Verified Generation（SVG）流程，能从单一代码库生成大量训练轨迹用于智能体训练。通过与已有高性能模型对比，SERA仅靠监督微调即可达到行业领先水平，且成本远低于传统的强化学习和合成数据方法。

Result: SERA在开源领域取得了最优结果，性能可与前沿开源权重模型Devstral-Small-2媲美。相同效果下，所需训练成本仅为强化学习的1/26、合成数据方法的1/57。SVG可将单一库扩展为数千条轨迹，扩大至20万以上赛道的大规模数据集，并用于详细的刻画分析。

Conclusion: 论文证明了开源编码智能体可高效专门化于私有代码库，比闭源系统更具扩展性和通用性，并有望推动相关研究发展。SERA和全部资源已经开源，有望加速社区创新。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [136] [Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers](https://arxiv.org/abs/2601.20796)
*Yiran Huang,Karsten Roth,Quentin Bouniot,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 本论文通过对小型transformer模型在合成任务上的实验证明，Transformer在跨模态上下文学习(ICL)中显示出数据复杂性和模型设计的核心机制，尤其发现多模态ICL存在显著学习不对称性。


<details>
  <summary>Details</summary>
Motivation: 受Transformer模型在多模态ICL中强大表现的启发，研究者希望揭示Transformer如何通过上下文示例学习和关联不同模态的信息，从而理解其内在机制。

Method: 采用可控的小型Transformer模型，设计合成分类任务，同时精确控制数据分布和模型结构，先研究单模态ICL机制，再扩展至多模态环境，并聚焦于旋转位置编码(RoPE)及其对ICL效能的影响。

Result: 发现RoPE会提升模型达到ICL的复杂性门槛，多模态下只需对副模态给予较低复杂性数据，仍可有效实现多模态ICL。机制分析显示，无论单模态还是多模态，其核心均是通过复制匹配示例标签的归纳机制，多模态训练能够将此回路推广跨模态。

Conclusion: 本研究揭示了Transformer模型中多模态ICL的机制，为理解其学习原理提供基础理论支持，并建立了可控测评环境，为后续研究奠定基础。

Abstract: Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.

</details>


### [137] [Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction](https://arxiv.org/abs/2601.20803)
*Aunabil Chakma,Mihai Surdeanu,Eduardo Blanco*

Main category: cs.CL

TL;DR: 提出了一种结合语法语义相似性与LLM生成样本的新型样本选择策略，用于提升一跳关系抽取的in-context learning效果，实验在多个数据集和模型上取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 在一跳关系抽取任务中，例子样本有限时，如何自动获取高质量的辅助样本显得尤为重要。当前方法主要依赖随机选择或LLM生成例子，但存在代表性不足或同质性高的问题。该文旨在解决样本多样性和代表性兼备的难题。

Method: 提出基于语法-语义结构相似性的样本自动筛选方法，并与LLM生成样本策略结合，形成混合例子选择框架。该方法在FS-TACRED与FS-FewRel数据集、Qwen和Gemma两类大模型上进行测试。

Result: 该混合样本选择方法能选出词汇和句式互补的样本，表现显著优于仅用单一策略，取得FS-TACRED业界最佳结果，在定制FewRel子集上也获得明显提升。

Conclusion: 结合语法-语义相似性筛选和LLM生成样本的方法能更全面刻画关系抽取场景，为提升小样本关系抽取的in-context learning提供实用框架，在跨数据集和模型间具有良好泛化性。

Abstract: This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.

</details>


### [138] [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/abs/2601.20834)
*Andrew Kyle Lampinen,Yuxuan Li,Eghbal Hosseini,Sangnie Bhardwaj,Murray Shanahan*

Main category: cs.CL

TL;DR: 本文研究了语言模型在对话过程中，其内部表示（如线性方向）如何动态变化，发现这些表示会根据对话内容发生显著改变，对解释性和模型控制能力提出了挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注语言模型中表示高层语义的线性方向，但这些表示是否会在具体对话情境中变化尚不清楚。本文旨在揭示语言模型内的语义表示如何随对话过程与内容而演化，以助于理解其可解释性和可控性。

Method: 作者通过模拟对话，跟踪语言模型（包括不同模型家族和层级）中的线性表示变化，包括在固定对话脚本和不同上下文（如科幻故事）下的实验。同时考察了主动操控这些方向（steering）在对话不同阶段的效果。

Result: 发现：1）代表事实与虚构的信息线性方向会随对话进展发生显著变化，并受对话内容影响；2）无论是否是原生或转录对话脚本，这种变化都成立，但在明确的虚构上下文下适应性明显减弱；3）对表示方向的主动操控在不同对话阶段效果不同。

Conclusion: 静态理解语言模型内部表示存在局限，解释和控制语言模型时应考虑其表征的动态演化；这些发现既提出了解释难题，也开启了新的上下文适应性研究方向。

Abstract: Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.

</details>


### [139] [When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation](https://arxiv.org/abs/2601.20858)
*David Tan,Pinzhen Chen,Josef van Genabith,Koel Dutta Chowdhury*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）在基准测试中存在的“污染”现象，这种污染会导致模型分数虚高，掩盖了记忆而非泛化能力的问题。通过FLORES-200多语言翻译基准，比较了受污染（Bloomz）和未污染（Llama）的模型，验证了污染对模型性能评估的影响，并探讨了检测与缓解记忆化输出的方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于翻译等多语言任务，如何准确评估模型的泛化能力变得至关重要。目前，部分基准数据已被用于模型训练，导致模型“记忆”参考答案，使评测结果失真。本文旨在揭示并分析这种“基准污染”现象，并探索其在多语言环境下的跨方向影响。

Method: 作者选用两个7-8B参数级别、不同训练背景的多语言LLM（Bloomz和Llama），并以FLORES-200翻译基准作为诊断工具。通过对比分析，研究了污染模型（Bloomz）和未污染模型（Llama）在基准测试中的表现，同时引入源文本扰动（如转述、替换实体）来测试模型是否依然输出记忆内容。

Result: 实验发现，Bloomz模型确实因训练集包含FLORES数据而产生基准污染。污染不仅体现在已见方向上，还在未见翻译方向因目标语言记忆而“转移”性能提升。此外，大量尝试扰动源文本后，模型依然倾向于输出记忆内容，但通过替换命名实体，BLEU分数明显下降，为检测污染提供了有效方法。

Conclusion: 基准污染会导致大语言模型在评测中出现误导性高分，尤其是在多语言翻译任务中可能通过目标端记忆跨方向影响评分。针对污染模型，引入命名实体替换是一种检测其记忆化行为的可行手段。未来模型评估需警惕基准污染，并考虑更健全的检测体系。

Abstract: Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to "uncontaminated" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [140] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种更高效的人在环强化学习（HiL-RL）方法，通过有选择性地采样，减少对人工干预的依赖，并显著提升操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有HiL-RL方法样本效率低，需要大量人工干预，导致高劳动力成本。作者希望通过提高样本效率，降低人工干预需求。

Method: 该方法通过主动选择对策略熵影响中等的样本，剔除带来熵陡降的捷径样本和影响微弱的噪声样本，从而在探索与利用之间取得更优权衡。其关键在于通过动作概率与策略软优势的协方差高效估算不同样本对策略熵的影响函数。

Result: 在四个真实操作任务实验表明，新方法在减少10.1%人工干预的同时，成功率提升了42.1%，优于现有方法。

Conclusion: 该方法有效降低了人工干预，提高了学习效率，为实际复杂操作任务中的HiL-RL应用提供了有力工具。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [141] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过动态精细化边连接和自适应采样，提升高维机器人在复杂环境下的路径规划效率与安全性，优于传统采样规划方法。


<details>
  <summary>Details</summary>
Motivation: 高维机器人路径规划在多障碍环境中既难找到可行路径，也难以获得最优路径。特别是在机械臂操作中，还涉及运动奇异点和自身碰撞等风险，当前方法效率与安全性均受限。

Method: 提出了JIT*算法，对EIT*方法进行改进，通过引入Just-in-Time模块（包括动态精细化边连接与瓶颈区域自适应采样）提升初始路径发现速度。同时，Motion Performance模块动态平衡机械臂操作的可操作性与轨迹代价，降低运动奇异性风险。

Result: 在常用采样规划器基础上，JIT*在$mathbb{R}^4$到$mathbb{R}^{16}$多空间维度下均表现出更优性能。实验证明JIT*在单臂和双臂任务上有效提升了规划效率及路径质量。

Conclusion: JIT*算法能够在高维复杂环境下实现高效、安全的路径规划，优于传统采样方法，具备推广应用于实际机器人系统的潜力。

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [142] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新方法REMAC，通过对已有策略进行掩码动作分块学习，在异步推理下提高机器人的实时性和稳定性。实验表明，该方法无额外延迟下显著提升任务完成速度和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的异步推理方法虽然加快了机器人响应速度，但简单集成常导致执行失败。以往研究只关注分块间不连续性（inter-chunk discontinuity），而忽视了动作分块内部与当前感知不一致（intra-chunk inconsistency）的问题。作者认为后者是执行失败被忽略的关键原因。

Method: 提出REMAC，利用掩码动作分块机制学习对已有策略的纠正，使策略在动作与实际执行出现偏差时依然鲁棒；引入前缀保持采样，进一步强化分块间的连续性。方法不增加额外延迟。

Result: 在多组仿真与实际机器人实验中，REMAC在不同延迟场景下均能加快任务执行速度，提高鲁棒性，并一贯实现更高的任务完成率。

Conclusion: REMAC有效缓解了动作分块内部不一致的问题，在不牺牲响应速度的前提下，大幅提升机器人在动态环境下的实时性、可靠性和完成效率，对实际物理系统的安全性及应用拓展具有较大意义。

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [143] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 本文提出了一种高斯过程（GP）更新方法，针对移动机器人测量中因定位误差带来的模型退化问题，利用核函数的可微性，通过二阶修正算法实时优化GP预测。


<details>
  <summary>Details</summary>
Motivation: GP模型广泛用于回归任务，但应用于移动机器人测量时，测量点位置不精确导致预测准确性下降，现有方法难以高效应对定位误差下的模型更新。

Method: 作者利用GP均值和协方差函数对测量位置的雅可比和黑塞矩阵，开发了一个基于二阶泰勒展开的修正算法。每当获得更准确的测量位置估计时，可快速实时修正GP模型，而无需完全重训练。

Result: 仿真结果显示，所提算法在保持高预测准确性的同时，计算效率优于每次全量重训练GP模型的方法。

Conclusion: 该方法有效提升了带测量位置不确定性的GP预测精度，并具备很好的计算效率，适合实际机器人场景下的实时应用。

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [144] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: 提出了一种针对可变形物体的语义推理与物理操作结合的新方法TRACER，提升了机械臂操作可变形物体时的视觉感知与功能区域定位的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有可变形物体机器人操作方法在处理外观多变、动态复杂等问题时，易出现功能区预测分散、不准确等缺陷，难以将高层语义意图准确映射到具体交互点，限制了任务执行能力。

Method: TRACER框架通过：1）树状Affordance Chain-of-Thought机制，将高层任务意图层次化分解为各阶段可执行子任务，2）空间约束边界精细化机制（SCBR）抑制视觉预测溢出，提升功能区边界准确性，3）交互收敛精细化流（ICRF）整合受噪声影响的像素，提高物理区域连续性和真实性。

Result: 在Fine-AGDDO15数据集及真实机器人平台上实验显示，TRACER对不同纹理和模式下的可变形物体具有更高的affordance定位精度，显著提升了任务成功率。

Conclusion: TRACER有效提升了机器人对可变形物体的操作能力，实现了高层语义推理与物理操作低层执行的高效衔接，对相关领域具有显著推动作用。代码和数据集将开源。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [145] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: 该论文提出了TouchGuide，一种结合视觉和触觉信息的新型策略，显著提升了机器人在精细接触操作任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人在需要精细和丰富接触的操作任务（如穿鞋带、传递芯片）中表现不佳，主要因为未能充分利用触觉反馈。因此，研究者期望通过更好地融合视觉和触觉信息来提升机器人的操作能力。

Method: 提出TouchGuide方法，分为两个阶段：第一阶段利用预训练视觉策略对动作做粗略预测；第二阶段引入接触物理模型（CPM），根据触觉反馈进一步修正动作。CPM通过对少量专家演示的对比学习训练，提供基于触觉的可行性分数，引导动作采样满足物理接触约束。此外，论文还提出了TacUMI数据采集系统，通过刚性指尖高效获得高质量触觉数据。

Result: 在包括鞋带穿线与电子元件递交等五个复杂任务中，TouchGuide均远超现有最先进的视觉-触觉结合策略，表现更优。

Conclusion: TouchGuide新范式通过视觉与触觉信息的有效融合，极大提升了机器人的复杂操作能力，并且TacUMI系统为后续工作提供了高效数据采集方案。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [146] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: 该论文提出了一种名为Shallow-pi的新方法，通过知识蒸馏显著减少VLA模型中transformer的层数，实现模型高效压缩且基本不损失性能，并在多种机器人平台验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型部署于机器人端时推理速度慢，主要优化点集中在token剪枝，而transformer层数的系统性削减研究较少。特别是在知识蒸馏下针对flow-based VLA模型的层数减少基本无人探索。

Method: 提出Shallow-pi知识蒸馏框架，大幅度减少VLM主干与flow-based动作头的transformer深度，从18层减至6层，通过知识蒸馏保留模型核心能力。

Result: 在主流机器人操作基准上，Shallow-pi实现推理速度提升2倍以上，成功率绝对下降低于1%，并在压缩VLA模型中表现为最佳。同时在Jetson Orin、Jetson Thor等工业硬件和多平台机器人系统内进行了实际部署验证。

Conclusion: 在加速模型推理应用于实时机器人场景的同时，Shallow-pi方法在保证性能的前提下，实现大幅压缩，为高效机器人自决策系统提供了可行路径。

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [147] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: 该论文针对视觉-语言-动作（VLA）模型在机器人操作中缺乏物理直觉的问题，提出了一种基于触觉-力对齐的新范式，通过引入TaF-VLA系统显著提升了复杂接触操作任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型过于依赖视觉，在需要精确力调控和物理推理的接触丰富任务中表现不佳。此前工作对触觉信息的处理方式忽视了表面形变与交互动力学间的本质联系。为解决这一挑战，作者寻求通过更物理本质的方式将触觉信息和VLA模型结合。

Method: 作者提出了从“触觉-视觉对齐”转向“触觉-力对齐”，具体方法为：开发了自动化触觉-力数据设备并建立了含千万级同步数据的TaF数据集；设计了Tactile-Force Adapter（TaF-Adapter）用于编码触觉并提取与物理交互动力学强相关的特征；将这一力对齐的触觉感知模块融入VLA主干网络，实现跨模态的物理推理能力。

Result: 大规模真实机器人实验表明，TaF-VLA策略在丰富接触任务中明显优于现有视觉-触觉对齐及单纯视觉基线方法，在稳健性与力感知能力方面均表现卓越。

Conclusion: 通过用触觉-力对齐的新思路和支持大规模数据的技术实现，该工作极大提升了机器人在涉及复杂物理交互场景下的泛化性和操作能力，推动了VLA模型的实际应用价值。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [148] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: 本文提出FAEA框架，直接应用大语言模型（LLM）智能体于机器人操控任务，无需特定演示或微调，依然能获得与现有视觉-语言-动作模型相当的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大部分视觉-语言-动作模型（VLA）在机器人操控中表现出色，但它们依赖于任务特定的演示数据和微调，且在领域转移下泛化性较差。作者希望探索通用LLM智能体（如原用于软件工程）的在机器人操控中的潜力，减少依赖演示和提升任务通用性。

Method: 作者提出FAEA（Frontier Agent as Embodied Agent）框架，直接将通用LLM智能体用于机器人操控任务，不作修改。通过与软件调试类似的迭代推理方式，智能体可推理出操控策略。使用Claude Agent SDK在LIBERO、ManiSkill3和MetaWorld基准测试上评估性能，并在有特权环境状态访问条件下进行实验。

Result: FAEA在LIBERO、ManiSkill3和MetaWorld上的任务成功率分别为84.9%、85.7%和96%，接近仅用少量演示训练的VLA模型。在LIBERO上引入一次人工反馈后，成功率提升至88.2%。这些结果均无需演示或微调。

Conclusion: 通用LLM智能体无需演示即可高效完成依赖思考和任务层级规划的操控任务，适合作为机器人控制范式的新选择，对泛化和数据自动生成具有实际意义。该方法将有助于机器人系统利用现有智能体设施及未来模型的持续进步。

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [149] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出了RF-MatID，这是首个大规模、开源、频带宽广、具备多样几何特性的射频（RF）材料识别数据集。该数据集包含16种细粒度类别，覆盖4-43.5 GHz频段，共142,000个样本，并引入了系统化的几何扰动，对主流深度学习方法进行了基准测试，为RF材料识别研究提供支撑。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法用于材料识别受到光学传感器的物理局限，而RF方法虽能反映材料本征属性，但受制于缺乏大规模公开数据集及算法基准，限制了AI在该方向的快速发展。

Method: 作者构建了RF-MatID数据集，涵盖16类材料，分5个大类，采集了142,000个时域和频域样本，在采集时系统性地控制了入射角、距离等几何扰动。并设置了多协议、多场景的测试基准，对深度学习材料识别模型进行评估。

Result: 该数据集支持了包括角度、距离迁移等多种复杂测试，首次系统性地评估了主流模型在大规模RF材料识别任务上的表现，并实现了完善的频率和区域分析。

Conclusion: RF-MatID为RF材料识别领域提供了标准数据集和评测基准，有助于科研复现、算法进步和跨领域应用，推动RF材料识别在实际场景的落地。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [150] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级对象中心化适配模块STORM，用于增强视觉基础模型在机器人操作中的表现，实现更好的泛化和控制性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型虽能提供强大的感知特征，但其稠密表示缺乏显式的对象结构，这限制了其在机器人操作任务中的稳健性和可控性。因此，亟需以对象为中心的方法来提升机器人对复杂环境中对象的辨识与操控能力。

Method: 提出了STORM模块，它在冻结视觉基础模型的前提下，增加了少量语义感知的slot。STORM采用多阶段训练策略：首先使用视觉-语义预训练（通过语言embedding）稳定对象中心slot，然后与下游的操作策略联合训练，从而对slot进行任务相关的调优。这样既维持了语义一致性，也有效对齐了感知与任务目标。

Result: 在对象发现基准测试和模拟操作任务中，STORM相较直接使用基础模型特征或端到端训练对象中心表征，提升了对视觉干扰的泛化能力和操控性能。

Conclusion: 多阶段适配机制能够高效地将通用基础模型特征转变为适用于机器人控制的任务感知对象中心表征，从而提升机器人的泛化和操作能力。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [151] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一套多机器人月球资源勘探KPI评估框架，解决以往不同机器人和实验难以横向比较的问题，为未来月球探索提供统一评价标准。


<details>
  <summary>Details</summary>
Motivation: 现有月球机器人勘探任务缺乏统一对齐科学目标与工程表现的评估标准，导致不同实验结果难以比较，影响系统性发展。

Method: 作者基于3个多机器人月球场景，提炼出符合科学目标和操作约束的KPI体系，并在实际多机器人野外测试中进行了框架验证。

Result: 该框架在效率和鲁棒性相关KPI方面应用便捷、实用；精度相关KPI因需真实地面基准数据，野外难以完全验证。

Conclusion: 所提KPI评估框架为多机器人外场试验提供了统一、面向目标的比较标准，有助于未来星球机器人系统的系统性开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [152] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 本文提出了一种通过振动声学感测实现高精度、全身接触定位的方法，通过仅七个低成本压电麦克风和音频频谱变换器，实现了低于5毫米的静态定位误差，并开源全部数据和模型。


<details>
  <summary>Details</summary>
Motivation: 传统机器人用触觉皮肤实现接触感知成本高且集成复杂，限制了大规模应用。

Method: 采用在机器人手部安装七个低价压电麦克风，通过音频频谱变换器(Audio Spectrogram Transformer)解析物理接触时产生的振动信号，以进行触点定位；对比不同材料/任务情境下的性能。

Result: 在静态环境下，系统定位误差小于5毫米。分析发现硬质材料适合脉冲定位，粗糙材料对轨迹追踪更佳。系统在机器人运动过程中也表现出良好的鲁棒性。

Conclusion: 简单振动信号可有效解码复杂接触动态，提出的方法为机器人感知提供了低成本、高适应性的解决方案，并开放全部资源以促进领域发展。

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [153] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: 本论文提出了MeCo框架，通过“缓存与重用”原理（memoization）提升多机器人协作的效率，显著降低了任务规划成本，并提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人协作方式依赖大量任务专用训练，导致适应新任务困难。近年来虽然引入大语言模型（LLM）增强协作灵活性，但每次面对类似任务时仍需从头规划，浪费大量计算资源。该问题在多机器人场景下尤为严重，因此亟需一种能够高效复用相似任务解决方案的方法。

Method: 作者提出MeCo框架，核心为基于相似性检测的“缓存与重用”方法。当遇到新任务时，系统可检索与之高度相关的已解决任务，并直接复用其解决方案，无需再次调用LLM。此外，还设计了MeCoBench基准评测系统，专门用于多机器人相似任务场景的性能测评。

Result: 实验证明，MeCo在多数场景下能大幅度减少规划成本，并且在协作成功率方面也优于现有先进方法。

Conclusion: 通过引入相似性检测与方案复用，MeCo有效提升了多机器人协作的通用性与效率，为多机器人系统的快速部署与适应多变任务提供了新思路。

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [154] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本文提出了一种用于腿式机器人强化学习（RL）新方法GPO，通过逐步扩展动作空间，提高了算法训练效率和性能，提升了机器人运动控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在腿式机器人高维动作与硬件限制下挑战大，且对基于力矩控制不够有效，亟需更泛化、训练稳定、无需大量人工启发的方法。

Method: 提出GPO训练框架，通过初期限制有效动作空间（用时变动作变换），收集更有效的数据和策略，然后逐步扩展动作空间来增强探索，实现PPO无失真地高效训练。

Result: 在四足和六足机器人上进行仿真与硬件实验证明，GPO训练出的策略在各种情况下均优于传统方法，实现了稳健零样本部署与更高性能。

Conclusion: GPO是一种通用、与环境无关的优化框架，能有效提升腿式机器人行走策略学习的表现和应用潜力。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [155] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 本论文提出了一种利用肌腱位移和张力数据，无需直接关节角传感器，实现类人机械手关节角度估算及手势闭环控制的方法。


<details>
  <summary>Details</summary>
Motivation: 由于在类人肌腱驱动机械手中集成关节编码器会影响机械紧凑性和灵巧性，因此需要通过其他方式感知关节状态。

Method: 首先构建了基于Denavit-Hartenberg参数的高效运动学模型。然后，采用简化的肌腱模型，推导肌腱状态与关节位姿之间的非线性方程，并通过非线性优化求解。最后，借助雅可比矩阵的PI控制器加前馈项，实现不依赖直接关节传感的手势跟踪闭环控制。

Result: 在MuJoCo仿真环境中，以具有每指5自由度、拇指6自由度的仿生机械手为测试对象，验证了所提关节角度估算与控制框架的有效性，并分析了其局限性。

Conclusion: 提出的基于肌腱状态的估算与控制方法能够实现离散手势的闭环跟踪，无需额外安装复杂的关节传感器，有助于类人机械手的小型化与灵巧性提升。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [156] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: 本文提出了一种全新的单步生成策略优化框架DMPO，实现了无需多步采样的实时机器人控制，兼具速度与表现，实验证明优于传统多步方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于diffusion和flow matching的生成式策略方法由于依赖多步采样，难以满足实时机器人的高频控制需求，这极大限制了其实际部署。

Method: DMPO框架结合了三大创新：1) MeanFlow，实现无需知识蒸馏的数学推导单步推理；2) dispersive正则项，避免表征塌缩；3) 强化学习微调，提升策略水平超越专家演示。还采用轻量级模型架构。

Result: 在RoboMimic操作和OpenAI Gym步态基准测试中，DMPO在控制精度和速度上都表现出与多步方法相当或更优的性能。在推理速度上大幅提升，达到5-20倍提升，实际控制频率高于120Hz。

Conclusion: DMPO有效结合理论创新与工程落地，实现了高效、实时的机器人控制，具备优越的实用性，并通过实际机器人实验证明其可落地性。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [157] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: 本文提出了一种弱监督融合感知与控制的显微操作机器人框架，实现了高精度（横向49微米、纵深291微米以内）的稳健操作，无需繁琐标定或人工标注。


<details>
  <summary>Details</summary>
Motivation: 传统自动化稳手机器人严重依赖人工密集的2D标注和复杂标定，不仅费时费力，也限制了实际应用推广。本文旨在解决这一瓶颈，通过弱监督方式简化感知与控制体系。

Method: 该方法利用可复用的预运行轨迹提取隐式空间信息，实现与标定相关的深度感知。通过对观测残差和标定模型残差建模，建立基于任务空间的误差预算，并结合自适应控制实现闭环高精度操作。

Result: 在最差情形下，横向闭环精度约49微米，纵深精度不超过291微米（均为95%置信区间）。使用者工作负荷（NASA-TLX）较基线降低77.1%。

Conclusion: 弱监督机器人系统显著提高了显微镜引导下的生物医学微操作可靠性，同时避免复杂的设置需求，具备广泛实用前景。

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [158] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 该论文提出了一种在ROS 2系统中实现知识图谱的方法，用于提升自主机器人任务的智能性与效率，并通过无人机搜救任务进行验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决自主机器人任务中对高效智能决策和任务执行的需求，作者希望利用知识图谱提升ROS 2系统的表现。

Method: 方法包括：定义初始和目标条件，结构化任务及子任务，规划任务顺序，将任务相关数据以知识图谱形式表示，并用高级语言设计任务。作者在Gazebo仿真环境下，通过Aerostack2框架和无人机搜救任务实现并验证了该方法。

Result: 实验证明，借助知识图谱，机器人在搜索和救援任务中的决策与执行能力得到了明显提升。

Conclusion: 提出的方法能够有效提升ROS 2系统下的机器人任务效率和智能水平，知识图谱为任务的智能规划和决策提供了有力支持。

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [159] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经风格迁移和变分自编码器的新方法，实现了强化学习策略从仿真到现实的迁移，显著减少了实际样本需求，并在机器人切割未知材料等高难度任务中取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖大量仿真数据，但仿真与现实存在差距，实际数据难以获得，阻碍其在实际机器人任务中的应用。需要发展数据高效、能适应现实复杂环境的方法。

Method: 作者将神经风格迁移理念引入到强化学习数据合成，通过变分自编码器学习自监督特征，实现无配对、无标签的现实数据风格迁移，合成更符合物理真实的训练轨迹，并建立弱配对的源-目标轨迹用于策略迁移。

Result: 在机器人切割未知材料的案例中，相较于基线方法（包括CycleGAN、条件VAE等），新方法以极少的现实数据实现了更高的任务完成速度和行为稳定性，并且对环境的几何和材料变化展现了较强的鲁棒性。

Conclusion: 该方法有效推动了强化学习策略在缺乏现实奖励、接触复杂性强任务中的迁移与适应，为实际机器人强化学习落地提供了更现实可行的解决方案。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
