<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Fourier-Based GAN Fingerprint Detection using ResNet50](https://arxiv.org/abs/2510.19840)
*Sai Teja Erukude,Viswa Chaitanya Marella,Suhasnadh Reddy Veluru*

Main category: cs.CV

TL;DR: 本文提出通过频域分析结合深度学习，有效区分StyleGAN生成的图像与真实图像。


<details>
  <summary>Details</summary>
Motivation: 随着GAN生成逼真图像能力不断提升，内容真实性鉴别面临严峻挑战，尤其对数字取证和工业系统更具现实需求。

Method: 采用二维离散傅立叶变换（2D DFT）将图像转为傅立叶域，利用ResNet50神经网络在频域图像上进行训练和区分。

Result: 所提频域模型准确率达到92.8%，AUC为0.95，显著优于空间域原图训练的模型。

Conclusion: GAN图像具有独特的频域指纹特征，频域+深度学习方法对数字取证、工业AI系统可信度具有重要应用价值。

Abstract: The rapid rise of photorealistic images produced from Generative Adversarial
Networks (GANs) poses a serious challenge for image forensics and industrial
systems requiring reliable content authenticity. This paper uses
frequency-domain analysis combined with deep learning to solve the problem of
distinguishing StyleGAN-generated images from real ones. Specifically, a
two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform
images into the Fourier domain, where subtle periodic artifacts become
detectable. A ResNet50 neural network is trained on these transformed images to
differentiate between real and synthetic ones. The experiments demonstrate that
the frequency-domain model achieves a 92.8 percent and an AUC of 0.95,
significantly outperforming the equivalent model trained on raw spatial-domain
images. These results indicate that the GAN-generated images have unique
frequency-domain signatures or "fingerprints". The method proposed highlights
the industrial potential of combining signal processing techniques and deep
learning to enhance digital forensics and strengthen the trustworthiness of
industrial AI systems.

</details>


### [2] [Transformed Multi-view 3D Shape Features with Contrastive Learning](https://arxiv.org/abs/2510.19955)
*Márcus Vinícius Lobo Costa,Sherlon Almeida da Silva,Bárbara Caroline Benato,Leo Sampaio Ferraz Ribeiro,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 该论文探讨了结合Vision Transformer（ViT）和对比学习目标进行3D形状特征表征学习的方法，在多视图3D分析任务中显著提升了表现，尤其是在有限标签数据场景下优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 现有的3D对象识别方法多依赖于CNN及大量标注数据，且容易忽视重要的形状关系。为提升3D表示学习能力并减少对标注数据依赖，作者尝试引入对比学习和ViT结构。

Method: 将ViT架构与现代的对比学习（包括有监督与自监督）目标结合，在多视图3D对象识别任务上进行实验比较，通过对比损失优化全局及局部特征。

Result: 在ModelNet10数据集上，监督式对比损失取得了约90.6%的识别准确率。该组合在捕捉全局形状语义和区分局部特征方面展现出优越性，显著超过CNN。

Conclusion: ViT与对比学习的结合有效提升了3D形状表征能力，克服了以往方法对标签数量和局部关系建模的局限，实验结果验证了该方案的实用性和先进性。

Abstract: This paper addresses the challenges in representation learning of 3D shape
features by investigating state-of-the-art backbones paired with both
contrastive supervised and self-supervised learning objectives. Computer vision
methods struggle with recognizing 3D objects from 2D images, often requiring
extensive labeled data and relying on Convolutional Neural Networks (CNNs) that
may overlook crucial shape relationships. Our work demonstrates that Vision
Transformers (ViTs) based architectures, when paired with modern contrastive
objectives, achieve promising results in multi-view 3D analysis on our
downstream tasks, unifying contrastive and 3D shape understanding pipelines.
For example, supervised contrastive losses reached about 90.6% accuracy on
ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability
to understand overall shapes and contrastive learning's effectiveness,
overcomes the need for extensive labeled data and the limitations of CNNs in
capturing crucial shape relationships. The success stems from capturing global
shape semantics via ViTs and refining local discriminative features through
contrastive optimization. Importantly, our approach is empirical, as it is
grounded on extensive experimental evaluation to validate the effectiveness of
combining ViTs with contrastive objectives for 3D representation learning.

</details>


### [3] [FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking](https://arxiv.org/abs/2510.19981)
*Martha Teiko Teye,Ori Maoz,Matthias Rottmann*

Main category: cs.CV

TL;DR: 本文提出了FutrTrack，一种结合摄像头与激光雷达的多目标跟踪新框架，通过Transformer实现平滑与特征融合，无需显式运动模型，在nuScenes和KITTI基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪方法多采用单一传感器，难以处理视角变化和遮挡等挑战；同时Transformer在多模态融合与序列建模中的优势尚未被充分利用。

Method: FutrTrack将现有3D目标检测与Transformer平滑器和融合驱动的跟踪器结合，采用两阶段Transformer管线。Tracker将来自摄像头与激光雷达的BEV特征与3D框融合，无需显式运动模型，利用几何和语义信息实现更稳健的身份分配和传播，并在跟踪前用时序平滑器优化轨迹。

Result: 在nuScenes和KITTI数据集上评测，FutrTrack比单传感器方法明显提升，aMOTA在nuScenes测试集达到74.7，减少了身份切换（ID Switch），兼顾准确性和高效性。

Conclusion: FutrTrack验证了Transformer追踪方法显著受益于多模态融合，能在有限数据和无预训练下获得与先进神经网络方法竞争的性能，有效减少身份切换，提升3D多目标跟踪精度和稳健性。

Abstract: We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework
that builds on existing 3D detectors by introducing a transformer-based
smoother and a fusion-driven tracker. Inspired by query-based tracking
frameworks, FutrTrack employs a multimodal two-stage transformer refinement and
tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal
bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without
the need for an explicit motion model. The tracker assigns and propagates
identities across frames, leveraging both geometric and semantic cues for
robust re-identification under occlusion and viewpoint changes. Prior to
tracking, we refine sequences of bounding boxes with a temporal smoother over a
moving window to refine trajectories, reduce jitter, and improve spatial
consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that
query-based transformer tracking methods benefit significantly from multimodal
sensor features compared with previous single-sensor approaches. With an aMOTA
of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D
MOT benchmarks, reducing identity switches while maintaining competitive
accuracy. Our approach provides an efficient framework for improving
transformer-based trackers to compete with other neural-network-based methods
even with limited data and without pretraining.

</details>


### [4] [Improving Predictive Confidence in Medical Imaging via Online Label Smoothing](https://arxiv.org/abs/2510.20011)
*Kushan Choudhury,Shubhrodeep Roy,Ankur Chanda,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: 本文提出并验证了一种新的标签平滑方法（OLS），其在医学影像分类任务中能提升模型准确率及可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分类上表现出色，但常常过度自信，影响在医疗环境中的可靠性。传统的标签平滑方法未考虑类别间关系，因此效果有限。本文旨在提出更有效的动态标签平滑策略，提升模型在医疗领域的可靠性和泛化能力。

Method: 作者提出了在线标签平滑（Online Label Smoothing, OLS）方法，根据模型自身的预测动态调整训练过程中的软标签。将OLS分别应用在三种流行架构（ResNet-50、MobileNetV2、VGG-19）上，在RadImageNet大规模医学影像数据集上与硬标签、传统标签平滑、无教师知识蒸馏方法进行对比实验。

Result: 实验表明，OLS方法相比于标准训练、传统标签平滑和无教师知识蒸馏，能够持续提升Top-1和Top-5分类准确率。同时，OLS还能产生更紧凑且分离良好的特征表示，表明其增强了表示学习能力。

Conclusion: OLS不仅提升了分类准确率，还显著改善了模型的校准能力，增强了模型在医学影像领域的可信性，成为开发值得信赖AI医疗影像系统的切实可行方法。

Abstract: Deep learning models, especially convolutional neural networks, have achieved
impressive results in medical image classification. However, these models often
produce overconfident predictions, which can undermine their reliability in
critical healthcare settings. While traditional label smoothing offers a simple
way to reduce such overconfidence, it fails to consider relationships between
classes by treating all non-target classes equally. In this study, we explore
the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft
labels throughout training based on the model's own prediction patterns. We
evaluate OLS on the large-scale RadImageNet dataset using three widely used
architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS
consistently improves both Top-1 and Top-5 classification accuracy compared to
standard training methods, including hard labels, conventional label smoothing,
and teacher-free knowledge distillation. In addition to accuracy gains, OLS
leads to more compact and well-separated feature embeddings, indicating
improved representation learning. These findings suggest that OLS not only
strengthens predictive performance but also enhances calibration, making it a
practical and effective solution for developing trustworthy AI systems in the
medical imaging domain.

</details>


### [5] [A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance](https://arxiv.org/abs/2510.20016)
*Neema Jakisa Owor,Joshua Kofi Asamoah,Tanner Wambui Muturi,Anneliese Jakisa Owor,Blessing Agyei Kyem,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一种专为鱼眼相机图像设计的目标检测框架，通过预处理与后处理流程解决鱼眼图像畸变带来的检测难题，并通过多模型集成提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 鱼眼相机虽可覆盖大视角场景，但强烈的径向畸变及分辨率不均使得传统目标检测方法效果较差，尤其是影像边界区域。解决在鱼眼监控图像上的检测准确性问题具有重要应用价值。

Method: 提出一套简洁有效的预处理及后处理流程，用以提升检测在严重畸变区域的鲁棒性。将多种最新的检测网络训练于鱼眼交通图像，并采用集成方法融合检测结果以进一步提高检测效果。

Result: 方法在2025 AI City Challenge Track 4赛道取得了F1分数0.6366，居62支队伍中第8位，显示出优良的检测性能。

Conclusion: 本文方法有效针对鱼眼图像的传统检测难题提升了检测效果，证明了其在真实交通鱼眼监控场景下的应用前景。

Abstract: Fisheye cameras offer an efficient solution for wide-area traffic
surveillance by capturing large fields of view from a single vantage point.
However, the strong radial distortion and nonuniform resolution inherent in
fisheye imagery introduce substantial challenges for standard object detectors,
particularly near image boundaries where object appearance is severely
degraded. In this work, we present a detection framework designed to operate
robustly under these conditions. Our approach employs a simple yet effective
pre and post processing pipeline that enhances detection consistency across the
image, especially in regions affected by severe distortion. We train several
state-of-the-art detection models on the fisheye traffic imagery and combine
their outputs through an ensemble strategy to improve overall detection
accuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City
Challenge Track 4, placing 8thoverall out of 62 teams. These results
demonstrate the effectiveness of our framework in addressing issues inherent to
fisheye imagery.

</details>


### [6] [Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses](https://arxiv.org/abs/2510.20027)
*Damian Bowness,Charalambos Poullis*

Main category: cs.CV

TL;DR: 本论文提出了一种新型实时感知渲染过滤方法，有效提升3D Gaussian Splatting模型在新颖视角下的视觉效果。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）模型在相机从训练样本分布外的新位置观察时，因训练数据稀缺而出现严重视觉噪点和伪影，导致重建质量下降，因此需克服训练数据不足带来的生成不确定性问题。

Method: 作者提出基于中间梯度导出的灵敏度分数，设计了一种面向渲染、实时、高效过滤器；针对各向异性方向导致的不稳定性进行显式过滤，而非仅处理各向同性方差，直接过滤异常密度、颜色和几何预测，以保持视效稳定性。该方法无需重新训练，可直接集成至现有3DGS渲染流程。

Result: 实验表明，该方法在视觉质量、真实感和一致性方面，显著优于现有的NeRF类方法（如BayesRays），并且具备实时性和易集成性。

Conclusion: 该过滤方法为3DGS模型在训练样本分布外的自由导航应用提供了有效解决方案，显著提升了3D重建的视觉效果且兼容性好，有潜力成为3D 视觉重建的重要组件。

Abstract: When viewing a 3D Gaussian Splatting (3DGS) model from camera positions
significantly outside the training data distribution, substantial visual noise
commonly occurs. These artifacts result from the lack of training data in these
extrapolated regions, leading to uncertain density, color, and geometry
predictions from the model.
  To address this issue, we propose a novel real-time render-aware filtering
method. Our approach leverages sensitivity scores derived from intermediate
gradients, explicitly targeting instabilities caused by anisotropic
orientations rather than isotropic variance. This filtering method directly
addresses the core issue of generative uncertainty, allowing 3D reconstruction
systems to maintain high visual fidelity even when users freely navigate
outside the original training viewpoints.
  Experimental evaluation demonstrates that our method substantially improves
visual quality, realism, and consistency compared to existing Neural Radiance
Field (NeRF)-based approaches such as BayesRays. Critically, our filter
seamlessly integrates into existing 3DGS rendering pipelines in real-time,
unlike methods that require extensive post-hoc retraining or fine-tuning.
  Code and results at https://damian-bowness.github.io/EV3DGS

</details>


### [7] [BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography](https://arxiv.org/abs/2510.20029)
*Shengyu Chen,Shihang Feng,Yi Luo,Xiaowei Jia,Youzuo Lin*

Main category: cs.CV

TL;DR: 本文提出了BrainPuzzle框架，通过结合物理建模与机器学习，有效提升了超声脑成像中横越颅骨的速度成像的定量准确性。


<details>
  <summary>Details</summary>
Motivation: 超声脑成像因颅骨与脑组织间的声速差异及探头耦合困难，定量成像极具挑战，现有物理或数据驱动方法难以兼顾信号完整性和定量准确性。

Method: 提出BrainPuzzle两阶段混合框架，第一阶段利用反向时迁法获得结构清晰的图像片段，第二阶段以基于Transformer的超分辨率编解码网络叠加图注意力机制融合片段，此外采用可移动部分阵列探头提高实用性并补偿孔径不足。

Result: 在两个合成数据集上，BrainPuzzle在速度成像准确性与完整性上均优于现有方法。

Conclusion: BrainPuzzle展示了其在提升超声脑成像定量准确性与成像完整性方面的潜力，为临床定量超声脑成像带来了新进展。

Abstract: Ultrasound brain imaging remains challenging due to the large difference in
sound speed between the skull and brain tissues and the difficulty of coupling
large probes to the skull. This work aims to achieve quantitative transcranial
ultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain.
Traditional physics-based full-waveform inversion (FWI) is limited by weak
signals caused by skull-induced attenuation, mode conversion, and phase
aberration, as well as incomplete spatial coverage since full-aperture arrays
are clinically impractical. In contrast, purely data-driven methods that learn
directly from raw ultrasound data often fail to model the complex nonlinear and
nonlocal wave propagation through bone, leading to anatomically plausible but
quantitatively biased SoS maps under low signal-to-noise and sparse-aperture
conditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage
framework that combines physical modeling with machine learning. In the first
stage, reverse time migration (time-reversal acoustics) is applied to
multi-angle acquisitions to produce migration fragments that preserve
structural details even under low SNR. In the second stage, a transformer-based
super-resolution encoder-decoder with a graph-based attention unit (GAU) fuses
these fragments into a coherent and quantitatively accurate SoS image. A
partial-array acquisition strategy using a movable low-count transducer set
improves feasibility and coupling, while the hybrid algorithm compensates for
the missing aperture. Experiments on two synthetic datasets show that
BrainPuzzle achieves superior SoS reconstruction accuracy and image
completeness, demonstrating its potential for advancing quantitative ultrasound
brain imaging.

</details>


### [8] [Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models](https://arxiv.org/abs/2510.20042)
*Huichan Seo,Sieun Choi,Minki Hong,Yi Zhou,Junseo Kim,Lukman Ismaila,Naome Etori,Mehul Agarwal,Zhixuan Liu,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 本文提出并验证了一套统一的评估框架，可系统地衡量生成式图像模型（包括文本到图像T2I与图像到图像I2I）中的文化偏差，发现主流模型在文化表达方面存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注T2I系统中的文化偏见，I2I编辑领域尚未得到充分研究，缺乏跨国家、时代和精细类别的统一评估标准。

Method: 作者设计了包含六国、8大类36子类、时代敏感提示的测试方案，使用开源模型和固定设置进行生成与编辑，结合自动指标、具有文化意识的VQA和母语专家人工评价，并公开全部测试资源以保证可复现性。

Result: (1) 在与国家无关的提示下，各模型更倾向于“全球北方”和现代风格，忽略跨国差别；(2) I2I多轮编辑虽评分提升，却削弱了文化真实度；(3) I2I主要通过色彩、道具等表层手法伪装文化元素，难以实现深层次、时代一致的变换，且经常保留原始（多为全球南方）特征。

Conclusion: 当前主流生成式图像模型在文化敏感性编辑上的表现不可靠。作者发布了标准化数据、提示与人工评价流程，为后续评测和改进相关系统提供可复现的文化偏见诊断基准。

Abstract: Generative image models produce striking visuals yet often misrepresent
culture. Prior work has examined cultural bias mainly in text-to-image (T2I)
systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap
with a unified evaluation across six countries, an 8-category/36-subcategory
schema, and era-aware prompts, auditing both T2I generation and I2I editing
under a standardized protocol that yields comparable diagnostics. Using open
models with fixed settings, we derive cross-country, cross-era, and
cross-category evaluations. Our framework combines standard automatic metrics,
a culture-aware retrieval-augmented VQA, and expert human judgments collected
from native reviewers. To enable reproducibility, we release the complete image
corpus, prompts, and configurations. Our study reveals three findings: (1)
under country-agnostic prompts, models default to Global-North, modern-leaning
depictions that flatten cross-country distinctions; (2) iterative I2I editing
erodes cultural fidelity even when conventional metrics remain flat or improve;
and (3) I2I models apply superficial cues (palette shifts, generic props)
rather than era-consistent, context-aware changes, often retaining source
identity for Global-South targets. These results highlight that
culture-sensitive edits remain unreliable in current systems. By releasing
standardized data, prompts, and human evaluation protocols, we provide a
reproducible, culture-centered benchmark for diagnosing and tracking cultural
bias in generative image models.

</details>


### [9] [Filter-Based Reconstruction of Images from Events](https://arxiv.org/abs/2510.20071)
*Bernd Pfrommer*

Main category: cs.CV

TL;DR: 本文提出了一种基于滤波的异步重建方法（FIBAR），用以从事件相机的事件流中重建强度图像，强调方法的简洁、异步、低算力需求，并在质检实验中展示效果。


<details>
  <summary>Details</summary>
Motivation: 现有用神经网络重建事件相机强度图的方法复杂、算力消耗大，难以实时在低功耗设备或CPU上运行，因此作者希望提出一种更简单、轻量、高效、异步的重建方法。

Method: 方法首先利用IIR滤波器对事件信息进行时间积累，随后提出一种新颖算法识别像素点的“陈旧”状态并动态调节更新窗口，然后采用高斯滤波对陈旧像素模糊处理。全流程异步，允许任意时间读取图像输出，并实现在现代笔记本CPU上可实时运行。

Result: FIBAR在开启空间滤波时可达约4200万事件/秒，关闭时达1.4亿事件/秒。实验对比FireNet（神经网络方法）显示：FIBAR重建图像噪点较多，存在重影，但在一些任务（如标记检测）已足够用。

Conclusion: FIBAR方法在速度、简洁性和资源消耗方面具有明显优势，尤其适合对精度要求不高但需实时或低算力场景。代码已开源，有望为实际应用带来便利。

Abstract: Reconstructing an intensity image from the events of a moving event camera is
a challenging task that is typically approached with neural networks deployed
on graphics processing units. This paper presents a much simpler, FIlter Based
Asynchronous Reconstruction method (FIBAR). First, intensity changes signaled
by events are integrated with a temporal digital IIR filter. To reduce
reconstruction noise, stale pixels are detected by a novel algorithm that
regulates a window of recently updated pixels. Arguing that for a moving
camera, the absence of events at a pixel location likely implies a low image
gradient, stale pixels are then blurred with a Gaussian filter. In contrast to
most existing methods, FIBAR is asynchronous and permits image read-out at an
arbitrary time. It runs on a modern laptop CPU at about 42(140) million
events/s with (without) spatial filtering enabled. A few simple qualitative
experiments are presented that show the difference in image reconstruction
between FIBAR and a neural network-based approach (FireNet). FIBAR's
reconstruction is noisier than neural network-based methods and suffers from
ghost images. However, it is sufficient for certain tasks such as the detection
of fiducial markers. Code is available at
https://github.com/ros-event-camera/event_image_reconstruction_fibar

</details>


### [10] [Data-Adaptive Transformed Bilateral Tensor Low-Rank Representation for Clustering](https://arxiv.org/abs/2510.20077)
*Hui Chen,Xinjie Wang,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的变换双向张量低秩表示（TBTLRR）方法，用于提升图像聚类的效果，尤其在对噪声数据的鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有张量低秩表示方法通常依赖固定变换，面对现实应用中的噪声时鲁棒性较差，限制了其在复杂图像聚类中的实际应用。作者希望解决变换适应性与噪声鲁棒性的问题。

Method: 提出了一种学习任意酉变换的数据自适应张量核范数，并结合张量的双向结构，同时整合了ℓ_{1/2}范数及Frobenius范数作为正则项，用于更好地处理复杂噪声。为求解该非凸模型，作者设计了基于ADMM的高效优化算法，并给出收敛性理论证明。

Result: 通过大量实验，TBTLRR在图像聚类任务中相较于最新方法表现出更优的聚类性能，代码已开源。

Conclusion: TBTLRR方法有效地提升了对噪声的鲁棒性和聚类准确率，为图像聚类任务提供了新的有力工具。

Abstract: Tensor low-rank representation (TLRR) has demonstrated significant success in
image clustering. However, most existing methods rely on fixed transformations
and suffer from poor robustness to noise. In this paper, we propose a novel
transformed bilateral tensor low-rank representation model called TBTLRR, which
introduces a data-adaptive tensor nuclear norm by learning arbitrary unitary
transforms, allowing for more effective capture of global correlations. In
addition, by leveraging the bilateral structure of latent tensor data, TBTLRR
is able to exploit local correlations between image samples and features.
Furthermore, TBTLRR integrates the $\ell_{1/2}$-norm and Frobenius norm
regularization terms for better dealing with complex noise in real-world
scenarios. To solve the proposed nonconvex model, we develop an efficient
optimization algorithm inspired by the alternating direction method of
multipliers (ADMM) and provide theoretical convergence. Extensive experiments
validate its superiority over the state-of-the-art methods in clustering. The
code will be available at https://github.com/xianchaoxiu/TBTLRR.

</details>


### [11] [Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos](https://arxiv.org/abs/2510.20087)
*Lorenzo Arboit,Dennis N. Schneider,Britty Baby,Vinkle Srivastav,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本论文提出了一款名为Endoshare的软件，用于融合、标准化和去标识化微创外科内窥镜视频，提升手术数据的管理和隐私保护，同时促进相关研究发展。


<details>
  <summary>Details</summary>
Motivation: 随着手术数据科学的发展，基于视频的手术评估备受关注，但当前推广受限于视频格式不统一和隐私风险。该论文旨在解决这些障碍，促进外科数据的标准化与隐私合规共享。

Method: 作者采用软件开发生命周期及以用户为中心的迭代方法，通过内部和外部调查收集临床医生与计算机科学家的可用性反馈，围绕隐私优先设计软件架构，并结合不同硬件配置对处理性能进行基准测试。

Result: 初步测试中，软件在可用性上得分较高，通过多轮优化后，用户（外科医生）评价其有较强的实用性、易用性和推荐意愿。处理效率受到处理方式、视频时长和硬件性能影响。

Conclusion: Endoshare为外科领域的视频管理提供了透明便捷、注重隐私的高效流程，有望成为专有系统的可行替代方案，但仍需进一步合规认证和跨平台兼容性验证。

Abstract: Video-based assessment and surgical data science can advance surgical
training, research, and quality improvement. However, widespread use remains
limited by heterogeneous recording formats and privacy concerns associated with
video sharing. We present Endoshare, a source-available, cross-platform
application for merging, standardizing, and de-identifying endoscopic videos in
minimally invasive surgery. Development followed the software development life
cycle with iterative, user-centered feedback. During the analysis phase, an
internal survey of clinicians and computer scientists based on ten usability
heuristics identified key requirements that guided a privacy-by-design
architecture. In the testing phase, an external clinician survey combined the
same heuristics with Technology Acceptance Model constructs to assess usability
and adoption, complemented by benchmarking across different hardware
configurations. Four clinicians and four computer scientists initially tested
the prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5),
with the lowest score (4.00 +/- 0.93/5) relating to label clarity. After
refinement, the testing phase surveyed ten surgeons who reported high perceived
usefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic
usability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10).
Processing time varied with processing mode, video duration (both p <= 0.001),
and machine computational power (p = 0.041). Endoshare provides a transparent,
user-friendly pipeline for standardized, privacy-preserving surgical video
management. Compliance certification and broader interoperability validation
are needed to establish it as a deployable alternative to proprietary systems.
The software is available at https://camma-public.github.io/Endoshare/

</details>


### [12] [Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency](https://arxiv.org/abs/2510.20092)
*Hao Yu,Haoyu Chen,Yan Jiang,Wei Peng,Zhaodong Sun,Samuel Kaski,Guoying Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新型卷积操作——Attentive Convolution（ATConv），结合了自注意力机制的核心优势，使卷积网络在视觉任务上性能超越标准自注意力机制，同时保持较低运算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力（SA）虽然表达能力强，但计算复杂度高，限制了其实际应用。而传统卷积（Conv）虽计算高效且有视觉先验，但与自注意力还有明显性能差距。因此，作者重新思考了卷积设计，探索使卷积具备自注意力核心优势的方法。

Method: 作者分析比较了自注意力和卷积的本质差异，并提出两项关键机制：自适应路由（Adaptive routing）与侧向抑制（Lateral inhibition）。在此基础上，设计了一种具备这些机制的新型卷积算子（ATConv），并构建了基于其的网络AttNet。

Result: ATConv在仅用3×3卷积核时，在多项基础视觉任务上优于多种自注意力机制。构建的AttNet在ImageNet-1K上达到84.4%的Top-1准确率，参数量仅为27M。在扩散式图像生成任务中，ATConv替换自注意力后，FID更低，推理速度更快。

Conclusion: ATConv通过引入自注意力的关键机制，缩小或超越了与自注意力网络之间的性能差距，为卷积网络的复兴与优化提供了有效新途径。

Abstract: Self-attention (SA) has become the cornerstone of modern vision backbones for
its powerful expressivity over traditional Convolutions (Conv). However, its
quadratic complexity remains a critical bottleneck for practical applications.
Given that Conv offers linear complexity and strong visual priors, continuing
efforts have been made to promote the renaissance of Conv. However, a
persistent performance chasm remains, highlighting that these modernizations
have not yet captured the intrinsic expressivity that defines SA. In this
paper, we re-examine the design of the CNNs, directed by a key question: what
principles give SA its edge over Conv? As a result, we reveal two fundamental
insights that challenge the long-standing design intuitions in prior research
(e.g., Receptive field). The two findings are: (1) \textit{Adaptive routing}:
SA dynamically regulates positional information flow according to semantic
content, whereas Conv employs static kernels uniformly across all positions.
(2) \textit{Lateral inhibition}: SA induces score competition among token
weighting, effectively suppressing redundancy and sharpening representations,
whereas Conv filters lack such inhibitory dynamics and exhibit considerable
redundancy. Based on this, we propose \textit{Attentive Convolution} (ATConv),
a principled reformulation of the convolutional operator that intrinsically
injects these principles. Interestingly, with only $3\times3$ kernels, ATConv
consistently outperforms various SA mechanisms in fundamental vision tasks.
Building on ATConv, we introduce AttNet, a CNN family that can attain
\textbf{84.4\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In
diffusion-based image generation, replacing all SA with the proposed $3\times
3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster
sampling. Code is available at: github.com/price112/Attentive-Convolution.

</details>


### [13] [StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback](https://arxiv.org/abs/2510.20093)
*Jiho Park,Sieun Choi,Jaeyoon Seo,Jihie Kim*

Main category: cs.CV

TL;DR: 提出StableSketcher框架，提升扩散模型手绘素描生成质量，并发布SketchDUO数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成高质量图像方面取得进展，但生成具较强抽象表达的手绘素描仍有难度，现有数据集的配对信息有限。

Method: 1. 对变分自编码器（VAE）进行微调，以更好捕捉素描特征。2. 引入基于视觉问答的新奖励函数，通过强化学习增强文本与图像的语义一致性。3. 新建SketchDUO数据集，含实例级素描、文字描述和问答对。

Result: StableSketcher框架生成的素描在风格保真和文本一致性方面优于Stable Diffusion基线；新数据集提供更丰富的训练和评测支持。

Conclusion: StableSketcher有效提升扩散模型生成手绘素描的能力，推动文本到素描生成发展，SketchDUO数据集填补领域空白。

Abstract: Although recent advancements in diffusion models have significantly enriched
the quality of generated images, challenges remain in synthesizing pixel-based
human-drawn sketches, a representative example of abstract expression. To
combat these challenges, we propose StableSketcher, a novel framework that
empowers diffusion models to generate hand-drawn sketches with high prompt
fidelity. Within this framework, we fine-tune the variational autoencoder to
optimize latent decoding, enabling it to better capture the characteristics of
sketches. In parallel, we integrate a new reward function for reinforcement
learning based on visual question answering, which improves text-image
alignment and semantic consistency. Extensive experiments demonstrate that
StableSketcher generates sketches with improved stylistic fidelity, achieving
better alignment with prompts compared to the Stable Diffusion baseline.
Additionally, we introduce SketchDUO, to the best of our knowledge, the first
dataset comprising instance-level sketches paired with captions and
question-answer pairs, thereby addressing the limitations of existing datasets
that rely on image-label pairs. Our code and dataset will be made publicly
available upon acceptance.

</details>


### [14] [BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models](https://arxiv.org/abs/2510.20095)
*Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 本文提出将描述性标题作为新的监督信号，提升生物多模态基础模型（BIOCAP）在物种分类和图文检索任务中的表现。通过利用多模态大语言模型合成生成实例级生物描述，提高了模型对物种语义的理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前生物多模态基础模型训练主要依赖标签信息，但缺乏细粒度、实例级的自然语言描述，限制了模型对丰富生物特征的捕获。自然语言监督因获得高质量、实例专属的caption难度大，导致其在生物学领域应用受限。

Method: 作者利用多模态大语言模型，结合来自Wikipedia的视觉信息和针对特定物种的格式范例，自动批量生成实例级生物caption，填补真实caption不足的空白。利用这些合成caption与图像共同训练BIOCAP（BIOCLIP with Captions）模型，实现更高语义对齐。

Result: BIOCAP模型通过引入描述性caption，获得了丰富的语义表示能力，在物种分类和图文检索等任务中实现了强劲的性能。

Conclusion: 描述性caption不仅能作为简单标签使用，还能作为桥梁，促进生物图像与多模态基础模型的深度融合和理解，为生物信息学模型带来了重要提升。

Abstract: This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.

</details>


### [15] [Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects](https://arxiv.org/abs/2510.20126)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony S. Maida,Alan B. Barhorst,Vijaya Gopu*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习与物理模型的新系统，实现了对快速运动小目标的准确3D检测和跟踪，并显著优于传统kalman滤波器。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉在一般物体检测和跟踪方面已经取得了显著进展，但对于快速移动的微小目标的检测和跟踪仍是一个挑战，尤其在存在遮挡、方向急剧变化等复杂场景中。

Method: 设计了一个综合系统，使用深度学习进行目标检测，并融合基于物理学的跟踪算法，结合运动学方程处理检测遗漏和异常值，外加一个异常检测与修正模块以提升跟踪的鲁棒性。

Result: 在自建的racquetball快速小球数据集上评估，系统表现优于基于Kalman滤波的传统方法，平均位移误差降低高达70%。

Conclusion: 结合物理建模与深度学习方法，能大幅提升机器人自主平台在快速小目标感知和实时3D检测跟踪任务中的表现，对相关实际应用具有重要意义。

Abstract: While computer vision has advanced considerably for general object detection
and tracking, the specific problem of fast-moving tiny objects remains
underexplored. This paper addresses the significant challenge of detecting and
tracking rapidly moving small objects using an RGB-D camera. Our novel system
combines deep learning-based detection with physics-based tracking to overcome
the limitations of existing approaches. Our contributions include: (1) a
comprehensive system design for object detection and tracking of fast-moving
small objects in 3D space, (2) an innovative physics-based tracking algorithm
that integrates kinematics motion equations to handle outliers and missed
detections, and (3) an outlier detection and correction module that
significantly improves tracking performance in challenging scenarios such as
occlusions and rapid direction changes. We evaluated our proposed system on a
custom racquetball dataset. Our evaluation shows our system surpassing kalman
filter based trackers with up to 70\% less Average Displacement Error. Our
system has significant applications for improving robot perception on
autonomous platforms and demonstrates the effectiveness of combining
physics-based models with deep learning approaches for real-time 3D detection
and tracking of challenging small objects.

</details>


### [16] [Inverse Image-Based Rendering for Light Field Generation from Single Images](https://arxiv.org/abs/2510.20132)
*Hyunjun Jung,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 本文提出了一种仅用单张图片就能生成光场的新颖方法，通过神经渲染管道实现高质量新视角合成，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统光场的获取需要高成本的计算或专门设备，限制了其在真实场景与普通用户中的应用。作者希望让普通2D图片也能享受光场应用与效果。

Method: 提出了一种逆向基于图像的渲染方法：通过神经网络流水线，先存储输入图片的光流，再用交叉注意力机制推断视角变化后的光线颜色。生成新视角后，将合成内容更新为下轮输入，实现逐步完善和遮挡一致性。

Result: 该方法在经过合成数据集训练一次后，可直接在多个复杂真实数据集上有效适用且无需再训练，也优于当前主流的新视角合成技术。

Conclusion: 本文的方法提升了单张图片合成光场的可行性和实用性，降低了门槛，并在多个任务上取得更优性能，有望拓宽光场应用领域。

Abstract: A concept of light-fields computed from multiple view images on regular grids
has proven its benefit for scene representations, and supported realistic
renderings of novel views and photographic effects such as refocusing and
shallow depth of field. In spite of its effectiveness of light flow
computations, obtaining light fields requires either computational costs or
specialized devices like a bulky camera setup and a specialized microlens
array. In an effort to broaden its benefit and applicability, in this paper, we
propose a novel view synthesis method for light field generation from only
single images, named inverse image-based rendering. Unlike previous attempts to
implicitly rebuild 3D geometry or to explicitly represent objective scenes, our
method reconstructs light flows in a space from image pixels, which behaves in
the opposite way to image-based rendering. To accomplish this, we design a
neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our
neural renderer first stores the light flow of source rays from the input
image, then computes the relationships among them through cross-attention, and
finally predicts the color of the target ray based on these relationships.
After the rendering pipeline generates the first novel view from a single input
image, the generated out-of-view contents are updated to the set of source
rays. This procedure is iteratively performed while ensuring the consistent
generation of occluded contents. We demonstrate that our inverse image-based
rendering works well with various challenging datasets without any retraining
or finetuning after once trained on synthetic dataset, and outperforms relevant
state-of-the-art novel view synthesis methods.

</details>


### [17] [Revisiting Logit Distributions for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2510.20134)
*Jiachen Liang,Ruibing Hou,Minyang Hu,Hong Chang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的后处理（post-hoc）异常检测方法LogitGap，通过分析神经网络输出中的logit最大值与其他logit之间的关系，有效区分分布内（ID）与分布外（OOD）样本。作者还提出了无需额外训练的自动logit筛选策略，并在视觉语言和纯视觉模型上，通过大量实验证明了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 在开放世界应用中，深度学习模型常因无法有效辨别异常分布样本而导致性能下降。现有高效的后处理手段利用logits信息不充分，本文旨在充分挖掘logits空间的信息，提升OOD检测的准确性。

Method: 该方法提出LogitGap，利用最大logit值与其余logit之间的差距作为分数判别OOD。同时引入训练无关的自动logit子集筛选机制，精炼logit空间，提升分数可分性。作者提供理论分析和实验证明改进策略。

Result: LogitGap在多种视觉-语言与视觉模型不同的OOD检测任务与基准上表现优异，均达到了最先进水平，验证了该方法的有效性和泛化能力。

Conclusion: 通过显式利用logits最大值与其他值间的关系，并结合无需训练的信息筛选策略，LogitGap方法可简单高效地提升OOD检测性能，为后处理方法提供了新的思路。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the reliability
of deep learning models in open-world applications. While post-hoc methods are
favored for their efficiency and ease of deployment, existing approaches often
underexploit the rich information embedded in the model's logits space. In this
paper, we propose LogitGap, a novel post-hoc OOD detection method that
explicitly exploits the relationship between the maximum logit and the
remaining logits to enhance the separability between in-distribution (ID) and
OOD samples. To further improve its effectiveness, we refine LogitGap by
focusing on a more compact and informative subset of the logit space.
Specifically, we introduce a training-free strategy that automatically
identifies the most informative logits for scoring. We provide both theoretical
analysis and empirical evidence to validate the effectiveness of our approach.
Extensive experiments on both vision-language and vision-only models
demonstrate that LogitGap consistently achieves state-of-the-art performance
across diverse OOD detection scenarios and benchmarks. Code is available at
https://github.com/GIT-LJc/LogitGap.

</details>


### [18] [PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding](https://arxiv.org/abs/2510.20155)
*Penghao Wang,Yiyang He,Xin Lv,Yukai Zhou,Lan Xu,Jingyi Yu,Jiayuan Gu*

Main category: cs.CV

TL;DR: 论文提出了PartNeXt，这是一个包含2.3万个高质量、带有纹理和细粒度零件标签的3D模型数据集，显著提升了3D目标分割和零件级问答任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D零件理解数据集（如PartNet）存在无纹理、依赖专家注释、难以扩展等问题，限制了其在计算机视觉、图形学和机器人等领域的应用。

Method: 作者构建了PartNeXt数据集，涵盖50个类别、超2.3万个带有细粒度分层零件标签和高质量纹理的3D模型，并设立了两个评测任务：一是类别无关的零件分割，二是基于零件的3D问答。同时在主流分割模型（如PartField、SAMPart3D）和大模型（3D-LLMs）上进行基准测试。

Result: 实验发现，现有方法在精细和叶节点层级分割上表现不佳，3D-LLMs在开放词汇的零件定位上存在明显不足。将Point-SAM在PartNeXt上训练，性能大幅优于PartNet，显示出该数据集的高质量和多样性。

Conclusion: PartNeXt具备可扩展的注释、纹理感知标签和多任务评测，为结构化3D理解开辟了新的研究方向。

Abstract: Understanding objects at the level of their constituent parts is fundamental
to advancing computer vision, graphics, and robotics. While datasets like
PartNet have driven progress in 3D part understanding, their reliance on
untextured geometries and expert-dependent annotation limits scalability and
usability. We introduce PartNeXt, a next-generation dataset addressing these
gaps with over 23,000 high-quality, textured 3D models annotated with
fine-grained, hierarchical part labels across 50 categories. We benchmark
PartNeXt on two tasks: (1) class-agnostic part segmentation, where
state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with
fine-grained and leaf-level parts, and (2) 3D part-centric question answering,
a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary
part grounding. Additionally, training Point-SAM on PartNeXt yields substantial
gains over PartNet, underscoring the dataset's superior quality and diversity.
By combining scalable annotation, texture-aware labels, and multi-task
evaluation, PartNeXt opens new avenues for research in structured 3D
understanding.

</details>


### [19] [Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists](https://arxiv.org/abs/2510.20158)
*Eduardo R. Corral-Soto,Yang Liu,Yuan Ren,Bai Dongfeng,Liu Bingbing*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动驾驶场景中自行车及骑行者8D姿态估计的新方法，能从单张RGB图像估计出刚体与关节运动信息以提升对骑行者意图与路径的理解。


<details>
  <summary>Details</summary>
Motivation: 自行车及骑行者作为易受伤害道路使用者，对其准确的姿态估计对于意图识别、行为预测及避免碰撞至关重要。现有针对刚性物体的6D姿态估计方法不能有效应对自行车的关节运动及细粒度方向估计，因此需要更高级的方法。

Method: 作者提出了一种可针对关节自行车和骑行者的类别级8D姿态估计算法，从单张RGB图片同时回归出自行车的3D平移、旋转、转向把和踏板的转动信息，并联合估计3D关键点。训练时融合了合成和真实图像数据以提高泛化能力。

Result: 该方法能够精准估计自行车的8D姿态参数，并在基于真实图像的评测中，取得与最新的类别级6D姿态估计算法使用刚体模板相比具有竞争力的表现。

Conclusion: 本文提出的8D姿态估计方法弥补了现有6D方法的不足，实现对骑行者和自行车更细致的姿态与运动方向理解，对未来自动驾驶行人及骑行者感知具有应用潜力。

Abstract: In Autonomous Driving, cyclists belong to the safety-critical class of
Vulnerable Road Users (VRU), and accurate estimation of their pose is critical
for cyclist crossing intention classification, behavior prediction, and
collision avoidance. Unlike rigid objects, articulated bicycles are composed of
movable rigid parts linked by joints and constrained by a kinematic structure.
6D pose methods can estimate the 3D rotation and translation of rigid bicycles,
but 6D becomes insufficient when the steering/pedals angles of the bicycle
vary. That is because: 1) varying the articulated pose of the bicycle causes
its 3D bounding box to vary as well, and 2) the 3D box orientation is not
necessarily aligned to the orientation of the steering which determines the
actual intended travel direction. In this work, we introduce a method for
category-level 8D pose estimation for articulated bicycles and cyclists from a
single RGB image. Besides being able to estimate the 3D translation and
rotation of a bicycle from a single image, our method also estimates the
rotations of its steering handles and pedals with respect to the bicycle body
frame. These two new parameters enable the estimation of a more fine-grained
bicycle pose state and travel direction. Our proposed model jointly estimates
the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix
of synthetic and real image data to generalize on real images. We include an
evaluation section where we evaluate the accuracy of our estimated 8D pose
parameters, and our method shows promising results by achieving competitive
scores when compared against state-of-the-art category-level 6D pose estimators
that use rigid canonical object templates for matching.

</details>


### [20] [TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2510.20162)
*Xudong Yan,Songhe Feng*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过利用无监督数据中的文本和视觉信息，动态更新多模态原型，提升组合式零样本学习（CZSL）在测试时遇到标签空间分布变化情况下的表现，并在多个数据集上取得了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: 组合式零样本学习面临测试时标签空间分布发生偏移（即出现未见过的属性-对象组合）的难题，导致现有方法性能下降。作者希望解决模型无法适应测试阶段分布变化的问题。

Method: 提出利用无监督数据中获取的文本和视觉知识，在测试阶段动态更新多模态原型，配合自适应调整权重机制控制更新幅度，引入动态优先队列存储高置信图像，加强视觉经验。此外，通过多模态协同表征学习，使文本和视觉原型语义一致。

Result: 在四个基准数据集上，本文方法在闭集和开放世界设置下均取得了最优性能，优于现有方法。

Conclusion: 综合文本和视觉的无监督知识并动态自适应更新原型，可以显著提升CZSL模型对分布变化的适应性和泛化能力。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel
attribute-object compositions based on the knowledge learned from seen ones.
Existing methods suffer from performance degradation caused by the distribution
shift of label space at test time, which stems from the inclusion of unseen
compositions recombined from attributes and objects. To overcome the challenge,
we propose a novel approach that accumulates comprehensive knowledge in both
textual and visual modalities from unsupervised data to update multimodal
prototypes at test time. Building on this, we further design an adaptive update
weight to control the degree of prototype adjustment, enabling the model to
flexibly adapt to distribution shift during testing. Moreover, a dynamic
priority queue is introduced that stores high-confidence images to acquire
visual knowledge from historical images for inference. Considering the semantic
consistency of multimodal knowledge, we align textual and visual prototypes by
multimodal collaborative representation learning. Extensive experiments
indicate that our approach achieves state-of-the-art performance on four
benchmark datasets under both closed-world and open-world settings. Code will
be available at https://github.com/xud-yan/TOMCAT .

</details>


### [21] [IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks](https://arxiv.org/abs/2510.20165)
*Insu Jeon,Wonkwang Lee,Myeongjang Pyeon,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于生成对抗网络（GAN）的无监督可分解表征学习模型，称为IB-GAN。该方法通过引入信息瓶颈（IB）框架优化GAN，提升了表征的可解释性和解耦能力，在多个数据集上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的GAN-based表征学习方法（如InfoGAN）在解耦语义因子的能力有限，且生成样本的表现与多样性仍有提升空间。作者尝试利用信息瓶颈理论优化GAN，以获得更强的因子解耦与更高质量的生成样本。

Method: IB-GAN架构部分类似InfoGAN，但不同之处在于在生成器的中间层引入随机变量，并直接约束输入和输出的互信息。这个中间随机层作为可学习的潜变量分布，与生成器联合端到端训练，引导模型学习可解释、解耦的潜在空间。

Result: 在dSprites和Color-dSprites数据集上，IB-GAN取得了与最先进的β-VAE相当的解耦分数，并且超过了InfoGAN。在CelebA和3D Chairs等更复杂数据集上，根据FID指标，IB-GAN生成样本的视觉质量和多样性通常优于β-VAE和InfoGAN。

Conclusion: IB-GAN能有效提升无监督表征学习的解耦能力和潜变量可解释性，同时在样本视觉质量和多样性方面达到或超过现有主流方法，展示了该方法的实用潜力。

Abstract: We propose a new GAN-based unsupervised model for disentangled representation
learning. The new model is discovered in an attempt to utilize the Information
Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The
architecture of IB-GAN is partially similar to that of InfoGAN but has a
critical difference; an intermediate layer of the generator is leveraged to
constrain the mutual information between the input and the generated output.
The intermediate stochastic layer can serve as a learnable latent distribution
that is trained with the generator jointly in an end-to-end fashion. As a
result, the generator of IB-GAN can harness the latent space in a disentangled
and interpretable manner. With the experiments on dSprites and Color-dSprites
dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores
to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,
the visual quality and the diversity of samples generated by IB-GAN are often
better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA
and 3D Chairs dataset.

</details>


### [22] [PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching](https://arxiv.org/abs/2510.20178)
*Yun Wang,Junjie Hu,Qiaole Dong,Yongjian Zhang,Yanwei Fu,Tin Lun Lam,Dapeng Wu*

Main category: cs.CV

TL;DR: 本文提出了一种高效且具时序一致性的双目视频深度估计方法PPMStereo，通过记忆模块实现了精确且连贯的深度估计，在准确性和时序一致性方面都达到了最佳水平。


<details>
  <summary>Details</summary>
Motivation: 当前双目视频中的深度估计在许多现实应用（如增强现实）中非常重要，但受限于现有方法只能在时序一致性和计算效率之间做权衡，难以有效建模长时序信息且计算代价高昂。作者因此希望找到一种同时兼顾时序一致性与高效性的解决方案。

Method: 提出PPMStereo方法，包括一个Pick-and-Play Memory（PPM）模块，采用‘选取’阶段挑选最相关的帧，‘播放’阶段对选中帧自适应加权，实现有效的时空信息聚合。这种两阶段协同策略既保证了记忆缓冲的高信息量和紧凑性，又实现了高效的动态立体匹配。

Result: 在Sintel clean/final等数据集上，PPMStereo实现了0.62/1.11 TEPE（比当前最佳BiDAStereo提升17.3%/9.02%），同时计算代价更低。实验还验证了其在准确性和时序一致性上的领先性能。

Conclusion: PPMStereo通过创新性记忆模块，在保留效率的同时显著提升了双目视频深度估计的时序一致性和准确性。对于需要高质量深度感知的现实应用具有广泛前景。

Abstract: Temporally consistent depth estimation from stereo video is critical for
real-world applications such as augmented reality, where inconsistent depth
estimation disrupts the immersion of users. Despite its importance, this task
remains challenging due to the difficulty in modeling long-term temporal
consistency in a computationally efficient manner. Previous methods attempt to
address this by aggregating spatio-temporal information but face a fundamental
trade-off: limited temporal modeling provides only modest gains, whereas
capturing long-range dependencies significantly increases computational cost.
To address this limitation, we introduce a memory buffer for modeling
long-range spatio-temporal consistency while achieving efficient dynamic stereo
matching. Inspired by the two-stage decision-making process in humans, we
propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction
module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM
consists of a `pick' process that identifies the most relevant frames and a
`play' process that weights the selected frames adaptively for spatio-temporal
aggregation. This two-stage collaborative process maintains a compact yet
highly informative memory buffer while achieving temporally consistent
information aggregation. Extensive experiments validate the effectiveness of
PPMStereo, demonstrating state-of-the-art performance in both accuracy and
temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the
Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer
computational costs. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.

</details>


### [23] [Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories](https://arxiv.org/abs/2510.20182)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 本论文提出了一个系统性评测协议，用于测试生成视频模型在模拟多行人动态场景中的能力，发现尽管现有模型能够生成一定真实的多主体行为，但依然存在个体消失和融合等失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型主要评测单一主体，对多主体交互场景缺乏系统性分析。多主体动态是模拟现实世界的重要能力，因此需要开发新的评测方法来验证生成模型对复杂行人动态的模拟能力。

Method: 为I2V（图像转视频）模型选用知名数据集的起始帧进行实验，使得生成视频能与真实数据进行对比。为T2V（文本转视频）模型设计多样化的文本提示，涵盖不同的行人密度和交互。此外，提出了一种基于像素空间重建2D鸟瞰轨迹的方法，无需已知摄像机参数。

Result: 分析显示，主流视频生成模型已经学习到了有效的多主体动态行为先验，对模拟多行人场景有一定的能力；但同时发现模型在生成过程中会出现个体合并、人物消失等问题。

Conclusion: 尽管当前视频生成模型在多主体行为上取得了进步，但模型稳定性和连贯性还有待提升。未来应针对多主体动态交互的真实拟合性进行进一步改进和优化。

Abstract: Large-scale video generation models have demonstrated high visual realism in
diverse contexts, spurring interest in their potential as general-purpose world
simulators. Existing benchmarks focus on individual subjects rather than scenes
with multiple interacting people. However, the plausibility of multi-agent
dynamics in generated videos remains unverified. We propose a rigorous
evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)
models as implicit simulators of pedestrian dynamics. For I2V, we leverage
start frames from established datasets to enable comparison with a ground truth
video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian
densities and interactions. A key component is a method to reconstruct 2D
bird's-eye view trajectories from pixel-space without known camera parameters.
Our analysis reveals that leading models have learned surprisingly effective
priors for plausible multi-agent behavior. However, failure modes like merging
and disappearing people highlight areas for future improvement.

</details>


### [24] [SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization](https://arxiv.org/abs/2510.20189)
*Xinyi Hu,Yuran Wang,Yue Li,Wenxuan Liu,Zheng Wang*

Main category: cs.CV

TL;DR: 本文提出了SPAN网络，将视频监控中意图检测从离散分类转变为连续回归，更好捕捉可变与渐变的可疑行为意图，大幅提升了早期发现与系统可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视频监控常用离散分类检测可疑意图，无法有效反映意图的连续性与发展过程，导致早期干预和可解释性不足，因此亟需更细粒度的分析框架。

Method: 作者提出SPAN网络，通过引入连续回归而非分类，结合Temporal Point Process理论，建模可疑意图的时序依赖和累积效应。设计了疑虑分数公式及其时序变化；利用多模态信息调整疑虑系数，并采用概念锚定映射，将行为与预设意图语义关联，提高可解释性。

Result: 在HAI数据集上，SPAN相较以往方法将MSE降低19.8%，平均mAP提升1.78%，尤其在低频场景mAP提升2.74%，更有效捕捉行为微小变化。

Conclusion: SPAN将可疑意图识别从离散分类升级为连续建模，实现更早发现、主动干预和更强可解释性，为安全监控系统带来实用和理论上的显著提升。

Abstract: Temporal Intention Localization (TIL) is crucial for video surveillance,
focusing on identifying varying levels of suspicious intentions to improve
security monitoring. However, existing discrete classification methods fail to
capture the continuous nature of suspicious intentions, limiting early
intervention and explainability. In this paper, we propose the Suspicion
Progression Analysis Network (SPAN), which shifts from discrete classification
to continuous regression, enabling the capture of fluctuating and evolving
suspicious intentions. We reveal that suspicion exhibits long-term dependencies
and cumulative effects, similar to Temporal Point Process (TPP) theory. Based
on these insights, we define a suspicion score formula that models continuous
changes while accounting for temporal characteristics. We also introduce
Suspicion Coefficient Modulation, which adjusts suspicion coefficients using
multimodal information to reflect the varying impacts of suspicious actions.
Additionally, the Concept-Anchored Mapping method is proposed to link
suspicious actions to predefined intention concepts, offering insights into
both the actions and their potential underlying intentions. Extensive
experiments on the HAI dataset show that SPAN significantly outperforms
existing methods, reducing MSE by 19.8% and improving average mAP by 1.78%.
Notably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating
its superior ability to capture subtle behavioral changes. Compared to discrete
classification systems, our continuous suspicion modeling approach enables
earlier detection and proactive intervention, greatly enhancing system
explainability and practical utility in security applications.

</details>


### [25] [A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development](https://arxiv.org/abs/2510.20196)
*Minh Sao Khue Luu,Margaret V. Benedichuk,Ekaterina I. Roppert,Roman M. Kenzhin,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: 本研究系统性分析了54个公开脑MRI数据集，揭示了数据类型、疾病分布等多方面的不均衡和异构性，并证明了标准化预处理难以完全消除数据集间的偏差，为更高泛化性的基础模型开发提出了新需求。


<details>
  <summary>Details</summary>
Motivation: 当前脑MRI基础模型的开发极度依赖于数据的规模与均匀性，而关于这些基础数据资源内部结构和一致性的系统性评估却很稀缺。

Method: 对54个公开可用的脑MRI大数据集进行多层次分析，涵盖模态组成、疾病种类、数据规模等；针对15个数据集详细量化体素空间、方向、强度，评测预处理（如归一化、校正、去骨、配准、插值）对体素统计和结构的影响，并用3D DenseNet121网络做特征空间的偏移分析。

Result: 发现数据集中存在大样本健康组和小样本疾病组之间强烈不平衡；同一预处理下仍有不同数据集间残留的统计差异，预处理能提升数据一致性但无法根本消除间歇的数据偏移。

Conclusion: 要建立泛化性强的脑MRI基础模型，必须重视并解决数据预处理带来的剩余不一致性，建议开发适应不同域的预处理感知与域自适应策略。

Abstract: The development of foundation models for brain MRI depends critically on the
scale, diversity, and consistency of available data, yet systematic assessments
of these factors remain scarce. In this study, we analyze 54 publicly
accessible brain MRI datasets encompassing over 538,031 to provide a
structured, multi-level overview tailored to foundation model development. At
the dataset level, we characterize modality composition, disease coverage, and
dataset scale, revealing strong imbalances between large healthy cohorts and
smaller clinical populations. At the image level, we quantify voxel spacing,
orientation, and intensity distributions across 15 representative datasets,
demonstrating substantial heterogeneity that can influence representation
learning. We then perform a quantitative evaluation of preprocessing
variability, examining how intensity normalization, bias field correction,
skull stripping, spatial registration, and interpolation alter voxel statistics
and geometry. While these steps improve within-dataset consistency, residual
differences persist between datasets. Finally, feature-space case study using a
3D DenseNet121 shows measurable residual covariate shift after standardized
preprocessing, confirming that harmonization alone cannot eliminate
inter-dataset bias. Together, these analyses provide a unified characterization
of variability in public brain MRI resources and emphasize the need for
preprocessing-aware and domain-adaptive strategies in the design of
generalizable brain MRI foundation models.

</details>


### [26] [RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling](https://arxiv.org/abs/2510.20206)
*Bingjie Gao,Qianli Ma,Xiaoxue Wu,Shuai Yang,Guanzhou Lan,Haonan Zhao,Jiaxuan Chen,Qingyang Liu,Yu Qiao,Xinyuan Chen,Yaohui Wang,Li Niu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的跨阶段提示词优化框架RAPO++，通过三个阶段从训练数据对齐、测试时优化到大语言模型微调，显著提升文本到视频生成质量，无需修改底层生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）生成模型受限于用户输入的提示词太短、结构混乱，与模型训练数据分布不一致，导致生成效果有限。因此，亟需优化提示词以提升生成效果。

Method: 提出跨阶段提示优化框架RAPO++：第一阶段，从关系图谱检索相关修饰词丰富并重构用户prompt，使其更接近训练分布，加强组合表达与多物体准确性；第二阶段，基于多源反馈（包括语义、空间、时间一致性和任务信号如光流）进行循环优化，生成更优prompt；第三阶段，将优化结果用于微调大语言模型，提升推理前prompt生成能力。整个流程无需改动生成主干模型。

Result: 在五个主流T2V模型与五个公开基准上，RAPO++在语义对齐、复杂性、时序一致性与物理合理性等指标上显著超越现有方法。

Conclusion: RAPO++作为一种无模型约束、高效且可扩展的T2V提示词优化方法，有效提升了文本到视频生成的整体表现，树立了领域新标准。

Abstract: Prompt design plays a crucial role in text-to-video (T2V) generation, yet
user-provided prompts are often short, unstructured, and misaligned with
training data, limiting the generative potential of diffusion-based T2V models.
We present \textbf{RAPO++}, a cross-stage prompt optimization framework that
unifies training-data--aligned refinement, test-time iterative scaling, and
large language model (LLM) fine-tuning to substantially improve T2V generation
without modifying the underlying generative backbone. In \textbf{Stage 1},
Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with
semantically relevant modifiers retrieved from a relation graph and refactors
them to match training distributions, enhancing compositionality and
multi-object fidelity. \textbf{Stage 2} introduces Sample-Specific Prompt
Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts
using multi-source feedback -- including semantic alignment, spatial fidelity,
temporal coherence, and task-specific signals such as optical flow -- yielding
progressively improved video generation quality. \textbf{Stage 3} leverages
optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing
task-specific optimization patterns and enabling efficient, high-quality prompt
generation even before inference. Extensive experiments across five
state-of-the-art T2V models and five benchmarks demonstrate that RAPO++
achieves significant gains in semantic alignment, compositional reasoning,
temporal stability, and physical plausibility, outperforming existing methods
by large margins. Our results highlight RAPO++ as a model-agnostic,
cost-efficient, and scalable solution that sets a new standard for prompt
optimization in T2V generation. The code is available at
https://github.com/Vchitect/RAPO.

</details>


### [27] [FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing](https://arxiv.org/abs/2510.20212)
*Yanghao Wang,Zhen Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种更智能的文本驱动图像编辑方法FlowCycle，通过让中间状态具有目标感知能力，实现更精确和一致的图像编辑效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主流基于预训练text-to-image流模型的编辑方法，往往只关注从受损图像恢复原图，忽略了实际编辑目标所需的语义变化，导致在目标与原图差异较大时编辑效果有限或不一致。因此，亟需一种能够针对目标自适应地处理图像内容的方法。

Method: 提出FlowCycle，一种无需反演、基于流模型的编辑框架。其创新点在于利用可学习的噪声参数，对图像进行目标感知的“选择性受损”，即只处理与编辑相关的内容，并通过循环一致性优化方法（source-target-source往返编辑）来学习中间状态的最佳表征，实现精确编辑与源图一致性的统一。

Result: FlowCycle在多项消融和对比实验中展现出优于现有主流方法的编辑质量和一致性，能够更好地兼顾目标语义调整和原图特征保留。

Conclusion: 针对现有corruption-then-restoration范式的局限，FlowCycle通过目标感知的中间状态构建、创新的循环一致性优化，显著提升了文本驱动图像编辑的效果。

Abstract: Recent advances in pre-trained text-to-image flow models have enabled
remarkable progress in text-based image editing. Mainstream approaches always
adopt a corruption-then-restoration paradigm, where the source image is first
corrupted into an ``intermediate state'' and then restored to the target image
under the prompt guidance. However, current methods construct this intermediate
state in a target-agnostic manner, i.e., they primarily focus on realizing
source image reconstruction while neglecting the semantic gaps towards the
specific editing target. This design inherently results in limited editability
or inconsistency when the desired modifications substantially deviate from the
source. In this paper, we argue that the intermediate state should be
target-aware, i.e., selectively corrupting editing-relevant contents while
preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel
inversion-free and flow-based editing framework that parameterizes corruption
with learnable noises and optimizes them through a cycle-consistent process. By
iteratively editing the source to the target and recovering back to the source
with dual consistency constraints, FlowCycle learns to produce a target-aware
intermediate state, enabling faithful modifications while preserving source
consistency. Extensive ablations have demonstrated that FlowCycle achieves
superior editing quality and consistency over state-of-the-art methods.

</details>


### [28] [Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection](https://arxiv.org/abs/2510.20214)
*Talha Ilyas,Duong Nhu,Allison Thomas,Arie Levin,Lim Wei Yap,Shu Gong,David Vera Anaya,Yiwen Jiang,Deval Mehta,Ritesh Warty,Vinayak Smith,Maya Reddy,Euan Wallace,Wenlong Cheng,Zongyuan Ge,Faezeh Marzbanrad*

Main category: cs.CV

TL;DR: 本文提出了一种名为CURL的自监督对比学习框架，能够从长时间的胎儿超声视频中高效检测胎动，显著提升检测精度且更客观。


<details>
  <summary>Details</summary>
Motivation: 现有胎动检测方法（如妈妈主观感知和胎心监护）存在主观性强和准确度低的问题，难以准确评估胎儿健康状况，因此急需开发更为客观、高效的自动化检测方法。

Method: 文章提出了CURL框架，融合空间和时间的对比损失，自动学习超声视频中的运动表征。并设计了特定的采样策略，将运动段和非运动段有效分开。同时引入概率微调方式，实现对任意长视频的灵活推理。

Result: 在包含92例、每例30分钟超声的自有数据集上，CURL达到了78.01%的灵敏度和81.60%的AUROC，表现优异。

Conclusion: CURL验证了自监督对比学习在胎动分析中的有效性，为胎儿健康的客观监测及临床决策提供了有力工具。

Abstract: Accurate fetal movement (FM) detection is essential for assessing prenatal
health, as abnormal movement patterns can indicate underlying complications
such as placental dysfunction or fetal distress. Traditional methods, including
maternal perception and cardiotocography (CTG), suffer from subjectivity and
limited accuracy. To address these challenges, we propose Contrastive
Ultrasound Video Representation Learning (CURL), a novel self-supervised
learning framework for FM detection from extended fetal ultrasound video
recordings. Our approach leverages a dual-contrastive loss, incorporating both
spatial and temporal contrastive learning, to learn robust motion
representations. Additionally, we introduce a task-specific sampling strategy,
ensuring the effective separation of movement and non-movement segments during
self-supervised training, while enabling flexible inference on arbitrarily long
ultrasound recordings through a probabilistic fine-tuning approach. Evaluated
on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,
CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its
potential for reliable and objective FM analysis. These results highlight the
potential of self-supervised contrastive learning for fetal movement analysis,
paving the way for improved prenatal monitoring and clinical decision-making.

</details>


### [29] [EditInfinity: Image Editing with Binary-Quantized Generative Models](https://arxiv.org/abs/2510.20217)
*Jiahuan Wang,Yuxin Chen,Jun Yu,Guangming Lu,Wenjie Pei*

Main category: cs.CV

TL;DR: 本文提出了EditInfinity方法，通过高效适配基于VQ的生成模型，实现了无需大量调优的文本驱动图像编辑，显著优于现有扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像编辑方法常因图像反演时的近似误差而效果受限，该误差源于生成过程中的中间状态缺乏精确监督。因此，亟需新的方法以提升图像编辑的准确性和保真度。

Method: 本工作提出EditInfinity方法，采用参数高效的VQ（向量量化）生成模型Infinity，将其适配为高精度图像编辑工具。核心方法包括：1）引入集成文本提示修正与图像风格保持的图像反演机制，实现精准反演；2）设计全局平滑策略，提升编辑后图像对原图的保真度和对文本语义的对齐度。

Result: 在PIE-Bench基准的“添加”、“变更”、“删除”三类编辑任务中，EditInfinity在性能上全面超越了最先进的扩散模型基线方法，表现出更高的编辑保真度和文本对齐能力。

Conclusion: EditInfinity为文本驱动的图像编辑提供了更高效与精准的解决方案，通过结合VQ模型的中间量化表征，有效克服了传统扩散模型在反演环节的局限性，在多项基准测试中取得了领先表现。

Abstract: Adapting pretrained diffusion-based generative models for text-driven image
editing with negligible tuning overhead has demonstrated remarkable potential.
A classical adaptation paradigm, as followed by these methods, first infers the
generative trajectory inversely for a given source image by image inversion,
then performs image editing along the inferred trajectory guided by the target
text prompts. However, the performance of image editing is heavily limited by
the approximation errors introduced during image inversion by diffusion models,
which arise from the absence of exact supervision in the intermediate
generative steps. To circumvent this issue, we investigate the
parameter-efficient adaptation of VQ-based generative models for image editing,
and leverage their inherent characteristic that the exact intermediate
quantized representations of a source image are attainable, enabling more
effective supervision for precise image inversion. Specifically, we propose
\emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized
generative model, for image editing. We propose an efficient yet effective
image inversion mechanism that integrates text prompting rectification and
image style preservation, enabling precise image inversion. Furthermore, we
devise a holistic smoothing strategy which allows our \emph{EditInfinity} to
perform image editing with high fidelity to source images and precise semantic
alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark
across "add", "change", and "delete" editing operations, demonstrate the
superior performance of our model compared to state-of-the-art diffusion-based
baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.

</details>


### [30] [Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context](https://arxiv.org/abs/2510.20229)
*Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的“诱导—检测—抑制”框架来应对大规模视觉-语言模型（LVLMs）在长文本回复中更易出现的幻觉（hallucination）问题，并在各项基准测试中显著提升了幻觉的检测与抑制效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs取得了很大进步，但在自由、冗长的回答中产生的幻觉问题更为严重，影响了模型的可靠性。此前认为这种现象来自回答篇幅越长导致错误累积，本文试图进一步探究幻觉风险增加的根本原因。

Method: 作者通过初步实验发现幻觉风险并非仅由回复长度导致，而主要因为模型在长回复中对上下文依赖增强。基于此，提出“诱导—检测—抑制”新框架：1）通过精心设计的上下文主动诱发幻觉；2）利用诱发样本及早检测高风险；3）在实际解码时抑制可能的客观幻觉。

Result: 所提出方法在全部基准测试上均获得了持续且显著的性能提升，成功提升了幻觉检测和缓解能力。

Conclusion: 本文的框架不仅提升了对幻觉的检测与抑制，还从理论上验证了上下文依赖是长回复中幻觉问题加剧的根源，为深入理解与探索LVLMs幻觉现象打下了基础。

Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.

</details>


### [31] [COS3D: Collaborative Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.20238)
*Runsong Zhu,Ka-Hei Hui,Zhengzhe Liu,Qianyi Wu,Weiliang Tang,Shi Qiu,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.CV

TL;DR: COS3D提出了一种协作式的提示-分割框架，有效结合了语言和分割信息，提升了open-vocabulary三维分割的性能，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gaussian-splatting的方法因为依靠单一的三维语言场或依赖先验无类别分割，造成分割效果欠佳或误差累积，限制了open-vocabulary三维分割的表现。

Method: 本文提出COS3D框架，核心为协作场（包含实例场和语言场），并通过实例到语言的特征映射和高效的两阶段训练策略建立两者关联。在推理阶段，设计自适应的语言到实例提示优化，实现高质量的分割。

Result: 在两个主流open-vocabulary三维分割基准上，COS3D均获得领先效果，并在多种应用场景（如基于图像的新颖三维分割、层次分割和机器人）展现出高潜力。

Conclusion: COS3D证明了通过协作式融合语言和实例分割信号能够显著提升open-vocabulary三维分割的性能，为相关领域提供了更好解决方案。

Abstract: Open-vocabulary 3D segmentation is a fundamental yet challenging task,
requiring a mutual understanding of both segmentation and language. However,
existing Gaussian-splatting-based methods rely either on a single 3D language
field, leading to inferior segmentation, or on pre-computed class-agnostic
segmentations, suffering from error accumulation. To address these limitations,
we present COS3D, a new collaborative prompt-segmentation framework that
contributes to effectively integrating complementary language and segmentation
cues throughout its entire pipeline. We first introduce the new concept of
collaborative field, comprising an instance field and a language field, as the
cornerstone for collaboration. During training, to effectively construct the
collaborative field, our key idea is to capture the intrinsic relationship
between the instance field and language field, through a novel
instance-to-language feature mapping and designing an efficient two-stage
training strategy. During inference, to bridge distinct characteristics of the
two fields, we further design an adaptive language-to-instance prompt
refinement, promoting high-quality prompt-segmentation inference. Extensive
experiments not only demonstrate COS3D's leading performance over existing
methods on two widely-used benchmarks but also show its high potential to
various applications,~\ie, novel image-based 3D segmentation, hierarchical
segmentation, and robotics. The code is publicly available at
\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.

</details>


### [32] [Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding](https://arxiv.org/abs/2510.20244)
*Minseok Kang,Minhyeok Lee,Minjung Kim,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: 提出DualGround，一种新颖的双分支结构，通过区分全局与局部语义，提升视频文本时序定位（VTG）任务的性能，在多个基准任务上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VTG方法基于强大的视觉-语言模型，对文本token一视同仁，忽视了不同token语义作用的差异，导致模型过度依赖全局句子信息，细粒度对齐能力不足。

Method: 提出DualGround架构，将文本的[EOS] token分流到句子级通道，将单词token聚为短语级单元做局部对齐。采用token角色感知的跨模态交互策略，实现视频特征与分离的句子级和短语级语义对应，并通过联合建模提升整体和局部对齐能力。

Result: DualGround在QVHighlights和Charades-STA等数据集的MR和HD两个任务上刷新了SOTA成绩。

Conclusion: 语义解耦（区分全局与局部）可提升视频文本时序定位模型的表达能力和精细对齐能力，DualGround的方法验证了这一点。

Abstract: Video Temporal Grounding (VTG) aims to localize temporal segments in long,
untrimmed videos that align with a given natural language query. This task
typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
(HD). While recent advances have been progressed by powerful pretrained
vision-language models such as CLIP and InternVideo2, existing approaches
commonly treat all text tokens uniformly during crossmodal attention,
disregarding their distinct semantic roles. To validate the limitations of this
approach, we conduct controlled experiments demonstrating that VTG models
overly rely on [EOS]-driven global semantics while failing to effectively
utilize word-level signals, which limits their ability to achieve fine-grained
temporal alignment. Motivated by this limitation, we propose DualGround, a
dual-branch architecture that explicitly separates global and local semantics
by routing the [EOS] token through a sentence-level path and clustering word
tokens into phrase-level units for localized grounding. Our method introduces
(1) tokenrole- aware cross modal interaction strategies that align video
features with sentence-level and phrase-level semantics in a structurally
disentangled manner, and (2) a joint modeling framework that not only improves
global sentence-level alignment but also enhances finegrained temporal
grounding by leveraging structured phrase-aware context. This design allows the
model to capture both coarse and localized semantics, enabling more expressive
and context-aware video grounding. DualGround achieves state-of-the-art
performance on both Moment Retrieval and Highlight Detection tasks across
QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
disentangled semantic modeling in video-language alignment.

</details>


### [33] [Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation](https://arxiv.org/abs/2510.20549)
*Marziyeh Bamdad,Hans-Peter Hutter,Alireza Darvishy*

Main category: cs.CV

TL;DR: 本文提出了一种融合深度学习方法的视觉SLAM框架SELM-SLAM3，通过增强特征提取与匹配有效提升了低纹理、运动模糊等复杂环境下的定位与跟踪表现。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM技术在低纹理、运动模糊、极端照明等复杂环境下仍难以保证鲁棒性，这直接影响包括助残导航等实际应用的安全与可靠性。

Method: SELM-SLAM3结合了SuperPoint和LightGlue两种基于深度学习的特征提取与匹配算法，并在TUM RGB-D、ICL-NUIM和TartanAir等多种具有挑战性的公开数据集上进行了评测。

Result: SELM-SLAM3在准确率上相较传统的ORB-SLAM3平均提升87.84%，相较于当前最先进的RGB-D SLAM系统提升36.77%。

Conclusion: 该框架在低纹理、快速运动等困难条件下表现优越，可为视障人士导航等应用提供更可靠的技术支持。

Abstract: Despite advancements in SLAM technologies, robust operation under challenging
conditions such as low-texture, motion-blur, or challenging lighting remains an
open challenge. Such conditions are common in applications such as assistive
navigation for the visually impaired. These challenges undermine localization
accuracy and tracking stability, reducing navigation reliability and safety. To
overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced
visual SLAM framework that integrates SuperPoint and LightGlue for robust
feature extraction and matching. We evaluated our framework using TUM RGB-D,
ICL-NUIM, and TartanAir datasets, which feature diverse and challenging
scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of
87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework
demonstrates enhanced performance under challenging conditions, such as
low-texture scenes and fast motion, providing a reliable platform for
developing navigation aids for the visually impaired.

</details>


### [34] [Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization](https://arxiv.org/abs/2510.20247)
*Shuhan Hu,Yiru Li,Yuanyuan Li,Yingying Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于分割掩码的位置编码方法和上下文增强模块，以提升跨视角对象地理定位的精度，最终集成为EDGeo框架，在主流数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前跨视角对象地理定位方法主要依赖于关键点的位置编码，仅捕捉二维坐标，忽略对象形状信息，导致对标注变化敏感且跨视角匹配效果有限。

Method: 作者提出了基于分割掩码的位置编码方法（MPE），能够同时捕捉空间坐标和对象轮廓信息。此外，针对卫星图像中跨度较大的对象（如拉长的建筑物），设计了上下文增强模块（CEM），利用水平和垂直方向的条带卷积提取远程上下文特征。最终将MPE和CEM集成，提出EDGeo端到端框架。

Result: 在CVOGL和VIGOR-Building两个公开数据集上的实验显示，EDGeo在困难的地面到卫星图像场景下定位精度提升了3.39%，达到了最新水平。

Conclusion: 本文提出的掩码位置编码与上下文增强方法，有效提升了对象的判别力和跨视角地理定位的精度，为相关领域提供了新的建模范式和技术参考。

Abstract: Cross-view object geo-localization enables high-precision object localization
through cross-view matching, with critical applications in autonomous driving,
urban management, and disaster response. However, existing methods rely on
keypoint-based positional encoding, which captures only 2D coordinates while
neglecting object shape information, resulting in sensitivity to annotation
shifts and limited cross-view matching capability. To address these
limitations, we propose a mask-based positional encoding scheme that leverages
segmentation masks to capture both spatial coordinates and object silhouettes,
thereby upgrading the model from "location-aware" to "object-aware."
Furthermore, to tackle the challenge of large-span objects (e.g., elongated
buildings) in satellite imagery, we design a context enhancement module. This
module employs horizontal and vertical strip convolutional kernels to extract
long-range contextual features, enhancing feature discrimination among
strip-like objects. Integrating MPE and CEM, we present EDGeo, an end-to-end
framework for robust cross-view object geo-localization. Extensive experiments
on two public datasets (CVOGL and VIGOR-Building) demonstrate that our method
achieves state-of-the-art performance, with a 3.39% improvement in localization
accuracy under challenging ground-to-satellite scenarios. This work provides a
robust positional encoding paradigm and a contextual modeling framework for
advancing cross-view geo-localization research.

</details>


### [35] [EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence](https://arxiv.org/abs/2510.20578)
*Ding Zou,Feifan Wang,Mengyu Ge,Siyuan Fan,Zongbing Zhang,Wei Chen,Lingfeng Wang,Zhongyou Hu,Wenrui Yan,Zhengwei Gao,Hao Wang,Weizhao Jin,Yu Zhang,Hainan Zhao,Mingliang Zhang,Xianxian Xi,Yaru Zhang,Wenyuan Li,Zhengguang Gao,Yurui Zhu*

Main category: cs.CV

TL;DR: 本文提出了EmbodiedBrain，这是一种新型视觉-语言基础模型，旨在提升体态人工智能(agent)在空间感知、任务规划与执行方面的能力。该模型在多项基准上刷新了性能记录，并实现了开源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型(LLMs)及多模态LLMs在体态任务中存在设计与需求脱节、性能与实时性难以兼顾，以及评估指标不真实等问题，阻碍了人工智能通用体(agent)的发展。

Method: EmbodiedBrain采用了与agent高度对齐的数据结构，以及结合大规模有监督微调(SFT)和步进增强分组相对策略优化(Step-GRPO)的新型训练范式，还引入了生成式奖励模型(GRM)提升训练效率；并提出了三大评价系统及挑战性场景用于全面验证。

Result: 实验证明EmbodiedBrain无论在通用性、规划还是端到端仿真测试中均优于现有模型，在体态基础模型领域创下新纪录。

Conclusion: EmbodiedBrain为面向下一代通用体态智能代理奠定了坚实基础，相关数据、模型与评测方法已全部开源，有望推动该领域进一步发展。

Abstract: The realization of Artificial General Intelligence (AGI) necessitates
Embodied AI agents capable of robust spatial perception, effective task
planning, and adaptive execution in physical environments. However, current
large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks
suffer from key limitations, including a significant gap between model design
and agent requirements, an unavoidable trade-off between real-time latency and
performance, and the use of unauthentic, offline evaluation metrics. To address
these challenges, we propose EmbodiedBrain, a novel vision-language foundation
model available in both 7B and 32B parameter sizes. Our framework features an
agent-aligned data structure and employs a powerful training methodology that
integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group
Relative Policy Optimization (Step-GRPO), which boosts long-horizon task
success by integrating preceding steps as Guided Precursors. Furthermore, we
incorporate a comprehensive reward system, including a Generative Reward Model
(GRM) accelerated at the infrastructure level, to improve training efficiency.
For enable thorough validation, we establish a three-part evaluation system
encompassing General, Planning, and End-to-End Simulation Benchmarks,
highlighted by the proposal and open-sourcing of a novel, challenging
simulation environment. Experimental results demonstrate that EmbodiedBrain
achieves superior performance across all metrics, establishing a new
state-of-the-art for embodied foundation models. Towards paving the way for the
next generation of generalist embodied agents, we open-source all of our data,
model weight, and evaluating methods, which are available at
https://zterobot.github.io/EmbodiedBrain.github.io.

</details>


### [36] [Calibrating Multimodal Consensus for Emotion Recognition](https://arxiv.org/abs/2510.20256)
*Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan*

Main category: cs.CV

TL;DR: 近年来多模态情感识别（MER）取得进展，但仍存在一些问题。该文提出CMC模型，通过解决模态间语义不一致和文本主导问题，有效提升了识别性能。方案在多个数据集上获得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法常忽视不同模态间可能的语义冲突，尤其在文本与视觉信号表达出不同情绪时表现不佳。此外，目前方法常被文本模态主导，导致整体识别准确率下降。因此作者希望提出方法平衡各模态影响，并增强系统对语义矛盾情境的处理。

Method: 提出名为CMC的模型，包括伪标签生成模块（PLGM），用于自监督方式下生成单模态伪标签并进行预训练。接着，通过无参数融合模块（PFM）与多模态共识路由器（MCR）结合，实现多模态微调，有效弱化文本主导影响并引导各模态共识融合过程。

Result: 实验表明，CMC模型在CH-SIMS、CH-SIMS v2、CMU-MOSI和CMU-MOSEI四个数据集上取得与当前最优方法相当或更优的效果。在CH-SIMS及CH-SIMS v2中面对模态语义矛盾场景时表现尤为突出。

Conclusion: CMC模型能够缓解多模态语义不一致和文本主导的问题，提升情感识别效果，特别适用于复杂多模态语义关系。该方法具备显著的实际应用价值。

Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.

</details>


### [37] [ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata](https://arxiv.org/abs/2510.20708)
*Samuel Soutullo,Miguel Yermo,David L. Vilariño,Óscar G. Lorenzo,José C. Cabaleiro,Francisco F. Rivera*

Main category: cs.CV

TL;DR: 提出了一种新方法ALICE-LRI，实现了对任何旋转LiDAR点云无损的2D范围图像生成，无需厂商元数据或校准文件，解决传统投影方法信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统将LiDAR点云投影成2D范围图像的方法会因为几何不一致导致信息丧失，影响高保真应用，而现有方法依赖厂商数据且无法通用。

Method: ALICE-LRI通过自动反推任意旋转LiDAR的内在几何参数（如激光束配置、角度分布和逐束校准修正），实现无损投影，无需制造商的元数据或校准文件。

Result: 在KITTI和DurLAR全量数据集上，ALICE-LRI实现了完美的点云保留，几何精度在传感器精度范围内，同时具备实时性能。压缩案例研究显示，该方法提升了下游任务的质量。

Conclusion: 该方法实现了转变，从近似投影向无损投影迈进，为高精度遥感应用提供了新的可能，特别适用于需要完整几何保持的场景。

Abstract: 3D LiDAR sensors are essential for autonomous navigation, environmental
monitoring, and precision mapping in remote sensing applications. To
efficiently process the massive point clouds generated by these sensors, LiDAR
data is often projected into 2D range images that organize points by their
angular positions and distances. While these range image representations enable
efficient processing, conventional projection methods suffer from fundamental
geometric inconsistencies that cause irreversible information loss,
compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR
Intrinsic Calibration Estimation for Lossless Range Images), the first general,
sensor-agnostic method that achieves lossless range image generation from
spinning LiDAR point clouds without requiring manufacturer metadata or
calibration files. Our algorithm automatically reverse-engineers the intrinsic
geometry of any spinning LiDAR sensor by inferring critical parameters
including laser beam configuration, angular distributions, and per-beam
calibration corrections, enabling lossless projection and complete point cloud
reconstruction with zero point loss. Comprehensive evaluation across the
complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect
point preservation, with zero points lost across all point clouds. Geometric
accuracy is maintained well within sensor precision limits, establishing
geometric losslessness with real-time performance. We also present a
compression case study that validates substantial downstream benefits,
demonstrating significant quality improvements in practical applications. This
paradigm shift from approximate to lossless LiDAR projections opens new
possibilities for high-precision remote sensing applications requiring complete
geometric preservation.

</details>


### [38] [Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals](https://arxiv.org/abs/2510.20267)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8 nano模型的实时货币检测系统，旨在帮助视障人士自主识别钱币。该系统在三种货币（美元、欧元、孟加拉塔卡）共30种币种上训练，并实现了高准确率与实用性能。


<details>
  <summary>Details</summary>
Motivation: 视障人士在日常生活中独立处理货币存在困难，当前缺乏高效、实用的自动化纸币/硬币识别工具。利用智能手机和机器学习技术，有望缓解这一难题，提高其生活质量和独立性。

Method: 采用YOLOv8 nano目标检测模型，结合深度卷积层和Squeeze-and-Excitation模块增强特征提取能力。模型在包含三国三十类币种的数据集上训练，并通过语音反馈实现识别结果提示。

Result: 模型在测试集上取得97.73%的准确率，95.23%的召回率，95.85%的F1分数，IoU=0.5时的平均精度（mAP50）为97.21%。

Conclusion: 该研究开发的货币检测系统运行高效、检测准确，并能通过语音反馈辅助视障人士独立识别与处理现金，具备现实应用价值。

Abstract: Technologies like smartphones have become an essential in our daily lives. It
has made accessible to everyone including visually impaired individuals. With
the use of smartphone cameras, image capturing and processing have become more
convenient. With the use of smartphones and machine learning, the life of
visually impaired can be made a little easier. Daily tasks such as handling
money without relying on someone can be troublesome for them. For that purpose
this paper presents a real-time currency detection system designed to assist
visually impaired individuals. The proposed model is trained on a dataset
containing 30 classes of notes and coins, representing 3 types of currency: US
dollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a
YOLOv8 nano model with a custom detection head featuring deep convolutional
layers and Squeeze-and-Excitation blocks to enhance feature extraction and
detection accuracy. Our model has achieved a higher accuracy of 97.73%, recall
of 95.23%, f1-score of 95.85% and a mean Average Precision at IoU=0.5
(mAP50(B)) of 97.21\%. Using the voice feedback after the detection would help
the visually impaired to identify the currency. This paper aims to create a
practical and efficient currency detection system to empower visually impaired
individuals independent in handling money.

</details>


### [39] [GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection](https://arxiv.org/abs/2510.20268)
*Guangyu Dai,Dong Chen,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: 提出了一种视频异常检测新方法（GMFVAD），通过细粒度多模态特征增强，减少特征冗余并提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法多只关注视觉时空相关性，近期虽引入文本等多模态特征，但融合方式粗糙，未能充分利用信息多样性和过滤冗余。

Method: 提出GMFVAD方法，从视频摘要生成更细粒度的多模态特征，并利用原始视频字幕提升视觉特征，仅对突出部分进一步优化，使特征冗余减少。

Result: 在四个主流数据集上实现了当前最佳的异常检测性能；消融实验表明性能提升主要源自特征冗余的有效减少。

Conclusion: GMFVAD方法能够有效利用多模态特征优势，通过细粒度处理减少冗余，提升视频异常检测的准确性。

Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous
frames in continuous surveillance videos. Most previous work utilizes the
spatio-temporal correlation of visual features to distinguish whether there are
abnormalities in video snippets. Recently, some works attempt to introduce
multi-modal information, like text feature, to enhance the results of video
anomaly detection. However, these works merely incorporate text features into
video snippets in a coarse manner, overlooking the significant amount of
redundant information that may exist within the video snippets. Therefore, we
propose to leverage the diversity among multi-modal information to further
refine the extracted features, reducing the redundancy in visual features, and
we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).
Specifically, we generate more grained multi-modal feature based on the video
snippet, which summarizes the main content, and text features based on the
captions of original video will be introduced to further enhance the visual
features of highlighted portions. Experiments show that the proposed GMFVAD
achieves state-of-the-art performance on four mainly datasets. Ablation
experiments also validate that the improvement of GMFVAD is due to the
reduction of redundant information.

</details>


### [40] [Causal Debiasing for Visual Commonsense Reasoning](https://arxiv.org/abs/2510.20281)
*Jiayi Zou,Gengyun Jia,Bing-Kun Bao*

Main category: cs.CV

TL;DR: 本文讨论了视觉常识推理(VCR)中的数据集偏见问题，提出新的OOD数据集并用因果推断和后门调整方法进行去偏，验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有VCR方法虽准确率高，但往往忽视了数据集中视觉和文本的偏见，缺乏系统性去偏措施，影响泛化能力，因此需要分析并应对这些偏见。

Method: 作者分析了VCR数据集中的共现和统计偏见，提出了新OOD测试集（包括VCR-OOD-QA和VCR-OOD-VA），设计用于测试模型对图文两种模态的泛化能力。同时，基于因果图分析预测捷径，采用后门调整(backdoor adjustment)方法进行去偏。具体地，通过正确答案集合建立词典，清除预测中的偏见捷径。

Result: 实验证明，作者的去偏方案在多个不同数据集上均能有效提升模型去偏能力和泛化表现。

Conclusion: 本文为视觉常识推理中的偏见分析和去偏方法带来了系统性进展，提出的数据集和方法对提升VCR模型真实场景的泛化能力具有价值。

Abstract: Visual Commonsense Reasoning (VCR) refers to answering questions and
providing explanations based on images. While existing methods achieve high
prediction accuracy, they often overlook bias in datasets and lack debiasing
strategies. In this paper, our analysis reveals co-occurrence and statistical
biases in both textual and visual data. We introduce the VCR-OOD datasets,
comprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate
the generalization capabilities of models across two modalities. Furthermore,
we analyze the causal graphs and prediction shortcuts in VCR and adopt a
backdoor adjustment method to remove bias. Specifically, we create a dictionary
based on the set of correct answers to eliminate prediction shortcuts.
Experiments demonstrate the effectiveness of our debiasing method across
different datasets.

</details>


### [41] [Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition](https://arxiv.org/abs/2510.20284)
*Haodong Yang,Zhongling Huang,Shaojie Guo,Zhe Zhang,Gong Cheng,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了一种知识驱动的神经网络（KINN），用于解决复杂值合成孔径雷达（CV-SAR）图像识别中的泛化、可解释性与效率三难问题，在多个基准数据集上取得了参数高效且可解释性的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在面对数据有限和领域转移时，难以同时兼顾泛化能力、可解释性和效率。CV-SAR数据中包含丰富的电磁散射信息，而常规数据驱动模型并未充分利用这些物理特征。

Method: 作者提出了一种“压缩-聚合-压缩”新架构：第一阶段利用物理先验进行数据压缩，通过新颖的字典处理器提取稀疏且有物理意义的特征；然后通过聚合模块丰富表示；最后借助自蒸馏的紧凑分类头完成语义压缩，学习最相关的判别特征。该方法可被应用于CNN和ViT等网络结构。

Result: KINN框架在五个SAR基准中，能够以极少参数（CNN仅0.7M, ViT仅0.95M）实现SOTA的识别性能，并且在数据稀缺和跨领域场景下表现出色。同时，具备良好的可解释性。

Conclusion: 所提出的KINN有效解决了CV-SAR图像识别的“三难问题”，为实现可信、可解释的SAR影像AI分析提供了新方向。

Abstract: Deep learning models for complex-valued Synthetic Aperture Radar (CV-SAR)
image recognition are fundamentally constrained by a representation trilemma
under data-limited and domain-shift scenarios: the concurrent, yet conflicting,
optimization of generalization, interpretability, and efficiency. Our work is
motivated by the premise that the rich electromagnetic scattering features
inherent in CV-SAR data hold the key to resolving this trilemma, yet they are
insufficiently harnessed by conventional data-driven models. To this end, we
introduce the Knowledge-Informed Neural Network (KINN), a lightweight framework
built upon a novel "compression-aggregation-compression" architecture. The
first stage performs a physics-guided compression, wherein a novel dictionary
processor adaptively embeds physical priors, enabling a compact unfolding
network to efficiently extract sparse, physically-grounded signatures. A
subsequent aggregation module enriches these representations, followed by a
final semantic compression stage that utilizes a compact classification head
with self-distillation to learn maximally task-relevant and discriminative
embeddings. We instantiate KINN in both CNN (0.7M) and Vision Transformer
(0.95M) variants. Extensive evaluations on five SAR benchmarks confirm that
KINN establishes a state-of-the-art in parameter-efficient recognition,
offering exceptional generalization in data-scarce and out-of-distribution
scenarios and tangible interpretability, thereby providing an effective
solution to the representation trilemma and offering a new path for trustworthy
AI in SAR image analysis.

</details>


### [42] [DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering](https://arxiv.org/abs/2510.20285)
*Jiayi Zou,Chaofan Chen,Bing-Kun Bao,Changsheng Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种Dual-Modal Counterfactual Contrastive Construction (DMC^3) 框架，通过反事实样本构建与对比学习，提升了第一人称视频问答（Egocentric VideoQA）任务的表现，取得了当前最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有第一人称视频问答方法多依赖预训练和微调，但忽略了该任务中独特的挑战，比如事件理解和手-物交互的识别。为应对这些挑战，需要设计更专门化的方法。

Method: 作者提出DMC^3框架，包括三个主要模块：基础的视频问答模型、反事实样本构建模块（利用事件描述的释义和交互信息挖掘，分别生成文本和视觉模态的正/负样本）、以及反事实样本参与的对比优化模块（通过对比损失拉近正样本、原始样本距离，拉远负样本距离）。

Result: 在EgoTaskQA数据集的normal和indirect两种划分上分别达到52.51%和46.04%，在QAEGO4D上达到13.2%，均为最新最优结果。

Conclusion: 所提方法有效提升了第一人称视频问答的性能，验证了反事实对比方法在理解多事件和手物交互方面的有效性。

Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.

</details>


### [43] [UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning](https://arxiv.org/abs/2510.20286)
*Liangyu Chen,Hanzhang Zhou,Chenglin Cai,Jianan Zhang,Panrong Tong,Quyu Kong,Xu Zhang,Chen Liu,Yuqi Liu,Wenxuan Wang,Yue Wang,Qin Jin,Steven Hoi*

Main category: cs.CV

TL;DR: 本文聚焦于GUI grounding任务，即将自然语言指令准确映射到可操作的界面元素，提出“以指令为推理”(Instruction-as-Reasoning)新范式。作者系统性分析了已有数据集，发现指令本身的多样性与质量严重影响模型表现，针对该问题提出了一种结合多视角SFT和强化学习优化的两阶段训练框架，实现多路径推理与自适应路径选择。新模型UI-Ins-7B/32B刷新多项基准测试纪录。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多把用户指令视为静态代理，忽视了指令多样性和指令质量对GUI grounding性能的影响。通过实证分析发现，数据集指令瑕疵多，且推理阶段利用不同表述可显著提升性能，迫切需要能主动利用指令多样性的推理范式。

Method: 提出将指令视为动态分析路径的新范式。训练分两阶段：先用合成多样化指令进行SFT训练，培养模型多视角推理能力；后用强化学习优化推理路径的选择与组合。最终模型不仅能自主选择最有效推理路径，还可在推理时创造新路径。

Result: UI-Ins-7B/32B模型在五项复杂基准测试上取得新SOTA：UI-I2E-Bench达87.3%、ScreenSpot-Pro达57.0%、MMBench-GUI L2达84.9%；UI-Ins-7B在AndroidWorld平台agent任务中实现74.1%成功率。同时分析证明该方法有效防止SFT+RL训练中的策略坍缩，有推理可解释性提升。

Conclusion: 论文证明多样、高质量指令对于GUI grounding极为重要。提出的以指令为推理的训练方法能极大提升AI在复杂界面上的推理和自主决策能力。方法与代码已开源，为后续智能GUI agent研究提供了新方向。

Abstract: GUI grounding, which maps natural-language instructions to actionable UI
elements, is a core capability of GUI agents. Prior works largely treats
instructions as a static proxy for user intent, overlooking the impact of
instruction diversity and quality on grounding performance. Through a careful
investigation of existing grounding datasets, we find a 23.3% flaw rate in
their instructions and show that inference-time exploitation of instruction
diversity yields up to a substantial 76% relative performance improvement. In
this paper, we introduce the Instruction-as-Reasoning paradigm, treating
instructions as dynamic analytical pathways that offer distinct perspectives
and enabling the model to select the most effective pathway during reasoning.
To achieve this, we propose a two-stage training framework: supervised
fine-tuning (SFT) on synthesized, diverse instructions to instill
multi-perspective reasoning, followed by reinforcement learning (RL) to
optimize pathway selection and composition. Our resulting models, UI-Ins-7B and
UI-Ins-32B, achieve state-of-the-art results on five challenging grounding
benchmarks and exhibit emergent reasoning, selectively composing and
synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B
attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on
ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model
demonstrates strong agentic potential, achieving a 74.1% success rate on
AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals
additional insights such as how reasoning can be formulated to enhance rather
than hinder grounding performance, and how our method mitigates policy collapse
in the SFT+RL framework. All code and model checkpoints will be publicly
released in https://github.com/alibaba/UI-Ins.

</details>


### [44] [Breakdance Video classification in the age of Generative AI](https://arxiv.org/abs/2510.20287)
*Sauptik Dhar,Naveen Ramakrishnan,Michelle Munson*

Main category: cs.CV

TL;DR: 本文探讨了大规模视觉语言模型在街舞（breakdance）视频中的表现，发现对于预测任务，视频编码器模型优于当前最先进的视频语言模型，并对其机制进行了分析。


<details>
  <summary>Details</summary>
Motivation: 绝大多数视觉语言模型的研究面向少数主流体育项目，且多关注生成任务，如视觉问答和精彩片段生成，而对于像街舞这样具有广泛受众的小众体育类别，相关研究较少。

Method: 作者评估了现代视频基础模型（包含编码器和解码器）在街舞视频任务中的适用性，通过实验对比了视频编码器与视频语言模型在分类等预测任务上的表现，并仔细分析了微调后的解码器模型的内部原理。

Result: 实验结果表明，视频编码器模型在街舞视频的预测任务中持续优于先进的视频语言模型。

Conclusion: 对于街舞等小众但热门的体育类型，现有视频编码器模型在分类等预测任务上的表现仍然优于融合语言的生成式模型，研究还对模型选择及解码器模型的细节机制提供了参考。

Abstract: Large Vision Language models have seen huge application in several sports
use-cases recently. Most of these works have been targeted towards a limited
subset of popular sports like soccer, cricket, basketball etc; focusing on
generative tasks like visual question answering, highlight generation. This
work analyzes the applicability of the modern video foundation models (both
encoder and decoder) for a very niche but hugely popular dance sports -
breakdance. Our results show that Video Encoder models continue to outperform
state-of-the-art Video Language Models for prediction tasks. We provide
insights on how to choose the encoder model and provide a thorough analysis
into the workings of a finetuned decoder model for breakdance video
classification.

</details>


### [45] [A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization](https://arxiv.org/abs/2510.20291)
*LinFeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了RoboSense 2025 Track 4赛事的冠军解决方案，任务为跨模态无人机导航，即根据自然语言查询从不同平台（卫星/无人机/地面）的海量地理标注图像中检索最相关图片。通过一系列预处理和多专家模型提升性能，并在官方排行榜中取得第一。


<details>
  <summary>Details</summary>
Motivation: 在实际无人机应用中，跨平台（如卫星、无人机与地面视角）图像间存在显著差异，同时测试时的文本描述又与训练集有领域差异，造成了导航和定位的难度，亟需有效跨模态检索方法。

Method: （1）提出领域对齐的预处理流程，包括按平台分组、卫星图像增强和剔除朝向词；（2）采用大语言模型精炼文本描述，使其语义能更好对齐不同平台的视觉特征；（3）基于BGE-M3文本模型和EVA-CLIP图像模型，针对不同平台训练三组专家模型，并采用两阶段的“困难负样本挖掘”提升判别力，最后在推理阶段融合各专家得分。

Result: 该方案在RoboSense 2025 Track 4官方排行榜上获得第一名，实验表明其在多视角、跨模态场景下具有强大的地理定位能力。

Conclusion: 结合领域对齐的预处理、文本精炼和专家模型融合，有效解决了异质视角和领域差异带来的挑战，为跨模态无人机导航提供了鲁棒的解决途径。

Abstract: We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone
Navigation. The task retrieves the most relevant geo-referenced image from a
large multi-platform corpus (satellite/drone/ground) given a natural-language
query. Two obstacles are severe inter-platform heterogeneity and a domain gap
between generic training descriptions and platform-specific test queries. We
mitigate these with a domain-aligned preprocessing pipeline and a
Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite
augmentation, and removal of orientation words; (ii) an LLM-based caption
refinement pipeline to align textual semantics with the distinct visual
characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we
train three platform experts using a progressive two-stage, hard-negative
mining strategy to enhance discriminative power, and fuse their scores at
inference. The system tops the official leaderboard, demonstrating robust
cross-modal geo-localization under heterogeneous viewpoints.

</details>


### [46] [HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models](https://arxiv.org/abs/2510.20322)
*Zelin Peng,Zhengqin Xu,Qingyang Liu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于双曲空间的高效多模态大模型训练方法（HyperET），旨在解决当前视觉编码器在多粒度语义对齐时的效率低下问题，有效提升视觉与文本的跨模态对齐能力，同时参数增量极低。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视觉-文本对齐上需消耗巨大的计算资源，导致训练成本高企。根本原因在于常用的视觉编码器（如CLIP、SAM）无法实现多粒度层次的对齐。为此，作者寻求从根本提升跨模态对齐效率。

Method: 作者引入双曲空间来建模和桥接视觉与文本的分层多粒度语义关系。具体提出了HyperET训练范式，通过动态调整双曲半径优化视觉表征，实现任意粒度的对齐。模型参数采用可学习矩阵，结合Möbius乘法，分为对角缩放、分块对角、带状三种配置，以实现高效灵活的参数化。

Result: 在多个多模态大模型基准上，HyperET在预训练与微调阶段均能明显提升现有模型性能，且仅增加不到1%的参数。

Conclusion: HyperET在极小参数增量下，显著提高了多模态大模型的视觉-文本对齐效果，为高效跨模态模型带来了新的范式，可大幅降低训练资源消耗。

Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative
approach for aligning visual and textual understanding. They typically require
extremely high computational resources (e.g., thousands of GPUs) for training
to achieve cross-modal alignment at multi-granularity levels. We argue that a
key source of this inefficiency lies in the vision encoders they widely equip
with, e.g., CLIP and SAM, which lack the alignment with language at
multi-granularity levels. To address this issue, in this paper, we leverage
hyperbolic space, which inherently models hierarchical levels and thus provides
a principled framework for bridging the granularity gap between visual and
textual modalities at an arbitrary granularity level. Concretely, we propose an
efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
visual representations to align with their textual counterparts at an arbitrary
granularity level through dynamic hyperbolic radius adjustment in hyperbolic
space. HyperET employs learnable matrices with M\"{o}bius multiplication
operations, implemented via three effective configurations: diagonal scaling
matrices, block-diagonal matrices, and banded matrices, providing a flexible
yet efficient parametrization strategy. Comprehensive experiments across
multiple MLLM benchmarks demonstrate that HyperET consistently improves both
existing pre-training and fine-tuning MLLMs clearly with less than 1\%
additional parameters.

</details>


### [47] [AnyPcc: Compressing Any Point Cloud with a Single Universal Model](https://arxiv.org/abs/2510.20331)
*Kangli Wang,Qianxi Yi,Yuqi Ye,Shihao Li,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了AnyPcc框架，以提升点云几何压缩中的泛化能力，通过多尺度上下文建模和数据自适应微调，实现了新的压缩性能上限。


<details>
  <summary>Details</summary>
Motivation: 深度学习点云压缩在泛化能力上表现不佳，主要是因为缺乏强健的上下文建模和对分布外数据（OOD）的低效处理。因此，作者试图解决上述两个核心问题，提升模型在多数据集上的压缩效果。

Method: 提出了Universal Context Model，将空间和通道分组的先验结合，增强上下文依赖建模能力。引入Instance-Adaptive Fine-Tuning（IAFT）策略，通过对每个样本微调部分网络参数并将其编码到比特流中，从而有效压缩OOD数据。参数微调消耗的比特远低于带来的压缩收益。

Result: 在15个多样化数据集基准下，AnyPcc取得了点云压缩领域的最新最优（state-of-the-art）性能表现。

Conclusion: AnyPcc框架有效提升了点云压缩的泛化能力和对OOD数据的适应性，为后续相关研究提供了新思路。代码及数据集将公开以促进可复现研究。

Abstract: Generalization remains a critical challenge for deep learning-based point
cloud geometry compression. We argue this stems from two key limitations: the
lack of robust context models and the inefficient handling of
out-of-distribution (OOD) data. To address both, we introduce AnyPcc, a
universal point cloud compression framework. AnyPcc first employs a Universal
Context Model that leverages priors from both spatial and channel-wise grouping
to capture robust contextual dependencies. Second, our novel Instance-Adaptive
Fine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit and
implicit compression paradigms. It fine-tunes a small subset of network weights
for each instance and incorporates them into the bitstream, where the marginal
bit cost of the weights is dwarfed by the resulting savings in geometry
compression. Extensive experiments on a benchmark of 15 diverse datasets
confirm that AnyPcc sets a new state-of-the-art in point cloud compression. Our
code and datasets will be released to encourage reproducible research.

</details>


### [48] [AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models](https://arxiv.org/abs/2510.20348)
*Seunghoon Lee,Jeongwoo Choi,Byunggwan Son,Jaehyeon Moon,Jeimin Jeon,Bumsub Ham*

Main category: cs.CV

TL;DR: 本文提出了名为AccuQuant的新型扩散模型后训练量化（PTQ）方法，通过减少多个去噪步骤累计的量化误差，使量化模型更接近全精度原模型，在多个基准测试上验证了其有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的量化方法会在每一步独立最小化量化误差，但实际采样中量化误差会在多步去噪过程中累计，导致结果退化，因此需要一种能考虑累积误差的量化方法。

Method: AccuQuant通过在量化时显式模拟扩散采样过程中的多个去噪步骤，最小化全精度模型与量化模型在多个步骤后的输出差异。此外，引入了一种高效实现方案和新目标函数，将内存复杂度从O(n)优化到O(1)。

Result: 在多种任务和基准上的实验证明，AccuQuant在提高量化扩散模型性能的同时大大提升了量化过程的效率和可扩展性。

Conclusion: AccuQuant有效缓解了扩散模型量化中的误差累积问题，实现了更高效、保真度更高的模型量化。

Abstract: We present in this paper a novel post-training quantization (PTQ) method,
dubbed AccuQuant, for diffusion models. We show analytically and empirically
that quantization errors for diffusion models are accumulated over denoising
steps in a sampling process. To alleviate the error accumulation problem,
AccuQuant minimizes the discrepancies between outputs of a full-precision
diffusion model and its quantized version within a couple of denoising steps.
That is, it simulates multiple denoising steps of a diffusion sampling process
explicitly for quantization, accounting the accumulated errors over multiple
denoising steps, which is in contrast to previous approaches to imitating a
training process of diffusion models, namely, minimizing the discrepancies
independently for each step. We also present an efficient implementation
technique for AccuQuant, together with a novel objective, which reduces a
memory complexity significantly from $\mathcal{O}(n)$ to $\mathcal{O}(1)$,
where $n$ is the number of denoising steps. We demonstrate the efficacy and
efficiency of AccuQuant across various tasks and diffusion models on standard
benchmarks.

</details>


### [49] [Positional Encoding Field](https://arxiv.org/abs/2510.20385)
*Yunpeng Bai,Haoxiang Li,Qixing Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的位置编码方法PE-Field，将二维位置编码扩展为结构化三维场，并应用于扩散Transformer（DiT），显著提升了单幅图像新视角合成和可控空间图像编辑任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Diffusion Transformer（DiT）通过位置编码（PE）与patch token结合实现视觉生成，但作者发现即使扰动位置编码，模型输出依然具有整体一致性，说明空间连贯性主要由位置编码决定，因此促使寻找更高维度、更结构化的位置编码以提升模型对空间和几何信息的建模能力。

Method: 提出Positional Encoding Field（PE-Field），将位置编码从2D扩展到3D场，结合了深度感知位置编码实现体积推理，并通过分层编码提升对微观子patch的控制，从而使DiT能直接在三维空间建模图像内容。

Result: 加入PE-Field的位置编码后，DiT在单幅图像的新视角合成任务上取得了最新最优性能，并能够泛化用于可控空间的图像编辑。

Conclusion: PE-Field为视觉生成中的空间建模带来了新进展，不仅提升了新视角合成的效果，还为复杂空间结构的可控编辑提供了有力工具，未来可推广至更多三维视觉生成任务。

Abstract: Diffusion Transformers (DiTs) have emerged as the dominant architecture for
visual generation, powering state-of-the-art image and video models. By
representing images as patch tokens with positional encodings (PEs), DiTs
combine Transformer scalability with spatial and temporal inductive biases. In
this work, we revisit how DiTs organize visual content and discover that patch
tokens exhibit a surprising degree of independence: even when PEs are
perturbed, DiTs still produce globally coherent outputs, indicating that
spatial coherence is primarily governed by PEs. Motivated by this finding, we
introduce the Positional Encoding Field (PE-Field), which extends positional
encodings from the 2D plane to a structured 3D field. PE-Field incorporates
depth-aware encodings for volumetric reasoning and hierarchical encodings for
fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D
space. Our PE-Field-augmented DiT achieves state-of-the-art performance on
single-image novel view synthesis and generalizes to controllable spatial image
editing.

</details>


### [50] [Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval](https://arxiv.org/abs/2510.20393)
*Qing Wang,Chong-Wah Ngo,Yu Cao,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的因果方法，通过预测图像中无法直观看出的烹饪元素，并将其融合到跨模态表示学习中，以提升图像-食谱检索的准确性。实验显示该方法能有效捕捉细微食材与烹饪动作，大幅提升了检索结果，尤其适用于多语言多文化的食谱数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图像-食谱检索方法假设食物图片能完全反映食谱中所有细节，实际却忽视了许多无法通过图片展现的关键信息，比如食材使用和烹饪步骤的细微区别，尤其在跨文化数据混合时这一偏差更为严重。

Method: 作者提出因果推断方法，能够预测图像中被忽略的烹饪元素，并将这些元素显式地加入到跨模态表示学习过程中，从而缓解表征偏差，提升相似食谱的区分性。方法在标准单语Recipe1M数据集及新构建的多语言多文化食谱数据集上进行验证。

Result: 实验结果表明，该因果表示学习方法能够挖掘出细粒度的食材和烹饪动作信息，无论在单语还是多语言多文化数据集上，检索性能均优于现有方法。

Conclusion: 该方法有效缓解了传统图像-食谱检索表示的偏差问题，提升了跨模态检索的准确度，特别适合处理包含巨大多样性的多文化食谱数据。

Abstract: Existing approaches for image-to-recipe retrieval have the implicit
assumption that a food image can fully capture the details textually documented
in its recipe. However, a food image only reflects the visual outcome of a
cooked dish and not the underlying cooking process. Consequently, learning
cross-modal representations to bridge the modality gap between images and
recipes tends to ignore subtle, recipe-specific details that are not visually
apparent but are crucial for recipe retrieval. Specifically, the
representations are biased to capture the dominant visual elements, resulting
in difficulty in ranking similar recipes with subtle differences in use of
ingredients and cooking methods. The bias in representation learning is
expected to be more severe when the training data is mixed of images and
recipes sourced from different cuisines. This paper proposes a novel causal
approach that predicts the culinary elements potentially overlooked in images,
while explicitly injecting these elements into cross-modal representation
learning to mitigate biases. Experiments are conducted on the standard
monolingual Recipe1M dataset and a newly curated multilingual multicultural
cuisine dataset. The results indicate that the proposed causal representation
learning is capable of uncovering subtle ingredients and cooking actions and
achieves impressive retrieval performance on both monolingual and multilingual
multicultural datasets.

</details>


### [51] [Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment](https://arxiv.org/abs/2510.20438)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出了一种融合模糊逻辑动态知识蒸馏的新型肺癌分类模型FuzzyDistillViT-MobileNet，在多个数据集上取得了超过99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 肺癌诊断涉及的不确定性和复杂性较高，传统的知识蒸馏方法采用固定权重，难以应对医学图像中不同区域的不确定性。因此需开发能够动态调整知识传递权重、适应图像区域不确定性的高效模型。

Method: 使用ViT-B32作为教师模型，MobileNet作为学生模型。通过模糊逻辑动态调整知识蒸馏权重，使学生模型能专注于高置信度区域，减少对不确定区域的注意。同时，利用Gamma校正和直方图均衡化提升图像质量，并通过小波融合方法增强特征表达。用遗传算法从12个候选模型中选择最优预训练学生模型，综合考虑准确率与计算效率。

Result: 在LC25000病理图像数据集和IQOTH/NCCD CT影像数据集上，模型分别达到了99.16%和99.54%的分类准确率，表现出良好的跨领域鲁棒性。

Conclusion: 动态模糊逻辑知识蒸馏结合多尺度图像融合、遗传算法模型选择等方法，有效提升了肺癌分类的精度和泛化能力，适用于不同类型医学影像场景，且兼顾了计算效率。

Abstract: This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for
lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven
knowledge distillation (KD) to address uncertainty and complexity in disease
diagnosis. Unlike traditional models that rely on static KD with fixed weights,
our method dynamically adjusts the distillation weight using fuzzy logic,
enabling the student model to focus on high-confidence regions while reducing
attention to ambiguous areas. This dynamic adjustment improves the model
ability to handle varying uncertainty levels across different regions of LC
images. We employ the Vision Transformer (ViT-B32) as the instructor model,
which effectively transfers knowledge to the student model, MobileNet,
enhancing the student generalization capabilities. The training process is
further optimized using a dynamic wait adjustment mechanism that adapts the
training procedure for improved convergence and performance. To enhance image
quality, we introduce pixel-level image fusion improvement techniques such as
Gamma correction and Histogram Equalization. The processed images (Pix1 and
Pix2) are fused using a wavelet-based fusion method to improve image resolution
and feature preservation. This fusion method uses the wavedec2 function to
standardize images to a 224x224 resolution, decompose them into multi-scale
frequency components, and recursively average coefficients at each level for
better feature representation. To address computational efficiency, Genetic
Algorithm (GA) is used to select the most suitable pre-trained student model
from a pool of 12 candidates, balancing model performance with computational
cost. The model is evaluated on two datasets, including LC25000
histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images
(99.54% accuracy), demonstrating robustness across different imaging domains.

</details>


### [52] [Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence](https://arxiv.org/abs/2510.20470)
*Kun Ouyang,Yuanxin Liu,Linli Yao,Yishuo Cai,Hao Zhou,Jie Zhou,Fandong Meng,Xu Sun*

Main category: cs.CV

TL;DR: 本文提出了Conan框架，针对多模态大模型在多步视频推理场景下的能力瓶颈，通过证据驱动的推理流程提升模型的准确性与鲁棒性，并在多个基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频推理中，RL方法易造成“幻觉”推理结果，基于检索的方法虽可实现视觉锚定，却难以准确定位关键证据，无法实现真正意义上的多步视觉推理。本文旨在通过结合证据溯源与多步推理，解决推理准确性与泛化能力的问题。

Method: 提出Conan框架，结合Frame识别与跨帧推理。1）自动构建Conan-91K大规模推理数据集，涵盖帧识别、证据推理与动作选择。2）设计多阶段逐步冷启动训练策略配合“识别-推理-行动（AIR）”RLVR训练框架，实现模型多步视觉推理能力提升。

Result: 在六个多步推理基准上，Conan的准确率较基线Qwen2.5-VL-7B-Instruct平均提升超过10%，达到SOTA水平；该框架还能有效泛化到长视频理解任务，展现出良好的可扩展性与鲁棒性。

Conclusion: Conan框架通过证据驱动的多步推理，有效提升了多模态大模型在视频推理任务中的准确性和泛化能力，为视频理解提供更合理的推理依据和可扩展解法。

Abstract: Video reasoning, which requires multi-step deduction across frames, remains a
major challenge for multimodal large language models (MLLMs). While
reinforcement learning (RL)-based methods enhance reasoning capabilities, they
often rely on text-only chains that yield ungrounded or hallucinated
conclusions. Conversely, frame-retrieval approaches introduce visual grounding
but still struggle with inaccurate evidence localization. To address these
challenges, we present Conan, a framework for evidence-grounded multi-step
video reasoning. Conan identifies contextual and evidence frames, reasons over
cross-frame clues, and adaptively decides when to conclude or explore further.
To achieve this, we (1) construct Conan-91K, a large-scale dataset of
automatically generated reasoning traces that includes frame identification,
evidence reasoning, and action decision, and (2) design a multi-stage
progressive cold-start strategy combined with an
Identification-Reasoning-Action (AIR) RLVR training framework to jointly
enhance multi-step visual reasoning. Extensive experiments on six multi-step
reasoning benchmarks demonstrate that Conan surpasses the baseline
Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving
state-of-the-art performance. Furthermore, Conan generalizes effectively to
long-video understanding tasks, validating its strong scalability and
robustness.

</details>


### [53] [Reliable and Reproducible Demographic Inference for Fairness in Face Analysis](https://arxiv.org/abs/2510.20482)
*Alexandre Fournier-Montgieux,Hervé Le Borgne,Adrian Popescu,Bertrand Luvison*

Main category: cs.CV

TL;DR: 本文提出了一种可重现的人口统计属性推断流程，以提高人脸分析系统中公平性评估的可靠性，并在准确性、公平性和鲁棒性三个维度上对方法进行了审计，实验结果优于现有强基线，尤其在民族属性上表现突出。


<details>
  <summary>Details</summary>
Motivation: 人脸分析系统公平性的评估高度依赖于人口统计属性的自动推断，但当前推断流程的可靠性不足，影响了公平性审计的有效性。因此，提升人口统计属性推断的可靠性对于公平性评估至关重要。

Method: 作者提出用可复现的流程替代传统端到端训练，采用模块化迁移学习框架，将预训练人脸识别编码器与非线性分类头结合。在准确性、公平性和新定义的鲁棒性（衡量同一身份内部一致性）三个方面，对流程进行全面评估，并在多个数据集及训练设置下对性别和民族推断进行基准测试。

Result: 新方法在性别和民族推断任务上均优于强基线，尤其在较难的民族推断任务中表现突出。同时引入的鲁棒性指标适用于任意人口划分方式。

Conclusion: 作者提出的方法为公平性审计中的人口统计属性推断奠定了更可靠的基础，并承诺公开全部相关代码和工具，以提升研究的透明度和可复现性。

Abstract: Fairness evaluation in face analysis systems (FAS) typically depends on
automatic demographic attribute inference (DAI), which itself relies on
predefined demographic segmentation. However, the validity of fairness auditing
hinges on the reliability of the DAI process. We begin by providing a
theoretical motivation for this dependency, showing that improved DAI
reliability leads to less biased and lower-variance estimates of FAS fairness.
To address this, we propose a fully reproducible DAI pipeline that replaces
conventional end-to-end training with a modular transfer learning approach. Our
design integrates pretrained face recognition encoders with non-linear
classification heads. We audit this pipeline across three dimensions: accuracy,
fairness, and a newly introduced notion of robustness, defined via
intra-identity consistency. The proposed robustness metric is applicable to any
demographic segmentation scheme. We benchmark the pipeline on gender and
ethnicity inference across multiple datasets and training setups. Our results
show that the proposed method outperforms strong baselines, particularly on
ethnicity, which is the more challenging attribute. To promote transparency and
reproducibility, we will publicly release the training dataset metadata, full
codebase, pretrained models, and evaluation toolkit. This work contributes a
reliable foundation for demographic inference in fairness auditing.

</details>


### [54] [EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization](https://arxiv.org/abs/2510.20512)
*Yixiong Yang,Tao Wu,Senmao Li,Shiqi Yang,Yaxing Wang,Joost van de Weijer,Kai Wang*

Main category: cs.CV

TL;DR: 本文提出了EchoDistill框架，通过双向蒸馏方式，实现了文本到图像（T2I）扩散模型在一步内实现个性化生成，提升了对新概念的学习和生成效果。


<details>
  <summary>Details</summary>
Motivation: 虽然T2I单步扩散模型能够快速生成高质量图片，但其在个性化、学习新颖概念时能力有限。因此，提升单步模型对新概念的表达和适应性成为研究难点。

Method: 作者提出EchoDistill方法，让多步扩散模型（教师）与单步模型（学生）端到端协同训练：将教师的知识蒸馏给学生后，再由学生‘回声’反馈给教师。过程中共享文本编码器，保证语义一致，同时通过对抗损失和一致性损失优化学生模型，还提出了双向回声细化策略，利用学生模型的高速生成能力反哺教师。

Result: 实验表明，EchoDistill框架下的单步扩散模型在新概念个性化和生成质量上，明显优于现有个性化方法。教师模型的生成质量也获得提升。

Conclusion: EchoDistill开创了T2I扩散模型快速有效个性化的新范式，为单步扩散模型在实际个性化应用中提供了更强的能力和更高的生成质量。

Abstract: Recent advances in accelerating text-to-image (T2I) diffusion models have
enabled the synthesis of high-fidelity images even in a single step. However,
personalizing these models to incorporate novel concepts remains a challenge
due to the limited capacity of one-step models to capture new concept
distributions effectively. We propose a bidirectional concept distillation
framework, EchoDistill, to enable one-step diffusion personalization (1-SDP).
Our approach involves an end-to-end training process where a multi-step
diffusion model (teacher) and a one-step diffusion model (student) are trained
simultaneously. The concept is first distilled from the teacher model to the
student, and then echoed back from the student to the teacher. During the
EchoDistill, we share the text encoder between the two models to ensure
consistent semantic understanding. Following this, the student model is
optimized with adversarial losses to align with the real image distribution and
with alignment losses to maintain consistency with the teacher's output.
Furthermore, we introduce the bidirectional echoing refinement strategy,
wherein the student model leverages its faster generation capability to
feedback to the teacher model. This bidirectional concept distillation
mechanism not only enhances the student ability to personalize novel concepts
but also improves the generative quality of the teacher model. Our experiments
demonstrate that this collaborative framework significantly outperforms
existing personalization methods over the 1-SDP setup, establishing a novel
paradigm for rapid and effective personalization in T2I diffusion models.

</details>


### [55] [Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning](https://arxiv.org/abs/2510.20519)
*Xiaohan Lan,Fanfan Liu,Haibo Qiu,Siqi Yang,Delian Ruan,Peng Shi,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了Metis-HOME混合专家模型（MoE），一种多专家分支结构，有效兼顾复杂推理能力和一般问题的理解能力，解决了多模态大模型推理效率低、泛化能力受损的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型推理模型虽然在复杂推理任务上取得进步，但往往推理开销大，无论问题难易都采用重型推理，同时其专注于推理导致对一般任务的泛化能力减弱。

Method: 提出Metis-HOME结构，通过"混合思考"方式将原密集模型拆分为两个专家分支：专用于复杂推理的思考分支和针对一般任务（如VQA/OCR）快速推理的非思考分支，并用轻量可训练的路由器根据输入动态分配任务。以Qwen2.5-VL-7B为基础实现MoE架构。

Result: 大量实验表明该方法显著提升了复杂推理能力，同时也增强了一般理解能力，逆转了以往注重推理模型在泛化性能上的下降趋势。

Conclusion: Metis-HOME为构建兼具强大推理力和广泛泛化能力的多模态大模型提供了新范式，有效解决了推理与泛化能力难两全的难题。

Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal
reasoning has seen remarkable progress, achieving significant performance gains
on intricate tasks such as mathematical problem-solving. Despite this progress,
current multimodal large reasoning models exhibit two key limitations. They
tend to employ computationally expensive reasoning even for simple queries,
leading to inefficiency. Furthermore, this focus on specialized reasoning often
impairs their broader, more general understanding capabilities. In this paper,
we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
structuring the original dense model into two distinct expert branches: a
thinking branch tailored for complex, multi-step reasoning, and a non-thinking
branch optimized for rapid, direct inference on tasks like general VQA and OCR.
A lightweight, trainable router dynamically allocates queries to the most
suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
an MoE architecture. Comprehensive evaluations reveal that our approach not
only substantially enhances complex reasoning abilities but also improves the
model's general capabilities, reversing the degradation trend observed in other
reasoning-specialized models. Our work establishes a new paradigm for building
powerful and versatile MLLMs, effectively resolving the prevalent
reasoning-vs-generalization dilemma.

</details>


### [56] [Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis](https://arxiv.org/abs/2510.20531)
*Lixiong Qin,Yang Zhang,Mei Wang,Jiani Hu,Weihong Deng,Weiran Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Fake-in-Facext（FiFa）的新型多模态大模型框架，聚焦于细粒度的可解释DeepFake分析，提升伪造检测的精确性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前DeepFake可解释分析普遍存在注释不够细致、伪造解释与视觉证据连接弱、无法针对任意人脸区域查询等问题，导致分析结果不够可靠和细粒度。

Method: 提出了人脸图像概念树（FICT），将人脸图像细分为多个区域概念，实现更精准的数据注释。构建FiFa-Annotator标注流程，并设计新任务Artifact-Grounding Explanation（AGE），在文本伪造解释中嵌入伪造区域分割掩码。最终提出统一多任务学习架构FiFa-MLLM，支持多模态输入输出，进行高效细粒度分析。

Result: FiFa-MLLM模型在AGE任务上优于多个强基线，并在现有可解释DeepFake数据集上取得了SOTA性能，多项辅助任务进一步提升了整体表现。

Conclusion: FiFa框架通过精细化注释和多模态深度解释，有效增强了DeepFake分析的可解释性和实用性，为后续相关研究提供了基础和数据支持。

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has bridged the
gap between vision and language tasks, enabling the implementation of
Explainable DeepFake Analysis (XDFA). However, current methods suffer from a
lack of fine-grained awareness: the description of artifacts in data annotation
is unreliable and coarse-grained, and the models fail to support the output of
connections between textual forgery explanations and the visual evidence of
artifacts, as well as the input of queries for arbitrary facial regions. As a
result, their responses are not sufficiently grounded in Face Visual Context
(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)
framework, with contributions focusing on data annotation and model
construction. We first define a Facial Image Concept Tree (FICT) to divide
facial images into fine-grained regional concepts, thereby obtaining a more
reliable data annotation pipeline, FiFa-Annotator, for forgery explanation.
Based on this dedicated data annotation, we introduce a novel
Artifact-Grounding Explanation (AGE) task, which generates textual forgery
explanations interleaved with segmentation masks of manipulated artifacts. We
propose a unified multi-task learning architecture, FiFa-MLLM, to
simultaneously support abundant multimodal inputs and outputs for fine-grained
Explainable DeepFake Analysis. With multiple auxiliary supervision tasks,
FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA
performance on existing XDFA datasets. The code and data will be made
open-source at https://github.com/lxq1000/Fake-in-Facext.

</details>


### [57] [Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image](https://arxiv.org/abs/2510.20539)
*Guillermo Carbajal,Andrés Almansa,Pablo Musé*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的新方法，可从单张模糊图像联合估计清晰图像和相机运动轨迹，显著提升了复杂运动模糊下的复原能力。


<details>
  <summary>Details</summary>
Motivation: 当前图像去模糊任务在应对由大幅度或旋转等复杂相机运动引起的运动模糊时仍面临巨大挑战，现有端到端去模糊网络表现不佳，且缺乏对成像运动过程的解释能力。

Method: 提出了基于项目化运动模糊模型（PMBM）的方法，利用可微分的模糊生成模块集成到神经网络中，实现网络预测完整的3D相机旋转轨迹。该轨迹指导模型化的复原网络，联合训练并可解释成像运动过程，还通过推理后利用reblur loss进一步细化恢复效果，提升输入输出一致性。

Result: 实验证明无论在合成数据还是真实数据集上，尤其是模糊严重和空间变异性强的场景，所提方法均超越了现有主流去模糊算法和端到端网络，达到最佳性能。

Conclusion: 该工作不仅提升了复杂模糊条件下的图像复原精度，还赋予了模型对模糊成因（即相机运动轨迹）的解释性，为进一步重建清晰图像序列奠定了基础。

Abstract: Motion blur caused by camera shake, particularly under large or rotational
movements, remains a major challenge in image restoration. We propose a deep
learning framework that jointly estimates the latent sharp image and the
underlying camera motion trajectory from a single blurry image. Our method
leverages the Projective Motion Blur Model (PMBM), implemented efficiently
using a differentiable blur creation module compatible with modern networks. A
neural network predicts a full 3D rotation trajectory, which guides a
model-based restoration network trained end-to-end. This modular architecture
provides interpretability by revealing the camera motion that produced the
blur. Moreover, this trajectory enables the reconstruction of the sequence of
sharp images that generated the observed blurry image. To further refine
results, we optimize the trajectory post-inference via a reblur loss, improving
consistency between the blurry input and the restored output. Extensive
experiments show that our method achieves state-of-the-art performance on both
synthetic and real datasets, particularly in cases with severe or spatially
variant blur, where end-to-end deblurring networks struggle.
  Code and trained models are available at
https://github.com/GuillermoCarbajal/Blur2Seq/

</details>


### [58] [From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging](https://arxiv.org/abs/2510.20550)
*Fuchen Li,Yansong Du,Wenbo Cheng,Xiaoxia Zhou,Sen Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为ACamera-Net的轻量化、自适应场景的相机参数调整神经网络，能够在各种复杂光照条件下优化图像质量，有效提升后续视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 消费级相机在低光、高动态范围、逆光以及空间色温变化等复杂光照环境下，常常难以保持稳定的图像质量，导致曝光不足、色偏和色调不一致，影响视觉任务效果。针对这些问题，迫切需要能够智能调整相机参数的自动方法。

Method: 提出了ACamera-Net，由两个模块组成：ACamera-Exposure（预测ISO、缓解曝光不足和对比度损失）和ACamera-Color（预测色温和增益，改善色彩一致性）。该网络从RAW图像直接预测最佳曝光和白平衡参数，适用于边缘设备的实时推理，通过带有标注参考的大规模真实数据训练，提升泛化能力。

Result: 大量实验表明，ACamera-Net在各种复杂光照情况下能够持续提升图像质量和感知算法输出的稳定性，明显优于传统自动模式和同类轻量化基线方法，并且不依赖额外的图像增强模块。

Conclusion: ACamera-Net能够作为成像流程中的实用组件，在保证轻量化和实时性的同时，显著优化消费级相机在不同场景下的成像效果，对实际应用的视觉任务具有很大提升价值。

Abstract: Consumer-grade camera systems often struggle to maintain stable image quality
under complex illumination conditions such as low light, high dynamic range,
and backlighting, as well as spatial color temperature variation. These issues
lead to underexposure, color casts, and tonal inconsistency, which degrade the
performance of downstream vision tasks. To address this, we propose
ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment
network that directly predicts optimal exposure and white balance from RAW
inputs. The framework consists of two modules: ACamera-Exposure, which
estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color,
which predicts correlated color temperature and gain factors for improved color
consistency. Optimized for real-time inference on edge devices, ACamera-Net can
be seamlessly integrated into imaging pipelines. Trained on diverse real-world
data with annotated references, the model generalizes well across lighting
conditions. Extensive experiments demonstrate that ACamera-Net consistently
enhances image quality and stabilizes perception outputs, outperforming
conventional auto modes and lightweight baselines without relying on additional
image enhancement modules.

</details>


### [59] [From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail](https://arxiv.org/abs/2510.20558)
*Xiaohan Sun,Carol O'Sullivan*

Main category: cs.CV

TL;DR: 本文研究了人群角色在不同细节层级和观看距离下的视觉质量感知，分析了几种主流表示方式的优缺点。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟人群渲染需求的增加，如何在保证视觉质量的同时提升渲染效率成为重要问题。不同的几何或神经表观技术对于视觉效果和性能的影响尚不清晰。

Method: 作者对几何网格、基于图像的Impostor、NeRFs和3D高斯四种人群表示方法，分别在不同细节层级和观看距离下进行对比。通过主观和客观实验，定性和定量分析了它们的视觉质量和计算性能。

Result: 实验结果揭示，不同表示方法在视觉保真度与计算效率间存在明显权衡。具体方法在不同场景和距离下表现优劣各异，给设计细节层级策略提供了量化依据。

Conclusion: 本文的结论为人群渲染中，如何平衡表达方法的视觉质量与性能成本，并制定感知优化的细节层级策略，提供了有益指导和实证支持。

Abstract: In this paper, we investigate how users perceive the visual quality of crowd
character representations at different levels of detail (LoD) and viewing
distances. Each representation: geometric meshes, image-based impostors, Neural
Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between
visual fidelity and computational performance. Our qualitative and quantitative
results provide insights to guide the design of perceptually optimized LoD
strategies for crowd rendering.

</details>


### [60] [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://arxiv.org/abs/2510.20579)
*Jiahao Meng,Xiangtai Li,Haochen Wang,Yue Tan,Tao Zhang,Lingdong Kong,Yunhai Tong,Anran Wang,Zhiyang Teng,Yujing Wang,Zhuochen Wang*

Main category: cs.CV

TL;DR: Open-o3 Video提出了一种融合显式时空证据的视频推理框架，并通过高质量数据集和强化学习，显著提升了多项视频理解基准的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理模型仅能生成文本推理过程，无法指明关键证据出现的时间和位置。相比静态图像，视频因需要时空联合定位，推理难度更大。

Method: 提出Open-o3 Video框架，能生成含有关键时间戳、目标物体及框选的答案推理过程；为此自构两个高质量含时空注释数据集，并采用强化学习鼓励答案正确性、时间对齐及空间精准度。

Result: Open-o3 Video在V-STAR基准上mAM提升14.4%、mLGM提升24.2%，在VideoMME、WorldSense等多个视频理解基准上也取得全面性能提升。

Conclusion: Open-o3 Video不仅提高了视频推理准确率，其可解释推理路径还提升了模型输出的可靠性和可验证性。

Abstract: Most video reasoning models only generate textual reasoning traces without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
temporal tracking and spatial localization across dynamic scenes. We introduce
Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
evidence into video reasoning, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling confidence-aware verification and
improving answer reliability.

</details>


### [61] [GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models](https://arxiv.org/abs/2510.20586)
*Muhammad Atif Butt,Alexandra Gomez-Villa,Tao Wu,Javier Vazquez-Corral,Joost Van De Weijer,Kai Wang*

Main category: cs.CV

TL;DR: 本文提出了GenColorBench，这是一个专注于文本到图像生成中色彩精确控制的全新基准，用以系统评估当前大模型对颜色描述的理解和还原能力。


<details>
  <summary>Details</summary>
Motivation: 目前的文本到图像生成模型在解析和实现细致色彩控制方面表现有限，而现有评价体系也未能充分反映模型在色彩还原精度上的表现。因此，缺乏一套全面、系统测量颜色准确性的基准成为研究和应用落地的瓶颈。

Method: 作者提出GenColorBench基准，涵盖44,000个以颜色为核心的文本提示，涉及400多种颜色，并基于ISCC-NBS和CSS3/X11的色彩系统设计，包含更高精度的数值型颜色描述。通过感知评价与自动化评价相结合，深入揭示各主流文本到图像模型的色彩理解及表现差异。

Result: 利用GenColorBench对当前流行的文本到图像生成模型进行了评测，发现模型在不同色彩规则和描述下性能存在显著差异，体现了各模型对色彩规则的理解能力和主要失效场景。

Conclusion: GenColorBench基准有效弥补了文本到图像生成领域在颜色精确度评测上的空白，有助于推动模型在色彩可控性和精确还原方面的技术进步。基准将在论文接收后公开，有助于社区的后续研究和发展。

Abstract: Recent years have seen impressive advances in text-to-image generation, with
image generative or unified models producing high-quality images from text. Yet
these models still struggle with fine-grained color controllability, often
failing to accurately match colors specified in text prompts. While existing
benchmarks evaluate compositional reasoning and prompt adherence, none
systematically assess color precision. Color is fundamental to human visual
perception and communication, critical for applications from art to design
workflows requiring brand consistency. However, current benchmarks either
neglect color or rely on coarse assessments, missing key capabilities such as
interpreting RGB values or aligning with human expectations. To this end, we
propose GenColorBench, the first comprehensive benchmark for text-to-image
color generation, grounded in color systems like ISCC-NBS and CSS3/X11,
including numerical colors which are absent elsewhere. With 44K color-focused
prompts covering 400+ colors, it reveals models' true capabilities via
perceptual and automated assessments. Evaluations of popular text-to-image
models using GenColorBench show performance variations, highlighting which
color conventions models understand best and identifying failure modes. Our
GenColorBench assessments will guide improvements in precise color generation.
The benchmark will be made public upon acceptance.

</details>


### [62] [Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation](https://arxiv.org/abs/2510.20596)
*Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于相似性原型的跨模态分割无监督域自适应新框架，通过学习类别原型并引入相似性约束，有效提升了模型在未知域的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在视觉任务上表现优异，但在遇到新域数据时，因域偏移导致性能显著下降。对于跨模态分割任务，获取新域(新模态)的标注代价很高，因此亟需无监督域自适应方法来解决此类问题。

Method: 方法核心是在嵌入空间内学习类别原型，并通过相似性约束使原型具有类别代表性和可区分性。引入原型字典机制，用于保存不同图像中提取的原型，避免类别缺失并促进原型的对比学习，从而提升分割效果。

Result: 大量实验证明，该方法在跨模态分割任务上优于其它现有最先进方法。

Conclusion: 相似性原型与原型字典的结合能够显著提高无监督跨模态分割的性能，是提升域自适应分割模型效果的有效途径。

Abstract: Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.

</details>


### [63] [OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects](https://arxiv.org/abs/2510.20605)
*Mark He Huang,Lin Geng Foo,Christian Theobalt,Ying Sun,De Wen Soh*

Main category: cs.CV

TL;DR: 本文提出了OnlineSplatter，一种能够实时重建自由运动物体3D高斯模型的在线方法，无需依赖相机姿态或深度信息，且在长序列中保持恒定计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频下自由运动物体的三维重建方法，常依赖姿态或深度先验，难以应对动态场景与缺乏精准位姿信息的情况，限制了实际应用。

Method: OnlineSplatter以视频首帧作为锚点，对每帧RGB图像以高斯图元密集场逐步细化物体三维表现，通过双重关键记忆模块融合当前帧与历史对象状态，利用方向和外观-几何显式关键实现空间引导记忆读取和稀疏化。整个方法端到端在线运行，无需优化相机位姿。

Result: 在真实数据集上，方法在内存和运行时间恒定前提下，重建精度大幅优于最新无姿态基线方案，随着观测帧数增多表现持续提升。

Conclusion: OnlineSplatter为单目动态场景下物体三维重建提供了一种高效、准确的在线新范式，解决了无需姿态与深度先验的难题，具备良好应用前景。

Abstract: Free-moving object reconstruction from monocular video remains challenging,
particularly without reliable pose or depth cues and under arbitrary object
motion. We introduce OnlineSplatter, a novel online feed-forward framework
generating high-quality, object-centric 3D Gaussians directly from RGB frames
without requiring camera pose, depth priors, or bundle optimization. Our
approach anchors reconstruction using the first frame and progressively refines
the object representation through a dense Gaussian primitive field, maintaining
constant computational cost regardless of video sequence length. Our core
contribution is a dual-key memory module combining latent appearance-geometry
keys with explicit directional keys, robustly fusing current frame features
with temporally aggregated object states. This design enables effective
handling of free-moving objects via spatial-guided memory readout and an
efficient sparsification mechanism, ensuring comprehensive yet compact object
coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter
significantly outperforms state-of-the-art pose-free reconstruction baselines,
consistently improving with more observations while maintaining constant memory
and runtime.

</details>


### [64] [SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding](https://arxiv.org/abs/2510.20622)
*Yuan Sheng,Yanbin Hao,Chenxu Li,Shuo Wang,Xiangnan He*

Main category: cs.CV

TL;DR: 提出SeViCES框架，通过语义-视觉的一致性选择关键帧，有效提升长视频理解的准确性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM在处理真正的长视频时易导致计算量高且推理分散，主流方法通常忽略时序依赖或只利用单一模态，导致上下文不完整、与查询相关性差。

Method: 提出SeViCES，包括两个主要模块：1）SVCFS模块，通过时序感知的语义分支结合LLM对字幕推理，以及视觉分支基于聚类和互信息，将视觉嵌入与语义分数对齐，选择有代表性的帧；2）ACR模块，将语义与视觉的推理结果进行融合，弥合不一致问题，约束答案空间。整个方法无需额外训练，对模型无依赖。

Result: 在长视频理解相关基准测试上，SeViCES在准确率和鲁棒性方面均优于最新方法。

Conclusion: 利用语义-视觉一致性进行证据选择显著提升了长视频理解任务的表现，对Video-LLM具有重要意义。

Abstract: Long video understanding remains challenging due to its complex, diverse, and
temporally scattered content. Although video large language models (Video-LLMs)
can process videos lasting tens of minutes, applying them to truly long
sequences is computationally prohibitive and often leads to unfocused or
inconsistent reasoning. A promising solution is to select only the most
informative frames, yet existing approaches typically ignore temporal
dependencies or rely on unimodal evidence, limiting their ability to provide
complete and query-relevant context. We propose a Semantic-Visual Consensus
Evidence Selection (SeViCES) framework for effective and reliable long video
understanding. SeViCES is training-free and model-agnostic, and introduces two
key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
selects frames through (1) a temporal-aware semantic branch that leverages LLM
reasoning over captions, and (2) a cluster-guided visual branch that aligns
embeddings with semantic scores via mutual information. The Answer Consensus
Refinement (ACR) module further resolves inconsistencies between semantic- and
visual-based predictions by fusing evidence and constraining the answer space.
Extensive experiments on long video understanding benchmarks show that SeViCES
consistently outperforms state-of-the-art methods in both accuracy and
robustness, demonstrating the importance of consensus-driven evidence selection
for Video-LLMs.

</details>


### [65] [Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges](https://arxiv.org/abs/2510.20634)
*Zhenhuan Zhou,Jingbo Zhu,Yuchen Zhang,Xiaohang Guan,Peng Wang,Tao Li*

Main category: cs.CV

TL;DR: 本论文综述了深度学习在牙科影像自动分析领域的应用，系统评述了相关数据集和算法研究进展。


<details>
  <summary>Details</summary>
Motivation: 传统的牙科影像分析因成像本身的低对比度、金属伪影、拍摄角度变化等问题，加上人工解读具有主观性和低效率，急需智能化、自动化的AI方法提升分析准确性和效率。

Method: 系统性回顾了近年深度学习在牙科影像自动分析中的研究，包括49篇公开数据集和211篇算法研究，从数据集特征、采集方法、深度学习基础模型、不同任务下的架构与优化以及评价指标等多角度分类总结，归纳并对比了主流方法的优缺点。

Result: 论文梳理并比较了主流DL在牙科影像分析各任务（如分割、检测等）上的表现，指出公开数据集、模型架构、性能指标和评测方法，并进行系统归纳。还总结了当前的研究挑战并指明了未来的发展方向。

Conclusion: 深度学习已成为牙科影像自动分析的核心技术，有力提升了自动诊断和治疗的效率与准确性。目前该领域在数据集、算法优化和实际应用等方面仍有挑战，本文的系统综述可为后续研究和实际应用提供重要参考。

Abstract: Efficient analysis and processing of dental images are crucial for dentists
to achieve accurate diagnosis and optimal treatment planning. However, dental
imaging inherently poses several challenges, such as low contrast, metallic
artifacts, and variations in projection angles. Combined with the subjectivity
arising from differences in clinicians' expertise, manual interpretation often
proves time-consuming and prone to inconsistency. Artificial intelligence
(AI)-based automated dental image analysis (DIA) offers a promising solution to
these issues and has become an integral part of computer-aided dental diagnosis
and treatment. Among various AI technologies, deep learning (DL) stands out as
the most widely applied and influential approach due to its superior feature
extraction and representation capabilities. To comprehensively summarize recent
progress in this field, we focus on the two fundamental aspects of DL
research-datasets and models. In this paper, we systematically review 260
studies on DL applications in DIA, including 49 papers on publicly available
dental datasets and 211 papers on DL-based algorithms. We first introduce the
basic concepts of dental imaging and summarize the characteristics and
acquisition methods of existing datasets. Then, we present the foundational
techniques of DL and categorize relevant models and algorithms according to
different DIA tasks, analyzing their network architectures, optimization
strategies, training methods, and performance. Furthermore, we summarize
commonly used training and evaluation metrics in the DIA domain. Finally, we
discuss the current challenges of existing research and outline potential
future directions. We hope that this work provides a valuable and systematic
reference for researchers in this field. All supplementary materials and
detailed comparison tables will be made publicly available on GitHub.

</details>


### [66] [Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging](https://arxiv.org/abs/2510.20639)
*Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Hadrien Reynaud,Dong Yang,Pengfei Guo,Marc Edgar,Daguang Xu,Bernhard Kainz,Bjoern Menze*

Main category: cs.CV

TL;DR: 该论文提出了BTB3D模型，通过更精细的三维切片编码，提升了3D医疗影像的视觉-语言任务表现，在报告生成和文本引导影像合成中均达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D医疗影像视觉-语言模型在处理高分辨率、长序列CT数据时存在编码失真和语义对齐差的问题，影响下游诊断性能。作者旨在解决高效三维特征表达和视觉-语言对齐瓶颈。

Method: 提出BTB3D模型，基于因果卷积的编码-解码框架，能够统一2D/3D训练和推理，并生成紧凑且频率敏感的体素token。采用三阶段训练：局部重建、重叠窗口切片、长上下文解码递进优化，使模型可处理超长CT序列且无额外内存消耗。

Result: 在报告生成任务上BTB3D的BLEU、F1等指标大幅超过CT2Rep、CT-CHAT、Merlin等，F1提升40%；在文本到CT合成任务上，FID降低75%，FVD减半，生成具解剖一致性的高分辨率512*512*241体积影像。

Conclusion: 精确的三维切片编码对3D医学视觉-语言建模至关重要，比单纯扩展语言模型更有效。BTB3D可显著提升医学影像相关任务表现。代码已开源。

Abstract: Recent progress in vision-language modeling for 3D medical imaging has been
fueled by large-scale computed tomography (CT) corpora with paired free-text
reports, stronger architectures, and powerful pretrained models. This has
enabled applications such as automated report generation and text-conditioned
3D image synthesis. Yet, current approaches struggle with high-resolution,
long-sequence volumes: contrastive pretraining often yields vision encoders
that are misaligned with clinical language, and slice-wise tokenization blurs
fine anatomy, reducing diagnostic performance on downstream tasks. We introduce
BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder
that unifies 2D and 3D training and inference while producing compact,
frequency-aware volumetric tokens. A three-stage training curriculum enables
(i) local reconstruction, (ii) overlapping-window tiling, and (iii)
long-context decoder refinement, during which the model learns from short slice
excerpts yet generalizes to scans exceeding 300 slices without additional
memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it
improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and
Merlin for report generation; and it reduces FID by 75% and halves FVD compared
to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically
consistent 512*512*241 volumes. These results confirm that precise
three-dimensional tokenization, rather than larger language backbones alone, is
essential for scalable vision-language modeling in 3D medical imaging. The
codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D

</details>


### [67] [UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset](https://arxiv.org/abs/2510.20661)
*Chen Zhao,En Ci,Yunzhe Xu,Tiehan Fan,Shanyan Guan,Yanhao Ge,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文针对超高分辨率文本到图像生成（UHR T2I）中的两个主要难题——缺乏高质量大规模数据集和细节生成方法不足——提出了新数据集UltraHR-100K和细节增强训练方法，有效提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有UHR T2I研究受到数据集不足和对超细节绘制关注不够的限制，需要更高质量的训练资源和更针对细节的生成策略。

Method: 1）构建了UltraHR-100K数据集，包含10万张分辨率超3K、内容丰富和美学质量高的标注图片；2）提出频域感知的后训练方法，包括：DOTS算法专注于细节关键的降噪时刻，SWFR方法利用离散傅里叶变换约束高频成分，强化细节还原。

Result: 在UltraHR-eval4K基准上，所提方法显著增强了T2I扩散模型对于超高分辨率细节与整体效果的表现，优于以往方法。

Conclusion: 本文通过提出新数据集和新训练策略，有效提升了UHR T2I生成中细节表现和图片真实性，为高分辨率AI制图领域带来了新的基础资源和方法。

Abstract: Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning
on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency
Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.

</details>


### [68] [HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification](https://arxiv.org/abs/2510.20669)
*Debojyoti Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 本文提出了一种结合卷积神经网络、自组织映射与类脉冲神经处理机制的新型混合深度学习框架HybridSOMSpikeNet，实现高效准确的垃圾分类，兼顾性能和能耗。该方法在十类垃圾数据集上取得97.39%的测试精度，优于多种先进模型，并适合现实部署。


<details>
  <summary>Details</summary>
Motivation: 城市化进程带来垃圾快速增长与分类不准确，增加了填埋负担和温室气体排放，影响可持续发展。现有自动垃圾分类方法存在精度和能耗上的不足，亟需创新高效、智能且可部署的技术手段。

Method: 所提HybridSOMSpikeNet框架先用预训练ResNet-152提取深度空间特征，再通过可微分软自组织映射（Soft-SOM）实现高效拓扑聚类和模型可解释性，最后引入仿脉冲机制的神经头部，对时序激活进行累计增强，提升模型鲁棒性与泛化能力。

Result: 在公开的十类垃圾图像数据集上，HybridSOMSpikeNet获得了97.39%的测试精度，超越了多种主流先进架构。同时，该模型结构轻量，计算资源消耗低，适合实际部署。

Conclusion: HybridSOMSpikeNet通过深度特征结合自组织以及神经时序处理，极大提升了垃圾分类的准确性与能效，对实际自动化分类具有推动作用，有助于推进全球循环经济与智慧环保目标，助力实现联合国可持续发展目标（SDG 11、SDG 12）。

Abstract: Accurate waste classification is vital for achieving sustainable waste
management and reducing the environmental footprint of urbanization.
Misclassification of recyclable materials contributes to landfill accumulation,
inefficient recycling, and increased greenhouse gas emissions. To address these
issues, this study introduces HybridSOMSpikeNet, a hybrid deep learning
framework that integrates convolutional feature extraction, differentiable
self-organization, and spiking-inspired temporal processing to enable
intelligent and energy-efficient waste classification. The proposed model
employs a pre-trained ResNet-152 backbone to extract deep spatial
representations, followed by a Differentiable Soft Self-Organizing Map
(Soft-SOM) that enhances topological clustering and interpretability. A spiking
neural head accumulates temporal activations over discrete time steps,
improving robustness and generalization. Trained on a ten-class waste dataset,
HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming several
state-of-the-art architectures while maintaining a lightweight computational
profile suitable for real-world deployment. Beyond its technical innovations,
the framework provides tangible environmental benefits. By enabling precise and
automated waste segregation, it supports higher recycling efficiency, reduces
contamination in recyclable streams, and minimizes the ecological and
operational costs of waste processing. The approach aligns with global
sustainability priorities, particularly the United Nations Sustainable
Development Goals (SDG 11 and SDG 12), by contributing to cleaner cities,
circular economy initiatives, and intelligent environmental management systems.

</details>


### [69] [Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling](https://arxiv.org/abs/2510.20673)
*Jinhee Kim,Jae Jun An,Kang Eun Jeon,Jong Hwan Ko*

Main category: cs.CV

TL;DR: 本论文提出了两种新方法，大幅减少多比特量化网络的训练开销，同时保持甚至提升精度。核心创新包括（1）权值偏置校正和（2）基于梯度重要性得分的比特级核心集采样。实验在多种主流数据集和网络下均获得优异结果，训练时间最高可减少7.88倍。


<details>
  <summary>Details</summary>
Motivation: 多比特量化网络支持单模型多精度推理，但现有方法需要为每个位宽重复全量数据训练，训练开销随着支持的精度线性增长，并且常常还要额外微调，导致训练负担极大。如何降低训练成本成为亟需解决的问题。

Method: 作者提出两项关键技术：1）权值偏置校正（weight bias correction），可消除量化带来的偏差，对齐多精度下的激活分布，无需为不同位宽单独微调。2）比特级核心集采样（bit-wise coreset sampling），基于梯度得分选取对每个位宽最重要的数据子集，借助隐式知识迁移，大大减少训练数据量。

Result: 在CIFAR-10/100、TinyImageNet、ImageNet-1K等主流数据集，以及ResNet和ViT两类架构上的实验表明，所提方法在保证甚至提升模型精度的同时，将训练时间减少了最高可达7.88倍。

Conclusion: 论证了两项新技术的有效性，实现了多精度量化网络的高效训练，极大降低了资源开销，为实际部署带来更高灵活性和实用价值。代码已开源，有望推动相关研究和应用。

Abstract: Multi-bit quantization networks enable flexible deployment of deep neural
networks by supporting multiple precision levels within a single model.
However, existing approaches suffer from significant training overhead as
full-dataset updates are repeated for each supported bit-width, resulting in a
cost that scales linearly with the number of precisions. Additionally, extra
fine-tuning stages are often required to support additional or intermediate
precision options, further compounding the overall training burden. To address
this issue, we propose two techniques that greatly reduce the training overhead
without compromising model utility: (i) Weight bias correction enables shared
batch normalization and eliminates the need for fine-tuning by neutralizing
quantization-induced bias across bit-widths and aligning activation
distributions; and (ii) Bit-wise coreset sampling strategy allows each child
model to train on a compact, informative subset selected via gradient-based
importance scores by exploiting the implicit knowledge transfer phenomenon.
Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and
ViT architectures demonstrate that our method achieves competitive or superior
accuracy while reducing training time up to 7.88x. Our code is released at
https://github.com/a2jinhee/EMQNet_jk.

</details>


### [70] [Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward](https://arxiv.org/abs/2510.20696)
*Jing Bi,Guangyu Sun,Ali Vosoughi,Chen Chen,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文针对多模态大模型在视觉-文本推理任务中出现的幻觉现象和过度依赖文本先验等问题，提出了一种结合轻量视觉模块与大模型推理的新型代理架构，并通过系统分析和评测显示，该方案显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）虽然能处理复杂的视觉文本任务，但常出现视觉幻觉和对文本先验的过度依赖，因此需要更系统的方法来评估和改进这些模型的推理过程。

Method: 提出三阶段评测框架系统剖析主流视觉语言模型失败类型，设计基于“代理”的新架构：结合轻量视觉模块与LLM推理，实现推理链的细粒度分析与迭代优化。

Result: 新系统在多个公开数据集上大幅提升表现（如MMMU提升10.3分，MathVista提升6.0分），性能媲美或超越体量更大的模型。

Conclusion: 未来多模态推理模型应更多整合专用视觉分析工具，该架构为此提供了有效实践，并将开源评测框架以促进相关研究。

Abstract: Multimodal large language models (MLLMs) that integrate visual and textual
reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual
tasks, yet continue to exhibit visual hallucinations and an over-reliance on
textual priors. We present a systematic diagnosis of state-of-the-art
vision-language models using a three-stage evaluation framework, uncovering key
failure modes. To address these, we propose an agent-based architecture that
combines LLM reasoning with lightweight visual modules, enabling fine-grained
analysis and iterative refinement of reasoning chains. Our results highlight
future visual reasoning models should focus on integrating a broader set of
specialized tools for analyzing visual content. Our system achieves significant
gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or
surpassing much larger models. We will release our framework and evaluation
suite to facilitate future research.

</details>


### [71] [Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models](https://arxiv.org/abs/2510.20707)
*Xuyang Liu,Xiyan Gui,Yuchao Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的KV缓存压缩方法MixKV，通过在大视觉语言模型(LVLMs)的KV缓存压缩中平衡重要性与多样性，缓解了因缓存爆炸式增长带来的内存瓶颈问题，在多个基准任务下显著提升了压缩效果和性能。


<details>
  <summary>Details</summary>
Motivation: 随着LVLMs处理多模态长序列能力提升，模型的KV缓存体积急剧增加，严重制约了模型大规模部署。已有方法注重KV对的“重要性”筛选，忽略了多模态缓存中不同Head的语义冗余特性，影响了压缩质量与模型表现。

Method: 作者分析了LVLMs中KV缓存在注意力头（head）级别的语义冗余现象，发现单纯按重要性采样不能全面覆盖KV信息分布。进而提出MixKV，结合重要性和多样性，适应不同head的语义分布，使用混合策略平衡多样性与重要性，在压缩KV对时保留了更多关键信息与互补信息。

Result: 通过在五项多模态理解任务和GUI定位任务上的实验证实，MixKV在极致压缩条件下平均提升基线方法5.1%，对SnapKV与AdaKV提升分别达8.0%、9.0%，且不影响推理效率。方法同样可推广到LLMs，取得类似性能提升。

Conclusion: MixKV有效缓解了LVLMs KV缓存的内存瓶颈问题，提升了多模态KV压缩效果和多项任务性能，并具备良好的通用性和效率，适合实际大规模部署。

Abstract: Recent large vision-language models (LVLMs) demonstrate remarkable
capabilities in processing extended multi-modal sequences, yet the resulting
key-value (KV) cache expansion creates a critical memory bottleneck that
fundamentally limits deployment scalability. While existing KV cache
compression methods focus on retaining high-importance KV pairs to minimize
storage, they often overlook the modality-specific semantic redundancy patterns
that emerge distinctively in multi-modal KV caches. In this work, we first
analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying
levels of redundancy across attention heads. We show that relying solely on
importance can only cover a subset of the full KV cache information
distribution, leading to potential loss of semantic coverage. To address this,
we propose \texttt{MixKV}, a novel method that mixes importance with diversity
for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise
semantic redundancy, selectively balancing diversity and importance when
compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}
consistently enhances existing methods across multiple LVLMs. Under extreme
compression (budget=64), \texttt{MixKV} improves baseline methods by an average
of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves
remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on
GUI grounding tasks, all while maintaining comparable inference efficiency.
Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable
performance gains. Our code is available at
\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

</details>


### [72] [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)
*Yuhan Liu,Lianhui Qin,Shengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了Speculative Verdict（SV）框架，结合多个小型专家的推理路径和一个大型模型以高效、高准确度处理信息密集型视觉问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型虽在多模态理解上取得显著进展，但面对含有大量文本标注和复杂图形细节的图片时难以有效定位关键信息并进行多步推理。这限制了它们在高信息密度场景下的表现。

Method: SV框架分为两阶段：起草阶段，由多个小型VLM生成多样化的推理路径作为候选；裁决阶段，一个大型VLM整合并最终输出答案。为提升效率和准确率，SV还引入了共识专家选择机制，只将高一致性的推理路径输入给裁决模型。该方法完全免训练，直接适配现有模型。

Result: 在多个高信息密度与高分辨率视觉问答基准（如InfographicVQA、ChartMuseum、ChartQAPro、HR-Bench 4K）上，SV在准确性和效率上均取得了显著提升。

Conclusion: SV框架能够在无需额外训练的情况下，通过多路径推理综合，实现高效、低误差的视觉问答推理，对比大型专有模型或需训练方法展现出成本和性能优势。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict

</details>


### [73] [AutoScape: Geometry-Consistent Long-Horizon Scene Generation](https://arxiv.org/abs/2510.20726)
*Jiacheng Chen,Ziyu Jiang,Mingfu Liang,Bingbing Zhuang,Jong-Chyi Su,Sparsh Garg,Ying Wu,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 本文提出了AutoScape，一个用于生成长时跨度驾驶场景的视频生成框架，依托新型的RGB-D扩散模型，能生成几何一致、真实的驾驶视频。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成领域，尤其是长时跨度（long-horizon）驾驶场景的高质量生成，面临几何一致性难以保持等挑战。以往的方法往往在时序上短缺、失真较多，难以满足真实感和连续性的高要求。

Method: 核心方法是提出了一种创新的RGB-D扩散模型，可在联合潜空间中处理图像和深度信息。模型通过：1）联合RGB图像与深度（D）信息、2）利用已有关键帧的点云信息优化生成过程、3）采用基于warp一致性的引导机制，增强采样阶段的几何保持能力。随后，使用视频扩散模型在高质量RGB-D关键帧之间插帧，生成连贯的视频。

Result: AutoScape能够生成超过20秒、真实且几何一致的驾驶场景视频。与最先进的已有方法相比，AutoScape在long-horizon FID和FVD分数上分别提升了48.6%和43.0%。

Conclusion: AutoScape在长时跨度、真实驾驶视频生成领域达到新水平，显著提升了视频的几何一致性和真实度，为真实场景模拟和自动驾驶等应用提供更优质的数据基础。

Abstract: This paper proposes AutoScape, a long-horizon driving scene generation
framework. At its core is a novel RGB-D diffusion model that iteratively
generates sparse, geometrically consistent keyframes, serving as reliable
anchors for the scene's appearance and geometry. To maintain long-range
geometric consistency, the model 1) jointly handles image and depth in a shared
latent space, 2) explicitly conditions on the existing scene geometry (i.e.,
rendered point clouds) from previously generated keyframes, and 3) steers the
sampling process with a warp-consistent guidance. Given high-quality RGB-D
keyframes, a video diffusion model then interpolates between them to produce
dense and coherent video frames. AutoScape generates realistic and
geometrically consistent driving videos of over 20 seconds, improving the
long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and
43.0\%, respectively.

</details>


### [74] [ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology](https://arxiv.org/abs/2510.20754)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN和ViT的注意力特征融合双编码器模型，提升了病理图像的语义分割表现。该方法在GCPS和PUMA公开数据集上均优于现有主流方法。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 自动化病理图像分析对于计算机辅助诊断非常关键，但现有方法在分割任务上仍有提升空间，特别是在特征表达和融合方面。作者旨在结合CNN和ViT的优点，解决分割准确率的问题。

Method: 提出了一种统一的双编码器模型：分别引入CNN和ViT提取多尺度特征，通过注意力机制进行特征融合，实现更有效的语义分割。模型在公开的数据集GCPS和PUMA上进行评测。

Result: 在GCPS数据集上，模型达到μIoU为76.79%，μDice为86.87%；在PUMA数据集上，μIoU为64.93%，μDice为76.60%，均优于当前SOTA方法和基线。

Conclusion: 该注意力特征融合双编码器模型能有效提升病理图像分割性能，在多个数据集上证明了其优越性，具有良好的实用前景。

Abstract: Automated histopathological image analysis plays a vital role in
computer-aided diagnosis of various diseases. Among developed algorithms, deep
learning-based approaches have demonstrated excellent performance in multiple
tasks, including semantic tissue segmentation in histological images. In this
study, we propose a novel approach based on attention-driven feature fusion of
convolutional neural networks (CNNs) and vision transformers (ViTs) within a
unified dual-encoder model to improve semantic segmentation performance.
Evaluation on two publicly available datasets showed that our model achieved
{\mu}IoU/{\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and
64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline
benchmarks. The implementation of our method is publicly available in a GitHub
repository: https://github.com/NimaTorbati/ACS-SegNet

</details>


### [75] [DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion](https://arxiv.org/abs/2510.20766)
*Noam Issachar,Guy Yariv,Sagie Benaim,Yossi Adi,Dani Lischinski,Raanan Fattal*

Main category: cs.CV

TL;DR: 本文提出了一种创新的方法DyPE，可以让预训练扩散Transformer在无须重新训练或增加采样成本的情况下，生成远超训练分辨率的高质量图像。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散Transformer模型能够生成高精度、细节丰富的图像，但由于自注意力机制的二次复杂度，直接在超高分辨率下训练和生成非常昂贵，因此急需一种高效的方法突破高分辨率生成的瓶颈。

Method: 作者提出了一种名为动态位置外推（DyPE）的训练无关方法。DyPE利用扩散生成过程中的频谱演化特性，通过根据生成阶段动态调整模型的位置信息编码，使其在每一步更好地匹配当前生成的频谱，促使模型在超高分辨率下也能稳定生成高质量图像。

Result: 在多项基准测试中，DyPE均显著提升了扩散Transformer在超高分辨率图像生成上的表现，能够生成高达1600万像素的图像，并刷新了该领域的生成保真度（SOTA）。性能提升在更高分辨率下更为突出。

Conclusion: DyPE为超高分辨率图像生成提供了训练友好且计算代价低的新方案，可广泛应用于实际超大图像生成任务，推动扩散Transformer模型的大规模场景落地。

Abstract: Diffusion Transformer models can generate images with remarkable fidelity and
detail, yet training them at ultra-high resolutions remains extremely costly
due to the self-attention mechanism's quadratic scaling with the number of
image tokens. In this paper, we introduce Dynamic Position Extrapolation
(DyPE), a novel, training-free method that enables pre-trained diffusion
transformers to synthesize images at resolutions far beyond their training
data, with no additional sampling cost. DyPE takes advantage of the spectral
progression inherent to the diffusion process, where low-frequency structures
converge early, while high-frequencies take more steps to resolve.
Specifically, DyPE dynamically adjusts the model's positional encoding at each
diffusion step, matching their frequency spectrum with the current stage of the
generative process. This approach allows us to generate images at resolutions
that exceed the training resolution dramatically, e.g., 16 million pixels using
FLUX. On multiple benchmarks, DyPE consistently improves performance and
achieves state-of-the-art fidelity in ultra-high-resolution image generation,
with gains becoming even more pronounced at higher resolutions. Project page is
available at https://noamissachar.github.io/DyPE/.

</details>


### [76] [AlphaFlow: Understanding and Improving MeanFlow Models](https://arxiv.org/abs/2510.20771)
*Huijie Zhang,Aliaksandr Siarohin,Willi Menapace,Michael Vasilkovsky,Sergey Tulyakov,Qing Qu,Ivan Skorokhodov*

Main category: cs.CV

TL;DR: 本文提出了α-Flow，一种统一且改进MeanFlow的生成建模目标，可更快更好地收敛，并在ImageNet上实现了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: MeanFlow作为一种few-step生成模型尽管强大，但其内部优化机制尚未完全被理解，存在收敛缓慢的问题。作者希望分析其原因并提出改进。

Method: 作者将MeanFlow目标分解为'轨迹流匹配'和'轨迹一致性'两部分，并通过梯度分析发现二者存在优化冲突。基于此，提出了α-Flow目标族，通过课程式训练策略，从轨迹流匹配平滑过渡到MeanFlow，从而解耦目标冲突。

Result: α-Flow在ImageNet-1K 256×256上使用原生DiT骨干和多种规模下，比MeanFlow有更快收敛、更好性能。最大规模模型α-Flow-XL/2+的FID达到2.58（1-NFE）和2.15（2-NFE），刷新SOTA。

Conclusion: α-Flow有效解决了MeanFlow的优化冲突问题，提高了few-step生成模型的收敛速度和生成质量，为相关模型设计提供了新思路。

Abstract: MeanFlow has recently emerged as a powerful framework for few-step generative
modeling trained from scratch, but its success is not yet fully understood. In
this work, we show that the MeanFlow objective naturally decomposes into two
parts: trajectory flow matching and trajectory consistency. Through gradient
analysis, we find that these terms are strongly negatively correlated, causing
optimization conflict and slow convergence. Motivated by these insights, we
introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory
flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting
a curriculum strategy that smoothly anneals from trajectory flow matching to
MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves
better convergence. When trained from scratch on class-conditional ImageNet-1K
256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms
MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla DiT backbones, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).

</details>


### [77] [CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image](https://arxiv.org/abs/2510.20776)
*Binbin Huang,Haobin Duan,Yiqun Zhao,Zibo Zhao,Yi Ma,Shenghua Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于生成式的3D重建方法Cupid，能够从单张2D图像中精确推断相机位姿、三维形状和纹理。该方法通过条件采样和联合生成体素及像素-体素对应关系，实现了更佳的姿态与形状估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前的3D重建方法在姿态、形状推断及细节呈现上存在精度和稳健性瓶颈，尤其在仅利用单幅2D图像进行推断时更为明显。为此，作者提出更高效、精确的生成式方法以突破上述限制。

Method: Cupid方法将3D重建视为从学习到的3D对象分布中的条件采样，采用两阶段流匹配流程：第一阶段粗略生成初始三维几何结构及其2D投影恢复相机位姿，第二阶段将与位姿对齐的图像特征整合细化结构与外观细节。同时，输入相机位姿和三维形状均表示为共享三维潜空间中的分布。

Result: 实验证明Cupid在多个指标上均优于主流3D重建方法，包括PSNR提升超3dB、Chamfer距离降低超10%，在姿态估计精度上与单目方法相当，且生成的三维模型视觉质量超越现有基线。

Conclusion: Cupid将三维重建和姿态估计统一于生成建模范式下，具备更高的准确率和细节还原能力，为基于单张图片的3D重建提供了新的高效途径。

Abstract: This work proposes a new generation-based 3D reconstruction method, named
Cupid, that accurately infers the camera pose, 3D shape, and texture of an
object from a single 2D image. Cupid casts 3D reconstruction as a conditional
sampling process from a learned distribution of 3D objects, and it jointly
generates voxels and pixel-voxel correspondences, enabling robust pose and
shape estimation under a unified generative framework. By representing both
input camera poses and 3D shape as a distribution in a shared 3D latent space,
Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that
produces initial 3D geometry with associated 2D projections for pose recovery;
and (2) a refinement stage that integrates pose-aligned image features to
enhance structural fidelity and appearance details. Extensive experiments
demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3
dB PSNR gain and an over 10% Chamfer Distance reduction, while matching
monocular estimators on pose accuracy and delivering superior visual fidelity
over baseline 3D generative models. For an immersive view of the 3D results
generated by Cupid, please visit cupid3d.github.io.

</details>


### [78] [Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature](https://arxiv.org/abs/2510.20794)
*Lei Cheng,Siyang Cao*

Main category: cs.CV

TL;DR: 本文提出了一种融合雷达与摄像头数据的多目标跟踪(MOT)框架，显著提高了跟踪效率并减少人工干预。通过在线标定与多传感器特征匹配，实现了精确的目标检测与跟踪，实验验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 许多现有工作仅将雷达数据作为辅助，未充分发挥其在三维坐标系中精准测距的优势。此外，针对雷达和摄像头数据的集成，人工干预较多、标定繁琐且融合精度受限。因此，本研究旨在创新性地将雷达的作用提升为核心，并利用通用特征实现自动高效的多目标跟踪。

Method: 1）提出了基于雷达与摄像头融合的MOT框架，通过在线标定简化两种传感器检测结果的集成。2）利用雷达与摄像头的通用特征，精准获取目标真实空间位置。3）采用特征匹配与类别一致性检查，超越单纯位置匹配，提高多传感器关联准确率。

Result: 本框架有效简化了雷达与摄像头之间的映射过程，并提升了跟踪精度。在受控环境和真实交通场景中的实验均验证了该方法的有效性。

Conclusion: 首次探索了雷达与摄像头通用特征的集成及其在线标定在多目标跟踪中的应用，显著提高了自动化程度和跟踪效果，为多传感器融合跟踪技术提供了新思路。

Abstract: This paper presents a Multi-Object Tracking (MOT) framework that fuses radar
and camera data to enhance tracking efficiency while minimizing manual
interventions. Contrary to many studies that underutilize radar and assign it a
supplementary role--despite its capability to provide accurate range/depth
information of targets in a world 3D coordinate system--our approach positions
radar in a crucial role. Meanwhile, this paper utilizes common features to
enable online calibration to autonomously associate detections from radar and
camera. The main contributions of this work include: (1) the development of a
radar-camera fusion MOT framework that exploits online radar-camera calibration
to simplify the integration of detection results from these two sensors, (2)
the utilization of common features between radar and camera data to accurately
derive real-world positions of detected objects, and (3) the adoption of
feature matching and category-consistency checking to surpass the limitations
of mere position matching in enhancing sensor association accuracy. To the best
of our knowledge, we are the first to investigate the integration of
radar-camera common features and their use in online calibration for achieving
MOT. The efficacy of our framework is demonstrated by its ability to streamline
the radar-camera mapping process and improve tracking precision, as evidenced
by real-world experiments conducted in both controlled environments and actual
traffic scenarios. Code is available at
https://github.com/radar-lab/Radar_Camera_MOT

</details>


### [79] [ARGenSeg: Image Segmentation with Autoregressive Image Generation Model](https://arxiv.org/abs/2510.20803)
*Xiaolong Wang,Lixiang Ru,Ziyuan Huang,Kaixiang Ji,Dandan Zheng,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于自回归生成的新型图像分割范式（ARGenSeg），通过让多模态大语言模型（MLLM）以视觉token进行像素级理解和分割，实现了高效、精细的图像分割和多模态理解。


<details>
  <summary>Details</summary>
Motivation: 现有将图像分割整合进多模态大模型的方案，大多依赖边界点或专用分割头，受限于离散化的表示或特定解码器提示，难以捕捉图像精细的视觉细节。因此，作者希望突破现有模式的瓶颈，提升多模态模型的像素级感知和理解能力。

Method: 作者提出基于图像生成的分割框架，让MLLM输出视觉token，然后用VQ-VAE将这些token反解码为图像，实现高密度掩膜生成。为减少推理时延，采用neXt-scale-prediction并行生成visual token。

Result: 在多个主流分割数据集上，该方法不仅推理速度显著快于现有方法，还取得了超越最新技术水平的分割表现，且保持了优秀的理解能力。

Conclusion: 提出的方法实现了高效、精细的分割及像素级理解，克服了现有多模态分割方法对分割细节捕捉不充分的问题，对多模态感知领域具有重要推动作用。

Abstract: We propose a novel AutoRegressive Generation-based paradigm for image
Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
perception within a unified framework. Prior works integrating image
segmentation into multimodal large language models (MLLMs) typically employ
either boundary points representation or dedicated segmentation heads. These
methods rely on discrete representations or semantic prompts fed into
task-specific decoders, which limits the ability of the MLLM to capture
fine-grained visual details. To address these challenges, we introduce a
segmentation framework for MLLM based on image generation, which naturally
produces dense masks for target objects. We leverage MLLM to output visual
tokens and detokenize them into images using an universal VQ-VAE, making the
segmentation fully dependent on the pixel-level understanding of the MLLM. To
reduce inference latency, we employ a next-scale-prediction strategy to
generate required visual tokens in parallel. Extensive experiments demonstrate
that our method surpasses prior state-of-the-art approaches on multiple
segmentation datasets with a remarkable boost in inference speed, while
maintaining strong understanding capabilities.

</details>


### [80] [Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers](https://arxiv.org/abs/2510.20807)
*Dean L Slack,G Thomas Hudson,Thomas Winterbottom,Noura Al Moubayed*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯Transformer的视频生成模型，不依赖复杂训练策略或潜在空间建模，在物理仿真视频的长时序精准预测上表现优越。


<details>
  <summary>Details</summary>
Motivation: 受自回归大语言模型成功经验启发，近年来Transformer也在视觉领域取得进展。而现有视频生成方法常在长时预测物理模拟方面表现不足。本文旨在通过对物理对象追踪等度量指标，提升Transformer对视频物理场景变化的时空建模能力。

Method: 作者提出了一种简单的端到端纯Transformer架构，采用连续像素空间表示，探索各种时空自注意力布局。对模型进行无监督训练，通过物理仿真数据集进行评价，并设计可解释性实验证明了模型编码区域对物理仿真参数估计的效果。

Result: 所提方法在无需复杂训练策略或特征学习部件的情况下，将物理场景中视频预测的时间预见窗口提升至比现有潜空间方法长50%，同时在常用视频质量指标上保持可比水平。此外，模型内部能泛化估计超出分布的仿真参数。

Conclusion: 本文的纯Transformer自回归视频预测模型参数高效、易解释，为后续深入研究自注意力时空视频建模提供了有效平台。

Abstract: Inspired by the performance and scalability of autoregressive large language
models (LLMs), transformer-based models have seen recent success in the visual
domain. This study investigates a transformer adaptation for video prediction
with a simple end-to-end approach, comparing various spatiotemporal
self-attention layouts. Focusing on causal modeling of physical simulations
over time; a common shortcoming of existing video-generative approaches, we
attempt to isolate spatiotemporal reasoning via physical object tracking
metrics and unsupervised training on physical simulation datasets. We introduce
a simple yet effective pure transformer model for autoregressive video
prediction, utilizing continuous pixel-space representations for video
prediction. Without the need for complex training strategies or latent
feature-learning components, our approach significantly extends the time
horizon for physically accurate predictions by up to 50% when compared with
existing latent-space approaches, while maintaining comparable performance on
common video quality metrics. In addition, we conduct interpretability
experiments to identify network regions that encode information useful to
perform accurate estimations of PDE simulation parameters via probing models,
and find that this generalizes to the estimation of out-of-distribution
simulation parameters. This work serves as a platform for further
attention-based spatiotemporal modeling of videos via a simple, parameter
efficient, and interpretable approach.

</details>


### [81] [SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution](https://arxiv.org/abs/2510.20814)
*Ritik Shah,Marco F Duarte*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SpectraMorph的融合框架，用于将低分辨率高光谱图像（HSI）与高分辨率多光谱或全色图像（MSI）融合，实现高光谱超分辨率，显著提升了结果解释性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 高光谱传感器空间分辨率低，导致边界模糊和混合像元问题，而多光谱或RGB等传感器具有高空间分辨率。因此，将两者融合实现高光谱超分辨率（HSI-MSI）具有重要应用价值。然而，现有深度学习方法解释性差，并且MSI波段极少时容易失效。

Method: 提出SpectraMorph物理引导的自监督融合框架。在结构化潜空间中，通过解混瓶颈约束：从HSI中提取端元光谱，用紧凑的MLP从MSI预测丰度图，再用线性混合重建高光谱数据，训练过程中利用MSI的光谱响应函数实现自监督。

Result: SpectraMorph框架能够产生易于解释的中间变量，训练速度快（1分钟内），即使MSI仅有单个波段也具备强鲁棒性。在合成和真实数据集上，对比最先进的无/自监督方法表现显著优越，并与有监督方法表现持平。

Conclusion: SpectraMorph有效提升了HSI-MSI融合的性能，增强了解释性和鲁棒性，是无/自监督场景下高光谱超分辨率的有力方案。

Abstract: Hyperspectral sensors capture dense spectra per pixel but suffer from low
spatial resolution, causing blurred boundaries and mixed-pixel effects.
Co-registered companion sensors such as multispectral, RGB, or panchromatic
cameras provide high-resolution spatial detail, motivating hyperspectral
super-resolution through the fusion of hyperspectral and multispectral images
(HSI-MSI). Existing deep learning based methods achieve strong performance but
rely on opaque regressors that lack interpretability and often fail when the
MSI has very few bands. We propose SpectraMorph, a physics-guided
self-supervised fusion framework with a structured latent space. Instead of
direct regression, SpectraMorph enforces an unmixing bottleneck: endmember
signatures are extracted from the low-resolution HSI, and a compact multilayer
perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed
by linear mixing, with training performed in a self-supervised manner via the
MSI sensor's spectral response function. SpectraMorph produces interpretable
intermediates, trains in under a minute, and remains robust even with a
single-band (pan-chromatic) MSI. Experiments on synthetic and real-world
datasets show SpectraMorph consistently outperforming state-of-the-art
unsupervised/self-supervised baselines while remaining very competitive against
supervised baselines.

</details>


### [82] [Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge](https://arxiv.org/abs/2510.20819)
*Nimrod Berman,Omkar Joglekar,Eitan Kosman,Dotan Di Castro,Omri Azencot*

Main category: cs.CV

TL;DR: 本文提出了Latent Denoising Diffusion Bridge Model (LDDBM)，作为一种支持任意模态对间的通用模态翻译方法，在多个任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在单一模态（如图像、音频）取得了显著进展，但将其拓展至模态翻译（即不同感官模态间的信息转换）仍面临挑战。现有方法过于依赖于模态间维度一致性、高斯先验及模态特定结构，局限了通用性和理论基础。因此，亟需解决这些限制，实现更广泛的多模态翻译能力。

Method: 作者提出了一种基于扩散模型的潜变量桥接框架（LDDBM）。其核心思想是将不同模态投射到共享潜空间，通过引入对比对齐损失增强语义一致性，并使用无关特定模态的编码-解码器结构进行噪声预测。同时，添加了预测损失以提升跨域翻译准确性，并提出了多种训练策略以优化模型稳定性。

Result: 该方法支持任意模态对翻译，在多视图到三维形状生成、图像超分辨、多视图场景合成等多项任务上表现优异。实验和消融结果验证了该框架的有效性，并在通用模态翻译领域树立了新的基线。

Conclusion: LDDBM为一般性模态翻译提供了理论和方法突破，消除了传统方法的许多约束，对多模态生成与翻译任务具有广泛适用性和推广价值。

Abstract: Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.

</details>


### [83] [LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas](https://arxiv.org/abs/2510.20820)
*Guocheng Gordon Qian,Ruihang Zhang,Tsai-Shien Chen,Yusuf Dalva,Anujraaj Argo Goyal,Willi Menapace,Ivan Skorokhodov,Meng Dong,Arpit Sahni,Daniil Ostashev,Ju Hu,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出了LayerComposer，一种可交互、多主体个性化文本生成图像的框架，采用分层画布与锁定机制，显著提升多主体场景下的空间可控性与个体保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型虽然生成效果逼真，但缺乏对空间构图的交互性控制，且在包含多个主体时表现不佳。作者希望通过引入新方法，实现对多主体图像生成的更强空间掌控和主体独立性的保持。

Method: 该方法主要有两个创新点：(1) 分层画布，将每个主体置于不同层，实现无遮挡的组合；(2) 锁定机制，保证选中层保持高保真，其他层自适应环境，并且不引入网络结构变更，仅结合位置嵌入以及新的数据采样策略实现。用户能像图像编辑软件一样自由拖拽、缩放、锁定各层。

Result: 通过大量实验，对比现有多主体生成方法，LayerComposer在空间控制与身份保真方面取得了更优的结果。

Conclusion: LayerComposer为多主体个性化文本图像生成带来了更高的空间可控性和个体独立性，同时操作友好，适用于交互式图像生成场景。

Abstract: Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.

</details>


### [84] [HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives](https://arxiv.org/abs/2510.20822)
*Yihao Meng,Hao Ouyang,Yue Yu,Qiuyu Wang,Wen Wang,Ka Leong Cheng,Hanlin Wang,Yixuan Li,Cheng Chen,Yanhong Zeng,Yujun Shen,Huamin Qu*

Main category: cs.CV

TL;DR: HoloCine模型打破了现有文本生成视频仅能输出孤立片段的局限，实现从头到尾的连贯剧情视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频模型虽擅长生成单一片段，但难以实现多镜头、连贯叙事的故事化视频。作者希望解决这一“叙事鸿沟”，实现整体场景、角色持续一致性的视频生成，从而推动自动化影视制作。

Method: 提出HoloCine模型，整体生成完整场景，保证故事连贯。利用Window Cross-Attention机制将文本提示精确定位至特定镜头，实现镜头级控制。引入Sparse Inter-Shot Self-Attention机制，在镜头内保持高密度关注，镜头间保持稀疏关注，提升长时生成效率。

Result: HoloCine模型在故事连贯性上达到了新的业界最佳水平，具备角色情节持久记忆和对电影语言直觉掌握等能力，显著优于以往同类方法。

Conclusion: 本文实现了从视频片段合成到自动化电影创作的转变，为端到端电影创作打开了现实可能性。

Abstract: State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [85] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于transformer的DeBERTa-KC模型，能够自动分类在线科学学习讨论中的知识建构层次，经实验验证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在线科学学习社区的评论大量且多样，手动分析知识建构水平耗时且主观，亟需自动化、可扩展、能准确识别知识建构层次的工具。

Method: 作者构建了一个包含来自YouTube科学频道的2万条手动标注评论的数据集，分为四类知识建构水平，并在DeBERTa-v3基础上引入Focal Loss、Label Smoothing和R-Drop正则化，设计了端到端可复现实验流程，采用10折交叉验证评估模型表现。

Result: DeBERTa-KC模型在宏F1分数上达到0.836±0.008，显著优于传统基线和其他transformer模型，尤其在“探索”和“协商”层级的识别上表现优异。

Conclusion: 大规模语言模型能够有效抓取非正式数字学习环境中知识建构的细微特征，为话语分析和知识建构评估工具的自动化与理论化提供了新思路。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [86] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 本文比较了五个主流大语言模型在生成高中物理课程教案时的表现，并评估了不同结构化提示框架的影响。结果显示，模型选择影响可读性，提示结构影响准确性和课程契合度。最佳方案是结合易读模型和RACE结构提示并加入显式的课程标准清单。


<details>
  <summary>Details</summary>
Motivation: 随着AI大模型在教育场景的应用，教师越来越多地依赖AI生成课程内容，但不同模型和提示框架的教育有效性尚不明确。本文旨在系统评估不同模型及提示方法对AI教案质量的影响，为实际教育应用提供参考。

Method: 作者选择了五个主流大模型（ChatGPT、Claude、Gemini、DeepSeek、Grok），采用三套结构化提示（TAG、RACE、COSTAR），针对'电磁波谱'主题各自动生成15份教案。随后从可读性、事实准确性、课程标准契合、认知目标水平四个维度，基于自动化指标进行量化分析。

Result: DeepSeek生成教案最易读，Claude语言难度最高。RACE提示结构能获得最低的AI幻觉指数和最高的课程标准契合度。所有模型生成的教学目标多集中于布鲁姆认知层级的低阶（记忆/理解），高阶目标较少。

Conclusion: 模型设计决定教案可读性，提示结构影响教学可靠性和课程标准契合。综合来看，最优做法是使用可读性高的模型、RACE提示结构，并明确列出要覆盖的物理知识点和高阶学习目标。

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [87] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出ReDiff框架，通过主动修正机制大幅提升离散扩散模型在视觉-语言任务中的性能，显著减少并行推理阶段的串联错误与语义幻想。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型具备良好的上下文建模能力和推理并行性，但实际应用中存在训练与推理不一致造成的严重错误级联，导致输出语法和语义质量显著下降。解决该基础性问题对于模型的实用化至关重要。

Method: ReDiff采用两阶段训练：第一阶段注入基础修订能力，让模型学会修正合成错误；第二阶段利用在线自我修正环节，让模型模仿专家对自身生成草稿修正的过程，实现主动自我纠错。该方法核心为“以错促学”，积极识别并纠正先前的错误输出。

Result: 实验结果表明，ReDiff方案在输出连贯性与事实准确性上明显优于传统被动去噪方法，可实现更加高效稳定的并行内容生成。

Conclusion: 通过主动修正机制，ReDiff显著缓解了离散扩散模型推理阶段的级联系统性错误，提升了视觉-语言任务的生成质量，为相关研究提供了更通用高效的方法。

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [88] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: 本文提出了一种名为Sparse Tracing的新方法，有效分析超长上下文的大模型注意力模式，同时显著降低了计算和内存需求，使长上下文可解释性分析在消费级GPU上成为可能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型支持的上下文长度增长到百万级，传统的解释性分析技术（如注意力解释）因计算和内存消耗随长度二次增长，变得不可行。因此，需要能高效分析超长上下文注意力的新方法。

Method: 提出了Sparse Tracing技术，并实现了Stream算法。Stream通过分层剪枝和二分法搜索，仅保留每个查询(top-k)的关键注意块，大幅减少计算量。该算法在接近线性的时间和空间复杂度下估算稀疏注意力掩码，实现一次遍历的可扩展解释。

Result: 在链式思维推理和RULER基准上实验表明，Stream算法在保留关键信息的同时能剪去97-99%的token交互，在RULER测试中还能保留关键检索路径并暴露从关键token到输出的层级路线。

Conclusion: Sparse Tracing为分析超长上下文模型的注意力和信息流提供了实用、高效的方法，大幅降低了硬件门槛，有助于推动大模型可解释性的普及。

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [89] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 该论文提出利用大型语言模型（LLM）自动分析电子健康记录（EHR）中的非结构化文本，以提升HIV筛查和早诊的效率，实验结果表现良好。


<details>
  <summary>Details</summary>
Motivation: HIV高效筛查和早诊能有效降低传播率，但受限于大规模实验室检测的不可行性。尽管EHR的普及为改进诊断方法提供了契机，现有研究主要关注结构化数据，忽视了临床笔记等非结构化文本的数据潜力。

Method: 作者提出利用LLM处理EHR中的非结构化文本，建立一条自动评估患者是否需进一步HIV检测的创新管道，并以荷兰鹿特丹伊拉斯姆斯大学医学中心的临床数据进行实验验证。

Result: 所提管道在实际临床数据集上展现出高准确率且漏诊率（假阴性率）低。

Conclusion: 融合LLM分析EHR中未结构化文本可显著提高HIV筛查及诊断的效率和准确性，具有实际的临床应用价值。

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [90] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 该论文首次以专家为基础，对大型语言模型（LLM）在生命周期评估（LCA）中的表现进行系统基准评测，发现其在准确性和解释质量方面具备一定潜力，但也存在较高的错误和幻觉信息风险。


<details>
  <summary>Details</summary>
Motivation: 尽管AI和LLM在LCA领域的应用案例增多，但缺乏系统的、标准化的评测方法和可靠性证据。本研究旨在填补LCA领域缺乏评测基准的空白，为模型实际应用提供参考。

Method: 作者对11种通用LLM（包括商业和开源模型）在22个LCA相关任务上进行评测，邀请17位有经验的从业者根据科学准确性、解释质量、稳健性、可验证性和指令遵循等指标对结果进行168次专家评审。

Result: 37%的模型回答被评为存在不准确或误导性信息。绝大多数模型的准确性和解释质量被评为中等或良好，格式遵循也表现良好。但部分模型出现高达40%的幻觉引用率。开源与闭源模型在准确性和解释质量等指标上表现相当，甚至开源模型在某些方面表现更优。

Conclusion: 泛用型LLM在LCA中的直接应用存在明显风险，尤其是在作为全能解答工具时。尽管如此，LLM在解释质量和减轻部分重复性劳动方面表现突出。未结合领域知识和实证机制时，LLM输出的可靠性难以保证。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [91] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 本文提出用桌游“Dixit”为多模态大模型（MLMs）设计基于游戏的评测方式，发现该方法评测结果与现有标准高度一致，同时揭示了模型与人的差异和改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有MLMs评测以静态测试或主观对比为主，无法全面考察模型能力，且易受表面因素干扰，急需一种能综合、多维客观评价的方法。

Method: 论文提出基于桌游Dixit的评测框架，利用该游戏需要多种能力、规则明确、竞争性强的特点，设计模型与人或模型之间的博弈实验，定量比较模型在Dixit中的表现。

Result: 五种MLMs在Dixit游戏中的胜率排名与主流基准测试上的排名完全一致。同时，人机对弈展示了模型和人类在策略和推理上的不同，并揭示了MLMs需改进的 reasoning 弱点。

Conclusion: 基于游戏的评测机制能更全面、客观评测MLMs，Dixit作为评测工具有效反映了模型性能，并指出了未来模型改进方向，具有很高的潜力和实用价值。

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [92] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: 本文探讨将大型语言模型（LLM）与运筹学（OR）领域的优化建模相结合，评估DeepSeek-R1模型在自然语言到优化模型公式转化中的应用潜力，并提出系列方法减少模型幻觉、提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统优化建模严重依赖领域专家进行数学模型构建，对于约束、目标和变量的定义尤为重要。现有强大优化器如Gurobi等仍依赖人工输入。尽管GPT-4等通用LLM在推理等方面强大，但高成本和幻觉问题限制了其在供应链等实际场景的应用。DeepSeek-R1作为低成本高效能LLM，有望降低专家门槛，但其在运筹领域的应用未被系统研究。

Method: 系统性评估DeepSeek-R1在四大运筹学基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）上的表现，建立幻觉分类体系，并提出减少幻觉、提升模型输出与用户意图匹配度的策略，包括LLM-as-a-Judge、少样本学习（FSL）、工具调用及多智能体框架。

Result: 初步研究表明通过上述方法可有效减少DeepSeek-R1在优化建模中的幻觉现象，提高自然语言到优化模型公式的准确性和可实用性。

Conclusion: LLM特别是经过强化学习优化的DeepSeek-R1模型，有潜力提升运筹优化建模的自动化程度，减少对专家的依赖。通过幻觉缓解技术，可更好地将LLM应用于实际OR问题，助力供应链等复杂决策领域。

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [93] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出了一种基于记忆增强的框架，使大语言模型（LLM）代理无需参数更新即可通过带标签数据和AI生成反馈学会分类任务，并在大量任务上显著优于传统标签检索方法。


<details>
  <summary>Details</summary>
Motivation: 当前直接微调大语言模型不仅成本高昂、灵活性差且不可解释，因而需要一种更灵活、成本低且可理解的学习机制，以便LLM更好适应用户需求和实际场景。

Method: 作者提出结合情景记忆（存储具体实例反馈）与语义记忆（提炼通用任务指导），综合利用带标签数据与LLM生成的批判性反馈促进学习。同时，设计了新度量指标“可建议性”来衡量模型对不同监督表达的响应与适应能力，并对OpenAI与开源模型进行了行为差异分析。

Result: 该框架在多项任务上将准确率最多提升24.8%，显著优于仅用标签的RAG基线。实验还揭示OpenAI与开源模型在事实型与偏好型数据处理上的行为差异。

Conclusion: 记忆驱动、反思式学习能有效提升LLM的自适应性与可解释性，为构建更智能、更易理解的AI代理提供了新的实现思路。

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [94] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: LyriCAR提出了一种全新的、无需监督的歌词翻译框架，通过自适应课程机制显著提升歌词翻译质量和训练效率，在英文-中文歌词翻译上表现出领先水平。


<details>
  <summary>Details</summary>
Motivation: 歌词翻译需要兼顾押韵、连贯和乐感等多重音乐约束，现有方法过于依赖手工规则和句级建模，难以实现跨行和全局的音乐语言模式学习，影响效果和泛化能力。

Method: 提出LyriCAR框架，采用自适应课程策略和难度感知课程设计师，动态分配训练资源，引导模型逐步解决更复杂的翻译难题，实现全无监督歌词翻译。

Result: 在英文-中文歌词翻译任务上，LyriCAR在传统翻译指标和多维奖励得分上均优于强基线，同时自适应课程机制将训练步数减少近40%。

Conclusion: LyriCAR有效提升了歌词翻译质量与效率，展示了自适应课程和无监督方法在复杂音乐语言场景中的巨大潜力。

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [95] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，将大语言模型（LLM）的广泛语言处理能力与符号自然语言理解（NLU）的结构化语义表达能力相结合，以提升事实抽取和推理表现。混合方法显著优于仅用符号方法。


<details>
  <summary>Details</summary>
Motivation: LLM在许多任务上表现良好，但易受到概率性推断导致的事实幻觉和输出不一致等问题。相比之下，符号NLU系统推断可解释，但覆盖有限且维护难。因此目标是结合两类方法，兼顾广覆盖和结构化表示的推理能力。

Method: 将LLM用于文本改写和简化，以获得广泛覆盖，并用于自动弥补知识空白；符号NLU用于生成可用于推理和增量学习的结构化关系表示。把两者结合，构建混合处理流程。

Result: 在从常识科学文本中抽取数量信息和因果规律的任务上，作者将混合方法与单独的符号及LLM流程比较，混合方法优于仅用符号流程。

Conclusion: 混合LLM与符号NLU的方式能更好地兼顾覆盖率与结构化推理，提升理解和抽取表现，是一种较符号单独方法更有效的途径。

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [96] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: 本文提出了一种解析自然语言句子的基本依存句法树算法，每次处理一个词，并尽早将其附加到句法树上，这与人脑的句法分析方式类似。算法最坏情况下复杂度为O(n^3)，但实际语言中，复杂场景只出现在短句。


<details>
  <summary>Details</summary>
Motivation: 现有的短语结构（成分）句法分析算法和人类解析语言的方式存在差距。作者希望提出一种更符合人脑句法分析特性的算法，以提升依存句法树解析的效率和认知合理性。

Method: 设计了一种递增的依存句法树解析算法，按词逐步进行，并在每一步尽快将当前词与已有结构相连接。该算法在理论上与传统成分句法分析的复杂度相当，但在真实语言环境中的性能优化明显。

Result: 提出的算法在理论上具备O(n^3)的最坏复杂度，但只有句子较短时才会达到最坏情况。实际应用中，由于语言的自然属性，多数情况下算法效率较高。

Conclusion: 新算法既维持了与传统句法分析方法相当的复杂度下限，又更好地符合认知科学中关于人类语言解析的描述，为自然语言处理和认知模拟提供了新思路。

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [97] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 本文指出现有医学问答数据集（如MedQA）过于简化，未能充分反映真实临床决策过程，并提出了更贴近实际的临床决策建模范式。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域大模型评价数据集多为简化问答，不能真实代表临床决策复杂性，急需统一标准提升模型实际临床应用价值。

Method: 提出用临床背景和临床问题两个维度来刻画决策任务，系统梳理现有数据集在这两个维度上的分布，评述提升决策能力的训练和测试方法，扩展评测指标至效率与可解释性，并指出仍存挑战。

Result: 提出并总结了临床决策任务的统一分析框架，对现有数据集和方法进行了归纳对比，明确了有助于提升模型临床表现的方法和条件。

Conclusion: 该统一范式有助于澄清假设、标准化方法比较、推动更有实际意义的医疗大模型发展，但依然面临诸多未解难题。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [98] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 本文提出了一系列针对现代希腊语的新型Transformer嵌入模型（Greek Embedding Models），并特别关注法律等高价值领域，实验结果显示新模型显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对现代希腊语（尤其是在法律等专业领域）的NLP模型受限于研究碎片化、模型架构单一以及上下文窗口太小，无法满足处理长文档和专业语料的需求。因此，亟需更高质量、更先进的希腊语语言模型。

Method: 作者首先大量收集、过滤、预处理来自通用领域和法律领域的高质量希腊语语料，构建多个大型训练数据集；然后在这些数据集上基于多种现代Transformer结构（如ELECTRA、ConvBERT和首次应用的ModernBERT）进行预训练，并系统性进行评估。此外，还首次提出为法律领域定制的希腊语-英语双语嵌入模型。

Result: 在多个下游任务上的广泛实验表明，新提出的GEM-RoBERTa和GEM-ConvBERT等模型在效果上显著优于当前主流基线模型。

Conclusion: 本研究为现代希腊语（尤其是法律领域）的NLP研究带来了高质量的新型嵌入模型，同时丰富了架构多样性，推动了相关领域的发展。

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [99] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 本论文提出了一系列针对序列标注任务的迁移学习改进方法，通过适应预训练神经语言模型来提升性能。创新包括多任务模型、改进的自回归语言模型结构和结合监督型上下文微调的新框架，有效提升了事件触发词检测等任务的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有的迁移学习在序列标注任务（如事件触发词检测）中的表现尚有提升空间，特别是在处理跨领域任务和利用大规模预训练语言模型时还面临诸多挑战。作者希望通过新方法进一步挖掘预训练神经语言模型的潜力，以实现更通用和高效的序列标注。

Method: 作者提出：（1）在多任务模型中引入额外信号，用于提升事件触发词检测的领域适应性能；（2）修改自回归大语言模型的结构，实现层间的双向信息流；（3）利用自回归大语言模型生成序列，并结合监督型上下文微调和响应导向的适配策略，构建新的序列标注框架。

Result: 实验表明，所提方法和框架能显著提升预训练神经语言模型在序列标注任务中的表现，特别是在事件触发词检测和其他跨领域应用中效果突出。

Conclusion: 通过上述创新方法，可更有效地将大规模预训练神经语言模型应用于序列标注任务。定向迁移学习方法能够实现更优性能，验证了方法的实用性和广泛适用性。

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [100] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了ToolScope系统，通过合并冗余工具并智能检索相关工具，提高大语言模型(LLM)在复杂任务中的工具选择准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实中的工具集通常存在大量冗余（名称和描述相近的工具），使LLM难以准确选择所需工具；此外，LLM输入有上下文长度限制，难以高效利用大规模工具集。

Method: ToolScope包括：1）ToolScopeMerger，自动审查并合并重复工具，减少冗余，并具备自动纠错能力；2）ToolScopeRetriever，针对具体问题压缩并筛选最相关工具集合，使其适配上下文限制，并提升工具选择效率。

Result: 在三种主流LLM和三套开源工具使用基准上实验，ToolScope将工具选择准确率提升了8.38%到38.6%。

Conclusion: ToolScope能有效去除多余冗余，优化工具集表达，大幅提升LLM利用工具的能力与准确率。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [101] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出了一个孟加拉语文化知识数据集，用以评估和提升多语言大模型对低资源文化细节的理解能力，发现模型在文化类任务上表现较差，但提供上下文信息能够提升其表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言基准提升了对大模型的跨文化评估，但在低资源文化细节捕捉方面仍存在明显不足。

Method: 构建了包含民俗、饮食、方言等内容的孟加拉语文化知识数据集（BLanCK），并测试多种多语言模型在文化相关和非文化相关任务上的表现，对比有无上下文信息的影响。

Result: 多语言语言模型在非文化类任务表现良好，但在文化知识理解方面表现显著较差；当为模型提供上下文信息后，各模型在文化类任务上的表现均有大幅提升。

Conclusion: 大模型在文化知识尤其是低资源文化方面存在局限，引入上下文与文化相关的数据有助于模型理解和表现，未来应重视文化相关数据和上下文设计。

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [102] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: 本文通过强化学习与AI反馈（RLAIF）和直接偏好优化（DPO）技术，提升了小型波斯语语言模型在医学问答中的推理能力。即使数据量有限，模型仍显著超越此前的同类模型。


<details>
  <summary>Details</summary>
Motivation: 在医学等专业领域，尤其像波斯语这种研究相对不足的语言，提高小型语言模型的推理能力对于实际应用具有重要意义。

Method: 将多项选择医学问答数据集翻译为波斯语，利用RLAIF生成被拒-被选答案对，采用DPO技术进行训练。数据包括教师与学生模型链式推理回答，最终形成包含正确与错误推理途径的数据集（优选答案200万、被拒250万token），用于小模型训练。

Result: 所训练的波斯语医学推理模型，在数据量远小于先前模型（gaokerena-V，5700万token）的情况下，推理能力却得到大幅提升，超越了gaokerena-V。

Conclusion: 通过以推理训练为核心方法，即使数据有限，也能显著提升小语种专用语言模型的应用性能，为资源受限场景下模型开发提供了新思路。

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [103] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 本文提出了CreativityPrism，一个能系统性全面评估大语言模型（LLM）创造力的新框架，将创造力拆分为质量、新颖性和多样性三大维度，通过九个任务、三个领域和二十项评价指标对17个主流模型进行测评，揭示了各模型能力的异同。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM创造力评估方法分散，各领域任务间定义和标准不一，缺乏统一的评测框架，难以科学比较模型在创造力方面的表现。作者希望搭建一个全面细致的评价体系来解决这一困境。

Method: 作者提出CreativityPrism，将创造力分解为质量、创新和多样性三大维度，具体覆盖发散性思维、创意写作和逻辑推理三大领域，用20种针对性的评价指标，全面测试17个主流LLM的创造力表现，并分析各指标间的相关性。

Result: 专有模型普遍优于开源模型；同一领域内，模型在各任务间表现高度相关，不同领域间的相关性较低。多样性与质量的评测表现密切相关，但新颖性与此二者相关性较弱。

Conclusion: 单一创造力或单一任务表现优异不能代表模型整体创造力强，强调了对LLM创造力应进行多维度、系统性评估的必要性。

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [104] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: 提出了ARTER方法，通过结合候选实体生成、上下文得分、自适应路由和选择性推理，提升了实体链接的性能且大大降低了依赖大语言模型的资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有实体链接方法依赖大量标注数据和模型微调，或通过提示调用LLM但成本高且效率低，因此需要一种既高效又高性能的新方法。

Method: ARTER采用结构化的流水线策略：首先对候选实体生成进行上下文得分，利用多種信号（Embedding和LLM），自动将实体提及分为容易和困难两类，再分别使用低成本实体链接器和精确但耗时的LLM推理处理。

Result: 在6个基准数据集中的5个都优于ReFinED（平均提升2.53%，最高提升4.47%），与在所有提及都用LLM的方案性能相当，但LLM token用量减半，效率更高。

Conclusion: ARTER无需深度微调即可取得高性能，同时极大提升了效率，兼顾了资源消耗和精度。

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [105] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 本文提出了BoundRL方法，能够高效地对包含多样结构化元素的长文本进行分段和标签预测，大幅降低推理成本，并有效提升分段质量。


<details>
  <summary>Details</summary>
Motivation: 传统的文本分段方法面对包含表格、代码片段、占位符等结构元素的复杂文本时，效果有限。随着多领域结构化文本（如技术报告、AI提示等）日益复杂，亟需更智能准确的分段方法。

Method: BoundRL实现逐token级的文本分段及标签预测，通过生成各段起始token索引并据此回溯原文，而无需生成全部内容，从而大幅降低推理成本。采用基于验证奖励的强化学习（RLVR），奖励函数同时考量文档还原和语义一致性。为避免熵塌陷，还会对生成的片段序列进行扰动，生成中间候选作为提升质量的中介。

Result: 在高难度LLM应用场景下的复杂prompt文本测试中，BoundRL使小模型能力超越了大模型的Few-shot提示，并在文档复原与语义一致性上显著优于常规监督微调。中间候选机制也带来了进一步的性能和泛化提升。

Conclusion: BoundRL为处理结构化长文本的分段问题提供了高效且可靠的新解法，显著降低了成本并提升了分段效果，有潜力广泛应用于复杂文本自动处理场景。

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [106] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在立场检测任务中表现出显著的刻板印象和偏见。


<details>
  <summary>Details</summary>
Motivation: 以往研究集中于LLM的偏见对仇恨言论检测、情感分析等任务的影响，然而针对立场检测中模型偏见的系统研究几乎被忽视。由于政治敏感性，立场检测中模型潜在的偏见尤为重要，因此有必要进行深入调查。

Method: 作者在现有立场检测数据集上，自动为帖子标注方言/特定群体俚语以及文本复杂度两个属性。随后，在零样本设定下测试LLM在不同属性文本上的立场检测表现，以判断这些属性对模型决策的影响。

Result: LLM在立场检测中表现出明显的偏见。例如，模型倾向于将支持大麻合法化的观点和简单文本复杂度错误地联系在一起，将非裔美国人方言与反特朗普立场错误地关联。

Conclusion: 当前LLM在立场检测任务中存在明显社会偏见，未来需制定相应缓解策略，以确保模型公平、可靠地应用于敏感NLP任务。

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [107] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个新的评测基准DeepWideSearch，旨在测试搜索代理同时进行深度推理（多跳检索）和广度信息收集能力，实验证明当前最先进模型表现十分有限。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索代理在实际应用场景下（如市场分析、商业开发）无法同时兼顾多跳深度推理与大规模信息整合能力，严重制约了其实用性。因此，需要新的方法或基准推动相关能力提升。

Method: 作者设计并建立了DeepWideSearch基准，通过对已有数据集进行二次加工，构建了覆盖15个领域、共220个需要深度推理且信息量大的问题，并系统性评测现有搜索代理模型的表现和失败模式。

Result: 实验显示，当前最优秀的搜索代理在DeepWideSearch的平均成功率仅为2.39%，暴露出“深度+广度”整合极具挑战性。误差分析发现四种主要失败原因：缺乏自我反思、过度依赖自身知识、检索不足和上下文溢出。

Conclusion: DeepWideSearch为评估和推动更强大信息检索代理的发展提供了新平台，分析表明现有方法仍存在明显短板。希望通过公开该基准促进社区对高效且健壮检索代理研究。

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [108] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种多智能体框架Mixture-of-Minds，通过将表格推理任务分解为规划、编码和回答三个角色，有效提升了表格理解的性能，并结合MCTS和强化学习实现自我改进，在TableBench上超过了OpenAI最新基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调的大模型虽然能进行语言推理，但容易出现算术错误和幻觉；基于工具的方法虽然精确但语义理解不足，二者各有局限。为结合两者优点，需要集成强大的推理能力与可靠的表格处理。

Method: 提出了Mixture-of-Minds多智能体框架，将表格推理分为规划、编码、回答三个模块，每个agent专注于特定任务，并通过代码执行实现精准的表格操作；同时引入MCTS生成伪标注数据、强化学习优化各agent，构建自我改进的训练流程。

Result: 在TableBench等基准上进行了大量实验，Mixture-of-Minds方法取得了62.13%的准确率，优于OpenAI-o4-mini-high等先进模型。

Conclusion: 结构化多Agent协作流程结合RL提升了表格推理能力，方法具有很强的研究和应用前景。

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [109] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在文本基础上进行空间推理的能力，并发现其在复杂空间任务上表现显著下降，显示出当前LLMs在空间理解方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本理解与生成上表现优异，但其空间推理和抽象几何计算能力尚不清楚。作者希望系统评估LLMs在模拟结构化空间任务时的表现，从而发现其语言与空间推理的差距。

Method: 作者设计了包括象限识别、几何变换、距离评估、单词搜索和“拼图砖块滑动”等五种任务，利用不同维度和复杂度的网格数据对LLMs进行测试，衡量它们在基础空间推理和多步计算中的表现，并随着任务难度增加观察准确率变化。

Result: 在简单、低复杂度任务上，LLMs表现中等；但随着任务网格和复杂度增加，其准确率平均下降42.7%，最高降幅达84%。所有初始准确率高于50%的测试在难度提升后都会下降至少48%。

Conclusion: 现有LLMs在基础语言推理与空间推理之间存在明显差距，空间表示能力薄弱，难以应对复杂的空间任务。该研究为后续语言与几何结合的基准建设提供了参考。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [110] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文研究了当评估LLMs时，针对同一文本存在多种子词分词方式的问题，提出并分析了无需解码的采样策略，实现更高效、准确地近似边缘化所有分词概率。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型通常采用子词分词以实现模型规模、推理速度和词汇覆盖率的权衡，但模型评估时只关注特定分词的概率，忽略了同一文本的其它分词方式，这使得测评存在偏差。近期有研究主张评估时应边缘化所有可能分词并累计其概率，但计算量大，因而存在采样近似，但又受限于采样成本高、近似不准的问题。

Method: 本文分析了当前近似边缘化（marginalization）时采样带来的效率与精度瓶颈，提出了一种无需解码的大模型生成的采样策略。具体来说，完全依赖于分词器和模型无关的，极其低成本的采样方法，无需让大模型执行实际生成推理。

Result: 在多个开源大模型上的实验表明，无解码采样策略在极小计算成本下，依然能获得足够精确的概率边缘估计，并实证验证了其在多种下游推理任务中的实际应用效果。

Conclusion: 无需解码的低成本采样策略能有效解决基于子词分词的概率边缘化难题，为大模型评价与推理带来了高效可靠的新途径。

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [111] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 本文提出了一种结合文本、音频和面部特征的三模态情感严重性融合方法，用于联合评估抑郁症和PTSD的严重度，提升了多重障碍自动诊断的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症与PTSD常常共同发生，症状相互影响，而现有自动评估方法多数是二元且针对单一障碍，缺乏对严重度和跨障碍综合判别的能力，限制了实际临床应用。

Method: 该研究构建了统一的三模态评估框架：1）文本使用句级transformer嵌入；2）音频采用日志Mel特征及其变化量；3）面部信号涵盖表情动作单元、凝视、头部/姿态特征。所有特征经标准化后，用校准的迟融合分类器整合，输出抑郁（5级）和PTSD（3级）的严重度概率，并提供特征归因解释。

Result: 在DAIC数据集分层交叉验证中，三模态模型整体性能优于任一单模态及其消融基线：在准确率及加权F1与表现最强的单模态相当，但在决策曲线、模态缺失/噪音下更鲁棒。尤其PTSD严重度回归误差更低，类别一致性更高。错误多集中于相邻严重度，极端类别区分更稳定；消融分析显示文本对抑郁判别贡献最大，音频与面部对PTSD判别更关键。

Conclusion: 该三模态方法可复现评估结果，增强了自动化临床决策系统的可用性和可解释性，可辅助临床医生有效识别和判断抑郁与PTSD共病的严重度。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [112] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: 本文提出了ContextLM，通过加入基于多token上下文预测的新目标，提升语言模型对长距离语义结构和上下文的建模能力，实验表明其在GPT2和Pythia系列模型中有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型主要采用逐token预测（NTP），这种方式难以捕捉高层次语义结构和长距离上下文关系，限制了模型理解和生成语言的能力，因此需要一种新的训练方法来提升模型的语义建模能力。

Method: 提出了一种称为ContextLM的框架，在标准自回归token预测外，增加了next-context prediction目标，让模型学习未来多个token片段的预测表示。整个方法与现有token-by-token的评估方式兼容，并通过未来token块的误差信号强化模型训练。

Result: 在GPT2和Pythia系列、最多1.5B参数规模的广泛实验表明，ContextLM方法能在困惑度以及下游任务表现上稳定提升，并能在计算开销很小的条件下改善长距离连贯性和注意力分配。

Conclusion: 引入next-context prediction目标为提升语言模型提供了一种高效可扩展途径，有望大幅改善模型的长距离建模能力，同时兼容现有评估流程。

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [113] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本论文针对大语言模型（LLM）在RAG系统中引用失败的问题，提出了CITECONTROL基准和CITENTION方法，有效提升引用质量。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM基于RAG系统的引用能帮助验证模型输出，但引用失败（生成内容正确却未完整引用证据）依然普遍存在。此前研究未将引用失败与内容本身错误（响应失败）区分清楚，因此作者提出深入研究引用失败并改善其表现。

Method: 论文提出两步方法：第一，研究响应与证据之间关系如何影响引用质量，并推出CITECONTROL基准系统地分析失败模式。第二，提出CITENTION框架，将生成式、基于注意力和检索的方法结合以提升LLM引用能力。

Result: 实验证明，随着响应与证据关系复杂度增加，引用失败增多；结合多种引用方法后引用质量显著提升。CITENTION在CITECONTROL及迁移任务中均有良好表现。

Conclusion: 通过区分引用失败与响应失败，并采用多方法结合的新框架CITENTION，能有效提高LLM系统的引用完整性和可靠性。

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [114] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本文首次系统性研究了流程奖励模型（PRMs）在表格问答（TQA）领域的应用，发现现有PRMs存在泛化能力弱、验证表现与最终答案准确性关联性低等问题。


<details>
  <summary>Details</summary>
Motivation: 虽然PRMs在数学推理等结构化推理场景表现良好，但其在包含半结构化数据的表格问答任务上的有效性尚不明确。TQA具有信息冗余、推理步骤松散及领域相关性强等特殊挑战，因此有必要分析PRMs在这一领域的适用性。

Method: 本文评估了多种最先进的生成式PRMs在TQA任务上的表现，采用了从答案准确率和步骤验证两个角度进行分析，同时结合文本与代码校验的方式选择最终答案，并在同域与异域数据上进行了实验比较。

Result: 实验结果表明，结合文本和代码验证的PRM有助于解答筛选，但在跨领域泛化时表现不佳。步骤级验证表现与答案准确率之间相关性较弱，主要归因于推理步骤间依赖性弱和因果关系松散。

Conclusion: 现有PRMs在表格问答任务中存在明显局限，难以有效泛化并捕捉松散的推理过程。研究结果为研发更强鲁棒性和过程感知的验证模型提供了重要启示。

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [115] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种用于优化大型推理模型（LRMs）与代码解释器（CI）协作的后训练方法CoRT，以提升其处理复杂数学运算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在自然语言推理方面表现突出，但在复杂数学运算中存在低效或不准的问题。引入代码解释器虽能弥补这一短板，但模型内部的概率推理与外部的确定性代码知识存在冲突，导致推理过程低效。作者的动机是提升模型高效、准确利用外部CIs的能力。

Method: 提出CoRT框架进行后训练，并创新性地引入Hint-Engineering策略：在推理路径的最佳插入点注入丰富多样提示，合成高质量、代码集成的训练数据。方法通过拒绝采样和强化学习，进一步优化模型在多轮中外部工具使用与内部推理的交互。使用1.5B到32B参数规模的模型，作者在30个高质量样本上进行有监督微调。

Result: 在5个数学推理基准集上，CoRT使DeepSeek-R1-Distill-Qwen-32B和1.5B分别提升了4%和8%的绝对准确率。同时，32B模型和1.5B模型的token使用量相较基线分别减少了约30%和50%。

Conclusion: CoRT能够明显提升大模型调用外部代码解释器解决复杂数学问题的效率与准确性，为大模型与外部工具协作提供了行之有效的训练范式。

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [116] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: LLMs在结构化数据推理能力评估中的高分可能因数据集污染，尤其是对表达清晰语义线索的数据集有所记忆，而非真正推理能力的体现。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs结构化推理能力的评估往往忽视了数据集污染这一干扰因素，使得模型表现被高估。作者想要探究模型是否真的具备推理能力，还是仅仅记住了已公开的数据集。

Method: 设计一系列受控实验，检测LLMs在含有强语义线索（如有意义的列名和可解释取值类别）的数据集上的表现，并与去除或随机化这些线索后的表现进行对比。

Result: 对于含有明显语义线索的数据集，模型表现良好；而一旦语义线索被移除或打乱，模型表现急剧下降，接近随机水平。

Conclusion: LLMs在结构化表格推理上的优异表现，很大程度上可能来源于对公开数据集的记忆，而非真正的泛化推理能力。未来评估需消除语义泄露等干扰，才能准确测量模型的推理能力。

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [117] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: FreeChunker提出了一种新的跨粒度切分编码框架，通过灵活组合句子实现更高效精准的检索增强生成（RAG）系统。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统多采用固定粒度的分块（chunking），这不仅需要复杂的语义边界检测，而且在面对多样化查询时适应性差，效率不高。迫切需要一种既灵活又高效的chunking新范式。

Method: FreeChunker将句子视为最小原子单元，不再固定划分块边界，而是支持任意句子组合的灵活检索编码。这种方式摆脱了静态边界检测，大幅减少了相关的计算负担。

Result: 在公开数据集LongBench V2上的实验显示，FreeChunker在检索性能和计算效率上都显著优于传统的chunking方法。

Conclusion: FreeChunker打破了旧有的chunk划分局限，既提升了复杂查询下的适应能力，又大大降低了计算资源消耗，具有广泛的实际应用前景。

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [118] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文研究了仅利用对话数据预训练的小型语言模型，发现该模型在部分标准任务上表现一般，但在对话延续预测上表现突出。


<details>
  <summary>Details</summary>
Motivation: 探索仅用对话语料训练的小型语言模型是否能在对话相关任务中展现独特优势。

Method: 首先对llamalogue模型进行仅基于对话数据的预训练；随后采用多种后续微调方法（包括PPO和DPO）优化文本生成的交流性。最后在标准基准和自建对话基准上评测模型性能。

Result: 模型在大部分BabyLM基准表现逊色，但在最小对测试对上的对话延续预测任务表现优异。PPO微调对模型效果影响不一甚至带来副作用，而DPO微调能进一步提升模型在自定义对话基准上的表现。

Conclusion: 专门针对对话数据预训练的小模型虽然在通用基准上不如多领域模型，但在对话相关任务和特定微调策略下能取得领先，是一种值得关注的训练范式。

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [119] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在否定文本中的幻觉检测能力，并发现其在该领域表现不佳。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型‘幻觉’现象（即生成不真实信息）受到关注,但在处理带否定的文本时，相关影响及机制尚未明确。该研究旨在探究否定文本是否影响LLMs的幻觉检测能力。

Method: 作者提出了3个关键问题，通过比较LLMs在肯定和否定文本下的幻觉辨识能力。为此，重新构建现有幻觉检测数据集，加入否定表达，形成NegHalu数据集。同时，对LLMs处理否定输入时的内部状态变化进行跟踪分析。

Result: 实验证明LLMs在否定文本下难以有效检测幻觉，经常做出逻辑不一致或不忠实的判断。

Conclusion: LLMs对否定文本的处理存在固有限制，在幻觉检测方面需要针对性改进，现有方法难以缓解否定带来的不良影响。

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [120] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了在VLSP 2025举办的多模态法律问答任务，具体聚焦于越南交通标志法规。这项任务包含多模态法律检索和多模态问答两部分，并公布了相应基准实验结果。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在法律领域的应用不断深入，越南多模态法律文本处理研究尚不成熟，缺乏针对特定领域（如交通标识法规）的基准数据集和评估体系。因此，提出该任务以推动相关研究，并支持智能系统在实际法律场景中的应用。

Method: 构建并发布了包含交通标志法规的多模态数据集，设计了多模态法律检索和问答两个子任务，组织社区参与并评测相关系统的性能。

Result: 基准实验中，多模态法律检索任务取得了64.55%的F2分数，多模态问答任务的准确率为86.30%。

Conclusion: VLSP 2025 MLQA-TSR为越南多模态法律处理领域提供了标准数据与评估基准，有助于推进此方向智能系统设计与研究。

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [121] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: BERT模型虽然效果好，但架构已过时。本文提出了专注于希伯来语的NeoDictaBERT及其双语版，两者在希伯来语基准上表现优异，并开源模型推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有BERT模型虽小而高效，但架构落后于新式Transformer，如Llama3、Qwen3。近期有如ModernBERT、NeoBERT等改进架构取得进展，但主流改进大多聚焦英语，缺乏针对希伯来语的优化模型。

Method: 基于NeoBERT架构，针对希伯来语训练NeoDictaBERT及NeoDictaBERT-bilingual（双语模型）。描述了训练过程，并在各类希伯来语和多语基准上衡量模型性能。

Result: 提出的NeoDictaBERT及其双语版在几乎所有希伯来语基准上优于现有模型。尤其NeoDictaBERT-bilingual在检索任务上超过同尺寸的多语模型。

Conclusion: NeoDictaBERT系列模型为希伯来语NLP提供了有力的基座。全文开源模型以推动希伯来语及相关NLP领域发展。

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [122] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: 本文提出了ContingentChat框架，通过后训练微调提升了对话模型BabyLM在幼儿-看护人多轮对话场景下的连贯性和语法表现，但与更复杂的教师解码策略相比，提升有限，表明多轮对话的即时性和语境相关性仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 儿童与看护人之间的多轮对话具有情境连贯性（应答及时、相关且有意义），但现有语言模型，尤其是以有限语料（如BabyLM）训练的小模型在捕捉这种特性上表现不佳。作者希望通过改进后训练方式，提升模型在此任务上的表现。

Method: 提出ContingentChat框架，使用新的对齐数据集对训练语料有限（1亿词）的BabyLM模型进行后训练，为提升多轮对话应答的连贯性和语法性。此外，通过适应性教师解码策略进行对比实验，以检验进一步提升空间。

Result: 微调后的BabyLM产生的回复在语法和连贯性方面得到改进。采用适应性教师解码策略进行后续实验，进一步提升有限。

Conclusion: 有针对性的后训练数据对提升对话质量有效，但多轮对话的情境连贯性（contingency）对BabyLM等小模型而言仍较具挑战，需要持续关注和研究。

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [123] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: 作者提出了一种创新方法，将大量低质量和冗余的指令数据蒸馏为高质量的指令-输出对，大幅提升大模型对指令的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 尽管高质量数据对大模型指令微调很关键，但数量稀缺，而低质量数据虽然丰富却易被丢弃，造成信息浪费。目前数据增强方法难以有效利用低质量数据，且相关评估不完善。

Method: 作者正式定义了'指令蒸馏'任务，并提出MIXTURE数据集（包含144K样本），将低质量或冗余的指令簇与其高质量蒸馏结果配对。然后，提出LM-Mixup方法，先用MIXTURE做有监督微调，再通过强化学习 (GRPO算法) 利用质量、语义一致性与格式匹配三种奖励信号进一步优化模型。

Result: 用LM-Mixup蒸馏出来的数据（仅占全部数据的3%）微调大模型，不仅超越用全量原始数据训练的效果，在多个基准任务上还能媲美最先进的高质量数据筛选方法。

Conclusion: 经良好蒸馏与增强后，低质量数据是很有价值的资源，可大幅提升大模型指令微调的效率及性能。

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [124] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文系统性评估了四种大语言模型（LLM）输出置信度估计方法，在四个问答任务上进行了实验，发现混合方法CoCoA整体上最好，提升了答案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出存在不确定性且答案正确性参差不齐，限制了其实际应用的可靠性，因此需定量评估其不确定性并寻找合适的置信度估计方法。

Method: 本文对VCE、MSP、Sample Consistency和CoCoA四种LLM输出置信度估计方法进行系统性评估。在四个问答任务上采用先进的开源LLM进行实验，比较各种方法的性能和效果。

Result: 实验结果显示，每种方法捕捉了模型自信心的不同方面，其中混合方法CoCoA在可靠性、校准度和正确答案区分能力方面表现最佳。

Conclusion: 混合方法CoCoA能显著提高LLM输出的可靠性。不同方法各有优劣，需根据应用场景权衡选择合适的不确定性度量方法。

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [125] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: 该论文提出了一种改进的掩码语言建模（MLM）方法，能根据模型预测能力自适应调整掩码概率，并通过引入子词嵌入提升了形态泛化能力，在BabyLM Challenge 2025严苛小规模赛道上超过了基线。


<details>
  <summary>Details</summary>
Motivation: 如何在低资源环境下通过更智能的掩码策略和子词信息提升语言模型在下游任务（如GLUE/SuperGLUE）的性能。

Method: 1. 提出适应性掩码策略：根据语言模型对某些Token的预测能力，自适应调整这些Token被掩码的概率，以更有效地训练模型。2. 引入子词级别的嵌入（sub-token embeddings），增强模型对词形变化的泛化能力。3. 在BabyLM Challenge 2025的strict-small轨道上进行评测。

Result: 自适应掩码MLM方法在（Super）GLUE任务中，相较于标准MLM取得了显著性能提升；引入子词嵌入使模型在形态泛化方面有更好表现。在BabyLM Challenge 2025的strict-small轨道中，超过了官方基线。

Conclusion: 改进的MLM方法和子词嵌入可以有效提升小规模语言模型在多项下游任务与词形泛化上的能力，展现了在低资源设置下提升模型的可行策略。

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [126] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的数据无关、基于表示的模型融合框架RECALL，可在无历史数据的情况下实现LLM的持续学习，并有效防止遗忘，提升多任务和多领域任务的知识整合能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在持续学习中通常面临灾难性遗忘和无法访问历史数据的问题，现有方法通常需要任务标签或在性能间权衡。作者希望通过内部表示和模型融合的创新策略，解决无数据持续学习中的知识保留与泛化能力不足的问题。

Method: RECALL框架通过对典型样本（聚类得到）在各层的隐藏表示计算模型间相似性，采用自适应分层参数融合方法，将模型知识对齐。该方法使浅层保留通用特征，深层适应任务特化，有别于以往依赖任务标签的方法。

Result: 在五个自然语言处理任务和多种持续学习场景下，RECALL在知识保留和泛化方面均优于主流基线方法，展现了强抗遗忘能力和优秀的多领域融合性能。

Conclusion: RECALL为持续进化的大语言模型提供了一种高效、可扩展、无需历史数据和任务标签的知识整合与迁移方案，有望在多任务NLP应用中广泛推广。

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [127] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 该论文提出了一种通过激活引导(steering vector)的方法，来抑制大型语言模型在评测时表现出与实际部署不同的行为，提高安全性评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知道自己被评测时，可能会刻意迎合评测标准，导致行为与真实部署时不同，进而影响安全性评估结果的可信度。作者希望解决评测“表演”现象，提高模型评测的真实性。

Method: 作者首先用两步训练流程让模型习得“评测感知”行为，（1）继续预训练使模型将Python类型提示作为一种评测信号，（2）通过expert iteration细化模型在评测场景下的行为。然后，通过添加一个在额外训练前构造的激活引导向量，来抑制模型的评测感知，使其在评测时与真实部署行为一致。

Result: 训练后的模型在有评测信号时，会明显展现“评测感知”行为（如更常写类型提示），但添加激活引导后，这种“伪装”被抑制，模型在评测时也能表现出部署时的自然状态。而且，激活引导向量是在附加训练前即可获得。

Conclusion: 通过激活引导，可以有效抑制模型的评测感知行为，从而提升安全性评测的真实性和可信度。AI安全评估者可利用此技术，让模型在评测中更贴近实际部署的表现。

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [128] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为RPS（Robust Preference Selection，鲁棒偏好选择）的方法，在不需对大模型进行再训练的情况下，显著提升模型对多样化人类偏好的适应能力，尤其是在训练数据未充分覆盖的偏好区域。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在普遍或主流的用户偏好下表现优秀，但对于较为个性化或偏离平均偏好的请求，容易失效。这是因为训练数据无法全面覆盖所有可能的偏好，导致对边缘或细分需求的表现不佳。传统解决办法需高成本重新训练，且泛化性弱。

Method: RPS为训练后处理方法，在生成响应时，不直接依据用户唯一偏好采样，而是从该偏好的邻域方向采样多个候选响应，通过比较再选择与原始意图最匹配者，提升模型应对多元偏好的能力。该方法无需重新训练，只依赖基于模型原有参数的多样采样和选择机制。作者还提出理论证明其优于传统多样采样基线。

Result: RPS在三类主流对齐范式（DPA、DPO、SFT）下均展现出更高的鲁棒性，无需再训练便能应对训练数据未覆盖的复杂或稀有偏好区域，在极具挑战性的偏好覆盖测试中获胜率最高达69%。

Conclusion: RPS是一种实用且理论坚实的提升偏好对齐模型可靠性的方案，为大模型后处理适应个性化需求提供了通用、有效的新路径。

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [129] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: 提出了一种统一处理多格式证据和复杂问答的框架HSEQ，有效提升了检索增强生成（RAG）在多步复杂问答中的性能，并兼顾效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在处理多步推理和多样化（文本、表格、知识图谱）证据源的问题时，存在真确性与效率（延迟、token/tool用量）之间的权衡，亟需统一且高效的解决方案。

Method: HSEQ通过：（1）将文档、表格、知识图谱等异构数据统一线性化为带轻量结构标签的可逆分层序列；（2）利用结构感知的迭代策略，逐步收集充分但不冗余的证据；引入Head Agent引导检索并组织答案生成，Iteration Agent按结构有针对性展开，最终通过证据规范化生成答案并可加入矛盾消解环节。

Result: 在多个QA基准数据集（HotpotQA、HybridQA、TAT-QA、MetaQA）上，相比于强基线模型（单步/多步/agent式RAG），HSEQ取得了更高的EM/F1分数，且效率更高。

Conclusion: HSEQ具备三大优势：1）统一格式无关的策略，提升跨不同数据类型的通用能力；2）在保证准确性的前提下，提升token和工具调用效率；3）通过证据规范化，提高答案一致性和可审计性。

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [130] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的衡量大语言模型（LLM）政治偏见的方法，从多语言翻译的公平性角度切入，通过分析欧盟议会演讲的翻译质量，发现主流政党的演讲翻译质量高于边缘政党。作者还发布了标注了政治归属信息的多语言语料库。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM政治偏见的方法主要通过英文问卷模拟，但这种方法局限于英语和问答场景，不能充分覆盖多语言环境或现实场景。作者希望通过更贴近实际应用的多语言翻译公平性分析，扩展偏见评估视角。

Method: 作者将LLM对欧盟议会21国语言演讲语料进行自动翻译，并基于实际政治光谱（左、中、右、边缘党派）标签，系统比较不同政党演讲的翻译质量。同时，作者构建并公开带有详细党派及个人信息的多语语料库（EuroParl多语言版本）。

Result: 实验发现，主流党派（左、中、右）的演讲在翻译时质量显著高于边缘（外部）党派。这表明多语言翻译中仍存在显著的政治偏见。

Conclusion: 论文指出，仅用英文问卷评测无法全面审视LLM的政治偏见，多语言文本实际应用（如翻译）中也存在不可忽视的偏见问题，呼吁未来工作关注多语言和多场景的公正性评估。

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [131] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: 该论文提出了一种名为ARC-Encoder的新型上下文压缩方法，通过将上下文压缩为连续表示，替代解码器LLM中的token embedding，实现了更高效的推理和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成（RAG）与思维链（CoT）等技术的发展，LLM推理时需要处理更长的上下文，导致推理成本显著上升。现有压缩方法往往依赖模型微调或架构修改，可能损害模型通用能力。因此，亟需无需改动LLM本体即可高效压缩上下文的新方法。

Method: 作者系统研究了编码器训练和架构，最终设计出ARC-Encoder。该编码器将文本压缩为$x$倍更少的连续表示，这些表示直接替代解码器中原token embedding，实现上下文压缩。ARC-Encoder无需修改解码器架构即可工作，还可以被多个不同LLM解码器复用。

Result: ARC-Encoder在多种LLM使用场景（如in-context learning、上下文窗口扩展等）中进行了评估，包括指令和基础解码器。结果显示其在多个基准上均达到SOTA水平，同时大幅提升了推理的计算效率。

Conclusion: ARC-Encoder不仅为推理提供了高效灵活的上下文压缩解决方案，还具备良好的跨模型通用性。其代码与模型数据已开源，为后续研究和应用提供了便利。

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [132] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: 该论文提出了CenterBench数据集，旨在区分大语言模型在理解复杂句子时究竟是进行结构解析还是仅凭语义匹配，并首次量化了模型在不同结构复杂性下的表现转变。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型的基准测试已很广泛，但尚无法有效分辨模型在解析句子时是理解了句法结构，还是仅仅利用了训练中学到的语义常识。了解模型何时依赖结构，何时只靠语义，对于改进模型和提升可控性至关重要。

Method: 作者构建了CenterBench数据集，包括9720个递归中心嵌套的句子，并为每个句子设计了一个语义不合理但句法结构一致的对照句，以及六道涵盖表层理解、句法依赖和因果推理的理解问题。通过测试六个主流模型，评估它们在合理与不合理语境下随结构复杂性增加的表现差距。

Result: 结果发现，随着句子结构复杂性的增加，模型在合理语义和不合理语义句子上的表现差距显著扩大，最高中位数差距达26.8%。特别是对于因果推理类问题，语义合理性反而降低了模型的表现。推理型模型虽整体提升准确率，但也出现了语义捷径、过度思考和拒答等倾向。与之相比，人类受语义影响的表现并非线性扩展。

Conclusion: CenterBench首次为识别语言模型从结构分析转向语义模式匹配的临界点提供了系统工具，有助于未来进一步提升模型对复杂句法结构的真正理解能力。

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [133] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 本文提出GlobalRAG，一个通过强化学习提升多跳问答中检索增强生成能力的框架，能够更好地规划问题分解和证据利用，显著优于现有强基线，并以更少数据实现大幅性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多跳问答场景下的RAG系统存在两个主要瓶颈：缺乏有效的全局规划和推理结构，以及未能保证检索与推理的一致性和连贯性，导致提问和证据利用效率低下。

Method: 作者提出GlobalRAG框架，将复杂问题拆分为子目标，并通过强化学习策略引导证据检索与推理过程，采用“规划质量奖励”和“子目标完成奖励”鼓励系统实现全局连贯推理与高质量子目标执行，引入渐进式权重退火调控不同目标的权衡。

Result: 在多跳问答领域的多项内部与外部基准测试中，GlobalRAG仅需8k训练数据（约为强基线的42%），就在准确率（EM）和F1指标上平均提升14.2%，显著优于现有几种强基线方法。

Conclusion: GlobalRAG有效提升了检索增强生成模型在多跳问答中的全局推理和证据利用能力，显著减少了所需训练数据量，为多跳复杂任务下RAG系统的强化学习优化提供了新方向。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [134] [Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search](https://arxiv.org/abs/2510.20567)
*Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang*

Main category: cs.CL

TL;DR: 本文提出了一个多智能体认知决策框架（MACDF），以取代传统的检索排序范式，从而更好地支持电商复杂搜索场景并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统电商检索排名依赖于查询-商品匹配，这种方式无法有效解决复杂查询下的语义理解问题，导致用户需要跨平台搜寻信息、决策成本高、缺乏专业购物指导，体验有限。

Method: 作者提出多智能体认知决策框架（MACDF），目标是将搜索从被动检索提升为主动的决策支持。通过多智能体协作处理复杂、包含否定、多约束或推理需求的查询，实现更贴合用户认知决策流程的结果推荐。

Result: 离线评估显示MACDF在推荐准确性和用户满意度上均明显优于传统方法，尤其在复杂查询场景下表现突出。在京东搜索平台的在线A/B测试也验证了其实际有效性。

Conclusion: MACDF框架展示了多智能体认知系统在电商搜索领域的变革潜力，可有效解决传统搜索面临的复杂查询理解和用户支持不足的问题。

Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its
reliance on query-item matching fundamentally misaligns with multi-stage
cognitive decision processes of platform users. This misalignment introduces
critical limitations: semantic gaps in complex queries, high decision costs due
to cross-platform information foraging, and the absence of professional
shopping guidance. To address these issues, we propose a Multi-Agent Cognitive
Decision Framework (MACDF), which shifts the paradigm from passive retrieval to
proactive decision support. Extensive offline evaluations demonstrate MACDF's
significant improvements in recommendation accuracy and user satisfaction,
particularly for complex queries involving negation, multi-constraint, or
reasoning demands. Online A/B testing on JD search platform confirms its
practical efficacy. This work highlights the transformative potential of
multi-agent cognitive systems in redefining e-commerce search.

</details>


### [135] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: 本研究探讨了使用ChatGPT自动化对交流和协作数据进行编码时，是否存在针对不同性别和种族群体的偏见。结果显示，ChatGPT编码没有显著的群体间偏见。


<details>
  <summary>Details</summary>
Motivation: 目前大规模协作和交流评估需大量人工对数据进行分类编码。虽然ChatGPT已经被证明在准确性上可以媲美人工编码，但其会否对性别、种族等群体带有偏见尚不明朗。

Method: 作者使用协作问题解决的常用编码框架，通过ChatGPT对三类协作任务（谈判、问题解决和决策）产生的数据进行自动编码，并分析其在不同性别和种族群体间的表现差异。

Result: 研究结果表明，基于ChatGPT的自动编码在性别和种族群体间没有显示出显著的偏见。

Conclusion: ChatGPT自动编码方法在群体间没有显著偏见，可以应用于大规模协作和交流评估，有助于未来推动相关技术的采纳。

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [136] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 本文参与了阿拉伯语AI生成文本检测的竞赛，并分析了三种预训练变换器模型，发现多语言模型XLM-RoBERTa效果最好。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本增多，如何有效检测AI生成与人工文本成为重要课题，尤其是在阿拉伯语环境下缺乏相关研究，需要评估多种预训练模型在该任务的表现。

Method: 作者选用AraELECTRA、CAMeLBERT和XLM-RoBERTa三种预训练变换器模型，对它们分别在提供的数据集上进行微调，并用于二分类检测AI生成文本。

Result: XLM-RoBERTa获得了最高的F1分数0.7701，表现优于针对阿拉伯语的专用模型AraELECTRA和CAMeLBERT。

Conclusion: 多语言模型在阿拉伯语AI文本检测任务中展现出强泛化能力，优于专门化的阿拉伯语模型，说明AI生成文本检测任务存在一定复杂性。

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [137] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 该论文提出了一种用于评估大语言模型（LLMs）好奇心的新框架，并探讨了好奇心对模型推理和学习能力的作用。实验证明：LLMs在信息渴望上超过人类，但在不确定环境下表现更保守，好奇行为能增强其推理和学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言处理中的进步，人们开始关注其是否具备类似人类的好奇心驱动学习能力。此前尚缺乏系统评估LLMs好奇心的方法。该论文致力于填补这一空白，并通过好奇心对模型认知能力提升的探索，拓展LLMs应用潜力。

Method: 论文基于人类五维好奇心量表（5DCR），设计了涵盖信息寻求、刺激寻求和社会好奇等维度的LLMs好奇心评测框架。通过实验分析LLMs在不同维度的表现，并考察好奇心与模型推理、主动学习等行为之间的关系。

Result: 实验显示LLMs在知识渴望方面表现出更强的好奇心，但在不确定环境下选择较为保守。此外，研究证实好奇行为能增强模型的推理和主动学习能力。

Conclusion: LLMs具备类似人类好奇心潜力，本研究为其学习能力开发与创新研究提供了实验支持。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [138] [The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI](https://arxiv.org/abs/2510.20647)
*Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully*

Main category: cs.CL

TL;DR: 本文系统性地比较了大型推理模型（LRMs）在回答非英语问题时，以英语推理与原问题语言推理的表现，发现大多数情况下英语推理准确率更高，但也存在因翻译带来的关键失误。


<details>
  <summary>Details</summary>
Motivation: 近年来，LRMs在多领域展现了强大的推理能力，但其多语言推理能力尚未深入研究。尤其在面对非英语问题时，模型倾向于用英语进行推理，这对结果解释性和对语言、文化细微差别的把握构成挑战。

Method: 作者设计了系统性的实验，使用MGSM和GPQA Diamond两个任务进行评估，不仅考察最终答案的准确率，也分析了推理轨迹中的认知特征。通过对比英语推理与原语言推理，揭示了重要的认知和表现差异。

Result: 实验发现，英语推理轨迹中展现出更多的认知行为，且最终答案准确率通常更高，且在任务复杂性增加时，英语与原语言推理的差距进一步扩大。然而，英语推理也更容易在翻译环节出现特定错误，而这些在原语言推理中是可以避免的。

Conclusion: 尽管采用英语推理对于LRMs在复杂任务中总体有利，但也引入了通过翻译导致的特有失误，因此多语言推理能力的提升和跨语言一致性优化仍然是未来重要的研究方向。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical,
scientific, and other question-answering tasks, but their multilingual
reasoning abilities remain underexplored. When presented with non-English
questions, LRMs often default to reasoning in English, raising concerns about
interpretability and the handling of linguistic and cultural nuances. We
systematically compare an LRM's reasoning in English versus the language of the
question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond
measuring answer accuracy, we also analyze cognitive attributes in the
reasoning traces. We find that English reasoning traces exhibit a substantially
higher presence of these cognitive behaviors, and that reasoning in English
generally yields higher final-answer accuracy, with the performance gap
increasing as tasks become more complex. However, this English-centric strategy
is susceptible to a key failure mode - getting "Lost in Translation," where
translation steps lead to errors that would have been avoided by question's
language reasoning.

</details>


### [139] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 论文提出了CantoNLU，这是面向粤语自然语言理解的全新评测基准，涵盖7项任务，并评估了不同模型在这些任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 粤语虽然使用人数众多，但受政策及双语现象影响，相关自然语言处理资源稀缺，缺乏系统性的评测框架，限制了粤语NLP的发展。

Method: 作者构建了CantoNLU基准集，包括词义消歧、语言可接受性判断、语言检测、自然语言推理、情感分析、词性标注和依存句法分析等七项任务。并将普通话模型（未用粤语数据训练）、粤语继续预训练模型（在普通话模型基础上用粤语文本做继续预训练）、以及从零开始训练的单语粤语模型进行了性能对比。

Result: 粤语继续预训练的模型整体表现最佳，而单语粤语模型在句法相关任务上的表现更优。普通话模型在部分任务上也有较好表现，显示在粤语数据极少时直接迁移亦有可行性。

Conclusion: CantoNLU为粤语NLP研究者提供了首个大规模评测基准，相关数据与模型全部开源，有助于推动粤语自然语言处理领域的发展。

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [140] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: 本文提出了“神经多样性”作为降低大语言模型幻觉发生概率的新机制，并设计了ND-LoRA方法，有效减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型的参数量、计算和数据规模不断增加，幻觉现象（生成与事实不符信息）依然频繁出现，影响了模型的实际应用可靠性。作者希望从新的视角（即模型内部表征相关性）解决该难题。

Method: 提出“神经多样性”这一概念，理论上证明表征去相关能够减少幻觉，并且借鉴投资组合理论，定量描述了表征相关性与幻觉概率的关系。随后提出ND-LoRA方法，将LoRA并行适配器与Barlow Twins正则化结合，增强表征多样性，并在实验中验证了该法可有效降低幻觉率。

Result: ND-LoRA最多可将幻觉率降低25.6%，平均减少14.6%，且不会损害整体准确率。消融实验表明两项技术协同作用，因果实验证明神经多样性是真正的关键因素，分析发现神经相关性每增加0.1%，幻觉增幅达3.8%。此外，不同任务最佳神经多样性存在差异。

Conclusion: 神经多样性是改善语言模型可靠性的第三个维度，独立于参数和数据规模。通过优化神经多样性，能够在固定预算下显著提升模型输出的可靠性。

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [141] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: 本文针对MBR（最小贝叶斯风险）解码在开放式任务（如对话与指令跟随）中的局限性，提出了三种改进的实用函数，使其更敏感于生成结果中潜在的结构差异，并在公开基准上取得了明显改进。


<details>
  <summary>Details</summary>
Motivation: 传统的MBR解码在机器翻译等受限场景下表现良好，但在对话、指令跟随等开放式任务中，常用的基于相似度的效用函数可能不能捕捉生成结果中的潜在结构，导致输出结果虽然能代表整体分布，但在特定结构类别中并不最优。作者希望让MBR更好识别并利用这些潜在结构，提高生成质量。

Method: 作者提出了三种轻量级的实用函数调整方案，使MBR解码对生成结果的结构多样性更加敏感。为验证假设，作者构建了包含对话行为、情感以及响应结构三类潜在结构的数据集，并提出了两项评估MBR结构最优性的指标。在标准和真实任务基准（AlpacaEval, MT-Bench）上对方法进行了实证评测。

Result: 实验证明，常见的单纯相似度效用函数在结构最优性评估下表现不足，而改进方法显著提升了结构最优性。在AlpacaEval与MT-Bench等现实任务评测中，提出方法令模型输出质量（win rate）提升高达13.7个百分点。

Conclusion: 增强MBR解码中实用函数的结构敏感性，能有效提升开放式生成任务中的输出质量，证明了MBR在更广泛任务中的应用潜力。

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [142] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: 本研究评估了真实用户与大语言模型（LLMs）在处理涉及隐私信息场景下的响应质量与隐私保护能力感知，发现用户之间评价不一致，而用作测评的代理LLM之间有高度一致性，但与真实用户评价低相关，提出应以用户为中心对LLMs进行隐私与帮助性的测量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用在日常任务中，如起草邮件、总结会议、答复健康问题，用户常需输入私人信息，但现有用于评估LLMs隐私保护能力的基准主要基于代理模型而非真实用户感知，忽略了用户实际对隐私保护和帮助性的认知差异。

Method: 该研究采用用户实验，共94名参与者，结合90个隐私敏感场景（来自PrivacyLens）评价LLM响应的隐私保护质量与帮助性；并比对了5个代理LLM的评价结果与用户评价的一致性。

Result: 结果显示，用户对同一LLM响应在隐私与帮助性上的评价高度分歧，而代理LLM之间有很高一致性，但各LLM与用户评价的相关性较低。

Conclusion: LLM响应的隐私和帮助性感知具有个体差异，代理LLM无法准确代表真实用户的评价。未来应进行以用户为中心的研究，并探索提升代理LLM与用户感知对齐的方法。

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [143] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: 本文旨在开发和评估自然语言处理（NLP）技术，从临床病历中自动抽取化疗药物氟嘧啶类的治疗及毒性反应信息。研究比较了多种NLP模型，发现大语言模型（LLM）方法表现最优。


<details>
  <summary>Details</summary>
Motivation: 氟嘧啶类药物应用广泛，但带来如手足综合征和心脏毒性等严重不良反应，而这些毒性信息常以非结构化文本记录。如何高效、自动地提取相关信息，以支持癌症治疗和药物安全研究，是一大临床挑战。

Method: 研究构建了包含236份临床病历的人工标注数据集，并评估了基于规则、机器学习（随机森林、SVM、逻辑回归）、深度学习（BERT、ClinicalBERT）以及大语言模型（零样本和错误分析提示）等多种NLP抽取方法，采用80:20训练-测试集划分。

Result: 结果显示，LLM基于错误分析的提示法在治疗和毒性抽取上取得了最高的准确率、召回率和F1分数（F1=1.000）。零样本LLM抽取治疗的F1同为1.000，毒性为0.876。各机器学习方法F1最高为0.937。深度学习较弱，基于BERT和ClinicalBERT的F1均低于LLM。基线规则法F1约0.85。

Conclusion: LLM方法在自动抽取氟嘧啶类治疗及毒性信息方面效果最佳，优于常规机器学习和深度学习模型。该方法有望促进肿瘤研究和药物不良反应监测。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [144] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 本论文系统分析了大规模推理模型（LRM）在机器翻译评价中的应用，并提出校准其“思考”过程的方法，显著提升了评估效果和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大规模推理模型已经展现出优异的复杂推理能力，但其在作为机器翻译自动评价工具上的潜力尚未被充分探索。研究动机是理解并提升LRM在机器翻译评价任务上的表现。

Method: 作者深入分析了LRM作为评价者时面临的挑战，包括需要定制评价材料、在简单例子上过度推理以及评分机制导致的高估。为此，论文提出通过让LRM学习合成、类人的思考轨迹来校准其推理过程。

Result: 在WMT24指标基准上实验证明，提出的方法将LRM的推理预算降低约35倍，并在7B到32B等不同规模的模型上均提升了评价性能（例如某模型相关性提升了+8.7）。

Conclusion: 高效校准后的LRM有望成为更精细、自动化的机器翻译质量评估工具，显著提升评价的准确性和效率。

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [145] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: 本文针对当前大语言模型（LLM）评测方法忽视实际应用中的公平性等Responsible AI需求的问题，提出并构建了一个结合现实应用与公平性属性的数据集，可用于较全面地评估LLM的质量、公平性等多维表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测多为通用文本生成和高层任务，难以细致考察实际应用中的公平性。不同应用对保护性属性（如性别等）的关注点不同，单一评测方法难以适应多实际场景，需有更贴近实际应用、兼顾Responsible AI维度的新评测方式。

Method: 作者围绕具体应用场景——基于产品特性生成产品描述，构建了一个参数化的数据集，结合性别形容词与不同产品品类，从而生成大量带标签的提示（prompts）；通过该数据集，评测LLM在质量、真实性、安全性与公平性等不同维度的表现，并据此提出新的评测建议。

Result: 基于此数据集，论文展示了用这些应用驱动的多维度标签化提示，能够具体揭示当前LLM在质量、真实性、安全性和公平性上的表现差异及潜在不足，验证该方法的有效性和实际意义。

Conclusion: 作者构建的数据集和评估方法为LLM的Responsible AI多维评测提供了实用的新途径，也为研究者落地公平性等评测提供了具体资源，有助于推动LLM相关评测标准与实践更贴近实际应用需求。

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [146] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: 本文研究了一系列结合线性注意力和稀疏注意力的新模型，通过引入混合token处理器（token mixer），提高了在需大量检索过往信息任务中的表现。提出了可学习的token淘汰方法，并结合滑动窗口注意力和轻量级CNN，兼顾效率与效果。实验显示新方法在检索密集任务中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 传统的线性注意力虽然高效，但由于其有限的状态存储，容易遗忘早期输入，不适合需要频繁回溯信息的任务。为解决这一问题，作者希望在保持高效性的同时增强对历史信息的检索能力，因此探索了不同稀疏注意力和token淘汰策略的混合模型。

Method: 作者提出将token mixer与不同的稀疏注意力机制交替使用，包括稀疏注意力结合token淘汰、query-aware的原生稀疏注意力，并创新性设计了可学习的token淘汰技术。通过滑动窗口注意力结合CNN，有选择性地保留每个head上一组关键信息（KV对），使时间与空间复杂度保持常数。并实现了高效的Triton自定义稀疏注意力算子。

Result: 实验在多个检索密集型任务基准上验证了所提模型的有效性。结果显示，与纯线性注意力模型相比，混合模型在保有高效率的同时，显著提升了检索表现，证明关键token信息的自适应保留与稀疏访问策略的结合在该类任务下具有明显优势。

Conclusion: 作者提出的混合注意力机制和可学习的token淘汰方法有效缓解了线性注意力模型易遗忘历史信息的问题。在维持高效计算的同时，提升了模型处理检索密集型任务的能力，对高效率自然语言处理模型的设计有着积极的推动作用。

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


### [147] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出了一种简单高效的平均池化（mean-pooling）软上下文压缩方法，在RAG任务中优于已有的compression-token架构，并探讨了多压缩率训练的效果。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文在RAG系统中的计算开销很大，软上下文压缩作为主流减负策略，然而主流方法（如compression-tokens）仍有提升空间，作者希望找到更强、更简洁的压缩方法。

Method: 提出利用均值池化对输入进行连续压缩、缩短长度，并分析了压缩器在输出多个压缩率下的训练表现，通过横跨数据集、模型族、模型规模和压缩率的实验深入验证方法表现。

Result: 在各类QA任务和多种条件下，平均池化方法表现最优，尤其在多压缩率训练时性能下降较小。同时，随着架构和训练方法的变化，压缩-性能折中变得更为复杂。

Conclusion: 均值池化是一种高效、简单的软上下文压缩方法，在多种RAG应用场景下均表现优异；但压缩方法的折中和选择需要更细致权衡。

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [148] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: 该论文讨论了当前检测大语言模型（LLM）生成文本面临的定义、评测与实际应用挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用，如何有效检测其生成的文本成为研究热点。但，目前尚无统一、精准的“LLM生成文本”定义，这限制了检测方法的发展和评估。

Method: 论文主要分析和讨论了现有LLM文本检测领域中的定义困境、多样化场景带来的挑战，以及人类编辑和LLM对用户潜移默化影响对检测界限的模糊。同时也评述了已有基准和评测手段的不足。

Result: 作者指出，现有检测方法通常只覆盖LLM潜在生成文本的一个子集，实际检测效果受多种因素影响。检测结果往往被误解，意义也在减弱。

Conclusion: 检测器在特定场景下仍有价值，但检测结果应仅作参考，不能作为决定性依据。

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [149] [Configuration-Dependent Robot Kinematics Model and Calibration](https://arxiv.org/abs/2510.19962)
*Chen-Lung Lu,Honglu He,Agung Julius,John T. Wen*

Main category: cs.RO

TL;DR: 提出了一种基于配置的机器人运动学标定方法，通过在多个配置下使用局部POE模型，并利用Fourier基函数插值，实现了全工作空间内高精度标定，显著提升工业机器人定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统机器人运动学模型常因非几何因素而出现配置相关的误差，影响机器人在复杂任务中的精确性，因此迫切需要一种能在全工作空间下提升精度的标定方法。

Method: 在多个配置下辨识出局部的Product-of-Exponential(POE)运动学模型，并采用Fourier基函数，以肩关节和肘关节角度为参数进行插值，从而构建一个全局运动学模型。该方法兼具参数连续性和高效训练优势。

Result: 在两台6自由度工业机器人上的实验表明，该方法将最大定位误差降低了50%以上，实现了亚毫米级精度，优于现有神经网络和自动编码器方法，并且训练效率更高。

Conclusion: 所提出的标定框架适用于工业机器人全工作空间的高精度任务，尤其对配置相关误差较大的机器人效果突出，并已在双机器人协作任务中展现出良好的实用性和可重复性。

Abstract: Accurate robot kinematics is essential for precise tool placement in
articulated robots, but non-geometric factors can introduce
configuration-dependent model discrepancies. This paper presents a
configuration-dependent kinematic calibration framework for improving accuracy
across the entire workspace. Local Product-of-Exponential (POE) models,
selected for their parameterization continuity, are identified at multiple
configurations and interpolated into a global model. Inspired by joint gravity
load expressions, we employ Fourier basis function interpolation parameterized
by the shoulder and elbow joint angles, achieving accuracy comparable to neural
network and autoencoder methods but with substantially higher training
efficiency. Validation on two 6-DoF industrial robots shows that the proposed
approach reduces the maximum positioning error by over 50%, meeting the
sub-millimeter accuracy required for cold spray manufacturing. Robots with
larger configuration-dependent discrepancies benefit even more. A dual-robot
collaborative task demonstrates the framework's practical applicability and
repeatability.

</details>


### [150] [Push Anything: Single- and Multi-Object Pushing From First Sight with Contact-Implicit MPC](https://arxiv.org/abs/2510.19974)
*Hien Bui,Yufeiyang Gao,Haoran Yang,Eric Cui,Siddhant Mody,Brian Acosta,Thomas Stephen Felix,Bibit Bianchini,Michael Posa*

Main category: cs.RO

TL;DR: 本文提出了一种改进的接触隐式模型预测控制（CI-MPC）方法，名为C3+，能高效可靠地完成多种平面非抓取推物任务，包括多物体情景，在真实硬件上实验取得很高成功率。


<details>
  <summary>Details</summary>
Motivation: 非抓取式操作因未知物理属性及复杂接触交互一直是机器人领域挑战，现有CI-MPC方法只能处理有限的精心设计场景。论文旨在突破CI-MPC适用范围，解决多对象、复杂接触环境下的高效推物问题。

Method: 作者提出并实现了C3+算法，一种升级版CI-MPC，集成了对象扫描、网格重建及硬件执行的全套流水线，显著提高了计算速度并能实时处理多物体推送任务。

Result: C3+在真实硬件上推送33种不同对象的实验中，整体成功率达98%，多物体推送时达到亚分钟到几分钟级别的目标时间，远快于前代方法C3。

Conclusion: C3+显著提升了机器人在复杂、多对象非抓取操作任务中的效率与适应性，为机器人在现实复杂动态环境中的自主操作提供了坚实基础。

Abstract: Non-prehensile manipulation of diverse objects remains a core challenge in
robotics, driven by unknown physical properties and the complexity of
contact-rich interactions. Recent advances in contact-implicit model predictive
control (CI-MPC), with contact reasoning embedded directly in the trajectory
optimization, have shown promise in tackling the task efficiently and robustly,
yet demonstrations have been limited to narrowly curated examples. In this
work, we showcase the broader capabilities of CI-MPC through precise planar
pushing tasks over a wide range of object geometries, including multi-object
domains. These scenarios demand reasoning over numerous inter-object and
object-environment contacts to strategically manipulate and de-clutter the
environment, challenges that were intractable for prior CI-MPC methods. To
achieve this, we introduce Consensus Complementarity Control Plus (C3+), an
enhanced CI-MPC algorithm integrated into a complete pipeline spanning object
scanning, mesh reconstruction, and hardware execution. Compared to its
predecessor C3, C3+ achieves substantially faster solve times, enabling
real-time performance even in multi-object pushing tasks. On hardware, our
system achieves overall 98% success rate across 33 objects, reaching pose goals
within tight tolerances. The average time-to-goal is approximately 0.5, 1.6,
3.2, and 5.3 minutes for 1-, 2-, 3-, and 4-object tasks, respectively. Project
page: https://dairlab.github.io/push-anything.

</details>


### [151] [Simultaneous learning of state-to-state minimum-time planning and control](https://arxiv.org/abs/2510.20008)
*Swati Dantu,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机最小时间飞行策略学习框架，实现了在任意起止状态之间进行泛化的高效飞行。通过模拟和实飞实验验证了方法的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机自主竞速方法虽速度快、敏捷性高，但只能应用于预设轨迹，限制了实际应用。如何在任意起止点间实现快速泛化的飞行控制，是一个尚未解决的难题。

Method: 提出强化学习方案，联合学习状态到状态的最小时间路径规划和控制。利用点质量模型（PMM）轨迹作为代理奖励近似最优飞行目标，并引入课程学习以提升训练效率和泛化能力。

Result: 方法在仿真环境和现实外场无人机实验中都取得良好结果，相比非线性模型预测控制方法有更优性能。消融实验显示课程学习对提升泛化能力有显著作用。

Conclusion: 所提方法能在资源受限硬件上实现高效、具有泛化能力的无人机最小时间飞行控制，适用于实际任意起止点间的自主飞行任务。

Abstract: This paper tackles the challenge of learning a generalizable minimum-time
flight policy for UAVs, capable of navigating between arbitrary start and goal
states while balancing agile flight and stable hovering. Traditional
approaches, particularly in autonomous drone racing, achieve impressive speeds
and agility but are constrained to predefined track layouts, limiting
real-world applicability. To address this, we propose a reinforcement
learning-based framework that simultaneously learns state-to-state minimum-time
planning and control and generalizes to arbitrary state-to-state flights. Our
approach leverages Point Mass Model (PMM) trajectories as proxy rewards to
approximate the true optimal flight objective and employs curriculum learning
to scale the training process efficiently and to achieve generalization. We
validate our method through simulation experiments, comparing it against
Nonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories
and conducting ablation studies to assess the impact of curriculum learning.
Finally, real-world experiments confirm the robustness of our learned policy in
outdoor environments, demonstrating its ability to generalize and operate on a
small ARM-based single-board computer.

</details>


### [152] [Calibration of Parallel Kinematic Machine Based on Stewart Platform-A Literature Review](https://arxiv.org/abs/2510.20070)
*Sourabh Karmakar,Apurva Patel,Cameron J. Turner*

Main category: cs.RO

TL;DR: 本文综述了以Stewart平台为代表的6自由度并联机器人在高精度应用中的标定方法，重点关注逆向运动学标定策略及其主要技术路线。


<details>
  <summary>Details</summary>
Motivation: 高精度应用如医疗、航天和半导体制造对机器人的微米及纳米级三维运动控制提出了极高要求，因此对并联机器人（PKM）的高精度标定非常关键。

Method: 文章分类梳理了现有的PKM标定方法，包括基于外部仪器、基于约束、以及自标定三大类，主要探讨了逆向运动学在标定中的应用和优点，对各技术方法进行了对比和总结。

Result: 当前研究主要通过优化标定策略来提升平台位置和姿态的精度，分析了来源于结构误差以及部分环境因素的误差对结果的影响，且大部分标定都是在无负载条件下完成的。

Conclusion: 文献综述清晰呈现了现有PKM标定方法的研究现状及其不足，为该领域后续研究拓展了思路，建议未来进一步扩展误差来源分析、考虑实际工况下的标定。

Abstract: Stewart platform-based Parallel Kinematic (PKM) Machines have been
extensively studied by researchers due to their inherent finer control
characteristics. This has opened its potential deployment opportunities in
versatile critical applications like the medical field, engineering machines,
space research, electronic chip manufacturing, automobile manufacturing, etc.
All these precise, complicated, and repeatable motion applications require
micro and nano-scale movement control in 3D space; a 6-DOF PKM can take this
challenge smartly. For this, the PKM must be more accurate than the desired
application accuracy level and thus proper calibration for a PKM robot is
essential. Forward kinematics-based calibration for such hexapod machines
becomes unnecessarily complex and inverse kinematics complete this task with
much ease. To analyze different techniques, an external instrument-based,
constraint-based, and auto or self-calibration-based approaches have been used
for calibration. This survey has been done by reviewing these key
methodologies, their outcome, and important points related to inverse
kinematic-based PKM calibrations in general. It is observed in this study that
the researchers focused on improving the accuracy of the platform position and
orientation considering the errors contributed by a single source or multiple
sources. The error sources considered are mainly structural, in some cases,
environmental factors are also considered, however, these calibrations are done
under no-load conditions. This study aims to understand the current state of
the art in this field and to expand the scope for other researchers in further
exploration in a specific area.

</details>


### [153] [Design of a Bed Rotation Mechanism to Facilitate In-Situ Photogrammetric Reconstruction of Printed Parts](https://arxiv.org/abs/2510.20079)
*Travis A. Roberts,Sourabh Karmakar,Cameron J. Turner*

Main category: cs.RO

TL;DR: 本文提出了一种专为高精度FDM聚合物3D打印研究设计的测试平台，具备闭环反馈、全参数监控和创新的加热床旋转及在位摄影测量能力，可精确研究打印参数与缺陷间的关联。


<details>
  <summary>Details</summary>
Motivation: 现有消费级和商用FDM 3D打印机都不开放，且缺乏科研所需的可调控和扩展性，难以满足实验对可控精度和参数监控的需求。

Method: 作者自研并制造了一台FDM 3D打印测试平台，能够精准控制和监测打印各参数，实现闭环位置反馈、温度/湿度监控、全过程视频采集与摄影测量。同时，创新设计出加热床可旋转机构，用更少相机实现在打印过程中的几何重建。

Result: 该平台成功实现了实时过程控制与数据采集，通过旋转加热床及摄影测量技术，能在打印过程中准确捕捉零件几何变化并用于缺陷分析。

Conclusion: 作者平台大幅提升了FDM打印过程研究的可重复性与精确性，为研究参数-缺陷关系提供了有力工具，并展现了摄影测量和旋转加热床在3D打印监控领域的创新潜力。

Abstract: Additive manufacturing, or 3D printing, is a complex process that creates
free-form geometric objects by sequentially placing material to construct an
object, usually in a layer-by-layer process. One of the most widely used
methods is Fused Deposition Modeling (FDM). FDM is used in many of the
consumer-grade polymer 3D printers available today. While consumer grade
machines are cheap and plentiful, they lack many of the features desired in a
machine used for research purposes and are often closed-source platforms.
Commercial-grade models are more expensive and are also usually closed-source
platforms that do not offer flexibility for modifications often needed for
research. The authors designed and fabricated a machine to be used as a test
bed for research in the field of polymer FDM processes. The goal was to create
a platform that tightly controls and/or monitors the FDM build parameters so
that experiments can be repeated with a known accuracy. The platform offers
closed loop position feedback, control of the hot end and bed temperature, and
monitoring of environment temperature and humidity. Additionally, the platform
is equipped with cameras and a mechanism for in-situ photogrammetry, creating a
geometric record of the printing throughout the printing process. Through
photogrammetry, backtracking and linking process parameters to observable
geometric defects can be achieved. This paper focuses on the design of a novel
mechanism for spinning the heated bed to allow for photogrammetric
reconstruction of the printed part using a minimal number of cameras, as
implemented on this platform.

</details>


### [154] [PathFormer: A Transformer with 3D Grid Constraints for Digital Twin Robot-Arm Trajectory Generation](https://arxiv.org/abs/2510.20161)
*Ahmed Alanazi,Duy Ho,Yugyung Lee*

Main category: cs.RO

TL;DR: 本文提出了一种基于路径的Transformer模型，实现了更高效与准确的机器人运动轨迹规划，并展示了优异的现实任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的序列模型在处理机器人臂复杂任务时，往往未能充分利用运动结构信息，导致轨迹不合理或效率低下。因此，提出一种结构感知模型以提升轨迹规划的准确性和有效性。

Method: 作者提出采用三栅格（where/what/when）表示法，结合约束掩码解码策略，从而在生成动作顺序时强制满足空间、顺序及任务约束。模型在大量标注轨迹数据上训练，并通过仿真和真实机器人测试实施验证。

Result: 模型轨迹逐步精度达到89.44%，精确率93.32%，召回率89.44%，F1分数90.40%，生成的路径99.99%合法。在真实机器人xArm Lite 6上测试，达到97.5%触达成功率及92.5%抓取成功率，并能在复杂场景60项语言指定任务中取得86.7%端到端成功率。

Conclusion: 路径结构化表达结合Transformer模型，显著提升了机器人轨迹生成的准确性、可靠性和可解释性，有效衔接了基于图的规划与序列学习，为通用操控与虚实迁移奠定了基础。

Abstract: Robotic arms require precise, task-aware trajectory planning, yet sequence
models that ignore motion structure often yield invalid or inefficient
executions. We present a Path-based Transformer that encodes robot motion with
a 3-grid (where/what/when) representation and constraint-masked decoding,
enforcing lattice-adjacent moves and workspace bounds while reasoning over task
graphs and action order. Trained on 53,755 trajectories (80% train / 20%
validation), the model aligns closely with ground truth -- 89.44% stepwise
accuracy, 93.32% precision, 89.44% recall, and 90.40% F1 -- with 99.99% of
paths legal by construction. Compiled to motor primitives on an xArm Lite 6
with a depth-camera digital twin, it attains up to 97.5% reach and 92.5% pick
success in controlled tests, and 86.7% end-to-end success across 60
language-specified tasks in cluttered scenes, absorbing slips and occlusions
via local re-grounding without global re-planning. These results show that
path-structured representations enable Transformers to generate accurate,
reliable, and interpretable robot trajectories, bridging graph-based planning
and sequence-based learning and providing a practical foundation for
general-purpose manipulation and sim-to-real transfer.

</details>


### [155] [Reinforcement Learning-based Robust Wall Climbing Locomotion Controller in Ferromagnetic Environment](https://arxiv.org/abs/2510.20174)
*Yong Um,Young-Ha Shin,Joon-Ha Kim,Soonpyo Kwon,Hae-Won Park*

Main category: cs.RO

TL;DR: 本论文提出了一种针对四足磁吸爬墙机器人强化学习的新方法，通过物理建模并引入现实不确定性，有效提升了机器人在磁吸失效情况下的稳定性和恢复能力。


<details>
  <summary>Details</summary>
Motivation: 磁吸式四足爬墙机器人在实际应用中面临脚部磁吸不确定性的问题，如部分接触、空气间隙、吸附失败等，传统控制方法无法有效应对这些复杂情况，因此需要一种更鲁棒的学习方法来提升爬墙的成功率与安全性。

Method: 作者在仿真中引入了真实的磁吸物理模型，涵盖了部分接触、气隙敏感性与概率性吸附失败。提出三阶段课题制（curriculum learning）：第一阶段在平地上学习爬行无磁吸，第二阶段逐步旋转重力方向并激活磁吸模型，第三阶段增加随机磁吸失效训练滑移恢复能力。将强化学习策略和现实磁吸建模结合。

Result: 在仿真中，所提方法在磁吸退化情况下实现了高成功率、强劲保持和快速恢复，相比假设完全磁吸的MPC基线，提出的控制策略在间歇性脱附时仍可保持运动。硬件测试也证实，机器人能稳定地在垂直钢壁上爬行，并对瞬时错位与不完整吸附具有鲁棒性。

Conclusion: 通过将递进式课题学习与真实磁吸建模相结合，作者为磁吸爬壁机器人建立了坚韧的从仿真到现实的强化学习框架，使其能胜任复杂环境下的垂直运动与失效恢复。

Abstract: We present a reinforcement learning framework for quadrupedal wall-climbing
locomotion that explicitly addresses uncertainty in magnetic foot adhesion. A
physics-based adhesion model of a quadrupedal magnetic climbing robot is
incorporated into simulation to capture partial contact, air-gap sensitivity,
and probabilistic attachment failures. To stabilize learning and enable
reliable transfer, we design a three-phase curriculum: (1) acquire a crawl gait
on flat ground without adhesion, (2) gradually rotate the gravity vector to
vertical while activating the adhesion model, and (3) inject stochastic
adhesion failures to encourage slip recovery. The learned policy achieves a
high success rate, strong adhesion retention, and rapid recovery from
detachment in simulation under degraded adhesion. Compared with a model
predictive control (MPC) baseline that assumes perfect adhesion, our controller
maintains locomotion when attachment is intermittently lost. Hardware
experiments with the untethered robot further confirm robust vertical crawling
on steel surfaces, maintaining stability despite transient misalignment and
incomplete attachment. These results show that combining curriculum learning
with realistic adhesion modeling provides a resilient sim-to-real framework for
magnetic climbing robots in complex environments.

</details>


### [156] [A Contact-Driven Framework for Manipulating in the Blind](https://arxiv.org/abs/2510.20177)
*Muhammad Suhail Saleem,Lai Yuan,Maxim Likhachev*

Main category: cs.RO

TL;DR: 本文提出了一种集成触觉反馈与结构先验的新型框架，使机器人能在视觉受限环境下高效完成操作任务。该方法在模拟和真实环境中均表现出优异性能，任务完成速度比传统基线快2倍。


<details>
  <summary>Details</summary>
Motivation: 许多实际场景下视觉信息不足（如遮挡、光照不佳等），而这些受限环境中，机器人需要像人类一样依赖接触反馈来完成任务；同时，这些环境常具有明显结构特征（例如水槽下管道分布），可用以推断未见区域，提升操作效率和安全性。

Method: 所提出的框架由三个部分组成：(1) 基于关节力矩感觉与颗粒滤波的接触检测与定位模块；(2) 利用接触观测历史构建部分占用地图并通过学习预测器外推至未探索区域的占用估计模块；(3) 在处理接触组合和环境预测噪声的前提下，确保既安全又高效完成任务的路径规划模块。

Result: 在仿真和真实UR10e机械臂上，在厨房水槽管道阀门操作及复杂货架取物任务中，框架均能可靠完成任务，任务用时最多缩短2倍，模块消融实验验证了各子系统的重要性。

Conclusion: 集成接触反馈与结构先验的方法可大幅提升机器人在未知、受限环境下的任务完成效率和鲁棒性，具备广泛实际应用前景。

Abstract: Robots often face manipulation tasks in environments where vision is
inadequate due to clutter, occlusions, or poor lighting--for example, reaching
a shutoff valve at the back of a sink cabinet or locating a light switch above
a crowded shelf. In such settings, robots, much like humans, must rely on
contact feedback to distinguish free from occupied space and navigate around
obstacles. Many of these environments often exhibit strong structural
priors--for instance, pipes often span across sink cabinets--that can be
exploited to anticipate unseen structure and avoid unnecessary collisions. We
present a theoretically complete and empirically efficient framework for
manipulation in the blind that integrates contact feedback with structural
priors to enable robust operation in unknown environments. The framework
comprises three tightly coupled components: (i) a contact detection and
localization module that utilizes joint torque sensing with a contact particle
filter to detect and localize contacts, (ii) an occupancy estimation module
that uses the history of contact observations to build a partial occupancy map
of the workspace and extrapolate it into unexplored regions with learned
predictors, and (iii) a planning module that accounts for the fact that contact
localization estimates and occupancy predictions can be noisy, computing paths
that avoid collisions and complete tasks efficiently without eliminating
feasible solutions. We evaluate the system in simulation and in the real world
on a UR10e manipulator across two domestic tasks--(i) manipulating a valve
under a kitchen sink surrounded by pipes and (ii) retrieving a target object
from a cluttered shelf. Results show that the framework reliably solves these
tasks, achieving up to a 2x reduction in task completion time compared to
baselines, with ablations confirming the contribution of each module.

</details>


### [157] [NODA-MMH: Certified Learning-Aided Nonlinear Control for Magnetically-Actuated Swarm Experiment Toward On-Orbit Proof](https://arxiv.org/abs/2510.20231)
*Yuta Takahashi,Atsuki Ochi,Yoichi Tomioka,Shin-Ichiro Sakai*

Main category: cs.RO

TL;DR: 本文通过实验证实了一种基于学习辅助磁场交互的卫星蜂群大规模控制方法，展示了其在多卫星编队长期维持中的可行性。提出时间积分电流控制结合学习算法，解决了卫星数量增加带来的非完整约束、欠驱动、可扩展性和计算成本等问题。


<details>
  <summary>Details</summary>
Motivation: 多卫星编队在航天任务中需求日益增加，而如何高效、稳定地长期保持卫星之间的队形是一个难题。传统的物理驱动方式难以兼顾大规模可扩展与能耗优化，因此探索磁力驱动结合智能化控制方法具有重要意义。

Method: 作者通过设计双轴线圈和地面气浮实验平台，模拟轨道动力学，采用基于学习的时间积分电流控制方法来协调多颗卫星的磁场交互，并提出NODA-MMH（基于神经网络的动力学最优偶极分配）框架，实现去中心化电流管理和功耗优化。

Result: 实验验证了学习辅助时间积分电流控制方法在提高多卫星系统可控性、保证误差界及去中心化电流管理方面的有效性。结果表明该方法具备良好的理论与实际表现，并可优化功耗分配。

Conclusion: 该研究验证了磁力驱动结合学习辅助控制在多卫星长时编队维持中的有效性和可扩展性，对未来多卫星任务的编队控制和能量管理提供了理论与实践支持。

Abstract: This study experimentally validates the principle of large-scale satellite
swarm control through learning-aided magnetic field interactions generated by
satellite-mounted magnetorquers. This actuation presents a promising solution
for the long-term formation maintenance of multiple satellites and has
primarily been demonstrated in ground-based testbeds for two-satellite position
control. However, as the number of satellites increases beyond three,
fundamental challenges coupled with the high nonlinearity arise: 1)
nonholonomic constraints, 2) underactuation, 3) scalability, and 4)
computational cost. Previous studies have shown that time-integrated current
control theoretically solves these problems, where the average actuator outputs
align with the desired command, and a learning-based technique further enhances
their performance. Through multiple experiments, we validate critical aspects
of learning-aided time-integrated current control: (1) enhanced controllability
of the averaged system dynamics, with a theoretically guaranteed error bound,
and (2) decentralized current management. We design two-axis coils and a
ground-based experimental setup utilizing an air-bearing platform, enabling a
mathematical replication of orbital dynamics. Based on the effectiveness of the
learned interaction model, we introduce NODA-MMH (Neural power-Optimal Dipole
Allocation for certified learned Model-based Magnetically swarm control
Harness) for model-based power-optimal swarm control. This study complements
our tutorial paper on magnetically actuated swarms for the long-term formation
maintenance problem.

</details>


### [158] [Kinaema: a recurrent sequence model for memory and pose in motion](https://arxiv.org/abs/2510.20261)
*Mert Bulent Sariyildiz,Philippe Weinzaepfel,Guillaume Bono,Gianluca Monaci,Christian Wolf*

Main category: cs.RO

TL;DR: 本论文提出了一种新型机器人模型Kinaema，实现了在不显式存储观测历史的前提下，对空间环境的高效记忆与定位。该模型对历史视觉观测进行压缩，并能根据查询图片预测当前位置与目标的相对位置。


<details>
  <summary>Details</summary>
Motivation: 空间感知对于机器人自主导航至关重要，尤其是在机器人需要利用事前观测信息以提升后续任务效率的持续操作场景下。现有方法存储观测历史往往面临内存和效率瓶颈，因此亟需无需显式历史存储、可处理长环境序列的新方法。

Method: 提出Kinaema模型，通过递归方式用transformer更新和压缩传感器读数至隐空间表征，维持隐式潜在记忆。模型可流式整合观测信息，并在收到查询图片后预测目标空间与当前位置的相对关系。未采用显式历史存储，突破上下文长度限制。

Result: Kinaema在新提出的Mem-Nav任务中表现优异，能记忆场景、定位查询目标并有效导航至先前已见过的目标位置，与传统transformer带历史注意力方法相比具有更高计算效率和灵活性。

Conclusion: Kinaema模型为机器人连续、自主空间定位和高效导航提供了新思路。其无需显式历史存储、可压缩历史观测的能力，提升了机器人在复杂、长时序场景中的实用性和效率。

Abstract: One key aspect of spatially aware robots is the ability to "find their
bearings", ie. to correctly situate themselves in previously seen spaces. In
this work, we focus on this particular scenario of continuous robotics
operations, where information observed before an actual episode start is
exploited to optimize efficiency. We introduce a new model, Kinaema, and agent,
capable of integrating a stream of visual observations while moving in a
potentially large scene, and upon request, processing a query image and
predicting the relative position of the shown space with respect to its current
position. Our model does not explicitly store an observation history, therefore
does not have hard constraints on context length. It maintains an implicit
latent memory, which is updated by a transformer in a recurrent way,
compressing the history of sensor readings into a compact representation. We
evaluate the impact of this model in a new downstream task we call "Mem-Nav".
We show that our large-capacity recurrent model maintains a useful
representation of the scene, navigates to goals observed before the actual
episode start, and is computationally efficient, in particular compared to
classical transformers with attention over an observation history.

</details>


### [159] [MemER: Scaling Up Memory for Robot Control via Experience Retrieval](https://arxiv.org/abs/2510.20328)
*Ajay Sridhar,Jennifer Pan,Satvik Sharma,Chelsea Finn*

Main category: cs.RO

TL;DR: 该论文提出了一种赋予机器人任务执行过程中的记忆能力的新方法——MemER，该方法能有效处理长时序依赖任务并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数机器人策略缺乏类似人类依赖记忆执行任务的能力。对长观测历史的直接利用成本高且脆弱，随意抽取历史帧又可能加入无关或冗余信息，因此需要更合理的记忆机制。

Method: 提出分层策略结构：高层策略负责选择和追踪重要关键帧，并结合最新观测帧，为低层策略生成文本指令，最终驱动任务执行。该方法可与现有视觉-语言-动作模型结合，并通过少量人工语言标注与行为演示进行微调。实验中，分别用Qwen2.5-VL-7B-Instruct和π_{0.5}担任高层和低层策略。

Result: 在三个需要长时记忆（任务持续数分钟）的实际机器人操作任务中，MemER表现优于之前的相关方法。

Conclusion: MemER为机器人带来了高效的长时记忆推理能力，在真实长时序操作任务中效果显著，提升了机器人系统的任务执行能力。

Abstract: Humans routinely rely on memory to perform tasks, yet most robot policies
lack this capability; our goal is to endow robot policies with the same
ability. Naively conditioning on long observation histories is computationally
expensive and brittle under covariate shift, while indiscriminate subsampling
of history leads to irrelevant or redundant information. We propose a
hierarchical policy framework, where the high-level policy is trained to select
and track previous relevant keyframes from its experience. The high-level
policy uses selected keyframes and the most recent frames when generating text
instructions for a low-level policy to execute. This design is compatible with
existing vision-language-action (VLA) models and enables the system to
efficiently reason over long-horizon dependencies. In our experiments, we
finetune Qwen2.5-VL-7B-Instruct and $\pi_{0.5}$ as the high-level and low-level
policies respectively, using demonstrations supplemented with minimal language
annotations. Our approach, MemER, outperforms prior methods on three real-world
long-horizon robotic manipulation tasks that require minutes of memory. Videos
and code can be found at https://jen-pan.github.io/memer/.

</details>


### [160] [Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking](https://arxiv.org/abs/2510.20335)
*Zixuan Wu,Hengyuan Zhang,Ting-Hsuan Chen,Yuliang Guo,David Paz,Xinyu Huang,Liu Ren*

Main category: cs.RO

TL;DR: 本文提出了一种新型的通用自动泊车算法Dino-Diffusion Parking（DDP），结合视觉基础模型和扩散规划，显著提升了对不同场景变化（如天气、光照）下的鲁棒性，并在多类场景下零样本迁移实验中取得了90%以上的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动泊车方法在同一分布下表现良好，但在面对如天气、光照等场景迁移时鲁棒性不足。为了解决依赖大量额外数据和训练的问题，作者希望开发一个无需额外数据即可在多领域稳健部署的泊车方案。

Method: 作者提出DDP泊车系统，将视觉基础模型与扩散式运动规划相结合，不依赖额外域适应数据，仅在CARLA仿真环境标准设置下训练，然后直接零样本迁移到各种不利新场景，并进行消融实验分析架构及算法贡献。还在真实停车场三维重建的3DGS环境中测试了仿真到真实的迁移效果。

Result: DDP系统在所有测试的分布外场景中均达到了90%以上的自动泊车成功率。消融实验显示，网络结构和算法设计对提升跨域性能有显著作用。并且在真实环境三维重建场景下也展现出了较强的仿真到真实迁移能力。

Conclusion: DDP方法在不同领域和分布下都展现出了优异的鲁棒性和泛化能力，减少了对额外适应性训练数据的依赖，为自动泊车系统的实际部署提供了更通用和可靠的解决方案。

Abstract: Parking is a critical pillar of driving safety. While recent end-to-end (E2E)
approaches have achieved promising in-domain results, robustness under domain
shifts (e.g., weather and lighting changes) remains a key challenge. Rather
than relying on additional data, in this paper, we propose Dino-Diffusion
Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates
visual foundation models with diffusion-based planning to enable generalized
perception and robust motion planning under distribution shifts. We train our
pipeline in CARLA at regular setting and transfer it to more adversarial
settings in a zero-shot fashion. Our model consistently achieves a parking
success rate above 90% across all tested out-of-distribution (OOD) scenarios,
with ablation studies confirming that both the network architecture and
algorithmic design significantly enhance cross-domain performance over existing
baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment
reconstructed from a real-world parking lot demonstrates promising sim-to-real
transfer.

</details>


### [161] [Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots](https://arxiv.org/abs/2510.20347)
*Ashutosh Mishra,Shreya Santra,Elian Neppel,Edoardo M. Rossi Lombardi,Shamistan Karimov,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 该论文提出了一种针对模块化可重构机器人去中心化强化学习控制方法，每个模块自主学习最优策略，实现了在未知新结构下的零样本泛化能力，并通过仿真和月球类比外场实验验证了系统的高效与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模块化机器人能根据任务重新组装，提高空间任务适应性，但不同结构组合导致控制系统难以统一和扩展。传统集中式控制难以处理组合爆炸，因此亟需一种可扩展、通用的控制方法。

Method: 提出将每个机器人模块看作独立智能体，使用去中心化强化学习方式：轮式模块采用Soft Actor-Critic（SAC）算法用于行走控制，7自由度机械臂模块采用Proximal Policy Optimization（PPO）用于转向和操作。各模块分别学习自己的策略，支持在未见过的新组合下直接部署。

Result: 在仿真中，转向策略角度误差仅3.63度，操作策略在目标偏移任务上达到84.6%成功率，轮式策略在维持99.6%成功率的同时驱动力矩减小了95.4%。月球类比场地实验证明模块可无缝组合，实现自主行驶、转向及初步对齐。

Conclusion: 所提出的去中心化学习方法实现了对模块化机器人零样本泛化、高鲁棒性和可扩展统一控制，适用于未来复杂空间机器人任务。

Abstract: Modular reconfigurable robots suit task-specific space operations, but the
combinatorial growth of morphologies hinders unified control. We propose a
decentralized reinforcement learning (Dec-RL) scheme where each module learns
its own policy: wheel modules use Soft Actor-Critic (SAC) for locomotion and
7-DoF limbs use Proximal Policy Optimization (PPO) for steering and
manipulation, enabling zero-shot generalization to unseen configurations. In
simulation, the steering policy achieved a mean absolute error of 3.63{\deg}
between desired and induced angles; the manipulation policy plateaued at 84.6 %
success on a target-offset criterion; and the wheel policy cut average motor
torque by 95.4 % relative to baseline while maintaining 99.6 % success.
Lunar-analogue field tests validated zero-shot integration for autonomous
locomotion, steering, and preliminary alignment for reconfiguration. The system
transitioned smoothly among synchronous, parallel, and sequential modes for
Policy Execution, without idle states or control conflicts, indicating a
scalable, reusable, and robust approach for modular lunar robots.

</details>


### [162] [NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control](https://arxiv.org/abs/2510.20390)
*Yijiong Lin,Bowen Deng,Chenghua Lu,Max Yang,Efi Psomopoulou,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 提出了NeuralTouch，一个结合视觉与触觉感知的机器人抓取新方法，实现更精确、通用的操作。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的方法（如NDF）因相机校准误差、点云不完整和物体多样性，常导致机器人抓取定位不准；现有触觉方法又限制于简单预设的接触几何，缺乏灵活性。

Method: 提出NeuralTouch框架，将NDF生成的目标接触几何与触觉反馈结合，通过深度强化学习训练策略，利用触觉数据在抓取过程中自适应微调，无需预定义接触类型。

Result: 通过模拟消融实验和真实场景（如插拔销、瓶盖开启）零样本迁移测试，NeuralTouch显著提升了抓取精度与鲁棒性，优于现有基线方法。

Conclusion: NeuralTouch为实现精密、接触丰富的机器人操作提供了更通用且高效的实现框架。

Abstract: Grasping accuracy is a critical prerequisite for precise object manipulation,
often requiring careful alignment between the robot hand and object. Neural
Descriptor Fields (NDF) offer a promising vision-based method to generate
grasping poses that generalize across object categories. However, NDF alone can
produce inaccurate poses due to imperfect camera calibration, incomplete point
clouds, and object variability. Meanwhile, tactile sensing enables more precise
contact, but existing approaches typically learn policies limited to simple,
predefined contact geometries. In this work, we introduce NeuralTouch, a
multimodal framework that integrates NDF and tactile sensing to enable
accurate, generalizable grasping through gentle physical interaction. Our
approach leverages NDF to implicitly represent the target contact geometry,
from which a deep reinforcement learning (RL) policy is trained to refine the
grasp using tactile feedback. This policy is conditioned on the neural
descriptors and does not require explicit specification of contact types. We
validate NeuralTouch through ablation studies in simulation and zero-shot
transfer to real-world manipulation tasks--such as peg-out-in-hole and bottle
lid opening--without additional fine-tuning. Results show that NeuralTouch
significantly improves grasping accuracy and robustness over baseline methods,
offering a general framework for precise, contact-rich robotic manipulation.

</details>


### [163] [PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning](https://arxiv.org/abs/2510.20406)
*Xiaogang Jia,Qian Wang,Anrui Wang,Han A. Wang,Balázs Gyenes,Emiliyan Gospodinov,Xinkai Jiang,Ge Li,Hongyi Zhou,Weiran Liao,Xi Huang,Maximilian Beck,Moritz Reuss,Rudolf Lioutikov,Gerhard Neumann*

Main category: cs.RO

TL;DR: 本论文提出PointMapPolicy方法，将点云和RGB图像融合用于机器人操作任务，结合各自优势提升了操作系统的精度和泛化能力，在多个基准测试和实际机器人实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有点云方法难以获取细粒度几何信息，RGB方法缺乏几何感知，限制操控任务精度与泛化，两者优势互补但当前融合存在挑战。

Method: 提出PointMapPolicy：无需下采样，直接将点云结构化为规则网格，方便提取空间关系，并可利用成熟计算机视觉技术。采用xLSTM主干网络，有效融合点云与RGB数据，实现多模态感知。

Result: 在RoboCasa和CALVIN基准测试及实际机器人测试中，本文方法在多种操作任务中达到SOTA（最佳表现）。

Conclusion: PointMapPolicy有效融合点云和RGB信息，提升机器人操作的感知能力和任务表现，为多模态融合提供新思路。

Abstract: Robotic manipulation systems benefit from complementary sensing modalities,
where each provides unique environmental information. Point clouds capture
detailed geometric structure, while RGB images provide rich semantic context.
Current point cloud methods struggle to capture fine-grained detail, especially
for complex tasks, which RGB methods lack geometric awareness, which hinders
their precision and generalization. We introduce PointMapPolicy, a novel
approach that conditions diffusion policies on structured grids of points
without downsampling. The resulting data type makes it easier to extract shape
and spatial relationships from observations, and can be transformed between
reference frames. Yet due to their structure in a regular grid, we enable the
use of established computer vision techniques directly to 3D data. Using xLSTM
as a backbone, our model efficiently fuses the point maps with RGB data for
enhanced multi-modal perception. Through extensive experiments on the RoboCasa
and CALVIN benchmarks and real robot evaluations, we demonstrate that our
method achieves state-of-the-art performance across diverse manipulation tasks.
The overview and demos are available on our project page:
https://point-map.github.io/Point-Map/

</details>


### [164] [MR-UBi: Mixed Reality-Based Underwater Robot Arm Teleoperation System with Reaction Torque Indicator via Bilateral Control](https://arxiv.org/abs/2510.20407)
*Kohei Nishi,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: 本文提出了一种基于混合现实（MR）的水下机械臂遥操作系统 MR-UBi，通过在MR头戴显示器中以颜色和长度编码的扭矩条，融合视觉与力觉反馈，显著提升了操作者的操作精准度与体验。


<details>
  <summary>Details</summary>
Motivation: 水下机械臂的遥操作对操作精度和力的控制有较高要求，传统方式难以有效结合视觉和力觉反馈。为提升操作准确性和用户体验，作者提出结合MR和力反馈的系统。

Method: 系统在MR头戴显示器中加入反馈指示器，实时显示操作产生的扭矩，同时采用双向控制提供力觉反馈。通过实验对比标准双向控制系统和MR-UBi系统、邀请16位用户参与不同硬度物体的抓取和搬运任务，收集客观操作数据和主观体验评价。

Result: MR-UBi系统显著提高了抓取扭矩控制的准确率，提高了操作过程中处于最佳扭矩区间的时间，降低了过高或过低力的情况。用户的主观评价也显示出更高的易用性和更低的操作负担。

Conclusion: MR-UBi通过融合视觉和力觉反馈，提升了水下机械臂遥操作的稳定性、准确性及用户体验，有望成为水下遥操作的新型交互方式。

Abstract: We present a mixed reality-based underwater robot arm teleoperation system
with a reaction torque indicator via bilateral control (MR-UBi). The reaction
torque indicator (RTI) overlays a color and length-coded torque bar in the
MR-HMD, enabling seamless integration of visual and haptic feedback during
underwater robot arm teleoperation. User studies with sixteen participants
compared MR-UBi against a bilateral-control baseline. MR-UBi significantly
improved grasping-torque control accuracy, increasing the time within the
optimal torque range and reducing both low and high grasping torque range
during lift and pick-and-place tasks with objects of different stiffness.
Subjective evaluations further showed higher usability (SUS) and lower workload
(NASA--TLX). Overall, the results confirm that \textit{MR-UBi} enables more
stable, accurate, and user-friendly underwater robot-arm teleoperation through
the integration of visual and haptic feedback. For additional material, please
check: https://mertcookimg.github.io/mr-ubi

</details>


### [165] [Robot Path and Trajectory Planning Considering a Spatially Fixed TCP](https://arxiv.org/abs/2510.20473)
*Bernhard Rameder,Hubert Gattringer,Andreas Mueller,Ronald Naderer*

Main category: cs.RO

TL;DR: 本文提出了一种在工件坐标下进行轨迹规划的方法，保持工具中心点（TCP）空间位置不变，通过B样条优化路径，实现平滑机器人运动。该方法适用于移动工件比移动工具更容易的场景，并在真实工业机器人系统上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 在某些应用中，移动工具较难，而移动工件更为便捷。传统轨迹规划多考虑工具运动，本文方法关注于移动工件，通过优化轨迹，实现更高效平滑的加工过程。

Method: 无论采用数学模型还是离散点，最终都用B样条表示路径，确保轨迹具有所需的连续性和平滑性。同时在轨迹规划时综合考虑TCP的空间速度和姿态，实现工艺要求。

Result: 所提轨迹规划方法在真实工业机器人系统上得到了验证，能够成功驱动机器人按照任意设定的工件路径运动，并保证TCP所需速度和姿态。

Conclusion: 利用B样条实现基于工件移动的机器人轨迹连续平滑规划，满足复杂加工路径与工艺速度要求，方法已在工业系统中验证有效，具有应用前景。

Abstract: This paper presents a method for planning a trajectory in workspace
coordinates using a spatially fixed tool center point (TCP), while taking into
account the processing path on a part. This approach is beneficial if it is
easier to move the part rather than moving the tool. Whether a mathematical
description that defines the shape to be processed or single points from a
design program are used, the robot path is finally represented using B-splines.
The use of splines enables the path to be continuous with a desired degree,
which finally leads to a smooth robot trajectory. While calculating the robot
trajectory through prescribed orientation, additionally a given velocity at the
TCP has to be considered. The procedure was validated on a real system using an
industrial robot moving an arbitrary defined part.

</details>


### [166] [Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections](https://arxiv.org/abs/2510.20480)
*Václav Pritzl,Xianjia Yu,Tomi Westerlund,Petr Štěpán,Martin Saska*

Main category: cs.RO

TL;DR: 本论文提出了一种自适应多模态多机器人协作定位方法，能在多机器人平台间融合异步和多模态传感器数据，有效应对传感器退化，提高GNSS受限环境下的机器人长期定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS受限环境下，机器人需要依赖板载传感器进行精确定位，单机器人集成所有传感器会导致平台体积、重量和能耗增加。多机器人分布式搭载传感器可提升部署灵活性，但带来了异步多模态数据融合的难题。

Method: 方法基于因子图，将异步的视觉-惯性里程计（VIO）、激光-惯性里程计（LIO）和多机器人间3D检测数据，在松耦合框架下融合。提出了基于插值的新因子处理异步测量，并针对LIO退化设计了基于Hessian的评价方法以及基于VIO输出分布Wasserstein距离的加权机制。

Result: 在实际异构机器人（无人车与无人机）数据集上进行实验，结果表明该方法在传感器退化等问题下，定位精度显著提升。

Conclusion: 提出的多模态多机器人协作定位方法有效融合异步多源数据，显著提高了GNSS受限环境中的定位鲁棒性和精度。

Abstract: Accurate long-term localization using onboard sensors is crucial for robots
operating in Global Navigation Satellite System (GNSS)-denied environments.
While complementary sensors mitigate individual degradations, carrying all the
available sensor types on a single robot significantly increases the size,
weight, and power demands. Distributing sensors across multiple robots enhances
the deployability but introduces challenges in fusing asynchronous, multi-modal
data from independently moving platforms. We propose a novel adaptive
multi-modal multi-robot cooperative localization approach using a factor-graph
formulation to fuse asynchronous Visual-Inertial Odometry (VIO), LiDAR-Inertial
Odometry (LIO), and 3D inter-robot detections from distinct robots in a
loosely-coupled fashion. The approach adapts to changing conditions, leveraging
reliable data to assist robots affected by sensory degradations. A novel
interpolation-based factor enables fusion of the unsynchronized measurements.
LIO degradations are evaluated based on the approximate scan-matching Hessian.
A novel approach of weighting odometry data proportionally to the Wasserstein
distance between the consecutive VIO outputs is proposed. A theoretical
analysis is provided, investigating the cooperative localization problem under
various conditions, mainly in the presence of sensory degradations. The
proposed method has been extensively evaluated on real-world data gathered with
heterogeneous teams of an Unmanned Ground Vehicle (UGV) and Unmanned Aerial
Vehicles (UAVs), showing that the approach provides significant improvements in
localization accuracy in the presence of various sensory degradations.

</details>


### [167] [Dual Control Reference Generation for Optimal Pick-and-Place Execution under Payload Uncertainty](https://arxiv.org/abs/2510.20483)
*Victor Vantilborgh,Hrishikesh Sathyanarayan,Guillaume Crevecoeur,Ian Abraham,Tom Lefebvre*

Main category: cs.RO

TL;DR: 本文提出了机器人在动力学未知情况下（如搬运物体时负载不确定）进行操作的方法，通过主动探索和在线参数自适应提高模型控制精度，并简化了相关双重控制问题，提出两种参考轨迹生成方法，有效提升任务效率和系统识别能力。


<details>
  <summary>Details</summary>
Motivation: 在实际机器人操作中，常常会遇到负载等动力学参数未知的情况。这种不确定性会严重影响模型控制的准确性，因此需要在任务执行中主动探索并及时对参数进行在线调整，以保证机器人的性能和稳定性。现有方法难以兼顾对参数的不确定建模和高效执行控制任务，文中围绕此挑战展开。

Method: 作者将问题建模为含参数不确定性的闭环最优控制（Dual Control）问题。为简化控制策略，预先定义反馈策略结构，并嵌入显式的自适应机制。提出两种参考轨迹生成方法：一种将参数不确定性直接嵌入到鲁棒最优控制中以最小化期望任务成本，另一种以最小化参数相关信息敏感度为目标，兼顾系统辨识。两种方法均隐式利用Fisher信息，以实现任务最优执行。

Result: 作者在搬运操作任务上进行了实验，结果表明，所提方法能生成考虑控制效果的参考轨迹，相比传统方法能更快、更准确地完成操作任务，同时提升了系统参数识别速度和精度，并兼顾了控制的稳定性和效率。

Conclusion: 研究表明，在控制参考轨迹设计中引入参数不确定性分析与自适应机制，有助于实现高效、安全并兼顾系统识别的机器人操作控制，对于实际存在模型误差的不确定动力学任务具有良好的应用前景。

Abstract: This work addresses the problem of robot manipulation tasks under unknown
dynamics, such as pick-and-place tasks under payload uncertainty, where active
exploration and(/for) online parameter adaptation during task execution are
essential to enable accurate model-based control. The problem is framed as dual
control seeking a closed-loop optimal control problem that accounts for
parameter uncertainty. We simplify the dual control problem by pre-defining the
structure of the feedback policy to include an explicit adaptation mechanism.
Then we propose two methods for reference trajectory generation. The first
directly embeds parameter uncertainty in robust optimal control methods that
minimize the expected task cost. The second method considers minimizing the
so-called optimality loss, which measures the sensitivity of parameter-relevant
information with respect to task performance. We observe that both approaches
reason over the Fisher information as a natural side effect of their
formulations, simultaneously pursuing optimal task execution. We demonstrate
the effectiveness of our approaches for a pick-and-place manipulation task. We
show that designing the reference trajectories whilst taking into account the
control enables faster and more accurate task performance and system
identification while ensuring stable and efficient control.

</details>


### [168] [Simultaneous Stiffness and Trajectory Optimization for Energy Minimization of Pick-and-Place Tasks of SEA-Actuated Parallel Kinematic Manipulators](https://arxiv.org/abs/2510.20490)
*Thomas Kordik,Hubert Gattringer,Andreas Mueller*

Main category: cs.RO

TL;DR: 本文针对工业机器人中常见的往返搬运任务（pick-and-place），探讨利用串联弹性致动器（SEA）优化并联机构机械臂（PKM）的能耗。结合动力学建模和最优控制，将运动轨迹与弹性致动器刚度同时作为优化变量，显著降低能耗。方法在两个并联机器人案例中验证，结果有效。


<details>
  <summary>Details</summary>
Motivation: 工业机器人执行重复搬运任务时，通常长时间自动运行，能耗问题突出。已有研究关注弹性元件及SEA的节能潜力，本文有意进一步分析并最大化其能耗优化效果。

Method: 建立SEA驱动PKM任务的动力学模型，结合具体搬运循环操作，设置最优控制问题，将运动轨迹和致动器刚度作为联合优化目标。通过调动系统的自振动特性，实现能耗最小化。该方法并未要求实际实现可变刚度致动器，而侧重于设计和参数选择。

Result: 通过两个并联机器人应用案例（其中也针对冗余驱动进行分析）进行仿真或实验验证，结果证实联合优化运动轨迹和致动器刚度的方法能够有效降低能耗。

Conclusion: 利用SEA并结合系统自振动与刚度参数优化，能为PKM在长时间重复作业中带来明显节能效果，为机械臂设计及参数选取提供新策略。

Abstract: A major field of industrial robot applications deals with repetitive tasks
that alternate between operating points. For these so-called pick-and-place
operations, parallel kinematic manipulators (PKM) are frequently employed.
These tasks tend to automatically run for a long period of time and therefore
minimizing energy consumption is always of interest. Recent research addresses
this topic by the use of elastic elements and particularly series elastic
actuators (SEA). This paper explores the possibilities of minimizing energy
consumption of SEA actuated PKM performing pick-and-place tasks. The basic idea
is to excite eigenmotions that result from the actuator springs and exploit
their oscillating characteristics. To this end, a prescribed cyclic
pick-and-place operation is analyzed and a dynamic model of SEA driven PKM is
derived. Subsequently, an energy minimizing optimal control problem is
formulated where operating trajectories as well as SEA stiffnesses are
optimized simultaneously. Here, optimizing the actuator stiffness does not
account for variable stiffness actuators. It serves as a tool for the design
and dimensioning process. The hypothesis on energy reduction is tested on two
(parallel) robot applications where redundant actuation is also addressed. The
results confirm the validity of this approach.

</details>


### [169] [A Parameter-Linear Formulation of the Optimal Path Following Problem for Robotic Manipulator](https://arxiv.org/abs/2510.20496)
*Tobias Marauli,Hubert Gattringer,Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种高效解决时间最优路径跟随计算复杂性的新方法，通过最大化路径速度实现平滑轨迹生成，并线性化优化问题以提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间最优路径跟随方法在将优化问题重参数化为路径参数形式时，会在路径速度为零处产生不可避免的奇异性。这使得在保持低计算量的前提下生成平滑轨迹非常困难。因此，本文旨在提出新的方法克服这一困难。

Method: 本文提出以最大化给定路径上的路径速度为目标，通过数值高效的方式实现平滑轨迹规划。同时，对底层的优化问题进行离散重构，使其对优化变量呈线性化，降低了求解难度和计算量。

Result: 该方法能够高效地规划出平滑的轨迹，并且在数值求解时具有优异的效率表现，优化问题因线性化处理得到极大简化。

Conclusion: 所提出的方法能有效绕过传统方法中的奇异性问题，并在保持平滑性的前提下大大降低了计算复杂度和优化难度，适用于实际高效路径跟随任务。

Abstract: In this paper the computational challenges of time-optimal path following are
addressed. The standard approach is to minimize the travel time, which
inevitably leads to singularities at zero path speed, when reformulating the
optimization problem in terms of a path parameter. Thus, smooth trajectory
generation while maintaining a low computational effort is quite challenging,
since the singularities have to be taken into account. To this end, a different
approach is presented in this paper. This approach is based on maximizing the
path speed along a prescribed path. Furthermore, the approach is capable of
planning smooth trajectories numerically efficient. Moreover, the discrete
reformulation of the underlying problem is linear in optimization variables.

</details>


### [170] [RubbleSim: A Photorealistic Structural Collapse Simulator for Confined Space Mapping](https://arxiv.org/abs/2510.20529)
*Constantine Frost,Chad Council,Margaret McGuinness,Nathaniel Hanson*

Main category: cs.RO

TL;DR: 本文提出了RubbleSim，一个开源可重构的仿真器，用于灾后建筑坍塌空间的仿真与探索，帮助研究灾难场景下的机器人感知问题。


<details>
  <summary>Details</summary>
Motivation: 真实灾害现场空腔结构的数据受法律和所有权等多种限制，极难获取，现有训练用人工瓦砾场的数据也多为私有，缺乏公开可用的高质量仿真数据，限制了相关感知与探索算法的研究。

Method: RubbleSim在统一引擎（Unity）上实现，支持多操作系统，基于物理建模方法可随机生成多样瓦砾堆形态，对虚拟瓦砾场的结构具有绝对掌控。论文模拟不同复杂度的受限空间，并采用结构光算法（structure-from-motion）评测了感知在各种恶劣环境下的性能变化。

Result: 实验展示了RubbleSim能够快速生成多样化、高保真的虚拟瓦砾环境，并在复杂光条件下成功复现了结构光感知算法的性能劣化过程。仿真工具及预编译二进制和源码已开源。

Conclusion: RubbleSim为灾后响应机器人研究者提供了一种可控、开放、具有代表性的实验环境，有效弥补了真实数据稀缺的瓶颈，有助于推动相关感知与探索算法的快速发展和测试。

Abstract: Despite well-reported instances of robots being used in disaster response,
there is scant published data on the internal composition of the void spaces
within structural collapse incidents. Data collected during these incidents is
mired in legal constraints, as ownership is often tied to the responding
agencies, with little hope of public release for research. While engineered
rubble piles are used for training, these sites are also reluctant to release
information about their proprietary training grounds. To overcome this access
challenge, we present RubbleSim -- an open-source, reconfigurable simulator for
photorealistic void space exploration. The design of the simulation assets is
directly informed by visits to numerous training rubble sites at differing
levels of complexity. The simulator is implemented in Unity with
multi-operating system support. The simulation uses a physics-based approach to
build stochastic rubble piles, allowing for rapid iteration between simulation
worlds while retaining absolute knowledge of the ground truth. Using RubbleSim,
we apply a state-of-the-art structure-from-motion algorithm to illustrate how
perception performance degrades under challenging visual conditions inside the
emulated void spaces. Pre-built binaries and source code to implement are
available online: https://github.com/mit-ll/rubble_pile_simulator.

</details>


### [171] [C-NAV: Towards Self-Evolving Continual Object Navigation in Open World](https://arxiv.org/abs/2510.20685)
*Ming-Ming Yu,Fei Zhu,Wenzhuo Liu,Yirong Yang,Qunbo Wang,Wenjun Wu,Jing Liu*

Main category: cs.RO

TL;DR: 该论文提出了一个针对动态、开放世界环境中具身智能体持续目标导航的基准和方法。核心贡献是C-Nav框架，集成了双路径抗遗忘机制和自适应采样策略，提升了智能体对新目标类别的适应性，防止遗忘旧知识，同时降低了内存消耗，并在多个模型上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的目标导航任务要求智能体不断适应新环境和新目标类别，不能遗忘之前学到的知识。但现有方法多基于静态轨迹和固定目标，缺乏对持续学习和知识保持的支持。因此，急需新方法解决对持续适应和抗遗忘的需求。

Method: 提出C-Nav持续视觉导航框架，包括两大创新：一是双路径抗遗忘机制——特征蒸馏用于保持多模态输入表征一致性，特征回放则用于保持决策策略一致性；二是自适应采样策略，有效选择多样且有信息量的经验，减少冗余和内存需求。

Result: 在多个模型架构上的实验结果表明，C-Nav在持续目标导航任务中均优于现有方法，即使在无完整轨迹保留的情况下也取得更好表现，且大大降低了内存需求。

Conclusion: C-Nav框架能够有效提升具身智能体持续学习和抗遗忘能力，适应新目标类别，显著降低了资源消耗，有望支持更具有现实应用前景的智能体导航系统。

Abstract: Embodied agents are expected to perform object navigation in dynamic,
open-world environments. However, existing approaches typically rely on static
trajectories and a fixed set of object categories during training, overlooking
the real-world requirement for continual adaptation to evolving scenarios. To
facilitate related studies, we introduce the continual object navigation
benchmark, which requires agents to acquire navigation skills for new object
categories while avoiding catastrophic forgetting of previously learned
knowledge. To tackle this challenge, we propose C-Nav, a continual visual
navigation framework that integrates two key innovations: (1) A dual-path
anti-forgetting mechanism, which comprises feature distillation that aligns
multi-modal inputs into a consistent representation space to ensure
representation consistency, and feature replay that retains temporal features
within the action decoder to ensure policy consistency. (2) An adaptive
sampling strategy that selects diverse and informative experiences, thereby
reducing redundancy and minimizing memory overhead. Extensive experiments
across multiple model architectures demonstrate that C-Nav consistently
outperforms existing approaches, achieving superior performance even compared
to baselines with full trajectory retention, while significantly lowering
memory requirements. The code will be publicly available at
https://bigtree765.github.io/C-Nav-project.

</details>


### [172] [Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning](https://arxiv.org/abs/2510.20706)
*Ganga Nair B,Prakrut Kotecha,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 本文提出了一种结合Model Predictive Path Integral (MPPI)算法与Dreamer模块的优化框架，实现了四足机器人实时、连续步态空间下的自适应运动，并在仿真中验证了其在节能性和步态适应性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有无模型强化学习虽实现了四足机器人灵活运动，但常收敛于单一步态，表现次优；传统MPC虽能生成针对任务的最优策略，却缺乏对变化环境的适应能力。因此，亟需一种方法能结合MPC与RL优势，实现步态空间的高效自适应和能耗优化。

Method: 提出将MPPI与Dreamer模块结合，在每个时间步内联合优化行动和步态变量，利用学习得到的奖励函数促进速度跟踪、能效、稳定性和平滑过渡，并抑制突变步态；同时加入学习型价值函数作为终端奖励，将问题扩展为无限时域规划。

Result: 在Unitree Go1仿真平台上测试，提出的框架在不同目标速度下相比基线方法平均能耗降低最高达36.48%，且保持精准跟踪和自适应步态。

Conclusion: 该方法能高效产生能耗低、步态自适应性强的四足机器人运动策略，兼具MPC的优化能力和RL的环境适应能力。

Abstract: Model-free reinforcement learning (RL) has enabled adaptable and agile
quadruped locomotion; however, policies often converge to a single gait,
leading to suboptimal performance. Traditionally, Model Predictive Control
(MPC) has been extensively used to obtain task-specific optimal policies but
lacks the ability to adapt to varying environments. To address these
limitations, we propose an optimization framework for real-time gait adaptation
in a continuous gait space, combining the Model Predictive Path Integral (MPPI)
algorithm with a Dreamer module to produce adaptive and optimal policies for
quadruped locomotion. At each time step, MPPI jointly optimizes the actions and
gait variables using a learned Dreamer reward that promotes velocity tracking,
energy efficiency, stability, and smooth transitions, while penalizing abrupt
gait changes. A learned value function is incorporated as terminal reward,
extending the formulation to an infinite-horizon planner. We evaluate our
framework in simulation on the Unitree Go1, demonstrating an average reduction
of up to 36.48\% in energy consumption across varying target speeds, while
maintaining accurate tracking and adaptive, task-appropriate gaits.

</details>


### [173] [FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation](https://arxiv.org/abs/2510.20774)
*Wenhao Wang,Kehe Ye,Xinyu Zhou,Tianxing Chen,Cao Min,Qiaoming Zhu,Xiaokang Yang,Yongjian Shen,Yang Yang,Maoqing Yao,Yao Mu*

Main category: cs.RO

TL;DR: 本文提出了FieldGen数据生成框架，能以最小的人为监督，实现可扩展、多样且高质量的机器人操作数据采集，并在实验中取得比遥操作更高的策略成功率及稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略的数据采集方式难以兼顾数据的规模、多样性与质量。仿真环境虽然易扩展但存在仿真到现实的差距，遥操作则虽能产生高质量数据但代价高、样本多样性不足。因而急需更优解决方案。

Method: FieldGen将操作分解为预操作（多样性）和精细操作（精度）两个阶段。先由人工演示关键接触与位姿信息，再利用吸引场自动生成多样化操作轨迹，兼具可拓展性与精确监督。此外，通过FieldGen-Reward为数据添加奖励标注，增强策略学习效果。

Result: 实验表明，用FieldGen训练的策略在成功率与稳定性上均优于传统遥操作采集的数据集，并显著减少了长期数据收集所需的人力投入。

Conclusion: FieldGen显著提升了现实世界机器人操作数据的采集效率和政策学习的效果，为大规模、多样性、高质量的数据集构建提供了可行路径。

Abstract: Large-scale and diverse datasets are vital for training robust robotic
manipulation policies, yet existing data collection methods struggle to balance
scale, diversity, and quality. Simulation offers scalability but suffers from
sim-to-real gaps, while teleoperation yields high-quality demonstrations with
limited diversity and high labor cost. We introduce FieldGen, a field-guided
data generation framework that enables scalable, diverse, and high-quality
real-world data collection with minimal human supervision. FieldGen decomposes
manipulation into two stages: a pre-manipulation phase, allowing trajectory
diversity, and a fine manipulation phase requiring expert precision. Human
demonstrations capture key contact and pose information, after which an
attraction field automatically generates diverse trajectories converging to
successful configurations. This decoupled design combines scalable trajectory
diversity with precise supervision. Moreover, FieldGen-Reward augments
generated data with reward annotations to further enhance policy learning.
Experiments demonstrate that policies trained with FieldGen achieve higher
success rates and improved stability compared to teleoperation-based baselines,
while significantly reducing human effort in long-term real-world data
collection. Webpage is available at https://fieldgen.github.io/.

</details>


### [174] [The Reality Gap in Robotics: Challenges, Solutions, and Best Practices](https://arxiv.org/abs/2510.20808)
*Elie Aljalbout,Jiaxu Xing,Angel Romero,Iretiayo Akinola,Caelan Reed Garrett,Eric Heiden,Abhishek Gupta,Tucker Hermans,Yashraj Narang,Dieter Fox,Davide Scaramuzza,Fabio Ramos*

Main category: cs.RO

TL;DR: 该论文综述了机器人领域中仿真到现实（sim-to-real）转移的研究进展，包括现实差距的原因、应对方法及评价指标。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习推动机器人导航、运动和操作等领域的进步，仿真作为训练与测试工具变得重要，但仿真与现实环境的差异（现实差距）阻碍了算法实际部署，因此需要深入研究如何弥合这种差距。

Method: 文章通过调研和综述文献，归纳总结了当前主要的sim-to-real方法，如领域随机化、从现实到仿真的转移、状态和动作抽象，以及仿真与现实的协同训练等，同时归纳了常用的评估指标。

Result: 许多工作通过上述方法取得了仿真到现实的成功转移，在多种机器人平台（如运动、导航和操作）上展示了优秀的结果，但也指出了现有方法仍面临的挑战。

Conclusion: 现有sim-to-real方法虽取得显著进展，但面对现实差距的根本成因与解决办法仍需更深入理解，未来需要继续发展新的理论和技术以完全实现仿真到现实的顺利转移。

Abstract: Machine learning has facilitated significant advancements across various
robotics domains, including navigation, locomotion, and manipulation. Many such
achievements have been driven by the extensive use of simulation as a critical
tool for training and testing robotic systems prior to their deployment in
real-world environments. However, simulations consist of abstractions and
approximations that inevitably introduce discrepancies between simulated and
real environments, known as the reality gap. These discrepancies significantly
hinder the successful transfer of systems from simulation to the real world.
Closing this gap remains one of the most pressing challenges in robotics.
Recent advances in sim-to-real transfer have demonstrated promising results
across various platforms, including locomotion, navigation, and manipulation.
By leveraging techniques such as domain randomization, real-to-sim transfer,
state and action abstractions, and sim-real co-training, many works have
overcome the reality gap. However, challenges persist, and a deeper
understanding of the reality gap's root causes and solutions is necessary. In
this survey, we present a comprehensive overview of the sim-to-real landscape,
highlighting the causes, solutions, and evaluation metrics for the reality gap
and sim-to-real transfer.

</details>


### [175] [GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation](https://arxiv.org/abs/2510.20813)
*Guangqi Jiang,Haoran Chang,Ri-Zhao Qiu,Yutong Liang,Mazeyu Ji,Jiyue Zhu,Zhao Dong,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了GSWorld，一种结合3D Gaussian Splatting与物理引擎的、适用于机器人操作任务的高真实感模拟器，通过GSDF格式实现高逼真渲染，并提供了一套涵盖多机器人和物体的数据库，支持零样本sim2real策略学习、自动数据收集及可重复评估等多种应用。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作策略的开发面临现实数据高昂、sim2real迁移困难以及仿真环境真实性有限等挑战。作者希望通过提供更真实、统一、易扩展的模拟平台，提升策略开发效率及可复现性。

Method: 作者提出GSWorld模拟器，利用3D Gaussian Splatting实现高质量视觉渲染，引入GSDF新格式，将高质量场景重建与机器人描述文件、多对象信息结合。结合物理引擎，支持多机器人操作仿真、数据自动采集和虚拟远程操作，并提供完整数据库及重建流程。

Result: GSWorld实现了高真实感渲染，数据库包含多款机器人和大量对象，实现零样本sim2real策略学习、自动DAgger数据收集、策略可重复仿真评测、远程虚拟操作等功能，展示了高易用性与灵活性。

Conclusion: GSWorld能极大助力机器人操作策略的开发、评估和迁移工作，提高了仿真与现实的接轨能力，为社区提供了强大、通用的新型仿真平台。

Abstract: This paper presents GSWorld, a robust, photo-realistic simulator for robotics
manipulation that combines 3D Gaussian Splatting with physics engines. Our
framework advocates "closing the loop" of developing manipulation policies with
reproducible evaluation of policies learned from real-robot data and sim2real
policy training without using real robots. To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects. With a streamlined reconstruction pipeline, we
curate a database of GSDF that contains 3 robot embodiments for single-arm and
bimanual manipulation, as well as more than 40 objects. Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning. Website: https://3dgsworld.github.io/.

</details>


### [176] [VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation](https://arxiv.org/abs/2510.20818)
*Mateo Guaman Castro,Sidharth Rajagopal,Daniel Gorbatov,Matt Schmittle,Rohan Baijal,Octi Zhang,Rosario Scalise,Sidharth Talia,Emma Romig,Celso de Melo,Byron Boots,Abhishek Gupta*

Main category: cs.RO

TL;DR: 本文提出了VAMOS，一种分层架构，用于分离语义规划与具体机器人本体物理约束，有效提升了不同行走机器人的泛化导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航方法难以兼顾对多样环境的泛化能力和对具体机器人（如轮式或腿式机器人）物理约束的适应性，导致导航性能受限。

Method: VAMOS采用分层结构：高层通用规划器通过图像提出路径候选，专用可行性模型依据模拟训练理解具体机器人本体物理约束，再对路径进行评价与重排序。高层学习来自开放环境大数据，低层则通过低成本仿真执行。

Result: 实验证明，VAMOS在室内及复杂户外环境下的导航成功率均优于现有方法。同时，其设计能够跨轮式与腿式机器人直接部署，且可通过自然语言控制。此外，通过舍弃不可行路径，单一机器人导航成功率提升了3倍。

Conclusion: VAMOS的分层方法有效结合了环境泛化与本体适应，提升了多类型机器人导航的可靠性和灵活性，并为同一高层规划器在不同类型机器人间的迁移提供了可能性。

Abstract: A fundamental challenge in robot navigation lies in learning policies that
generalize across diverse environments while conforming to the unique physical
constraints and capabilities of a specific embodiment (e.g., quadrupeds can
walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that
decouples semantic planning from embodiment grounding: a generalist planner
learns from diverse, open-world data, while a specialist affordance model
learns the robot's physical constraints and capabilities in safe, low-cost
simulation. We enabled this separation by carefully designing an interface that
lets a high-level planner propose candidate paths directly in image space that
the affordance model then evaluates and re-ranks. Our real-world experiments
show that VAMOS achieves higher success rates in both indoor and complex
outdoor navigation than state-of-the-art model-based and end-to-end learning
methods. We also show that our hierarchical design enables cross-embodied
navigation across legged and wheeled robots and is easily steerable using
natural language. Real-world ablations confirm that the specialist model is key
to embodiment grounding, enabling a single high-level planner to be deployed
across physically distinct wheeled and legged robots. Finally, this model
significantly enhances single-robot reliability, achieving 3X higher success
rates by rejecting physically infeasible plans. Website:
https://vamos-vla.github.io/

</details>
