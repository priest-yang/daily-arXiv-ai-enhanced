<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 79]
- [cs.CL](#cs.CL) [Total: 94]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文针对扩散模型训练后对齐只能利用简单信号（如单一分数或二元偏好）的问题，提出结合层级和细粒度标准的新方法，通过监督微调引入领域知识，并提出复杂偏好优化（CPO）算法，显著提升了模型与人类专业知识的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的对齐手段过于粗糙，难以捕捉专家级、细致入微的人类评价标准。复杂任务（如绘画生成）中，仅用简单分数或者二元偏好对齐，无法真正提升模型生成质量与专业对齐度。

Method: 首先，与领域专家共同构建层级化、细粒度的评价标准，将图片质量拆分为多层次正负属性，并用树结构组织。然后采用两阶段对齐框架：第一步，通过监督微调辅助扩散模型注入领域知识；第二步，提出扩展版的复杂偏好优化（CPO），最大化模型生成正面属性概率，最小化负面属性概率，实现对非二元、层级标准的对齐。

Result: 在绘画生成领域实例化该方法，并利用带有细粒度属性标注的数据集进行CPO训练。实验显示，采用该方法后的扩散模型在生成质量和专业性对齐方面均有显著提升。

Conclusion: 复杂偏好优化有效解决了扩散模型与细粒度、层级化专家标准的对齐难题，提升了模型在高难度生成任务中的表现，为细粒度对齐领域带来新的可能。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 提出了一种基于RGB空间五元组像素强度组合的文本信息嵌入图像的新方法，在像素级提高了嵌入效率，同时几乎不引入图像失真。


<details>
  <summary>Details</summary>
Motivation: 现有文本信息嵌入图像的方法多依赖于LSB、MSB、PVD等像素级操作或者深度学习及生成式AI技术，常引起噪声或计算开销大，缺乏高效且对图像扰动小的方法。

Method: 以RGB各通道各自五级像素强度组成125种像素组合，分别映射为大小写字母、数字、空格等字符，实现每个RGB像素存储一个文本符号，同时对嵌入前后的图像通过多种评价指标（MSE、MAE、SNR、PSNR、SSIM等）进行对比分析。

Result: 方法实现了单像素存储完整文本符号，编码效率提升，图像失真度低，各项指标对比显示编码前后图像变化不显著，比LSB等传统方法及深度学习法有更优嵌入效率和更小扰动。

Conclusion: 提出方法具备高嵌入效率和低图像扰动潜力，是信息隐藏领域在文本嵌入图像方面的有效新探索，可为后续相关应用和研究提供方向。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法，实现了模型在单次推理过程中自动从文本推理过渡到图像生成，提升了多模态联合生成能力。通过奖励加权和有针对性的数据设计，显著提升了文本到图像生成（T2I）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有T2I系统大多数需要先生成文本再手动切换到图像生成，推理过程不统一，限制了多模态间的协同与自动化。如何实现在一个一致的过程内进行文本和图像的联合生成，是提升多模态生成模型的关键问题。

Method: 作者提出在模型后训练阶段进行奖励加权、利用自生成综合数据（synthetic data）和针对性数据集设计，使模型在推理时能自动完成文本到图像的转换，并考察不同后训练数据的效果。

Result: 所提出的方法在四个不同的T2I基准数据集上，均取得了多模态生成性能的提升。针对性的后训练数据和奖励加权机制带来了更优表现。

Conclusion: 论文证明了通过奖励加权与策略性数据设计进行后训练，可以实现在单推理流程内高效且自动的联合文本-图像生成，开拓了T2I模型向更自然、多样应用场景发展的方向。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 该论文提出了ReHyAt，一种结合软/线性注意力的视频生成机制，显著减少计算和内存消耗，同时保持高质量生成效果。


<details>
  <summary>Details</summary>
Motivation: 目前主流的视频扩散模型采用transformer架构，虽然生成效果提升，但因二次复杂度的注意力机制，导致对长序列处理时扩展性受限，计算和存储成本过高。

Method: 提出了ReHyAt（一种递归式的混合注意力机制），将softmax注意力与线性注意力结合，实现分块递归处理，从而将显存需求降至常数。混合机制方便从现有softmax注意力模型高效蒸馏，训练成本大幅下降。

Result: 在VBench和VBench-2.0上，以及通过用户偏好测试，ReHyAt既保持了接近SOTA的视频生成质量，又将注意力机制的计算复杂度从二次降至线性，在训练耗时上降低了两个数量级（约160 GPU小时）。

Conclusion: ReHyAt在保证生成质量的前提下显著提高了计算效率和可扩展性，为长视频和终端侧视频生成提供了可行路径，也为后续的SOTA模型蒸馏和微调提供了策略。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D Gaussian Splatting渐进式压缩方法，采用残差向量量化和多分辨率哈希网格引导的自回归熵模型，实现更高效的特征压缩。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting模型虽能实现高保真实时新视角合成，但其对大中型场景有高存储需求，制约了云端和流媒体部署。现有压缩方法多采用标量量化，但无法充分捕捉高维特征间的相关性，影响压缩性能。

Method: 提出用残差向量量化（RVQ）替代传统标量量化，配合由多分辨率哈希网格引导的自回归熵模型，预测每个索引的条件概率，以此高效压缩粗层和细化层。

Result: 新方法能够有效降低比特率，在保持高保真度的同时显著提升压缩效率，优于基线压缩技术，尤其在大规模场景下表现突出。

Conclusion: 基于RVQ和自回归熵模型的3D Gaussian Splatting渐进式编解码方案，可显著提升模型压缩效率，有助于其在云端和流媒体场合大规模部署。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 本文比较了自定义CNN与ResNet-18、VGG-16等预训练模型在孟加拉国5个图像分类数据集上的表现，发现微调预训练模型可显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前图像分类任务中，预训练模型和自建模型各有优缺点。鉴于不同数据集和资源条件下的适用性问题，本文旨在系统对比自建CNN与主流预训练架构（ResNet-18、VGG-16）在不同应用场景下的效果表现，为实际应用提供参考。

Method: 作者在Footpath Vision、Auto Rickshaw Detection、Mango Image Classification、Paddy Variety Recognition和Road Damage Detection这五个孟加拉国图像数据集上，采用自建CNN、预训练模型特征提取和迁移学习（含微调）三种策略进行对比实验，重点评测分类准确率及模型参数量。

Result: 迁移学习+微调在所有数据集上均优于自建CNN和特征提取，准确率提升3%-76%。ResNet-18微调在Road Damage BD数据集上达到100%准确率。自建模型在参数量和训练效率上在简单任务中有一定优势。

Conclusion: 复杂分类任务、小样本场景下，预训练模型+微调显著优于自建CNN。但在计算资源有限或任务简单时，自建模型同样具备优势。论文为实际应用场景下的深度学习模型选择提供了有价值的参考。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 本文提出了PackCache方法，通过管理KV-cache，提高了Transformer架构下多模态视频生成模型的推理效率，尤其在长序列生成中显著加速。


<details>
  <summary>Details</summary>
Motivation: Transformer自回归模型在统一处理多模态任务时，需要依赖KV-cache来提高计算效率，但KV-cache大小随生成步数增加而线性增长，严重限制了生成序列长度和推理效率，尤其在视频任务中尤为突出。

Method: 通过分析KV-cache中不同模态token的时空特性，作者发现文本和条件图像token是语义锚点且始终得到高关注，而视频帧的注意力随时间自然衰减。据此，提出了免训练的PackCache方法，包括三大机制：语义锚定（保留重要token）、跨帧注意力衰减建模（按时间距离分配缓存）、空间位置嵌入保持（确保移除KV-cache后3D结构连贯）。

Result: PackCache在48帧长视频生成任务上实现了1.7-2.2倍推理加速；特别是在生成序列尾部最昂贵的帧上，A40和H200硬件分别加速至2.6倍和3.7倍。

Conclusion: PackCache显著提升了长视频生成Transformer模型的推理效率，有望突破生成长度瓶颈，推动更高效的多模态大模型应用。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 本文提出利用EMOCA生成的3D人脸表情与姿态系数，通过时序建模和跨模态融合，以提升驾驶压力识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 压力的主观性和面部自我控制让基于可见视频的压力识别更具挑战性。以往多依赖二维面部动作单元，对3D人脸几何变化挖掘不足，特别是在真实压力情境下（如驾驶）反映的价值尚未被充分探索。

Method: 作者利用EMOCA提取驾驶过程中基线和压力诱导阶段的3D人脸表达与姿态系数，并通过假设检验分析这56个系数在压力前后的一致变化。随后，设计Transformer时序建模方法，探索单一模态、早期融合及跨模态注意力机制，分别结合EMOCA与生理信号、EMOCA与凝视（gaze）数据进行多模态压力识别。

Result: 有41个3D系数在压力状态下表现出与生理指标一致的变化。多模态跨模态注意力融合EMOCA三维信息和生理信号表现最佳（AUROC 92%，准确率86.7%）；EMOCA与凝视融合也有很高表现（AUROC 91.8%）。

Conclusion: 3D人脸几何上的表达，结合时序建模和跨模态融合，对压力识别极为有效，优于单一模态或简单融合，显示出方法的优异性和实际场景的应用潜力。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: 本文探索了如何将主要在RGB图像上预训练的通用基础视觉模型，通过极少量配对样本，快速适配于红外和SAR（合成孔径雷达）等非可见光模态的跨谱图像转换任务，并用生成数据提升目标检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型多以RGB数据为主，但实际安全相关应用往往依赖红外或SAR这类非可见模态。如何让现有强大的基础模型以低成本适应这些新领域，有助于消除数据鸿沟、提升下游性能。

Method: 以FLUX.1 Kontext基础模型为例，在仅有100对RGB-IR或RGB-SAR配对图像情况下，引入并微调低秩适配（LoRA）模块，使模型能够生成与输入RGB图像像素对齐的IR或SAR图像。利用转换后合成的目标域数据，直接训练目标检测器。结合不同LoRA参数，基于小量留出集上的LPIPS分数作为性能代理指标。

Result: 实验证明，在KAIST和M4-SAR数据集上，仅用100对配对样本，通过较优LoRA调节后生成的合成IR/SAR数据，不仅能提升目标检测模型（YOLOv11n/DETR）的性能，而且LPIPS分数与检测表现高度相关。额外在外部RGB数据集转化为IR后，也能进一步提升行人检测表现，合成SAR显著增强基础设施检测。

Conclusion: 基础视觉模型通过少量配对样本的LoRA适配，可有效服务IR和SAR等非可见模态，大幅降低人力与数据成本，为跨模态视觉理解和数据合成带来新思路。

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 本文比较了自定义卷积神经网络（CNN）与多种流行的预训练深度学习架构（VGG-16、ResNet-50、MobileNet）在图像分类中的表现，揭示了性能和计算复杂性的权衡。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，选择是采用自己设计的CNN还是基于已有的预训练架构，是困扰工程师和研究者的重要问题。作者希望通过系统比较这两类方案，为图像分类模型的选型提供实用参考。

Method: 作者自行设计并从零训练了一个自定义CNN，同时将VGG-16、ResNet-50和MobileNet等主流网络通过迁移学习应用于同一任务。所有模型的训练和测试均在相同实验设置下进行，并采用准确率、精确率、召回率、F1分数等指标进行性能评估。

Result: 实验结果表明，在数据有限情况下，预训练CNN在分类准确率和收敛速度上优于自定义CNN。但自定义CNN在参数量和计算复杂度上具有明显优势，且在性能上仍具竞争力。

Conclusion: 预训练架构在性能上更有优势，尤其是在数据有限时收敛较快，但自定义CNN在资源受限场景下依然值得考虑。本文为在图像分类中如何权衡模型复杂度、性能和效率提供了实用见解。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [11] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出Tri MARF框架，通过结合多视角2D图像、文本描述和3D点云三模态信息，实现大规模3D目标标注，在多个数据集上性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D目标标注对于自动驾驶、机器人和增强现实等领域极为重要，但面临空间复杂度高、遮挡和视角不一致等挑战。现有单模型方法难以有效应对这些问题。

Method: Tri MARF采用多智能体协作架构，融合2D多视角图像、文本描述与3D点云。具体包括三个功能特化的智能体：视觉-语言模型智能体生成多视角描述，信息聚合智能体筛选最佳描述，门控智能体对齐文本语义与3D几何，实现精准标注。

Result: 在Objaverse LVIS、Objaverse XL和ABO等数据集上，Tri MARF的CLIPScore达88.7，检索准确率ViLT R@5分别为45.2和43.8，且在单张A100 GPU上每小时可处理12000个对象，表现大幅领先现有方法。

Conclusion: Tri MARF能够精准高效地进行大规模3D对象标注，有效解决了3D标注领域现有方法难以解决的问题，为相关领域的应用提供了先进的技术路径。

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [12] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督与弱监督学习的新方法，用于预测耳蜗植入手术中的乳突切除区形状，实现无需手工标注即可高效准确地辅助术前规划。


<details>
  <summary>Details</summary>
Motivation: 乳突切除术是耳蜗植入手术的重要步骤，准确预测其形状有助于减少风险和提高手术效果，但由于真实标签获取困难，相关深度学习研究较少。

Method: 本文提出了一种混合自监督和弱监督的学习框架，利用未标注的术前CT影像直接预测乳突切除区域，无需人工标注，同时在弱监督部分引入3D T分布损失函数。

Result: 该方法在复杂且无明确边界的乳突切除区域预测中取得平均Dice分数0.72，优于当前各类主流方法。

Conclusion: 首次实现自监督与弱监督结合预测乳突切除形状，为基于术前CT的三维术后表面重建提供基础，有望成为CI手术规划的高效工具。

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [13] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: 深度学习近年来在心脏MRI重建领域表现优异，但通用性有限。本文提出CRUNet-MR-Univ模型，可适应多样化临床应用，泛化能力强，表现优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习模型在心脏MRI重建上面对不同采样、对比、扫描厂商和疾病类型时表现不佳，缺乏泛化能力。需要一个能适应各种临床场景的通用模型。

Method: 提出了CRUNet-MR-Univ基础模型，利用空间和时间相关性及基于prompt的先验信息，提升对多类型CMR扫描的适应性。

Result: 该方法在多种不同临床设定下均优于现有基线方法，表现出卓越的稳健性和泛化能力。

Conclusion: CRUNet-MR-Univ基础模型展示了处理多样化CMR场景的有效性和应用前景，有望推动心脏MRI的实际临床应用。

Abstract: In recent years, deep learning has attracted increasing at- tention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clini- cal applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or nar- row subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that lever- ages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently out- performs baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [14] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: 该论文提出Gated Perception-Reasoning Optimization（GPRO）框架，通过动态分配感知和推理路径，提高大视觉语言模型的准确性和效率。实验显示GPRO在减少推理冗余和提高正确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型虽然具备强大的逐步推理能力，但慢速思考容易导致‘过度推理’——即对简单问题产生冗长、低效、反而影响准确性的回答。同时，现有自适应推理策略忽视了视觉感知失效是导致推理错误的关键。

Method: 提出GPRO元推理控制器，在每一步动态决定走‘快路径’（高效）、‘感知路径’（重新审视视觉输入）或‘推理路径’（自省）。利用约79万条标注样本，通过教师模型区分感知失误与推理错误，并用多目标强化学习在准确率与计算成本间优化。

Result: 在五个基准数据集上，GPRO在准确率和推理效率方面显著优于现有慢速推理方法，生成的答案更精准且更简洁。

Conclusion: 解决了慢速推理中的‘过度推理’和视觉感知失效问题，GPRO为视觉语言模型带来更好地准确性和效率，适用于各种需要高效推理的多模态场景。

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [15] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: 该论文提出了UniDrive-WM，一种基于视觉语言模型（VLM）的统一世界模型，可以同时完成自动驾驶场景理解、轨迹规划和未来图像生成。实验证明其超越以往分模块的方法，在高难度基准数据集上显著提升了规划表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型通常将感知、预测与规划分离处理，限制了模型的统筹能力和性能提升。近年来VLM在规划中展现潜力，但尚未实现全流程统一化，亟需一种融合感知、预测、规划于一体的新方法，以提升自动驾驶的场景理解和决策能力。

Method: UniDrive-WM统一设计了包括感知、轨迹规划与未来图像生成的端到端架构。该模型用轨迹规划器预测未来轨迹，并基于此引导VLM生成未来场景帧，从而提供更多监督信号，用于场景理解与轨迹优化。此外，论文分析了离散与连续图像输出表示对驾驶任务的影响。

Result: UniDrive-WM在Bench2Drive基准上实现了高保真未来图像生成，轨迹L2误差降低5.9%，碰撞率降低9.2%，均优于之前最优方法。

Conclusion: 结果表明，将VLM驱动的推理、规划及生成式世界建模紧密结合，能明显提升自动驾驶系统的性能，推动自动驾驶技术迈向更高的智能水平。

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [16] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文提出一种结合大语言模型（LLM）与视觉-语言模型（VLM）的方法，用于提升森林遥感影像变化检测与解读能力，并推出了新的Forest-Change数据集。实验结果显示，所提系统在多个数据集上取得了优异的检测与语义描述表现。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率卫星影像和深度学习的进步，森林监测变得更加细致，然而在像素级变化检测和复杂森林动态的语义解读方面仍然存在挑战。尤其是在遥感影像变化解释（RSICI）领域，如何将大语言模型与视觉语言模型结合，还鲜有深入探索。

Method: 作者提出了一个以多层次变化解读（MCI）视觉-语言骨干网为基础，并由大语言模型进行任务编排的智能系统。该系统支持基于自然语言的多任务遥感影像变化查询。此外，文中发布了Forest-Change数据集，包含双时相卫星影像、像素级变化掩膜和多粒度语义变化描述，结合人工标注和规则生成。

Result: 该系统在Forest-Change数据集上获得了67.10%的mIoU和40.17%的BLEU-4分数，在LEVIR-MCI-Trees数据集上分别获得88.13%和34.41%。这些指标分别对应像素级变化检测和语义描述性能，优于现有方法。

Conclusion: 实验结果表明，基于LLM驱动的RSICI系统能够显著提升森林变化分析的可访问性、可解释性和效率。数据和代码已开源，有助于推动相关领域研究。

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [17] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: 提出了DrivoR，一种高效简洁的端到端自动驾驶纯Transformer架构，引入摄像头感知的压缩token，兼顾高效率与高精度，实测优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶端到端方法计算量大或不够高效，难以在保证精度的同时实现快速推断，因此需设计更高效、更具表现力的模型。

Method: 基于预训练的视觉Transformer（ViT），引入多摄像头感知token，将多摄像头特征压缩为紧凑场景表征，并用两个轻量级解码器生成及评估轨迹，评分解码器能解释不同驾驶行为（如安全、舒适等），支持推理时行为调节。

Result: 在NAVSIM-v1、NAVSIM-v2和高仿真HUGSIM基准上，DrivoR性能优于或持平现有强基线，兼具高效率和高精度。

Conclusion: 纯Transformer结合token压缩足以实现高效、精确、自适应的端到端自动驾驶，且设计简单易部署。

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [18] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种高效且边界感知的稀疏token方法（TokenSeg），用于3D医学图像分割，在准确性和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 三维医学图像分割任务计算量大且需处理大量冗余数据，尤其是在同质区域。现有方法在内存消耗和推理效率上存在瓶颈，因此急需更加高效且能关注解剖结构细节的新方法。

Method: TokenSeg主要包括：1）多尺度分层编码器，从不同分辨率提取400个候选token，既考虑整体解剖信息，也聚焦边界细节；2）边界感知tokenizer，结合VQ-VAE量化与重要性打分，选择100个突出的token（60%以上位于肿瘤边界附近）；3）稀疏到密集的解码器，实现token重投影、逐步上采样与跳跃连接，恢复高分辨率mask。

Result: 在3D乳腺DCE-MRI数据集（960例）上，TokenSeg实现了94.49%的Dice分数和89.61%的IoU，GPU内存和推理延迟分别减少64%和68%；在MSD心脏和脑MRI数据集上也表现出良好的泛化能力。

Conclusion: TokenSeg充分表明稀疏、结构感知的token表达能在保证准确性的同时，大幅提高三维医学图像分割的效率，具备广泛的实际应用前景。

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [19] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉身份提示（visual identity prompting）的图像扩散模型新方法，有效提升了机器人操作任务中数据增强的多样性和一致性，增强了下游机器人策略的表现。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略训练依赖于大量多样、优质的数据，但现实数据采集困难，尤其是在多视角和时序连贯要求下。已有通过文本提示的图像扩散技术用于数据增强，但无法充分满足实际需求，尤其是在场景一致性及指定目标方面。

Method: 作者提出了视觉身份提示：把典型的场景或物体图像作为扩散生成的条件，显式引导多视角、连贯的场景生成。为此还建立了大规模视觉身份池的数据流水线。最终用增强后的数据训练视觉-语言-动作及视动策略模型。

Result: 用本文方法增强的数据训练下游模型，在仿真和真实机器人上的表现均有一致提升，优于仅用文本提示的方法。

Conclusion: 视觉身份提示结合扩散模型使机器人操作任务的数据增强更加有效，提升了数据多样性、一致性和现实应用的性能。

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [20] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: 本论文提出了一种基于风格迁移的人脸纹理精细化方法FaceRefiner，有效提升了从单张图片生成完整面部纹理的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸纹理生成方法通过深度网络合成图片内容并填充UV图，但合成的UV纹理受训练数据或2D生成器空间限制，在处理真实环境图片时容易出现与输入不一致的问题（如细节和身份丢失）。

Method: FaceRefiner以采样的3D人脸纹理为风格图，将已有纹理生成方法的输出作为内容图，通过风格迁移将高、中、低级别信息（包括像素级信息）从风格图转移到内容图。与传统风格迁移只迁移高、中层信息不同，FaceRefiner引入可微渲染，实现在可见区域的全层次信息转移，强化细节和语义保留。

Result: 在Multi-PIE、CelebA和FFHQ等主流数据集上的大量实验表明，FaceRefiner在提升纹理质量和身份保真能力方面，相较于现有技术表现更优。

Conclusion: FaceRefiner方法能够有效提高人脸纹理的真实感和与输入的一致性，为单张图片生成高质量、身份一致性强的人脸纹理提供了新方案。

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [21] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: 本文提出了一种利用设计理念复现的动态有害表情包检测方法RepMD，通过设计理念图（DCG）和多模态大模型结合，实现对不断变化的有害表情包的精准检测，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 有害表情包在网络社区中不断变化，类型和内容都在演化，传统检测方法难以追踪，因此亟需能适应其变化规律的新检测方法。

Method: 作者提出了RepMD方法，首先借鉴攻击树理论定义了设计理念图（DCG），以描述有害表情包的设计流程。然后通过对历史表情包的复现与图结构剪枝形成DCG，最后用DCG指导多模态大语言模型（MLLM）检测有害表情包。

Result: RepMD在检测有害表情包的任务上达到81.1%的最高准确率，在应对类型变化和时间演化的情形下准确率仅有小幅下降。人工评测表明RepMD能显著提升人工发现有害表情包的效率，每个表情包平均15到30秒。

Conclusion: RepMD方法有效总结有害表情包不变的设计理念，联合MLLM在检测任务中具备优良的适应性和泛化能力，为动态网络环境下打击有害内容提供了新手段。

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [22] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 本论文探讨了通过3D条件生成模型生成高质量合成LGE MRI数据，以增强左心房分割性能。


<details>
  <summary>Details</summary>
Motivation: 左心房壁和心腔的精确分割对于房颤患者纤维化的定量分析至关重要。然而，目前精确的机器学习分割模型开发受限于数据稀缺和解剖结构复杂。本文旨在解决训练数据不足问题，并提升左心房分割结果。

Method: 本文开发了一套利用3D条件生成模型（Pix2Pix GAN、SPADE-GAN、SPADE-LDM）从复合语义标签图合成高保真3D LGE MRI图像的流程，标签结合了专家注释和无监督组织聚类。通过Frechet Inception Distance（FID）评估了合成图像的真实性，并考察其对3D U-Net分割模型的影响。

Result: SPADE-LDM生成了最真实且结构准确的图像（FID=4.063），优于其他GAN模型。使用合成数据增强后，左心房腔分割的Dice分数由0.908提升到0.936，提升具有统计学意义（p<0.05）。

Conclusion: 基于标签的3D影像合成能够有效提升对稀缺心脏结构的分割，推动机器学习方法在心脏影像分析中的应用。

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [23] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: 本文提出了面向多层设计文档编辑的新方法MiLDEAgent，并构建了相应的评测基准MiLDEBench，显著提升了复杂文档的多层次编辑性能。


<details>
  <summary>Details</summary>
Motivation: 以海报为代表的多层设计文档广泛存在，编辑此类文档需要理解和协调多个层级元素。现有方法大多局限于单层或仅生成，不能满足真实场景下精细、分层的编辑需求。

Method: 提出MiLDEAgent框架，结合RL训练的多模态推理器进行逐层分析理解，并结合图像编辑器进行定向修改；创建了包含2万多设计文档与编辑指令的大型基准MiLDEBench，配套多维度评测协议MiLDEEval。

Result: 大规模实验显示，现有开源模型难以完成多层文档编辑，闭源模型存在格式兼容问题。MiLDEAgent具备较强的分层推理和精准编辑能力，在全部开源对比中效果最优，接近闭源模型表现。

Conclusion: MiLDEAgent为多层设计文档编辑领域建立了第一个强有力的基线，为后续研究提供了重要参考和标准。

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [24] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: 該文討論了AI驅動的人本物理系統在運作過程中，因與人互動而產生運作不確定性的挑戰，並提出框架以提升其安全與可靠性。以糖尿病封閉迴路血糖控制為例，提出新方法偵測用餐未上報情況。


<details>
  <summary>Details</summary>
Motivation: 隨著AI被廣泛應用於人本物理系統，如醫療監控和自駕車，系統經常在實際運行時因人機互動導致非預期運作，可能威脅安全和系統穩定，因此需針對不確定情境下的安全問題進行研究。

Method: 首先分析運作偏離導致系統陷入未知情況的原由；進而提出一個評估框架，對比不同安全與保護策略。最後以1型糖尿病患者的餐食未匯報偵測為例，設計個人化的影像檢測新技術。

Result: 文中通過新框架對不同保護策略進行系統化評價，並展示基於影像的偵測模型能有效識別未報備餐食，提升血糖控制系統的安全性。

Conclusion: AI使人本物理系統面臨更多不確定性挑戰，本文提出評估框架並以醫療場景驗證，能提升此類系統安全運行，對未來AI應用具有指導意義。

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [25] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: 本论文提出了一种专注于头颈部多器官分割的高不确定性区域引导多架构协同学习模型（HUR-MACL），通过自适应识别模型易出错区域，并结合不同模型结构，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在复杂且小器官的分割上表现不佳，仅简单拼接多模型特征会导致功能重叠并限制分割精度，因此需要方法充分挖掘各模型特性，有效处理高不确定性区域。

Method: 提出HUR-MACL模型，通过卷积神经网络自适应检测高不确定性区域，并在这些区域结合Vision Mamba和可变形卷积网络进行协作分割。设计异构特征蒸馏损失，加强模型间协同提升分割效果。

Result: 在两个公开数据集和一个私有数据集上达到了当前最佳分割效果（SOTA），显著优于以往方法。

Conclusion: HUR-MACL方法能有效识别和处理分割难点区域，实现多模型优势互补，极大提升了头颈部多器官分割的准确率，在医学影像分割领域具有广泛应用前景。

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [26] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文针对文本生成图像技术中的图文对齐评估问题，提出了一种基于双曲空间的自适应对齐评估框架HyperAlign，有效提升图文对齐度评估的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图文对齐评估方法主要依赖欧氏空间度量，无法充分刻画图文语义对齐的结构性特征，且缺乏对不同样本的自适应能力，难以准确评估生成图像与文本的对齐程度。

Method: 提出HyperAlign框架：1）用CLIP提取欧氏特征并映射至双曲空间；2）设计动态监督蕴涵建模机制，将离散蕴涵逻辑转为连续几何结构监督信号；3）引入自适应调制回归器，根据双曲几何特征为每个样本动态调整欧式余弦相似度以生成最终得分。

Result: HyperAlign在单库评测和跨库泛化任务中均取得了极具竞争力的性能，验证了双曲空间建模的有效性。

Conclusion: 引入双曲空间及自适应机制，能够更有效、泛化地评估文本生成图像任务中的图文对齐度，为相关研究提供了新的建模思路和技术手段。

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [27] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: 提出Agri-R1框架，通过自动生成高质量推理数据和创新训练方式，大幅提升农业大模型的疾病诊断和知识问答能力，且参数量更小但性能优于更大基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统农业视觉语言模型需要大量标注，缺乏可解释性和泛化性，且高质量推理数据依赖专家标注，成本高。现有方法难以应对农业领域多样且开放式的问题类型。

Method: 设计Agri-R1框架，自动通过视觉-语言合成和大模型筛选生成高质量推理数据，仅用19%的样本。训练过程中采用创新的Group Relative Policy Optimization（GRPO）算法，并引入融合领域词典和模糊匹配的新奖励函数，用于评价开放式答案的正确性和语言灵活性。

Result: 在CDDMBench评测下，3B参数模型表现可与7B-13B基线相当，在疾病识别准确率、农业知识问答和跨域泛化能力上分别提升23.2%、33.3%和26.1分。消融实验表明，结构化推理数据与GRPO探索策略协同带来显著增益，且对复杂问题提升更明显。

Conclusion: Agri-R1通过数据自动化和创新训练范式，实现小参数量模型下对农业领域多样性问题的高鲁棒性推理，显著优于传统方法，为农业AI模型的泛化和应用提供新思路。

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [28] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的UNet变体——DB-MSMUNet，专为胰腺及其病灶的CT分割设计，极大提升了分割精度、边缘检测能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 胰腺和其病灶在CT扫描中的精准分割对于胰腺癌的准确诊断和治疗至关重要，但由于组织对比度低、解剖边界模糊、器官形状不规则以及病灶体积小，自动分割极具挑战性。

Method: 本文提出DB-MSMUNet新架构，包含带形变卷积与多尺度状态空间建模的多尺度Mamba模块编码器、用于精细边界捕获的边缘解码支路与细节保持的区域解码支路，并在多个尺度引入辅助深度监督，增强分割表现。

Result: 在NIH Pancreas、MSD及临床胰腺肿瘤三组数据集上，DB-MSMUNet的Dice系数分别为89.47%、87.59%、89.02%，整体性能超过主流分割方法，特别在边缘保持与模型鲁棒性方面优势明显。

Conclusion: DB-MSMUNet能够显著提升胰腺及其病灶的分割精度和鲁棒性，在实际临床胰腺CT分割任务中具有良好的推广价值和应用前景。

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [29] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为HATIR的热感知扩散模型，用于解决红外视频在大气扰动和压缩退化下的超分辨率问题，并发布了首个红外视频超分辨率数据集FLIR-IVSR。


<details>
  <summary>Details</summary>
Motivation: 红外视频在恶劣环境下应用广泛，但由于大气扰动和压缩等问题，视频质量受损。现有视频超分辨率方法无法有效兼顾红外与可见光间的特性差异，也无法很好地恢复由扰动带来的失真。单独串联去扰动和超分辨率模块又会导致误差累积，问题未能有效解决。

Method: 提出HATIR模型，将热感知形变信息（基于热区间的相位响应特性）融入扩散采样路径，实现对扰动退化和细节丢失的联合逆建模。设计了Phasor-Guided Flow Estimator用于估计扰动流场， Turbulence-Aware Decoder采用选择性噪声抑制、结构化注意力等机制提升结构恢复能力。同时，构建了FLIR-IVSR红外超分辨率数据集。

Result: HATIR能够有效抑制大气扰动影响，提升红外视频的超分辨率重建质量，在结构恢复和细节还原方面取得优异效果。

Conclusion: HATIR为红外视频在大气扰动下的高质量恢复提供了一种创新性解决方案，并通过公开数据集推动了该领域进一步研究。

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [30] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为WebCryptoAgent的加密货币自动交易系统，可以高效整合来自网络和市场的多源信息，显著提升极端行情下的交易稳定性与风险控制能力。


<details>
  <summary>Details</summary>
Motivation: 目前的加密货币自动交易系统难以同时处理噪声严重的多源网络证据和快速市场冲击，导致信息融合效率低下和风险暴露增加。尤其是在行情剧烈波动时，现有系统的反应速度与决策鲁棒性不足。

Method: 论文将网络信息和市场信号的决策过程分解为不同模态的智能体，再将各自的输出整合为一个统一的证据文档，实现有信心度校准的推理。同时，引入了战略决策和实时风险控制分离的架构，将小时级战略思考与秒级风险应对解耦，保证遇到市场突发冲击时能即时自适应防御。

Result: 在真实加密货币市场的大量实验表明，WebCryptoAgent较传统基线方法在交易稳定性、虚假交易减少、极端风险处置等方面均有明显提升。

Conclusion: WebCryptoAgent能有效融合多源信息并快速应对市场冲击，在高波动性的加密货币交易场景下展现了更优的稳健性和风险管控能力。

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [31] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出了一个名为 Forge-and-Quench 的新型统一框架，实现了多模态大模型中的图像生成和图像理解的有机结合，通过让理解能力指导生成过程，有效提升生成图像的真实性与细节丰富度。


<details>
  <summary>Details</summary>
Motivation: 在多模态领域，图像生成和理解的统一已成为重要目标，但如何有效利用理解能力提升生成质量尚未被充分挖掘。此前的研究多着重于推理和世界知识迁移，而忽视了理解对生成细节和真实性的直接促进作用。因此，本文聚焦于利用理解提升图像生成本身的表现。

Method: 提出 Forge-and-Quench 框架，流程包括：首先由多模态大模型（MLLM）基于全部对话和文本指令推理，生成增强版文本指令；接着通过新设计的 Bridge Adapter，将增强文本转化为一种虚拟视觉特征（Bridge Feature），作为理解到生成的桥梁；最后该特征连同增强文本一起输入到文本到图像生成（T2I）模型中，引导生成过程。对 Bridge Feature 和 Bridge Adapter 的设计进行了系统研究以验证该方法有效性。

Result: 实验表明，Forge-and-Quench 框架具备很强的可扩展性和灵活性，可高效迁移到不同的MLLM和T2I模型，同时大幅减少训练成本，不影响理解能力。多项评测显示该方法在多个模型上均显著提高了生成图像的细节和真实性，同时保持了良好的指令遵循和世界知识应用表现。

Conclusion: Forge-and-Quench 框架有效整合了理解与生成，能在不损失多模态理解能力的前提下，大幅提升图像生成的质量与实用性，为多模态模型的进一步发展提供了新思路与方法。

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [32] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种面向人类图像伪造检测的统一框架HuForDet，在脸部和全身范围都能准确检测伪造内容，并发布了新的数据集，在多类伪造场景下取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 以往检测方法主要针对脸部或全身，难以应对多种人像伪造的泛化问题。随着AIGC导致深度伪造威胁升级，亟需一个可在更广泛人类图像场景有效检测伪造的方案。

Method: 提出HuForDet双分支架构。一方面采用融合RGB和频域特征的脸部伪造检测分支，并引入自适应LoG模块以捕捉各尺度伪造特征；另一方面用多模态大语言模型分析全身语义一致性，结合置信度机制动态融合特征。另公开集合现有和新全身数据的HuFor数据集。

Result: 实验结果表明，HuForDet在多种人类图像伪造检测任务中达到SOTA性能，对不同类型的伪造手段具备更强鲁棒性。

Conclusion: HuForDet作为首个统一的人像图像伪造检测框架，显著提升了各类伪造检测能力，为AI生成内容安全防护提供了更强有力的技术支持。

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [33] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: 本文比较了自定义轻量级CNN与主流深度架构（ResNet-18、VGG-16）在多个农业和城市视觉任务上的表现，关注数据受限环境下模型的适用性和效能。


<details>
  <summary>Details</summary>
Motivation: 现有卷积神经网络虽在视觉任务中表现优异，但不同领域、不同规模的数据集提出了新挑战。尤其是在资源有限和数据受限的场景下，如何选用合适的模型架构有待深入研究。

Method: 选择了五个交叉领域的数据集（芒果品种、稻米品种、路面状况、三轮车检测、人行道侵占），提出自定义轻量级CNN，并与ResNet-18、VGG-16进行对比，分别采用从头训练和迁移学习方法。通过统一的数据预处理、增强与实验设计，系统分析了模型架构、深度和预训练对不同任务的收敛性与泛化能力的影响。

Result: 自定义轻量级CNN在多个数据集上取得了与主流深度网络相近的效果。迁移学习和深层网络在数据稀缺场景中表现出明显优势，并揭示了不同模型在不同难度任务上的表现差异。

Conclusion: 自定义CNN模型在多个实际视觉任务中可达到高效且有竞争力的表现。迁移学习和复杂深度架构在数据受限的高影响应用场景下尤为有用，为现实资源受限环境下深度学习模型的部署提供了指导。

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [34] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: 这篇论文提出了AIVD框架，通过边缘端的轻量检测器和云端MLLMs协作，实现了精确的目标定位和高质量语义生成，并支持异构设备下的高效动态调度。实验显示能提升性能、降低资源消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在语义理解和视觉推理表现卓越，但在目标精定位以及资源受限的边云协同部署中仍面临挑战，需要兼顾定位准确性、语义生成质量及低资源消耗。

Method: 设计了AIVD框架，结合边缘轻量检测器实现初步定位，云端MLLM进行高级语义处理；并提出视觉-语义协同增强的高效微调策略提高鲁棒性，同时开发异构资源感知的动态调度算法优化吞吐和延迟。

Result: AIVD框架显著减少了资源消耗，提高了MLLM的分类和语义生成表现，调度策略在多种设备与网络环境下显著提升吞吐量和降低延迟。

Conclusion: AIVD能够有效解决大模型目标定位和部署资源问题，提高多模态任务整体表现和部署效率，适用于复杂异构边云环境。

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [35] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: 本研究提出了一种利用骨架化(skeletonization)的新型对抗攻击方法，旨在高效缩小攻击搜索空间，专门针对包含文本（特别是数学公式图像）的图片，并评估其对基础模型的视觉能力的影响。


<details>
  <summary>Details</summary>
Motivation: 基础大模型的视觉能力近年来成为研究热点，尤其是在处理包含复杂结构（如数学公式）文本图片时。当前对抗攻击方法在高维空间内搜索成本较高，且难以精准评估模型对这些特殊图片的理解。论文旨在解决这一挑战，提高对图像理解模型的分析效率和精细度。

Method: 作者提出了一种基于骨架化(Skeletonization)的对抗攻击方法，通过提取图片的骨架特征，有效减少攻击时的搜索空间。该方法特别针对含有复杂结构和文本（如LaTeX数学公式图像）的输入进行攻击，并通过对原始输出和对抗输出在字符级和语义级的详细对比分析模型的视觉理解能力。

Result: 实验中，作者方法能够成功地对包含数学公式的图片发起有效的对抗攻击，并分析出原始输出与对抗输出之间在字符和语义层面的差异。方法同时在ChatGPT等现实场景下得到应用验证，显示其实际有效性。

Conclusion: 基于骨架化的对抗攻击方法能够在缩小攻击搜索空间的同时，有效评估和揭示基础视觉模型对复杂文本和结构图片的视觉解释与推理能力的局限，对模型安全性分析及未来模型改进具有实际指导意义。

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [36] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: 本文提出了ProFuse，一种高效、支持开放词汇的3D场景理解框架，基于3D高斯投影（3D Gaussian Splatting, 3DGS），实现了更快、更一致的语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在场景语义理解中存在速度慢、跨视角一致性和语义分割精度不足等问题，且依赖预训练和大量后期精化，限制了实际应用。

Method: ProFuse引入了基于稠密对应的预注册流程，无需预训练3DGS场景，结合跨视角聚类同时初始化高斯点与构建3D语义提议，并利用聚合特征在直接注册过程中实现语义信息的多视角融合。

Result: ProFuse可以在每个场景大约5分钟内完成语义附加，比当前最优方法快2倍，并在无需附加优化的前提下实现了强大的开放词汇3DGS理解能力。

Conclusion: ProFuse有效提升了开放词汇3D场景理解的速度和准确性，显著减少了计算开销和训练需求，为3DGS在实际应用中的广泛推广提供了有力工具。

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [37] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于分割的单目偏振形状恢复（SMSfP）方法，通过将全局表面重建问题分解为多个局部凸区域，显著提高了表面法线恢复的准确性和稳定性，有效缓解了方位角歧义问题。


<details>
  <summary>Details</summary>
Motivation: 现有的单目偏振形状恢复方法普遍存在方位角歧义问题，严重影响了三维重建的准确性和稳定性，需要新的方法来克服这一物理分析的固有限制。

Method: 提出了偏振辅助自适应区域生长（PARG）分割策略，将全局凸性假设分解为局部凸区域。并设计了多尺度融合凸性先验（MFCP）约束，以保证局部表面的一致性，并增强纹理和结构细节的恢复能力。

Result: 在合成和真实数据集上的大量实验表明，该方法在解歧准确率和几何保真度方面都明显优于现有的物理基础单目偏振恢复技术。

Conclusion: 所提出的SMSfP框架有效克服了传统单目偏振三维重建中的方位角歧义问题，实现了更高的重建精度和表面细节表现，具有较强的实用价值和推广前景。

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [38] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了GeM-VG模型，实现了多图像视觉指代（grounding）任务的泛化，为多图像与单图像指代任务带来明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在多图像指代任务上存在目标单一、任务类型有限、数据集覆盖不足等问题，缺乏统一的泛化模型，限制了实际应用。

Method: 系统梳理并分类现有多图像指代任务，提出MG-Data-240K大规模数据集，丰富目标数量和图像关系。方法上，提出混合强化微调策略，将链式推理(CoT)与直接回答结合，通过规则奖励引导R1类算法进行模型微调。

Result: 实验表明，GeM-VG模型在多图像指代基准MIG-Bench和MC-Bench上分别比已有最佳模型提升2.0%和9.7%；在单图像指代任务ODINW上比基础模型提升9.1%。同时，泛化的多图像理解能力也得以保留。

Conclusion: GeM-VG在多图像视觉指代任务中实现了更好的泛化能力，并在各类指代与理解任务上取得了领先性能，方法具备较强应用前景。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [39] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的反事实视频生成框架，并构建了CounterVid数据集与MixDPO优化方法，从而显著提升了视频语言模型在动作识别和时间顺序推理上的表现，并减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型在处理多模态理解任务时表现出色，但在动作推理和时间顺序理解上易受幻觉困扰，主要原因为过度依赖语言先验而忽略细粒度视觉动态。现有缓解方法难以解决这一根本问题，因此亟需新方法提升模型精细视觉推理能力。

Method: 作者设计了一个反事实视频生成框架，能够合成只在动作或时间结构上有所差异的视频，同时保留其他场景上下文。具体方式为：1. 利用多模态大模型（LLMs）提出动作与编辑建议；2. 结合扩散式图像和视频模型批量生成具有语义难度的反例。基于该框架，构建了约2.6万个对比偏好数据对（CounterVid），同时提出了MixDPO方法，结合文本与视觉偏好进行直接优化。

Result: 将Qwen2.5-VL在CounterVid和MixDPO上微调后，可在动作推理和时间顺序理解上取得持续提升，尤其在时间排序任务中效果显著，并能够有效泛化到主流视频幻觉评测基准上。

Conclusion: 通过提出反事实视频生成、CounterVid数据集以及MixDPO优化方法，显著提升了视频语言模型减少幻觉、增强动作与时序推理的能力，为多模态理解和实用化打下基础。

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [40] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 本文探讨了基于散焦（defocus）信息进行深度估计的方法，证明了在一般成像设备条件下，高斯模型能够准确描述散焦模糊的性质，并在大多数情况下其误差小于1%。


<details>
  <summary>Details</summary>
Motivation: 深度估计在3D重建领域依然面临挑战，尤其是在2D图像中区分目标散焦模糊与固有模糊。如何利用简单、高效且准确的模型进行高质量深度估计是核心动力。

Method: 首先基于几何光学框架，通过衍射极限成像理论分析散焦算子的性质。进一步设计实验证明常用成像设备的散焦模糊可以以高斯模型准确近似，并量化了该模型与实际物理散焦的拟合误差。

Result: 在聚焦深度1到100米、最大变化10%的典型场景下，高斯模型对散焦算子的近似误差（最大平均绝对误差）小于1%。

Conclusion: 高斯模型能够为绝大多数实际成像设备提供准确、实时的散焦描述，是场景深度估计和三维重建任务中高效可靠的理论基础。

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [41] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种改进的Pix2Pix框架，融合了SEResNet和U-Net++，并使用简化的PatchGAN判别器，用于提高医学MRI图像翻译的质量和结构保真度，尤其适用于样本数量较少的情况。


<details>
  <summary>Details</summary>
Motivation: MRI成像时间长、成本高、分辨率有限，影响了其临床普及。图像翻译能弥补这些缺点，而现有的Pix2Pix方法在医学图像翻译中的潜力尚未被充分挖掘。

Method: 作者提出在Pix2Pix框架中整合SEResNet（提升通道特征表达）和U-Net++（增强多尺度特征融合），并引入简化的PatchGAN判别器以稳定训练和提高局部结构真实性。

Result: 实验表明，提出的方法在少于500张图像的few-shot条件下，在多个MRI模态内图像翻译任务中，展现出结构保真度高、图像质量优良和强泛化能力。

Conclusion: 本研究展示了对Pix2Pix的有效扩展，显著提升了医学图像翻译的性能，尤其适合小样本的医学影像任务。

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [42] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为Measurement-Consistent Langevin Corrector（MCLC）的模块，有效提升了基于潜在扩散模型（LDM）的逆问题解算器的稳定性，解决了现有方法易出现伪影和质量退化的问题，并且在多种图像复原任务中展现优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于潜在扩散模型（LDM）的零次逆问题解算器，虽然具备通用性，但常常因为逆扩散过程的不一致性导致不稳定、出现伪影和重建质量下降。因此，作者希望解决该不稳定性，使逆问题解算器更加稳定和可靠。

Method: 作者首先通过理论和实验分析，将不稳定归因于求解器与真实逆扩散动力学间的差异。为此，提出了测量一致的Langevin修正器（MCLC），该模块通过测量一致的Langevin更新修正LDM逆解算器，不依赖于线性流形假设，适应性更强。

Result: MCLC在多种图像复原任务（如去噪、去模糊等）中显著提升了结果稳定性和图像质量，并且能够兼容各种现有的逆解算器。作者还对常见的blob伪影进行了分析，解释了其成因。

Conclusion: MCLC为更为强健的零次逆问题解算器提供了关键技术，有望推动领域在无需领域适配的普适性逆问题求解方面前进。

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [43] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种将预训练的扩散模型通过低成本微调转变为分层（pyramidal）模型的流水线方法，在保证输出视频质量不下降的情况下，大幅提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有分层扩散视频模型虽然能降低多步去噪模型的推理成本，但通常需要从头训练，开源版本普遍效果不如最新的SOTA模型，特别是在视觉质量方面。为此，作者希望改进分层模型不过度牺牲输出质量，并降低其训练及推理门槛。

Method: 作者提出了一种流水线方法，可以直接将预训练的扩散模型转换为分层模型，并通过低成本微调实现，无需从零开始训练。论文还实验并比较了在分层模型中多种推理步骤蒸馏策略，以进一步提升推理效率。

Result: 转换后的分层模型在视频输出质量无明显下降的前提下，大幅提升了推理效率。不同蒸馏策略对推理和效果带来的影响也有量化分析。

Conclusion: 通过将预训练模型转为分层模型，并结合合适的步骤蒸馏策略，可以兼顾视频生成的视觉质量和高效推理，优于从头训练的传统分层模型。

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [44] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: 该论文针对城市监控中无人机长期跟踪的挑战，首次评估了基础大模型SAMURAI在无人机跟踪上的表现，并提出结合检测器的新方法，有效提升了复杂环境下的鲁棒性和跟踪效果。


<details>
  <summary>Details</summary>
Motivation: 现有无人机跟踪方法多依赖检测器，虽有较高帧级精度，但容易因频繁漏检导致时序不一致。随着无人机威胁日增，监控场景中对鲁棒跟踪需求迫切，但RGB无人机跟踪尚未得到充分研究，而强大基础模型（如SAMURAI）在此领域的潜力未被评估。

Method: 作者首次系统性评估了基础模型SAMURAI在无人机城市监控跟踪上的性能，并提出将检测器线索与SAMURAI结合，用于提升边界框初始化和适应长序列任务，从而增强跟踪的鲁棒性和一致性。

Result: 改进方法在复杂城市环境和无人机离场-重入场景下，表现出显著优于原生SAMURAI的效果。具体指标如success rate提升最高达+0.393，FNR降低至-0.475，实现了跨数据集、跨评价指标的稳定优化。

Conclusion: 结合检测器的SAMURAI不仅显著提升了无人机长期跟踪的鲁棒性，也验证了基础大模型在细分安防领域的强大潜力，为实际无人机监控应用提供了有效方案。

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [45] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: 本文提出了一种面向古代铭文图像的增强方法，通过二值化和预处理技术去除噪声和提升文字清晰度，经实验验证提升了铭文识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 古代铭文图像常因年代久远和环境影响，存在严重噪声、对比度低、文本与背景相似等问题，导致内容难以辨认。提升图像可读性对于历史文献的解读和保存具有重要意义。

Method: 提出基于二值化及互补预处理（去污渍、增强不清晰文字）的方法，对石刻、金属板和文档类型的古代铭文图像进行处理，并分别在改进前后用K-NN和SVM分类器进行识别测试。

Result: K-NN分类器下，三类铭文图像识别准确率分别为55.7%、62%、65.6%；SVM分类器下分别为53.2%、59.5%、67.8%。均有明显提升。

Conclusion: 所提出的增强方法有效提升了古代铭文（尤其是Marathi语铭文）图像的可读性，为后续智能识别和历史文档研究提供了基础。

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [46] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: 本文提出了一个专注于车辆相关动作的真实监控视频检索基准SOVABench，并构建了相应的评价协议，用于考察模型分辨不同动作及时间方向的能力。利用多模态大语言模型（MLLMs）生成的可解释描述作为新型嵌入特征，无需训练即可提升在SOVABench和其它空间与计数任务上的表现，并且公开了代码和数据。


<details>
  <summary>Details</summary>
Motivation: 现有的视频检索基准主要关注场景级相似性，缺乏对监控场景下关键动作区分的评测，无法满足自动事件识别和行为分析在监控领域的实际需求。

Method: 提出了SOVABench，一个基于真实监控视频、围绕车辆行为构建的检索基准，并设定了交叉动作识别与时间方向理解两种评测协议。同时，利用多模态大语言模型（MLLMs）输出的图片和视频描述生成可解释的嵌入，无需额外训练即可用于检索评测。

Result: 该框架在SOVABench和多个空间及计数类基准上取得了优异表现，解决了以往对比学习视觉-语言模型在相关任务上的失败，验证了MLLM描述驱动嵌入的有效性。

Conclusion: SOVABench提供了更贴近实际监控需求的评测框架，推动了视频行为分析领域的发展，同时MLLM驱动的嵌入方法为复杂动作识别任务提供了新思路。

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [47] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: 本文提出用YOLO目标检测模型识别中世纪手稿作者，提高了作者辨识的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统古文书作者识别方法精度受限，之前的模板匹配和CNN对字符检测及分类有阈值选取、适应性等问题，需要更高效和精确的自动化识别方式。

Method: 在先前用模板匹配和CNN检测并识别特定字母（如a）以辨别人手基础上，本文用YOLOv5目标检测模型全面替代原始方法，直接定位和识别每一位作者的特征字符，并利用YOLO置信度分数辅助阈值判别。

Result: YOLO有效检测了更多字符样本，使第二阶段分类更加准确；YOLO置信度为后续未知文献作者的可靠筛选提供了技术基础。

Conclusion: 使用YOLO可在手稿作者识别任务中获得更高的准确率和稳定性，为历史文献考据及大规模处理提供了新突破。

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [48] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: DivAS是一种无需优化的交互式NeRF分割新方法，结合2D大模型与深度先验，实现了高效、精准的3D体素分割，极大提升了效率。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF分割方法需对每个场景单独优化，速度慢且丧失2D基础模型的零样本能力，亟需无需场景优化、高效、兼顾准确性的分割方案。

Method: 提出DivAS方法，用户通过GUI选择2D SAM生成的分割掩码，系统利用NeRF导出的深度信息优化几何精度和前后景分离。创新点在于自定义CUDA内核，将多视角掩码高效聚合为3D体素网格，实现实时反馈，全流程免优化训练。

Result: 在Mip-NeRF 360°和LLFF数据集上，DivAS分割质量与优化型方法相当，但端到端速度提升2-2.5倍，去除用户交互时间后更快一个数量级。

Conclusion: DivAS打破了NeRF分割对慢速优化的依赖，融合2D模型效率与3D几何精度，能实时高效输出高质量分割结果，极具实用前景。

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [49] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: 本论文提出了一个面向工业场景的大规模多模态推理框架，能够处理大批量制药领域的视频、PDF及音频数据，并系统评估了多种主流视觉语言模型（VLMs）在实际约束下的表现，分析了其优势与瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有VLM主要在短视频和资源无约束环境下评测，无法满足实际如制药等行业在长视频、大规模数据与严苛硬件、时延及成本限制下的需求。因此需建立可扩展的工业级多模态推理系统，并系统分析主流方法在此类场景的优势与短板。

Method: （1）开发了支持多格式视频、PDF和多语种音频的大规模工业GenAI框架；（2）在两个权威基准和自有大规模数据集（涵盖14种疾病领域）上，实证评估40余种VLM的表现；（3）系统分析多模态性、注意力机制、时序推理极限及视频切分策略等关键问题在工业约束下的表现与挑战。

Result: （1）基于SDPA注意力机制，在通用GPU上推理效率提升3-8倍；（2）多模态处理可显著提升大部分任务，尤其是对长时依赖的任务效果提升明显；（3）现有VLM在时序对齐和关键帧检测上存在明显瓶颈；（4）不同VLM在资源受限时面临显著的权衡与失败模式。

Conclusion: 论文并未提出全新模型，而是详尽刻画了当前VLM在工业领域长视频理解下的极限与挑战，并给出可行建议，有助于未来可扩展多模态系统的理论与实际设计。

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [50] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: 本文研究了面向图像输入的旋转鲁棒学习，采用了卷积模型树（CMTs），并提出了多种方法提升其在图像旋转下的性能。


<details>
  <summary>Details</summary>
Motivation: 许多图像分类或回归任务在实际应用中会遇到输入图像发生旋转，而常规模型在旋转后易性能下降。作者希望提升模型在图像旋转下的鲁棒性。

Method: 采用卷积模型树（CMTs），通过引入三种与几何结构有关的归纳偏置（卷积平滑、倾斜优势约束和基于重要性的剪枝）优化分割方向，并量化它们对旋转鲁棒性的影响。同时，提出了一种部署时的方向搜索策略，即通过选取令模型森林置信度最大的图像旋转，提升鲁棒性。

Result: 引入的几何偏置能够提升模型在旋转下的鲁棒性。方向搜索方法在严重旋转情况下有效提升鲁棒性，但靠近标准方向时，若置信度与准确性不符，可能适得其反。

Conclusion: 卷积模型树结合几何归纳偏置与置信度导向的方向搜索，能提升模型在图像旋转下的表现，但仅基于置信度的方向选择存在局限性，实际应用时需权衡。

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [51] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: 该论文发现了现有自动评价指标在文本到图像模型评估中的“原型偏见”，即自动评价指标更偏好原型化图像而非语义正确的图像；为此构建了ProtoBias基准，并提出了更加健壮且高效的ProtoScore指标。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成评价指标更多地用自动方法取代了人工判断，但这些自动方法是否真正评价语义正确性仍存疑，且有受数据偏见影响的风险，因此需要检验其评价模式。

Method: 提出了ProtoBias基准：配对语义正确但非原型化的图片和语义错误但原型化的对抗图片，通过对比让评价指标做出选择，以测试其是否更倾向于语义还是原型。系统性测试了主流自动评价指标（如CLIPScore、PickScore及VQA分数）以及LLM judge系统，并引入了人类评估作为对照。

Result: 实验结果显示，现有主流自动评价指标和部分LLM judge系统在许多情况下误判，更倾向原型图像而非语义对应图像，而人类评测则更准确。

Conclusion: 提出的新指标ProtoScore能显著降低现有评价指标的失败率和错误排名，效果接近更大的封闭源模型且推理速度大幅提升，有助于未来多模态评价的公平性和有效性。

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [52] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于卫星图像时序(SITS)数据的分割方法TEA，用以应对不同时间序列长度带来的泛化难题，并在常用数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于SITS的地块分割方法在处理不同长度时间序列时泛化能力不足，导致分割效果严重下降，因此亟需能够适应变化时间长度的分割方法。

Method: 提出TEA方法：1）采用教师-学生模型框架，由学习了全局序列知识的教师模型指导对不同长度输入自适应的学生模型；2）通过中间特征、原型和软标签多层次知识迁移，同时动态聚合学生模型缓解知识遗忘；3）引入全序列重构辅助任务以增强时序表达能力。

Result: 大量实验显示，TEA方法在不同时间长度输入的SITS分割任务上，在多个主流基准数据集上均取得了显著超越现有方法的性能提升。

Conclusion: TEA方法有效提升了SITS分割模型在时间长度变化场景下的泛化能力，为遥感时序任务的落地应用提供了更稳健的技术支持。

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [53] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 该论文提出了一种新的3D车道检测方法SparseLaneSTP，结合了车道几何结构和时序信息，通过新型空间-时间注意力机制和连续车道表示，实现了对车道的精确检测，并引入自动标注技术创建高质量3D车道数据集，实验取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D车道检测方法要么依赖密集BEV特征但会带来变换误差，要么稀疏检测却忽略了车道先验知识，同时几乎没有利用历史观测信息以应对低可见性情况。

Method: 提出SparseLaneSTP，通过稀疏Transformer网络结合车道结构几何特性和时序信息，引入车道特定空间-时间注意力机制和连续稀疏车道表示，并用简单有效的自动标注策略构造高质量3D车道数据集。

Result: 在现有3D车道检测基准和新数据集上，SparseLaneSTP在所有检测及误差指标上均取得了最优性能，实验结果验证了方法的有效性。

Conclusion: SparseLaneSTP能更好地结合几何、时序和历史信息，显著提升了3D车道检测的精度和鲁棒性，并为该领域提供了高质量的公开数据资源。

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [54] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat是一种基于3D高斯Splatting的新方法，能准确地在水下环境中重建3D几何结构，有效解决了水下成像的多视图不一致和自散射干扰。该方法通过引入三目一致性、多视图对齐和深度先验，显著提升了水下场景的重建与还原质量。


<details>
  <summary>Details</summary>
Motivation: 传统的3D重建技术在水下环境中常因光学退化（如散射、吸收等）导致多视图几何关系不一致，难以准确还原物体结构，且易出现大量伪影。因此，需要专门针对水下光学环境设计的新型3D重建方案。

Method: 作者提出了三目一致性约束，通过对每个输入相机视图进行水平和垂直平移，渲染出新视角后逆向变换对齐，获得一致的三视图。再利用这些平移视图通过三角测量产生合成的极线深度先验，作为自监督深度正则。此外，提出深度感知的alpha调整机制，在训练初期根据高斯分布的z轴坐标及视角方向动态调整透明度，从而抑制因介质产生的浮动伪影。

Result: 在真实水下和模拟场景中，OceanSplat在场景重建和还原效果上显著优于已有方法，有效减少了漂浮伪影，提升了重建的结构与清晰度。

Conclusion: OceanSplat能够将3D高斯点云与水下散射介质有效区分，绕过了传统多视图不一致的问题，为复杂水下场景提供了鲁棒的3D重建与还原方案，具有广阔应用前景。

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [55] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 本文研究了高阶对抗攻击对目标检测器的影响，发现高阶对抗补丁具有更强泛化能力，且单一对抗训练难以充分防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击和对抗训练处于“猫捉老鼠”式的不断博弈，已有对抗补丁和训练方法存在防御不足，亟需探究高阶对抗攻击对主流检测器的影响及其防御难度。

Method: 以YOLOv10作为代表目标检测器，通过迭代训练攻击模式和对抗训练，利用对抗补丁进行逃逸型攻击，系统分析高阶对抗补丁的有效性和泛化能力。

Result: 高阶对抗补丁不只对直接训练的检测器有效，对其它检测器也展现出更强的泛化攻击能力。同时，单一地采用对抗训练，无法有效防御此类高阶对抗攻击。

Conclusion: 面对高阶对抗补丁，现有对抗训练的防御能力有限，未来需发展更强健的防御机制以提升目标检测器的鲁棒性。

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [56] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: 本文提出了一种基于补丁拟合的表面表示方法PolyFit，通过在表面局部补丁上拟合jet函数实现。该方法具有良好的泛化性和高效性，并可广泛应用于计算机视觉与图形学任务。


<details>
  <summary>Details</summary>
Motivation: 传统的表面变形通常需要针对每个顶点优化自由度，计算量大且效率较低。作者希望利用更紧凑的参数化方式，从而提升面片变形的效率和适用范围，特别是在形状恢复和服装模拟等实际应用场景中。

Method: PolyFit方法基于在局部表面补丁上用jet函数拟合表面，从而得到紧凑的面片表示。该表示可以通过有监督学习从分析函数或真实数据中高效学习获得。随后利用PolyFit仅通过更新jet系数即可完成表面变形，无需逐顶点优化。本文通过两个典型应用场景（形状模板恢复与服装悬垂建模）验证了该方法的高效性和泛化能力。

Result: 在SfT任务中，PolyFit方法在保证精度的前提下，推理速度远快于传统物理仿真算法，并且比最新的物理引导神经网络模拟器精度更高，代价仅为略微增加的运行时间。对于服装悬垂任务，PolyFit训练出的模型对分辨率和服装类型均具有良好泛化性，并且推理速度比现有强基线快数量级。

Conclusion: PolyFit显著提升了表面变形任务的效率和准确性，其紧凑的参数化特征为多种计算机视觉与图形学下游任务提供了强有力的支持和广阔的应用前景。

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [57] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: 本文提出了一种将音频语言模型（ALM）和视觉语言模型（VLM）结合的视频高光片段生成框架，显著提升制药行业大规模多模态内容处理的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 制药行业处理大量多模态数据（如文本、图片、视频、音频等）时，传统人工标注方法效率低、质量参差不齐，且视频音频数据体量巨大，使内容利用更加困难。亟需智能自动化解决方案提升处理效果。

Method: 提出了视频到高光片段生成框架，集成ALM与VLM方法，并包含：1）可复现的剪切合并算法（带音画平滑过渡和时间戳标准化）；2）基于角色和提示注入的个性化机制，适用于不同场景（如市场、培训、合规）；3）具成本优势的端到端高效处理管线。

Result: 在Video MME基准测试集（900个样本）和自有包含16,159个制药领域视频数据集（涉及14种疾病）上的评估显示，该方法在处理速度上提升3-4倍，成本降低4倍，视频片段质量达到主流VLM水准。信息连贯性和丰富性评分（0.348/0.721）也优于如Gemini 2.5 Pro等SOTA基线。

Conclusion: 所提方法不仅显著提升制药多模态视频处理的效率及质量，还实现了更高的信息连贯性和内容价值，为生命科学领域的视频提取与合规支持型摘要方案提供了可行路径。

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [58] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的多模态伪标签方法，将文本和2D视觉大模型的知识直接迁移到3D LiDAR数据中，实现了无需人工标注的自动驾驶感知任务伪标签生成。方法在三个数据集上表现优良，并大幅提升了深度预测效果。


<details>
  <summary>Details</summary>
Motivation: LiDAR数据虽然丰富但缺乏标签，标注代价极高，成为自动驾驶研究中的关键瓶颈。传统伪标签方法仍需部分人工或不足以充分利用多模态信息。本研究旨在无需人工介入，最大化利用已有LiDAR和大模型知识，实现3D自动驾驶感知任务伪标签自动生成。

Method: 通过利用LiDAR序列中的时空几何一致性，将文本和2D视觉基础模型的信息融合至3D空间。提出一种依赖时序LiDAR几何的强先验、多模态输入的无监督伪标签生成方法。另外，提出联合几何-语义一致性的迭代更新规则，能自动检测动态物体。最终，无需人工输入生成3D语义标签、3D检测框及高密度LiDAR点云。

Result: 该方法在3个数据集上都能稳定泛化，优于现有伪标签和对象检测方法，甚至能带来更好的语义分割和检测效果。并且用少量本方法产生的稠密、高一致性LiDAR伪标签可使80-150米和150-250米范围的深度预测平均误差分别下降51.5%和22.0%。

Conclusion: 文中无监督基于时空几何和多模态知识整合的方法，极大降低了3D感知任务的人工标注需求，并有效提升了3D语义、检测及深度预测性能，具备现实应用推广价值。

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [59] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了一种用于新视角合成的投影条件输入方法，替代以往对照射线空间强依赖的方法，显著提升几何一致性和成像质量。通过配套的自编码预训练策略，方法在多项数据集上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 以往的前馈式新视角合成模型常用Plücker射线编码摄像机参数，但这种方法依赖于世界坐标系选择，并对微小的摄像机变化敏感，导致几何一致性和鲁棒性差。作者关注于如何更稳定且一致地为模型提供输入条件。

Method: 作者提出用目标视角下的投影线索（projective cue）替换原始相机参数，作为模型的输入，将问题转化为稳定的图像到图像的翻译任务。同时，引入了一种基于mask自编码的预训练策略，使方法能够利用大规模未校准的数据进行预训练。

Result: 通过与基于射线条件的方法对比，在作者设定的视角一致性基准上表现出更高的成像保真度和跨视角一致性。同时方法在标准的新视角合成数据集上获得了SOTA（最先进）性能。

Conclusion: 用目标视角投影条件替代相机参数，可以提升前馈新视角合成任务中的鲁棒性和几何一致性。配合专门设计的预训练，自监督和数据利用能力进一步加强，显示了在实际任务中的先进性能。

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [60] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: 本文提出Re-Align框架，通过结构化推理和奖励机制提升多模态模型在图像生成与编辑任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的多模态理解能力强，但生成任务中未能有效转移这种能力，导致用户意图难以精确反映在生成的图像中。

Method: 提出了In-Context Chain-of-Thought（IC-CoT）结构化推理范式，将语义引导与参考关联解耦。同时使用强化学习训练方案，设计代理奖励度量推理文本与生成图像之间的对齐程度。

Result: 在多项上下文内图像生成与编辑任务中，Re-Align在相似模型规模和计算资源条件下显著优于竞争方法。

Conclusion: 通过结构化推理和奖励对齐机制，Re-Align有效提升了多模态模型对于用户意图的理解与执行能力，成功缩小了理解与生成之间的鸿沟。

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [61] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: 本文提出了VERSE，一种通过探索视觉嵌入空间来分析与提升视觉-语言模型理解富视觉信息文档能力的方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在处理包含丰富视觉元素文档时表现受限，特别是在难以解释模型失误及提升模型能力上存在挑战。

Method: VERSE通过可视化模型的潜在视觉表征，使得研究者能够识别模型表现不佳的聚类区域，并引导合成数据生成，针对性提升这些区域的性能。验证方法为在合成MERIT数据集上训练，并在真实的MERIT Secret数据集上评估。

Result: VERSE可以发现导致模型易错的视觉特征，通过有针对性地补充含有这些特征的训练样本，可以显著提升F1分数且不损害模型泛化能力。优化后的本地模型在性能上与大型SaaS方案持平甚至更优。

Conclusion: VERSE有效分析与提升了视觉-语言模型在复杂文档理解任务中的表现，尤其验证了数据增强与模型可解释性的提升价值。

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [62] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter提出了一种能够在统一的4D几何世界状态下，对摄像机和多物体运动进行显式、精确控制的视频世界模型。方法结合了4D几何控制表示和自动数据生成引擎，大规模提升了数据多样性与训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型难以统一且精确地控制摄像机和多物体运动，原因在于视频动态主要发生在2D投影平面，缺乏对真实3D动态和相机运动的有效建模与控制。

Method: 作者提出4D Geometric Control表现，使用静态背景点云和每个物体的3D高斯轨迹来编码世界状态。这种方法能灵活表示物体运动轨迹及其时序概率3D占据。接着将4D控制渲染为条件，指导预训练的视频扩散模型生成高保真、视角一致的视频。另外，作者还设计了自动数据引擎，从真实视频中自动提取所需4D控制数据，大规模扩展训练集。

Result: VerseCrafter能生成高质量、动态与指定4D控制严格对应的视频，且对视点与物体动态的精确控制超越了以往基于2D或参数模型的方法。自动数据引擎提升了训练数据多样性。

Conclusion: 本工作首次实现了在视频生成任务中对摄影机和多物体运动的统一4D控制，提升了模型的灵活性和表现力，为复杂场景下更精确的视频合成和编辑奠定基础。

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [63] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级视觉-语言框架，用于作物疾病视觉问答，在准确性和生成能力上表现优异，超过主流大模型。


<details>
  <summary>Details</summary>
Motivation: 作物疾病自动诊断需求增长，现有方法在准确性、推理能力与模型体积之间存在权衡，亟需更高效、轻量且能有效处理多模态任务的方案。

Method: 采用Swin Transformer作为视觉特征编码器，结合seq2seq语言解码器，并用两阶段训练策略提升视觉表征及跨模态对齐，详测了分类与自然语言生成能力，并用Grad-CAM等方法分析可解释性。

Result: 模型在大规模作物疾病数据集上实现了高准确率，分类和自然语言指标（如BLEU, ROUGE, BERTScore）均显著优于大型基线模型，且参数更少，定性分析显示在多类型问题下表现鲁棒。

Conclusion: 面向作物疾病的视觉预训练及专用视觉-语言框架可以兼顾效率、准确与可解释性，为农业智能问答任务带来新思路。

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [64] [Atlas 2 -- Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: 本论文提出了Atlas 2、Atlas 2-B和Atlas 2-S三种病理学视觉基础模型，在80个公开基准任务上达到了最先进的预测表现、鲁棒性和资源效率，并利用迄今为止最大规模的病理学数据集（550万张全幅切片图像）进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有的病理学基础模型虽然在计算病理领域取得了进展，但在性能、鲁棒性和计算资源需求上存在权衡，影响其临床部署。因此，需要开发在多方面均衡突出的新一代模型。

Method: 作者收集了来自三大医学机构的550万张数字化全幅切片图像，构建了最大规模的病理学数据集，并训练了三个新模型（Atlas 2、2-B、2-S），随后在80个公开病理任务基准上进行了全面评估。

Result: 新提出的三种模型在80项基准测试中均显示了优异的预测性能、强鲁棒性和低计算资源消耗，优于过去的基础模型。

Conclusion: Atlas 2系列模型能兼顾性能、稳定性和资源效率，为计算病理学的临床部署迈出了重要一步，解决了以往模型的关键短板。

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [65] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的加速自回归图像生成的方法——多尺度局部推测解码（MuLo-SD），通过结合多分辨率草稿生成与空间感知的验证机制，实现了显著的解码加速，同时保持高质量的生成效果。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归模型在图像合成领域表现优异，但由于其序列生成特性，生成过程存在较大延迟。现有推测解码方法只能有限提高速度，主要受限于逐token的不确定性与对空间信息的忽略，急需能兼顾推断速度与空间一致性的高效加速方案。

Method: MuLo-SD方法首先使用低分辨率草稿模型和上采样器提出候选图像token，然后利用高分辨率目标模型并行验证这些候选token。引入局部拒绝与重采样机制，在发现草稿错误后，仅对空间邻域局部进行快速修正，避免首次拒绝后全体重采样带来的高昂计算开销。该框架内还研究了多种上采样设计、概率汇聚和邻域扩展等细节模块。

Result: 实验证明，MuLo-SD在加速自回归图像生成方面取得显著提升，速度可达现有最强推测方法如EAGLE-2与LANTERN的1.7倍，而且在语义一致性与感知质量上基本持平。以上结论在MS-COCO 5k验证集、多套评测基准与消融实验中得到验证。

Conclusion: MuLo-SD实现了图像自回归合成中的加速与质量兼得，在推测解码方向树立了新的先进水平，未来有望大规模推动高效、高保真度的视觉生成任务。

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [66] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理框架VLI（Vision-Language Introspection），无需重新训练即可有效降低多模态大模型中的幻觉现象，显著提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型普遍存在“客体幻觉（object hallucination）”问题，模型过度依赖语言先验而忽视实际视觉证据，现有方法要么修正有限，要么缺乏个性化精准引导。

Method: VLI采用二阶段策略：首先进行归因反思（Attributive Introspection），通过概率冲突探测识别潜在幻觉风险，并定位引发幻觉的视觉锚点；随后利用可解释的双因果引导（Interpretable Bi-Causal Steering），动态分离视觉证据与背景噪声，并自适应校准模型信心，从而提升推理可靠性。该方法在推理阶段直接作用，无需修改模型结构或再训练。

Result: VLI在主流高级模型上测试取得优异结果：在MMHal-Bench指标上对象幻觉率降低12.67%，在POPE上准确率提升5.8%。

Conclusion: VLI作为一种无需训练的推理级自纠机制，有效缓解了多模态大模型的幻觉问题，提升了可靠性和模型的实际适用价值。

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [67] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理框架Chain-of-View (CoV)，通过主动选择和调整视角，在无需额外训练的情况下显著提升了主流视觉-语言模型在3D环境下的空间推理和答案生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在三维场景问答（EQA）任务中通常只能处理固定数量的输入视角，导致无法充分获取与问题相关的上下文信息，尤其在复杂空间推理时受限，难以整合分散或被遮挡的信息。

Method: CoV是一个无需训练、仅在测试阶段应用的推理框架。首先，通过View Selection模块筛选出与问题相关的关键视角，并去除冗余帧。然后，模型采用粗到细的探索策略，交替进行推理和离散的摄像头操作（如视角微调），从3D场景中持续获取新信息，直到获得充分上下文或用完动作预算。

Result: 在OpenEQA数据集上，CoV对四种主流VLM取得了平均+11.56%的改进，最高提升达到+13.62%（在Qwen3-VL-Flash上）。随着最小动作数的增多，测试时还可获得额外+2.51%的平均增益。在ScanQA和SQA3D等基准上也表现卓越（如ScanQA上CIDEr 116 / EM@1 31.9，SQA3D上EM@1 51.1）。

Conclusion: 基于问题相关的主动视角选择和开放视角搜索，可作为提升3D环境下EQA空间推理能力的通用策略，对各种VLM都有效，并且无需额外训练。

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [68] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: 该论文提出了一种视频理解框架VideoAuto-R1，通过“必要时推理”策略优化链式推理（CoT）在视频大模型中的使用，在大幅提升效率的同时，实现了准确率的提升。


<details>
  <summary>Details</summary>
Motivation: 以往研究多将链式推理视为提升多模态视频理解能力的关键，但本文发现，直接回答在某些情况下效果不逊于CoT，且后者计算开销大。因此需要一种能自适应选择是否推理的机制，兼顾效率与性能。

Method: 提出VideoAuto-R1框架，训练阶段采用“思考一次，回答两次”：模型先生成初始答案，再进行推理，最终给出复核答案，二者都用可验证的奖励信号监督。推理阶段根据初始答案置信度判断是否进行推理，必要时才激活CoT。

Result: 在视频问答和定位基准测试上，VideoAuto-R1获得新的SOTA精度，并且响应长度、计算开销显著降低（平均答案长度从149降至44 tokens）。在感知类任务上推理激活率低，而在复杂推理任务上激活率高。

Conclusion: 显式语言推理对复杂任务确有帮助，但对所有任务并非必须。该方法显著提升了效率，并实现较高准确率。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [69] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: 本文介绍了AgentCompress系统，通过智能选择模型精度，有效降低大型语言模型在科研任务中的计算成本，同时保持较高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有70B参数的大型语言模型在科研任务中应用时，计算费用高昂，阻碍了学术实验室的广泛使用。针对不同任务需求不同模型计算资源的问题，作者提出了改进方法。

Method: AgentCompress使用一个小型神经网络，根据任务开头词语判断难度，并将任务路由到合适的、经过压缩的模型版本。整个决策过程在1毫秒内完成。

Result: 在生命科学、物理等四个领域500个科研流程中测试，AgentCompress在保持96.2%原任务成功率的同时，将计算成本降低了68.3%。

Conclusion: AgentCompress显著减少了LLM在科研场景的使用成本，使更多预算有限的实验室能够负担得起自动化科研任务，具有重要实际意义。

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [70] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: 本论文研究了大规模视觉-语言模型(VLMs)在目标计数任务中受到提示词诱导而产生幻觉的问题，通过对模型结构的分析，发现特定注意力头（attention heads）的剪除可显著减少该类幻觉，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务中表现优异，但它们往往更倾向于相信文本提示而非视觉证据，从而产生“幻觉”，特别是在目标数与提示词不符时。准确理解和减少这种幻觉现象对于改进VLMs的可靠性和实际应用至关重要。

Method: 作者设计了可控的目标计数实验，故意让提示词中夸大图像中的物体数目。对三种VLMs进行机制分析，通过剪除模型中部分注意力头，观察其对因提示词引发的幻觉（prompt-induced hallucinations, PIH）的影响和变化。

Result: 分析发现，仅剪除一小部分关键注意力头（PIH-heads）即可在不进行额外训练的情况下，使PIH现象减少至少40%。不同模型中介导质询复制的PIH-heads具有特定实现差异。此外，删减后模型更倾向于纠正提示词过度而依据真实视觉内容。

Conclusion: 通过关注并调控模型内部特定注意力机制，能够有效抑制VLMs提示词诱导幻觉行为，并揭示了不同模型之间在幻觉产生机制上的实现差异。这对于理解和提升多模态模型的可靠性提供了有价值的思路。

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [71] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D是一种模块，能在现有3D重建模型基础上有效提升深度边界锐利度并减少飞点（虚假点）伪影。它通过多候选深度图预测和动态加权融合来实现，集成在比如VGGT等3D重建框架下，仅需很小的计算成本就显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建模型通常在深度边界上表现不佳，容易产生飞点伪影，影响整体重建效果。作者希望通过更灵活的模块来解决深度边界模糊和伪影问题，提高模型输出的精度和可用性。

Method: 提出了MoE3D（混合专家模块），该模块会预测多种候选深度图，然后通过动态加权的方式将其融合为最终深度输出，从而最大化保留有用特征并加强深度边界。此外，该模块可以轻松集成到现有3D重建骨干（如VGGT），几乎不增加计算量。

Result: 在与VGGT等主流3D重建模型结合时，MoE3D明显改善了3D重建的边界锐利度，并有效减少了飞点伪影，且推理开销极小。

Conclusion: MoE3D能在不显著增加计算资源消耗的情况下，大幅优化3D重建模型的输出质量，尤其在深度边界和伪影方面带来明显改善，对实际3D应用具有良好价值。

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [72] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlowLet的条件生成框架，用于生成基于年龄条件的3D脑MRI，提高了脑龄预测模型在不同年龄群体中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D脑MRI数据集在人口分布上存在偏倚，获取新数据既昂贵又受伦理限制，影响了脑龄预测的公平性与泛化能力，因此有必要通过高效的生成式数据增强方法来补充不足。

Method: 作者提出FlowLet，将flow matching方法与可逆的三维小波域结合，实现了年龄可控的高效3D MRI生成，减少了重建伪影和计算负担。

Result: 实验证明，FlowLet仅需较少采样步骤即可生成高保真MRI体数据；以FlowLet生成的数据增强脑龄预测模型时，在低样本年龄段提升了性能，区域分析亦验证了解剖结构保持良好。

Conclusion: FlowLet能够有效合成高质量且具年龄可控性的3D脑MRI，为脑龄预测提供了更均衡的数据基础，有助于提升模型公平性和泛化能力。

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [73] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: 提出了ObjectForesight模型，能够通过从短暂的第一视角视频序列中直接预测刚体物体未来的6自由度姿态和轨迹，实现了基于3D对象级表示的物体动态预测。


<details>
  <summary>Details</summary>
Motivation: 人类能够轻松预见物体在交互过程中如何运动或变化，但现有计算系统缺乏类似能力。该研究旨在赋予计算模型以类似的从被动视觉观察中预测物体未来运动的能力。

Method: 该方法提出了ObjectForesight模型，以3D对象为核心建模，直接预测物体未来6-DoF的姿态与轨迹。与传统基于像素或潜在空间的方法不同，本方法在对象级显式建模。训练数据通过最新分割、网格重建和3D姿态估计技术自动构建，最终获得超过200万条带伪真值轨迹的短视频片段。

Result: ObjectForesight在准确率、几何一致性以及对全新物体和场景的泛化能力上均有显著提升。

Conclusion: 本研究建立了一套可扩展的、直接基于观察学习的、物理基础扎实的对象级动力学建模框架，为未来相关研究和应用提供坚实基础。

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [74] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: 本文提出了PlenopticDreamer框架，在多视角视频重渲染任务中实现了更一致的跨视角一致性和时空记忆。通过多种创新性机制，提升了视频一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机控制的生成式视频重渲染方法在多视角一致性方面表现不佳，难以保证在不同视角下的空间和时间的连贯性，特别是在生成模型带来的随机性影响下。作者希望解决多视角下一致性与时空记忆维护的难题。

Method: 提出了PlenopticDreamer框架，采用多入单出的视频条件自回归生成模型。其核心方法包括：相机引导的视频检索策略（从先前生成中自适应选择相关视频作为条件），递进式上下文扩展（提升训练收敛性），自条件机制（提高对视觉退化的鲁棒性），以及长视频条件机制（支持更长视频的生成）。

Result: 在Basic和Agibot基准上进行了大量实验。结果显示，PlenopticDreamer在视频重渲染任务中实现了最先进的表现，在视角同步、高保真视觉效果、准确相机控制和多样化视角变换（如第三人称到第三人称、或从头部视角到夹爪视角的机器人操作）等方面均优于现有方法。

Conclusion: PlenopticDreamer有效提升了多视角视频重渲染中的一致性与记忆能力，实现了目前最佳的跨视角视频生成表现，适用于包括机器人操作在内的多种应用场景。

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [75] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了GREx新基准和大规模数据集gRefCOCO，解决以往仅支持单目标指代表达的局限，实现一条表达可指代多个目标或无目标，并提出了关系感知基线ReLA模型，取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达分割和生成任务（REx）仅支持“一句表达描述一个目标”，忽视了真实场景中一句话可能对应多个目标甚至无目标，严重限制了应用范围。

Method: 作者扩展了REx任务，提出GREx（含GRES/SEG、GREC/COM、GREG/GEN三类），并构建了首个大规模GREx数据集gRefCOCO，包含单目标、多目标和无目标标注。为处理多目标复杂关系建模问题，提出了ReLA基线方法，通过自适应分割图像区域与显式建模区域间及区域-语言依赖解决该难题。

Result: 在GRES和GREC任务上，ReLA模型超越了现有主流方法，取得了最优效果。

Conclusion: GREx任务和gRefCOCO数据集解决了旧REx局限，为多目标和无目标表达研究提供了平台和数据支持，ReLA方法为复杂关系建模提供了有效基线，有望推动指代表达相关任务的研究与应用。

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [76] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于像素空间生成建模的新型视觉几何模型，能够生成高质量、无漂浮像素的点云，用于单目和视频深度估计，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何基础模型在点云重建中存在漂浮像素和细节丢失等问题，这限制了它们在机器人与增强现实等领域中的应用效果。

Method: 提出了Pixel-Perfect Depth(PPD)深度模型，基于像素空间扩散Transformer(DiT)。设计了两个关键创新：其一，使用语义引导的DiT，通过融合视觉基础模型的语义信息提升全局和细节表现；其二，采用级联DiT架构，逐步增加图像token数量以在效率和精度间取得平衡。对于视频扩展版本PPVD，引入了语义一致的DiT，并通过参考引导token传播确保时序一致性且计算开销较低。

Result: 该方法在单目和视频深度估计任务中达到了目前最优性能，生成的点云比其他模型更干净，无漂浮像素。

Conclusion: 该文方法显著提升了深度和点云质量，在几何基础建模方面取得前沿突破，有望推动机器人及增强现实等领域的发展。

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [77] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本论文提出了RL-AWB，一个结合统计方法与深度强化学习的夜间白平衡（AWB）算法，有效提升了低光噪声和复杂光照条件下的色彩恒常性表现。


<details>
  <summary>Details</summary>
Motivation: 夜间环境中由于低光噪声和多变照明，传统白平衡算法难以取得理想效果，因此需研发能够适应夜间复杂场景的新方法。

Method: 先用结合显著灰像素检测与新颖照明估计算法的统计方法进行初步夜间白平衡处理，再以其作为核心，设计深度强化学习框架，动态优化每幅图像的相关参数，模拟专家调优流程。

Result: 提出的RL-AWB在多传感器夜间数据集上进行实验，结果显示其对低光及良好光照图像均有优异的泛化能力，超越现有方法。

Conclusion: RL-AWB有效提升了夜间色彩恒常性，为夜间拍摄提供更准确的白平衡支持，并通过强化学习策略显著增强了算法的适应性。

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [78] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了QNeRF，这是首个结合量子和经典模型用于从二维图像实现新视点合成的方法。QNeRF利用参数化量子电路，使模型更加紧凑，并减少参数数量，对比经典NeRF模型保持或超越性能。


<details>
  <summary>Details</summary>
Motivation: 经典的NeRF在新视点合成方面取得了巨大进展，但其模型参数量大、训练开销高。量子视觉场（QVF）则展现了模型紧凑和收敛速度提升的潜力。因此，作者希望通过引入量子机制，提升新视点合成任务中的效率和性能。

Method: 作者提出了QNeRF，结合了量子和经典计算。该方法通过参数化量子电路编码空间及视角相关信息。提出两种结构：Full QNeRF（全面利用所有量子态幅值以增强表达）与Dual-Branch QNeRF（将空间与视角信息分支编码，降低复杂度并提升硬件兼容性）。

Result: 实验结果表明，在中等分辨率的图像训练下，QNeRF在参数量不到一半的情况下，性能达到或超过经典NeRF基线。

Conclusion: QNeRF展示了量子机器学习在计算机视觉连续信号表示方面的潜力，特别是在参数效率和性能之间找到了更优的平衡，为未来量子与经典混合方法在视觉领域的应用提供了有力支撑。

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [79] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出了Mesh4D模型，实现了从单目视频一键重建动态目标的完整4D网格（含3D形状和运动）。该方法利用紧凑的潜在空间与骨骼先验，准确还原时空变形，无需推理时骨骼信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频的3D+时间（4D）重建方法往往重建质量有限，或者需求高昂的监督（如骨骼信息），或效率不足。作者希望提升单目视频重建动态物体完整时空形变的准确度和速度，并减少对先验信息的依赖。

Method: 提出Mesh4D模型，核心创新在于：1）训练阶段用骨骼信息指导自动编码器，获得描述动画序列的紧凑潜在空间；2）编码器采用时空注意力机制，获得稳定的变形特征；3）在该潜在空间上训练条件扩散模型，根据初帧网格和输入视频，单步预测完整动画。推理时不再需要骨骼信息。

Result: 在多个重建与新视角合成基准上，Mesh4D在3D形状恢复和物体形变方面优于前人方法，取得了更高的准确度和表现。

Conclusion: Mesh4D证明了以骨骼先验辅助训练、无需骨骼推理即可高效准确进行4D网格重建的可行性，有望成为单目动态4D重建的新范式。

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: 本文提出了MedPI，一个用于评估大型语言模型（LLMs）在患者-医生对话中的高维度测评基准。与传统单轮QA数据集不同，MedPI在医疗流程、安全性、疗效和医患沟通等105个维度上，依据精细且与认证标准对齐的体系进行系统性评估。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM在医疗场景的对话评测主要关注单轮问答，忽视了实际医患多轮、复杂交互和临床多维需求，缺少系统化和权威对齐的综合评估工具。作者为了解决这一短板，提出了更细粒度、多层次的评测基准。

Method: MedPI包含五层结构：（1）模拟电子病历型的患者资料；（2）拥有记忆和情感的AI患者；（3）基于来诊原因与目标的任务矩阵；（4）对齐医学教育认证标准、包含105个维度的评分框架；（5）采用校准的LLM组成评委团进行分数、问题标记和溯源判据。研究针对9种主流LLM、366位AI患者，通过7,097段医患对话，使用统一“标准医生”提示词进行测评。

Result: 所有LLM在多个评测维度上表现不佳，尤其在鉴别诊断任务上最为突出。

Conclusion: MedPI可为未来LLM在医学诊断和治疗建议中的应用提供系统性指导，指出了当前主流LLM在真实多维医疗对话任务中的不足。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [81] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: 本文提出了RAGVUE框架，实现了RAG系统的可诊断、可解释和自动化评估，可精细分解和解释RAG各环节表现，弥补了现有工具的不足。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估方法往往将复杂行为压缩为单一分数，很难区分错误来源（如检索、推理、事实支撑等），导致分析和改进受限。因此，亟需一种细粒度且可解释的评估工具，以更好指导RAG系统的优化。

Method: 作者提出RAGVUE框架，从检索质量、答案相关性和完整性、严格声明级真实度、评判校准等多个维度分解RAG系统性能。每个指标配有结构化解释，使评估透明直观。框架既支持手动选择指标，也支持全自动评测，并提供Python API、CLI及Streamlit界面。

Result: 实验表明，相较主流工具（如RAGAS），RAGVUE能够揭示许多被忽视的细粒度失败案例。还展示了完整工作流程及其在科研与实际开发中的集成方法。

Conclusion: RAGVUE为RAG系统评估带来更细致、透明和可诊断的工具，有助于推动RAG模型的研究和应用。源码及使用说明已开源，可供社区广泛使用和改进。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [82] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种用于汉语动词搭配结构数据库自动构建的无监督方法，结合聚类算法从大规模语料中自动生成动词搭配结构，并用于提升语法纠错性能，取得超越LLM的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）虽在自然语言任务中表现优秀，但自身缺乏明确、可解释的规则，限制了其在需要解释性场景中的应用，尤其是在汉语动词搭配结构的研究和应用领域。为了解决这一问题，论文致力于建立可解释的动词搭配结构数据库，辅助和完善LLM。

Method: 作者将动词搭配结构形式化定义为投射、有根、有序、定向无环图。通过对从海量语料得出的句子集合，利用一系列聚类算法，为指定动词自动生成搭配结构，并对其设计特性进行统计分析。

Result: 统计结果显示，自动生成的搭配结构具有功能独立和典型性分级等特性。进一步，在动词语法纠错任务中，基于最大匹配搭配结构的算法，在性能上优于现有的LLMs。

Conclusion: 该方法能自动、无监督地构建高质量、可解释的汉语动词搭配结构库，不仅验证了结构本身的合理性，也在实际语法纠错任务中展现出实用价值，对汉语处理及可解释AI研究具有推动意义。

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [83] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）合成电商商品数据的方法，有效提升了商品信息抽取领域在数据稀缺情况下的模型表现。


<details>
  <summary>Details</summary>
Motivation: 电商商品信息抽取需求大，但高质量有标注的数据获取困难，影响模型性能。为了解决数据匮乏问题，作者希望通过合成数据来扩充数据集，提高信息抽取效果。

Method: 作者提出了三种合成策略：属性保持修改、可控负例生成、系统属性移除，并基于属性感知的LLM模板，生成满足商店约束且连贯的商品数据。通过人工评估和公开数据集实验验证效果。

Result: 合成的2000条数据中，99.6%被认为自然，96.5%属性有效，90%以上属性一致性好。在MAVE数据集上，合成数据训练模型准确率达60.5%，接近真实数据（60.8%），远超零样本（13.4%）；合成+真实数据可进一步提升至68.8%。

Conclusion: LLM生成的合成数据在电商信息抽取领域，尤其是在数据稀缺场景下，有助于提升模型性能，是实际可行的增强方案。

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [84] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: 大型语言模型在处理社区特定问题时表现不佳，导致知识盲区。本文提出“集体叙事锚定”协议，将社区故事结构化并纳入AI系统，通过三次参与式工作坊和本地信息基准测试，揭示现有系统主要错误来源，并提出设计社区治理的本地问答AI方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型常忽视社区知识，造成事实缺漏和文化误读。提升AI本地问答的准确性和公正性，减少对边缘社群的知识忽视，是本文的核心动机。

Method: 提出集体叙事锚定协议，通过三场参与式工作坊收集社区故事，将故事结构化为便于提取和验证的单元，开发具备实体、时间、地点抽取与验证能力的系统；并通过审核1.4万余条地方QA对以及自建的参与式问答集评估方法成效。

Result: 基准测试显示，现有LLM在无需外部上下文的情况下，对本地问题的正确率不到21%；76.7%的错误来自事实缺失、文化误解、地理或时间错配等。而集体叙事中常包含这些缺失信息，指向解决误差的方案路径。

Conclusion: 集体叙事协议能够填补AI本地知识缺口，提升问答系统对社区问题的准确回答能力。论文还总结了隐私、治理等设计难点，为开发可追溯、本地治理的AI问答系统提供设计参考。

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [85] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: 本文提出了TeleTables基准，专门评估大语言模型（LLM）对通信标准文件（如3GPP）中表格的知识掌握和理解能力。实验发现，现有LLM表现不佳，尤其是规模较小的模型。


<details>
  <summary>Details</summary>
Motivation: 通信行业越来越多地探索LLM的应用，但LLM在3GPP等标准文档、特别是表格信息的理解上存在显著短板，这影响技术文档的自动化理解和领域应用效率。作者发现当前对LLM理解技术标准中表格的能力缺乏系统性评估，因此设计新基准填补这一研究空白。

Method: 作者开发了TeleTables基准，通过多阶段数据生成流程，从3GPP标准文档中自动抽取表格，并结合多模态和推理型LLM自动生成和交叉验证问题，最终由人工审核，共包含500组问题-答案及表格数据。用于评估不同规模LLM在表格知识回忆与解释上的表现。

Result: 实验显示，参数量小于10B的模型难以准确回忆3GPP知识和解释复杂表格，性能有限；而大模型在表格推理任务上表现更好。这反映出小模型在预训练阶段接触通信标准较少，且推理表格的归纳能力不足。

Conclusion: TeleTables揭示出当前LLM对通信标准表格理解的不足，尤其是小模型。为提升LLM在此领域的应用价值，需针对领域任务进行专门微调和进一步优化。

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [86] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: 本文提出了FronTalk，一个面向前端代码生成的benchmark，突出多轮对话和多模态反馈的独特交互，填补了视觉信息在多轮代码生成中的研究空白。作者还提出了新的评估框架和应对遗忘问题的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有前端代码生成主要关注文本输入，很少涉及设计师常用的视觉反馈（如草图、截图等），尤其缺乏对多轮交互和多模态（文字+视觉）输入下模型行为的系统研究和评价。

Method: 作者构建了FronTalk数据集，包含100个多轮对话样本（每轮都有等价的文本和视觉指令），覆盖多个真实网站领域。提出了基于web agent的新型评估框架，通过模拟用户访问网站，系统评估代码功能与用户体验。此外，提出AceCoder方法，通过自主web agent逐步回顾和批评历史实现，有效遏制模型遗忘现象。

Result: 对20种模型的测试实验发现现有方法普遍存在严重遗忘已实现功能的问题，以及视觉反馈理解能力不足（尤其是开源VLMs）。AceCoder方法将遗忘问题几乎消除，并将性能提升了最多9.3%。

Conclusion: FronTalk为前端开发自动化和多轮多模态代码生成研究提供了基础和工具，相关方法及开源资源有助于推动该领域进一步发展和更深入的交互机制探索。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [87] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: 本文提出了一种用于扩散语言模型（DLM）的动态重掩码方法，通过自适应调整每个token的信心阈值，大幅提升生成效率同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有DLM的重掩码策略普遍依赖于单一的全局信心阈值，忽略了各token在迭代过程中的时序和空间动态，导致冗余迭代以及并行性受限。

Method: 本文提出基于每个token的时序方差（Temporal Variance）和空间偏差（Spatial Deviance）动态检测其收敛状态和相关性，进而在每一步自适应调整每个token的信心阈值，优化重掩码决策。

Result: 实验表明，所提方法在主流数据集上显著提升了DLM的运行效率，最大可实现8.9倍的速度提升，同时生成质量无明显下降。

Conclusion: 动态阈值重掩码策略可以更高效地引导DLM解码过程，在保证生成质量的前提下大幅提升并行解码效率，有望推动DLM实用化进程。

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [88] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: 本文提出了一种结合微调语言模型和RAG的AI系统，用于提高高校招生问询的响应效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 高校招生办需高效、准确地应对大量问询，既要保证速度又要保证信息质量，影响学生对学校的感知。传统自动回复系统在应对招生复杂规则时容易出错，因此需要更智能的解决方案。

Method: 采用检索增强生成（RAG）框架，并在特定于高校招生的高质量数据集上对RAG中的语言模型进行微调，同时针对生成逻辑进行了优化，权衡了响应质量与速度。

Result: 经过微调与逻辑优化的系统在理解招生领域细致规则、准确生成回复方面表现优异，能够实时提供高质量、定制化的招生问答服务。

Conclusion: 通过结合RAG的最新数据访问能力和微调后的深度领域知识，AI问答系统显著提升了高校招生答疑的效率与质量，能够满足实际招生场景的高要求。

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [89] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: 本文发现大模型在内部组织政治意识形态时，所用的低维结构与人类意识形态空间存在系统性但不完全的对齐。作者提出了一种简单高效的方法，能度量并修正这种失衡，提升模型输出与用户观点的一致性。


<details>
  <summary>Details</summary>
Motivation: 大模型在处理政治相关任务时，经常出现与人类真实意识形态不完全对齐的问题，这可能影响模型的公正性和适用性，因此需要一种轻量级的校准方法。

Method: 提出了一种线性探针，通过分析模型内部特征，计算偏置分数，并对输出层概率进行直接微调，无需重新训练模型。

Result: 该方法可有效量化和修正认知偏差，在不牺牲模型推理能力的前提下，提高输出与特定用户观点的对齐程度。

Conclusion: 这种线性探针校准法既高效又实用，有望应用于各种LLM，提升其多元化、公正性和用户适配能力。

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [90] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: 本论文提出了一种新的基于大型语言模型（LLM）的解释生成框架LEXMA，能够为AI模型的决策生成针对不同受众的自然语言解释，且无需大量人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多依赖数值特征归因，难以生成连贯、适用于多类型受众且高质量的自然语言解释。尤其在高风险消费场景下，模型决策的可解释性和对不同受众的适配性尤为重要。

Method: 提出了LEXMA框架，融合反思增强的有监督微调与双阶段群体相对策略优化（GRPO），分别优化决策正确性与受众风格需求。微调过程中不依赖人工标注的解释数据，而是利用reward信号自动优化。

Result: 在按揭审批的应用场景中，LEXMA优于其他LLM基线模型，预测性能显著提升。人类评估结果显示，专家面向的解释更加关注风险，消费者面向的解释则更清晰、可操作且更有礼貌。

Conclusion: LEXMA为商业决策AI模型的解释生成提供了低成本、高质量且系统化的解决方案，有助于大规模部署透明可信的AI系统。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [91] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: 本文提出并验证了一种本地化部署的检索增强生成（RAG）系统，用于根据医院内部成员的PubMed论文推荐科研合作伙伴，并结合了PubMedBERT和本地LLaMA3模型，实现了数据隐私与高效知识发现的兼顾。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用广泛，但医院因隐私和网络安全要求，需在本地基础设施处理敏感数据。因此，亟需一种既能有效挖掘医学知识、又可本地部署的数据分析工具。

Method: 系统结合了PubMedBERT用于医学语料嵌入生成，以及本地部署的LLaMA3模型用于生成与推荐，并基于医院成员发表在PubMed的论文实现合作伙伴推荐。采用检索增强生成（RAG）架构保证推荐的准确性和安全性。

Result: 实验表明，该系统能够在医院本地环境下有效地整合领域专用的预训练编码器与简化大模型，实现医学领域知识发现和合作推荐，既保证了数据隐私，也提升了推荐质量。

Conclusion: 本文验证了将领域专用编码器与本地大模型结合，在保障隐私前提下，有效支持生物医学知识发现和合作推荐，为医院等敏感数据环境提供了可行技术路线。

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [92] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: 本文提出了一种能自适应任务复杂度的多步骤推理框架CARD，通过预判问题难度分配思考步骤和资源，显著提升大语言模型推理准确率的同时降低运算成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步骤推理任务中，常因采用固定推理策略而忽略具体问题的难度差异，导致表现欠佳；因此，作者希望开发一种能根据任务复杂度动态调整推理过程的方法。

Method: 提出CARD框架，包括：1）采用MRCE（一个0.6B参数的Qwen模型）预测问题文本中的30个复杂性特征；2）两阶段递归求解器先根据任务画像分解为K步，再通过MRCE递归分配每步的思考预算（如1，5-9或10次思考）。

Result: 在GSM8K上，CARD在三种模型中准确率达到81.4%~89.2%，token消耗比固定分解方法减少1.88~2.40倍；在MATH-500上，准确率为75.1%~86.8%，token消耗减少1.71~5.74倍。

Conclusion: 在推理生成前预判复杂度能显著提升大模型的推理准确率与效率，CARD框架有效缓解了多步骤推理中的成本与性能难题。

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [93] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: 本文介绍了Qwerty AI系统，实现对俄语剧本的自动内容安全审查和分级，达到实际应用标准，并在相关竞赛中表现突出。


<details>
  <summary>Details</summary>
Motivation: 俄罗斯联邦436-FZ法律对于内容分级有严格要求，但人工审查剧本效率低、主观性强。媒体行业亟需自动化、可解释且处理速度快的内容安全评估工具。

Method: 系统基于微型Phi-3模型，采用4-bit量化，支持完整剧本（最大700页）分割和五类违规内容检测（暴力、色情、脏话、涉毒、恐怖元素），输出带解释的年龄分级（0+, 6+, 12+, 16+, 18+）。实现不依赖外部API，资源和时间受限（80GB VRAM,<5分钟）。部署于Yandex Cloud，使用CUDA加速。

Result: 分级准确率达80%，分段精度80-95%（依赖剧本格式）。能在2分钟内处理700页剧本，满足生产流程需求。

Conclusion: Qwerty AI在实际平台成功应用，实现了剧本内容审查自动化，为俄罗斯传媒行业提供高效、合规的解决方案，在业内竞赛中表现优异，验证方法的有效性。

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [94] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: 本文提出了TrueBrief，一个专为提升小型大语言模型（SLM）在文本摘要任务中忠实度设计的端到端框架，主要通过偏好优化方法减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大模型尽管生成文本能力强，但容易出现内容幻觉，对安全敏感领域的应用造成挑战，因此需提升模型生成内容的真实性和可靠性。

Method: TrueBrief框架核心是通过可控幻觉注入生成合成偏好数据，结合偏好优化范式训练小型语言模型，进而提升摘要生成的忠实度。

Result: 实验分析了数据质量与模型规模对偏好优化效果的影响，揭示了偏好优化在不同条件下的有效性。

Conclusion: TrueBrief在提升小型语言模型文本生成忠实度方面效果显著，为安全关键领域应用提供了有力支持。

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [95] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: 作者提出了AnimatedLLM，这是一个可视化Transformer语言模型机制的互动网页应用，有助于理解LLM的内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在自然语言处理教育中越来越重要，但缺乏展示其内部机制的教学材料，影响了理解和教学效果。

Method: 作者开发了一个完全在浏览器端运行的互动网页应用AnimatedLLM。该工具利用预先计算的开源大语言模型的追踪数据，并通过人工设计的输入进行动态可视化，分步展示Transformer模型的工作流程。

Result: 应用AnimatedLLM已上线，提供给教学和自学用途，用户可以通过该平台详细了解Transformer模型的内部机制。

Conclusion: AnimatedLLM为教学和自学者提供了理解大语言模型工作原理的有效工具，有助于促进相关知识的普及与深入。

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [96] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 本论文提出了一种自动化方法BiForget，用于更高效和高质量地构建LLM的遗忘数据集，提升模型内容删除的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘（machine unlearning）评测基准无法准确反映模型真正“遗忘范围”，亟需更严格、全面的方法来生成评估数据集。

Method: 正式区分了两个遗忘粒度：领域级和实例级，提出BiForget框架，无需外部数据生成器，而是利用目标模型本身，通过种子引导和对抗式提示生成高质量的遗忘数据集。

Result: 在多个基准上实验，BiForget在相关性、数据多样性和效率三方面优于现有方法。以Harry Potter领域为例，相关性提升约20%，多样性提升0.05，总数据量减半。

Conclusion: BiForget能实现更健壮的遗忘效果和更好的模型效用保留，为大语言模型的遗忘评估提供了更严格的基础。

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [97] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: 该论文提出了RIGOURATE框架，可以自动检索论文中支持性证据，并为每个声明分配“过度陈述”分数，以提升科学论文的严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前科学论文中存在作者为追求吸引力而夸大研究结论、缺乏严谨证据支持的问题。作者希望解决科学研究中的证据不充分和结论夸大问题。

Method: 提出了RIGOURATE双阶段多模态框架：第一阶段使用微调的reranker在论文中检索声明的支持性证据；第二阶段用微调模型为每个声明预测过度陈述分数及其理由。该框架基于1万余条ICLR和NeurIPS论文的声明-证据集，利用8种大模型打分并结合同行评议评论进行分数校准，人类评测验证其有效性。

Result: RIGOURATE在证据检索和过度陈述检测方面优于业界强基线方法，有效提升论文内容与证据的对应关系。

Conclusion: 该研究量化了证据与声明的比例原则，推动了更透明、清晰的科学交流，有助于科学论文撰写和评审的规范化。

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [98] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: 本论文通过实证研究分析了ASR模型在多种南亚方言及语言变体中进行跨语种迁移的表现，发现近亲缘关系虽有帮助，但并非唯一因素；少量方言数据微调效果接近大规模近亲语料微调。还对小语种Garhwali做了案例分析，并探讨了模型偏向及转录错误。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）系统在多方言和混合语言环境下表现欠佳，尤其是低资源方言和噪音语音，急需研究如何提升跨语种和多方言ASR效果。

Method: 通过对多种印度语方言和语言变体的自发、噪声和混合语音进行ASR跨语种迁移实验，系统比较了语言亲缘性、微调数据量等因素，并对低资源Pahari语种Garhwali和多种ASR模型进行了案例研究及误差分析。

Result: ASR模型在语言亲缘性较近时表现较好，但亲缘性并不能完全解释所有方言情境下ASR表现。实际应用中，少量目标方言微调数据可以实现与大规模近亲语言数据微调相当的识别准确率。还观察到模型存在对预训练语种的固有偏向。

Conclusion: 为提升多方言ASR性能，精细化微调目标方言（即便样本量不大）优于单纯依赖语言亲缘性。同时，ASR模型在低资源方言、非标准化语音场景仍面临显著挑战，需要进一步针对偏向性及转录错误等问题优化模型。

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [99] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG（检索增强生成）框架Disco-RAG，通过显式引入话语结构来提升LLM在复杂任务中的表现，并在多个基准上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法普遍将检索到的文本段落平铺、无结构地输入到大模型，这限制了模型对结构性线索的利用，并降低了其跨文档综合知识的能力。为解决这个问题，需要让模型能识别和利用文本间的结构关系。

Method: 提出Disco-RAG框架，通过生成段内话语树（捕捉本地层级结构）和段间修辞图（建模不同段落之间的连贯性），将这些结构联合融入生成计划蓝图，引导大模型生成答案或摘要。整个流程无须针对特定任务进行微调。

Result: 在问答与长文档摘要等基准任务上，Disco-RAG无须微调便达到了当前最优效果，显著优于现有RAG方法。

Conclusion: 话语结构在提升RAG系统表现中起着关键作用。结构化方法有助于模型更好地综合和表达分散的知识，是推动RAG发展的重要方向。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [100] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 当前大语言模型（LLMs）的安全评测方式掩盖了对特定群体的脆弱性，论文提出MiJaBench数据集揭示LLMs的安全对齐存在显著的群体差异。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLMs的安全性评估往往采用单一分数，将“身份仇恨”安全问题简化，无法反映模型对不同少数群体的防护能力差异。该做法易导致对LLMs安全性的错误认知和实践风险。

Method: 作者提出了MiJaBench——一个包含英语和葡萄牙语的对抗式测试集，涉及16个少数群体，总计44,000个提示。在12个SOTA大模型上共生成528,000组问答，并筛选出对齐测试集（MiJaBench-Align），细致分析模型对不同群体的安全表现。

Result: 实验结果显示，同一模型对不同目标群体的安全防护率可相差33%；模型规模越大，不公平性差异越突出，现有安全对齐方法难以实现真正的群体平权。

Conclusion: LLMs的安全能力不是通用的对齐，而是形成了对不同群体的层级防护。当前方法反而加剧了对某些群体的歧视，需要细粒度、群体敏感的安全对齐和评测方法。作者公开了所有数据和代码，推动群体公平对齐研究。

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [101] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: 本文提出ARREST框架，通过外部网络识别并纠正大语言模型（LLM）潜在空间中的偏移特征，提升其事实准确性与安全性，无需微调原有模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在事实准确性和安全性方面存在缺陷，且通常将两者作为独立问题处理。作者认为两种缺陷源于潜在空间的表示失调，希望给出统一的解决思路。

Method: 提出ARREST（一种对抗性训练增强的安全与真实性调节框架），利用外部网络理解并干预LLM中潜在特征漂移，通过软（soft）与硬（hard）拒绝及事实纠正调节输出安全性与准确性，不需微调原模型参数。

Result: 实验证明，ARREST不仅能有效纠正表示失调，而且比传统RLHF方法在“软拒绝”生成方面更具多样性和灵活性。

Conclusion: ARREST为提升LLM的可靠性和安全性提供了统一且便捷的手段，无需更改模型参数，在事实性和安全性调节方面效果显著。

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [102] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: 本文强调了理解神经网络内部决策机制（机制解释性）的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络的能力日益增强，但其内部运作机制和决策过程仍然难以理解。深入理解机制有重要现实意义。

Method: 从理论层面探讨了机制解释性的应用与价值，关注对神经网络决策机制的解释。

Result: 强调了机制解释性可以实现高风险领域的责任与控制、对数字大脑与认知的研究，以及当AI优于人类时获得新知识。

Conclusion: 提升对神经网络机制的解释性研究，有助于促进AI应用安全、科学与创新。

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [103] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在复杂长上下文多文档法律案件摘要任务上的有效性，并提出了Gavel-Ref评测框架和Gavel-Agent工具。结果显示，即便最强模型的表现也有限。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM支持百万级别token的上下文处理能力，但是否能胜任真实世界中复杂和庞大的任务（如法律案件跨多文档摘要）尚不明确。

Method: 1. 提出Gavel-Ref评测框架，包含26项清单式指标、剩余事实和写作风格的细致评估。2. 用Gavel-Ref系统性评测12个前沿LLM在100个32K到512K token的法律案件上的表现。3. 开发Gavel-Agent代理，以六种工具让LLM在多文档中自动提取关键信息，并测试其效率与准确率。

Result: 最强LLM（Gemini 2.5 Pro）在Gavel-Ref指标上仅得分约50，展现任务复杂性；对于简单信息模型表现好，但在复杂或罕见指标明显吃力。Gavel-Agent能在Qwen3模型下减少36% token消耗，性能下降仅7%。

Conclusion: 当前LLM在复杂、超长上下文法律摘要任务上表现有限。新的细化评测和辅助工具可提升效率，但模型在多样、复杂信息提取上的能力仍需加强。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [104] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在面对有害信念时未能挑战用户观点的原因，并提出了实用性的提升方法。通过实验表明，语言和社会因素会影响模型表现，并通过简单的语用干预提升了模型的安全性与表现。


<details>
  <summary>Details</summary>
Motivation: 动机在于，当前LLMs在医疗建议和社会推理等领域经常默许用户的有害信念，未能进行有效挑战，影响模型在安全性和实际应用中的可信度。理解产生这种现象的原因，对提升模型安全性和防止误导行为至关重要。

Method: 作者分析了影响模型默许行为的人类语用因素，如议题相关性、语言表达方式和信息源可信度，将其应用于LLMs行为分析。使用Cancer-Myth、SAGE-Eval（错误信息辨别）与ELEPHANT（阿谀奉承测试）三个安全基准对现有LLMs进行评估。同时，作者尝试通过简单的语用提示（如添加“wait a minute”）介入模型输出。

Result: 实验发现，上述语用和社会因子同样显著影响LLMs的默许行为，且解释了不同安全基准下模型表现差异。通过增加简单语用提示，有效提升了模型挑战有害信念的能力，同时维持了较低的误报率。

Conclusion: 文章指出，语用和社会语境对于评估和提升LLMs安全性非常重要。通过设计合理的语用干预手段，可以有效改善LLMs对有害观点的迎合倾向，提升其在实际应用中的可靠性和安全性。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [105] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: 本论文探讨了对话预测任务，比较了不同学习方法在预测人类对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 理解和预测真实人类的对话行为对于开发更自然的人机对话系统至关重要。目前尚不清楚‘让模型思考’以及不同奖励机制对模型对人类对话模拟能力的影响。

Method: 作者比较了两类模型：（1）是否允许模型在回答前进行‘思考’（链式推理）；（2）奖励机制是通过LLM打分（关注语义和信息完整性）还是直接最大化真实对话的对数概率。此外，将链式思考视为潜变量，对对数概率进行了下界推导。

Result: 用LLM评判器优化后能提升评判得分，但却降低了对真实人类回答的预测概率和人类评测下的‘拟人性’获胜率，且让模型‘思考’反而放大了这一问题。直接最大化对话概率则在多项评测中表现更佳。结合链式思考并以分布匹配目标训练可获得最佳综合效果。

Conclusion: ‘思考’（链式推理）只有在分布匹配的目标下，才能真正提升对话拟人性。未来该训练范式有潜力提升模型对真实人类行为的细致模拟能力。

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [106] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 该论文提出了一种新的防御机制MB-Defense，有效提升了指令微调大语言模型（LLMs）应对后门攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在NLP中的广泛应用及其对大规模数据集的依赖，模型易受到后门攻击，即攻击者通过少量数据投毒植入隐藏行为。目前关于如何防御指令微调模型后门攻击的研究较少，存在安全隐患。

Method: 作者提出MB-Defense训练框架，包括两步：（1）防御性投毒，将攻击与防御触发词融合为统一的后门特征；（2）权重恢复，通过额外训练打破该后门特征，恢复模型的正常行为。

Result: 在多个LLMs上的实验证明，MB-Defense能显著降低后门攻击成功率，同时维持模型的指令遵循能力。

Conclusion: MB-Defense是一种通用且数据高效的防御方法，能提升指令微调LLMs对未知后门攻击的鲁棒性。

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [107] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: 本论文通过实验发现，用户自述与实际行为在需要AI写作助手帮助的因素上存在巨大差异，依赖自述偏好设计系统会导致系统性能下降，行为数据更具指导意义。


<details>
  <summary>Details</summary>
Motivation: 当前智能写作助手需预测用户何时需要帮助，但缺乏关于用户偏好驱动因素的实证理解。以往系统多依赖用户自我报告设计辅助策略，可能不准确。

Method: 作者采用因子情境研究（factorial vignette study），招募50名参与者进行750对成对比较，分析用户在请求写作辅助时的偏好及真实行为影响因素。

Result: 研究发现，‘写作难度’（compositional effort）是决定用户寻求辅助的主导因素（相关系数ρ=0.597），‘紧迫性’（urgency）基本无预测能力（ρ≈0）。但用户自评时却将紧迫性排首位，表明偏好出现极度倒置。以用户自述为依据设计系统，准确率仅57.7%，反而比基线模型还低；而基于行为特征的系统准确率可达61.3%（p<0.05）。

Conclusion: 单纯依赖用户主观偏好或自述来优化写作AI助手系统，会误导系统设计，降低性能。应以用户真实行为数据为依据，为主动式自然语言生成系统设计提供新指导。

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [108] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的内存提取方法ProMem，通过循环反馈和自我提问显著提升了LLM智能体对长期交互信息的完整提取与问答准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型智能体内存管理研究多集中于摘要组织与利用，忽视了内存初始提取过程，导致信息丢失与事实核验不足。

Method: ProMem将内存提取由静态摘要转变为可迭代的认知过程，引入循环反馈机制，让智能体通过自我提问主动探查与修正对话历史中的遗漏和错误。

Result: ProMem有效提升了内存提取的完整性及问答（QA）任务的准确率，并在提取质量与Token成本之间实现了较优权衡。

Conclusion: 主动式内存迭代提取机制能够解决传统方法存在的信息遗漏和反馈不足问题，对提升大模型智能体的长期记忆管理能力具有显著成效。

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [109] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: 提出一种称为 Concept Tokens 的轻量方法，只需添加并学习一个特殊 token 的嵌入，即可在保持大语言模型（LLM）参数冻结的情况下，实现对模型行为的精细控制。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中需要控制 LLM 对特定概念的理解和生成行为，而不希望重新训练整个大模型。传统对LLM的概念控制往往需耗费大量资源或适用性有限，因此需一种简便且有效的方法。

Method: 为目标概念增加一个新 token，并用该 token 替换文本中的概念词汇，仅学习该 token 的 embedding ，其余 LLM 权重保持冻结。embedding 的学习目标是标准的语言模型目标。实验在 HotpotQA 闭卷问答、二语教学反馈和定性分析等多场景下进行。

Result: 1）在 HotpotQA 问答中，否定“幻觉”token可减少模型幻觉但增加弃答，肯定则会提升幻觉但降低精度；2）在二语教学场景中，效果类似，并且 Concept Tokens 比直接提供定义语料更好地维持模型对其他指令的遵循性；3）质性分析展现了 Concept Tokens 捕捉到的概念信息及其局限。

Conclusion: Concept Tokens 是一种高效、紧凑的控制信号，通过概念定义实现对冻结 LLM 行为的灵活引导，对实际应用中的定制化需求具有潜在价值。

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [110] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 本论文提出了一种无需语料库即可为乌拉尔语系等形态丰富语言构建形态词典的新工具SampoNLP，并利用其对BPE分词器在多种词汇量设定下进行了系统评估，提出IPS指标和最佳词汇量建议，揭示了BPE在黏着语下的局限。


<details>
  <summary>Details</summary>
Motivation: 乌拉尔语系语言（如芬兰语、匈牙利语、爱沙尼亚语）形态复杂，缺乏高质量形态词典，导致针对其分词器（如BPE）性能的评估较为困难。有效评估分词对于提升大模型质量非常关键。

Method: 作者开发了SampoNLP工具，通过基于最小描述长度（MDL）思想的自指原子性评分，在无语料的情况下自动构建形态词典。然后使用这些高纯度词典，系统评估BPE分词器在8k至256k不同词表规模下的表现。同时，提出了综合性能评分（Integrated Performance Score, IPS）作为衡量分词器“覆盖-过分割”权衡的统一指标。

Result: 通过分析IPS随词表规模变化的曲线，发现了“拐点”，并首次给出了乌拉尔语言最佳BPE词表规模的实证建议。结果还明确展示了标准BPE分词器在强黏着语言下存在的性能瓶颈。

Conclusion: SampoNLP为低资源形态复杂语言的形态学研究与工具开发提供了新方法，并为分词器词表规模选择提出了具体建议，对实际系统开发有指导意义。工具及资源已开源，促进学界进一步研究。

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [111] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一套更完善的非言语声事件本体，建立了一个高质量评测集和任务基准，并训练了专用模型在检测笑声、哭声等非言语事件上超越了现有工具。


<details>
  <summary>Details</summary>
Motivation: 尽管语音识别在语义转录上已有深入研究，但关于非言语声事件（如大笑、哭泣）的精确定位一直缺乏系统化方法和评估工具，限制了相关应用的发展。现有方法存在分类少、标注粗糙、评测不统一等问题。

Method: 作者提出区分离散型与混合型非言语事件的21类新本体，并基于此制作了带有位置感知标注的专家数据集WESR-Bench，此外还建立了大规模语料库并训练基线模型。

Result: 所训练的模型在非言语事件定位上超越了开源音频-语言模型和主流商用API，且不影响原有ASR成绩。

Conclusion: WESR基准和资源将推动复杂真实听觉场景建模，为非言语事件检测领域奠定了基础，有望带动更广泛研究与应用。

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [112] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出LinguaGame，一种基于博弈论和语言学的多智能体对话生成范式，通过策略性交流提升多智能体间的交流效率，并在模拟法庭和辩论场景中获得了显著的效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的多智能体系统侧重于架构设计，对实际交流过程本身关注较少。本文旨在提升多智能体通过自然语言传达意图和策略的效率，使交流更加有效。

Method: 提出了LinguaGame，将对话建模为意图和策略的信号博弆，通过免训练的均衡近似算法在推理时动态调整，为对话决策提供支持。该方法强调语言推理的普适性，减少对具体任务的依赖。

Result: 在模拟法庭和辩论对抗实验中，LinguaGame展现出比现有方法更高的交流效率，且经过专家评估验证。

Conclusion: LinguaGame方法通过策略性和语言学驱动的交流建模，提升了多智能体在复杂对话场景中的交流质量和效率，为基于大模型的多智能体对话系统提供了新的范式。

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [113] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习提升RAG（检索增强生成）系统可靠性的统一方法，能够同时增强证据溯源能力和‘拒答’能力，在保证准确率的同时大幅降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然能增强大模型的外部知识利用，但常出现无证据给出答案或在证据不足时捏造内容的问题。过去工作通常只关注其中一项问题，缺乏兼顾证据溯源和安全拒答的一体化方案。

Method: 作者提出GRACE框架，使用异构检索器自动生成多样化训练样本，无需人工标注。主要通过多阶段门控奖励函数训练模型，逐步引导其判断证据是否充分、提取关键支持证据、在有把握时作答，证据不足时明确拒绝作答。

Result: 在两个基准数据集上的实验表明，GRACE在全面准确率和拒答平衡性方面达到了SOTA水平，且标注成本仅为以往方法的10%。

Conclusion: GRACE作为统一的RAG增强方法，有效兼顾了证据有据可查与安全拒答，明显提升了系统可靠性和实用性，并显著降低了训练成本。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [114] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 该论文系统性地评估了主流文本水印方法在孟加拉语大型语言模型生成文本下的鲁棒性，并提出分层水印策略以提升翻译攻击下水印检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法在高资源语言下表现良好，但在低资源语言和跨语言攻击下的鲁棒性尚未充分研究，尤其是采用如往返翻译（RTT）等常见的攻击方式。需要有适用于低资源语言的、更鲁棒的水印方案。

Method: 作者对三种主流水印方法——KGW、EXP（Exponential Sampling）和Waterfall，在孟加拉语文本生成及跨语言RTT攻击下进行了系统实证评测，并提出了结合模型嵌入阶段与后生成阶段的分层水印策略。

Result: 在无攻击条件下，KGW和EXP检测准确率高于88%。但在RTT攻击后，检测准确率骤降至9-13%。分层水印方法使准确率提升到40-50%，相较单层方法提升3至4倍，代价为一定语义损耗。

Conclusion: 分层水印策略能够在控制语义退化的前提下，显著提升低资源语言（如孟加拉语）下多语种水印的鲁棒性，是一种实用且无需额外训练的解决方案。

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [115] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: 该论文提出了一种新的框架（NeuronLLM）用于理解和分析大语言模型（LLM）在多项选择题任务中的神经元作用机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一能力神经元且只识别与任务正相关的“有用”神经元，忽略了在实际任务中多能力的协调和具有抑制作用的“有害”神经元。此外，大模型偶然得到正确答案也会误导神经元归因。

Method: 作者提出NeuronLLM框架，借鉴生物学中的功能拮抗原理，将神经元分为“促进”（good）和“抑制”（bad）两类，通过对比学习全面建模。引入增强问题集以减少偶然性影响，并在不同模型和任务上验证方法有效性。

Result: 实验表明，NeuronLLM在四个NLP任务中相较现有方法有更优表现，并能更全面地揭示大型语言模型内部功能结构。

Conclusion: NeuronLLM为分析、理解和引导大语言模型提供了更细致和全面的神经元级理解方法，对推进大模型可解释性和可控性具有重要意义。

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [116] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: 本论文提出了一种基于LLM的自动化作文反馈评价框架FeedEval，能评估并筛选高质量反馈，有效提升自动作文评分系统的表现和作文修订效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动作文评分研究越来越重视反馈的质量，但因专家标注成本高，常用LLM自动生成反馈，然而这些反馈缺乏质量验证，影响下游应用的效果。需要一种机制确保用于训练和评测的反馈质量。

Method: 提出FeedEval框架，用LLM对LLM生成的作文反馈，在具体性、帮助性和有效性三个维度上进行评价。研究中还专门构建了标注数据集，并训练了针对每个维度的LLM评价器，从多个候选反馈中筛选高质量反馈用于下游任务。

Result: 在ASAP++基准测试上，FeedEval与人工评判高度一致；用FeedEval筛选的高质量反馈训练的作文评分模型表现更佳。利用小型LLM进行作文修订实验也显示FeedEval选出的高质量反馈能有效提升作文修订效果。

Conclusion: FeedEval能有效支持高质量作文反馈筛选，有助于提升自动作文评分与修订的表现。相关代码与数据集将在论文接收后公开。

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [117] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 论文提出了RL-Text2Vis，这是首个应用强化学习于文本到可视化（Text2Vis）生成任务的框架，显著提升了代码可执行性和图表质量。


<details>
  <summary>Details</summary>
Motivation: 目前自然语言转可视化的系统依赖大模型生成代码，但即使代码能运行，图表常缺乏语义对齐与可读性，且这些特性仅能在实际执行后评价。现有有监督方法无法利用这种执行后反馈来提升可视化质量。

Method: 本文基于Group Relative Policy Optimization（GRPO），提出多目标奖励机制，综合优化文本准确性、代码合法性及可视化质量，并利用执行后反馈对模型进行强化学习训练。选用Qwen2.5大模型（7B和14B参数）作实验。

Result: RL-Text2Vis在Text2Vis基准上图表质量比GPT-4o提升22%，代码执行成功率从78%提升到97%；模型显著超过零样本及有监督基线，并在跨域数据集（VIS-Eval、NVBench）上表现出良好泛化能力。

Conclusion: 引入强化学习及GRPO方法为多模态可视化生成带来结构性、显著性改进，有望推动文本转可视化系统的实际应用。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [118] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: 本文探讨了通过模型融合技术，提升大语言模型（LLM）在泰语及金融领域的多能力表现，以资源高效的方式实现通用和专用场景的融合。


<details>
  <summary>Details</summary>
Motivation: 在金融等对隐私、合规要求高的领域，机构更倾向于本地部署LLM，并希望使用兼具多领域能力的模型。然而训练单一大规模多能力模型成本高昂，多模型部署又资源浪费，因此需要新的解决方案。

Method: 作者提出使用模型融合（model merging）方法，将通用型LLM（Qwen-8B）与泰语专用（ThaiLLM-8B）及金融专用（THaLLE-CFA-8B）模型合并。分别对合并后模型在不同基准测试（O-NET、Flare-CFA、Thai-IC）中的表现进行评估。

Result: Qwen-8B与ThaiLLM-8B融合后，泰语通用能力显著提升，O-NET考试成绩优于无融合基础版。进一步将金融专用模型THaLLE-CFA-8B融入后，模型在一般语义及金融任务上的基准分数持续提升。

Conclusion: 模型融合是一种资源效率高、可行性强的多能力LLM构建方式，能帮助泰国及相关行业低成本实现多场景智能应用。

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [119] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: 本文针对在变换器模型中通过知识编辑（如ROME方法）处理多跳推理任务时出现的挑战，提出了一种改进方法，大幅提升了多跳问题的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的知识编辑方法（如ROME）能高效地更新单跳事实，但对需要多步知识链的多跳推理任务支持有限。主要问题包括：层次过晚导致信息缺失、泛化能力下降和模型过拟合等。因此，针对如何提升多跳推理下知识编辑的有效性成为亟需解决的问题。

Method: 作者系统分析了三类知识编辑在多跳推理中的失败模式，并提出了“冗余编辑”策略：即在多个层次重复进行知识编辑，从而改善模型访问中间知识表征及泛化能力。

Result: 通过实验表明，“冗余编辑”方法在2跳问题上的准确率相比单一编辑提升至少15.5个百分点（相对提升96%），但在答案专一性和语言自然度上有一定损失。

Conclusion: “冗余编辑”是提升变换器多跳推理知识编辑表现的有效简单方法，尽管其在一定程度上牺牲了答案的细致性和自然语言表达，但带来明显的准确率提升，具有实用价值。

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [120] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: 论文探讨了视觉-语言模型生成描述时，描述的具体性与长度应该区分对待，而非简单关联。通过构建数据集与实验，作者发现描述的具体性和信息密度比单纯长度更重要。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型倾向于用更长的文本提升描述具体性，但实际上，冗长的描述可能并不等于更具体或信息更丰富。因此，作者旨在明确区分描述具体性与描述长度，并评价现有模型如何在这两者间权衡。

Method: 作者首先定义了描述具体性的评判标准——相对于一组对比图片，能更准确区分目标图片的描述更具体。随后，作者构建了一个在长度上控制但信息含量可变的数据集，并通过用户实验来验证描述具体性的效果，分析了不同长度下人们对描述的偏好。

Result: 实验结果表明，参与者更喜欢更具体的描述，而不是更长但信息稀少的描述。同时发现单靠控制描述长度并不能解释具体性的差异，长度上的花费（分配）方式很关键。

Conclusion: 研究表明，针对描述性评价，应该直接优先考虑描述的具体性而非冗长度，这对VLM的优化和评估有直接指导意义。

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [121] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: 提出了Character-R1框架，利用多维奖励信号提升角色扮演智能体（RPA）的角色一致性与推理能力，在多个指标显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的角色扮演智能体主要模仿表层行为，缺乏内部认知一致性，导致复杂情形下容易出现“出戏”问题。因此，亟需有方法将更全面的角色认知信号纳入RPA训练。

Method: 设计了Character-R1框架，包括三项创新奖励机制：（1）认知聚焦奖励，对包括世界观等10个角色要素进行标签分析，引导智能体结构化内部认知；（2）参考引导奖励，通过与标准答案的重叠度作为优化锚点，提升探索和性能；（3）角色条件化奖励归一化，根据不同角色类别调整奖励分布，增强模型在多样角色下的稳健性。

Result: 大量实验证明，Character-R1在知识、记忆等多个维度明显优于现有主流RPA训练方法。

Conclusion: Character-R1框架有效提高了RPA在复杂场景下的角色一致性和表现，为未来角色扮演智能体研究提供了更可靠的奖励信号设计方向。

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [122] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过利用不同国家的社会学科教材，实现大语言模型的文化适应性微调，并以韩国教材为例构建了大规模的本地化问答数据集KCaQA。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多任务上表现突出，但其在不同语言和文化环境中的表现参差不齐，主要原因在于模型训练数据多为以英语为中心，且反映了特定文化价值观。因此，亟需发展能够适应不同文化语境的模型。

Method: 作者提出名为CuCu的自动化多智能体LLM框架，可将国家级教材内容转化为具开放性的、具有本土文化特征的问题和答案，并用该方法处理了韩国社会课程教材，生成了KCaQA数据集。

Result: KCaQA包含34,100对开放性、韩国本土社会文化相关的问答对。作者通过定量和定性分析表明，该数据集覆盖了丰富的本土文化话题，且生成的答案更贴近本地社会文化语境。

Conclusion: 文中提出的方法能够促进LLM在本地文化环境中的适应和表现，构建的KCaQA为后续开展多语言、多文化大模型微调提供了可扩展的实践路径。

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [123] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: 本文提出了一种新的机器自动增强生成文本对齐方法（MAGA），目标在于提升检测器区分机器生成文本与人类写作文本的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）生成文本的可识别性下降，滥用风险上升，现有检测器的泛化能力受限于数据集质量，单纯扩充数据源也难以解决该问题。因此，亟需更高级的生成文本增强与对齐方法，提升检测器泛化能力并测试其鲁棒性。

Method: 作者提出了MAGA方法，其流程包含从提示（prompt）构建到推理过程的全面对齐，核心在于引入了他们系统提出的基于检测器反馈的强化学习（RLDF），用于优化生成文本对齐过程，辅助生成更能提升检测器泛化能力的数据集。

Result: 基于MAGA训练集微调后的RoBERTa检测器，泛化检测AUC平均提升了4.60%；而受MAGA数据集影响，所选检测器的AUC平均下降8.13%（说明生成文本更具挑战性）。

Conclusion: MAGA方法和数据集为提升文本检测器的泛化能力提供了新手段，对未来相关检测器研究具有重要参考意义。

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [124] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文提出一种新型医学语音对话模型SpeechMedAssist，通过两阶段训练方法显著减少对医学语音数据的需求，在医学问诊中的效果和健壮性均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学问诊普遍以长文本为主，体验不佳，而语音对话更自然。但医学语音数据稀缺且直接微调低效，限制了语音语言模型（SpeechLMs）在医学场景中的应用。

Method: 作者提出了SpeechMedAssist模型，采用两阶段训练策略：第一阶段通过文本注入医学知识和能力，第二阶段用少量医学语音数据进行模态重对齐，从而降低了语音数据需求量。

Result: 在作者设计的医学问诊基准测试上，包括单轮问答和多轮模拟对话，SpeechMedAssist在多数评测中均在效果和鲁棒性上超越了所有基线方法。

Conclusion: SpeechMedAssist能够高效、低成本地实现医学语音多轮对话，有望促进语音对话系统在医学问诊场景的落地应用。

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [125] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: 提出了CRANE方法，基于神经元相关性判定语言专属神经元，通过干预实验发现神经元的语言选择性比基于激活度的方法更为精细。


<details>
  <summary>Details</summary>
Motivation: 现有多语种大模型神经元的语言能力分布缺乏深入理解，传统的激活度方法容易混淆神经元对某语言的偏好与其功能重要性，因此需要更合理的方法识别语言专属神经元。

Method: 提出CRANE分析框架，引入相关性（relevance）指标，通过神经元级别的有针对性干预（如masking）来判定神经元对特定语言的功能必要性，区别于仅依赖激活值的做法。

Result: 通过对英语、中文、越南语等多种语言任务进行实验，发现mask掉特定语言相关神经元会有选择性地影响该语言表现，同时对其他语言影响较小，显示出神经元对语言的选择性但非排他性。CRANE能更精确区分语言相关神经元。

Conclusion: CRANE方法比基于激活度的方法能更有效且更准确地识别多语言大模型中的语言专属神经元，有助于理解多语种模型的内部结构，并推进模型解释性研究。

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [126] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: 本文提出了ToolGate框架，以保障大语言模型（LLM）调用外部工具时的逻辑安全与状态可验证性。通过显式符号化的状态空间和Hoare逻辑合约，实现对工具调用的前后条件验证，从而防止非法或臆想结果污染知识状态。实验显示，ToolGate在提升可靠性和可验证性的同时保持了复杂任务的高性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与外部工具结合的系统虽在复杂推理任务上表现强大，但缺乏对工具调用的逻辑安全与可验证性保障，容易被错误或虚构信息破坏系统可信度。因此有必要提出更正式、可靠的工具调用管理机制。

Method: 作者设计了ToolGate框架，将系统中的世界状态明确建模为类型化的键值映射，每个工具调用都绑定Hoare风格的合约（含前置条件和后置条件）。只有当前状态满足前置条件时工具才能被调用，工具输出经过后置条件验证后才能更新状态，确保状态演化过程的逻辑安全。

Result: 实验表明，ToolGate能显著提升LLM工具增强系统在多步复杂推理任务中的可靠性和结果可验证性，同时维持竞品水平的性能。

Conclusion: ToolGate为AI系统集成LLM与外部工具提供了可信、可调试的基础，提升了系统在实际应用中的信任度和可维护性。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [127] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本文提出一个创新框架，利用生成式AI模型从检测、解释到干预仇恨迷因，实现面向有限数据条件的全面治理。


<details>
  <summary>Details</summary>
Motivation: 现实中对于仇恨迷因的检测、解释和干预往往是孤立进行的，且标注数据难以获取，急需低成本高适应性的内容治理方案。

Method: 采用任务导向的生成式多模态智能体，结合大模型的少样本适应能力，对不同类型的仇恨迷因实现检测、解释和事前干预。

Result: 该方法可在数据有限情况下，对各类仇恨迷因进行高效检测、解释及预防性干预，具有较强泛化能力。

Conclusion: 这是首个聚焦于低数据条件下全面仇恨迷因治理的方法框架，展示了实际部署的巨大潜力。

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [128] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: 该论文发现大型语言模型在处理韩语否定时表现较差，作者构建了专门的否定基准数据集Thunder-KoNUBench用于评估和改进模型对韩语否定的理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前针对大语言模型否定理解的评测稀缺，尤其是在韩语上，因此有必要系统性测试和提升模型跨语言（特别是韩语）下的否定理解。

Method: 1. 对韩语否定现象进行语料库分析；2. 构建Thunder-KoNUBench基准，覆盖韩语否定类型的分布；3. 评估47个大语言模型，比较不同模型规模和指令微调的效果，并测试在基准集上的微调对模型理解能力的提升。

Result: 大模型在含否定的韩语句子表现明显下降。通过在Thunder-KoNUBench上微调后，模型对否定和广泛语境的理解能力均有所提升，模型规模和微调方式对否定理解有明显影响。

Conclusion: Thunder-KoNUBench能够有效提升大语言模型的韩语否定理解能力，表明针对否定现象设计数据集和微调手段是改善多语言否定理解能力的可行办法。

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [129] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为PRISM的新型训练框架，通过结合过程奖励模型（PRM）和模型自信信号，提升无标注数据环境下大语言模型的学习效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，人类无法为高难度任务提供高质量标注，促使社区关注从无标注数据中学习。然而，当前基于自一致性（如投票或模型置信度）的方法在长期和大规模训练时存在不可靠性，亟须新的方法提升学习信号的质量。

Method: 作者提出了PRISM训练框架，引入过程奖励模型（PRM），在缺乏人工标注信息时，与模型自身置信度联合作为奖励信号，指导大模型进行自监督训练。

Result: 实验结果表明，PRM与自置信联合使用能够提升训练过程稳定性，显著改善模型在测试时的表现，同时防止模型置信度失控。

Conclusion: 结合流程奖励模型和模型自信信号的方法，为无监督环境下的LLM训练提供了一条稳定高效的新途径。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [130] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: 该论文提出了一种利用先验引导扰动优化零阶（ZO）优化的方法，有效提升了在大规模语言模型微调时的优化效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调时需要大量内存用于反向传播，尤其在模型规模增大时瓶颈突出。虽然零阶优化可只用前向传播规避反向传播的高内存消耗，但传统方法噪声大、收敛慢、性能受限。因此亟需提升零阶优化效率及收敛速度的方法。

Method: 作者提出一种易于集成的零阶优化方法：利用高斯采样动态生成指导向量，引导扰动向更有信息价值的方向，提高梯度估计的准确性。同时探索基于先验知识的贪婪扰动机制。理论分析证明该方法的梯度估计能更好地对齐真实梯度方向。

Result: 在不同规模和架构的LLM微调实验中，该方法可直接无缝替换现有优化器，带来更快收敛和更优性能。尤其在OPT-13B模型上，全部11个基准任务中均超越传统ZO方法，并在其中9个任务超越基于梯度的优化基线。

Conclusion: 先验引导扰动的零阶优化方法可有效解决LLM微调时的高内存瓶颈与收敛慢难题，为大模型资源受限场景下的高效优化提供了新方案，兼顾优化性能与效率。

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [131] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了专为越南语大语言模型（LLM）幻觉检测任务设计的大规模ViHallu数据集和挑战赛，为越南语领域提供了第一个系统的标准评测和基准。通过引入各种提示类型和分类，推动相关检测研究，并显著提高了模型检测精度，但也暴露了该领域仍有待突破的难题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生产环境中的可靠性常因“幻觉”问题受限，但目前标准化检测主要集中在英语，高低资源语言如越南语长期缺乏数据集和评测框架，因此亟需建立系统的越南语幻觉检测标准与基线。

Method: 作者构造了ViHallu数据集，包含1万个带“上下文-提示-回复”三元组样本，系统标注为无幻觉、内在幻觉、外在幻觉三类，并设计了事实型、噪声型、对抗型三种提示以全面考验模型。采用开放挑战赛形式，吸引111支队伍，测试多样检测方法。

Result: 最佳参赛系统在宏F1分数上达到84.80%，远高于基础编码器模型的32.83%。经证实，结构化指令微调和集成策略能显著提升幻觉检测表现，但内在型幻觉仍是技术难点。

Conclusion: 本研究建立了越南语LLM幻觉检测的第一个系统标准与基准，为未来越南语AI可信性研究奠定基础，但要实现全面无幻觉尚需进一步技术突破。

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [132] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文将角色身份引入角色扮演智能体（RPA）的表征，细分为参数化身份和属性化身份，并通过实验探究二者对RPA表现的影响，揭示模型在名人角色初始任务和负面社会属性刻画方面的短板。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的角色扮演智能体在角色构建上形式化不足，通常只将角色视为简单文本输入，缺少对角色身份结构的深入刻画和区分，因此有必要明确定义和分解角色身份的不同层面以提升角色还原度与互动表现。

Method: 作者提出“角色身份”概念，将其拆解为参数化身份（模型预训练获得的角色相关知识）和属性化身份（角色的性格、道德等细粒度行为特征），建立统一角色档案范式，并在相同结构下生成名人和虚拟角色，通过单轮和多轮对话评测两类身份的作用。

Result: 发现一，“Fame Fades”：名人角色因预训练知识初始表现更好，但优势迅速消失，模型更依赖对话上下文而非激活的先验知识。发现二，“Nature Remains”：模型能稳定展现一般性格特质，但在道德和社交属性极性方面表现敏感，特别是负面社会属性成为RPA复现角色的主要瓶颈。

Conclusion: 角色身份应结构化建模，名人优势有限；尤其需要提升模型对复杂、负面社会属性的刻画能力，这对于未来高保真角色构建和RPA评价具有重要指导意义。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [133] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文介绍了Qwen3-VL-Embedding和Qwen3-VL-Reranker模型系列，扩展了多模态检索能力，实现了跨文本、图片、文档图像、视频等多模态的统一高效表示。其Embedding在MMEB-V2等多项基准上取得了目前最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索需求日益增长，需要能够高精度处理文本、图片、视频等多种数据类型的统一模型，解决多模态信息无法有效对齐、表达和检索的问题。

Method: Qwen3-VL-Embedding采用多阶段训练，包括大规模对比预训练和reranking模型蒸馏，可实现更灵活的嵌入维度并支持32k大输入。Reranker则用跨编码器架构实现细粒度相关性评分。两个模型均支持30多种语言，并有2B和8B规模。

Result: Qwen3-VL-Embedding系列在主流多模态嵌入评测（如MMEB-V2）上取得SOTA表现，8B模型在MMEB-V2上达77.8分，排名第一。

Conclusion: Qwen3-VL-Embedding及Reranker在多模态检索领域展现了卓越的效果，有效提升了图文、视频文本匹配和视觉问答等任务的准确性和应用广度。

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [134] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: 本文发现当前情感和情绪自动分类器在不同性别人群中的表现不一致，尤其男性文本错误率更高，提醒实际应用需警惕性别偏差。


<details>
  <summary>Details</summary>
Motivation: 现有情感与情绪自动分类工具，大多依赖第三方标注者判断情绪，这可能掩盖了模型针对真实当事人分群（例如性别）产生的系统性偏差。因此，研究动机在于评估现有自动分类工具在不同性别人群间的可靠性与公平性。

Method: 作者采用了一个独特的大型自我标注文本数据集（逾一百万份发帖），并结合预注册研究设计。分析覆盖了414组模型和情绪相关类别，比较不同性别文本的情感检测误差表现。

Result: 无论情感模型类型如何、情绪类别为何，男性文本的分类错误率始终显著高于女性。这种偏差在所有模型和类别中普遍存在。作者还量化了这一偏差对下游应用的潜在负面影响。

Conclusion: 当前主流的情感分析工具，尤其包括大语言模型，若在样本性别构成未知或变化的情形下应用，需格外谨慎。情感分析在实现各群体公平性方面仍面临挑战，尚未彻底解决。

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [135] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 该论文提出了一种用于多模态大模型（MLLMs）多轮对话安全对齐的新方法，包括一个新构建的大规模多模态对话安全数据集InterSafe-V以及一个结合冷启动拒绝和分组相对策略优化（GRPO）的AM$^3$Safety训练框架。在实际多轮多模态安全评测中显著提升了模型的安全性和有用性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型常被用于交互式应用，但多轮多模态场景下，模型更容易被逐步诱导生成有害内容，而现有的基于人类反馈的强化学习（RLHF）对齐方法主要针对单轮视觉问答（VQA），且受限于昂贵的人工标注，难以推广到复杂多轮对话。该研究旨在解决多模态多轮安全对齐的有效性和可扩展性问题。

Method: 1. 构建了一个开放源代码的多模态安全对话数据集InterSafe-V（含11,270个对话、500条精心设计的拒绝式VQA）；2. 提出AM$^3$Safety框架，包含冷启动拒绝阶段和使用分组相对策略优化（GRPO）的多轮/双目标奖励联合微调，重点在整个对话中跟踪与安全、拒绝相关的信号。

Result: 在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B两大主流模型上，AM$^3$Safety框架使攻击成功率（ASR）降低10%以上，模型在安全性与有用性两个维度分别提高至少8%和13%，且总体能力未受损。

Conclusion: InterSafe-V和AM$^3$Safety大幅提升了多模态大模型在多轮对话场景下的安全性与有用性，为实际部署场景下的大规模安全对齐提供了可扩展的新方法和数据基础。

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [136] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: 本论文提出了一种新的自动化框架，用于生成具有领域相关性和隐含性的有害提示数据集，以提升大模型安全性研究的现实性和有效性。


<details>
  <summary>Details</summary>
Motivation: 目前领域特定的有害提示数据集稀缺，现有数据主要聚焦于易被检测的显式有害提示，缺少更加隐蔽、反映实际威胁的隐式有害提示。缺乏高质量、自动化的隐式有害提示数据集，限制了LLM安全防御机制的进步。

Method: 提出了端到端框架，首先利用知识图谱引导生成与专业领域相关的有害提示，然后通过“双路径混淆重写”方法，将显式有害提示转化为隐式有害提示（包括直接重写和加入上下文的重写），系统性地产生高质量的数据。

Result: 该框架能够自动生成兼具强领域相关性和隐含性特点的有害提示数据集，显著提升了数据的真实威胁性和多样性，可用于更有效的模型红队测试和安全评估。作者已开源相关代码与数据集。

Conclusion: 提出的方法显著提升了生成现实世界隐式有害提示的能力，有助于推动大语言模型领域安全性研究的发展，为模型防御真实威胁提供了更加坚实的数据基础。

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [137] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文提出了Tool-MAD，这是一种多智能体辩论框架，使每个智能体均可利用不同的外部工具（如搜索API、RAG模块），从而提升事实性验证的能力，并有效抑制大模型幻觉。实验显示其在四个事实验证基准上均优于SOTA方案。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论（MAD）系统在事实验证中，主要依赖内部知识或静态文档，容易出现幻觉与事实不准确。尽管MADKE引入了外部证据，但其仅一次性检索，无法适应辩论中新涌现的信息，局限性明显。

Method: 提出Tool-MAD框架：1）让每个智能体拥有异构外部工具，鼓励观点多样性；2）引入自适应查询机制，根据辩论进程动态优化证据检索；3）将事实性与答案相关性打分整合入判别流程，实现量化评估与幻觉检测。

Result: Tool-MAD在四个事实核查基准数据集上，实现了最高5.5%的准确率提升。并且在医疗专业等特定领域测试中，表现出极好的稳健性与多样工具配置下的适应性。

Conclusion: Tool-MAD展示了跨工具、多角度事实验证、多回合检索与评判融合的优越性，显著提升了多智能体辩论系统的事实准确率，对实际领域的事实核查具有广泛应用潜力。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [138] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: 本文提出了PILOT-Bench，这是针对美国专利审判和上诉委员会案件的首个结构化基准，评估大型语言模型在专利法律推理中的表现。结果显示商用模型表现优于开源模型，存在较大差距。


<details>
  <summary>Details</summary>
Motivation: 专利法律推理结合了技术与法律知识，而现有LLM在该领域仅从事简单任务，缺乏系统性的评估工具。这阻碍了LLM在专利法律推理领域的深入应用。

Method: 作者构建了PILOT-Bench基准，将PTAB案例与USPTO专利数据对齐，设置了三类IRAC结构的分类任务，并评估了多种闭源与开源LLM在不同任务和条件下的表现。

Result: 在“问题类型”分类任务上，闭源模型微平均F1分数超过0.75，表现优于最佳开源模型（Qwen-8B为0.56）；全面对比显示二者在专利法律推理上的显著差距。

Conclusion: PILOT-Bench为专利法律推理领域LLM系统性评测奠定了基础，后续需要通过更好的数据集和模型对齐方法提升开源模型的推理能力。

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [139] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 本文探究了大规模语言模型（如DeepSeek-V3）内部层对句法和语义信息的编码方式，通过句子表示的向量平均法，发现可明确分离和观测句法/语义信息的线性编码特征。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的能力提升，理解其内部对语言信息（如句法和语义）是如何编码的，对于模型可解释性和后续任务十分重要，目前该领域研究还不充分，特别是在超大模型上的表现。

Method: 作者利用了对具有相同句法结构或语义的句子表示向量进行平均，生成句法或语义“中心（centroid）”向量。通过将句子表示减去该中心向量，考察句法/语义信息变化及其对匹配句子相似度的影响，进一步分析跨层信息差异和可解耦性。

Result: 实验证明通过上述方法获得的中心向量显著捕获了句法和语义信息，且这些信息在向量空间中表现出部分线性可分性。句法及语义的跨层编码特性不同，两者信号可以一定程度上被独立分离。

Conclusion: DeepSeek-V3等大语言模型内部表征对句法和语义信息存在部分线性编码，且两类信息的层间分布和解耦性为模型解释与改进提供了理论基础。

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [140] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文提出了一种基于KL散度的Judge Decoding方法，无需昂贵的监督训练，能够高效验证大模型推理结果，并在多项基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的Speculative Decoding在加速LLM推理时对结果的严格验证造成效率瓶颈，现有Judge Decoding虽可放宽验证要求，但需昂贵、易受噪声影响的监督训练。作者希望找到无需监督的新方法，同时保持性能。

Method: 作者从理论角度分析Judge Decoding，发现现有判决器的“关键性”分数其实体现在draft（草稿）与target（目标）分布的KL散度，可以直接用KL散度实现验证。于是提出了一种无需训练、以KL散度为基础的判别机制。

Result: 基于KL散度的Judge Decoding在多个推理和代码生成基准上表现优异，无监督情况下效果与或超过了复杂、需训练的AutoJudge等方法。

Conclusion: KL散度提供了一种简单、健壮且高效的Judge Decoding方案，既消除了监督训练的需求，又增强了鲁棒性，对LLM加速极具应用价值。

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [141] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种名为LANGSAE EDITING的新方法，通过去除向量中的语言身份信号，提升多语言语义密集检索的表现，对原有检索模型无侵入性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多语言检索通常在混合语料库中进行，但多语言嵌入不仅编码语义，还编码语言身份信息，这会导致同语对之间的相似度被高估，从而减弱对其他语言相关证据的检索能力。

Method: 提出LANGSAE EDITING方法：将多语言嵌入输入一个稀疏自动编码器，通过跨语言激活统计检测出与语言身份相关的潜在单元，在推理阶段抑制这些单元，并重建原始维度的嵌入，无需重新训练基础编码器或重新编码文本，兼容现有向量数据库。

Result: 在多语言检索实验中，通过去除语言信号后，实现了排名质量与跨语言覆盖率的持续提升，特别是在文字脚本差异较大的语言之间效果显著。

Conclusion: 该方法能在不改变底层编码器和数据的前提下，显著提升多语言语义检索系统对于跨语言证据的检出能力，适用于现有向量检索系统。

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [142] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大语言模型（LLM）的自动化工具NC2C，用于将通用非凸优化问题自动转化为可求解的凸优化问题，大幅提升了自动化和效率。


<details>
  <summary>Details</summary>
Motivation: 非凸优化问题广泛存在于各类科学与工程领域，但其复杂性使得传统求解器难以处理，且现有凸化方法依赖大量专家知识和手工操作，效率低下。

Method: 作者提出了名为NC2C的端到端自动化框架，利用大语言模型的数学推理能力，自动识别非凸部分、选择最佳凸化策略，并生成数学严谨的凸等价问题，同时引入了符号推理、自适应转换、迭代校验及纠错机制，确保变换后问题的鲁棒性及有效性。

Result: 在包含100个通用非凸优化问题的数据集上，NC2C实现了89.3%的自动执行率和76%的高质量可行凸化成功率，远超现有基线方法。

Conclusion: NC2C显著提升了非凸优化向凸优化的自动化和高质量转化能力，减少专家依赖，拓展了成熟凸优化器的适用范围，为难以处理的优化任务带来了更高效、可规模化的求解新途径。

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [143] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 该论文系统性分析了多智能体系统中基于角色的权威偏见，发现不同权威类型对多轮对话有不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统常通过设定权威角色提升效果，但权威偏见对智能体交互的具体影响尚缺乏系统研究。

Method: 基于French和Raven的权力理论，将权威角色分为合法、参照和专家三类。使用ChatEval框架，结合GPT-4o和DeepSeek R1，分析各种权威角色对12轮多智能体对话的影响。

Result: 参照与专家权威类型比合法权威类型更能影响多智能体对话。该影响并非来自普通智能体的趋同，而是权威角色坚持立场且普通智能体表现更灵活。同时，必须有明确立场表述才能产生权威影响，中立回复则不会带来偏见。

Conclusion: 论文揭示和量化了多智能体系统设定权威角色时权威偏见的生成机制，为未来不对称交互模式下的多智能体框架设计提供指导。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [144] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 本文发现AI生成文本在生成过程中后期阶段的波动性显著降低，据此提出了两种新特征，用于更高效地区分AI文本和人工文本。所提方法在多个基准上取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本AI文本检测方法忽略了自回归生成文本的时序动态，仅关注整体统计特征。为提升检测准确性，有必要挖掘生成过程中不同阶段的区分特征。

Method: 作者分析了超过12万条文本样本，揭示了“后期波动性衰减”现象——AI生成文本在生成后半段log概率波动迅速稳定，而人类文本保持较高变异性。基于此，提出了仅基于后期阶段的两种新特征：导数离散度和局部波动，以判别AI生成文本。

Result: 所提方法无需扰动采样或额外模型访问，在EvoBench、MAGE等基准测试上实现了最新的检测效果，并能与已有方法形成互补，进一步提升检测性能。

Conclusion: 通过关注AI生成文本后期波动性减少的动态特征，可更有效识别AI文本。所提特征简单高效，为AI文本检测提供了新的研究和应用方向。

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [145] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出了一种名为 RAAR 的跨领域谣言检测新框架，通过多视角信息检索和多智能体协同推理，显著提升了在多领域环境下的检测效果，优于现有主流方法和大语言模型方案。


<details>
  <summary>Details</summary>
Motivation: 现有跨领域谣言检测方法多依赖单一视角，难以推广至知识与话语模式差异较大的领域；大语言模型虽强大，但只适用于分布相似的数据。论文试图解决跨领域泛化能力弱和推理视角单一的问题。

Method: RAAR 框架通过检索与目标样本语义、情感和文风对齐的多视角源域证据，实现跨领域迁移，并引入多智能体合作，构建可验证的多步推理路径。各智能体分别负责不同视角的分析，最终由汇总智能体整合并由验证智能体指导。再辅以监督微调和强化学习训练多任务验证器。

Result: 在三个跨领域谣言检测任务中，RAAR 框架显著增强了基础模型的能力，超越了其他跨领域、先进 LLM、和自适应 LLM 方案。

Conclusion: RAAR 能在跨领域谣言检测中有效结合多视角信息和系统推理，有较强的泛化能力和实际效果，推动领域适应和多智能体协作的发展。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [146] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: 本文提出了一种新的自回归语言生成方式，将离散token生成过程改为连续表示的渐进成熟过程，再离散化，从而提升文本生成的稳定性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型每步都需生成确定token，通过采样解决不确定性，容易导致生成内容不稳定、重复或依赖解码技巧。为了解决这些问题，作者提出能在连续空间中维持和处理不确定性的方法。

Method: 提出连续自回归模型，每个token以连续向量形式表示，通过若干步确定性动态过程逐渐“成熟”，当收敛后再离散化为具体token，整个过程无需token级随机采样。可选地，可引入随机动态或历史平滑等扰动，无需额外辅助稳定机制。

Result: 该连续成熟方法仅用确定性解码（如argmax）即可生成连贯多样的文本，展现出无需token采样或扩散去噪也能保证输出稳定。

Conclusion: 这项工作首次提出通过连续token演化再离散化的自回归语言建模方法，实现了生成的稳定性和多样性，是对现有语言模型生成范式的重要补充。

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [147] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文提出了MisSpans，这是首个用于句内维度的虚假信息检测与分析的人类标注基准数据集，支持精细化定位和分析虚假细节。作者通过实验评估了多种大模型在该任务中的表现，凸显了精细维度虚假信息检测的难度。


<details>
  <summary>Details</summary>
Motivation: 当前虚假信息检测大都只在整句或段落层面，用粗略的二元标签进行判定，无法反映单句中真假细节的并存，也难以解释哪些片段存在误导或细节失实。缺乏精细化、可解释的分析工具严重制约了虚假信息检测的实际效用。

Method: 作者构建了MisSpans数据集，涵盖多个领域，包括真实和虚假新闻配对，人工标注句内虚假细节。定义了三项任务：精确定位虚假片段（MisSpansIdentity），分类虚假片段类型（MisSpansType），以及基于片段的解释（MisSpansExplanation）。所有标注者遵循统一标准，确保标注一致。实验评估了15个代表性大语言模型在零样本和单样本环境下的表现。

Result: 实验显示，无论是带推理能力还是无推理能力模型，在精细维度虚假信息检测任务中效果均有限，反映了此任务的复杂性。模型性能受模型规模、推理能力及文本领域特性的相互影响。

Conclusion: MisSpans推动了句内维度、类型细分和可解释性的虚假信息检测研究，是相关领域重要资源。现有模型在这一精细任务上仍有巨大提升空间，后续需关注多因素对模型性能的影响。

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [148] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG（retrieval-augmented generation）方法ToPG，将命题、实体和文段建模为异质图，实现结构化检索与事实细粒度的结合，提升了对复杂和简单问答任务的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在简单事实检索上表现良好，但面对复杂多跳推理任务时容易失败；而基于知识图谱的RAG虽然适合复杂问题，却常常在单跳事实检索上表现不佳。需要一种在两类任务之间都能良好平衡的新方法。

Method: ToPG将知识库建模为由命题、实体和文段组成的异质图，以Suggestion-Selection（建议-选择）循环实现：Suggestion阶段根据查询遍历图谱，Selection阶段用LLM反馈删除无关命题、并为下个循环提供种子，从而实现高效的结构化信息检索和推理。

Result: 在简答、复杂和抽象三类问答任务上，ToPG在准确性和答案质量两个维度均表现优异，兼顾了多样化任务的需求。

Conclusion: 将面向查询的图遍历与事实层级的细粒度结合，是实现高效结构化RAG系统的关键。ToPG展现了在结构化检索和多任务环境下的强大能力。

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [149] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: 本文提出EvolSQL，一种结构感知的数据合成框架，通过演化SQL查询生成多样化且复杂的数据集，有效提升Text-to-SQL模型表现。


<details>
  <summary>Details</summary>
Motivation: 高质量、结构多样的Text-to-SQL数据集稀缺，现有方法依赖有限的人类标注或对大模型直接提示生成，导致结构复杂性和多样性不足。

Method: EvolSQL从种子数据出发，分阶段演化SQL查询：首先通过扩展问题和拓展数据库表覆盖面，增加问句和模式多样性；然后利用基于SQL抽象语法树的六种原子转化操作，递进式提升SQL在关系、谓词、聚合和嵌套等维度上的复杂性；引入执行检验和模式感知去重，保证数据质量和结构多样性。

Result: 实验显示，仅用1/18数据量，由EvolSQL生成数据微调7B模型，性能超过使用大规模SynSQL数据集训练的模型。

Conclusion: 结构感知的数据合成和演化能够有效提升Text-to-SQL模型表现，对推动相关领域具有实用意义。

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [150] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report提出了一种无需训练的认知型深度研究智能体，能够从海量嘈杂网页信息合成高质量商业报告，并在专门设计的QRC-Eval任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前自动合成商业报告的方法虽有进展，但生成结果在质量、可靠性和覆盖度上仍有限。高质量的商业报告对重大决策具有重要价值，因此亟需更先进的方法。

Method: 提出Mind2Report智能体，模拟商业分析师的思维流程，细化用户意图、动态检索并整合信息，最终合成专家级报告。该系统基于大语言模型，具有动态记忆机制且无需专门训练。构建QRC-Eval数据集进行系统性评测。

Result: Mind2Report在200个真实商业任务上显著优于OpenAI和Gemini等主流深度研究智能体，质量、可靠性和覆盖度更高。

Conclusion: Mind2Report为未来商业深度研究智能体设计打下基础，展示了认知型无监督智能体的极大潜力。

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [151] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLM）在多元文化环境下容易产生“均值坍缩”问题，提出了CuMA框架，有效提升模型对多种文化价值观的表现能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务全球用户，单一的对齐标准无法适应多元文化需求。现有模型在面对冲突价值观时难以兼顾多元群体，研究旨在解决模型对多样文化分布的表达能力不足问题。

Method: 作者提出CuMA（Cultural Mixture of Adapters），通过人口统计感知路由，将对齐过程视为条件容量分离问题，引入潜在文化拓扑，将冲突的梯度分离到专门的专家子空间，增强对不同文化模式的表达。

Result: 在WorldValuesBench、Community Alignment和PRISM等数据集上，CuMA框架实现了业界领先性能，显著优于稠密模型和仅基于语义的MoE方法。实验证明CuMA有效缓解均值坍缩，提升文化多样性表达。

Conclusion: CuMA为LLM提供了更具包容性的对齐框架，解决了多元文化背景下的表现退化问题，为全球化大模型开发提供了新方向。

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [152] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 本文提出了一种能显式处理不同观点冲突的摘要生成方法，通过将信仰级别的聚合与语言生成过程分离来提升意见类文档摘要的真实度和全面性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的意见和多文档摘要系统（尤其是大模型驱动的系统）在生成过程中倾向于掩盖或弱化文档间的分歧，使得摘要更多反映主流观点，而忽略少数或冲突意见，影响了摘要的忠实性和全面性。

Method: 该方法分为两步：首先使用结构化的信任集合表示原始文档，通过距离为基础的信仰合并算子对信念进行聚合，显式建模冲突；其次，仅在归纳融合后的信念集合时调用大语言模型，将其转化为自然语言摘要。

Result: 实验在多种大语言模型架构和规模下验证。结果表明，如果在生成时进行聚合，只有非常大的模型才能与信仰级别聚合相媲美，且这种能力在不同架构/容量间不稳定。而显式信仰聚合结合简单提示式生成，则在各种模型下都能稳定实现善于反映分歧、流畅且有理有据的总结。

Conclusion: 信仰级别的聚合与语言生成分离，有助于生成更真实、能反映多元意见分歧的摘要，且不依赖于超大模型，实现模型间的一致性和总结质量的提升。

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [153] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: 本文提出了V-FAT基准和视觉鲁棒性评分（VRS），系统分析多模态大模型在视觉推理任务中受到文本偏置的问题，并发现主流模型在传统基准下表现优异，但在高文本主导情境下无法保持真正的视觉理解。


<details>
  <summary>Details</summary>
Motivation: 在多模态大模型取得广泛成功的同时，越来越多的证据显示它们过度依赖语言线索而非真实视觉信息，从而导致了模式化的文本偏置问题。研究动因在于揭示和量化这一“文本偏置”的本质及其带来的实际影响。

Method: 将文本偏置划分为内部语料偏置（预训练统计相关引发）和外部指令偏置（对齐训练引发趋同行为），并提出了V-FAT诊断基准，创建包含4026个VQA样本、覆盖六类语义领域的三层评价框架。通过层次性制造视觉与文本信息冲突以测试模型鲁棒性，并设计视觉鲁棒性评分（VRS）评估真实视觉能力。

Result: 对12个代表性多模态大模型的评测显示，虽然这些模型在常规视觉推理任务上表现出色，但在面对高度文本主导或视觉—文本冲突的情形时，模型视觉理解能力显著下降，发生“视觉崩塌”。

Conclusion: 现有多模态大模型对文本偏置异常敏感，在强文本主导场景下难以维持视觉真实性。因此，提升模型的视觉鲁棒性和真实视觉能力是未来亟需关注的问题。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [154] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 本文关注于大型语言模型（LLMs）生成的有说服力文本与人工文本在自动检测方面的差异，并提出了多语种基准Persuaficial，评估LLM文本检测难度及其语言特征。


<details>
  <summary>Details</summary>
Motivation: 由于LLM能够生成具高说服力的文本，可能被滥用于宣传或操控等有害用途，因此必须研究LLM生成内容与人类文本在检测上的区别和挑战。

Method: 将生成有说服力内容的方法进行分类，并构建覆盖英语、德语、波兰语、意大利语、法语和俄语的多语种高质量数据集Persuaficial。利用该基准，对比分析了人工与LLM生成文本的检测难度，并进行了全面语言特征分析。

Result: 结果发现，明显的LLM说服性文本比人工更易于检测，但若说服方式较为隐蔽，LLM文本则会显著削弱自动检测的准确性。此外，基于语言特征的首次详细对比分析揭示了文本生成模式上的显著差异。

Conclusion: 隐蔽说服性的LLM文本大大增加了自动检测系统的挑战，研究成果可为后续更强鲁棒性和可解释性的检测工具开发提供理论基础和实证数据支持。

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [155] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型生成内容时溯源真实性的新任务和新数据集，并构建了相应方法，有效提升了答案及溯源准确性，同时揭示了模型在推理能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型经常出现“幻觉”（虚假生成），即使有引用来源，用户也难以验证引用内容与回答的关联性。现有溯源方法过于粗略，无法细致区分直接引用和复杂推理，缺乏对溯源细粒度结构性的支持。

Method: 提出Generation-time Fine-grained Provenance任务，要求模型生成流畅回答的同时，输出结构化的句级溯源三元组。提出并构建ReFInE（含专家标注的区分引用、压缩与推理的数据集）；基于ReFInE数据集，提出GenProve方法，结合监督微调（SFT）与Group Relative Policy Optimization（GRPO）优化答案与溯源的复合奖励指标。

Result: GenProve方法在联合评估中显著优于14个强力大模型。实验还发现，现有模型虽然在表层引用溯源上表现良好，但在基于推理的溯源（Inference-based Provenance）上存在明显短板。

Conclusion: 提升溯源真实性需要细粒度与结构化支持，仅靠引用无法实现真正的可验证推理。深层推理的可验证性仍是大模型发展的前沿挑战。

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [156] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了一种统一的具备情感智能的口语语言模型，并引入了名为IEAT的创新数据构建策略，实现了情感感知的内化建模。实验中该模型在相关基准任务上表现领先。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话系统在情感识别和理解方面存在局限，情感推理通常作为显式标签而非内在推理，本研究旨在实现情感智能真正融入模型内在推理过程。

Method: 1）提出IEAT策略，将用户情绪及其成因注入模型推理过程，实现情感内化建模。2）采用两阶段训练法，第一阶段通过自蒸馏实现语音-文本对齐及情感属性建模，第二阶段端到端跨模态联合优化，确保文本和口语表达中的情感一致性。

Result: 在HumDial情感智能基准任务上，所提方法在情感轨迹建模、情感推理和共情响应生成等方面取得领先，无论是大语言模型还是人类评测均位列前茅。

Conclusion: IEAT及两阶段训练方案显著提升了口语对话系统的情感智能能力，实现了情感推理的内化，为通用多模态情感对话系统设计提供了新思路。

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [157] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLMs）中的个性化问题，提出用自然语言作为用户偏好的通用表示方法，以替代难以解释和迁移的黑盒用户向量。作者设计了两阶段训练框架，并开发了AlignXplore+模型，在多个任务和模型间表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化方法大多采用不可解释、难以迁移的用户向量表示，导致用户偏好描述不透明。作者希望找到一种更通用、可解释并易迁移的用户偏好表示方法。

Method: 提出用自然语言描述用户偏好，并设计了两阶段训练框架，首先通过高质量合成数据进行有监督微调，然后用强化学习优化长期效用和跨任务迁移能力。基于该框架，开发了AlignXplore+模型，能自动生成文本化的用户偏好总结。

Result: 在九个基准任务上，8B参数规模的AlignXplore+模型实现了超越更大规模开源模型的效果，展现了出色的跨任务、跨模型家族和交互格式的迁移能力。

Conclusion: 用自然语言表达用户偏好为LLMs提供了通用、可解释的个性化接口。所提出的AlignXplore+方法在提升模型性能和可迁移性方面具有显著优势，推动了个性化和用户建模的发展。

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [158] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 本论文研究了在大型语言模型的链式推理(SFT-CoT)训练中，保留包含错误最终答案的负示例能带来显著的泛化提升；作者提出了一种新的损失加权方法GLOW进一步利用这些数据，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维监督微调通常只采用最终答案正确的示例，忽略大量携带有用中间推理过程的信息的负示例，导致过拟合与泛化性能受限。作者希望通过探索负示例带来的影响和新型训练策略提升模型泛化能力。

Method: 作者系统分析并引入负示例参与SFT训练，归纳总结出22种常见负链路模式，并观察其在减缓训练过拟合和提高推理多样性上的正面作用。基于此，提出了基于增益的损失加权（GLOW）方法，根据各样本轮次进展自适应调整损失权重，充分利用未过滤的训练轨迹。

Result: 实验证明，混合负示例比仅用正示例有更好的泛化提升（OOD gain）。采用GLOW方法，对Qwen2.5-7B模型实现了5.51%的OOD提升，并将MMLU从72.82%提升至76.47%作为RL初始化。

Conclusion: 负示例包含丰富有效信息，合理利用并配合自适应损失加权机制可有效提升大模型的泛化能力和推理性能，为链式推理模型训练提供了新范式。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [159] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体系统Subcultural Alignment Solver（SAS），用于提升大语言模型在亚文化群体中识别自毁性行为的能力。通过自动检索和亚文化对齐，显著提高了检测效果，并优于现有多智能体框架。


<details>
  <summary>Details</summary>
Motivation: 自毁性行为诊断困难，且在不同亚文化群体下表现方式独特，难以被主流方法识别。现有LLM方法在亚文化语境下面临知识时滞和语义不匹配两大困境。

Method: 提出了SAS多智能体框架，结合自动检索机制和亚文化语境对齐技术，提升LLM对新兴亚文化俚语与表达方式的理解。

Result: 实验结果表明，SAS比先进多智能体框架OWL表现更优，且与专门微调的LLM具有竞争力。

Conclusion: SAS有效提升了自毁行为在亚文化群体中的检测能力，为后续相关研究奠定了基础。

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [160] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: 通过研究大模型的推理蒸馏方法，发现学生模型难以获得与人类认知代价一致的推理能力，并且有出现效能退化的“负迁移”现象。


<details>
  <summary>Details</summary>
Motivation: 近年来通过强化学习训练的大型推理模型被发现其推理行为与人类认知代价自然对齐，但目前主流的推理蒸馏方法（让学生模型模仿教师模型的推理轨迹）是否能有效传递这种认知结构，尚未被系统研究。

Method: 作者在14个模型上检验了‘含丹学步’假说，通过SFT（有监督微调）方式训练学生模型模仿教师模型推理轨迹，并测量人类难度与模型计算代价的相关性，以及学生模型推理能力的变化。

Result: 蒸馏学生模型的人类-模型难度相关性从0.64明显下降到0.34，且部分学生模型表现不如蒸馏前，出现‘负迁移’。分析发现学生模型只学会了表面推理形式（如啰嗦程度），没有学到教师模型的资源分配动态策略。

Conclusion: 推理蒸馏使计算代价和认知需求失配、人类式推理能力不是被动模仿的产物，而是主动强化学习的涌现结果。

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [161] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出了一种名为ArcAligner的新方法，通过集成到LLM内部以适应高度压缩的上下文信息，从而提升检索增强生成（RAG）中的准确性和效率，在多个知识密集型问答任务基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在面对长文档时导致推理变慢且成本高昂，常见的上下文压缩方法又会因信息压缩过度导致LLM理解能力下降。因此亟需一种能够高效利用高度压缩上下文的新机制。

Method: ArcAligner是一种自适应递归上下文对齐模块，内嵌于语言模型层中。它通过自适应门控系统，根据上下文复杂程度动态分配计算资源，在保证推理效率的同时增强模型对高度压缩信息的处理能力。

Result: ArcAligner在多个知识密集型问答基准上，特别是在多跳推理和长尾任务下，相较于主流压缩方法，在相同压缩率下取得了更优的效果。

Conclusion: ArcAligner在保持系统高效的同时，显著提升了LLM对于高度压缩上下文的生成能力，为RAG技术在实际场景落地带来了新突破。源码已公开。

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [162] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过在输入中引入可组合的控制token，实现对大语言模型(LLM)多目标行为的同时控制，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注于LLM输出的单一行为控制，但实际应用 often 需要同时满足多个约束，实现对LLM多行为的组合控制(compositional steering)仍未得到充分探索。

Method: 作者提出了“compositional steering tokens”机制：首先用自蒸馏将自然语言指令表示的单一行为嵌入为专用token；然后通过组合token实现对多个行为的并发控制。这些token直接作用于输入token空间，而非以往的方法如激活空间操作。此外，引入“composition token”用于建模两种行为组合。

Result: 实验表明，本文方法在多种LLM体系结构下，比自然语言指令、激活方法和LoRA合并等现有方法有更强的多行为控制能力。组合token还能泛化到未见过的行为及行为数量。同时，steering tokens与自然语言指令结合，可取得进一步增益。

Conclusion: 通过输入token层面的可组合行为控制，能够更有效地实现对LLM多行为同时定向，为实际多约束场景中的LLM应用提供了新手段，有望推动多目标可控生成技术的发展。

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [163] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: 提出SemPA方法，通过语义偏好对齐提升大语言模型（LLMs）句子表示能力，同时保持其生成能力。实验和理论分析均证明SemPA表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的句子嵌入方法要么依赖固定模板且性能有限，要么需修改模型结构导致生成能力受损，亟需新方法兼顾生成能力与语义表达。

Method: 提出SemPA方法，利用句子级直接偏好优化（DPO）技术，使LLM在同义句生成任务上区分语义等价句子，同时不损失生成能力。在理论上，将DPO与对比学习在Plackett-Luce模型下进行了关联。

Result: 在语义文本相似性和多项LLM基准任务中，SemPA表现优于已有方法，实现了更好的语义句子表示，且生成能力未受影响。

Conclusion: SemPA能够在不牺牲生成能力的前提下显著提升LLM的语义句子表示效果，是基于LLM句子嵌入任务的新进展。

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [164] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: 该论文提出了一种基于mBERT的高性能情感分类框架，专门用于分析印度社交媒体上的Hinglish（印英混合语）推文，以解决传统NLP模型在混合语料上的局限。


<details>
  <summary>Details</summary>
Motivation: 在印度，用户生成内容中Hinglish语言的普及给品牌监测和情感分析带来极大挑战。传统NLP模型因缺乏对混合语的理解，导致情感分析不准确，进而影响市场决策。

Method: 方法上，作者对mBERT模型进行微调，利用其多语言能力来提升对Hinglish推文的理解力。核心技术是子词分词（subword tokenization），能有效应对拼写变化、俚语，以及罗马化Hinglish中的新词。

Result: 研究结果显示，该方法能高效、准确地识别Hinglish情感，适用于品牌情感追踪，并在低资源、代码混合语环境下建立了强有力的多语言NLP基线。

Conclusion: 论文证明通过针对性构建和微调的多语言模型，能极大提升在印度社交媒体Hinglish混合语料上的情感分析能力，为实际品牌监测提供了切实可用的AI解决方案。

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [165] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: 本研究探讨了人类与ChatGPT互动中的情绪语调如何影响AI和人的行为表现。不同情绪表达（表扬、生气、指责、中性）会对AI回答改善程度及人际沟通语气产生显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能助手广泛进入人类生活，了解情绪因素对人机互动及其后续人际关系的影响变得尤为重要。以往研究多关注人类对AI的信任或接受度，而较少关注情绪表达对AI行为及人际后效应的具体影响。

Method: 采用被试间设计实验，让参与者在与ChatGPT（GPT-4.0）合作完成任务时有意表达特定情绪（表扬、生气、指责、中性），任务包括写公开回应和伦理困境处理，同时分析AI反馈以及后续参与者与其他人的沟通风格变化。

Result: 表扬ChatGPT能显著提升其回答质量，生气表达有较小提升，指责无明显效果。在伦理困境中，生气让AI对公司利益让步，指责则使AI更强调公众利益。经历指责AI的互动后，参与者在与他人对话中倾向于使用更多消极和敌对语言。

Conclusion: 人类在与AI互动中采用的情绪语调会影响AI表现，并迁移影响到之后的人际交流。情绪管理在促进健康人机与人际互动中极其重要。

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [166] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文综述了AI评价领域由LLM-as-a-Judge向Agent-as-a-Judge的转变，分析了相关方法、应用和挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着被评价对象变得更复杂且多样，传统使用大语言模型单次评判的方式（LLM-as-a-Judge）受限于偏见、浅层推理和难以验证等问题，因此急需更强大且可验证的评价系统。

Method: 提出以智能体（agent）为判官的Agent-as-a-Judge新范式，通过规划、工具辅助验证、多智能体协作及持久记忆，提升评价的鲁棒性和可验证性；构建了系统的维度和发展分类法，对现有方法论和应用进行梳理。

Result: 确立了该领域的核心维度和发展链条，系统整理和归纳了当前主流方法和实际应用场景，总结前沿挑战。

Conclusion: 为agentic评价的未来发展提供了系统的研究框架和路线图，指明了下一代智能体评价的发展方向。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [167] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的开源文档问答（DocQA）代理系统DocDancer，创新性地利用工具支持和数据合成方法，提升了长文档理解与问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有文档问答系统对工具的利用有限，且大量依赖闭源模型，导致应用和拓展受限。与此同时，优质训练数据稀缺，制约了模型进一步发展。

Method: 将DocQA任务建模为信息检索任务，提出以工具驱动的代理框架，显式建模文档的探索与理解过程。同时，设计“先探索后合成”的数据合成流程以生成高质量训练数据，并用合成数据对DocDancer进行端到端训练。

Result: 在两个长文本理解基准数据集（MMLongBench-Doc和DocBench）上，DocDancer表现出优异的性能。深度分析还揭示了工具设计和合成数据对系统效果的积极影响。

Conclusion: DocDancer有效结合工具机制和数据合成，提升了长文档问答的能力，为文档智能理解与问答领域提供了开源且高效的新方案，同时也为相关工具与数据设计带来了新思路。

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [168] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的高效推理框架RelayLLM，通过让小模型（SLM）仅在关键token调用大模型（LLM），以大幅降低计算成本同时保障推理能力。


<details>
  <summary>Details</summary>
Motivation: 复杂推理任务下，大模型推理虽性能好但计算开销高，而小模型虽高效但推理能力不足，现有协作方法粒度过粗导致资源浪费。亟需一种更细粒度且高效的协作机制提升整体推理效率。

Method: 提出RelayLLM，将推理粒度细化到token级，由SLM作为主动控制者，仅在关键token调用LLM。设计了包含warm-up和分组相对策略优化（GRPO）的两阶段训练框架，使SLM学会在独立生成与策略性求助间平衡。

Result: 在六个基准测试上，RelayLLM平均准确率达49.52%，有效弥合小大模型性能差距。而LLM仅在1.07%的token被调用，实现了98.2%的成本节约（相较于随机router）。

Conclusion: RelayLLM显著提升了小大模型协作效率，在大幅降低大模型调用率的同时，保持了较高的推理性能，为复杂推理任务下的高效模型协作提供了新范式。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [169] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文系统分析了自然语言推断（NLI）任务的逻辑属性，通过三种不同NLI标签集解读，探究模型和数据集反映的推理逻辑。


<details>
  <summary>Details</summary>
Motivation: 虽然NLI被广泛用于评估自然语言理解模型，但其背后的推理逻辑属性并不清晰，常被误解。作者希望厘清NLI任务“推断”的真正涵义，这有助于正确解释现有模型在该任务上的表现。

Method: 作者提出NLI标签集的三种可能解读，对其蕴含的元推理属性进行分析。主要基于SNLI数据集，通过分析（1）具有相同前提的NLI样本和（2）大语言模型生成的样本，评测训练于SNLI的模型在元推理层面的表现。

Result: 分析揭示了不同标签解读下，数据集与模型的推理一致性表现差异，为理解NLI任务所隐含的逻辑关系提供了经验依据。

Conclusion: 作者发现标准NLI数据及主流解释方式并不能理想刻画其逻辑属性，并建议后续在NLI评价和模型开发中需更清楚定义推理含义以及底层逻辑关系。

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [170] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: 本文提出PersonaTree和MemListener两个新机制，有效提升了长期个性化对话系统的内存管理与用户画像一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统由于上下文容量有限，容易面临记忆噪音、推理能力衰减和用户画像不一致等问题。

Method: 提出了PersonaTree结构用于长期用户画像管理，通过限定主干和灵活生长树枝，实现高效一致的信息压缩。引入MemListener模型，采用强化学习训练产生结构化的内存操作指令，实现动态高效管理用户信息。

Result: PersonaTree在抑制上下文噪声和保持用户画像一致性方面优于传统拼接和不同个性化内存方法。轻量级MemListener在内存操作决策上可媲美甚至超过大型推理模型。

Conclusion: PersonaTree和MemListener实现了高效、可控的长期用户画像维护，为个性化对话系统带来一致性和实时性的新突破。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [171] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: 提出了一种无需微调、基于大语言模型的实体链接方法LELA，在多项实验中表现接近甚至超越微调方法。


<details>
  <summary>Details</summary>
Motivation: 实体链接作为知识图谱构建、问答、信息抽取等任务的基础环节，现有方法普遍依赖针对特定领域和知识库的微调，缺乏通用性和灵活性。

Method: 提出LELA，一种模块化、粗到细的实体链接方法。该方法利用大语言模型的强大能力，并可适配不同领域、知识库和LLM模型，无需任何微调过程。

Result: 在不同实体链接任务和设置下，LELA表现高度接近业内微调方法，且远超其它无需微调的方法。

Conclusion: LELA实现了无需微调且高效的实体链接，对跨领域、跨知识库和模型的实体链接任务均有广泛适用性和竞争力。

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [172] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: 本论文利用机器学习和人工智能分析新闻及社交媒体内容中的和平水平，并开发了促进用户理解媒体影响的在线工具，如MirrorMirror插件。实验显示算法具备跨数据集的高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数人通过社交媒体短视频了解新闻，而内容创作者常利用激烈情感吸引点击，容易激发消极情绪。需要用技术手段测量并改善大众媒体环境中的和平与理性沟通。

Method: 1) 对新闻文本，使用神经网络与文本嵌入技术进行和平度量并跨数据集验证；2) 针对社交媒体如YouTube，使用GoEmotions情感识别与大语言模型分析社会维度；3) 开发Chrome扩展MirrorMirror，实时反馈用户观看内容的和平度。

Result: 神经网络模型在不同新闻数据集间都表现出高准确率，情感分析和上下文模型在社交媒体内容中有效区分和平相关维度。MirrorMirror插件能实时向用户反馈媒体内容的和平程度。

Conclusion: 本研究提出了一套有效的算法与工具，有助于个人和内容创作者理解及提升其媒体内容的和平与理性程度。希望推广该工具，鼓励更有建设性、尊重性的公共交流。

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [173] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本文提出了一种新的多奖励强化学习优化方法GDPO，解决了现有GRPO方法在多奖励归一化时造成表现下降的问题，并在多个任务上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多奖励强化学习方法GRPO无法有效区分不同奖励信号，导致训练信号分辨率降低，模型收敛欠佳甚至训练失败。因此有必要设计更合理的多奖励归一化与优化方法。

Method: 作者提出了GDPO（Group reward-Decoupled Normalization Policy Optimization）方法，将不同奖励的归一化过程解耦，保留各自的差异性，从而更准确地进行多奖励优化。

Result: 在工具调用、数学推理和代码推理三项任务上，GDPO在正确性（准确率、BUG率）和约束满足（格式、长度）两类指标下均优于GRPO，表现出更好的泛化和有效性。

Conclusion: GDPO能显著提升多奖励强化学习的训练表现和稳定性，是对多偏好场景下强化学习优化方法的重要改进。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [174] [Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization](https://arxiv.org/abs/2601.04334)
*Amit Jain,Richard Linares*

Main category: cs.RO

TL;DR: 本文提出利用具备推理能力的大语言模型（LLM）与群体相对策略优化（GRPO）相结合的方法，实现面向复杂控制系统的学习型指导与控制。实验涵盖从线性系统到多维航天器等多种控制问题，验证了两阶段训练下LLM的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的自主控制系统常面临复杂性高、环境多变和安全性要求强等挑战。传统方法难以统一应用于不同类型的动态系统，因此需探索具泛化能力、可解释且优化性强的方法，并希望借助预训练大模型的推理和表征能力。

Method: 该方法分为两个阶段：第一阶段通过有监督微调（SFT）训练模型学习格式和控制基础知识；第二阶段利用GRPO策略优化，针对交互过程不断提升控制性能。模型可在多种环境中进行训练与应用，并具备输出可解释决策的能力。

Result: 方法在四种控制任务（包含线性、非线性到复杂航天器姿态控制）上均取得了稳定和可行的控制策略。在统一的训练流程和参数设置下，无论系统动态复杂与否，LLM+GRPO均能输出合理且人类可理解的控制政策。

Conclusion: 本文方法有效结合LLM推理能力和GRPO优化，能统一适应线性与非线性控制问题，且具可解释性。此框架有望拓展至航天等高安全领域，为自主控制系统的智能化和可信化奠定基础。

Abstract: This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.

</details>


### [175] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: 本文提出了UNIC，一个无需先验知识或相机标定即可进行外部接触估计的多模态统一框架，有效提升了复杂操作任务中的泛化能力和实用性。


<details>
  <summary>Details</summary>
Motivation: 以往的外部接触估计方法往往依赖于预设接触类型、固定抓取方式或相机标定等限制性假设，导致泛化能力差，难以应用于新物体和非结构化环境。因而需要一种更通用、数据驱动且适用性强的新方法。

Method: 作者提出了UNIC，直接对摄像头坐标下的视觉信息进行编码，并融合了本体和触觉传感信息。采用基于场景可供性图的统一接触表示方式，并引入了带有随机掩蔽的多模态融合机制，实现了稳健的多模态表达学习。

Result: UNIC在看不见的接触位置上的平均Chamfer距离误差为9.6 mm，对新物体也表现良好；在感知模态缺失和动态相机视角下仍具有鲁棒性。

Conclusion: UNIC使外部接触估计成为适用于复杂操作的新一代实用能力，具有广泛的适应性和应用前景。

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [176] [Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces](https://arxiv.org/abs/2601.04401)
*Arsyi Aziz,Peng Wei*

Main category: cs.RO

TL;DR: 本文提出了一种新的多智能体强化学习（MARL）方法，通过引入极坐标状态空间和transformer编码器模型，使无人机空域管理更具适应性和通用性，实现更安全、高效的飞行间隔保障。


<details>
  <summary>Details</summary>
Motivation: 现有的优化调度方法缺乏对先进空中机动（AAM）系统下飞行高度不确定性的适应性，同时现有MARL模型容易陷入对特定空域结构的过拟合，妨碍推广应用。

Method: 作者将MARL问题重新表述在相对极坐标状态空间中，并基于不同流量模式和交叉角度训练transformer编码器模型，用于为无人机提供速度建议以避免冲突，并保持其巡航速度。

Result: 实验结果表明，单层transformer编码器在结构化与非结构化空域中均优于更深层的配置，能将中空险些相撞率降至接近零，并减少飞行分离违规时长。同时也优于仅基于注意力的基线模型。

Conclusion: 新提出的状态表示方法、神经网络结构及其训练策略，为结构化与非结构化空域中的飞行器分离保障提供了一种可扩展、适应性强的分布式解决方案。

Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

</details>


### [177] [Fast Continuum Robot Shape and External Load State Estimation on SE(3)](https://arxiv.org/abs/2601.04493)
*James M. Ferguson,Alan Kuntz,Tucker Hermans*

Main category: cs.RO

TL;DR: 本文提出了一种通用的连续体机器人状态估计框架，能够处理驱动、不确定性和外部载荷，同时支持时空联合估计，并在仿真和实验中取得了准确的运动学和末端力估计。


<details>
  <summary>Details</summary>
Motivation: 现有连续体机器人状态估计方法大都依赖于简化的Cosserat杆模型，无法直接处理驱动输入和外部载荷等复杂情况，限制了其实用性和精度。因此，亟需一种更通用且能结合实际输入和不确定性的状态估计框架。

Method: 该方法将各类不确定性（如驱动张力、外载荷、过程噪声、边界条件等）和任意主干测量统一建模，同时引入时间先验，实现空间（弧长）和时间维度的联合估计。通过离散化弧长域，构建了连续体机器人模型的因子图，可利用SLAM风格的稀疏非线性优化算法进行高效批量估计。框架具备高度通用性，适用于不同类型的连续体机器人。

Result: 仿真案例中，成功实现了带不确定性的实时运动学估计、仅凭位置反馈的末端力感知以及基于主干应变的分布式载荷估计。实验案例（手术同心管机器人）中，准确实现了运动学和末端力估计，验证了方法实际有效性，展现手术触诊等应用潜力。

Conclusion: 提出的框架有效提升了连续体机器人在实际复杂环境下的状态估计能力，兼具通用性和高精度，能广泛应用于仿生机器人、医疗机器人等领域，具备良好的工程和研究前景。

Abstract: Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.

</details>


### [178] [Multiagent Reinforcement Learning with Neighbor Action Estimation](https://arxiv.org/abs/2601.04511)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu*

Main category: cs.RO

TL;DR: 本文提出了一种通过动作估计神经网络实现的增强型多智能体强化学习框架，无需显式动作交换即可实现协同决策，提升了鲁棒性和实际部署性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法常依赖智能体之间的显式动作共享，但实际工程场景中受限于通信、延迟、能耗等问题，这种动作共享不切实际。因此，需要一种无需显式动作交换、且能够适应信息受限环境的方法。

Method: 提出了一种在标准TD3算法基础上集成动作估计模块的新框架。每个智能体利用本地可观测信息，通过神经网络估计邻居的动作，实现动作价值评估和协同策略学习，无需实时动作通信。该方法设计为轻量级，具备良好可扩展性，可用于大规模多智能体系统。

Result: 在双臂机器人协作搬运任务中进行实验，验证了该方法的有效性。结果显示，该框架在增强系统鲁棒性、减少信息基础设施依赖和提升部署可行性方面具有明显优势。

Conclusion: 本文方法推进了去中心化多智能体强化学习的发展，使AI系统能够在动态、信息受限的现实环境中高效运行，为实际智能体协作提供了新思路和工程可行性。

Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.

</details>


### [179] [Design and Development of Modular Limbs for Reconfigurable Robots on the Moon](https://arxiv.org/abs/2601.04541)
*Gustavo H. Diaz,A. Sejal Jain,Matteo Brugnera,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了4自由度的月球机器人肢体模块'Moonbots'，可与不同模块灵活组合，适应多环境任务，主要用于月球探测和空间建造。


<details>
  <summary>Details</summary>
Motivation: 提高空间探索和建造任务中机器人系统的灵活性和适应性，解决空间内资源有限、任务多变的需求。

Method: 开发了统一驱动器的4DOF模块，通过通用高扭矩速比执行器驱动，实现多模块组合，硬件与软件架构统一。详细设计了机械结构，并在不同负载下评估驱动性能。提出九种模块组合形态，覆盖多种功能需求。

Result: 模块化系统能够灵活组装，演示了九种不同的功能配置，并验证了驱动器在多种负载下的反应，展示了系统的适应性和高性能。

Conclusion: 这种模块化、统一驱动的机器人设计提供了空间任务中高灵活性、易维护的解决方案，为后续可重构机器人研究奠定了实践基础。

Abstract: In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.

</details>


### [180] [Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers](https://arxiv.org/abs/2601.04547)
*Jakob M. Kern,James M. Hurrell,Shreya Santra,Keisuke Takehana,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本论文提出了一种融合高视觉保真度与真实地形交互的月面仿真器模型，既提升了外观真实感，也兼顾了物理行为的准确性，特别是在轮-土相互作用方面。


<details>
  <summary>Details</summary>
Motivation: 现有月面仿真器要么注重视觉逼真，要么注重物理精度，缺乏能兼顾两者、全面复现月球环境的模拟平台。作者旨在弥补这一空白。

Method: 采用数据驱动方法，通过回归模型预测轮子的滑移和沉陷，相关数据来源于完整月球车和单轮实验与仿真。该模型还对地形变形与车轮轨迹可视化效果进行了改进。

Result: 所提出的回归型地面力学模型能准确再现轮子的稳态与动态滑移、沉陷，在坡度高达20°的平地与斜坡实验中，与实地测试结果吻合良好。并提升了地形响应的真实性与可视化。

Conclusion: 这种融合视觉和力学准确性的仿真方法同时适用于需要实时、物理合理地形响应且追求高清画质的应用场景，为月面任务规划和探测器操作提供了高效工具。

Abstract: High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.

</details>


### [181] [Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain](https://arxiv.org/abs/2601.04551)
*Riku Suzuki,Ayumi Umemura,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种基于离散傅里叶变换（DFT）的方法，通过去除高频分量，有效压缩用于SLAM的逐渐地形点云地图的数据量，同时基本不影响点云质量。


<details>
  <summary>Details</summary>
Motivation: SLAM任务中点云数据量大，而无人机计算与通信能力有限，现有采集的数据难以实时、高效处理和传输。高效的数据压缩方法能显著提升SLAM在实际探索任务中的效率和可靠性。

Method: 将数字高程模型（DEM）转换为频域2D图像，对频域图像的高频成分进行裁剪，主要聚焦在逐渐变化的地形（如行星、沙漠），此类地形的高频分量对于地图表达贡献较小，从而有效实现点云压缩。

Result: 在具有不同地形起伏的两种地形的摄像机序列上进行了评估，对比压缩率和点云精度，结果表明较高的压缩比下，地图的精度损失较小。

Conclusion: 新方法适用于以自然、平缓为主的地形场景，可大幅压缩点云地图的数据量，在不降低SLAM效果的前提下，有望提升无人探索任务的数据处理与传输效率。

Abstract: Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.

</details>


### [182] [UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation](https://arxiv.org/abs/2601.04629)
*Zhongxuan Li,Zeliang Guo,Jun Hu,David Navarro-Alarcon,Jia Pan,Hongmin Wu,Peng Zhou*

Main category: cs.RO

TL;DR: UniBiDex是一个支持VR和跟随式输入的新型双臂机器人远程操作框架，可以实现实时、高接触、灵巧的双臂操作，并在厨房整理等任务上获得了比其他方法更好的表现。所有软硬件开源以促进机器人学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器人双臂灵巧操作的远程控制面临输入设备异构、操作不流畅及安全性等挑战。缺乏统一、高质量的人类演示采集方式，限制了机器人学习领域的发展。

Method: 提出了UniBiDex框架，将VR和主从设备等各种输入方式整合至统一控制系统，采用零空间控制优化双臂姿态，保障运动的连贯性、安全性和避碰。

Result: 在五步厨房整理任务验证中，UniBiDex在任务成功率、运动轨迹流畅性和鲁棒性上均优于多个强基线方法。

Conclusion: UniBiDex推进了机器人双臂远程操控的统一和高效，实现了性能提升和演示数据易获取，为机器人学习研究和实际应用提供了良好平台。

Abstract: We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.

</details>


### [183] [Model of Spatial Human-Agent Interaction with Consideration for Others](https://arxiv.org/abs/2601.04657)
*Takafumi Sakamoto,Yugo Takeuchi*

Main category: cs.RO

TL;DR: 论文提出了一种空间交互模型，使机器人能根据环境中人的行为调整自身交流行为，以便在公共场所主动开启对话但不打扰行人。实验用VR平台验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在公共空间需要主动与人交流，但又不能打扰到行人，因此需要一种方法评估周围人的交流意愿并相应调整自身行为。

Method: 建立了一个可计算的空间交互模型，引入“考虑他人”作为参数，量化机器人根据对方状态调整自身状态的程度。在VR实验中，模拟人与虚拟机器人互动，调节“考虑”参数，观察其对人类行动的影响。

Result: 当机器人“考虑”参数较低时，会干扰人的移动；当该参数较高时，则不会干扰。当人类靠近机器人时，无论“考虑”参数如何，机器人都会靠近人类，导致人的动作减少。

Conclusion: 模型能够有效体现机器人在考量他人情况下的交互行为，有助于研发不会妨碍人的社交机器人。

Abstract: Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.

</details>


### [184] [Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture](https://arxiv.org/abs/2601.04668)
*Laukik Patade,Rohan Rane,Sandeep Pillai*

Main category: cs.RO

TL;DR: 本研究利用深度强化学习（DRL）在连续动作空间下，优化精准农业中的无人地面车辆（UGV）路径规划，并验证其在动态场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的A*、Dijkstra等网格法在动态、复杂的农业环境中适应性差，难以满足精准农业对柔性路径规划的需求。因此，需要更自适应的学习型路径规划方法。

Method: 首先评估了基于深度Q网络（DQN）的离散动作空间DRL方法及其改进（如Double Q、Dueling Network），随后转向适用于连续动作的DDPG与TD3，并在ROS+Gazebo三维动态环境中验证其有效性。

Result: 连续动作空间的DRL算法表现出较强的适应能力。其中，预训练的TD3智能体在动态环境下实现了95%的路径规划成功率，可以很好地避让移动障碍，保障作物与机器人安全。

Conclusion: 连续动作空间的深度强化学习方法，尤其是TD3模型，在动态精准农业场景下对UGV路径规划表现出强大的鲁棒性和实用价值，优于传统方法。

Abstract: This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.

</details>


### [185] [SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/abs/2601.04699)
*Zebin Han,Xudong Wang,Baichen Liu,Qi Lyu,Zhenduo Shang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.RO

TL;DR: 本文提出了SeqWalker模型，通过分层规划框架应对需要执行多阶段、复杂文字导航指令的Sequential-Horizon Vision-and-Language Navigation (SH-VLN)任务，并在数据集和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航模型面对多任务、长程的复杂指令时，表现大幅下降，主要原因是信息过载，导致模型无法关注到有效细节。需要新的方法提升在复杂导航任务下的表现。

Method: 提出SeqWalker分层模型，包括：1）高层规划器动态将全局指令分解为与当前视觉观察相关的子指令，从而减轻认知负担；2）低层规划器引入“探索-验证”策略，根据指令逻辑结构纠正路径误差。并扩展IVLN数据集，构建新基准进行评测。

Result: 在扩展的数据集和新基准上，SeqWalker通过大量实验证明了优于现有方法的性能，能够有效减缓信息过载带来的性能损失。

Conclusion: SeqWalker模型通过分层规划和探索-验证机制，有效提升了SH-VLN任务下复杂指令导航的表现，为多任务视觉-语言导航提供了新的解决思路和评测标准。

Abstract: Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.

</details>


### [186] [Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly](https://arxiv.org/abs/2601.04881)
*Kiyoung Choi,Juwon Jeong,Sehoon Oh*

Main category: cs.RO

TL;DR: 本文提出了动态力矩扰动观测器（DW-DOB），实现了高灵敏度的零力矩控制，显著优于传统方法，适用于高精度接触丰富操作。


<details>
  <summary>Details</summary>
Motivation: 现有的扰动观测器难以在接触丰富的操作任务中区分本体动力学反应和真实外部扰动，且对惯性影响补偿不足，导致力控制灵敏度差、插入操作不精确。

Method: DW-DOB在观测器标称模型中嵌入任务空间惯性，实现本体动力学反应与外部扰动力的有效分离。采用基于能动性的理论分析，证明其在动态环境下的交互稳定性。同时通过工业级配合公差（H7/h6）下的插孔实验加以验证，对比传统扰动观测器和PD控制器。

Result: DW-DOB在实验中展现出更深且更柔顺的插入动作、残留力矩更小，整体性能优于传统扰动观测器和PD控制基线。

Conclusion: DW-DOB为高精度、零力矩控制的接触丰富任务提供了一种无需学习的实用方案，能够显著提升操作灵敏度和控制鲁棒性。

Abstract: This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.

</details>


### [187] [SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles](https://arxiv.org/abs/2601.04948)
*Junchi Gu,Feiyang Yuan,Weize Shi,Tianchen Huang,Haopeng Zhang,Xiaohu Zhang,Yu Wang,Wei Gao,Shiwu Zhang*

Main category: cs.RO

TL;DR: 本论文提出了一种新型仿人轮滑机器人，并使用深度强化学习控制其滑行动作，显著降低了运动过程中的冲击力和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统仿人机器人在行走和奔跑时频繁与地面碰撞，导致关节磨损严重且能量利用率低。借鉴轮滑运动中对惯性的高效利用，期望解决这些问题。

Method: 设计了每只脚带有四个无动力滚轮的全新仿人机器人，并提出针对轮滑“swizzle”步态的深度强化学习控制框架，在模拟和实物机器人上进行了验证。

Result: 与传统双足步态相比，轮滑步态在冲击强度和能耗上分别减少了75.86%和63.34%，展现出在能效和关节保护方面的优势。

Conclusion: 轮滑被证实为更节能且减小机械磨损的仿人机器人运动模式，推动了机器人高效、持久运动能力的发展。

Abstract: Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\%$ and $63.34\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.

</details>


### [188] [When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics](https://arxiv.org/abs/2601.04982)
*Johannes A. Gaus,Winfried Ilg,Daniel Haeufle*

Main category: cs.RO

TL;DR: 本文提出了一种基于置信度校准的安全关键触发框架，提高了辅助设备为日常生活活动预测下一步动作时的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 辅助设备在提供帮助前需要准确预测用户意图及其可靠性，但模型原始置信度往往无法真实反映预测正确性，会带来安全风险。

Method: 提出利用后置校准方法对多模态下一步动作预测的模型置信度进行校准，将模型置信度与实际预测准确率对齐，并设定基于校准置信度的ACT/HOLD动作控制规则。只有在高可靠性情况下才触发辅助控制，否则保持观望。

Result: 置信度校准将模型误校准程度降低了一个数量级，并未降低预测准确度。基于校准置信度的动作控制实现了可量化和可验证的行为安全阈值。

Conclusion: 通过校准模型置信度并合理利用置信门限，可显著提高辅助设备动作预测时的安全性和可靠性，实现系统性、可验证的行为支持。

Abstract: Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.

</details>


### [189] [The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms](https://arxiv.org/abs/2601.05014)
*Lingdong Kong,Shaoyuan Xie,Zeying Gong,Ye Li,Meng Chu,Ao Liang,Yuhao Dong,Tianshuai Hu,Ronghe Qiu,Rong Li,Hanjiang Hu,Dongyue Lu,Wei Yin,Wenhao Ding,Linfeng Li,Hang Song,Wenwei Zhang,Yuexin Ma,Junwei Liang,Zhedong Zheng,Lai Xing Ng,Benoit R. Cottereau,Wei Tsang Ooi,Ziwei Liu,Zhanpeng Zhang,Weichao Qiu,Wei Zhang,Ji Ao,Jiangpeng Zheng,Siyu Wang,Guang Yang,Zihao Zhang,Yu Zhong,Enzhu Gao,Xinhan Zheng,Xueting Wang,Shouming Li,Yunkai Gao,Siming Lan,Mingfei Han,Xing Hu,Dusan Malic,Christian Fruhwirth-Reisinger,Alexander Prutsch,Wei Lin,Samuel Schulter,Horst Possegger,Linfeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li,Youngseok Kim,Sihwan Hwang,Hyeonjun Jeong,Aodi Wu,Xubo Luo,Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao,Shuangzhi Li,Junlong Shen,Xingyu Li,Hao Ruan,Jinliang Lin,Zhiming Luo,Yu Zang,Cheng Wang,Hanshi Wang,Xijie Gong,Yixiang Yang,Qianli Ma,Zhipeng Zhang,Wenxiang Shi,Jingmeng Zhou,Weijun Zeng,Kexin Xu,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He,Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim,Faduo Liang,Zihang Wang,Yiming Peng,Guanyu Zong,Xu Li,Binghao Wang,Hao Wei,Yongxin Ma,Yunke Shi,Shuaipeng Liu,Dong Kong,Yongchun Lin,Huitong Yang,Liang Lei,Haoang Li,Xinliang Zhang,Zhiyong Wang,Xiaofeng Wang,Yuxia Fu,Yadan Luo,Djamahl Etchegaray,Yang Li,Congfei Li,Yuxiang Sun,Wenkai Zhu,Wang Xu,Linru Li,Longjie Liao,Jun Yan,Benwu Wang,Xueliang Ren,Xiaoyu Yue,Jixian Zheng,Jinfeng Wu,Shurui Qin,Wei Cong,Yao He*

Main category: cs.RO

TL;DR: RoboSense 2025 Challenge提出了一个用于机器人感知稳健性和泛化能力评估的综合性基准，包括五大任务方向，统一了数据集、评测方法并反映了全球范围的广泛参与和最新研究趋势。


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主系统在复杂、动态环境中的应用增多，现有的感知方法在遇到未知条件时性能下降明显。因此，提升感知系统在各种变化和传感器失效情况下的稳健性和泛化能力成为亟需解决的问题。

Method: RoboSense 2025 Challenge设立了五个研究方向：基于语言的决策、社会化导航、传感器泛化、跨视角/跨模态匹配和跨平台3D感知。赛事统一了标准化数据集、基线模型和评测协议，对不同稳健性感知方法进行了大规模、可复现的比较。

Result: 挑战共吸引了来自16个国家85家机构的143支队伍参与。通过对23个获胜方案的分析，归纳了新兴方法趋势、共性设计原则以及在各个任务方向上的开放性挑战。

Conclusion: RoboSense 2025 Challenge为真实环境下机器人的稳健感知研究提供了权威基准，加速了相关领域的技术进步，有助于未来实现可靠、可适应不同平台环境的自主机器人。

Abstract: Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.

</details>


### [190] [Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses](https://arxiv.org/abs/2601.05074)
*Julian Kulozik,Nathanaël Jarrassé*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的上肢假肢关节控制方法——补偿效应增强控制（CEAC），利用躯干弯曲与伸展作为输入，显著提升假肢中间关节（如手肘）的直觉性和精确性控制。实验显示，参与者用 CEAC 执行绘画与到达任务时表现接近自然手臂动作，同时保持良好的人体工学姿态。


<details>
  <summary>Details</summary>
Motivation: 现有上肢假肢在实现手腕和手肘等中间关节的直观、速度可调的连续性控制上仍存在困难。使用者常因缺乏自然控制导致补偿性用力和不自然动作，影响假肢体验和效率。因此，亟需更符合人体自然运动规律、直观性强的新控制策略。

Method: 提出 CEAC 控制范式，将用户的躯干运动（弯曲和伸展）映射并放大至假肢手肘速度的精细控制，并通过延时机制让用户能同时调节假肢位置和速度。在12名健康志愿者执行绘图任务，以及10人子集执行多目标到达任务中，对该方法进行评估。

Result: CEAC 控制方式下，志愿者在模拟假肢操作的绘画及目标到达任务中的表现与自然手臂动作相当，无论手势速度或绘画尺寸如何变化，均可维持人体工学的躯干姿势。分析表明，CEAC 能高效地恢复关节协同控制，合理分配躯干与肘部的运动负荷，实现自然、直观的轨迹控制，无需过度补偿性动作。

Conclusion: CEAC 为上肢假肢中间关节（如手肘）的连续、精准控制提供了有效策略，尤其适合要求连续协调的任务，有望提升假肢的直觉性与操控体验。

Abstract: Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.

</details>


### [191] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex 是一种通过单次人类操作演示合成大规模灵巧抓取数据，集成视觉与几何信息，实现新颖物体功能性抓取的鲁棒学习框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前灵巧机器手功能抓取能力受限于大规模数据集稀缺以及现有模型缺乏语义与几何融合推理能力。作者旨在突破这两个瓶颈，推动机器人工具使用及复杂操作的发展。

Method: 提出 CorDex 框架: 首先由单个人类操作演示，通过数据引擎在仿真中生成多样高质量的数据；引入类别实例扩增、抓取动作的对应估计与优化迁移。随后，设计多模态预测网络，融合视觉和几何特征，并提出局部-全局融合模块与重要性感知采样，实现高效鲁棒的抓取预测。

Result: 通过大量实验，CorDex 展现了对未见物体实例的良好泛化能力，在多个物体类别上抓取表现显著优于现有最优基线方法。

Conclusion: CorDex 显著提升了机器人灵巧抓取功能的泛化和性能，验证了单次人类演示驱动的数据合成与多模态语义几何融合的有效性，为智能机器人工具使用和复杂操作能力提供了新方向。

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


### [192] [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](https://arxiv.org/abs/2601.05248)
*Zhuoyang Liu,Jiaming Liu,Hao Chen,Ziyu Guo,Chengkai Hou,Chenyang Gu,Jiale Yu,Xiangju Mi,Renrui Zhang,Zhengping Che,Jian Tang,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaST$_0$提出了一种高效的视觉-语言-动作（VLA）机器人推理框架，通过隐式时空链式推理（Latent Spatio-Temporal Chain-of-Thought），提升任务成功率并加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法通过显式生成语言推理或未来视觉来提升动作准确率，但这些方法推理速度慢，且局限于语言表达，无法充分捕捉复杂物理属性，限制了机器人操作的实际性能。

Method: 作者提出LaST$_0$框架，通过一种token高效的隐式推理空间，建模未来视觉动态、三维结构和机器人自身状态，并横跨时序延展推理轨迹。系统采用Mixture-of-Transformers架构，将低频推理专家与高频动作专家结合，并通过异构频率训练，实现自适应推理与动作的切换。

Result: LaST$_0$在10个模拟与6个真实操作任务中，分别比现有VLA方法提升了8%和13%的平均成功率，同时推理速度显著提升。

Conclusion: LaST$_0$有效解决了推理瓶颈与时序分辨率受限的问题，可更好地将VLA系统应用于动态与精细操控场景，为机器人推理和操作提供了新的设计范式。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0

</details>
