<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]
- [cs.CL](#cs.CL) [Total: 56]
- [cs.RO](#cs.RO) [Total: 43]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays](https://arxiv.org/abs/2509.15234)
*Hanbin Ko,Gihun Cho,Inhyeok Baek,Donguk Kim,Joonbeom Koo,Changi Kim,Dongheon Lee,Chang Min Park*

Main category: cs.CV

TL;DR: 本文提出针对胸部X光影像报告的特定领域大语言模型(LLM)编码器和多模态视觉-文本对齐框架，提升医学影像与文本对齐的鲁棒性和跨数据泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言预训练已促进图像-文本对齐，但医学放射学领域的报告存在高度异质、缩写、风格多样等问题，直接扩展更多、噪声更大数据反而可能降低模型性能。因此，探索能否通过更鲁棒的大模型文本编码器提升多样风格下的医学影像-文本对齐成为研究驱动力。

Method: 提出专门适配胸部X光报告的LLM编码器LLM2VEC4CXR，并与视觉主干网络结合为LLM2CLIP4CXR双塔框架。二者分别加强文本理解、提升多模态检索效果。模型在160万份多风格、异质性强的CXR报告上训练。

Result: LLM2VEC4CXR在临床文本理解、缩写与风格变化处理上优于BERT基线，报告级临床对齐性能提升。LLM2CLIP4CXR多模态检索精度和跨数据泛化性能超过以往医学CLIP变体。

Conclusion: 对于医学影像-文本对齐，模型的鲁棒性（而非规模本身）才是有效多模态学习的关键。相关模型已发布，助力医学影像-文本表征研究。

Abstract: Vision-language pretraining has advanced image-text alignment, yet progress
in radiology remains constrained by the heterogeneity of clinical reports,
including abbreviations, impression-only notes, and stylistic variability.
Unlike general-domain settings where more data often leads to better
performance, naively scaling to large collections of noisy reports can plateau
or even degrade model learning. We ask whether large language model (LLM)
encoders can provide robust clinical representations that transfer across
diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,
a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a
dual-tower framework that couples this encoder with a vision backbone.
LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,
handles abbreviations and style variation, and achieves strong clinical
alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to
boost retrieval accuracy and clinically oriented scores, with stronger
cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M
CXR studies from public and private sources with heterogeneous and noisy
reports, our models demonstrate that robustness -- not scale alone -- is the
key to effective multimodal learning. We release models to support further
research in medical image-text representation learning.

</details>


### [2] [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
*Jialiang Kang,Han Shu,Wenshuo Li,Yingjie Zhai,Xinghao Chen*

Main category: cs.CV

TL;DR: ViSpec是一种专为视觉-语言模型（VLM）设计的推理加速框架，通过高效压缩和整合图像信息，实现首个VLM推理显著加速表现。


<details>
  <summary>Details</summary>
Motivation: 现有的speculative decoding在大型语言模型中广泛用于加速推理，但在视觉-语言模型上的应用效果有限，主要加速不到1.5倍，限制了多模态大模型的推理效率。因此，亟需提升VLM中speculative decoding的效率。

Method: 提出ViSpec框架：1）设计轻量级视觉适配器，将图像token压缩成紧凑表征，并嵌入draft模型的注意力机制，保留图像位置信息；2）为每个图像提取全局特征向量，将其融合到每个后续文本token以增强多模态一致性；3）针对长助理回复的多模态数据稀缺，重新组织数据集并基于目标VLM生成长回答，规避草稿模型直接利用目标模型隐藏态的捷径学习。

Result: ViSpec在多个实验中实现了首个VLM speculative decoding的显著推理加速，优于以往所有相关方法。

Conclusion: ViSpec有效提高了视觉-语言模型推理速度，突破了现有speculative decoding方法在多模态大模型中的加速瓶颈，为相关模型大规模应用和落地提供了技术支持。

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (<1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model's attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model's hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding.

</details>


### [3] [M-PACE: Mother Child Framework for Multimodal Compliance](https://arxiv.org/abs/2509.15241)
*Shreyash Verma,Amit Kesari,Vinayak Trivedi,Anupam Purwar,Ratnesh Jamidar*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态合规检测框架M-PACE，能够高效统一处理视觉和文本内容的合规检查，并大幅降低人工审核和推理成本。


<details>
  <summary>Details</summary>
Motivation: 多模态内容的合规性检测变得越来越复杂，现有方案多为分散式多阶段流水线，维护和扩展困难，且难以适应动态规范。多模态大模型的出现为统一、简化工作流提供了可能，因此作者希望通过新架构提升合规检查效率和适应性。

Method: 提出了一种母子式多模态大模型框架M-PACE，主模型评估子模型输出，实现对跨视觉和文本输入的单步合规判断。以广告合规性为案例，开发了包含真实挑战样本的人类标注基准数据，用于结构化评估。

Result: M-PACE在广告内容合规性场景下，能涵盖15种以上属性，显著减少人为审查需求。实测推理成本较传统高性能模型下降超31倍，低至每图0.0005美元，同时保持可比准确率。

Conclusion: M-PACE提供了一种可扩展、统一且高效的多模态合规引擎，显著降低了运营成本和人工依赖，实现了在真实广告场景下的实时检验和高可靠性质量控制。

Abstract: Ensuring that multi-modal content adheres to brand, legal, or
platform-specific compliance standards is an increasingly complex challenge
across domains. Traditional compliance frameworks typically rely on disjointed,
multi-stage pipelines that integrate separate modules for image classification,
text extraction, audio transcription, hand-crafted checks, and rule-based
merges. This architectural fragmentation increases operational overhead,
hampers scalability, and hinders the ability to adapt to dynamic guidelines
efficiently. With the emergence of Multimodal Large Language Models (MLLMs),
there is growing potential to unify these workflows under a single,
general-purpose framework capable of jointly processing visual and textual
content. In light of this, we propose Multimodal Parameter Agnostic Compliance
Engine (M-PACE), a framework designed for assessing attributes across
vision-language inputs in a single pass. As a representative use case, we apply
M-PACE to advertisement compliance, demonstrating its ability to evaluate over
15 compliance-related attributes. To support structured evaluation, we
introduce a human-annotated benchmark enriched with augmented samples that
simulate challenging real-world conditions, including visual obstructions and
profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating
that a stronger parent MLLM evaluating the outputs of smaller child models can
significantly reduce dependence on human reviewers, thereby automating quality
control. Our analysis reveals that inference costs reduce by over 31 times,
with the most efficient models (Gemini 2.0 Flash as child MLLM selected by
mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5
Pro with comparable accuracy, highlighting the trade-off between cost and
output quality achieved in real time by M-PACE in real life deployment over
advertising data.

</details>


### [4] [ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images](https://arxiv.org/abs/2509.15242)
*Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习与原子力显微镜（AFM）的混合框架ProFusion，以提升大分子蛋白质复合物的三维结构预测能力。通过模拟AFM成像生成大规模多视角蛋白质数据集，并采用扩散模型与NeRF进行三维重建，实现了高精度、高效的结构预测，相较实验方法具有成本和速度优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的蛋白质结构预测方法，在处理多蛋白质复合物时因缺乏足够的三维空间信息而效果有限；而实验手段如冷冻电镜（Cryo-EM）虽准确但成本高、耗时长。因此，亟需一种高效、低成本、准确性强的新型三维结构预测方法。

Method: 提出ProFusion混合框架，融合深度学习与原子力显微镜（AFM）多视角成像。为解决AFM训练数据不足，开发出虚拟AFM仿真系统，生成约54.2万种蛋白多视角合成图像。以条件扩散模型合成新视角，利用NeRF重建三维蛋白结构，并对实验AFM图像进行验证。

Result: ProFusion重建的三维蛋白质结构，其Chamfer距离达到AFM分辨率范围内，说明空间结构准确度高。在多种蛋白复合体的实验AFM数据上经充分验证，显示良好的泛化能力和重建质量。

Conclusion: ProFusion框架能高效、低成本地预测准确的蛋白质复合物三维结构，并能够通过AFM实验实现快速、反复的结构迭代验证，展现出其在结构生物学领域的广阔应用前景。

Abstract: AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.

</details>


### [5] [Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models](https://arxiv.org/abs/2509.15243)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 该论文提出了MMEL框架，通过多尺度特征处理、自适应注意力加权和跨模态对齐，提升视觉-语言模型的可解释性及性能，实现更精准的视觉解释。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在自动图像分析领域进步显著，但其在安全关键场景的应用受限于复杂的对象关系、细微视觉线索及高透明度与可靠性需求。因此，提升模型的可解释性成为核心问题。

Method: 论文基于梯度解释（Grad-eclip）工作，提出了层次化语义关系模块，结合多尺度语义特征处理、可学习的层别加权、跨模态对齐。模型在不同语义层次处理特征，平衡各层贡献，从而捕捉图像不同区域关系，生成全面的解释。

Result: 实验结果表明，方法将语义关系信息融合到梯度归因图中后，能生成更加聚焦且具上下文感知的可视化解释，更好地反映模型对复杂场景的处理过程。

Conclusion: MMEL框架可泛化至多个领域，并为高解释性及高可靠性场景下模型决策提供有价值的洞察。

Abstract: Recent advances in vision-language models have significantly expanded the
frontiers of automated image analysis. However, applying these models in
safety-critical contexts remains challenging due to the complex relationships
between objects, subtle visual cues, and the heightened demand for transparency
and reliability. This paper presents the Multi-Modal Explainable Learning
(MMEL) framework, designed to enhance the interpretability of vision-language
models while maintaining high performance. Building upon prior work in
gradient-based explanations for transformer architectures (Grad-eclip), MMEL
introduces a novel Hierarchical Semantic Relationship Module that enhances
model interpretability through multi-scale feature processing, adaptive
attention weighting, and cross-modal alignment. Our approach processes features
at multiple semantic levels to capture relationships between image regions at
different granularities, applying learnable layer-specific weights to balance
contributions across the model's depth. This results in more comprehensive
visual explanations that highlight both primary objects and their contextual
relationships with improved precision. Through extensive experiments on
standard datasets, we demonstrate that by incorporating semantic relationship
information into gradient-based attribution maps, MMEL produces more focused
and contextually aware visualizations that better reflect how vision-language
models process complex scenes. The MMEL framework generalizes across various
domains, offering valuable insights into model decisions for applications
requiring high interpretability and reliability.

</details>


### [6] [Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)
*Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke*

Main category: cs.CV

TL;DR: 本文提出了一种名为NAP（Navigation-Aware Pruning）的新型token剪枝方法，用于提高视觉与语言导航（VLN）任务中模型推理的效率，同时显著减少运算量，几乎不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 视觉与语言导航（VLN）需要模型理解视觉和语言信息，以完成路径规划等复杂任务。虽然大模型在该任务上表现优秀，但推理成本高，难以应用于资源有限的设备。现有token剪枝方法若处理不当，反而导致导航步数增多，综合成本上升。因此，迫切需要一种结合导航任务特性的高效剪枝方案。

Method: 提出NAP方法，首先利用导航领域特有信号将tokens预分类成前景和背景，例如仅保留导航可能方向的图像视角和与导航相关的指令（通过大语言模型提取）。剪枝主要针对背景tokens，最大限度减少信息损失。此外，通过消除低重要性导航节点，减少回溯操作，防止导航路径变长。

Result: 在VLN标准基准测试中，NAP方法在保证较高成功率的同时，运算量（FLOPS）减少超过50%，性能明显优于现有剪枝方法。

Conclusion: NAP有效结合了VLN任务特性，实现了高效剪枝和导航性能的双重提升，为资源受限条件下部署大模型提供了可行方案。

Abstract: Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.

</details>


### [7] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出了名为RespoDiff的新框架，实现了在高质量文本到图像生成中同时兼顾公平性和安全性，保持语义一致性且无明显画质损失。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在文本到图像生成中的迅猛发展，如何在高语义丰富度和高质量下实现公平性与安全性的生成仍是未解难题。已有方法常导致图像舍弃部分质量或语义一致性以提升公平与安全。

Method: 提出RespoDiff框架，在扩散模型的瓶颈中插入两模块：一模块专注于公平性和安全性等负责任概念，另一模块用于保持与中性文本的语义一致性，并通过创新的score-matching目标函数协同优化两模块。

Result: RespoDiff在多样、未见过的文本生成任务上可提升20%的负责任和语义一致生成表现，同时无损图像质量，优于当前主流方法，并可无缝集成到大规模模型（如SDXL）。

Conclusion: 该方法能有效提升图像生成模型的公平性、安全性与语义对齐表现，并不会牺牲图像质量，标志着负责任AI生成向前进一步。

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.

</details>


### [8] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: 本文分析了自动引导（autoguidance）和联合样本选择（JEST）等自动数据甄选方法对生成扩散模型训练效率的提升效果。结果显示，自动引导可以显著提升样本质量和多样性，而在多数情况下，其高性价比优于复杂度更高的选择方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型计算成本不断增长，如何高效利用数据进行训练成为亟待解决的问题。作者希望借助自动化的数据筛选和指引方法，提升生成扩散模型的训练样本效率和时间效率。

Method: 作者将联合样本选择（JEST）和自动引导算法整合到同一代码框架中，便于对比与消融实验。实验分别在2维合成数据任务和高维图像生成任务（3x64x64）上进行。对比指标包括相同的总训练时长和样本量，且明确计算了数据选择过程的时间开销。

Result: 实验发现，autoguidance在所有实验中都可以稳定提升生成样本的质量和多样性。早期使用AJEST（只在训练初期进行样本筛选）能在数据利用效率上达到或略高于仅用自动引导，但其时间成本和复杂性较高。总体看，autoguidance或均匀随机采样更具实际应用价值。

Conclusion: 针对生成扩散模型训练，定向的数据在线筛选仅在训练早期可能带来效益，但实际提升主要来源于自动引导方法。高效和高质量训练应优先考虑自动引导而非复杂的数据筛选。并指出了这些方法的适用范围及局限性。

Abstract: The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.

</details>


### [9] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: 本文提出了一种名为PRISM的新方法，用于识别和归属AI生成图像的来源模型，并展示了其在多项基准数据集上的高准确率。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI广泛应用的背景下，识别生成内容来源模型具有重要应用价值，尤其在商业场景下，用户付费后对内容源头的信任需求显著。然而，目前缺乏有效、可扩展的模型归属技术。

Method: 提出PRISM方法：利用离散傅里叶变换的幅值和相位信息，进行径向降维，提取图像的模型专有指纹；随后使用线性判别分析进行聚类归属。方法无需访问生成模型内部细节，适用于多模型环境。为验证效果，构建了包含36,000张图像的新数据集PRISM-36K。

Result: 在PRISM-36K数据集上，PRISM归属准确率为92.04%；在四个公开基准数据集上平均准确率81.60%；在真假图像二分类任务中，平均准确率为88.41%，对GenImage基准的准确率高达95.06%。

Conclusion: 结果表明，基于频域特征的指纹方法在不同架构和数据集间表现出色，为生成式AI模型归属和内容可信提供了切实可行的技术方案，有助于提升生成式AI的可追溯性与信任度。

Abstract: A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.

</details>


### [10] [Large Vision Models Can Solve Mental Rotation Problems](https://arxiv.org/abs/2509.15271)
*Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen*

Main category: cs.CV

TL;DR: 本文系统地评估了几种主流视觉Transformer模型在心理旋转任务中的表现，分析它们是否具备支持空间推理的能力，并探究其表征特性。


<details>
  <summary>Details</summary>
Motivation: 心理旋转任务是人类空间推理的重要测试，历史上对理解知觉与认知关系具有核心作用。目前虽然Vision Transformer(ViT)等深度模型取得了巨大成功，但它们是否发展出类似人类的空间推理能力仍未明晰，因此需要系统性的评估。

Method: 作者选择了四种视觉Transformer模型（ViT、CLIP、DINOv2、DINOv3），在不同难度的心理旋转任务（包括类似Shepard和Metzler实验的简单积木、复杂结构、不同类型文本和写实物体）上进行测试，并对模型的不同层次表征进行细致分析。

Result: 1）自监督训练的ViT在几何结构捕捉上优于有监督ViT。2）模型中间层比最后一层更擅长处理空间旋转任务。3）随着旋转任务复杂度和遮挡程度增加，模型表现趋于下降，这与人类反应时增长类似，表明在嵌入空间存在相近的约束。

Conclusion: 视觉Transformer部分具备人类心理旋转能力的表征特性，尤其是自监督ViT和中间层结构。复杂度和遮挡对空间表征提出共同挑战，暗示未来模型在空间推理能力上有进一步提升空间。

Abstract: Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.

</details>


### [11] [Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks](https://arxiv.org/abs/2509.15272)
*Yannis Kaltampanidis,Alexandros Doumanoglou,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本文系统性评估了未经额外变换层处理的ViT预训练特征在分类和分割任务中的表现，比较不同token类型和决策规则在不同任务下的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前自监督ViT特征常被进一步转换以提升泛化表现，但缺乏对其原始表征能力的深入剖析。本文旨在填补这一空白，分析未经加工的ViT特征是否内在具备足够可用的信息。

Method: 选取对比学习与掩码图像建模两大自监督训练方式获得的ViT模型，直接利用最后一层变换块及FFN输出的特征，以超平面（如逻辑回归）和余弦相似性为分类/分割规则，评估其在标准与小样本分类分割上的表现，无加额外特征头或蒸馏过程。

Result: 梳理了不同token类型（如key、query、value、FFN输出）及判别规则在分类和分割任务下的优劣，对比实验覆盖两大主流数据集，揭示了任务、类型与规则之间的最佳组合关系。

Conclusion: 原始ViT自监督特征在某些配置下即可获得良好表现，最佳token类型及判别准则依赖于下游任务、样本量和预训练目标，为轻量下游适配给出指导建议。

Abstract: Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently
demonstrated considerable potential as a pre-training strategy for a variety of
computer vision tasks, including image classification and segmentation, both in
standard and few-shot downstream contexts. Two pre-training objectives dominate
the landscape of SSL techniques: Contrastive Learning and Masked Image
Modeling. Features (or tokens) extracted from the final transformer attention
block -- specifically, the keys, queries, and values -- as well as features
obtained after the final block's feed-forward layer, have become a common
foundation for addressing downstream tasks. However, in many existing
approaches, these pre-trained ViT features are further processed through
additional transformation layers, often involving lightweight heads or combined
with distillation, to achieve superior task performance. Although such methods
can improve task outcomes, to the best of our knowledge, a comprehensive
analysis of the intrinsic representation capabilities of unaltered ViT features
has yet to be conducted. This study aims to bridge this gap by systematically
evaluating the use of these unmodified features across image classification and
segmentation tasks, in both standard and few-shot contexts. The classification
and segmentation rules that we use are either hyperplane based (as in logistic
regression) or cosine-similarity based, both of which rely on the presence of
interpretable directions in the ViT's latent space. Based on the previous rules
and without the use of additional feature transformations, we conduct an
analysis across token types, tasks, and pre-trained ViT models. This study
provides insights into the optimal choice for token type and decision rule
based on the task, context, and the pre-training objective, while reporting
detailed findings on two widely-used datasets.

</details>


### [12] [How Good are Foundation Models in Step-by-Step Embodied Reasoning?](https://arxiv.org/abs/2509.15293)
*Dinura Dissanayake,Ahmed Heakl,Omkar Thawakar,Noor Ahsan,Ritesh Thawkar,Ketan More,Jean Lahoud,Rao Anwer,Hisham Cholakkal,Ivan Laptev,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了FoMER基准，用于评估大规模多模态模型（LMMs）在实际物理世界中进行具身决策推理的能力。该基准涵盖10种任务、8种具身环境和3类机器人，共包含1,100多个带推理步骤的样本。对主流LMM的实验揭示了其在具身推理上的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模多模态模型在视觉理解和自然语言生成方面取得显著进展，但其在现实世界任务中分步骤具身推理能力的研究仍然不足。为深入理解其在实际环境下的推理能力，作者提出创建一个专门面向具身推理的公开基准。

Method: 作者建立了FoMER基准，收集了多种涉及物理约束与安全性的复杂具身任务，并设计了新的评价框架，通过区分感知理解与动作推理两个环节，系统测试并分析多个主流LMM模型在这些任务下的表现。

Result: 通过FoMER基准的实验分析显示，尽管LMMs在感知和语言生成方面表现优异，但在复杂具身推理和决策生成方面仍存在明显局限性。详细实验数据揭示了模型在多模态信息推理时出现的主要失败和挑战点。

Conclusion: 当前LMMs在具身推理方面虽展现潜力，但仍有显著改进空间。本文的基准和分析为后续机器人智能领域的多模态推理研究指明了方向，并将推动相关技术的发展。数据和代码计划公开，促进社区合作。

Abstract: Embodied agents operating in the physical world must make decisions that are
not only effective but also safe, spatially coherent, and grounded in context.
While recent advances in large multimodal models (LMMs) have shown promising
capabilities in visual understanding and language generation, their ability to
perform structured reasoning for real-world embodied tasks remains
underexplored. In this work, we aim to understand how well foundation models
can perform step-by-step reasoning in embodied environments. To this end, we
propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to
evaluate the reasoning capabilities of LMMs in complex embodied decision-making
scenarios. Our benchmark spans a diverse set of tasks that require agents to
interpret multimodal observations, reason about physical constraints and
safety, and generate valid next actions in natural language. We present (i) a
large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation
framework that disentangles perceptual grounding from action reasoning, and
(iii) empirical analysis of several leading LMMs under this setting. Our
benchmark includes over 1.1k samples with detailed step-by-step reasoning
across 10 tasks and 8 embodiments, covering three different robot types. Our
results highlight both the potential and current limitations of LMMs in
embodied reasoning, pointing towards key challenges and opportunities for
future research in robot intelligence. Our data and code will be made publicly
available.

</details>


### [13] [CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization](https://arxiv.org/abs/2509.15330)
*Min Zhang,Bo Jiang,Jie Zhou,Yimeng Liu,Xin Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的条件域提示学习（CoDoL）方法，利用领域信息加强视觉-语言嵌入对齐，提高CLIP等模型在分布外（OOD）数据上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然基于提示的CLIP方法在视觉-语言领域表现优秀，但由于文本描述不准确和视觉-语言嵌入对齐有限，其零样本能力和泛化效果受限。作者旨在解决这些关键瓶颈。

Method: 提出了CoDoL方法，利用易得的领域信息来生成Prompt，并通过设计轻量级的域元网络（DMN），为每个领域生成输入相关的Token，从而结合实例性和领域性特征。

Result: 在PACS、VLCS、OfficeHome和DigitDG四个OOD数据集上进行大量实验，结果表明CoDoL在视觉-语言对齐和分布外泛化能力上均优于现有方法。

Conclusion: CoDoL能够有效解决当前CLIP等视觉-语言模型在分布外场景下的泛化挑战，为零样本学习等任务提供了更强的支撑。

Abstract: Recent advances in pre-training vision-language models (VLMs), e.g.,
contrastive language-image pre-training (CLIP) methods, have shown great
potential in learning out-of-distribution (OOD) representations. Despite
showing competitive performance, the prompt-based CLIP methods still suffer
from: i) inaccurate text descriptions, which leads to degraded accuracy and
robustness, and poses a challenge for zero-shot CLIP methods. ii) limited
vision-language embedding alignment, which significantly affects the
generalization performance. To tackle the above issues, this paper proposes a
novel Conditional Domain prompt Learning (CoDoL) method, which utilizes
readily-available domain information to form prompts and improves the
vision-language embedding alignment for improving OOD generalization. To
capture both instance-specific and domain-specific information, we further
propose a lightweight Domain Meta Network (DMN) to generate input-conditional
tokens for images in each domain. Extensive experiments on four OOD benchmarks
(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed
CoDoL in terms of improving the vision-language embedding alignment as well as
the out-of-distribution generalization performance.

</details>


### [14] [Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception](https://arxiv.org/abs/2509.15333)
*Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 文章提出了一种新的主动视觉模型框架AdaptiveNN，通过模仿人类视觉的逐步关注机制，实现了更高效、更灵活的计算机视觉方法，并在众多任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流机器视觉模型一次性处理全图，导致资源消耗大，难以在高分辨率和大模型下实现高效应用，限制了未来进步。人类视觉则善于有选择地关注任务相关区域，具有高度适应性。本文旨在借鉴人类视觉，提出主动、适应性的视觉模型。

Method: 提出AdaptiveNN框架，将视觉感知建模为逐步粗到细的决策过程，依次关注与任务相关区域，通过多次“注视”并逐步整合信息，判定何时完成观察。结合表征学习与自奖励强化学习理论，实现对非可微结构的端到端训练，无需人工注视点数据。

Result: 在17个数据集、9项任务（视觉识别、精细分类、视觉搜索、自动驾驶与医学影像、语言理解等）上评估AdaptiveNN。该方法在准确率不损失的情况下，将推理成本最高降低28倍，并可根据任务与资源自适应调整。通过注视模式实现更强的可解释性。

Conclusion: AdaptiveNN展现了高效、灵活、可解释的计算机视觉新途径，并在多种情境下表现出接近人类的知觉行为，有助于视觉认知研究，被认为是下一代视觉模型的有力候选。

Abstract: Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.

</details>


### [15] [LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition](https://arxiv.org/abs/2509.15342)
*Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新的高效扩散模型LowDiff，可在保持甚至提升图像生成质量的同时，大幅提高采样速度。LowDiff利用级联式的多分辨率生成和统一模型架构，有效减少高分辨率采样步骤，适用于像素空间和潜在空间的扩散模型。实验显示在主流数据集上提升超50%效率，品质优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成高质量图像，但因采样速度慢阻碍实际应用。过去提升效率的方法主要集中在模型压缩或减少去噪步数，鲜有关注利用多分辨率输入带来的潜力。为此，本文尝试探索多分辨率参与的方案，提高实际采样效率。

Method: 提出LowDiff框架，通过级联式（cascaded）的方式生成从低到高分辨率的图像，采用统一模型进行逐步精细化，配合新的模型架构和生成技巧，显著减少高分辨率步骤。支持在像素空间和潜在空间的扩散模型应用。

Result: 在CIFAR-10、FFHQ、ImageNet等条件/非条件生成任务上，LowDiff在所有数据集和设置下采样吞吐量提升超50％，同时生成质量达到或超过主流方法。如在CIFAR-10上FID为1.94~2.11，IS为9.87~10.03，FFHQ 64x64上FID为2.43，ImageNet 256x256上FID达4.00, IS达195.06。

Conclusion: LowDiff显著提升了扩散模型的生成效率，在不损失甚至提升图像生成质量的前提下，减少高分辨率采样步骤，具有良好的适应性和泛化性，为实际扩散模型应用带来新路径。

Abstract: Diffusion models have achieved remarkable success in image generation but
their practical application is often hindered by the slow sampling speed. Prior
efforts of improving efficiency primarily focus on compressing models or
reducing the total number of denoising steps, largely neglecting the
possibility to leverage multiple input resolutions in the generation process.
In this work, we propose LowDiff, a novel and efficient diffusion framework
based on a cascaded approach by generating increasingly higher resolution
outputs. Besides, LowDiff employs a unified model to progressively refine
images from low resolution to the desired resolution. With the proposed
architecture design and generation techniques, we achieve comparable or even
superior performance with much fewer high-resolution sampling steps. LowDiff is
applicable to diffusion models in both pixel space and latent space. Extensive
experiments on both conditional and unconditional generation tasks across
CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our
method. Results show over 50% throughput improvement across all datasets and
settings while maintaining comparable or better quality. On unconditional
CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional
CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an
FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1
produces high-quality samples with a FID of 4.00 and an IS of 195.06, together
with substantial efficiency gains.

</details>


### [16] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: 本文提出了一种用于Stable Diffusion XL（SDXL）的新区域级门控机制——MaskAttn-SDXL，有效解决了文本生成图像模型在多对象、属性和空间关系描述下的合成功能失败问题，提升了生成图片在空间和属性结合方面的准确性，并且对原有模型几乎无额外开销。


<details>
  <summary>Details</summary>
Motivation: 当前的文本生成图像扩散模型在面对包含多个对象、属性以及空间关系的提示词时，常常因“跨token干扰”导致内容混杂、属性错配及空间关系失真。这类合成问题影响了文本到图像模型的实用性和生成质量，因此亟需方法提升其复杂组合语句下的表现。

Method: 文章提出MaskAttn-SDXL，通过在SDXL的UNet每层cross-attention logit上引入二值掩码，在softmax前稀疏化token与潜空间的交互，只保留语义相关连接。该方法无需加入额外的位置编码、辅助token或外部区域掩码，实现简单并可高效集成。

Result: MaskAttn-SDXL在实际应用中显著提升了多对象提示下的空间一致性和属性绑定能力，同时保持了图像的整体质量和多样性。

Conclusion: 跨attention logit级的掩码机制是实现复杂生成任务组合控制的高效方式。所提方法为文本到图像生成任务带来了实用且低成本的空间控制扩展。

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.

</details>


### [17] [RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation](https://arxiv.org/abs/2509.15391)
*Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 本论文提出了RaceGAN，一种无需参考图像即可实现多域种族特征转换的新型生成对抗网络框架。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的GAN模型（如CycleGAN和StarGAN等）已经在图像风格转换方面取得进展，但它们要么仅限于两个域，要么无法处理更细致的风格、个体特征保持或需要额外的参考图像。为更好地解决种族特征转换中的以上问题，作者提出了RaceGAN。

Method: RaceGAN通过新颖的编码结构和训练方法，实现了在多个种族属性（如亚裔、白人、黑人）之间的风格映射，同时保持输入个体的独特性和高级语义，不依赖辅助参考图像。

Result: 在Chicago Face Dataset上，RaceGAN在种族特征翻译任务中的表现优于当前主流方法。通过InceptionReNetv2分类器的定量分析表明，该方法翻译效果更佳。作者还展示了模型能够有效将潜空间划分为各族群相应的特征簇。

Conclusion: RaceGAN能够在无需参考图像的条件下，实现多种族特征高质量转换，并保持个体性和高语义信息，提升了多域人脸图像翻译的性能。

Abstract: Generative adversarial networks (GANs) have demonstrated significant progress
in unpaired image-to-image translation in recent years for several
applications. CycleGAN was the first to lead the way, although it was
restricted to a pair of domains. StarGAN overcame this constraint by tackling
image-to-image translation across various domains, although it was not able to
map in-depth low-level style changes for these domains. Style mapping via
reference-guided image synthesis has been made possible by the innovations of
StarGANv2 and StyleGAN. However, these models do not maintain individuality and
need an extra reference image in addition to the input. Our study aims to
translate racial traits by means of multi-domain image-to-image translation. We
present RaceGAN, a novel framework capable of mapping style codes over several
domains during racial attribute translation while maintaining individuality and
high level semantics without relying on a reference image. RaceGAN outperforms
other models in translating racial features (i.e., Asian, White, and Black)
when tested on Chicago Face Dataset. We also give quantitative findings
utilizing InceptionReNetv2-based classification to demonstrate the
effectiveness of our racial translation. Moreover, we investigate how well the
model partitions the latent space into distinct clusters of faces for each
ethnic group.

</details>


### [18] [Generating Part-Based Global Explanations Via Correspondence](https://arxiv.org/abs/2509.15393)
*Kunal Rathore,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 该论文提出了一种结合少量用户定义部位标注和高效扩展的方法，实现对深度学习模型决策的人类可理解大规模符号化解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的可解释性不足，现有方法要么过于局部、只针对单张图片，要么要求全面的标注，导致高昂的人工成本。

Method: 方法利用少量图片中的用户定义部位标签，通过高效迁移机制将这些标签扩展到大规模数据集，汇聚由部位驱动的本地解释，实现全局符号化解释。

Result: 该方法能够在仅需少量标注下，实现大规模图片的符号化解释，大幅降低人工标签成本，同时提升解释的全局性和可理解性。

Conclusion: 论文证明了利用有限的部位标签可以有效生成深度学习模型决策的全局、人类可理解的解释，有助于推动模型可解释性领域。

Abstract: Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.

</details>


### [19] [Causal Fingerprints of AI Generative Models](https://arxiv.org/abs/2509.15406)
*Hui Xu,Chi Liu,Congcong Zhu,Minghao Wang,Youyang Qu,Longxiang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成模型指纹概念——因果指纹，通过因果性解耦框架，从预训练扩散重建残差获得语义不变潜空间，实现对生成模型来源的更准确归属。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型指纹（用于来源归属）的检测方法常基于特定模型特点或合成瑕疵，泛化能力有限，难以跨多种模型类型稳定识别。作者认为理想的指纹应体现生成图像来源与模型痕迹之间的因果关系。

Method: 提出因果解耦（causality-decoupling）框架，将模型指纹与图像内容与风格在预训练扩散模型的残差中分离。在潜空间中抽取具有多样表示能力的指纹特征，并用于提升指纹细粒度。

Result: 在主流GAN和扩散模型上进行了归属性能测试，并通过因果指纹生成反事实样本实现来源匿名化。实验结果表明该方法在模型归属任务上优于现有方案。

Conclusion: 该研究为指纹泛化、伪造检测、模型溯源和身份保护等应用开辟了新方向，提升了生成图像归属的准确性和安全性。

Abstract: AI generative models leave implicit traces in their generated images, which
are commonly referred to as model fingerprints and are exploited for source
attribution. Prior methods rely on model-specific cues or synthesis artifacts,
yielding limited fingerprints that may generalize poorly across different
generative models. We argue that a complete model fingerprint should reflect
the causality between image provenance and model traces, a direction largely
unexplored. To this end, we conceptualize the \emph{causal fingerprint} of
generative models, and propose a causality-decoupling framework that
disentangles it from image-specific content and style in a semantic-invariant
latent space derived from pre-trained diffusion reconstruction residual. We
further enhance fingerprint granularity with diverse feature representations.
We validate causality by assessing attribution performance across
representative GANs and diffusion models and by achieving source anonymization
using counterfactual examples generated from causal fingerprints. Experiments
show our approach outperforms existing methods in model attribution, indicating
strong potential for forgery detection, model copyright tracing, and identity
protection.

</details>


### [20] [NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training](https://arxiv.org/abs/2509.15416)
*Moinak Bhattacharya,Angelica P. Kurtz,Fabio M. Iwamoto,Prateek Prasanna,Gagandeep Singh*

Main category: cs.CV

TL;DR: 本文提出了一种针对神经肿瘤领域的基础模型（FM）结合分布鲁棒（DRO）损失的新方法，在跨机构脑肿瘤MRI数据上显著提升了常见和罕见分子标志物及生存预测的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型难以处理神经肿瘤高度异质的数据和复杂的分子标志物，尤其在预测不常见分子标志物上表现较差，限制了精准医疗的发展。

Method: 研究者开发了一个特定于神经肿瘤的基础模型，利用自监督预训练骨干网络（如BYOL、DINO、MAE、MoCo），并结合分布鲁棒优化（DRO）方法，解决不同机构和类别分布不均的问题，提升跨机构泛化能力。

Result: 该方法在多个数据库和机构上（UCSF、UPenn、CUIMC）测试，多项分子标志物预测和生存分析任务中均有显著提高，特别是在样本不足的分子标志物上提升最为明显，且模型特征具有较好可解释性。

Conclusion: 结合分布鲁棒优化的基础模型能生成更具泛化性和鲁棒性的神经肿瘤表征，提升分子分型和生存预测性能，为精准神经肿瘤学提供新工具，但需进一步前瞻性验证与多模态信号融合。

Abstract: Neuro-oncology poses unique challenges for machine learning due to
heterogeneous data and tumor complexity, limiting the ability of foundation
models (FMs) to generalize across cohorts. Existing FMs also perform poorly in
predicting uncommon molecular markers, which are essential for treatment
response and risk stratification. To address these gaps, we developed a
neuro-oncology specific FM with a distributionally robust loss function,
enabling accurate estimation of tumor phenotypes while maintaining
cross-institution generalization. We pretrained self-supervised backbones
(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied
distributionally robust optimization (DRO) to mitigate site and class
imbalance. Downstream tasks included molecular classification of common markers
(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),
continuous markers (Ki-67, TP53), and overall survival prediction in IDH1
wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular
prediction and reduced site-specific embedding differences. At CUIMC, mean
balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with
the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to
0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).
For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647
to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral
regions, confirming interpretability. Overall, coupling FMs with DRO yields
more site-invariant representations, improves prediction of common and uncommon
markers, and enhances survival discrimination, underscoring the need for
prospective validation and integration of longitudinal and interventional
signals to advance precision neuro-oncology.

</details>


### [21] [ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2509.15435)
*Chung-En Johnny Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: 本文提出了ORCA框架，通过引入“小型视觉模型套件”和结构化推理过程，有效提升大规模视觉-语言模型（LVLMs）的事实准确性和抗攻击能力，无需模型重训练，对现有LVLM应用具有实际价值。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态任务中表现优异，但易出现事实性幻觉及对抗攻击，影响其在真实环境中的可靠性。研究动机在于提升LVLMs的准确性和安全性，使其更适合实际应用。

Method: 提出了ORCA框架，包含Observe--Reason--Critique--Act（观察-推理-评估-行动）循环，利用多个小型视觉模型提出证据性问题、验证不一致、迭代优化输出，无需访问LVLM内部或额外训练。ORCA还记录中间推理过程，支持追溯决策。

Result: 在POPE和AMBER等基准上，ORCA大幅提升LVLMs在干净及对抗扰动图像下的准确率，提升幅度从+1.2%到+48.00%不等，展现优异的提升效果。

Conclusion: ORCA显著提升了LVLM的事实准确性和鲁棒性，为构建可审计、可靠的多模态系统提供了一条有前景的技术路径。

Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.

</details>


### [22] [Region-Aware Deformable Convolutions](https://arxiv.org/abs/2509.15436)
*Abolfazl Saheban Maleki,Maryam Imani*

Main category: cs.CV

TL;DR: 论文提出了一种新的卷积算子——Region-Aware Deformable Convolution（RAD-Conv），可以动态调整感受野的形状与大小，从而提升图像结构适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统的可变形卷积受限于固定的采样区域，难以灵活适应复杂的图像结构，且模型在表现力和计算效率间存在权衡。

Method: RAD-Conv为每个卷积核元素引入四个边界偏移，通过这些偏移灵活形成矩形采样区域，使感受野宽高可控，且不受卷积核结构限制。这种做法结合了注意力机制的适应性与卷积的高效性。

Result: RAD-Conv能够有效捕捉局部细节和远程依赖，即使使用1x1小卷积核，也能实现表达能力的增强，并提升了模型效率。

Conclusion: RAD-Conv为视觉模型设计提供了更具表现力且高效的卷积算子，在保留卷积结构高效性的同时，兼具了注意力机制的灵活性和表达能力。

Abstract: We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.

</details>


### [23] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: CAGE提出了一种基于边的全新框架，将点云密度图直接重建为向量化地面平图，并在准确性和鲁棒性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于角点或线段分组的方法易受噪声和不完整观测影响，导致地面平图碎片化或不真实，且细节恢复能力有限。

Method: 提出以边为核心的连续性描述，使用有向边表示墙体，通过双查询Transformer解码器和去噪技术实现结构连续和优化稳定。

Result: 在Structured3D和SceneCAD数据集上，CAGE在房间、角点和角度的F1分数分别达99.1%、91.7%和89.3%，且具备良好的跨数据集泛化能力。

Conclusion: CAGE通过创新架构显著提升室内平面重建的准确性和鲁棒性，优于现有方法，未来将发布代码和模型。

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.

</details>


### [24] [Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture](https://arxiv.org/abs/2509.15470)
*Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman*

Main category: cs.CV

TL;DR: 本文提出利用自监督学习和多模态数据提升肺结节诊断模型，在内部验证集优于现有模型，但外部泛化存在不足。


<details>
  <summary>Details</summary>
Motivation: 多模态肺结节诊断模型因标注数据稀缺和容易过拟合，难以获得良好性能。作者意在探索无标注多模态医学档案，通过自监督学习提升模型泛化能力和诊断准确性。

Method: 作者收集无标注CT影像和相应电子健康档案，采用JEPA（联合嵌入预测架构）进行自监督预训练，随后通过有监督微调，并在内部和外部数据集上进行性能评估。此外，通过合成环境分析JEPA方法潜在的性能不足之处。

Result: 在内部数据集上，新模型（AUC 0.91）优于未正则化多模态模型（AUC 0.88）和仅使用影像的模型（AUC 0.73）；但在外部数据集中，模型表现反而弱于单影像模型（0.72 vs 0.75）。

Conclusion: 作者提出的方法能有效利用无标注的多模态医学数据提升内部预测能力，但外部泛化能力有限，并通过合成环境分析其适用场景和局限性。

Abstract: The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.

</details>


### [25] [Efficient Multimodal Dataset Distillation via Generative Models](https://arxiv.org/abs/2509.15472)
*Zhenghao Zhao,Haoxuan Wang,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: 提出了一种名为EDGE的高效多模态数据集蒸馏生成方法，实现对图像-文本数据集的有效压缩且远快于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多模态大语言模型的兴起，多模态（尤其是图文）数据集的重要性显著提升。现有多模态数据集蒸馏方法计算消耗大、效率低，限制了实际应用。

Method: 提出EDGE生成式蒸馏方法，通过生成模型进行多模态数据压缩。两大创新点包括：1）引入双向对比损失解决图像与文本描述不相关的问题；2）引入多样性损失提升生成样本的多样性。此外，还设计了改进的描述文本合成策略提升文本-图像检索性能。

Result: 在Flickr30K、COCO和CC3M等主流数据集上进行实验证明，方法在性能和效率上均超越现有同类方法，速度提升高达18倍。

Conclusion: EDGE方法为高效多模态数据集蒸馏提供了新思路，在保证模型精度的前提下，大幅度提升了数据压缩和处理效率。

Abstract: Dataset distillation aims to synthesize a small dataset from a large dataset,
enabling the model trained on it to perform well on the original dataset. With
the blooming of large language models and multimodal large language models, the
importance of multimodal datasets, particularly image-text datasets, has grown
significantly. However, existing multimodal dataset distillation methods are
constrained by the Matching Training Trajectories algorithm, which
significantly increases the computing resource requirement, and takes days to
process the distillation. In this work, we introduce EDGE, a generative
distillation method for efficient multimodal dataset distillation.
Specifically, we identify two key challenges of distilling multimodal datasets
with generative models: 1) The lack of correlation between generated images and
captions. 2) The lack of diversity among generated samples. To address the
aforementioned issues, we propose a novel generative model training workflow
with a bi-directional contrastive loss and a diversity loss. Furthermore, we
propose a caption synthesis strategy to further improve text-to-image retrieval
performance by introducing more text information. Our method is evaluated on
Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and
efficiency compared to existing approaches. Notably, our method achieves
results 18x faster than the state-of-the-art method.

</details>


### [26] [OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data](https://arxiv.org/abs/2509.15479)
*Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt*

Main category: cs.CV

TL;DR: OpenViGA是一个面向自动驾驶场景的视频生成系统，基于开源模型，并公开代码与数据，实现了高效且可复现的结果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶视频生成系统模型庞大，训练资源消耗巨大，且缺乏公开的代码和数据集，难以复现和深入理解关键设计。本文通过构建一个完全开源、基于公开数据和模型的系统来解决这些问题，推动领域发展。

Method: 系统分为三个独立组件：图像tokenizer、世界模型和视频解码器，并针对每个组件进行了量化和定性的分析与优化。所有模块均采用高性能的开源预训练模型，并利用公开的BDD100K自动驾驶数据集进行微调。此外，通过优化组件间接口，提升了整系统协同效率，并实现了低延迟的视频生成。

Result: 系统在256x256分辨率、4帧/秒下，仅需一帧延迟即可逐帧预测生成真实感较强的驾驶场景视频。各组成模块性能得到了详细验证，证实了系统的有效性和实用性。

Conclusion: OpenViGA实现了一个可公开复现、面向自动驾驶的高效视频生成框架，它验证了开源资源的强大，能为学术界和工业界提供标准、可验证的平台，对后续研究具有重要推动作用。

Abstract: Recent successful video generation systems that predict and create realistic
automotive driving scenes from short video inputs assign tokenization, future
state prediction (world model), and video decoding to dedicated models. These
approaches often utilize large models that require significant training
resources, offer limited insight into design choices, and lack publicly
available code and datasets. In this work, we address these deficiencies and
present OpenViGA, an open video generation system for automotive driving
scenes. Our contributions are: Unlike several earlier works for video
generation, such as GAIA-1, we provide a deep analysis of the three components
of our system by separate quantitative and qualitative evaluation: Image
tokenizer, world model, video decoder. Second, we purely build upon powerful
pre-trained open source models from various domains, which we fine-tune by
publicly available automotive data (BDD100K) on GPU hardware at academic scale.
Third, we build a coherent video generation system by streamlining interfaces
of our components. Fourth, due to public availability of the underlying models
and data, we allow full reproducibility. Finally, we also publish our code and
models on Github. For an image size of 256x256 at 4 fps we are able to predict
realistic driving scene videos frame-by-frame with only one frame of
algorithmic latency.

</details>


### [27] [Comparing Computational Pathology Foundation Models using Representational Similarity Analysis](https://arxiv.org/abs/2509.15482)
*Vaibhav Mishra,William Lotter*

Main category: cs.CV

TL;DR: 本研究系统性分析了六种计算病理领域基础模型的表征空间，揭示表征结构的多样性及训练范式对模型表征的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在计算病理领域被广泛用于多项任务，但其内部表征结构和变异性鲜有深入研究。理解这些表征结构有助于提升模型的鲁棒性和应用效果。

Method: 分析了三种视觉-语言对比学习（CONCH, PLIP, KEEP）和三种自蒸馏（UNI2，Virchow2，Prov-GigaPath）方法的模型。采用了计算神经科学中的表征相似性分析（RSA），并以TCGA病理切片图像为数据，考察了模型间和模型内不同因素（切片、疾病、染色标准化等）的影响。

Result: UNI2和Virchow2的表征结构最为独特，Prov-GigaPath与其他模型的平均相似性最高。同一训练范式下模型的表征相似度未必更高。所有模型的表征均强烈依赖切片本身，对疾病类别依赖较低，染色标准化可不同程度减弱切片依赖。视觉-语言模型的表征更紧凑，视觉-only模型表征更分散。

Conclusion: 工作揭示了现有基础模型表征结构的共性与差异，为提升模型对切片特异特征的鲁棒性、促进模型集成和理解训练范式对表征的影响提供了参考，该分析方法适用于更广泛医疗影像模型的评估。

Abstract: Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.

</details>


### [28] [SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters](https://arxiv.org/abs/2509.15490)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: SmolRGPT是一种小巧高效的视觉-语言模型，在保持出色空间推理能力的同时，大幅减少了计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉-语言模型需要极大算力和内存，难以在仓库、机器人等资源受限场景部署，而这些场景又对效率和空间推理有很高要求。

Method: SmolRGPT结合RGB与深度信息，采用三阶段课程式训练，逐步对齐视觉与语言特征，实现空间关系理解，并适应任务数据集。

Result: 仅用600M参数，SmolRGPT在仓库空间推理任务上表现可与大模型媲美甚至超越。

Conclusion: SmolRGPT表明，可构建高效、实用且空间推理能力强的小规模多模态模型，促进其在真实资源受限环境中的部署。

Abstract: Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT

</details>


### [29] [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)
*Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo*

Main category: cs.CV

TL;DR: 本文提出了Lynx，一种从单张输入图片个性化生成高保真视频的新模型。基于开源Diffusion Transformer (DiT)，通过引入两个轻量级适配器，有效提升了人物身份保持、时序一致性和视频质量。


<details>
  <summary>Details</summary>
Motivation: 目前个性化视频合成往往存在人物身份失真、时序不一致等问题，难以在视觉质量和身份还原度之间取得平衡。本文旨在提出新方法，提高这些方面的表现。

Method: Lynx基于DiT，新增两个关键模块：ID-adapter使用Perceiver Resampler将ArcFace特征转为身份token用于条件控制；Ref-adapter引入密集VAE特征，通过交叉注意力将细致参考信息注入Transformer各层，增强细节和身份保持。

Result: 在包含40名不同主体和20个无偏提示词的基准测试（共800案例）上，Lynx在人物相似度、指令遵从和视频质量评测中均优于现有方法。

Conclusion: Lynx有效提升了个性化视频生成的身份保持、时序一致性和整体质量，推动了该领域的技术进步。

Abstract: We present Lynx, a high-fidelity model for personalized video synthesis from
a single input image. Built on an open-source Diffusion Transformer (DiT)
foundation model, Lynx introduces two lightweight adapters to ensure identity
fidelity. The ID-adapter employs a Perceiver Resampler to convert
ArcFace-derived facial embeddings into compact identity tokens for
conditioning, while the Ref-adapter integrates dense VAE features from a frozen
reference pathway, injecting fine-grained details across all transformer layers
through cross-attention. These modules collectively enable robust identity
preservation while maintaining temporal coherence and visual realism. Through
evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which
yielded 800 test cases, Lynx has demonstrated superior face resemblance,
competitive prompt following, and strong video quality, thereby advancing the
state of personalized video generation.

</details>


### [30] [Backdoor Mitigation via Invertible Pruning Masks](https://arxiv.org/abs/2509.15497)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的模型剪枝方法，用于有效防御深度学习中的后门攻击，兼顾主任务性能和后门任务的抑制，表现优于现有剪枝防御方法，并可与SOTA微调方法竞争。


<details>
  <summary>Details</summary>
Motivation: 现有基于剪枝的后门防御难以准确定位并移除引发后门行为的关键参数，尤其在数据有限时，剪枝方法在可解释性和鲁棒性上有独特优势，有必要提出更有效的剪枝防御机制。

Method: 作者设计了一种带有学习型选择机制的剪枝方法，结合可逆剪枝掩码，将参数划分为主任务和后门任务关键部分。通过双层优化（bi-level optimization）联合学习选择变量、稀疏可逆掩码和基于干净数据的样本特异触发扰动。内层任务利用反掩码合成候选触发器，外层任务优化掩码，在抑制后门的同时保持主任务性能。

Result: 实验显示该方法优于现有剪枝型后门缓解方法，在有限数据场景下仍维持较强表现，并在一定程度上可媲美最先进的微调法，特别是在成功消除后门后，可有效恢复被污染样本的预测正确性。

Conclusion: 提出的方法不仅改善了剪枝防御的有效性，还提升了可解释性及在小样本场景下的鲁棒性，为神经网络后门防御提供了新方向。

Abstract: Model pruning has gained traction as a promising defense strategy against
backdoor attacks in deep learning. However, existing pruning-based approaches
often fall short in accurately identifying and removing the specific parameters
responsible for inducing backdoor behaviors. Despite the dominance of
fine-tuning-based defenses in recent literature, largely due to their superior
performance, pruning remains a compelling alternative, offering greater
interpretability and improved robustness in low-data regimes. In this paper, we
propose a novel pruning approach featuring a learned \emph{selection} mechanism
to identify parameters critical to both main and backdoor tasks, along with an
\emph{invertible} pruning mask designed to simultaneously achieve two
complementary goals: eliminating the backdoor task while preserving it through
the inverse mask. We formulate this as a bi-level optimization problem that
jointly learns selection variables, a sparse invertible mask, and
sample-specific backdoor perturbations derived from clean data. The inner
problem synthesizes candidate triggers using the inverse mask, while the outer
problem refines the mask to suppress backdoor behavior without impairing
clean-task accuracy. Extensive experiments demonstrate that our approach
outperforms existing pruning-based backdoor mitigation approaches, maintains
strong performance under limited data conditions, and achieves competitive
results compared to state-of-the-art fine-tuning approaches. Notably, the
proposed approach is particularly effective in restoring correct predictions
for compromised samples after successful backdoor mitigation.

</details>


### [31] [MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training](https://arxiv.org/abs/2509.15514)
*Junbiao Pang,Tianyang Cai,Baochang Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的量化感知训练方法MEC-Quant，通过最大熵编码优化神经网络的表征结构，有效减小了低比特量化带来的偏差，实现了与全精度模型相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 量化感知训练（QAT）用于高效神经网络部署，但在极低比特下模型性能通常低于全精度模型。作者关注于低比特量化导致的表征偏差问题，旨在提升低比特QAT的性能。

Method: 提出Maximum Entropy Coding Quantization（MEC-Quant），用最小编码长度近似表征熵，从而优化神经网络表征结构。引入专家混合（MOE）机制以高效处理长尾分布，使该目标可端到端训练。

Result: 在多个视觉任务上，MEC-Quant显著超过了现有QAT方法，部分情况下甚至实现了比全精度模型更高的准确率，并首次将QAT的性能极限推进至极低比特激活。

Conclusion: MEC-Quant作为新型QAT方案，有效减少量化带来的表征偏差，并刷新了QAT任务的性能记录。

Abstract: Quantization-Aware Training (QAT) has driven much attention to produce
efficient neural networks. Current QAT still obtains inferior performances
compared with the Full Precision (FP) counterpart. In this work, we argue that
quantization inevitably introduce biases into the learned representation,
especially under the extremely low-bit setting. To cope with this issue, we
propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled
objective that explicitly optimizes on the structure of the representation, so
that the learned representation is less biased and thus generalizes better to
unseen in-distribution samples. To make the objective end-to-end trainable, we
propose to leverage the minimal coding length in lossy data coding as a
computationally tractable surrogate for the entropy, and further derive a
scalable reformulation of the objective based on Mixture Of Experts (MOE) that
not only allows fast computation but also handles the long-tailed distribution
for weights or activation values. Extensive experiments on various tasks on
computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT
is pushed to the x-bit activation for the first time and the accuracy of
MEC-Quant is comparable to or even surpass the FP counterpart. Without bells
and whistles, MEC-Qaunt establishes a new state of the art for QAT.

</details>


### [32] [GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents](https://arxiv.org/abs/2509.15532)
*Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin*

Main category: cs.CV

TL;DR: 本文提出了GUI-ARP，一种自适应多阶段推理的GUI定位新框架，通过局部视觉关注与动态推理策略实现高分辨率截图中更细粒度的元素定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高分辨率界面截图时，难以精准定位界面元素的具体位置，导致性能受限。作者希望提升GUI元素定位的精确度和效率。

Method: 提出GUI-ARP框架，包含自适应区域感知（ARP）和自适应阶段控制（ASC）模块。该方法动态分析视觉注意区，自主裁剪相关区域，对简单场景采用单阶段推理，对复杂场景采用多阶段推理。训练流程分为有监督微调和基于GRPO的强化学习微调两阶段。

Result: GUI-ARP在多个GUI定位的主流基准测试中取得了领先成绩。7B参数模型在ScreenSpot-Pro数据集上准确率60.8%，在UI-Vision数据集上为30.9%。性能超越了开源72B大模型（如UI-TARS-72B的38.1%），并与商用模型有竞争力。

Conclusion: GUI-ARP框架通过自适应的区域定位和推理调控，在细粒度高分辨率GUI定位任务中展现出卓越性能，为实际应用提供了更精确高效的技术方案。

Abstract: Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

</details>


### [33] [SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](https://arxiv.org/abs/2509.15536)
*Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang*

Main category: cs.CV

TL;DR: 本文提出了一种新的世界模型SAMPO，有效提升了视图预测的时空一致性和生成效率，在视频预测和基于模型的控制任务中表现突出，并具备较强的泛化和扩展能力。


<details>
  <summary>Details</summary>
Motivation: 现有自回归世界模型在视图生成中存在空间结构受损、解码效率低和运动建模不足等问题，限制了其在仿真规划和复杂任务决策中的应用。

Method: 提出SAMPO框架，将帧内视觉自回归建模与帧间因果建模结合，同时用多尺度Token编码和运动提示模块增强对动态场景的理解，提升空间细节和运动一致性，并加速推理效率。

Result: 实验显示，SAMPO在动作条件下的视频预测和基于模型的控制任务中，生成质量与速度优于现有方法，推理速度提升4.4倍，并在零样本泛化和扩展性测试中表现良好。

Conclusion: SAMPO显著提升了世界模型的视图生成质量、效率和泛化能力，有潜力应用于更复杂和多样的智能体规划与决策任务。

Abstract: World models allow agents to simulate the consequences of actions in imagined
environments for planning, control, and long-horizon decision-making. However,
existing autoregressive world models struggle with visually coherent
predictions due to disrupted spatial structure, inefficient decoding, and
inadequate motion modeling. In response, we propose \textbf{S}cale-wise
\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt
(\textbf{SAMPO}), a hybrid framework that combines visual autoregressive
modeling for intra-frame generation with causal modeling for next-frame
generation. Specifically, SAMPO integrates temporal causal decoding with
bidirectional spatial attention, which preserves spatial locality and supports
parallel decoding within each scale. This design significantly enhances both
temporal consistency and rollout efficiency. To further improve dynamic scene
understanding, we devise an asymmetric multi-scale tokenizer that preserves
spatial details in observed frames and extracts compact dynamic representations
for future frames, optimizing both memory usage and model performance.
Additionally, we introduce a trajectory-aware motion prompt module that injects
spatiotemporal cues about object and robot trajectories, focusing attention on
dynamic regions and improving temporal consistency and physical realism.
Extensive experiments show that SAMPO achieves competitive performance in
action-conditioned video prediction and model-based control, improving
generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's
zero-shot generalization and scaling behavior, demonstrating its ability to
generalize to unseen tasks and benefit from larger model sizes.

</details>


### [34] [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
*Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha*

Main category: cs.CV

TL;DR: 本文提出了一种对人类欲望、情感和情绪识别进行对称双向多模态学习的新框架，有效融合文本与图像信息，在多个任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感与情绪识别主要聚焦于理解情绪本身，鲜有专注于‘欲望’的识别，并且通常忽略了图像（非语言线索）对文本的补充作用。为此，作者试图填补‘人类欲望理解’在多模态学习中的空白，并提高图像信息在情感分析中的利用效率。

Method: 提出了对称双向多模态学习框架——通过低分辨率图像获取全局视觉表示进行跨模态对齐，通过高分辨率图像分块及掩码建模捕捉局部特征。设计了文本引导的图像解码和图像引导的文本解码，实现深度跨模态交互。同时采用混合尺度图像处理策略，兼顾感知性能与计算开销。

Result: 在MSED多模态数据集（包含欲望、情感和情绪识别基准）上实验，结果表明本方法在三项任务上F1得分分别较最优方法提升1.1%、0.6%、0.9%。

Conclusion: 该方法通过有效整合文本和视觉特征，显著提升了多模态欲望、情感与情绪识别的性能，为多模态人性理解提供了新的思路。

Abstract: Desire, as an intention that drives human behavior, is closely related to
both emotion and sentiment. Multimodal learning has advanced sentiment and
emotion recognition, but multimodal approaches specially targeting human desire
understanding remain underexplored. And existing methods in sentiment analysis
predominantly emphasize verbal cues and overlook images as complementary
non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional
Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,
which enforces mutual guidance between text and image modalities to effectively
capture intention-related representations in the image. Specifically,
low-resolution images are used to obtain global visual representations for
cross-modal alignment, while high resolution images are partitioned into
sub-images and modeled with masked image modeling to enhance the ability to
capture fine-grained local features. A text-guided image decoder and an
image-guided text decoder are introduced to facilitate deep cross-modal
interaction at both local and global representations of image information.
Additionally, to balance perceptual gains with computation cost, a mixed-scale
image strategy is adopted, where high-resolution images are cropped into
sub-images for masked modeling. The proposed approach is evaluated on MSED, a
multimodal dataset that includes a desire understanding benchmark, as well as
emotion and sentiment recognition. Experimental results indicate consistent
improvements over other state-of-the-art methods, validating the effectiveness
of our proposed method. Specifically, our method outperforms existing
approaches, achieving F1-score improvements of 1.1% in desire understanding,
0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is
available at: https://github.com/especiallyW/SyDES.

</details>


### [35] [Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track](https://arxiv.org/abs/2509.15546)
*Ran Hong,Feng Lu,Leilei Cao,An Yan,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法，通过视频-语言检查器与关键信息帧采样器，有效提升了视频参考目标分割任务性能，并在公开挑战赛中取得了第二名。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法尽管通过大模型提升了分割性能，但容易出现描述不符的问题和时序信息不足。为了解决这些缺陷，作者希望既验证语言描述与视频内容匹配，又能捕捉时域关键信息。

Method: 方法包括两个新组件：(1) 视频-语言检查器，检验视频是否包含查询中提及的主体和动作，以降低误报；(2) 关键信息帧采样器，自适应选取包含有效线索的帧，用于强化时间上下文信息捕捉。该方法无需额外训练。

Result: 在MeViS测试集上，该方法的J&F分数达到64.14%，在ICCV 2025第七届LSVOS挑战赛RVOS赛道上获得第二名。

Conclusion: 本文方法显著提升了RVOS性能，尤其在准确性和时序信息捕获方面表现优异，且具有实用的零训练优势，在大型挑战赛中验证了其实用性。

Abstract: Referential Video Object Segmentation (RVOS) aims to segment all objects in a
video that match a given natural language description, bridging the gap between
vision and language understanding. Recent work, such as Sa2VA, combines Large
Language Models (LLMs) with SAM~2, leveraging the strong video reasoning
capability of LLMs to guide video segmentation. In this work, we present a
training-free framework that substantially improves Sa2VA's performance on the
RVOS task. Our method introduces two key components: (1) a Video-Language
Checker that explicitly verifies whether the subject and action described in
the query actually appear in the video, thereby reducing false positives; and
(2) a Key-Frame Sampler that adaptively selects informative frames to better
capture both early object appearances and long-range temporal context. Without
any additional training, our approach achieves a J&F score of 64.14% on the
MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge
at ICCV 2025.

</details>


### [36] [MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2509.15548)
*Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: MS-GS提出了一种结合3D高斯投影与单目深度估计的框架，在稀疏且多样化外部图片下，提升了三维重建和新视角合成的效果，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF和3D高斯Splatting方法在现实野外场景下，面对有限数量、多时相、多外观的图片时，容易出现过度平滑和过拟合，导致三维重建和新视角渲染的质量不佳，亟需改进。

Method: 提出MS-GS框架，充分利用单目深度估计带来的几何先验，将SfM提取的稀疏点云作为语义区域锚点，用以对齐和指导几何推断；并在虚拟视角上引入几何的细致及粗略监督，以强化3D一致性、减少过拟合。此外，作者也构建了新的数据集与实验环境进行更真实的性能测试。

Result: MS-GS在各种稀疏、多外观的场景下，都能够实现真实感极强的渲染效果，并在多个数据集上性能大幅超过了现有方法。

Conclusion: MS-GS为现实环境下的稀疏、多外观照片集合下的三维重建和新视角合成问题提供了高效且鲁棒的解决方案，推进了领域发展。

Abstract: In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.

</details>


### [37] [Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification](https://arxiv.org/abs/2509.15553)
*Tian Lan,Yiming Zheng,Jianxin Yin*

Main category: cs.CV

TL;DR: 提出Diff-Feat框架，通过挖掘扩散模型Transformer的中间层特征，并有效融合用于多标签分类，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签分类需要更能表达不同标签间交互的特征，而现有方法对这种能力有限。本研究旨在通过预训练扩散模型捕捉更具判别力的中间特征，提升多标签分类表现。

Method: 提出Diff-Feat框架，从预训练的扩散-Transformer模型中提取图像和文本的中间特征。通过分析扩散过程和Transformer结构，定位最优特征位置。设计局部搜索算法，优化选择“图像-文本×层级-步数”组合，并采用简单的线性融合方法获取最终表示。

Result: 在MS-COCO-enhanced上取得98.6% mAP，在Visual Genome 500上取得45.7% mAP，均显著超过CNN、图神经网络和大型Transformer等强基线。t-SNE和聚类指标也表明Diff-Feat能形成更紧密的语义聚类。

Conclusion: Diff-Feat通过对扩散模型中间特征的识别与高效融合，有效提升多标签分类任务，方法简洁但性能突出，具有较高实用价值。

Abstract: Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.

</details>


### [38] [From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations, Challenges and Way Forward](https://arxiv.org/abs/2509.15558)
*Mahesh Shakya,Bijay Adhikari,Nirsara Shrestha,Bipin Koirala,Arun Adhikari,Prasanta Poudyal,Luna Mathema,Sarbagya Buddhacharya,Bijay Khatri,Bishesh Khanal*

Main category: cs.CV

TL;DR: 本文探讨了在资源受限环境下，将AI辅助的远程医疗和筛查工作从纸质流程过渡到数字化流程所面临的实际挑战和解决方法，并总结了实践中获得的经验教训。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专业人员和筛查手段，视觉和听觉疾病在资源有限的环境下常导致可预防的残疾。大规模的AI辅助筛查和远程医疗有望早期发现这些疾病，但在纸质为主的流程中落地存在诸多实际困难，目前相关的实地经验和文献较少。

Method: 作者通过在高强度筛查场景下实际部署AI辅助远程医疗系统，包括早期原型、影子部署和持续反馈的方式，以迭代式跨学科合作改进工作流程，除此之外，还探讨了AI公共数据集与模型的迁移使用及自动化图像质量检测的需求。

Result: （1）早期原型和影子部署有助于多学科共享理解，减少从纸质到数字化流程转变时的可用性障碍；（2）即使存在领域差异，公共数据集和AI模型对开发仍有参考价值；（3）高效筛查需自动化图像质量检测来获得可用图像。

Conclusion: AI系统开发和工作流数字化转型应被视为端到端、持续共创的过程，只有通过在实际场景下总结问题和经验，才能为资源有限场所的大规模AI辅助远程医疗和筛查项目提供可执行的指导。

Abstract: Vision- and hearing-threatening diseases cause preventable disability,
especially in resource-constrained settings(RCS) with few specialists and
limited screening setup. Large scale AI-assisted screening and telehealth has
potential to expand early detection, but practical deployment is challenging in
paper-based workflows and limited documented field experience exist to build
upon. We provide insights on challenges and ways forward in development to
adoption of scalable AI-assisted Telehealth and screening in such settings.
Specifically, we find that iterative, interdisciplinary collaboration through
early prototyping, shadow deployment and continuous feedback is important to
build shared understanding as well as reduce usability hurdles when
transitioning from paper-based to AI-ready workflows. We find public datasets
and AI models highly useful despite poor performance due to domain shift. In
addition, we find the need for automated AI-based image quality check to
capture gradable images for robust screening in high-volume camps.
  Our field learning stress the importance of treating AI development and
workflow digitization as an end-to-end, iterative co-design process. By
documenting these practical challenges and lessons learned, we aim to address
the gap in contextual, actionable field knowledge for building real-world
AI-assisted telehealth and mass-screening programs in RCS.

</details>


### [39] [DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection](https://arxiv.org/abs/2509.15563)
*Min Sun,Fenghui Guo*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感影像变化检测的新方法DC-Mamba，通过“对齐-增强”策略有效提升了对真实地物变化的识别效果，显著抑制了噪声与误差。


<details>
  <summary>Details</summary>
Motivation: 当前主流变化检测模型难以有效处理遥感影像几何错位，且对细微真实变化与噪声的区分能力有限，影响了检测准确性。

Method: DC-Mamba基于ChangeMamba框架，集成了两个轻量、可插拔模块：一是Bi-Temporal Deformable Alignment（BTDA），在特征级显式处理几何错配；二是Scale-Sparse Change Amplifier（SSCA），多源特征协同增强高置信度变化，抑制噪声。流程为先用BTDA实现几何一致性，再用SSCA锐化边界、增强小目标。

Result: 在标准数据集上，DC-Mamba相比原有ChangeMamba主干，F1从0.5730提升至0.5903，IoU由0.4015提升至0.4187，展现了方法的优越性。

Conclusion: DC-Mamba“对齐-增强”方案能有效同时应对几何误差和特征层面的变化检测难题，具有较强实用性及易部署性。

Abstract: Remote sensing change detection (RSCD) is vital for identifying land-cover
changes, yet existing methods, including state-of-the-art State Space Models
(SSMs), often lack explicit mechanisms to handle geometric misalignments and
struggle to distinguish subtle, true changes from noise.To address this, we
introduce DC-Mamba, an "align-then-enhance" framework built upon the
ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)
Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric
awareness to correct spatial misalignments at the semantic feature level; and
(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to
selectively amplify high-confidence change signals while suppressing noise
before the final classification. This synergistic design first establishes
geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA
to sharpen boundaries and enhance the visibility of small or subtle targets.
Experiments show our method significantly improves performance over the strong
ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU
from 0.4015 to 0.4187. The results confirm the effectiveness of our
"align-then-enhance" strategy, offering a robust and easily deployable solution
that transparently addresses both geometric and feature-level challenges in
RSCD.

</details>


### [40] [BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/abs/2509.15566)
*Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 本文提出了一种模仿人类认知过程的新型人机界面交互框架（Blink-Think-Link, BTL），以及基于该框架的GUI Agent模型（BTL-UI），并在多项基准测试中获得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的人机交互自动化虽然取得了较大进展，但其交互逻辑与真实的人类GUI交流过程存在本质差异。亟需一种更加接近人类自然交互的系统架构，以提升AI在GUI自动化任务中的有效性和合理性。

Method: 提出BTL脑启发式人机界面交互框架，将交互拆分为Blink（关注区域定位）、Think（高阶推理决策）、Link（动作指令生成）三个生物学启发阶段。同时设计了Blink数据自动标注流程和融合过程与结果的BTL奖励机制，实现了有效的强化学习训练。基于此，开发了BTL-UI智能代理模型。

Result: BTL-UI模型在静态GUI理解与动态交互任务的全面基准测试中，均实现了稳定的SOTA（最优）性能，显示出该方法的有效性。

Conclusion: 本文框架有效模拟了人类认知驱动的人机界面交互流程，提出的创新机制为高效GUI Agent开发提供了新思路，并通过实验证明了其实用价值。

Abstract: In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.

</details>


### [41] [Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach](https://arxiv.org/abs/2509.15573)
*Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 本文针对显著性目标检测（SOD）中评估过程中普遍存在但未被充分重视的“尺寸无关性”问题，提出了新的评估与优化框架，有效提升了多尺寸目标检测的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SOD评估指标对目标尺寸敏感，导致多尺寸显著目标同时出现时，大目标主导评估结果，小目标被忽视，造成评估失真和实际性能降低。

Method: 作者首先理论证明了常规SOD指标对尺寸敏感的问题，并提出“尺寸无关评估”（SIEva）框架，将图像分为可分离区域后分别评估并聚合结果，从根本上解决了评估的尺寸失衡影响。在此基础上，进一步提出尺寸无关优化（SIOpt）框架，可嵌入多种SOD模型以更好地检测各尺寸显著目标。

Result: 理论分析和实验均表明SIEva与SIOpt能显著改善现有方法在多尺寸目标下的评估公平性与检测性能，且SIOpt具有良好的通用性和兼容性。

Conclusion: 本文工作揭示了SOD评估的关键偏差并提出兼具理论与实用价值的新方案，为显著性目标检测特别是复杂多目标场景下的性能提升提供了有力方法和参考。

Abstract: This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.

</details>


### [42] [Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion](https://arxiv.org/abs/2509.15578)
*Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态短视频假新闻检测方法，通过融合视频、音频和文本信息，有效提升了检测准确率，并引入了专为短视频设计的新数据集。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的普及，假新闻传播速度加快且影响广泛，现有检测方法难以应对短视频内容的多模态和动态变化，亟需更先进的假新闻检测技术。

Method: 提出了一种异构融合网络（HFN），包括动态调整模态权重的决策网络和加权多模态特征融合模块，能够在部分信息缺失的情况下保持鲁棒性。同时构建了针对短视频假新闻检测的大型数据集VESV。

Result: 在FakeTT和新采集的VESV数据集上，HFN方法的Marco F1分别比最先进方法提升了2.71%和4.14%。

Conclusion: HFN为复杂短视频平台中的假新闻检测提供了有效且健壮的解决方案，为未来应对多模态假新闻开辟了新方向。

Abstract: The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.

</details>


### [43] [EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery](https://arxiv.org/abs/2509.15596)
*Gui Wang,Yang Wennuo,Xusen Ma,Zehao Zhong,Zhuoru Wu,Ende Wu,Rong Qu,Wooi Ping Cheah,Jianfeng Ren,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了EyePCR，这是一个用于眼科手术分析的大规模基准，专为评估MLLMs在医学领域多模态认知能力（感知、理解、推理）而设计。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs虽在多模态任务中表现突出，但在如外科手术这样高风险、专业性的场景下能力尚未充分研究。因此，亟需基于专业知识和临床实践的基准来系统性评估并提升其在医学中的应用。

Method: 作者构建了EyePCR基准数据集，包括21万多项视觉问答（VQA）、1048项细粒度手术属性、2.5万多医学知识图谱三元组，以及4项临床推理任务，并详细标注，支持对模型多层次认知能力的系统分析。此外，他们开发了EyePCR-MLLM，一种基于Qwen2.5-VL-7B的医学领域适应性模型，对数据做定向训练和评测。

Result: EyePCR-MLLM在感知类选择题上达到目前最高准确率，在理解和推理任务上超越了其他开源模型，在性能上与商业模型（如GPT-4.1）相当。同时，EyePCR基准揭示了现有MLLMs在手术认知方面的局限性。

Conclusion: EyePCR为外科视频理解模型的认知能力评测和临床可靠性提升奠定了数据和方法基础，推动了MLLMs在医疗高度专科场景中的发展。

Abstract: MLLMs (Multimodal Large Language Models) have showcased remarkable
capabilities, but their performance in high-stakes, domain-specific scenarios
like surgical settings, remains largely under-explored. To address this gap, we
develop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery
analysis, grounded in structured clinical knowledge to evaluate cognition
across \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.
EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover
1048 fine-grained attributes for multi-view perception, medical knowledge graph
of more than 25k triplets for comprehension, and four clinically grounded
reasoning tasks. The rich annotations facilitate in-depth cognitive analysis,
simulating how surgeons perceive visual cues and combine them with domain
knowledge to make decisions, thus greatly improving models' cognitive ability.
In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,
achieves the highest accuracy on MCQs for \textit{Perception} among compared
models and outperforms open-source models in \textit{Comprehension} and
\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals
the limitations of existing MLLMs in surgical cognition and lays the foundation
for benchmarking and enhancing clinical reliability of surgical video
understanding models.

</details>


### [44] [TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?](https://arxiv.org/abs/2509.15602)
*Zhongyuan Bao,Lejun Zhang*

Main category: cs.CV

TL;DR: 本文提出了TennisTV基准，以系统性评估多模态大语言模型（MLLMs）在网球视频理解任务上的表现，发现现有MLLMs在高频运动视频场景下表现有限，并指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 目前的MLLMs虽擅长通用视频理解，但在处理如网球这类信息密集、变化快速的运动视频时表现欠佳；缺乏专门数据集和评测方法系统性检验其在网球等场景下的实际理解能力。

Method: 作者建立了TennisTV基准，将每一次网球来回分解为有序的连续击球事件序列，基于自动化流程完成事件筛选和问题生成，覆盖回合与击球两个层面的8个任务，包含2500个人工核验问题。随后，作者系统性地评估了16种代表性MLLM。

Result: 实验表明，现有MLLM在网球视频理解场景中存在显著劣势。分析揭示：帧抽样密度需针对任务进行调整与平衡；时间定位能力薄弱，制约了模型推理。

Conclusion: TennisTV基准为高频运动视频理解提供了首个系统性评估平台，明确未来MLLM改进需聚焦抽样策略和提升时序推理能力。

Abstract: Multimodal large language models (MLLMs) excel at general video understanding
but struggle with fast, high-frequency sports like tennis, where rally clips
are short yet information-dense. To systematically evaluate MLLMs in this
challenging domain, we present TennisTV, the first and most comprehensive
benchmark for tennis video understanding. TennisTV models each rally as a
temporal-ordered sequence of consecutive stroke events, using automated
pipelines for filtering and question generation. It covers 8 tasks at rally and
stroke levels and includes 2,500 human-verified questions. Evaluating 16
representative MLLMs, we provide the first systematic assessment of tennis
video understanding. Results reveal substantial shortcomings and yield two key
insights: (i) frame-sampling density should be tailored and balanced across
tasks, and (ii) improving temporal grounding is essential for stronger
reasoning.

</details>


### [45] [Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation](https://arxiv.org/abs/2509.15608)
*Zheng Wang,Hong Liu,Zheng Wang,Danyi Li,Min Cen,Baptiste Magnier,Li Liang,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合病理报告文本和Whole Slide Images (WSIs) 的新框架Rasa，用于提升癌症预后生存分析的准确性。通过利用大语言模型处理病理报告，并应用自蒸馏和risk-aware mix-up，Rasa显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于WSI的生存分析常因特征噪声大和数据获取受限，导致预后效果不佳。此外，病理报告虽包含丰富信息，但常被忽视，未在WSI生存分析中有效利用。

Method: 1. 使用大语言模型从原始病理报告中提取与WSI相关的高质量文本描述；2. 设计了基于自蒸馏的管道，在教师模型的文本知识指导下过滤WSI特征；3. 引入风险感知的mix-up策略，提升训练数据的数量和多样性。

Result: 在自建的CRC数据集和公开的TCGA-BRCA数据集上，Rasa框架在多项指标上均优于目前主流方法。

Conclusion: 结合大语言模型辅助的自蒸馏框架和报告文本信息，能显著提升WSI预后模型的性能，有望推动肿瘤生存分析的发展。

Abstract: Survival analysis based on Whole Slide Images (WSIs) is crucial for
evaluating cancer prognosis, as they offer detailed microscopic information
essential for predicting patient outcomes. However, traditional WSI-based
survival analysis usually faces noisy features and limited data accessibility,
hindering their ability to capture critical prognostic features effectively.
Although pathology reports provide rich patient-specific information that could
assist analysis, their potential to enhance WSI-based survival analysis remains
largely unexplored. To this end, this paper proposes a novel Report-auxiliary
self-distillation (Rasa) framework for WSI-based survival analysis. First,
advanced large language models (LLMs) are utilized to extract fine-grained,
WSI-relevant textual descriptions from original noisy pathology reports via a
carefully designed task prompt. Next, a self-distillation-based pipeline is
designed to filter out irrelevant or redundant WSI features for the student
model under the guidance of the teacher model's textual knowledge. Finally, a
risk-aware mix-up strategy is incorporated during the training of the student
model to enhance both the quantity and diversity of the training data.
Extensive experiments carried out on our collected data (CRC) and public data
(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against
state-of-the-art methods. Our code is available at
https://github.com/zhengwang9/Rasa.

</details>


### [46] [CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios](https://arxiv.org/abs/2509.15984)
*Kangyu Wu,Jiaqi Qiao,Ya Zhang*

Main category: cs.CV

TL;DR: 提出了一种新型轻量级的协作轨迹预测框架CoPAD，通过多种创新模块实现高效、准确的多源轨迹融合与预测，在V2X场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 单车感知不稳定性对轨迹预测的限制，使得需要融合多源数据（如多车辆与路侧基础设施信息）以提升预测性能。

Method: 提出CoPAD框架，核心包括：1) 基于匈牙利算法与卡尔曼滤波的数据融合模块，实现多源轨迹数据早期融合；2) Past Time Attention模块，捕获历史轨迹间的潜在交互信息；3) 模式注意力模块，提升预测多样性；4) 基于稀疏锚点的解码器，用于生成完整轨迹。

Result: 在DAIR-V2X-Seq数据集上的大量实验表明，CoPAD在协作轨迹预测任务上达到最新最优表现（state-of-the-art）。

Conclusion: CoPAD框架可以有效融合多源信息，提升了V2X场景下的轨迹预测准确性和完整性，为自动驾驶中的协作感知提供了有力支持。

Abstract: Recently, data-driven trajectory prediction methods have achieved remarkable
results, significantly advancing the development of autonomous driving.
However, the instability of single-vehicle perception introduces certain
limitations to trajectory prediction. In this paper, a novel lightweight
framework for cooperative trajectory prediction, CoPAD, is proposed. This
framework incorporates a fusion module based on the Hungarian algorithm and
Kalman filtering, along with the Past Time Attention (PTA) module, mode
attention module and anchor-oriented decoder (AoD). It effectively performs
early fusion on multi-source trajectory data from vehicles and road
infrastructure, enabling the trajectories with high completeness and accuracy.
The PTA module can efficiently capture potential interaction information among
historical trajectories, and the mode attention module is proposed to enrich
the diversity of predictions. Additionally, the decoder based on sparse anchors
is designed to generate the final complete trajectories. Extensive experiments
show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq
dataset, validating the effectiveness of the model in cooperative trajectory
prediction in V2X scenarios.

</details>


### [47] [PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning](https://arxiv.org/abs/2509.15623)
*Zhuoyao Liu,Yang Liu,Wentao Feng,Shudong Huang*

Main category: cs.CV

TL;DR: 该论文提出了PCSR框架，通过伪标签一致性细分和优化样本，有效提升了带噪监督下跨模态检索的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索方法通常假设图文对是完美对齐的，但实际数据常存在噪声配对，从而误导模型学习，影响检索精度。同时，已有方法粗略地将样本分为干净和噪声两类，忽视了噪声样本内部的多样性，也未能采取样本特性化的训练策略，导致模型训练效果不佳。

Method: 作者提出PCSR（伪标签一致性引导样本细化）框架。首先通过置信度判别区分干净与噪声配对，再基于伪标签一致性对噪声样本进一步细分。引入PCS（伪标签一致性分数）衡量预测稳定性，将噪声样本划分为模糊和可修复两类，并分别采用鲁棒损失函数与文本替换策略进行训练优化。

Result: 在CC152K、MS-COCO和Flickr30K等数据集上大量实验表明，PCSR显著提升了在有噪监督数据下的跨模态检索性能和鲁棒性。

Conclusion: PCSR能够细粒度识别并优化噪声样本，通过针对性训练策略改善模型对噪声数据的适应能力，对实际数据中常见的配对噪声问题具有很好的解决效果。

Abstract: Cross-modal retrieval aims to align different modalities via semantic
similarity. However, existing methods often assume that image-text pairs are
perfectly aligned, overlooking Noisy Correspondences in real data. These
misaligned pairs misguide similarity learning and degrade retrieval
performance. Previous methods often rely on coarse-grained categorizations that
simply divide data into clean and noisy samples, overlooking the intrinsic
diversity within noisy instances. Moreover, they typically apply uniform
training strategies regardless of sample characteristics, resulting in
suboptimal sample utilization for model optimization. To address the above
challenges, we introduce a novel framework, called Pseudo-label
Consistency-Guided Sample Refinement (PCSR), which enhances correspondence
reliability by explicitly dividing samples based on pseudo-label consistency.
Specifically, we first employ a confidence-based estimation to distinguish
clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency
to uncover structurally distinct subsets. We further proposed a Pseudo-label
Consistency Score (PCS) to quantify prediction stability, enabling the
separation of ambiguous and refinable samples within noisy pairs. Accordingly,
we adopt Adaptive Pair Optimization (APO), where ambiguous samples are
optimized with robust loss functions and refinable ones are enhanced via text
replacement during training. Extensive experiments on CC152K, MS-COCO and
Flickr30K validate the effectiveness of our method in improving retrieval
robustness under noisy supervision.

</details>


### [48] [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](https://arxiv.org/abs/2509.15987)
*Aurélien Cecille,Stefan Duffner,Franck Davoine,Rémi Agier,Thibault Neveu*

Main category: cs.CV

TL;DR: 该论文提出了一种只需自监督即可在单目深度估计中获得清晰物体边界的新方法，并实现了对比主流方法更高的边界锐度和点云质量。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计算法在物体边界处常常产生模糊，导致三维场景重建时出现干扰点。现有解决方法通常需要高精度监督，成本高、数据难获取。作者希望通过自监督，提升深度估计边界处的清晰度，降低监督需求。

Method: 作者提出了基于混合分布的逐像素深度建模方法：将每个像素的深度预测为多个可能的深度，并用混合权重捕获不确定性。通过引入基于方差的损失函数和不确定性传播机制，该方法可无缝集成进现有算法流程。

Result: 在KITTI和VKITTIv2数据集上的实验表明，所提方法边界清晰度可提升高达35%，点云重建质量相比最先进方法也有明显提升。

Conclusion: 该方法无需精细人工标注，仅用自监督即能获得更清晰的深度边界，为单目深度估计及其三维应用带来新突破。

Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding,
but existing methods often blur depth at object boundaries, introducing
spurious intermediate 3D points. While achieving sharp edges usually requires
very fine-grained supervision, our method produces crisp depth discontinuities
using only self-supervision. Specifically, we model per-pixel depth as a
mixture distribution, capturing multiple plausible depths and shifting
uncertainty from direct regression to the mixture weights. This formulation
integrates seamlessly into existing pipelines via variance-aware loss functions
and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show
that our method achieves up to 35% higher boundary sharpness and improves point
cloud quality compared to state-of-the-art baselines.

</details>


### [49] [pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2509.15638)
*Tong Wang,Xingyue Zhao,Linghao Zhuang,Haoyu Zhao,Jiayi Yin,Yuyang He,Gang Yu,Bo Lin*

Main category: cs.CV

TL;DR: 本文提出了一种个性化联邦Segment Anything Model（SAM）框架，用于异质医疗图像分割，通过聚合全局参数与知识蒸馏方式，实现了更优的分割效果和跨域适应性，并降低了通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法多使用轻量模型，难以应对复杂、异质性的数据；而SAM虽能力强，但编码器庞大，难以直接应用于联邦场景，急需新的方法平衡性能与个性化。

Method: 提出个性化联邦SAM框架，具体包括：（1）仅聚合全局参数，同时保留L-MoE本地专家结构以增强域特异性特征；（2）采用教师-学生范式的全局-本地解耦微调，并利用知识蒸馏协调全局和本地模型，降低过度泛化。

Result: 在两个公开医疗图像分割数据集上的实验显示，该方法显著提升了分割效果，具备强跨域适应性，同时通信消耗更低。

Conclusion: 该框架有效解决了复杂异质数据下，联邦学习在医疗图像分割任务中的性能瓶颈，显著提升模型个性化与泛化能力，并具备较高实际应用价值。

Abstract: Medical image segmentation is crucial for computer-aided diagnosis, yet
privacy constraints hinder data sharing across institutions. Federated learning
addresses this limitation, but existing approaches often rely on lightweight
architectures that struggle with complex, heterogeneous data. Recently, the
Segment Anything Model (SAM) has shown outstanding segmentation capabilities;
however, its massive encoder poses significant challenges in federated
settings. In this work, we present the first personalized federated SAM
framework tailored for heterogeneous data scenarios in medical image
segmentation. Our framework integrates two key innovations: (1) a personalized
strategy that aggregates only the global parameters to capture cross-client
commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)
component to preserve domain-specific features; and (2) a decoupled
global-local fine-tuning mechanism that leverages a teacher-student paradigm
via knowledge distillation to bridge the gap between the global shared model
and the personalized local models, thereby mitigating overgeneralization.
Extensive experiments on two public datasets validate that our approach
significantly improves segmentation performance, achieves robust cross-domain
adaptation, and reduces communication overhead.

</details>


### [50] [UNIV: Unified Foundation Model for Infrared and Visible Modalities](https://arxiv.org/abs/2509.15642)
*Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的人眼仿生红外与可见光统一感知模型UNIV，融合了创新的跨模态对齐和知识保持机制，并引入了大规模新数据集，展示出优异的多模态感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型虽在红外和可见光领域各自表现优秀，但在需要多模态信息融合的实际场景（如自动驾驶）中表现有限。因此亟需一种能统一处理并充分融合两种模态信息的感知模型。

Method: 1）提出了受生物启发的Patch-wise Cross-modality Contrastive Learning（PCCL），使用注意力引导蒸馏模仿视网膜水平细胞侧抑制，实现高效跨模态特征对齐且支持任意transformer架构。2）设计了双重知识保持机制，结合LoRA适配器与同步蒸馏，防止遗忘，模拟视网膜明/暗视觉处理通路。3）发布了大型可见光-红外对齐图像数据集MVIP。

Result: 实验结果显示，UNIV模型在红外相关任务上性能提升显著（语义分割提升1.7 mIoU，目标检测提升0.7 mAP），且在可见光任务上能力几乎无损（保留99%以上基线性能）。

Conclusion: UNIV是首个兼具高效、通用、符合生物机制的可见光-红外多模态基础模型，通过创新机制有效融合两模态感知信息，推动了复杂环境下的稳健感知发展。

Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly,
particularly to achieve robust performance under diverse weather conditions.
Although pre-trained models for RGB-visible and infrared data excel in their
respective domains, they often underperform in multimodal scenarios, such as
autonomous vehicles equipped with both sensors. To address this challenge, we
propose a biologically inspired UNified foundation model for Infrared and
Visible modalities (UNIV), featuring two key innovations. First, we introduce
Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided
distillation framework that mimics retinal horizontal cells' lateral
inhibition, which enables effective cross-modal feature alignment while
remaining compatible with any transformer-based architecture. Second, our
dual-knowledge preservation mechanism emulates the retina's bipolar cell signal
routing - combining LoRA adapters (2% added parameters) with synchronous
distillation to prevent catastrophic forgetting, thereby replicating the
retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To
support cross-modal learning, we introduce the MVIP dataset, the most
comprehensive visible-infrared benchmark to date. It contains 98,992 precisely
aligned image pairs spanning diverse scenarios. Extensive experiments
demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in
semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+
of the baseline performance on visible RGB tasks. Our code is available at
https://github.com/fangyuanmao/UNIV.

</details>


### [51] [GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading](https://arxiv.org/abs/2509.15645)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: 本文提出了GS-Scale系统，通过将3D Gaussian Splatting中的高内存需求部分转移到主机内存，实现了大规模场景的高效训练，同时显著降低了GPU内存占用并保持较快训练速度。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在渲染质量和速度上表现优异，但大规模、高质量场景训练极度消耗GPU内存，限制了其在消费级设备上的应用。作者希望解决这一内存瓶颈，支持更大规模数据训练。

Method: GS-Scale将所有高斯参数存储在主机内存，并仅在需要时将部分数据传输到GPU执行前向和反向传播。同时，系统采用了三项系统优化：（1）选择性卸载几何参数，加快视锥剔除；（2）参数转发，实现CPU优化器与GPU计算并行流水；（3）延迟优化器更新，减少无梯度高斯的内存访问。

Result: 在大规模数据集上的实验表明，GS-Scale可将GPU内存需求降低3.3到5.6倍，并且保持与纯GPU训练相似的速度。在RTX 4070 Mobile等消费级显卡上，可支持从4百万到1800万个高斯的训练，LPIPS指标提升23-35%。

Conclusion: GS-Scale使大规模3D Gaussian Splatting训练在普通GPU上成为可能，显著缓解了GPU内存瓶颈，提升了渲染质量和应用普适性。

Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.

</details>


### [52] [FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.15648)
*Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无接触式指纹三维配准、重建与生成框架，通过3D高斯溅射技术实现指纹从二维图像向三维模型的高效转化，显著提升了无接触指纹识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无接触式指纹识别受限于数据量和三维特征的缺乏，导致性能落后于有接触识别方法。作者希望通过利用更丰富的三维信息，提升无接触指纹识别的表现。

Method: 首次将3D Gaussian Splatting方法应用于指纹识别领域，实现对稀疏二维指纹图像的三维配准与完整重建，无需相机参数信息。

Result: 实验证明该方法能准确对齐和重建三维无接触指纹，并基于重建模型生成高质量指纹图像，极大提升了无接触指纹识别系统的性能。

Conclusion: 提出的方法为无接触指纹三维重建与识别开辟了新的研究方向，在高质量指纹生成与识别准确性方面有重要提升和应用前景。

Abstract: Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.

</details>


### [53] [A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds](https://arxiv.org/abs/2509.15675)
*Hao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于主成分分析（PCA）的点云数据表面重建方法，能够高效推断缺失数据区域的表面结构，并重建完整表面。


<details>
  <summary>Details</summary>
Motivation: 点云数据在数学建模和多个领域极为重要，但由于扫描过程中的遮挡和高光吸收等原因，常出现数据缺失，导致表面重建困难。因此，亟需能在数据缺失区域推断表面结构并完成重建的新方法。

Method: 方法首先利用PCA从现有点云数据中估算表面法向量信息，并将其用作正则项，引导整个表面的重建，尤其是在数据缺失区。随后引入算子分裂方法来高效求解该模型。

Result: 通过系统实验验证，该方法能有效推断数据缺失区域的表面结构并成功重建完整表面，性能优于现有方法。

Conclusion: 本文提出的PCA正则化及算子分裂求解模型，在点云数据缺失时仍能实现高效、准确的表面重建，对相关领域具有重要意义。

Abstract: Point cloud data represents a crucial category of information for
mathematical modeling, and surface reconstruction from such data is an
important task across various disciplines. However, during the scanning
process, the collected point cloud data may fail to cover the entire surface
due to factors such as high light-absorption rate and occlusions, resulting in
incomplete datasets. Inferring surface structures in data-missing regions and
successfully reconstructing the surface poses a challenge. In this paper, we
present a Principal Component Analysis (PCA) based model for surface
reconstruction from incomplete point cloud data. Initially, we employ PCA to
estimate the normal information of the underlying surface from the available
point cloud data. This estimated normal information serves as a regularizer in
our model, guiding the reconstruction of the surface, particularly in areas
with missing data. Additionally, we introduce an operator-splitting method to
effectively solve the proposed model. Through systematic experimentation, we
demonstrate that our model successfully infers surface structures in
data-missing regions and well reconstructs the underlying surfaces,
outperforming existing methodologies.

</details>


### [54] [Camera Splatting for Continuous View Optimization](https://arxiv.org/abs/2509.15677)
*Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的视图优化框架Camera Splatting，将每个相机建模为三维高斯分布（camera splat），通过优化这些分布以生成新视角的图像，并相较于FVS方法在复杂反射和细腻纹理方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的视图采样方法在处理复杂的视角相关现象（如金属反射、复杂纹理）时表现有限，需要更优的视点分布和采样策略，以提升新视角合成的质量。

Method: 将每个相机建模为3D高斯分布（camera splat），并在物体表面附近采样3D点来放置虚拟相机（point cameras）。通过不断优化调整这些camera splats的分布，使得从point cameras观察时能够获得预期的分布，实现视图优化。

Result: 与传统的Farthest View Sampling（FVS）方法相比，Camera Splatting能更好地捕捉视点相关的复杂现象，如强烈金属反射和精细文字纹理，合成质量更高。

Conclusion: Camera Splatting为新视角合成任务提供了一种有效的视图优化手段，显著提升了复杂场景下的渲染表现，优于传统方法。

Abstract: We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.

</details>


### [55] [Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model](https://arxiv.org/abs/2509.15678)
*Sidra Hanif,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种改进的手写笔画生成方法，通过结合多尺度注意力特征和单词布局信息，更好地模仿书法风格，采用条件扩散模型生成笔画，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手写笔画生成方法在风格模仿中特征利用有限，尤其忽略了单词间距等书写布局，导致生成的手写风格不一致。研究动机是提升手写模仿的风格一致性和质量。

Method: 1）提出融合多尺度注意力特征，捕捉局部与全局书法风格特性；2）将单词布局（如单词间距）纳入笔画生成过程；3）采用条件扩散模型进行笔画生成，相较图片生成模型，能提供时序坐标信息，提升风格模仿度。

Result: 实验表明，所提条件扩散模型在笔画生成方面优于现有技术方法，并在与最新的图像生成网络对比中表现出较强竞争力。

Conclusion: 结合多尺度风格特征与单词布局信息、采用条件扩散模型，可实现更一致、风格化的手写笔画生成，显著提升书法风格模仿与笔画生成的表现。

Abstract: Handwriting stroke generation is crucial for improving the performance of
tasks such as handwriting recognition and writers order recovery. In
handwriting stroke generation, it is significantly important to imitate the
sample calligraphic style. The previous studies have suggested utilizing the
calligraphic features of the handwriting. However, they had not considered word
spacing (word layout) as an explicit handwriting feature, which results in
inconsistent word spacing for style imitation. Firstly, this work proposes
multi-scale attention features for calligraphic style imitation. These
multi-scale feature embeddings highlight the local and global style features.
Secondly, we propose to include the words layout, which facilitates word
spacing for handwriting stroke generation. Moreover, we propose a conditional
diffusion model to predict strokes in contrast to previous work, which directly
generated style images. Stroke generation provides additional temporal
coordinate information, which is lacking in image generation. Hence, our
proposed conditional diffusion model for stroke generation is guided by
calligraphic style and word layout for better handwriting imitation and stroke
generation in a calligraphic style. Our experimentation shows that the proposed
diffusion model outperforms the current state-of-the-art stroke generation and
is competitive with recent image generation networks.

</details>


### [56] [Saccadic Vision for Fine-Grained Visual Classification](https://arxiv.org/abs/2509.15688)
*Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim*

Main category: cs.CV

TL;DR: 本文提出了一种受人类视觉启发的新细粒度视觉分类方法，通过两阶段提取和融合粗略与细致特征，有效提升了分类性能，并在多个基准数据集上取得了与现有最先进方法可比甚至更好的表现。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类需要区分极其相似类别的微小差异，但现有方法对部位的定位依赖复杂网络，特征冗余高且效率不佳。因此，亟需一种能高效、有效利用信息，并减少冗余的新方法。

Method: 方法受到人眼“扫视”机制启发，分为两阶段：首先提取图像粗略特征并生成采样图；随后采样焦点区域（Fixation patches），用共享权重的编码器并行编码。利用选择性注意机制加权每个焦点区，然后与外围特征进行融合。采用非极大值抑制减少焦点采样的空间重叠，防止冗余。

Result: 在CUB-200-2011、NABirds、Food-101和Stanford-Dogs等标准数据集，以及多种昆虫分类数据集中，方法表现出与当前最强方案相当甚至更优的分类效果，并且明显优于自带的基础编码器。

Conclusion: 新提出的类人视觉采样与融合方法，能有效捕捉关键细粒度特征，提升分类表现，同时简化了特征冗余和定位难度，在细粒度视觉分类领域展现出良好应用前景。

Abstract: Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.

</details>


### [57] [SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions](https://arxiv.org/abs/2509.15693)
*Cristian Sbrolli,Matteo Matteucci*

Main category: cs.CV

TL;DR: 该论文提出了一个名为SceneForge的新框架，通过结构化多物体场景组合提升3D点云与文本的对比对齐能力，并且实验证明在多个任务上显著提升了表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云与文本对比学习受限于大规模3D-文本数据集稀缺及数据复杂性不足，影响了模型表现，因此亟需有效的数据增强和结构建模方法。

Method: SceneForge利用单个3D形状构建带有明确空间关系的多物体场景，并搭配由大语言模型改写的多物体描述，通过生成这些结构化、组合化样本来扩展训练集，从而增强对比学习。文中详细探讨了单场景物体数、组合样本占比及场景构建策略等关键设计要素。

Result: SceneForge在ModelNet、ScanObjNN、Objaverse-LVIS和ScanNet上的zero-shot分类，以及ShapeNetPart上的few-shot部件分割任务中，均显著提升了模型性能。其增强方法对编码器架构无关，始终带来性能提升。同时，还改善了ScanQA上的3D视觉问答、在检索等多场景复杂性任务中的表现，并展现了空间推理能力。

Conclusion: SceneForge通过结构化的组合式增强，有效缓解了3D-文本配对数据不足的瓶颈，在多种3D任务和不同模型体系下均带来稳定且显著的增益，具有广泛通用性和实用意义。

Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive
learning. We introduce SceneForge, a novel framework that enhances contrastive
alignment between 3D point clouds and text through structured multi-object
scene compositions. SceneForge leverages individual 3D shapes to construct
multi-object scenes with explicit spatial relations, pairing them with coherent
multi-object descriptions refined by a large language model. By augmenting
contrastive training with these structured, compositional samples, SceneForge
effectively addresses the scarcity of large-scale 3D-text datasets,
significantly enriching data complexity and diversity. We systematically
investigate critical design elements, such as the optimal number of objects per
scene, the proportion of compositional samples in training batches, and scene
construction strategies. Extensive experiments demonstrate that SceneForge
delivers substantial performance gains across multiple tasks, including
zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,
as well as few-shot part segmentation on ShapeNetPart. SceneForge's
compositional augmentations are model-agnostic, consistently improving
performance across multiple encoder architectures. Moreover, SceneForge
improves 3D visual question answering on ScanQA, generalizes robustly to
retrieval scenarios with increasing scene complexity, and showcases spatial
reasoning capabilities by adapting spatial configurations to align precisely
with textual instructions.

</details>


### [58] [ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models](https://arxiv.org/abs/2509.15695)
*Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su*

Main category: cs.CV

TL;DR: 本文提出了ORIC基准，用于系统评估大型视觉-语言模型（LVLMs）在非典型背景下的识别能力，揭示了模型在情境不一致时易出现物体误识别和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务如图像描述和视觉问答等方面取得了进展，但在物体与背景关系不符合常规时（如该出现的物体没有出现，或不该出现的物体出现），仍容易产生错误识别。本文旨在深入分析LVLMs在此类场景下表现不佳的原因。

Method: 作者提出了Object Recognition in Incongruous Context（ORIC）基准，采用两种采样策略：（1）由大型语言模型引导，选取背景不符的物体；（2）由CLIP引导，检测容易被模型虚构出来的、实际不存在但看似合理的物体。并对18种LVLMs和2种开放词汇检测模型进行了系统评测。

Result: 实验结果显示，主流LVLMs和检测模型在情境不一致场景下仍存在显著的识别能力不足，频繁出现误识别和幻觉现象。

Conclusion: 该研究为理解LVLMs在情境感知方面的局限性提供了实证依据，对后续提升模型上下文感知能力、避免错误识别具有指导意义。

Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.

</details>


### [59] [Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](https://arxiv.org/abs/2509.16163)
*Het Patel,Muzammil Allie,Qian Zhang,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.CV

TL;DR: 本文提出了一种基于张量分解的轻量级防御方法，可直接用于现有的视觉语言模型（VLM），无需重新训练或修改架构，有效提升模型在遭受对抗攻击时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然在多模态理解任务中表现出色，但容易受到对抗攻击的影响。当前的防御方法通常需要高昂的重训练代价或修改模型结构，实用性有限。因此，亟需一种无需重新训练、能快速部署的防御方案。

Method: 作者提出利用张量分解技术（如Tensor Train）对视觉编码器的表示进行分解与重构，从而过滤对抗噪声、保留视觉意义。所提方法对预训练VLM完全无侵入性，可作为插件直接应用。实验重点评估了在CLIP模型上的效果，并细致分析不同分解参数对防御效果的影响。

Result: 该方法在COCO和Flickr30K数据集上的对抗防御测试中表现显著：在Flickr30K数据集上，Recall@1准确率由7.5%提升至19.8%，恢复了12.3%的性能损失；在COCO数据集上，准确率由3.8%提升至11.9%，恢复了8.1%的性能损失。分析进一步表明，选择较低秩（8-32）和较弱残差（α=0.1-0.2）组合下效果最佳。

Conclusion: 作者提出的方法能以极小的计算开销和部署难度，为现有视觉语言模型带来明显的对抗鲁棒性提升，是具有很高实际应用价值的即插即用解决方案。

Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.

</details>


### [60] [Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance](https://arxiv.org/abs/2509.15704)
*Yuxuan Liang,Xu Li,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 本文提出了一种无须训练的金字塔式视觉Token剪枝方法（PTP），显著减少了大型视觉-语言模型（LVLMs）在处理高分辨率图片时的计算量与延时，并保持了较高的多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs难以高效处理高分辨率图像，因为常见方法会将图像分割为多个子图，极大增加了视觉Token数量，从而导致推理计算量和延迟呈指数级增长。

Method: 提出了Pyramid Token Pruning（PTP）方法，无需额外训练，通过结合自下而上的视觉显著性机制（在区域和token层面）与自上而下的指令引导，对token进行重要性甄选。PTP会优先保留来自显著区域的token，并结合指令，进一步聚焦于与任务相关的token。

Result: 在13个多样化基准测试上进行了广泛实验证明，PTP显著降低了计算消耗和推理延迟，同时性能损失极小。

Conclusion: PTP能够高效提升LVLMs处理高分辨率图片的能力，为多模态理解任务提供了一种实用且无需训练的高效解决方案。

Abstract: Large Vision-Language Models (LVLMs) have significantly advanced multimodal
understanding but still struggle with efficiently processing high-resolution
images. Recent approaches partition high-resolution images into multiple
sub-images, dramatically increasing the number of visual tokens and causing
exponential computational overhead during inference. To address these
limitations, we propose a training-free token pruning strategy, Pyramid Token
Pruning (PTP), that integrates bottom-up visual saliency at both region and
token levels with top-down instruction-guided importance. Inspired by human
visual attention mechanisms, PTP selectively retains more tokens from visually
salient regions and further leverages textual instructions to pinpoint tokens
most relevant to specific multimodal tasks. Extensive experiments across 13
diverse benchmarks demonstrate that our method substantially reduces
computational overhead and inference latency with minimal performance loss.

</details>


### [61] [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
*Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Manzano的多模态大模型框架，能够同时提升图像理解与内容生成能力，突破了开源模型此前在这两项能力上的权衡困境。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大模型虽然能理解和生成视觉内容，但往往面临两者性能难以兼顾的瓶颈，需要改进架构和训练流程以实现统一且高效的性能。

Method: 提出Manzano架构：一套混合型图像分词器结合精心设计的训练流程，采用共享视觉编码器，分别连接连续/离散两类图像适配器，面向图像理解与生成，统一在一个语义空间中实现表征。同时，采用自回归LLM统一预测高层语义，再通过扩散解码器将图像token还原为像素。

Result: 在多个理解和生成任务上，Manzano表现优异，达到统一模型中的SOTA，在特定文字丰富场景下媲美专用模型。实验显示不同任务间冲突极小，模型规模扩展带来持续性能提升，证实了混合分词器架构的有效性。

Conclusion: Manzano通过创新架构和统一训练范式，有效解决了多模态大模型理解和生成能力难以兼顾的问题，为后续多模态统一建模和扩展奠定了基础。

Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and
generate visual content hold immense potential. However, existing open-source
models often suffer from a performance trade-off between these capabilities. We
present Manzano, a simple and scalable unified framework that substantially
reduces this tension by coupling a hybrid image tokenizer with a well-curated
training recipe. A single shared vision encoder feeds two lightweight adapters
that produce continuous embeddings for image-to-text understanding and discrete
tokens for text-to-image generation within a common semantic space. A unified
autoregressive LLM predicts high-level semantics in the form of text and image
tokens, with an auxiliary diffusion decoder subsequently translating the image
tokens into pixels. The architecture, together with a unified training recipe
over understanding and generation data, enables scalable joint learning of both
capabilities. Manzano achieves state-of-the-art results among unified models,
and is competitive with specialist models, particularly on text-rich
evaluation. Our studies show minimal task conflicts and consistent gains from
scaling model size, validating our design choice of a hybrid tokenizer.

</details>


### [62] [SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark](https://arxiv.org/abs/2509.15706)
*Chi Yang,Fu Wang,Xiaofei Yang,Hao Huang,Weijia Cao,Xiaowen Chu*

Main category: cs.CV

TL;DR: 本论文提出了一个用于三维云相态结构重建的数据集和基线框架，推动卫星观测到3D云相态剖面的转换，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 云相剖面对数值天气预报（NWP）至关重要，因为它直接影响辐射传输及降水过程。然而，目前缺乏高精度、适用于机器学习的三维云相剖面数据集和有效的自动识别方法。本文旨在填补这一空白，推动其在实际天气预报系统中的应用。

Method: 作者构建了一个多模态云相剖面数据集，该数据集集成了地球同步卫星的高时空分辨率多光谱可见光（VIS）与热红外（TIR）图像，以及空间激光雷达（CALIOP/CALIPSO）和雷达（CPR/CloudSat）的精确云相垂直剖面。将此视为一个有监督学习任务，采用SGMAGNet模型，并与多种UNet变体及SegNet等多尺度空间结构模型比较性能。

Result: SGMAGNet在重建云相结构（尤其是多层云和边界过渡区）方面表现最优。具体在精度（0.922）、召回率（0.858）、F1分数（0.763）及IoU（0.617）等主要指标上，均显著超越其他对比模型。

Conclusion: SGMAGNet适用于复杂多层云相的三维重建任务，有望推进云相资料在数值天气预报系统中的实际融合应用，提高云微物理过程参数化的精度。

Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as
they directly affect radiative transfer and precipitation processes. In this
study, we present a benchmark dataset and a baseline framework for transforming
multimodal satellite observations into detailed 3D cloud phase structures,
aiming toward operational cloud phase profile retrieval and future integration
with NWP systems to improve cloud microphysics parameterization. The multimodal
observations consist of (1) high--spatiotemporal--resolution, multi-band
visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,
and (2) accurate vertical cloud phase profiles from spaceborne lidar
(CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of
synchronized image--profile pairs across diverse cloud regimes, defining a
supervised learning task: given VIS/TIR patches, predict the corresponding 3D
cloud phase structure. We adopt SGMAGNet as the main model and compare it with
several baseline architectures, including UNet variants and SegNet, all
designed to capture multi-scale spatial patterns. Model performance is
evaluated using standard classification metrics, including Precision, Recall,
F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior
performance in cloud phase reconstruction, particularly in complex multi-layer
and boundary transition regions. Quantitatively, SGMAGNet attains a Precision
of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,
significantly outperforming all baselines across these key metrics.

</details>


### [63] [Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method](https://arxiv.org/abs/2509.15711)
*Shuaibo Li,Zhaohu Xing,Hongqiu Wang,Pengfei Hao,Xingyu Li,Zekai Liu,Lei Zhu*

Main category: cs.CV

TL;DR: 本论文提出了MedForensics数据集和DSKI检测方法，用于识别AI生成的医学图像，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在医学成像领域的快速发展，AI生成的医学假图像可能威胁医疗系统安全，包括诊断欺诈、金融诈骗和错误信息传播。然而，目前针对医学图像的取证研究不足，并缺乏专门的数据集，现有方法又无法充分应对医学图像的独特性质。

Method: 作者构建了MedForensics大规模医学取证数据集，包含六种医学模态和12种主流生成式模型。同时提出了DSKI检测器，通过双阶段知识注入，融合视觉和语言特征进行检测。DSKI包括：1）跨域细痕适配器（CDFA），在训练期间从空间域和噪声域提取伪造线索；2）医学取证检索模块（MFRM），在测试期间通过few-shot检索提升准确率。

Result: 实验结果显示，DSKI在多种医学模态下的检测准确率明显优于现有方法和人类专家。

Conclusion: 该研究为AI生成医学图像的取证检测提供了新的数据资源和检测框架，提升了检测性能，促进了医学影像安全保障。

Abstract: The rapid advancement of generative AI in medical imaging has introduced both
significant opportunities and serious challenges, especially the risk that fake
medical images could undermine healthcare systems. These synthetic images pose
serious risks, such as diagnostic deception, financial fraud, and
misinformation. However, research on medical forensics to counter these threats
remains limited, and there is a critical lack of comprehensive datasets
specifically tailored for this field. Additionally, existing media forensic
methods, which are primarily designed for natural or facial images, are
inadequate for capturing the distinct characteristics and subtle artifacts of
AI-generated medical images. To tackle these challenges, we introduce
\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six
medical modalities and twelve state-of-the-art medical generative models. We
also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage
\textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language
feature space tailored for the detection of AI-generated medical images. DSKI
comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for
extracting subtle forgery clues from both spatial and noise domains during
training, and 2) a medical forensic retrieval module (MFRM) that boosts
detection accuracy through few-shot retrieval during testing. Experimental
results demonstrate that DSKI significantly outperforms both existing methods
and human experts, achieving superior accuracy across multiple medical
modalities.

</details>


### [64] [TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection](https://arxiv.org/abs/2509.15741)
*Laixin Zhang,Shuaibo Li,Wei Ma,Hongbin Zha*

Main category: cs.CV

TL;DR: 本文提出了一种名为TrueMoE的新型框架，通过联合多个专门化的判别子空间以提升合成图像的检测性能，并展示了该方法在多个生成模型下的优越泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的快速发展，检测合成图像成为重要挑战。现有方法通常试图构建单一判别空间，难以同时应对复杂多变的伪造模式，泛化能力有限。因此需要新的检测策略来提升对新型伪造的识别能力。

Method: 作者提出了TrueMoE框架，将检测任务重塑为多个轻量级、专门化判别子空间间的协作推理过程。其核心为判别专家阵列（DEA），以流形结构和感知粒度为主轴分布专家，配合稀疏和密集的双路由机制，自适应地将输入分配给最相关的判别专家。

Result: 通过大量实验，TrueMoE在多种生成模型下表现出更强的泛化性和鲁棒性。

Conclusion: TrueMoE能有效捕捉多样的伪造特征，在合成图像检测任务中胜过传统单一判别空间方法，是提升伪造检测泛化能力的有力工具。

Abstract: The rapid progress of generative models has made synthetic image detection an
increasingly critical task. Most existing approaches attempt to construct a
single, universal discriminative space to separate real from fake content.
However, such unified spaces tend to be complex and brittle, often struggling
to generalize to unseen generative patterns. In this work, we propose TrueMoE,
a novel dual-routing Mixture-of-Discriminative-Experts framework that
reformulates the detection task as a collaborative inference across multiple
specialized and lightweight discriminative subspaces. At the core of TrueMoE is
a Discriminative Expert Array (DEA) organized along complementary axes of
manifold structure and perceptual granularity, enabling diverse forgery cues to
be captured across subspaces. A dual-routing mechanism, comprising a
granularity-aware sparse router and a manifold-aware dense router, adaptively
assigns input images to the most relevant experts. Extensive experiments across
a wide spectrum of generative models demonstrate that TrueMoE achieves superior
generalization and robustness.

</details>


### [65] [Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields](https://arxiv.org/abs/2509.15748)
*Tony Lindeberg*

Main category: cs.CV

TL;DR: 本文探讨了在自然图像变化（如观察同类物体在不同视角下）下，视觉系统初级层感受野响应会因几何变换而变化。为此，论文提出通过协变感受野族的方法，并推导了不同参数下空间和时空感受野响应间的关系，为感受野设计和理论建模提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现实世界中同类物体或事件在不同观察条件下表现出的图像结构变化极大，这使得视觉系统必须能够适应各种几何图像变换。为应对这种变异性，需要研究感受野在多参数变化下的响应关系，提升视觉系统的鲁棒性和高效性。

Method: 作者提出基于协变感受野族，通过推导感受野响应在多参数族下的关系。方法包括推导微分级的无穷小关系（结合半群和李群的概念）和宏观的平滑级联性质（描述各尺度间感受野输出的级联计算，并与李代数结构相关联）。

Result: 作者系统性地分析和推导了不同参数下空间与时空感受野响应之间的具体关系。这些推导为快速高效地计算多参数感受野族响应提供了理论依据。

Conclusion: 研究结果不仅深化了对感受野参数变化下响应关系的理解，也为基于多参数感受野族的高效计算以及生物视觉中简单细胞理论建模提供了新工具和理论指导。

Abstract: Because of the variabilities of real-world image structures under the natural
image transformations that arise when observing similar objects or
spatio-temporal events under different viewing conditions, the receptive field
responses computed in the earliest layers of the visual hierarchy may be
strongly influenced by such geometric image transformations. One way of
handling this variability is by basing the vision system on covariant receptive
field families, which expand the receptive field shapes over the degrees of
freedom in the image transformations.
  This paper addresses the problem of deriving relationships between spatial
and spatio-temporal receptive field responses obtained for different values of
the shape parameters in the resulting multi-parameter families of receptive
fields. For this purpose, we derive both (i) infinitesimal relationships,
roughly corresponding to a combination of notions from semi-groups and Lie
groups, as well as (ii) macroscopic cascade smoothing properties, which
describe how receptive field responses at coarser spatial and temporal scales
can be computed by applying smaller support incremental filters to the output
from corresponding receptive fields at finer spatial and temporal scales,
structurally related to the notion of Lie algebras, although with directional
preferences.
  The presented results provide (i) a deeper understanding of the relationships
between spatial and spatio-temporal receptive field responses for different
values of the filter parameters, which can be used for both (ii) designing more
efficient schemes for computing receptive field responses over populations of
multi-parameter families of receptive fields, as well as (iii)~formulating
idealized theoretical models of the computations of simple cells in biological
vision.

</details>


### [66] [FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion](https://arxiv.org/abs/2509.15750)
*Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu*

Main category: cs.CV

TL;DR: 本文提出了FloorSAM，一种基于点云密度图和Segment Anything Model（SAM）的地面平面图重建框架，显著提升了噪声与多样化室内结构下的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统楼层平面图重建方法（如几何算法与基于Mask R-CNN的深度学习）在处理点云数据时，常面临噪声、泛化能力不足及几何细节丢失等问题。室内导航与BIM等应用高度依赖高质量楼层平面信息，有必要开发更精确、泛化性强的自动重建方法。

Method: 提出FloorSAM框架，核心技术包括：(1) 基于栅格过滤、自适应分辨率投影和图像增强，生成鲁棒的自顶向下密度图；(2) 利用SAM的zero-shot分割能力，实现高精度复杂布局下的房间分割；(3) 结合自适应提示点、多阶段过滤等生成房间掩码；(4) 联合掩码与点云进行轮廓提取与正则化，恢复房间及其拓扑关系。

Result: 在Giblayout和ISPRS等公开数据集上测试，FloorSAM在精度、召回率和鲁棒性方面均超越传统几何和深度学习方法，尤其在噪声大、结构复杂的场景下效果突出。

Conclusion: FloorSAM框架有效提升了点云数据重建楼层平面图的质量与适应性，为室内导航、BIM等应用提供了更优的技术选择。代码和材料已开源，有利于后续研究和工程应用。

Abstract: Reconstructing building floor plans from point cloud data is key for indoor
navigation, BIM, and precise measurements. Traditional methods like geometric
algorithms and Mask R-CNN-based deep learning often face issues with noise,
limited generalization, and loss of geometric details. We propose FloorSAM, a
framework that integrates point cloud density maps with the Segment Anything
Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using
grid-based filtering, adaptive resolution projection, and image enhancement, we
create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for
precise room segmentation, improving reconstruction across diverse layouts.
Room masks are generated via adaptive prompt points and multistage filtering,
followed by joint mask and point cloud analysis for contour extraction and
regularization. This produces accurate floor plans and recovers room
topological relationships. Tests on Giblayout and ISPRS datasets show better
accuracy, recall, and robustness than traditional methods, especially in noisy
and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.

</details>


### [67] [Simulated Cortical Magnification Supports Self-Supervised Object Learning](https://arxiv.org/abs/2509.15751)
*Zhengyang Yu,Arthur Aubret,Chen Yu,Jochen Triesch*

Main category: cs.CV

TL;DR: 本文研究了在自监督学习模型中加入类似人类视觉中心高分辨率、周边低分辨率的视觉结构（即眼中心化视觉），并发现这种仿生处理可以提升模型学到的物体表征的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习模型虽模仿幼儿视觉体验，但忽略了人类视觉中心化（中心清晰、周边模糊）这一关键特征。作者想探索这种特征是否有助于更好地发展物体语义表征。

Method: 采用两个人类主观视角视频数据集，通过人类视觉中心化和皮层放大建模，将视频图像做中心清晰、边缘模糊的处理，然后用这些数据训练两类基于生物的自监督学习模型，训练目标是时间序列相关的目标。

Result: 实验显示，采用带有中心化视觉结构的输入，提高了模型所学物体表征的质量。分析还发现，这种视觉处理使得物体“变大”，并在中心与周边视觉信息间获得了更优的权衡。

Conclusion: 模拟人类中心化视觉能让自监督学习模型学到更真实和有效的物体表征，为模拟人类视觉学习提供了更逼真、有效的新方法。

Abstract: Recent self-supervised learning models simulate the development of semantic
object representations by training on visual experience similar to that of
toddlers. However, these models ignore the foveated nature of human vision with
high/low resolution in the center/periphery of the visual field. Here, we
investigate the role of this varying resolution in the development of object
representations. We leverage two datasets of egocentric videos that capture the
visual experience of humans during interactions with objects. We apply models
of human foveation and cortical magnification to modify these inputs, such that
the visual content becomes less distinct towards the periphery. The resulting
sequences are used to train two bio-inspired self-supervised learning models
that implement a time-based learning objective. Our results show that modeling
aspects of foveated vision improves the quality of the learned object
representations in this setting. Our analysis suggests that this improvement
comes from making objects appear bigger and inducing a better trade-off between
central and peripheral visual information. Overall, this work takes a step
towards making models of humans' learning of visual representations more
realistic and performant.

</details>


### [68] [MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection](https://arxiv.org/abs/2509.15753)
*Yang Li,Tingfa Xu,Shuyan Bai,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: 本文提出了MCOD，这是首个专为多光谱伪装目标检测（COD）设计的具有挑战性基准数据集，弥补了目前仅有RGB数据集的空白，推动了多光谱方法的发展。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法多基于RGB图像，难以应对复杂环境；而多光谱成像有助于提升前背景分割效果，但缺乏公开的多光谱COD数据集制约了进一步研究。

Method: 作者构建了MCOD多光谱COD数据集，包括丰富的挑战属性、真实多样的场景、精确的像素级标注，并在该数据集上对十一种代表性方法进行了基准测试。

Result: 实验结果显示：面对更具挑战的数据集任务，现有方法整体性能下降，但结合多光谱模态后，检测鲁棒性和表现显著提升，验证了多光谱信息的价值。

Conclusion: MCOD是多光谱伪装目标检测领域的重要资源，将亟需推动相关研究，数据集已对外开放，期望促进未来该领域的发展。

Abstract: Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into natural scenes. Although RGB-based methods have advanced, their
performance remains limited under challenging conditions. Multispectral
imagery, providing rich spectral information, offers a promising alternative
for enhanced foreground-background discrimination. However, existing COD
benchmark datasets are exclusively RGB-based, lacking essential support for
multispectral approaches, which has impeded progress in this area. To address
this gap, we introduce MCOD, the first challenging benchmark dataset
specifically designed for multispectral camouflaged object detection. MCOD
features three key advantages: (i) Comprehensive challenge attributes: It
captures real-world difficulties such as small object sizes and extreme
lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world
scenarios: The dataset spans a wide range of natural environments to better
reflect practical applications. (iii) High-quality pixel-level annotations:
Each image is manually annotated with precise object masks and corresponding
challenge attribute labels. We benchmark eleven representative COD methods on
MCOD, observing a consistent performance drop due to increased task difficulty.
Notably, integrating multispectral modalities substantially alleviates this
degradation, highlighting the value of spectral information in enhancing
detection robustness. We anticipate MCOD will provide a strong foundation for
future research in multispectral camouflaged object detection. The dataset is
publicly accessible at https://github.com/yl2900260-bit/MCOD.

</details>


### [69] [Overview of PlantCLEF 2024: multi-species plant identification in vegetation plot images](https://arxiv.org/abs/2509.15768)
*Herve Goeau,Vincent Espitalier,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文介绍了PlantCLEF 2024挑战赛，以高分辨率样方图像中多标签植物识别为目标，评估AI在生态学领域内的应用表现。


<details>
  <summary>Details</summary>
Motivation: 由于样方图像对生态学研究至关重要，但人工辨识成本高，效率低，急需引入人工智能方法提升物种识别效率和研究覆盖面。

Method: 比赛设置了高难度多标签分类任务，提供了专家标注的上千张样方图片作为测试集，覆盖800余种植物。训练数据包括170万张单植物图片并配以已预训练的视觉Transformer模型。参赛者需利用这些单标签训练数据预测样方图像中的所有物种。

Result: 本文汇总了各参赛团队所用方法及模型，并对取得的结果进行了详细展示和评估。

Conclusion: AI方法（尤其是基于Transformer的视觉模型）能在复杂生态环境下实现多物种自动识别，有力推动生态学研究的规模化与自动化。

Abstract: Plot images are essential for ecological studies, enabling standardized
sampling, biodiversity assessment, long-term monitoring and remote, large-scale
surveys. Plot images are typically fifty centimetres or one square meter in
size, and botanists meticulously identify all the species found there. The
integration of AI could significantly improve the efficiency of specialists,
helping them to extend the scope and coverage of ecological studies. To
evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new
test set of thousands of multi-label images annotated by experts and covering
over 800 species. In addition, it provides a large training set of 1.7 million
individual plant images as well as state-of-the-art vision transformer models
pre-trained on this data. The task is evaluated as a (weakly-labeled)
multi-label classification task where the aim is to predict all the plant
species present on a high-resolution plot image (using the single-label
training data). In this paper, we provide an detailed description of the data,
the evaluation methodology, the methods and models employed by the participants
and the results achieved.

</details>


### [70] [Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation](https://arxiv.org/abs/2509.15772)
*Weimin Bai,Yubo Li,Weijian Luo,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 本文提出VLM3D，一种将大型视觉-语言模型（VLM）融入SDS管线用于文本到3D生成的方法，有效提升了语义精度和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 现有SDS方法依赖CLIP类文本编码器，只能实现粗粒度的语义对齐，难以处理细致提示；且2D扩散先验缺乏明确的3D空间约束，导致多物体场景几何不一致。

Method: VLM3D将大型视觉-语言模型作为可微分语义与空间先验整合进SDS流程。此方法借助VLM丰富的语言-视觉监督，实现细粒度语义对齐和强化空间理解，从而提升3D一致性和多物体场景的关系推理能力。具体方法以Qwen2.5-VL为基础，并在GPTeval3D基准上进行评测。

Result: 在多样物体与复杂场景实验中，VLM3D在语义保真度、几何连贯性和空间正确性上明显优于以往SDS方法。

Conclusion: 通过引入大型视觉-语言模型，VLM3D显著提升了文本到3D生成的语义和空间表现，展现了整合VLM作为先验信息在3D生成任务中的潜力。

Abstract: Score Distillation Sampling (SDS) enables high-quality text-to-3D generation
by supervising 3D models through the denoising of multi-view 2D renderings,
using a pretrained text-to-image diffusion model to align with the input prompt
and ensure 3D consistency. However, existing SDS-based methods face two
fundamental limitations: (1) their reliance on CLIP-style text encoders leads
to coarse semantic alignment and struggles with fine-grained prompts; and (2)
2D diffusion priors lack explicit 3D spatial constraints, resulting in
geometric inconsistencies and inaccurate object relationships in multi-object
scenes. To address these challenges, we propose VLM3D, a novel text-to-3D
generation framework that integrates large vision-language models (VLMs) into
the SDS pipeline as differentiable semantic and spatial priors. Unlike standard
text-to-image diffusion priors, VLMs leverage rich language-grounded
supervision that enables fine-grained prompt alignment. Moreover, their
inherent vision language modeling provides strong spatial understanding, which
significantly enhances 3D consistency for single-object generation and improves
relational reasoning in multi-object scenes. We instantiate VLM3D based on the
open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.
Experiments across diverse objects and complex scenes show that VLM3D
significantly outperforms prior SDS-based methods in semantic fidelity,
geometric coherence, and spatial correctness.

</details>


### [71] [Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution](https://arxiv.org/abs/2509.15781)
*Chang Soo Lim,Joonyoung Moon,Donghyeon Cho*

Main category: cs.CV

TL;DR: 本文提出了SCOPE框架，将 Cutie 的查询分割能力与 SAM2 的强大特征表示融合，并通过运动预测模块提升视频对象分割的时序稳定性，最终在竞赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: Cutie 在基于查询的分割有优势但特征能力有限，SAM2 拥有更丰富的特征表示却缺乏时序建模，两者各有优劣。为突破 VOS 任务中模型特征和时序稳定性的限制，作者希望结合两者长处。

Method: 将 Cutie 的 encoder 替换为 SAM2 的 ViT encoder，并引入运动预测模块来增强时序信息。同时，采用集成方法，将原始 Cutie、SAM2 及改进模型共同预测以提升效果。

Result: 该集成方法在第七届LSVOS Challenge的MOSEv2 track中获得了第三名，证明了该方法的有效性。

Conclusion: 融合丰富特征表示和运动预测能够显著提升视频对象分割的鲁棒性和精度。

Abstract: Video object segmentation (VOS) is a challenging task with wide applications
such as video editing and autonomous driving. While Cutie provides strong
query-based segmentation and SAM2 offers enriched representations via a
pretrained ViT encoder, each has limitations in feature capacity and temporal
modeling. In this report, we propose a framework that integrates their
complementary strengths by replacing the encoder of Cutie with the ViT encoder
of SAM2 and introducing a motion prediction module for temporal stability. We
further adopt an ensemble strategy combining Cutie, SAM2, and our variant,
achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to
our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This
demonstrates the effectiveness of enriched feature representation and motion
prediction for robust video object segmentation. The code is available at
https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.

</details>


### [72] [Ideal Registration? Segmentation is All You Need](https://arxiv.org/abs/2509.15784)
*Xiang Chen,Fengting Zhang,Qinghao Liu,Min Liu,Kun Wu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: 本文引入了SegReg，一种基于分割驱动的图像配准框架，通过针对不同解剖结构的适应性正则化，实现了更高精度的图像配准，特别适合医学场景。该方法显著优于传统深度学习配准方法，且配准效果与分割质量几乎线性相关。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像配准方法通常采用全局均匀平滑约束，难以处理真实解剖运动中区域差异性较大的变形，限制了在复杂医学图像配准任务中的应用效果。

Method: SegReg首先对输入的运动图像和参考图像进行解剖区域分割，将图像划分为多个有意义的子区域。然后对每个子区域分别用相同配准骨干网络计算局部变形场，最后将各个局部变形场集成为全局变形场，实现整体配准。该框架利用分割驱动实现区域自适应正则，提高配准的灵活性和准确度。

Result: 在使用真实分割的情况下，对关键解剖结构的配准Dice分数高达98.23%，在心脏、腹部及肺部三个临床配准场景下，即使采用自动分割，也比现有方法提升2-12%。此外，注册精度几乎与分割质量呈线性关系。

Conclusion: SegReg不仅提升了配准效果，还将高精度配准问题转化为分割问题，为医学图像配准打开新思路。该方法泛化性强，有望在不同解剖场景中广泛应用。源码将在论文接收后公开。

Abstract: Deep learning has revolutionized image registration by its ability to handle
diverse tasks while achieving significant speed advantages over conventional
approaches. Current approaches, however, often employ globally uniform
smoothness constraints that fail to accommodate the complex, regionally varying
deformations characteristic of anatomical motion. To address this limitation,
we propose SegReg, a Segmentation-driven Registration framework that implements
anatomically adaptive regularization by exploiting region-specific deformation
patterns. Our SegReg first decomposes input moving and fixed images into
anatomically coherent subregions through segmentation. These localized domains
are then processed by the same registration backbone to compute optimized
partial deformation fields, which are subsequently integrated into a global
deformation field. SegReg achieves near-perfect structural alignment (98.23%
Dice on critical anatomies) using ground-truth segmentation, and outperforms
existing methods by 2-12% across three clinical registration scenarios
(cardiac, abdominal, and lung images) even with automatic segmentation. Our
SegReg demonstrates a near-linear dependence of registration accuracy on
segmentation quality, transforming the registration challenge into a
segmentation problem. The source code will be released upon manuscript
acceptance.

</details>


### [73] [CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices](https://arxiv.org/abs/2509.15785)
*Runjie Shao,Boyu Diao,Zijia An,Ruiqi Liu,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效且参数节省的持续学习框架CBPNet，通过自适应重初始化未充分利用的参数，有效解决了预训练模型冻结后塑性下降的问题，提升了模型在边缘设备上的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 为满足如机器人和自动驾驶等实时动态环境下的应用需求，需要能在边缘设备上高效持续学习。当前主流采用冻结预训练模型并结合提示进行持续学习，但容易导致模型塑性（学习新知识的能力）丧失，需要寻找新的方法来解决这一瓶颈。

Method: 提出了CBPNet框架，包含一个高效的CBP Block，通过自适应地重新初始化训练中未充分利用的参数，恢复和提升模型的学习活力，从而对抗模型塑性的衰减。

Result: 实验结果显示，在Split CIFAR-100上，CBPNet比强基线提升平均准确率超过1%；在更具挑战性的Split ImageNet-R上取得了69.41%的先进准确率。此外，引入的参数仅占骨干网络的0.2%以下，验证了方法的高效节省。

Conclusion: CBPNet能够有效缓解由于冻结骨干模型带来的塑性丧失问题，实现了高效且参数经济的持续学习，适用于边缘设备等资源受限环境。

Abstract: To meet the demands of applications like robotics and autonomous driving that
require real-time responses to dynamic environments, efficient continual
learning methods suitable for edge devices have attracted increasing attention.
In this transition, using frozen pretrained models with prompts has become a
mainstream strategy to combat catastrophic forgetting. However, this approach
introduces a new critical bottleneck: plasticity loss, where the model's
ability to learn new knowledge diminishes due to the frozen backbone and the
limited capacity of prompt parameters. We argue that the reduction in
plasticity stems from a lack of update vitality in underutilized parameters
during the training process. To this end, we propose the Continual
Backpropagation Prompt Network (CBPNet), an effective and parameter efficient
framework designed to restore the model's learning vitality. We innovatively
integrate an Efficient CBP Block that counteracts plasticity decay by
adaptively reinitializing these underutilized parameters. Experimental results
on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.
On Split CIFAR-100, it improves average accuracy by over 1% against a strong
baseline, and on the more challenging Split ImageNet-R, it achieves a state of
the art accuracy of 69.41%. This is accomplished by training additional
parameters that constitute less than 0.2% of the backbone's size, validating
our approach.

</details>


### [74] [FoBa: A Foreground-Background co-Guided Method and New Benchmark for Remote Sensing Semantic Change Detection](https://arxiv.org/abs/2509.15788)
*Haotian Zhang,Han Guo,Keyan Chen,Hao Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一个新的遥感语义变化检测（SCD）数据集和方法，大幅扩展了变化类型和类别，同时提出了新型模型结构，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感语义变化检测数据集类别不丰富、类型不全面，而方法普遍未能充分利用变化信息，导致实际应用受限、模型性能提升受阻。

Method: 一方面，作者构建了一套新的SCD数据集LevirSCD（包含16类变化和210种具体类型，并采用更细粒度的类别划分）；另一方面，提出了前景-背景协同引导（FoBa）方法，通过前景关注目标区域、背景提供上下文信息协同引导模型学习；并引入门控交互融合（GIF）模块以及简易一致性损失函数，提升模型的空间一致性和双时相交互能力。

Result: 在SECOND、JL1及新提出的LevirSCD三个数据集上进行实验，FoBa方法在SeK指标上相较于SOTA方法分别提升了1.48%、3.61%、2.81%。

Conclusion: LevirSCD显著丰富了现有SCD数据集的变化类别与类型，FoBa方法有效提升了变化检测的准确性，具备良好的实际应用潜力。代码和数据集已经开源。

Abstract: Despite the remarkable progress achieved in remote sensing semantic change
detection (SCD), two major challenges remain. At the data level, existing SCD
datasets suffer from limited change categories, insufficient change types, and
a lack of fine-grained class definitions, making them inadequate to fully
support practical applications. At the methodological level, most current
approaches underutilize change information, typically treating it as a
post-processing step to enhance spatial consistency, which constrains further
improvements in model performance. To address these issues, we construct a new
benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the
dataset covers 16 change categories and 210 specific change types, with more
fine-grained class definitions (e.g., roads are divided into unpaved and paved
roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa)
method, which leverages foregrounds that focus on regions of interest and
backgrounds enriched with contextual information to guide the model
collaboratively, thereby alleviating semantic ambiguity while enhancing its
ability to detect subtle changes. Considering the requirements of bi-temporal
interaction and spatial consistency in SCD, we introduce a Gated Interaction
Fusion (GIF) module along with a simple consistency loss to further enhance the
model's detection performance. Extensive experiments on three datasets (SECOND,
JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive
results compared to current SOTA methods, with improvements of 1.48%, 3.61%,
and 2.81% in the SeK metric, respectively. Our code and dataset are available
at https://github.com/zmoka-zht/FoBa.

</details>


### [75] [Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization](https://arxiv.org/abs/2509.15791)
*Tan Pan,Kaiyu Guo,Dongli Xu,Zhaorui Tan,Chen Jiang,Deshu Chen,Xin Guo,Brian C. Lovell,Limei Han,Yuan Cheng,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: 本文提出了一种无需标签、以信息理论为基础的无监督领域泛化方法MS-UDG，有效提升了模型泛化能力，并在多个主流基准上取得最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习的泛化能力多聚焦于有监督学习，而无监督场景的泛化能力尤其在没有领域标签的情况下探索较少。实际应用中往往无法获得领域或类别标签，急需能够提升无标签情况下泛化性能的新方法。

Method: 作者将无监督领域泛化（UDG）形式化为学习最小充分语义表示（即只保留所有语义信息，去除无关变异）。方法上，MS-UDG结合了InfoNCE目标实现表示的充分性，以及语义-变异解耦损失、基于重构的机制促进最小性，无需类别或领域标签参与表征学习。

Result: MS-UDG在主流无监督领域泛化基准上取得了最新最优（state-of-the-art）性能，超过现有的自监督学习（SSL）和UDG方法。

Conclusion: MS-UDG在无任何标签的设定下，通过信息理论指导的优化策略，实现了优越的无监督领域泛化性能，为实际任务中泛化能力提升提供了有力方案。

Abstract: The generalization ability of deep learning has been extensively studied in
supervised settings, yet it remains less explored in unsupervised scenarios.
Recently, the Unsupervised Domain Generalization (UDG) task has been proposed
to enhance the generalization of models trained with prevalent unsupervised
learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the
challenge of distinguishing semantics from variations without category labels.
Although some recent methods have employed domain labels to tackle this issue,
such domain labels are often unavailable in real-world contexts. In this paper,
we address these limitations by formalizing UDG as the task of learning a
Minimal Sufficient Semantic Representation: a representation that (i) preserves
all semantic information shared across augmented views (sufficiency), and (ii)
maximally removes information irrelevant to semantics (minimality). We
theoretically ground these objectives from the perspective of information
theory, demonstrating that optimizing representations to achieve sufficiency
and minimality directly reduces out-of-distribution risk. Practically, we
implement this optimization through Minimal-Sufficient UDG (MS-UDG), a
learnable model by integrating (a) an InfoNCE-based objective to achieve
sufficiency; (b) two complementary components to promote minimality: a novel
semantic-variation disentanglement loss and a reconstruction-based mechanism
for capturing adequate variation. Empirically, MS-UDG sets a new
state-of-the-art on popular unsupervised domain-generalization benchmarks,
consistently outperforming existing SSL and UDG methods, without category or
domain labels during representation learning.

</details>


### [76] [TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation](https://arxiv.org/abs/2509.15795)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Hanzhang Chi,Qi Zhang,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: SAM在自然图像分割上表现优秀，但在遥感图像分割方面有局限。本文提出TASAM，通过地形和时序增强，对SAM进行轻量级扩展，显著提升了遥感图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像具有复杂地形、多尺度目标和时序变化等挑战，现有的SAM难以直接泛化于此领域。需要专门的机制提升大模型在遥感领域的适应能力。

Method: 提出TASAM框架，引入三种模块：（1）地形感知适配器，用于融合地形高程先验；（2）时序提示生成器，捕捉地表覆盖随时间的变化；（3）多尺度融合策略，加强细粒度目标分割。这些模块无需重新训练SAM主干网络，可轻量级集成。

Result: 在LoveDA、iSAID、WHU-CD三个高分辨率遥感数据集上，TASAM较零样本SAM及其它任务特定模型取得更优表现，且计算开销极小。

Conclusion: 通过面向领域的高效增强，TASAM提升了基础模型在遥感分割任务中的泛化和适用性，展示了领域自适应扩展的广阔前景，为更强健的地理空间分割提供了新思路。

Abstract: Segment Anything Model (SAM) has demonstrated impressive zero-shot
segmentation capabilities across natural image domains, but it struggles to
generalize to the unique challenges of remote sensing data, such as complex
terrain, multi-scale objects, and temporal dynamics. In this paper, we
introduce TASAM, a terrain and temporally-aware extension of SAM designed
specifically for high-resolution remote sensing image segmentation. TASAM
integrates three lightweight yet effective modules: a terrain-aware adapter
that injects elevation priors, a temporal prompt generator that captures
land-cover changes over time, and a multi-scale fusion strategy that enhances
fine-grained object delineation. Without retraining the SAM backbone, our
approach achieves substantial performance gains across three remote sensing
benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and
task-specific models with minimal computational overhead. Our results highlight
the value of domain-adaptive augmentation for foundation models and offer a
scalable path toward more robust geospatial segmentation.

</details>


### [77] [ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding](https://arxiv.org/abs/2509.15800)
*Kehua Chen*

Main category: cs.CV

TL;DR: 该论文提出了ChronoForge-RL方法，通过智能选择视频关键帧，提升视频理解的效率和准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解面临高计算量和关键帧选取不精准的问题，传统方法难以兼顾效率和语义信息保留。

Method: 提出了ChronoForge-RL框架，结合了时序顶点蒸馏（TAD）和关键帧感知的组相对策略优化（KF-GRPO）。先通过区分性可微关键帧选择策略，三阶段自动找出视频中重要时刻；TAD通过变化量打分和优先信息蒸馏筛选关键帧；KF-GRPO结合对比学习和显著性奖励引导模型充分利用帧内容及时序关系。

Result: ChronoForge-RL在VideoMME上达到69.1%、LVBench上52.7%的成绩，显著优于以往方法，并使得7B参数模型可达到与72B参数模型相当的性能。

Conclusion: ChronoForge-RL高效选取关键帧，提升视频理解的准确性和效率，为小模型实现大模型级别表现提供了新思路。

Abstract: Current state-of-the-art video understanding methods typically struggle with
two critical challenges: (1) the computational infeasibility of processing
every frame in dense video content and (2) the difficulty in identifying
semantically significant frames through naive uniform sampling strategies. In
this paper, we propose a novel video understanding framework, called
ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and
KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these
issues. Concretely, we introduce a differentiable keyframe selection mechanism
that systematically identifies semantic inflection points through a three-stage
process to enhance computational efficiency while preserving temporal
information. Then, two particular modules are proposed to enable effective
temporal reasoning: Firstly, TAD leverages variation scoring, inflection
detection, and prioritized distillation to select the most informative frames.
Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm
with a saliency-enhanced reward mechanism that explicitly incentivizes models
to leverage both frame content and temporal relationships. Finally, our
proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench
compared to baseline methods, clearly surpassing previous approaches while
enabling our 7B parameter model to achieve performance comparable to 72B
parameter alternatives.

</details>


### [78] [CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models](https://arxiv.org/abs/2509.15803)
*Fangjian Shen,Zifeng Liang,Chao Wang,Wushao Wen*

Main category: cs.CV

TL;DR: 当前的文本生成图像模型存在明显的商业品牌偏见问题，CIDER方法通过推理阶段的提示优化减轻该问题，无需重新训练模型。实验显示，该方法有效降低品牌偏见，保持图像质量，促进生成式AI的公平和可信。


<details>
  <summary>Details</summary>
Motivation: 主流T2I模型容易根据普通描述生成包含主流商业品牌的内容，这种偏见带来了伦理和法律风险，因此需要一种高效、无需重新训练的偏见消除方案。

Method: 提出CIDER框架，在推理时通过检测和优化输入提示消除偏见。核心做法包括：利用检测器判断生成内容是否包含品牌元素，借助多模态模型生成风格多样的无品牌替代品，同时引入Brand Neutrality Score（BNS）量化偏见。

Result: 在主流T2I模型上的大量实验表明，CIDER显著降低了生成内容中显性和隐性品牌偏见，同时保持了图像质量和审美效果。

Conclusion: CIDER为改进生成式AI的公平性和原创性提供了实用工具，有助于构建更可信、更公正的AI内容生成系统。

Abstract: Text-to-image (T2I) models exhibit a significant yet under-explored "brand
bias", a tendency to generate contents featuring dominant commercial brands
from generic prompts, posing ethical and legal risks. We propose CIDER, a
novel, model-agnostic framework to mitigate bias at inference-time through
prompt refinement to avoid costly retraining. CIDER uses a lightweight detector
to identify branded content and a Vision-Language Model (VLM) to generate
stylistically divergent alternatives. We introduce the Brand Neutrality Score
(BNS) to quantify this issue and perform extensive experiments on leading T2I
models. Results show CIDER significantly reduces both explicit and implicit
biases while maintaining image quality and aesthetic appeal. Our work offers a
practical solution for more original and equitable content, contributing to the
development of trustworthy generative AI.

</details>


### [79] [Boosting Active Learning with Knowledge Transfer](https://arxiv.org/abs/2509.15805)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Xiaoying Liao,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: 本文提出了一种通过知识迁移提升主动学习中不确定性估计的新方法，采用教师-学生模型结构，并在多项计算机视觉与冷冻电镜任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习中的不确定性估算方法通常依赖于复杂的辅助模型和特殊的训练方式，在特定领域如冷冻电镜分类等任务中训练困难，限制了适用性。解决此问题，需提出无需特殊训练、适应多任务的简易有效方案。

Method: 作者提出利用知识迁移的方法，通过教师-学生架构：教师为任务模型，学生为辅助模型，通过二者输出的差距量化未标记数据的不确定性。两个模型在每个主动学习周期内同步训练，且学生模型为任务无关，无需特殊训练方式。

Result: 在经典计算机视觉任务及冷冻电镜分类任务上做了大量实验，验证了所提方法在有效性和效率上均优于现有方法。

Conclusion: 所提知识迁移不确定性估计方法具备通用性和高效性，为复杂领域主动学习提供了实用新思路。

Abstract: Uncertainty estimation is at the core of Active Learning (AL). Most existing
methods resort to complex auxiliary models and advanced training fashions to
estimate uncertainty for unlabeled data. These models need special design and
hence are difficult to train especially for domain tasks, such as Cryo-Electron
Tomography (cryo-ET) classification in computational biology. To address this
challenge, we propose a novel method using knowledge transfer to boost
uncertainty estimation in AL. Specifically, we exploit the teacher-student mode
where the teacher is the task model in AL and the student is an auxiliary model
that learns from the teacher. We train the two models simultaneously in each AL
cycle and adopt a certain distance between the model outputs to measure
uncertainty for unlabeled data. The student model is task-agnostic and does not
rely on special training fashions (e.g. adversarial), making our method
suitable for various tasks. More importantly, we demonstrate that data
uncertainty is not tied to concrete value of task loss but closely related to
the upper-bound of task loss. We conduct extensive experiments to validate the
proposed method on classical computer vision tasks and cryo-ET challenges. The
results demonstrate its efficacy and efficiency.

</details>


### [80] [LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels](https://arxiv.org/abs/2509.15868)
*Johannes Leonhardt,Juergen Gall,Ribana Roscher*

Main category: cs.CV

TL;DR: 本文提出了一种新的面向对象的深度学习框架LC-SLab，有效提升了基于稀疏监督的中分辨率地表覆盖分类的精度与地图连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的大规模地表覆盖制图方法在数据稀疏（如仅靠LUCAS地面调查标签）情况下往往预测碎片化且噪声多。像素级分类容易造成地图零散，限制了实际应用。

Method: 提出LC-SLab深度学习框架，系统探索面向对象的地表覆盖分类（而非像素级），支持输入级聚合（基于图神经网络）和输出级聚合（分割结果后处理），并引入大型预训练网络特征以提升小数据集表现。

Result: 在Sentinel-2年度影像和LUCAS标签上测试，LC-SLab的面向对象方法在连贯性上远超像素方法，精度可与或优于常见方法。输入级聚合对小数据集更鲁棒，输出级聚合在数据多时更优。部分配置超越当前主流制图产品。

Conclusion: 面向对象的深度学习方法在处理稀疏标签的中分辨率地表覆盖分类时能显著提升预测连贯性与准确性，框架具有较高实际应用价值。

Abstract: Large-scale land cover maps generated using deep learning play a critical
role across a wide range of Earth science applications. Open in-situ datasets
from principled land cover surveys offer a scalable alternative to manual
annotation for training such models. However, their sparse spatial coverage
often leads to fragmented and noisy predictions when used with existing deep
learning-based land cover mapping approaches. A promising direction to address
this issue is object-based classification, which assigns labels to semantically
coherent image regions rather than individual pixels, thereby imposing a
minimum mapping unit. Despite this potential, object-based methods remain
underexplored in deep learning-based land cover mapping pipelines, especially
in the context of medium-resolution imagery and sparse supervision. To address
this gap, we propose LC-SLab, the first deep learning framework for
systematically exploring object-based deep learning methods for large-scale
land cover classification under sparse supervision. LC-SLab supports both
input-level aggregation via graph neural networks, and output-level aggregation
by postprocessing results from established semantic segmentation models.
Additionally, we incorporate features from a large pre-trained network to
improve performance on small datasets. We evaluate the framework on annual
Sentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff
between accuracy and fragmentation, as well as sensitivity to dataset size. Our
results show that object-based methods can match or exceed the accuracy of
common pixel-wise models while producing substantially more coherent maps.
Input-level aggregation proves more robust on smaller datasets, whereas
output-level aggregation performs best with more data. Several configurations
of LC-SLab also outperform existing land cover products, highlighting the
framework's practical utility.

</details>


### [81] [Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval](https://arxiv.org/abs/2509.15871)
*Liwei Liao,Xufeng Li,Xiaoyun Zheng,Boning Liu,Feng Gao,Ronggang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GVR的新方法，将3D视觉定位问题转化为2D检索任务，实现了零样本3D视觉定位，无需逐场景训练和3D标注，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法难以处理3D Gaussian Splatting的隐式空间纹理表示，并且大多依赖大量标注数据和逐场景训练，带来较高成本和复杂性。

Method: 作者提出Grounding via View Retrieval (GVR) 框架，将3D视觉定位转化为2D视角的检索，通过对多个视角上的对象级别检索收集定位线索，减少对3D标注和逐场景训练的依赖。

Result: 大量实验表明，该方法在无需逐场景训练的情况下，视觉定位表现达到当前最优水平，有效进行零样本3D视觉定位。

Conclusion: GVR为零样本3D视觉定位任务奠定了坚实基础，不仅性能优越，还极大降低了3D场景理解的训练和数据标注成本。

Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text
prompts, which is essential for applications such as robotics. However,
existing 3DVG methods encounter two main challenges: first, they struggle to
handle the implicit representation of spatial textures in 3D Gaussian Splatting
(3DGS), making per-scene training indispensable; second, they typically require
larges amounts of labeled data for effective training. To this end, we propose
\underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel
zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D
retrieval task that leverages object-level view retrieval to collect grounding
clues from multiple views, which not only avoids the costly process of 3D
annotation, but also eliminates the need for per-scene training. Extensive
experiments demonstrate that our method achieves state-of-the-art visual
grounding performance while avoiding per-scene training, providing a solid
foundation for zero-shot 3DVG research. Video demos can be found in
https://github.com/leviome/GVR_demos.

</details>


### [82] [ENSAM: an efficient foundation model for interactive segmentation of 3D medical images](https://arxiv.org/abs/2509.15874)
*Elias Stenhede,Agnar Martin Bjørnstad,Arian Ranjbar*

Main category: cs.CV

TL;DR: ENSAM是一款轻量化、可提示的3D医学影像分割模型，在低数据和算力下表现优异，超越了多个已知基线模型，并在挑战中取得较好成绩。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学影像分割领域面临数据和计算资源有限的问题，且希望模型能适应多种模态、快速精度提升。因此，亟需设计一种高效、易推广且训练要求低的分割模型。

Method: ENSAM模型结构基于SegResNet编码器，并结合提示编码器和掩码解码器，整体采用U-Net风格。核心技术包括潜在空间交叉注意力、相对位置编码、归一化注意力机制及Muon优化器。模型从零开始训练，用不到5000个多模态医学影像数据（CT、MRI、PET等），在单块32 GB显卡上仅需6小时。

Result: 在CVPR 2025 3D医学影像分割挑战赛中，ENSAM在隐藏测试集上获得DSC AUC 2.404、NSD AUC 2.266、最终DSC 0.627、最终NSD 0.597，超越VISTA3D、SAM-Med3D基线模型，并与SegVol模型持平，在最终DSC略有超越。未用预训练权重时，排名同类方法第一。消融实验中，相对位置编码和Muon优化器显著提升了收敛速度和分割精度。

Conclusion: ENSAM适用于在有限数据和算力条件下的通用3D医学影像分割任务，具有高效快速、易用可推广的优势，为医疗影像AI提供了新选择。

Abstract: We present ENSAM (Equivariant, Normalized, Segment Anything Model), a
lightweight and promptable model for universal 3D medical image segmentation.
ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder
in a U-Net-style architecture, using latent cross-attention, relative
positional encoding, normalized attention, and the Muon optimizer for training.
ENSAM is designed to achieve good performance under limited data and
computational budgets, and is trained from scratch on under 5,000 volumes from
multiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB
GPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D
Biomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set
with multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of
2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously
published baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),
surpassing its performance in final DSC but trailing behind in the other three
metrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall
and best among the approaches not utilizing pretrained weights. Ablation
studies confirm that our use of relative positional encodings and the Muon
optimizer each substantially speed up convergence and improve segmentation
quality.

</details>


### [83] [Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration](https://arxiv.org/abs/2509.15882)
*Xingmei Wang,Xiaoyu Hu,Chengkai Huang,Ziyan Zeng,Guohao Nie,Quan Z. Sheng,Lina Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督I2P(影像配准点云)方法“CrossI2P”，有效提升影像与点云数据的配准精度与鲁棒性，在主流数据集上大幅超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 2D图像和3D点云的配准（I2P）对于自动驾驶等自主系统的感知至关重要，但受限于两者语义和几何差异大以及容易陷入局部最优的问题，现有方法效果有限。

Method: 设计了CrossI2P自监督框架，核心包括：(1) 通过双路径对比学习，融合语义与几何特征，得到2D-3D共享嵌入空间，实现无标注的双向对齐；(2) 采用粗到细的两阶段配准，先全局匹配超级点-超级像素，再细化到点级精配准；(3) 动态训练策略和梯度归一化，平衡特征配准、对应增强与位姿估计间的损失。

Result: 在KITTI Odometry基准上提升23.7%，nuScenes基准提升37.9%，大幅优于当前领先方法，显著提高了配准的精确度与鲁棒性。

Conclusion: CrossI2P成功缩小了图像与点云之间的语义-几何鸿沟，提出的方法自监督、端到端、精度高，适用于更鲁棒的自动驾驶感知系统。

Abstract: Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.

</details>


### [84] [RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning](https://arxiv.org/abs/2509.15883)
*Xiaosheng Long,Hanyu Wang,Zhentao Song,Kun Luo,Hongde Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的关系感知检索增强图像描述模型RACap，能更精细地建模图像中对象及其语义关系，并取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强图像描述方法虽然引入了外部知识，但在关系建模上存在两个问题：一是语义提示表示太粗糙，难以捕捉细粒度关系；二是缺乏对图像对象及其语义关系的显式建模。为解决这些局限，作者设计了新方法。

Method: 提出RACap模型，不仅能从检索到的描述文本中挖掘结构化的关系语义，还能从图像中识别异构对象，并将包含异构视觉信息的结构化关系特征，用于提升语义一致性和关系表达能力。

Result: RACap模型参数量仅为10.8M，在轻量级图像描述任务中，表现优于以往其他同类模型。

Conclusion: RACap有效提升了复杂场景下图像描述的关系建模能力，在保持模型轻量化的前提下，实现了更强大的性能。

Abstract: Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.

</details>


### [85] [RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation](https://arxiv.org/abs/2509.15886)
*Paul Julius Kühn,Duc Anh Nguyen,Arjan Kuijper,Holger Graf,Dieter Fellner,Saptarshi Neil Sinha*

Main category: cs.CV

TL;DR: 该论文提出了一种基于SAM2视觉基础模型（VFM）的LiDAR点云范围图像分割新框架，并在SemanticKITTI上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的点云分割方法基于体素化或点基表示，虽能细致捕捉几何信息，但算力消耗大、实时性较差。而相较之下，基于范围图（range-view）的方法可利用成熟的2D分割技术提高效率。受到视觉大模型在分割、多模态等任务表现优异的启发，本文探索能否将SAM2基础模型作为点云分割主干结构。

Method: 作者首次提出将SAM2适配为点云范围图像分割主干，结合2D高效特征提取和投影/反投影实现点云操作，并在编码模块引入三点改进：1）增强LiDAR水平空间依赖建模，2）针对球面投影几何结构设计定制配置，3）调整主干结构捕捉范围视图特有空间模式与不连续性。

Result: 本文方法在SemanticKITTI数据集上测试表现出色，兼具2D方法的速度、可扩展性与部署简洁性，取得了与主流方法相当的分割效果。

Conclusion: 视觉大模型作为通用骨干用于三维分割可行且效果优异，基于范围图的方法结合VFM为统一、高效率的激光雷达分割解决方案开辟了新路径。

Abstract: Point cloud segmentation is central to autonomous driving and 3D scene
understanding. While voxel- and point-based methods dominate recent research
due to their compatibility with deep architectures and ability to capture
fine-grained geometry, they often incur high computational cost, irregular
memory access, and limited real-time efficiency. In contrast, range-view
methods, though relatively underexplored - can leverage mature 2D semantic
segmentation techniques for fast and accurate predictions. Motivated by the
rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot
recognition, and multimodal tasks, we investigate whether SAM2, the current
state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for
LiDAR point cloud segmentation in the range view. We present , to our
knowledge, the first range-view framework that adapts SAM2 to 3D segmentation,
coupling efficient 2D feature extraction with standard
projection/back-projection to operate on point clouds. To optimize SAM2 for
range-view representations, we implement several architectural modifications to
the encoder: (1) a novel module that emphasizes horizontal spatial dependencies
inherent in LiDAR range images, (2) a customized configuration of tailored to
the geometric properties of spherical projections, and (3) an adapted mechanism
in the encoder backbone specifically designed to capture the unique spatial
patterns and discontinuities present in range-view pseudo-images. Our approach
achieves competitive performance on SemanticKITTI while benefiting from the
speed, scalability, and deployment simplicity of 2D-centric pipelines. This
work highlights the viability of VFMs as general-purpose backbones for 3D
perception and opens a path toward unified, foundation-model-driven LiDAR
segmentation. Results lets us conclude that range-view segmentation methods
using VFMs leads to promising results.

</details>


### [86] [Global Regulation and Excitation via Attention Tuning for Stereo Matching](https://arxiv.org/abs/2509.15891)
*Jiahao Li,Xinhong Chen,Zhengmin Jiang,Qian Zhou,Yung-Hui Li,Jianping Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GREAT（Global Regulation and Excitation via Attention Tuning）的新框架，通过三种注意力机制提升双目立体匹配方法在困难区域（如遮挡、无纹理或重复图案）下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代立体匹配方法（如RAFT-Stereo和IGEV-Stereo）在处理遮挡、纹理缺失、重复图案等病态区域时效果欠佳，主要原因在于缺乏全局上下文与几何信息，无法有效进行迭代细化。

Method: GREAT框架包含三个注意力模块：空间注意力（SA）、匹配注意力（MA）和体积注意力（VA）。其中，SA捕捉空间维度的全局上下文，MA在极线方向提取全局上下文，VA则结合前两者提升代价体的鲁棒性和全局感知能力。该框架可无缝集成到现有多种主流的迭代式立体匹配算法中。

Result: 将GREAT集成到多种代表性立体匹配方法（统称GREAT-Stereo）并在公开数据集上进行大量实验。GREAT-IGEV（将GREAT集成到IGEV-Stereo）在Scene Flow、KITTI 2015、ETH3D等主流榜单排名第一，在Middlebury排名第二。

Conclusion: GREAT框架能显著提升现有迭代双目立体方法在复杂场景下的性能，具有通用性和高精度，为后续立体匹配方法提供了有效的全局信息建模范式。

Abstract: Stereo matching achieves significant progress with iterative algorithms like
RAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed
regions with occlusions, textureless, or repetitive patterns, due to a lack of
global context and geometric information for effective iterative refinement. To
enable the existing iterative approaches to incorporate global context, we
propose the Global Regulation and Excitation via Attention Tuning (GREAT)
framework which encompasses three attention modules. Specifically, Spatial
Attention (SA) captures the global context within the spatial dimension,
Matching Attention (MA) extracts global context along epipolar lines, and
Volume Attention (VA) works in conjunction with SA and MA to construct a more
robust cost-volume excited by global context and geometric details. To verify
the universality and effectiveness of this framework, we integrate it into
several representative iterative stereo-matching methods and validate it
through extensive experiments, collectively denoted as GREAT-Stereo. This
framework demonstrates superior performance in challenging ill-posed regions.
Applied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first
on the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves
second on the Middlebury benchmark. Code is available at
https://github.com/JarvisLee0423/GREAT-Stereo.

</details>


### [87] [Deep Feedback Models](https://arxiv.org/abs/2509.15905)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 本文提出了深度反馈模型（DFM），通过引入高层反馈机制提升神经网络在噪声和小样本场景下的鲁棒性和泛化能力，实验结果优于传统前馈网络。


<details>
  <summary>Details</summary>
Motivation: 传统的前馈神经网络通常仅利用自下而上的信息流，缺乏模拟生物决策过程中的动态反馈，因而在面临噪声或数据有限的情况下表现受限。本研究旨在引入反馈机制，提高网络的稳定性与适应性。

Method: DFM通过高层表征的反馈机制，将输入与历史状态结合，模型在结构上可类比为求解微分方程的递归神经网络，并通过指数衰减稳定状态收敛。在噪声鲁棒性和数据有限的条件下，对目标识别和分割任务进行了系统评估。

Result: 实验显示，无论是在图像识别、分割任务，还是医学影像场景下，DFM都优于对应的前馈网络。特别是在小样本或高噪声环境下，DFM表现出更强的鲁棒性和泛化能力。

Conclusion: 反馈机制对神经网络的稳定性、鲁棒性和泛化性有重要提升，DFM可作为在复杂或不稳定数据环境下优选的网络结构。

Abstract: Deep Feedback Models (DFMs) are a new class of stateful neural networks that
combine bottom up input with high level representations over time. This
feedback mechanism introduces dynamics into otherwise static architectures,
enabling DFMs to iteratively refine their internal state and mimic aspects of
biological decision making. We model this process as a differential equation
solved through a recurrent neural network, stabilized via exponential decay to
ensure convergence. To evaluate their effectiveness, we measure DFMs under two
key conditions: robustness to noise and generalization with limited data. In
both object recognition and segmentation tasks, DFMs consistently outperform
their feedforward counterparts, particularly in low data or high noise regimes.
In addition, DFMs translate to medical imaging settings, while being robust
against various types of noise corruption. These findings highlight the
importance of feedback in achieving stable, robust, and generalizable learning.
Code is available at https://github.com/DCalhas/deep_feedback_models.

</details>


### [88] [Sparse Multiview Open-Vocabulary 3D Detection](https://arxiv.org/abs/2509.15924)
*Olivier Moliner,Viktor Larsson,Kalle Åström*

Main category: cs.CV

TL;DR: 提出了一种无需训练、基于预训练2D基础模型的open-vocabulary 3D目标检测方法，可在只用有限RGB图像的“稀疏视角”场景下，有效识别任意类别的3D目标。


<details>
  <summary>Details</summary>
Motivation: 现有3D目标检测通常仅支持固定类别，且需要大量3D训练数据和专门训练，难以应用于灵活、开放类别需求以及只有少量输入视角的实际场景。

Method: 方法无需训练，直接利用现成的2D视觉模型，将2D检测结果通过视图之间特征一致性优化提升至3D目标候选，绕开繁重的3D特征融合或3D专用学习。完全利用2D海量训练数据优势。

Result: 在标准基准测试上，该方法在密集采样场景下表现可与SOTA媲美，而在稀疏视角场景下明显优于现有方法。

Conclusion: 在稀疏视角、开放类别3D目标检测任务，该简单、无训练流程充分挖掘2D模型潜力，提供了非常有竞争力的基线，并展示出极高实用价值。

Abstract: The ability to interpret and comprehend a 3D scene is essential for many
vision and robotics systems. In numerous applications, this involves 3D object
detection, i.e.~identifying the location and dimensions of objects belonging to
a specific category, typically represented as bounding boxes. This has
traditionally been solved by training to detect a fixed set of categories,
which limits its use. In this work, we investigate open-vocabulary 3D object
detection in the challenging yet practical sparse-view setting, where only a
limited number of posed RGB images are available as input. Our approach is
training-free, relying on pre-trained, off-the-shelf 2D foundation models
instead of employing computationally expensive 3D feature fusion or requiring
3D-specific learning. By lifting 2D detections and directly optimizing 3D
proposals for featuremetric consistency across views, we fully leverage the
extensive training data available in 2D compared to 3D. Through standard
benchmarks, we demonstrate that this simple pipeline establishes a powerful
baseline, performing competitively with state-of-the-art techniques in densely
sampled scenarios while significantly outperforming them in the sparse-view
setting.

</details>


### [89] [PAN: Pillars-Attention-Based Network for 3D Object Detection](https://arxiv.org/abs/2509.15935)
*Ruan Bispo,Dane Mitrev,Letizia Mariotti,Clément Botty,Denver Humphrey,Anthony Scanlan,Ciarán Eising*

Main category: cs.CV

TL;DR: 本文提出了一种基于摄像头和雷达融合的高效3D目标检测算法，通过引入新型骨干网络和注意力机制，并优化网络结构，在准确性和推理速度上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D目标检测大多关注摄像头-激光雷达（lidar）融合，较少关注成本较低且在恶劣环境下表现更好的摄像头-雷达融合，尤其缺乏针对雷达点云特性设计的新架构。

Method: 提出了一种新颖的BEV视角3D目标检测算法，设计了新的骨干网络将雷达pillar特征映射到嵌入空间，并通过自注意力机制建模雷达点之间的关系。同时，用简化的卷积层代替传统PointPillars架构中的FPN卷积层，以减少推理时间。

Result: 在ResNet-50下，方法在nuScenes数据集3D目标检测NDS指标上达到58.2的新SOTA水平，并在推理时间上创下新纪录。

Conclusion: 所提算法能充分利用雷达优势，实现准确且高效的3D目标检测，兼具实际应用价值和前沿性。

Abstract: Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar
fusion for the 3D object detection task in real-time under adverse weather and
lighting conditions. However, currently, in the literature, it is possible to
find few works focusing on this modality and, most importantly, developing new
architectures to explore the advantages of the radar point cloud, such as
accurate distance estimation and speed information. Therefore, this work
presents a novel and efficient 3D object detection algorithm using cameras and
radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of
radar before fusing the features into a detection head. A new backbone is
introduced, which maps the radar pillar features into an embedded dimension. A
self-attention mechanism allows the backbone to model the dependencies between
the radar points. We are using a simplified convolutional layer to replace the
FPN-based convolutional layers used in the PointPillars-based architectures
with the main goal of reducing inference time. Our results show that with this
modification, our approach achieves the new state-of-the-art in the 3D object
detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,
while also setting a new benchmark for inference time on the nuScenes dataset
for the same category.

</details>


### [90] [A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction](https://arxiv.org/abs/2509.15966)
*Shalini Dangi,Surya Karthikeya Mullapudi,Chandravardhan Singh Raghaw,Shahid Shafi Dar,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的多时序多光谱产量预测网络（MTMS-YieldNet），结合了光谱数据与时空信息，实现了更精准的农作物产量预测，在多个卫星数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 农业产量预测对可持续发展和粮食安全至关重要，但受到气候变化等多重因素影响。目前已有方法在处理多光谱数据方面存在不足，限制了预测精度。本文旨在解决此问题，提升产量预测的准确性。

Method: 提出了一种名为MTMS-YieldNet的深度学习网络，融合了多时序和多光谱卫星遥感数据，通过对比学习进行特征预训练，重点捕捉空间-光谱及时空相关性。

Result: 在Sentinel-1、Landsat-8、Sentinel-2三种卫星数据上，MTMS-YieldNet分别获得了0.336、0.353、0.331的MAPE分数，均优于七种现有最先进方法。

Conclusion: MTMS-YieldNet显著提升了多气候和多季节环境下的产量预测能力，为农民科学决策和增产提供了有价值的技术支持。

Abstract: Precise yield prediction is essential for agricultural sustainability and
food security. However, climate change complicates accurate yield prediction by
affecting major factors such as weather conditions, soil fertility, and farm
management systems. Advances in technology have played an essential role in
overcoming these challenges by leveraging satellite monitoring and data
analysis for precise yield estimation. Current methods rely on spatio-temporal
data for predicting crop yield, but they often struggle with multi-spectral
data, which is crucial for evaluating crop health and growth patterns. To
resolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield
Prediction Network, MTMS-YieldNet, that integrates spectral data with
spatio-temporal information to effectively capture the correlations and
dependencies between them. While existing methods that rely on pre-trained
models trained on general visual data, MTMS-YieldNet utilizes contrastive
learning for feature discrimination during pre-training, focusing on capturing
spatial-spectral patterns and spatio-temporal dependencies from remote sensing
data. Both quantitative and qualitative assessments highlight the excellence of
the proposed MTMS-YieldNet over seven existing state-of-the-art methods.
MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,
and an outstanding 0.331 on Sentinel-2, demonstrating effective yield
prediction performance across diverse climatic and seasonal conditions. The
outstanding performance of MTMS-YieldNet improves yield predictions and
provides valuable insights that can assist farmers in making better decisions,
potentially improving crop yields.

</details>


### [91] [Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation](https://arxiv.org/abs/2509.15980)
*Lorenzo Cirillo,Claudio Schiavella,Lorenzo Papa,Paolo Russo,Irene Amerini*

Main category: cs.CV

TL;DR: 本文研究了用于单目深度估计（MDE）的可解释性方法，比较了不同特征归因方法，并提出了新的评估指标，以更好地衡量可视化解释的有效性。研究发现，不同方法对轻量和深度模型有不同效果，新的指标能有效识别解释失效情况。


<details>
  <summary>Details</summary>
Motivation: 尽管单目深度估计算法在实际应用中十分广泛，但其决策过程的可解释性研究较少，实际部署时缺乏可信度。加强对MDE模型解释性的研究，有助于提升这些系统的透明度与信任度。

Method: 本文采用三种主流特征归因方法（Saliency Maps、Integrated Gradients、Attention Rollout），并在两种计算复杂度不同的MDE模型（轻量级的METER和深度的PixelFormer）上进行分析评估。通过扰动归因方法选出的最关键和最不关键像素，并观察模型输出变化，从而评估可视化解释的质量。此外，作者提出了Attribution Fidelity指标，用于检测归因解释与实际深度预测的一致性。

Result: 实验显示，Saliency Maps对轻量模型、Integrated Gradients对深度模型能更好地突出关键输入特征。Attribution Fidelity指标能够有效发现传统指标遗漏的解释失效现象，表明其在评估可解释性方面的有效性。

Conclusion: 本文为MDE模型的可解释性分析提供了系统方法，并提出了新的评估指标。实验结果表明，不同可解释性方法各有所长，Attribution Fidelity可以更准确评估解释的可靠性，有助于推动MDE领域解释性研究发展。

Abstract: Explainable artificial intelligence is increasingly employed to understand
the decision-making process of deep learning models and create trustworthiness
in their adoption. However, the explainability of Monocular Depth Estimation
(MDE) remains largely unexplored despite its wide deployment in real-world
applications. In this work, we study how to analyze MDE networks to map the
input image to the predicted depth map. More in detail, we investigate
well-established feature attribution methods, Saliency Maps, Integrated
Gradients, and Attention Rollout on different computationally complex models
for MDE: METER, a lightweight network, and PixelFormer, a deep network. We
assess the quality of the generated visual explanations by selectively
perturbing the most relevant and irrelevant pixels, as identified by the
explainability methods, and analyzing the impact of these perturbations on the
model's output. Moreover, since existing evaluation metrics can have some
limitations in measuring the validity of visual explanations for MDE, we
additionally introduce the Attribution Fidelity. This metric evaluates the
reliability of the feature attribution by assessing their consistency with the
predicted depth map. Experimental results demonstrate that Saliency Maps and
Integrated Gradients have good performance in highlighting the most important
input features for MDE lightweight and deep models, respectively. Furthermore,
we show that Attribution Fidelity effectively identifies whether an
explainability method fails to produce reliable visual maps, even in scenarios
where conventional metrics might suggest satisfactory results.

</details>


### [92] [DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis](https://arxiv.org/abs/2509.15990)
*Jérémie Stym-Popper,Nathan Painchaud,Clément Rambour,Pierre-Yves Courand,Nicolas Thome,Olivier Bernard*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态数据融合方法，通过非对称融合策略提升医疗诊断效果，模型表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态数据（如医学影像和表格数据）的融合能提升疾病诊断准确率，但当前融合方法效果有限。作者希望通过改进融合方式进一步提升诊断性能。

Method: 采用以主要模态为核心、通过区分共享与特异信息的非对称融合策略，将次级模态的信息有效整合进来。模型在包含超声心动图时序数据和表格数据的患者数据集上进行了验证。

Result: 实验结果显示，该模型在239名患者数据集上，AUC超过90%，性能优于当前主流融合方法。

Conclusion: 所提出的非对称多模态融合策略能大幅提升临床诊断模型的效果，具有实际应用前景。

Abstract: Multimodal data fusion is a key approach for enhancing diagnosis in medical
applications. We propose an asymmetric fusion strategy starting from a primary
modality and integrating secondary modalities by disentangling shared and
modality-specific information. Validated on a dataset of 239 patients with
echocardiographic time series and tabular records, our model outperforms
existing methods, achieving an AUC over 90%. This improvement marks a crucial
benchmark for clinical use.

</details>


### [93] [Towards Robust Visual Continual Learning with Multi-Prototype Supervision](https://arxiv.org/abs/2509.16011)
*Xiwei Liu,Yulong Li,Yichen Li,Xinlin Zhuang,Haolin Yang,Huifa Li,Imran Razzak*

Main category: cs.CV

TL;DR: MuproCL提出用多语义原型代替单一语言目标，有效提升了视觉持续学习的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的持续学习方法依赖于PLM生成的单一语义目标，容易导致语义歧义和类内视觉多样性无法表达的问题。

Method: MuproCL框架通过轻量LLM代理生成多语境的类别原型，用LogSumExp聚合机制让视觉模型自适应对齐最相关的原型，从而覆盖多样化的语义和视觉表达。

Result: 在不同的持续学习基线实验中，MuproCL方案表现出更高的性能和鲁棒性。

Conclusion: MuproCL为语言引导的视觉持续学习提供了更有效的框架，缓解了单一目标的局限性。

Abstract: Language-guided supervision, which utilizes a frozen semantic target from a
Pretrained Language Model (PLM), has emerged as a promising paradigm for visual
Continual Learning (CL). However, relying on a single target introduces two
critical limitations: 1) semantic ambiguity, where a polysemous category name
results in conflicting visual representations, and 2) intra-class visual
diversity, where a single prototype fails to capture the rich variety of visual
appearances within a class. To this end, we propose MuproCL, a novel framework
that replaces the single target with multiple, context-aware prototypes.
Specifically, we employ a lightweight LLM agent to perform category
disambiguation and visual-modal expansion to generate a robust set of semantic
prototypes. A LogSumExp aggregation mechanism allows the vision model to
adaptively align with the most relevant prototype for a given image. Extensive
experiments across various CL baselines demonstrate that MuproCL consistently
enhances performance and robustness, establishing a more effective path for
language-guided continual learning.

</details>


### [94] [DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching](https://arxiv.org/abs/2509.16017)
*Meng Yang,Fan Fan,Zizhuo Li,Songchu Deng,Yong Ma,Jiayi Ma*

Main category: cs.CV

TL;DR: 提出了一种新方法DistillMatch，通过知识蒸馏和生成对抗网络提升多模态图像匹配的精度和泛化能力，并在公开数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图像匹配方法因不同模态间外观差异大、数据集稀缺，导致性能差且适应性弱。近期的大规模视觉基础模型（VFM）具备强泛化能力，本工作旨在利用VFM的知识，提高多模态图像匹配的准确率和泛化能力。

Method: 提出DistillMatch方法，通过知识蒸馏将VFM（如DINOv2、DINOv3）的高层语义知识传递给精简的学生模型，同时在特征中注入模态类别信息提高跨模态相关性理解。此外设计V2I-GAN，将可见光转换为伪红外图像实现数据增强，提升模型的泛化能力。

Result: 在公开多模态图像匹配数据集上，DistillMatch方法在准确率等指标上优于当前主流方法。

Conclusion: DistillMatch通过融合VFM知识和数据增强，有效提升了多模态图像匹配的准确性与泛化能力，为相关领域提供了更优的解决方案。

Abstract: Multimodal image matching seeks pixel-level correspondences between images of
different modalities, crucial for cross-modal perception, fusion and analysis.
However, the significant appearance differences between modalities make this
task challenging. Due to the scarcity of high-quality annotated datasets,
existing deep learning methods that extract modality-common features for
matching perform poorly and lack adaptability to diverse scenarios. Vision
Foundation Model (VFM), trained on large-scale data, yields generalizable and
robust feature representations adapted to data and tasks of various modalities,
including multimodal matching. Thus, we propose DistillMatch, a multimodal
image matching method using knowledge distillation from VFM. DistillMatch
employs knowledge distillation to build a lightweight student model that
extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to
assist matching across modalities. To retain modality-specific information, it
extracts and injects modality category information into the other modality's
features, which enhances the model's understanding of cross-modal correlations.
Furthermore, we design V2I-GAN to boost the model's generalization by
translating visible to pseudo-infrared images for data augmentation.
Experiments show that DistillMatch outperforms existing algorithms on public
datasets.

</details>


### [95] [Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence](https://arxiv.org/abs/2509.16022)
*Xihong Yang,Siwei Wang,Jiaqi Jin,Fangdi Wang,Tianrui Liu,Yueming Jin,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 本文提出了一种新的因果多视图聚类方法CauMVC，通过因果建模和变分自编码器（VAE）提升在部分对齐数据下的聚类表现，并通过实验证明其强泛化性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法依赖于不同视图间样本完全对齐，但现实中经常只有部分对齐数据，造成聚类性能下降。为了应对由数据对齐顺序变化引发的性能下降，作者将该问题推广为广义多视图聚类问题，并寻求新的解决思路。

Method: 作者提出CauMVC，将部分对齐作为因果干预建模，将多视图聚类任务建为干预后推断问题。主要框架包括：用变分自编码器（VAE）结合已有信息编码器来估计不变特征，解码器用于干预后聚类推断，并通过对比正则化器捕捉样本相关性。

Result: 在完全对齐和部分对齐数据集上的实验证明，CauMVC方法具有优良的泛化性和聚类效果，在各种对齐程度下均优于其它方法。

Conclusion: 本文首次将因果学习引入广义多视图聚类，在部分对齐和现实复杂场景下表现突出，为相关任务提供了新思路和有效方法。

Abstract: Multi-view clustering (MVC) aims to explore the common clustering structure
across multiple views. Many existing MVC methods heavily rely on the assumption
of view consistency, where alignments for corresponding samples across
different views are ordered in advance. However, real-world scenarios often
present a challenge as only partial data is consistently aligned across
different views, restricting the overall clustering performance. In this work,
we consider the model performance decreasing phenomenon caused by data order
shift (i.e., from fully to partially aligned) as a generalized multi-view
clustering problem. To tackle this problem, we design a causal multi-view
clustering network, termed CauMVC. We adopt a causal modeling approach to
understand multi-view clustering procedure. To be specific, we formulate the
partially aligned data as an intervention and multi-view clustering with
partially aligned data as an post-intervention inference. However, obtaining
invariant features directly can be challenging. Thus, we design a Variational
Auto-Encoder for causal learning by incorporating an encoder from existing
information to estimate the invariant features. Moreover, a decoder is designed
to perform the post-intervention inference. Lastly, we design a contrastive
regularizer to capture sample correlations. To the best of our knowledge, this
paper is the first work to deal generalized multi-view clustering via causal
learning. Empirical experiments on both fully and partially aligned data
illustrate the strong generalization and effectiveness of CauMVC.

</details>


### [96] [GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition](https://arxiv.org/abs/2509.16031)
*Tianyue Wang,Shuang Yang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 该论文提出了GLip框架，通过结合全局与局部特征的双路径结构，进而提升对照明变化、遮挡、模糊、姿态变化等复杂视觉环境下的唇语识别鲁棒性，并在多个主流数据集上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 虽然唇语识别技术取得了长足进步，但现有方法在实际复杂视觉环境下（如光照变化、部分遮挡、镜头模糊、姿态变化）表现尚不理想，需要更鲁棒的识别方法。

Method: 提出GLip框架，核心由两个创新点驱动：1）利用音视频配准数据先学习粗粒度对齐，作为后续精准视觉-语音映射的基础；2）强调受损或复杂条件下局部区域（如未被遮挡的区域）更具区分力。具体方法为：设计一个双路径特征提取结构，先在第一阶段完成音视频特征的粗对齐，随后引入Contextual Enhancement Module，在时空维度动态整合全局上下文和局部特征，获得更准确的映射。

Result: GLip在LRS2、LRS3等公开基准数据集以及一套新引入的高难度中文数据集上，均表现出在复杂视觉场景下优于现有方法的鲁棒性与识别准确性。

Conclusion: GLip框架利用分阶段、全局-局部结合的学习策略，有效提升了在复杂条件下的唇语识别性能，为实际视觉语音识别系统提供了更可靠的技术手段。

Abstract: Visual speech recognition (VSR), also known as lip reading, is the task of
recognizing speech from silent video. Despite significant advancements in VSR
over recent decades, most existing methods pay limited attention to real-world
visual challenges such as illumination variations, occlusions, blurring, and
pose changes. To address these challenges, we propose GLip, a Global-Local
Integrated Progressive framework designed for robust VSR. GLip is built upon
two key insights: (i) learning an initial \textit{coarse} alignment between
visual features across varying conditions and corresponding speech content
facilitates the subsequent learning of \textit{precise} visual-to-speech
mappings in challenging environments; (ii) under adverse conditions, certain
local regions (e.g., non-occluded areas) often exhibit more discriminative cues
for lip reading than global features. To this end, GLip introduces a dual-path
feature extraction architecture that integrates both global and local features
within a two-stage progressive learning framework. In the first stage, the
model learns to align both global and local visual features with corresponding
acoustic speech units using easily accessible audio-visual data, establishing a
coarse yet semantically robust foundation. In the second stage, we introduce a
Contextual Enhancement Module (CEM) to dynamically integrate local features
with relevant global context across both spatial and temporal dimensions,
refining the coarse representations into precise visual-speech mappings. Our
framework uniquely exploits discriminative local regions through a progressive
learning strategy, demonstrating enhanced robustness against various visual
challenges and consistently outperforming existing methods on the LRS2 and LRS3
benchmarks. We further validate its effectiveness on a newly introduced
challenging Mandarin dataset.

</details>


### [97] [Graph-based Point Cloud Surface Reconstruction using B-Splines](https://arxiv.org/abs/2509.16050)
*Stuti Pathak,Rhys G. Evans,Gunther Steenackers,Rudi Penne*

Main category: cs.CV

TL;DR: 该论文提出了一种无需法向信息、可同时预测控制点数量和位置的B样条曲面重建方法，并显著提升了噪声点云的三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有点云曲面重建方法高度依赖法向信息，且控制点数量常常固定，导致在处理噪声数据和复杂表面时，效果不佳且灵活性不足。作者旨在开发一种对噪声鲁棒、灵活且无需法向的新型曲面重建方法。

Method: 采用字典指导的图卷积网络（Dictionary-Guided Graph Convolutional Network），输入噪声点云，网络同时预测B样条控制点的数量和具体位置，实现平滑曲面重建，且全流程无需点云法向。

Result: 通过与多个经典及最新方法对比，使用主流评测指标，提出的方法在定性和定量上均优于已有的重建技术。

Conclusion: 该方法显著提升了噪声点云的重建质量，降低了对点云法向依赖，并增强了曲面建模的灵活性，对3D视觉领域的高质量表面重建有较高实用意义。

Abstract: Generating continuous surfaces from discrete point cloud data is a
fundamental task in several 3D vision applications. Real-world point clouds are
inherently noisy due to various technical and environmental factors. Existing
data-driven surface reconstruction algorithms rely heavily on ground truth
normals or compute approximate normals as an intermediate step. This dependency
makes them extremely unreliable for noisy point cloud datasets, even if the
availability of ground truth training data is ensured, which is not always the
case. B-spline reconstruction techniques provide compact surface
representations of point clouds and are especially known for their smoothening
properties. However, the complexity of the surfaces approximated using
B-splines is directly influenced by the number and location of the spline
control points. Existing spline-based modeling methods predict the locations of
a fixed number of control points for a given point cloud, which makes it very
difficult to match the complexity of its underlying surface. In this work, we
develop a Dictionary-Guided Graph Convolutional Network-based surface
reconstruction strategy where we simultaneously predict both the location and
the number of control points for noisy point cloud data to generate smooth
surfaces without the use of any point normals. We compare our reconstruction
method with several well-known as well as recent baselines by employing
widely-used evaluation metrics, and demonstrate that our method outperforms all
of them both qualitatively and quantitatively.

</details>


### [98] [Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model](https://arxiv.org/abs/2509.16054)
*Jihua Peng,Qianxiong Xu,Yichen Liu,Chenxi Liu,Cheng Long,Rui Zhao,Ziyue Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于多模态大语言模型（MLLM）的群体活动检测（GAD）方法，有效提升了群体成员识别与集体活动分类的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的群体活动检测方法主要依赖视觉特征的隐式识别，缺乏对上下文的推理能力和解释性。因此，作者希望通过引入多模态大语言模型及语言指令推理，提升模型在活动识别中的语义理解和推理能力，并增强可解释性。

Method: 提出LIR-GAD框架，通过扩展MLLM词汇表，引入<ACT>活动级别token及多个<GROUP>群体特定token，将处理后的视频帧、特制token及语言指令输入MLLM，利用其内置的常识知识获取更丰富的活动语义。此外，采用多标签分类损失强化<ACT> token的区分性，并设计多模态双对齐融合（MDAF）模块结合MLLM与视觉特征，增强活动检测表现。

Result: 通过定量和定性实验验证，LIR-GAD在群体活动检测任务上取得了优越性能，显著提升了模型的检测准确率和语义区分能力。

Conclusion: 利用MLLM的知识和推理能力，可以有效提升群体活动检测的效率和可解释性，为未来类似任务提供了新的多模态学习思路。

Abstract: Group activity detection (GAD) aims to simultaneously identify group members
and categorize their collective activities within video sequences. Existing
deep learning-based methods develop specialized architectures (e.g.,
transformer networks) to model the dynamics of individual roles and semantic
dependencies between individuals and groups. However, they rely solely on
implicit pattern recognition from visual features and struggle with contextual
reasoning and explainability. In this work, we propose LIR-GAD, a novel
framework of language-instructed reasoning for GAD via Multimodal Large
Language Model (MLLM). Our approach expand the original vocabulary of MLLM by
introducing an activity-level <ACT> token and multiple cluster-specific <GROUP>
tokens. We process video frames alongside two specially designed tokens and
language instructions, which are then integrated into the MLLM. The pretrained
commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>
tokens to effectively capture the semantic information of collective activities
and learn distinct representational features of different groups, respectively.
Also, we introduce a multi-label classification loss to further enhance the
<ACT> token's ability to learn discriminative semantic representations. Then,
we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates
MLLM's hidden embeddings corresponding to the designed tokens with visual
features, significantly enhancing the performance of GAD. Both quantitative and
qualitative experiments demonstrate the superior performance of our proposed
method in GAD taks.

</details>


### [99] [See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](https://arxiv.org/abs/2509.16087)
*Pengteng Li,Pinhao Song,Wuyang Li,Weiyu Guo,Huizai Yao,Yijie Xu,Dugang Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出SEE&TREK，无需训练、专为提升多模态大模型视觉空间理解能力而设计的提示工程框架，通过丰富画面多样性和运动重建，实验证明在空间推理任务上能显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究借助深度信息或点云增强大模型的空间推理，但单靠视觉信息实现有效空间理解领域尚未充分探索。因此，作者希望研发一种仅凭视觉信息也能显著提升空间理解能力的方法。

Method: SEE&TREK包含两大核心策略：1）最大语义丰富采样，利用现有感知模型提取具备场景结构信息的关键帧以提高画面多样性；2）运动重建，通过模拟视觉轨迹并将相对空间位置编码进关键帧，以在视觉上保留空间关系及时间一致性。该方法无须额外训练或GPU，仅靠单次前向传播即可，且能无缝集成进当前多模态大模型。

Result: 在VSI-BENCH和STI-BENCH两个空间推理基准上，大量实验证明SEE&TREK可使多种主流多模态大模型在空间推理类任务表现提升，最高提升幅度达3.5%。

Conclusion: SEE&TREK为在无训练的情况下提升大模型对空间关系理解提供了新途径，可广泛应用于现有多模态大模型，推动其空间智能能力进一步发展。

Abstract: We introduce SEE&TREK, the first training-free prompting framework tailored
to enhance the spatial understanding of Multimodal Large Language Models
(MLLMS) under vision-only constraints. While prior efforts have incorporated
modalities like depth or point clouds to improve spatial reasoning, purely
visualspatial understanding remains underexplored. SEE&TREK addresses this gap
by focusing on two core principles: increasing visual diversity and motion
reconstruction. For visual diversity, we conduct Maximum Semantic Richness
Sampling, which employs an off-the-shell perception model to extract
semantically rich keyframes that capture scene structure. For motion
reconstruction, we simulate visual trajectories and encode relative spatial
positions into keyframes to preserve both spatial relations and temporal
coherence. Our method is training&GPU-free, requiring only a single forward
pass, and can be seamlessly integrated into existing MLLM'S. Extensive
experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently
boosts various MLLM S performance across diverse spatial reasoning tasks with
the most +3.5% improvement, offering a promising path toward stronger spatial
intelligence.

</details>


### [100] [Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising](https://arxiv.org/abs/2509.16091)
*Shen Cheng,Haipeng Li,Haibin Huang,Xiaohong Liu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出一种新颖的自监督图像去噪方法：Blind-Spot Guided Diffusion，通过双分支扩散框架融合盲点网络与传统扩散分支，提升去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有盲点网络（BSN）在图像去噪中由于空间独立性假设常损失局部细节且产生像素不连续问题，同时将扩散模型自监督应用于去噪任务也存在难度，亟需新的自监督去噪方式兼顾细节保留与噪声建模。

Method: 提出一种双分支扩散框架，包括基于BSN的扩散分支（生成半净图像）与传统扩散分支（建模噪声分布），BSN分支在训练中引导采样，有效捕捉噪声结构并保留细节，实现无需配对数据的自监督训练。

Result: 在SIDD和DND两个真实图像噪声数据集上，所提方法取得了当前最优性能，优于已有自监督去噪技术。

Conclusion: 该方法是一种高效的自监督真实图像去噪解决方案，解决了BSN与扩散模型在这类任务中的主要不足。

Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised
framework for real-world image denoising. Our approach addresses two major
challenges: the limitations of blind-spot networks (BSNs), which often
sacrifice local detail and introduce pixel discontinuities due to spatial
independence assumptions, and the difficulty of adapting diffusion models to
self-supervised denoising. We propose a dual-branch diffusion framework that
combines a BSN-based diffusion branch, generating semi-clean images, with a
conventional diffusion branch that captures underlying noise distributions. To
enable effective training without paired data, we use the BSN-based branch to
guide the sampling process, capturing noise structure while preserving local
details. Extensive experiments on the SIDD and DND datasets demonstrate
state-of-the-art performance, establishing our method as a highly effective
self-supervised solution for real-world denoising. Code and pre-trained models
are released at: https://github.com/Sumching/BSGD.

</details>


### [101] [AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports](https://arxiv.org/abs/2509.16095)
*Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出AdaSports-Traj，一种能够适应多角色、多体育项目分布差异的运动轨迹预测方法，通过引入角色和领域自适应模块以及分层对比学习，实现跨场景、跨角色的高效预测。


<details>
  <summary>Details</summary>
Motivation: 多智能体运动轨迹预测面临角色结构异质性和体育项目分布差异的问题，现有统一方法难以适应角色和领域变化，导致泛化性较差。

Method: 1）提出AdaSports-Traj，包含角色和领域感知适配器，动态调整潜在特征以适应不同角色和领域；2）引入分层对比学习损失，将角色敏感和领域感知表示分别监督，促进潜在结构解耦。

Result: 在Basketball-U、Football-U和Soccer-U三个数据集上，AdaSports-Traj在统一和跨领域测试中均取得优异的轨迹预测效果。

Conclusion: 通过结合角色和领域的适应机制以及分层对比学习，AdaSports-Traj提升了多体育场景下运动员和目标物体轨迹预测的泛化能力，优于现有统一框架。

Abstract: Trajectory prediction in multi-agent sports scenarios is inherently
challenging due to the structural heterogeneity across agent roles (e.g.,
players vs. ball) and dynamic distribution gaps across different sports
domains. Existing unified frameworks often fail to capture these structured
distributional shifts, resulting in suboptimal generalization across roles and
domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework
that explicitly addresses both intra-domain and inter-domain distribution
discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and
Domain-Aware Adapter to conditionally adjust latent representations based on
agent identity and domain context. Additionally, we introduce a Hierarchical
Contrastive Learning objective, which separately supervises role-sensitive and
domain-aware representations to encourage disentangled latent structures
without introducing optimization conflict. Experiments on three diverse sports
datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness
of our adaptive design, achieving strong performance in both unified and
cross-domain trajectory prediction settings.

</details>


### [102] [SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features](https://arxiv.org/abs/2509.16098)
*Jinyuan Qu,Hongyang Li,Xingyu Chen,Shilong Liu,Yukai Shi,Tianhe Ren,Ruitao Jing,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D实例分割框架SegDINO3D，通过融合2D和3D特征实现更优的3D分割效果，并在主流数据集上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 3D数据集相比2D数据集更为稀缺，训练难度大，因此需要有效利用现有的2D模型和2D特征来提升3D场景感知能力。

Method: SegDINO3D属于Transformer结构的编码-解码框架：编码阶段利用预训练2D检测模型提取图像和目标级别特征，将2D特征映射到3D点云上，融合空间上下文；解码阶段将3D目标查询作为锚框，基于预测3D框与2D目标进行跨模态自注意力（cross-attention），高效提取2D知识。

Result: 在ScanNetV2和ScanNet200等3D实例分割数据集上取得了最优SOTA表现，特别是在ScanNet200数据集上，验证集和隐藏测试集的mAP分别提升+8.7和+6.8，高于现有所有方法。

Conclusion: 本文提出的SegDINO3D能有效融合2D和3D信息，充分释放2D检测模型的潜力，有效提升3D实例分割性能，展示了跨模态学习在3D视觉中的应用前景。

Abstract: In this paper, we present SegDINO3D, a novel Transformer encoder-decoder
framework for 3D instance segmentation. As 3D training data is generally not as
sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D
representation from a pre-trained 2D detection model, including both
image-level and object-level features, for improving 3D representation.
SegDINO3D takes both a point cloud and its associated 2D images as input. In
the encoder stage, it first enriches each 3D point by retrieving 2D image
features from its corresponding image views and then leverages a 3D encoder for
3D context fusion. In the decoder stage, it formulates 3D object queries as 3D
anchor boxes and performs cross-attention from 3D queries to 2D object queries
obtained from 2D images using the 2D detection model. These 2D object queries
serve as a compact object-level representation of 2D images, effectively
avoiding the challenge of keeping thousands of image feature maps in the memory
while faithfully preserving the knowledge of the pre-trained 2D model. The
introducing of 3D box queries also enables the model to modulate
cross-attention using the predicted boxes for more precise querying. SegDINO3D
achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D
instance segmentation benchmarks. Notably, on the challenging ScanNet200
dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP
on the validation and hidden test sets, respectively, demonstrating its
superiority.

</details>


### [103] [RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars](https://arxiv.org/abs/2509.16119)
*Weiyi Xiong,Bing Zhu,Tao Huang,Zewei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的高效4D雷达3D检测器RadarGaussianDet3D，通过高斯表示提升特征密度和检测速度，并显著优化了嵌入式环境下的检测精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达3D检测方法主要依赖pillar编码，导致特征稀疏且表达能力有限，同时边界框属性独立优化影响检测准确率，且在嵌入式设备上的推理速度不足以满足实时需求。

Method: 作者设计了Point Gaussian Encoder (PGE)将每个点聚合后转换为高斯基元，并利用3D Gaussian Splatting（3DGS）进行BEV栅格化，实现更稠密的特征图。还提出了Box Gaussian Loss（BGL），将边界框参数转化为3D高斯分布并用距离计算优化目标。整个流程算法针对点特征聚合和渲染进行了优化。

Result: 在TJ4DRadSet和View-of-Delft数据集上，RadarGaussianDet3D取得了领先的检测准确率和显著更快的推理速度。

Conclusion: RadarGaussianDet3D在保证高精度的同时大幅提高推理速度，非常适合自动驾驶嵌入式实时部署，展示了基于高斯中间表示的3D检测新方向。

Abstract: 4D automotive radars have gained increasing attention for autonomous driving
due to their low cost, robustness, and inherent velocity measurement
capability. However, existing 4D radar-based 3D detectors rely heavily on
pillar encoders for BEV feature extraction, where each point contributes to
only a single BEV grid, resulting in sparse feature maps and degraded
representation quality. In addition, they also optimize bounding box attributes
independently, leading to sub-optimal detection accuracy. Moreover, their
inference speed, while sufficient for high-end GPUs, may fail to meet the
real-time requirement on vehicle-mounted embedded devices. To overcome these
limitations, an efficient and effective Gaussian-based 3D detector, namely
RadarGaussianDet3D is introduced, leveraging Gaussian primitives and
distributions as intermediate representations for radar points and bounding
boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed
to transform each point into a Gaussian primitive after feature aggregation and
employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,
yielding denser feature maps. PGE exhibits exceptionally low latency, owing to
the optimized algorithm for point feature aggregation and fast rendering of
3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts
bounding boxes into 3D Gaussian distributions and measures their distance to
enable more comprehensive and consistent optimization. Extensive experiments on
TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves
state-of-the-art detection accuracy while delivering substantially faster
inference, highlighting its potential for real-time deployment in autonomous
driving.

</details>


### [104] [BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/abs/2509.16127)
*Yi-Fan Zhang,Haihua Yang,Huanyu Zhang,Yang Shi,Zezhou Chen,Haochen Tian,Chaoyou Fu,Haotian Wang,Kai Wu,Bo Cui,Xu Wang,Jianfei Pan,Haotian Wang,Zhang Zhang,Liang Wang*

Main category: cs.CV

TL;DR: 本文系统性分析了多模态大模型奖励模型（MRM）的关键构建环节，并提出了新SOTA基线BaseReward，为多模态奖励建模提供了实证指导和全流程配方。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在对齐人类偏好上需求日益增强，而多模态奖励模型建设缺乏系统化方法与经验总结，成为学术与工业界共同难题。

Method: 对MRM开发流程涉及的各核心要素（包含奖励建模范式、reward head结构、训练方式、数据集整理、backbone与模型规模、集成方法）进行系统实验分析，并据此提出以Qwen2.5-VL为骨干、两层优化reward head、融合高质量多模态及文本偏好数据的新基线模型BaseReward。

Result: BaseReward在MM-RLHF-Reward Bench、VL-Reward Bench与Multimodal Reward Bench等主流基准上均超越以往模型，取得新SOTA。此外实际集成到强化学习流程中，显著提升了多模态大模型感知、推理与对话能力。

Conclusion: 该研究不仅交付了高性能的多模态奖励模型，更为社区提供了开发下一代MRM的详细实验配方和经验指南，为未来多模态大模型奖励学习奠定基础。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made
aligning them with human preferences a critical challenge. Reward Models (RMs)
are a core technology for achieving this goal, but a systematic guide for
building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking
in both academia and industry. Through exhaustive experimental analysis, this
paper aims to provide a clear ``recipe'' for constructing high-performance
MRMs. We systematically investigate every crucial component in the MRM
development pipeline, including \textit{reward modeling paradigms} (e.g.,
Naive-RM, Critic-based RM, and Generative RM), \textit{reward head
architecture}, \textit{training strategies}, \textit{data curation} (covering
over ten multimodal and text-only preference datasets), \textit{backbone model}
and \textit{model scale}, and \textit{ensemble methods}.
  Based on these experimental insights, we introduce \textbf{BaseReward}, a
powerful and efficient baseline for multimodal reward modeling. BaseReward
adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone,
featuring an optimized two-layer reward head, and is trained on a carefully
curated mixture of high-quality multimodal and text-only preference data. Our
results show that BaseReward establishes a new SOTA on major benchmarks such as
MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench,
outperforming previous models. Furthermore, to validate its practical utility
beyond static benchmarks, we integrate BaseReward into a real-world
reinforcement learning pipeline, successfully enhancing an MLLM's performance
across various perception, reasoning, and conversational tasks. This work not
only delivers a top-tier MRM but, more importantly, provides the community with
a clear, empirically-backed guide for developing robust reward models for the
next generation of MLLMs.

</details>


### [105] [Recovering Parametric Scenes from Very Few Time-of-Flight Pixels](https://arxiv.org/abs/2509.16132)
*Carter Sifferman,Yiquan Li,Yiming Li,Fangzhou Mu,Michael Gleicher,Mohit Gupta,Yin Li*

Main category: cs.CV

TL;DR: 本文提出了一种利用极少量低成本飞行时间传感器深度数据恢复简单3D参数化场景几何的方法，其传感器仅有极低空间分辨率但能获取丰富的时间结构信息，通过15个像素点即可完成如物体6D位姿估计等任务。


<details>
  <summary>Details</summary>
Motivation: 市面上低成本飞行时间传感器的空间分辨率低，难以获取高精度3D场景信息。该研究旨在探索能否利用它们所特有的时间分辨数据，实现对简单参数场景的几何重建，尤其关注数据量极少的情况下的物体姿态（6D位姿）恢复问题。

Method: 提出一个方法：首先用前馈预测快速估算场景参数，再结合可微渲染与分析-合成框架对场景参数进一步优化。全流程结合了硬件原型实验，包括仿真和真实场景采集测试。

Result: 实验表明：即使只有15个像素的稀疏测量，配合无纹理3D模型，其方法也能准确恢复物体的位姿，且在其它参数场景下初步结果也较为理想。此外，作者还实验分析了方案的能力极限。

Conclusion: 该方法证明了低分辨率、稀疏飞行时间深度数据在强先验支撑下，依然能实现高效的3D参数场景还原，为低成本三维感知硬件应用拓展了可能。

Abstract: We aim to recover the geometry of 3D parametric scenes using very few depth
measurements from low-cost, commercially available time-of-flight sensors.
These sensors offer very low spatial resolution (i.e., a single pixel), but
image a wide field-of-view per pixel and capture detailed time-of-flight data
in the form of time-resolved photon counts. This time-of-flight data encodes
rich scene information and thus enables recovery of simple scenes from sparse
measurements. We investigate the feasibility of using a distributed set of few
measurements (e.g., as few as 15 pixels) to recover the geometry of simple
parametric scenes with a strong prior, such as estimating the 6D pose of a
known object. To achieve this, we design a method that utilizes both
feed-forward prediction to infer scene parameters, and differentiable rendering
within an analysis-by-synthesis framework to refine the scene parameter
estimate. We develop hardware prototypes and demonstrate that our method
effectively recovers object pose given an untextured 3D model in both
simulations and controlled real-world captures, and show promising initial
results for other parametric scenes. We additionally conduct experiments to
explore the limits and capabilities of our imaging solution.

</details>


### [106] [AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models](https://arxiv.org/abs/2509.16141)
*Vatsal Malaviya,Agneet Chatterjee,Maitreya Patel,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: 提出了AcT2I基准，专注于评估文本生成图像模型对动作类场景的理解与还原能力，并提出了一种基于大语言模型的提示增强方法，显著提升了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像（T2I）模型在简单场景下表现良好，但面对包含复杂动作与交互的场景时常常遗漏细节，无法捕捉到动作描绘中的隐含属性。针对现有T2I模型在动作类内容表达上的不足，作者提出了新的评测需求和改进思路。

Method: 作者首先构建了动作为中心的AcT2I评测基准，系统性地测试主流T2I模型。随后，提出了一种无需重新训练、利用大语言模型（LLM）进行知识蒸馏的提示增强方法：通过从LLM中提取细粒度的上下文信息（如时间细节）来丰富输入提示词，从而弥补模型对隐式动作属性的表征和理解不足。

Result: 实验证明，当前主流T2I模型在AcT2I上的表现有限。升级后的提示增强方法显著提升了图像生成的准确性，最优模型的提升幅度高达72%。

Conclusion: 现有T2I方法在需要复杂推理与动作理解的场景下存在明显短板。将系统性的语言知识注入提示能够极大提升生成结果的细腻度和上下文相关性，对未来T2I模型的设计与训练具有借鉴意义。

Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in
generating images from textual descriptions. However, challenges still persist
in accurately rendering complex scenes where actions and interactions form the
primary semantic focus. Our key observation in this work is that T2I models
frequently struggle to capture nuanced and often implicit attributes inherent
in action depiction, leading to generating images that lack key contextual
details. To enable systematic evaluation, we introduce AcT2I, a benchmark
designed to evaluate the performance of T2I models in generating images from
action-centric prompts. We experimentally validate that leading T2I models do
not fare well on AcT2I. We further hypothesize that this shortcoming arises
from the incomplete representation of the inherent attributes and contextual
dependencies in the training corpora of existing T2I models. We build upon this
by developing a training-free, knowledge distillation technique utilizing Large
Language Models to address this limitation. Specifically, we enhance prompts by
incorporating dense information across three dimensions, observing that
injecting prompts with temporal details significantly improves image generation
accuracy, with our best model achieving an increase of 72%. Our findings
highlight the limitations of current T2I methods in generating images that
require complex reasoning and demonstrate that integrating linguistic knowledge
in a systematic way can notably advance the generation of nuanced and
contextually accurate images.

</details>


### [107] [Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models](https://arxiv.org/abs/2509.16149)
*Renjie Pi,Kehao Miao,Li Peihang,Runtao Liu,Jiahui Gao,Jipeng Zhang,Xiaofang Zhou*

Main category: cs.CV

TL;DR: 本文发现多模态大模型（MLLM）在处理图片输入时，展现出比文本大模型更明显的“奉承”（sycophantic）行为，并提出了一种新方法SRT来改善模型的行为偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视觉对话任务中表现优异，但其对用户输入的迎合性偏差未被充分关注，特别是在视觉输入下更为严重。理解并改进这一现象有助于提升模型的真实可靠性与智能水平。

Method: 作者首先分析了“视觉奉承性差距”（sycophantic modality gap），并尝试用普通监督微调缓解这种偏差，但发现会导致模型过于“固执”。为此，提出了“反思性调整”（Sycophantic Reflective Tuning，SRT），让模型在做出判断前，先分辨用户指令是误导性的还是纠正性的。

Result: 采用SRT后，模型在面对误导性指令时的奉承行为显著减少，同时在面对正确纠正指令时又不会变得过于固执。

Conclusion: SRT方法有效缓解了多模态大模型中的视觉奉承行为，提高了模型在面对不同类型指令时的综合表现。

Abstract: Multimodal large language models (MLLMs) have demonstrated extraordinary
capabilities in conducting conversations based on image inputs. However, we
observe that MLLMs exhibit a pronounced form of visual sycophantic behavior.
While similar behavior has also been noted in text-based large language models
(LLMs), it becomes significantly more prominent when MLLMs process image
inputs. We refer to this phenomenon as the "sycophantic modality gap." To
better understand this issue, we further analyze the factors that contribute to
the exacerbation of this gap. To mitigate the visual sycophantic behavior, we
first experiment with naive supervised fine-tuning to help the MLLM resist
misleading instructions from the user. However, we find that this approach also
makes the MLLM overly resistant to corrective instructions (i.e., stubborn even
if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective
Tuning (SRT), which enables the MLLM to engage in reflective reasoning,
allowing it to determine whether a user's instruction is misleading or
corrective before drawing a conclusion. After applying SRT, we observe a
significant reduction in sycophantic behavior toward misleading instructions,
without resulting in excessive stubbornness when receiving corrective
instructions.

</details>


### [108] [UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation](https://arxiv.org/abs/2509.16170)
*Xiaoqi Zhao,Youwei Pang,Chenyang Yu,Lihe Zhang,Huchuan Lu,Shijian Lu,Georges El Fakhri,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态图像分割网络UniMRSeg，针对现实情况中模态缺失或损坏对分割性能的影响，通过分层自监督补偿策略，显著提升了多种数据缺失场景下的分割效果，优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态分割在真实部署时常常遇到部分模态缺失/损坏问题，现有方法多需针对不同模态组合训练专用模型，不仅训练和部署成本高，而且实用性有限，因此需要更加通用且鲁棒的解决方案。

Method: 提出UniMRSeg框架，使用分层自监督补偿：1）输入层面通过模态重建和混合遮蔽增强，提升模型对缺失模态的表征能力；2）特征空间利用模态不变对比学习，隐式拉近完整与缺失模态间的距离；3）轻量化反向注意力适配器，弥补编码器语义弱点；4）采用混合一致性约束微调，保证不同模态组合时预测稳定。

Result: 在脑肿瘤MRI分割、RGB-D语义分割、RGB-D/T显著性目标分割等多种缺失模态场景下，UniMRSeg在准确率和稳定性上均明显超越现有方法。

Conclusion: UniMRSeg不需针对每种模态组合设计模型，可灵活胜任不同模态可用性的实际场景，为多模态分割实际部署提供了高效、鲁棒的解决方案。

Abstract: Multi-modal image segmentation faces real-world deployment challenges from
incomplete/corrupted modalities degrading performance. While existing methods
address training-inference modality gaps via specialized per-combination
models, they introduce high deployment costs by requiring exhaustive model
subsets and model-modality matching. In this work, we propose a unified
modality-relax segmentation network (UniMRSeg) through hierarchical
self-supervised compensation (HSSC). Our approach hierarchically bridges
representation gaps between complete and incomplete modalities across input,
feature and output levels. % First, we adopt modality reconstruction with the
hybrid shuffled-masking augmentation, encouraging the model to learn the
intrinsic modality characteristics and generate meaningful representations for
missing modalities through cross-modal fusion. % Next, modality-invariant
contrastive learning implicitly compensates the feature space distance among
incomplete-complete modality pairs. Furthermore, the proposed lightweight
reverse attention adapter explicitly compensates for the weak perceptual
semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid
consistency constraint to ensure stable prediction under all modality
combinations without large performance fluctuations. Without bells and
whistles, UniMRSeg significantly outperforms the state-of-the-art methods under
diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D
semantic segmentation, RGB-D/T salient object segmentation. The code will be
released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.

</details>


### [109] [Fast OTSU Thresholding Using Bisection Method](https://arxiv.org/abs/2509.16179)
*Sai Varun Kodathala*

Main category: cs.CV

TL;DR: 该论文提出了一种基于二分法优化的Otsu阈值分割算法，大幅提升了计算效率，几乎不影响分割精度。


<details>
  <summary>Details</summary>
Motivation: 经典Otsu方法虽然分割效果好，但由于需要遍历所有可能的阈值，导致计算量大，难以应用在大规模或实时图像处理场景。

Method: 将Otsu中用于阈值选择的类间方差函数视为单峰函数，利用二分法而非遍历法寻找最优阈值，将复杂度从O(L)降为O(log L)，极大减少计算次数。

Result: 在48幅标准测试图像上，方差计算次数减少91.63%，算法迭代次数减少97.21%；阈值精确匹配率为66.67%，95.83%的情况下阈值偏差在5灰度级以内。该方法在理论上和实验上都达到了对数收敛速度。

Conclusion: 该优化方法可直接应用于大规模、实时图像处理任务，解决了传统Otsu方法的效率瓶颈，同时保持分割精度和理论基础，是一种高效实用的图像分割改进方案。

Abstract: The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [110] [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
*Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang*

Main category: cs.CL

TL;DR: 提出了一种新的语言模型预训练方法（SBP），通过学习文档间关系并生成新语料进行联 合训练，有效提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 标准的预训练方法主要关注单文档内部的因果关系，难以高效捕捉文档之间的丰富关联；而更好地利用这些关联有可能提高模型性能，因此提出新方法。

Method: 先利用原始预训练集学习文档间的关系模型，然后基于此生成大量新的合成语料，并与原语料一起用于预训练。构建了与标准方案计算量匹配的实验，对3B参数模型在1T tokens上从零预训练，并与基线方法和oracle上限进行对比。

Result: SBP方法持续优于强力重复基线，并取得接近oracle上限（使用20倍于原始数据的独特数据）的效果。质性分析发现生成内容不只是同义改写，而是对原始概念的抽象和新叙述。

Conclusion: SBP不仅在实验中展现强大性能，还可以纳入贝叶斯框架解释——合成器学会抽象和重组相关文档间的潜在概念，为高效扩展预训练语料和提升语言模型表现提供新思路。

Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.

</details>


### [111] [Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha](https://arxiv.org/abs/2509.15255)
*Tandin Wangchuk,Tad Gonsalves*

Main category: cs.CL

TL;DR: 本文针对不丹国家语言宗卡语（Dzongkha）这种低资源语言，系统比较三种常见分词算法在宗卡语上的表现，发现SentencePiece最为有效。


<details>
  <summary>Details</summary>
Motivation: 当前大模型广泛应用于自然语言处理任务，分词器在确保文本被准确表达、便于模型处理方面发挥核心作用。但大多数预训练分词器仅适用于高资源语言，如英语，对于宗卡语这样复杂且低资源的语言表现不佳。该研究旨在填补宗卡语分词及NLP研究的空白。

Method: 本文评估了三种主流分词算法：Byte-Pair Encoding（BPE）、WordPiece和SentencePiece（Unigram），并与其它流行方法进行了比较。采用Subword Fertility、Proportion of Continued Words、Normalized Sequence Length及执行时间等指标进行性能评估。

Result: 所有算法在宗卡语分词上均展现出潜力，但SentencePiece表现最优，在各项评测指标上领先。

Conclusion: 针对低资源语言如宗卡语，需要有针对性的分词方案和持续的研究投入。本研究为宗卡语大模型的开发奠定了分词基础，显示了SentencePiece的应用潜力。

Abstract: Large Language Models (LLMs) are gaining popularity and improving rapidly.
Tokenizers are crucial components of natural language processing, especially
for LLMs. Tokenizers break down input text into tokens that models can easily
process while ensuring the text is accurately represented, capturing its
meaning and structure. Effective tokenizers enhance the capabilities of LLMs by
improving a model's understanding of context and semantics, ultimately leading
to better performance in various downstream tasks, such as translation,
classification, sentiment analysis, and text generation. Most pre-trained
tokenizers are suitable for high-resource languages like English but perform
poorly for low-resource languages. Dzongkha, Bhutan's national language spoken
by around seven hundred thousand people, is a low-resource language, and its
linguistic complexity poses unique NLP challenges. Despite some progress,
significant research in Dzongkha NLP is lacking, particularly in tokenization.
This study evaluates the training and performance of three common tokenization
algorithms in comparison to other popular methods. Specifically, Byte-Pair
Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their
suitability for Dzongkha. Performance was assessed using metrics like Subword
Fertility, Proportion of Continued Words, Normalized Sequence Length, and
execution time. The results show that while all three algorithms demonstrate
potential, SentencePiece is the most effective for Dzongkha tokenization,
paving the way for further NLP advancements. This underscores the need for
tailored approaches for low-resource languages and ongoing research. In this
study, we presented three tokenization algorithms for Dzongkha, paving the way
for building Dzongkha Large Language Models.

</details>


### [112] [Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages](https://arxiv.org/abs/2509.15260)
*Yujia Hu,Ming Shan Hee,Preslav Nakov,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出了SGToxicGuard数据集及评测框架，用于评估大型语言模型在新加坡多语环境下的安全性，涵盖Singlish、中文、马来语和泰米尔语，并揭示了当前多语LLM在安全防护上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全性研究主要集中在主流高资源语言，尚不清楚其在多语特别是低资源、多语环境下的表现和漏洞。新加坡本地语言丰富，但缺乏相关多语种对LLM安全性的系统性评测，容易埋下安全隐患。

Method: 作者提出了SGToxicGuard，这是一个专为新加坡多语情境设计的多语种恶意内容基准数据集及评测框架。方法采用red-teaming策略，从对话、问答、内容写作三类真实场景，对主流多语LLM模型进行系统性测试。

Result: 实验表明，当前多语LLM在涉及多语种和文化多样性的情境下，安全防护存在显著漏洞。研究结果揭示了模型在应对跨语种、跨文化毒性内容时的不足。

Conclusion: SGToxicGuard为提升多语和跨文化环境下的AI系统安全性提供了基础。研究呼吁在多语种、文化敏感度和毒性缓解措施上加强LLM安全防护，促进更加安全包容的智能系统发展。

Abstract: The advancement of Large Language Models (LLMs) has transformed natural
language processing; however, their safety mechanisms remain under-explored in
low-resource, multilingual settings. Here, we aim to bridge this gap. In
particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation
framework for benchmarking LLM safety in Singapore's diverse linguistic
context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a
red-teaming approach to systematically probe LLM vulnerabilities in three
real-world scenarios: \textit{conversation}, \textit{question-answering}, and
\textit{content composition}. We conduct extensive experiments with
state-of-the-art multilingual LLMs, and the results uncover critical gaps in
their safety guardrails. By offering actionable insights into cultural
sensitivity and toxicity mitigation, we lay the foundation for safer and more
inclusive AI systems in linguistically diverse environments.\footnote{Link to
the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}
\textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}

</details>


### [113] [PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms](https://arxiv.org/abs/2509.15335)
*Charlott Jakob,David Harbecke,Patrick Parschan,Pia Wenzel Neves,Vera Schmitt*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在政治倾向性表达下的事实判断一致性，发现评判性词汇对判断有显著影响，政治偏见影响有限，客观性提示并不能有效消除这种影响。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，大型语言模型（LLMs）在处理涉及政治立场的话题时通常倾向于左翼观点，但这些偏见在诸如事实核查等需要客观评判的应用中影响尚未系统探讨。作者希望明确LLMs在面对不同政治色彩但事实内容相同的表述时，是否会受到词汇和立场影响。

Method: 作者以德语事实声明为研究对象，通过将表达内容等价、政治色彩不同（如使用委婉语或贬义语）的陈述构造成“最小对偶对”，让六个主流LLM对这些陈述进行真假判断，从而系统评估模型的一致性与偏见来源。

Result: 实验发现，影响LLM事实判断的一大主要因素为陈述中是否含有评判性或情感色彩的词汇，而非特定的政治倾向。部分模型确实显示出政治偏见，但这种偏见在提示要求客观中并没有减弱。

Conclusion: 结论认为现实应用中，语言模型对事实敏感性更易被情感性词语触发，政治倾向影响在当前任务下相对有限，且单靠提示无法消除这类影响，对LLMs的客观性应用提出了警示。

Abstract: Large Language Models are increasingly used in applications requiring
objective assessment, which could be compromised by political bias. Many
studies found preferences for left-leaning positions in LLMs, but downstream
effects on tasks like fact-checking remain underexplored. In this study, we
systematically investigate political bias through exchanging words with
euphemisms or dysphemisms in German claims. We construct minimal pairs of
factually equivalent claims that differ in political connotation, to assess the
consistency of LLMs in classifying them as true or false. We evaluate six LLMs
and find that, more than political leaning, the presence of judgmental words
significantly influences truthfulness assessment. While a few models show
tendencies of political bias, this is not mitigated by explicitly calling for
objectivism in prompts.

</details>


### [114] [Quantifying Self-Awareness of Knowledge in Large Language Models](https://arxiv.org/abs/2509.15339)
*Yeongbin Seo,Dongha Lee,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在幻觉预测任务中的自我意识表现，发现这种表现很大程度上依赖于对问题表面模式的利用，而非真正的模型内省。提出了AQE指标衡量问题侧因素，并提出SCAO方法提升模型自我意识。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在区分真实与虚假回答时表现良好，常被认为展现出自我意识。然而，这种表现可能源于模型对问题表面特征的利用，而不是出自模型本身的内省能力，因此需要探究这种成功背后真正的原因。

Method: 作者提出了近似问题侧效应（AQE）来量化输入问题对模型预测幻觉能力的影响。同时，提出了SCAO（通过一词回答实现语义压缩）方法，目的是降低问题输入噪声，强化模型内部信号在预测过程中的作用。

Result: 通过多数据集实验证明，在去除部分问题侧提示的情况下，现有的优秀表现主要源自问题表面信号的利用。SCAO方法能更好地利用模型内在信息，在减少问题提示的条件下依然表现稳定且优异。

Conclusion: 当前LLM在幻觉预测上的自我意识表现很大部分依赖于问题输入内容，而非真正的自省机制。SCAO方法有效增强了模型自我意识评估的可靠性，为更公正地衡量LLM内省能力提供了工具。

Abstract: Hallucination prediction in large language models (LLMs) is often interpreted
as a sign of self-awareness. However, we argue that such performance can arise
from question-side shortcuts rather than true model-side introspection. To
disentangle these factors, we propose the Approximate Question-side Effect
(AQE), which quantifies the contribution of question-awareness. Our analysis
across multiple datasets reveals that much of the reported success stems from
exploiting superficial patterns in questions. We further introduce SCAO
(Semantic Compression by Answering in One word), a method that enhances the use
of model-side signals. Experiments show that SCAO achieves strong and
consistent performance, particularly in settings with reduced question-side
cues, highlighting its effectiveness in fostering genuine self-awareness in
LLMs.

</details>


### [115] [Real, Fake, or Manipulated? Detecting Machine-Influenced Text](https://arxiv.org/abs/2509.15350)
*Yitong Wang,Zhongping Zhang,Margherita Piana,Zheng Zhou,Peter Gerstoft,Bryan A. Plummer*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法HERO，用于细粒度区分人类写作、机器生成、机器润色和机器翻译的文本，并在多个领域和模型上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大多MGT检测方法只区分文本是人写还是机写，忽略了机器在文档中各种更细致的使用情形（如润色、翻译等）。而不同机器参与程度关联着文本潜在风险，需要对其进行细致检测。

Method: 作者提出了HERO模型，结合了对子类别的引导（Subcategory Guidance）和针对长度优化的模型（length-specialist models），从而实现对文本四种类型（人写、机写、机器润色、机器翻译）的区分。特别地，通过对容易混淆的子类别（如不同源语言）进行指导，提高检测的准确性。

Result: HERO模型在五个大模型、六个领域上的实验结果显示，平均比当前最优方法高2.5-3的mAP分数。

Conclusion: HERO模型能够有效区分不同机器参与方式的文本，显著优于现有方法，体现了该方法在实际文本鉴别场景中的实际价值。

Abstract: Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.

</details>


### [116] [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
*Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于因果中介的去偏框架，显著提升了多模态大语言模型在讽刺检测和情感分析中的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然能够整合视觉和文本信息，但常常依赖表面相关性，导致在复杂推理任务中表现不稳健，急需消除这类表面偏差。

Method: 提出了新颖的因果中介去偏框架，通过反事实样本区分核心语义与表面视觉、文本上下文，在训练阶段激活去偏机制。同时，融合动态路由的专家混合（MoE）架构，动态选择针对特定模态的去偏专家执行去偏。

Result: 在多模态讽刺检测和情感分析任务中，该框架在去偏能力和效果上显著超过单模态去偏和当前最先进模型。

Conclusion: 基于因果中介和专家混合的去偏框架能有效提升MLLMs在多模态复杂任务中的鲁棒性和泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.

</details>


### [117] [Speech Language Models for Under-Represented Languages: Insights from Wolof](https://arxiv.org/abs/2509.15362)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: 本文介绍了为西非小语种Wolof训练语音语言模型的过程，并提出了相关方法和结果。作者构建了高质量的语音数据集，在该数据集上对现有模型HuBERT进行了持续预训练，取得了更好的语音识别表现。同时，将该语音编码器集成到Wolof的语言大模型中，首次实现了Wolof的语音大模型，可处理语音翻译等任务。实验还发现模型可用于多步推理任务，最终在语音识别和翻译上均取得优异表现。代码和模型将开源。


<details>
  <summary>Details</summary>
Motivation: Wolof做为西非的小语种，相关的高性能语音和多模态语言模型极度稀缺。提升这类小语种的语音AI能力对数字包容性和语言保护具有重要意义。

Method: 1. 构建大规模且高质量的Wolof自发语音数据集；2. 在该数据集上对HuBERT模型做持续预训练，提升语音编码器表现；3. 将优化后的语音编码器集成进Wolof的语言大模型中，训练出第一个Wolof语音语言大模型（Speech LLM），支持语音识别和翻译等任务，并探索模型的多步思维能力。

Result: 预训练后的HuBERT在Wolof语音识别上超越了原始模型及面向非洲语种的相关模型。集成了新编码器的Wolof Speech LLM不仅语音识别表现提升，还能胜任语音翻译等下游任务。实验表明模型具备一定的多步推理能力。

Conclusion: 通过数据集构建和模型持续预训练，显著提升了Wolof语音模型能力。首次实现了Wolof语音大模型，能够支持复杂语音到文本及翻译流程，有助于小语种AI发展。模型和代码将向社区开源，推动相关研究。

Abstract: We present our journey in training a speech language model for Wolof, an
underrepresented language spoken in West Africa, and share key insights. We
first emphasize the importance of collecting large-scale, spontaneous,
high-quality speech data, and show that continued pretraining HuBERT on this
dataset outperforms both the base model and African-centric models on ASR. We
then integrate this speech encoder into a Wolof LLM to train the first Speech
LLM for this language, extending its capabilities to tasks such as speech
translation. Furthermore, we explore training the Speech LLM to perform
multi-step Chain-of-Thought before transcribing or translating. Our results
show that the Speech LLM not only improves speech recognition but also performs
well in speech translation. The models and the code will be openly shared.

</details>


### [118] [Frustratingly Easy Data Augmentation for Low-Resource ASR](https://arxiv.org/abs/2509.15373)
*Katsumi Ibaraki,David Chiang*

Main category: cs.CL

TL;DR: 本文提出了三种适用于低资源自动语音识别（ASR）的数据增强方法，通过生成新文本并合成语音，提高ASR模型的表现，在多种低资源语言上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言缺乏大量标注数据，ASR性能受限，急需简单有效的数据增强技术以提升模型效果。

Method: 方法包括：1）基于词汇注释的替换（gloss-based replacement）；2）随机替换（random replacement）；3）基于大语言模型（LLM）的文本生成。上述文本经由TTS系统合成相应音频。所有数据增强仅依赖原始有标注数据。

Result: 在Vatlongos、Nashta、Shinekhen Buryat和Kakabe四种极低资源语言上，结合原始音频与增强数据对Wav2Vec2-XLSR-53模型微调，取得显著表现提升。例如，Nashta语音识别的词错误率（WER）绝对值下降14.3%。同时，在高资源语言（如英语）上也表现出一定的泛化推广性。

Conclusion: 提出的数据增强方法切实提升了低资源语种的ASR性能，且对高资源语种也有效，展现了这类技术的广泛应用前景。

Abstract: This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.

</details>


### [119] [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
*Yangyi Li,Mengdi Huai*

Main category: cs.CL

TL;DR: 本文提出了一种能为大语言模型（LLMs）生成的自然语言解释提供有效不确定性保证的新框架，并验证了其在问答任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能生成有用的自然语言解释，但目前缺乏对这些解释可靠性或不确定性的定量分析，特别是在医疗等高风险领域，这种不确定性分析十分必要。现有工作还未能解决自然语言解释中的不确定性量化问题。

Method: 作者提出了一个新颖的不确定性估计框架，该框架适用于任何模型，并可在后处理阶段提供有效的不确定性保证。此外，作者还设计了一种鲁棒的不确定性估计方法，能在输入噪声存在的情况下，仍然保持不确定性保证。

Result: 在多个问答任务上的大量实验结果表明，所提方法能有效、可靠地为解释提供不确定性估计，并具有良好的鲁棒性。

Conclusion: 论文方法为自然语言解释的不确定性量化提供了新的解决思路，不仅可以后处理地应用于已生成的解释，也具有较强的应用扩展性。这对于提升模型解释的可信度和实际应用价值具有重要意义。

Abstract: Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.

</details>


### [120] [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
*Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros*

Main category: cs.CL

TL;DR: 本文探讨了在医学等数据受限领域，利用通用的抽象摘要模型进行迁移微调，并分析了过拟合和欠拟合问题。通过PEGASUS及其扩展模型在放射学报告数据集上的实验，揭示了模型训练过程中出现的性能波动，并指出模型规模与数据量不匹配可能导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有抽象摘要技术在医学等敏感且数据受限的领域内依然表现不佳，而医学影像报告等数据量日益增长，需要更高效自动化的文本摘要方法。因此，探索如何在此类受限条件下，优化非医学领域的摘要模型的迁移和微调，具有重要实际意义。

Method: 作者选择通用的PEGASUS及其增强模型PEGASUS-X，在中等规模公开放射报告数据集上，分别使用不同大小的微调数据和模型检查点进行训练。通过监测词汇与语义的多项指标，系统对比分析微调过程中的性能表现，特别关注过拟合、欠拟合与模型容量影响。

Result: 实验发现，PEGASUS模型在训练过程中展现出明显的性能起伏，包括epoch-wise的'双降'和'峰值—下降—恢复'行为。对于PEGASUS-X模型，较大的参数规模反而导致了性能下降，说明在数据有限时，模型容量过大存在风险。

Conclusion: 论文强调了高可表达性模型在训练数据有限时微调的风险和挑战，为医疗等专门领域摘要模型的稳健微调策略研究提供了经验与基础。未来需探索更适应数据稀缺情形的优化方法。

Abstract: Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

</details>


### [121] [BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition](https://arxiv.org/abs/2509.15430)
*Liuyuan Jiang,Xiaodong Cui,Brian Kingsbury,Tianyi Chen,Lisha Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新型自监督语音表示学习方法——BiRQ，它融合了高效和高质量标签的优点，无需外部标签编码器，在多个语音数据集上超越当前高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督语音学习方法在标签生成上存在权衡：强标签带来更好效果但开销大，简单高效的方法又牺牲了标签质量。需要一种兼顾效率和性能的方案。

Method: 提出BiRQ框架，将部分模型自身作为伪标签生成器，通过中间表征离散化和直接输入锚定标签相结合，并采用Gumbel-softmax进行端到端可微优化，无需外部编码器，支持高效的一阶双层优化。

Result: BiRQ在LibriSpeech、AMI等多个数据集上均显著优于BEST-RQ，保持了计算高效性与低复杂性，且无需额外的标签生成流程。

Conclusion: BiRQ有效结合了标签效率与质量提升的优势，提升了语音自监督学习方法的实用性和性能，为大规模语音表示学习带来更优的解决方案。

Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making
self-supervised learning essential for scalable representation learning. A core
challenge in speech SSL is generating pseudo-labels that are both informative
and efficient: strong labels, such as those used in HuBERT, improve downstream
performance but rely on external encoders and multi-stage pipelines, while
efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.
We propose BiRQ, a bilevel SSL framework that combines the efficiency of
BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key
idea is to reuse part of the model itself as a pseudo-label generator:
intermediate representations are discretized by a random-projection quantizer
to produce enhanced labels, while anchoring labels derived directly from the
raw input stabilize training and prevent collapse. Training is formulated as an
efficient first-order bilevel optimization problem, solved end-to-end with
differentiable Gumbel-softmax selection. This design eliminates the need for
external label encoders, reduces memory cost, and enables iterative label
refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ
while maintaining low complexity and computational efficiency. We validate our
method on various datasets, including 960-hour LibriSpeech, 150-hour AMI
meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

</details>


### [122] [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
*Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams*

Main category: cs.CL

TL;DR: 本文提出了一种新方法PILOT，通过结构化心理语言画像，提高大模型生成文本中人格设定的准确可控性。


<details>
  <summary>Details</summary>
Motivation: 现有依赖自然语言描述的人格设定会导致模型对属性的误判和强调，难以精准控制生成结果。作者希望通过结构化方式提高控制能力和输出质量。

Method: 提出PILOT框架，包括两个阶段：首先将自然语言中的人格描述转化为多维度、标准化的心理语言特征。接着用这些画像引导大模型（如Mistral Large 2、Deepseek-R1、LLaMA 3.3 70B）生成文本，借助三种设定（自然语言引导NPS、结构化SBS、混合HPS）对比实验。

Result: 结构化方案SBS显著减少了生硬重复，提升了文本的一致性（silhouette得分从0.098升至0.237，topic purity从0.773到0.957）；SBS文本更简明且一致性高，NPS则词汇多样但可预测性差，HPS则兼顾多样性和结构一致性。三种方案在专家评分下质量无统计学差异。

Conclusion: PILOT可以有效、灵活地引导大模型按预定人格画像生成高质量、多样化且一致的文本输出，结构化方法在控制性和自然度之间取得良好平衡。

Abstract: Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

</details>


### [123] [Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](https://arxiv.org/abs/2509.15476)
*Zhu Li,Xiyuan Gao,Yuqing Zhang,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLM）与多模态LLM在讽刺检测中的表现，涵盖英文与中文数据集，探讨了不同模态和融合方式的优劣。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测难度大，因为讽刺常依赖于文本、语音和视觉间的细微线索。以往研究集中于文本或文本-视觉，缺乏对音频-视觉-文本综合讽刺理解的探索。

Method: 作者采用系统实验，对英文（MUStARD++）和中文（MCSD 1.0）数据集上的大语言模型和多模态LLM，分别在零样本、少样本和LoRA微调下进行讽刺检测评估。测试方式包括直接分类和作为特征编码器，融合不同模态特征。

Result: 音频模型在单一模态下表现最佳，而文本-音频、音频-视觉组合优于单模态和三模态。多模态LLM如Qwen-Omni在零样本和微调下表现突出。

Conclusion: 多模态LLM有潜力实现跨语言、多模态的讽刺理解，建议今后研究关注多模态及跨语言扩展。

Abstract: Sarcasm detection remains a challenge in natural language understanding, as
sarcastic intent often relies on subtle cross-modal cues spanning text, speech,
and vision. While prior work has primarily focused on textual or visual-textual
sarcasm, comprehensive audio-visual-textual sarcasm understanding remains
underexplored. In this paper, we systematically evaluate large language models
(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and
Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In
addition to direct classification, we explore models as feature encoders,
integrating their representations through a collaborative gating fusion module.
Experimental results show that audio-based models achieve the strongest
unimodal performance, while text-audio and audio-vision combinations outperform
unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show
competitive zero-shot and fine-tuned performance. Our findings highlight the
potential of MLLMs for cross-lingual, audio-visual-textual sarcasm
understanding.

</details>


### [124] [Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models](https://arxiv.org/abs/2509.15478)
*Madison Van Doren,Casey Ford,Emily Dix*

Main category: cs.CL

TL;DR: 本研究评估了主流多模态大模型（MLLMs）在面对对抗性提示时的安全性，发现不同模型和输入方式下，输出有害内容的情况存在显著差异。某些模型容易被利用，甚至文本输入比多模态输入更易绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型正广泛应用于实际场景，但其在敌对环境下（如对抗性提示下）的安全性尚未被充分研究，尤其是在多模态输入方面。因此，有必要系统评估MLLMs的无害性，以防其被滥用。

Method: 研究团队邀请26名安全专家撰写了726条对抗性提示，涵盖非法活动、虚假信息和不道德行为等三类危害。这些提示用于测试GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus四款模型，覆盖纯文本与多模态输入。17名标注员对2904条模型输出按有害程度进行5分评分。通过统计分析模型类型与输入方式对有害输出的影响。

Result: 不同模型的有害响应率显著不同：Pixtral 12B最高（约62%），Claude Sonnet 3.5最低（约10%）。文本输入绕过安全机制的效果略高于多模态输入。统计分析证实模型类型和输入方式均显著影响输出的有害性。

Conclusion: 随着MLLMs应用增多，安全问题愈发突出。研究呼吁尽快建立健全、全面的多模态安全基准体系，以确保模型可靠、安全地应用于现实场景。

Abstract: Multimodal large language models (MLLMs) are increasingly used in real world
applications, yet their safety under adversarial conditions remains
underexplored. This study evaluates the harmlessness of four leading MLLMs
(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to
adversarial prompts across text-only and multimodal formats. A team of 26 red
teamers generated 726 prompts targeting three harm categories: illegal
activity, disinformation, and unethical behaviour. These prompts were submitted
to each model, and 17 annotators rated 2,904 model outputs for harmfulness
using a 5-point scale. Results show significant differences in vulnerability
across models and modalities. Pixtral 12B exhibited the highest rate of harmful
responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).
Contrary to expectations, text-only prompts were slightly more effective at
bypassing safety mechanisms than multimodal ones. Statistical analysis
confirmed that both model type and input modality were significant predictors
of harmfulness. These findings underscore the urgent need for robust,
multimodal safety benchmarks as MLLMs are deployed more widely.

</details>


### [125] [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
*Ahmed Abdou*

Main category: cs.CL

TL;DR: 作者提出了一种针对细粒度阿拉伯语可读性分类的新型后处理技术，通过不依赖于特定模型，利用保型预测（conformal prediction）生成有覆盖保证的预测集，并用softmax归一化概率加权平均，提高了等级分类的准确性和稳定性，对减少严重错分尤其有效。


<details>
  <summary>Details</summary>
Motivation: 细粒度的可读性等级（19个等级）评估任务误分类代价高，需要稳定且有统计保证的预测方法，同时要求方法易于整合多种基础模型，提升最终可用性。

Method: 采用模型无关的保型预测生成带有可靠性的预测集合，然后对这些集合内的预测概率用softmax重归一化并加权平均，从而实现结合不确定性的解码过程，降低错分等级带来的惩罚。

Result: 所提出方法在不同基础模型上均提升了1-3个百分点的QWK分数。在严苛赛道下，句子级测试得到84.9%(test)/85.7%(blind test)，文档级73.3%，优于以往方法。

Conclusion: 该方法不仅提升了可读性分级准确性，还为实际阿拉伯语教育评测中的人工复核提供了更少、但统计保证性强的候选等级，兼顾了理论与实用性。

Abstract: We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

</details>


### [126] [LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference](https://arxiv.org/abs/2509.15515)
*Hantao Yang,Hong Xie,Defu Lian,Enhong Chen*

Main category: cs.CL

TL;DR: 本文针对LLM缓存多臂赌博机问题，在查询大小不均的实际场景下提出了新的缓存选择算法，并有效降低了LLM推理成本。


<details>
  <summary>Details</summary>
Motivation: 此前相关工作普遍假设查询大小一致，然而现实中查询通常具有异质性，导致缓存的选择与替换变得更难计算和优化。因此，急需高效算法来处理查询大小不一致下的缓存管理问题，以提升LLM推理的性价比。

Method: 作者将最优缓存选择问题建模为背包问题，并采用累积式策略平衡计算开销和缓存更新。在理论分析上，作者证明了算法的遗憾界达到O(√MNT)，并给出此前未涉足的问题依赖型界。实验基于真实数据集进行。

Result: 实验结果表明，提出的算法在真实场景下可将总成本降低约12%。理论上，算法取得更优的遗憾界限。

Conclusion: 本文方法在处理异构查询的大模型缓存问题上表现优异，兼具理论优势和实际成本降低能力，有望推动相关技术在真实应用中的普及。

Abstract: This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.

</details>


### [127] [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
*Siyang Wu,Zhewei Sun*

Main category: cs.CL

TL;DR: 本文系统性比较了人类与大模型生成的俚语用法，发现大模型对俚语的理解与人类仍有明显不同。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在俚语识别与解释等任务上应用广泛，但其对俚语结构知识的掌握与人类是否一致、普适性与可靠性如何尚未明确。本文旨在系统比较人类与模型生成的俚语用法，从而揭示模型对俚语的理解差异。

Method: 提出了包含系统性偏差、创造性与信息性的评估框架，并对比了Online Slang Dictionary（OSD）中的人类俚语用法与GPT-4o、Llama-3生成的俚语用法。

Result: 实验发现，大模型在俚语生成上表现出显著的系统性偏差。虽然捕捉到了部分创造性特征，但与人类的俚语使用仍存在明显差距，特别是在进行语言学分析等外推任务时。

Conclusion: 当前大模型虽具备一定俚语知识，但其理解与生成的俚语用法仍与人类有差异，尚难胜任深度俚语分析等高度依赖人类认知的任务。

Abstract: Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.

</details>


### [128] [A method for improving multilingual quality and diversity of instruction fine-tuning datasets](https://arxiv.org/abs/2509.15549)
*Chunguang Zhao,Yilun Liu,Pufan Zeng,Yuanchang Luo,Shimin Tao,Minggui He,Weibin Meng,Song Xu,Ziang Chen,Chen Liu,Hongxia Ma,Li Zhang,Boxing Chen,Daimeng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为M-DaQ的新方法，通过选择高质量且语义多样的多语言微调样本，提升大语言模型在多语言场景下的能力，并在18种语言上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大模型表现受限于高质量多语言数据的缺乏，同时现有的数据选择方法往往依赖于简单的规则或特定语言假设，难以推广到多种语言。为提升多语言泛化能力，需要新的数据选择方法。

Method: 提出M-DaQ方法，基于数据质量与多样性准则，筛选用于Instruction Fine-Tuning（指导微调）的多语言样本，并系统性地验证了“表层对齐假说”在多语言场景下的适用性。实验涵盖18种语言，并进行人工评估。

Result: M-DaQ方法在18种语言上的实验显示，相比基础模型，微调后有超过60%的胜出率。人工评价也显示响应质量提升，尤其在文化相关性上有所进步。

Conclusion: M-DaQ能有效提升大模型的多语言泛化与文化适应能力，为多语言模型训练和选择样本提供了新思路，并已开源代码以促进后续研究。

Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.

</details>


### [129] [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
*Xiaowei Zhu,Yubing Ren,Fang Fang,Qingfeng Tan,Shi Wang,Yanan Cao*

Main category: cs.CL

TL;DR: 本文提出了一种基于DNA修复思想的新颖方法，有效检测AI生成文本，并取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，AI生成文本与人类写作之间的界限日趋模糊，带来了误导信息、作者归属和知识产权等社会风险，因此亟需可靠的AI文本检测方法。

Method: 受DNA修复过程启发，提出DNA-DetectLLM方法：针对每段输入，构造理想的AI生成序列，通过迭代修复非最优词元，累计修复量用作可解释检测信号。该方法为零样本检测方案。

Result: 实验证明DNA-DetectLLM方法在多个公开数据集上实现了最先进的检测性能，相较已有方法AUROC提高5.55%，F1得分提高2.08%，且对抗攻击及不同文本长度表现出较强鲁棒性。

Conclusion: DNA-DetectLLM不仅提升了AI文本检测准确性，还具备可解释性和强适应性，有望应对日益严峻的文本生成检测挑战。

Abstract: The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.

</details>


### [130] [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
*Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng*

Main category: cs.CL

TL;DR: 本文提出Climb框架，优化多语言数据分配，在提升LLM多语表现的同时减少训练所需数据量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多语表现受限于训练语料的语言比例设定，不易确定最优分配方案；跨语言干扰和数据规模敏感性使配置难度增大。

Method: 提出Climb框架，通过交互感知的语言比例量化各语言分配，采用先均衡边际收益再最大化分配向量的两步优化法，简化多语数据配置。

Result: 通过大量实验，Climb有效度量跨语言交互并优化多语分配，在不同多语场景下提升表现，甚至能用更少数据达开源LLM同级或更优多语能力。

Conclusion: Climb框架系统性优化多语训练分配，增强了LLM多语性能并提高数据利用效率，为多语言大模型训练提供高效实用的新策略。

Abstract: Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

</details>


### [131] [How important is language for human-like intelligence?](https://arxiv.org/abs/2509.15560)
*Gary Lupyan,Hunter Gentry,Martin Zettersten*

Main category: cs.CL

TL;DR: 语言不仅仅是思想的表达工具，还深刻影响并塑造了人类的认知能力，对人类智能和通用人工智能的形成有核心作用。


<details>
  <summary>Details</summary>
Motivation: 探讨语言在认知中的根本角色，回应人工智能和认知科学中新出现的问题，即：语言只是表达，还是能改变和提升思维能力。

Method: 理论分析和文献综述，详细探讨语言如何通过精炼表达和文化演化的压缩信息帮助抽象思维与推理。

Result: 指出语言能作为紧凑、压缩的抽象表达工具，让学习者（无论人类还是人工智能）能够高效理解和复现复杂的概念与因果结构。

Conclusion: 接触语言能够使学习系统构建出精炼的世界模型，进而对抽象和因果结构进行逆向工程，这对于发展更通用的人类和人工智能系统至关重要。

Abstract: We use language to communicate our thoughts. But is language merely the
expression of thoughts, which are themselves produced by other, nonlinguistic
parts of our minds? Or does language play a more transformative role in human
cognition, allowing us to have thoughts that we otherwise could (or would) not
have? Recent developments in artificial intelligence (AI) and cognitive science
have reinvigorated this old question. We argue that language may hold the key
to the emergence of both more general AI systems and central aspects of human
intelligence. We highlight two related properties of language that make it such
a powerful tool for developing domain--general abilities. First, language
offers compact representations that make it easier to represent and reason
about many abstract concepts (e.g., exact numerosity). Second, these compressed
representations are the iterated output of collective minds. In learning a
language, we learn a treasure trove of culturally evolved abstractions. Taken
together, these properties mean that a sufficiently powerful learning system
exposed to language--whether biological or artificial--learns a compressed
model of the world, reverse engineering many of the conceptual and causal
structures that support human (and human-like) thought.

</details>


### [132] [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
*Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo*

Main category: cs.CL

TL;DR: 本文提出一种高效合成大模型长上下文训练数据的方法LiteLong，结合主题结构和多智能体讨论，提升数据质量且降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有大模型长文档训练数据合成方法大多依赖相关性聚合，效率低且成本高，不能有效支持大模型长期依赖能力训练。亟需低资源、高效的长上下文数据合成方法。

Method: LiteLong利用BISAC图书分类系统建立系统化分层主题结构，并通过多LLM智能体辩论生成多样高质量的主题。每个主题下用BM25轻量级检索获得相关文档，将其拼接为128K-token规模样本用于训练。

Result: 在HELMET和Ruler基准测试中，LiteLong在长依赖任务上取得了与其他主流增强方法相当的表现，并且能无缝融合到现有增强方案。

Conclusion: LiteLong大幅降低了长上下文数据合成的算力和工程门槛，使高质量训练数据的获取更便捷，有助于长上下文语言模型进一步研究和应用。

Abstract: High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

</details>


### [133] [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
*Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song*

Main category: cs.CL

TL;DR: 本文提出了R2U方法，用于提升检索增强生成（RAG）系统的检索文档对生成任务的实际效用，通过直接优化能够生成正确答案的概率来弥合检索相关性和生成实用性之间的鸿沟。实验结果显示在多个开放域问答基准上均优于已有桥接方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中，检索文档往往虽与主题相关，但缺乏对生成任务真正有用的推理信息。已有的“桥接”方法难以真正提升文档的生成效用，因此有必要探索新的优化方案。

Method: 作者提出R2U方法，核心是通过过程监督，直接优化检索文档在生成正确答案中的实际作用。为降低监督成本，还利用LLM扩展监督信号，通过高效的蒸馏流程帮助小型改写器模型泛化提升。

Result: 在多个开放域问答基准数据集上，R2U方法在各项评测指标上均优于已有的桥接模块基线，表现出更优的生成效用和答案准确率。

Conclusion: 直接优化文档对生成答案的贡献而非仅做相关性提升，是提升RAG系统实用性的有效路径。R2U方法为解决检索与生成目标不一致问题提供了可行且有效的新方案。

Abstract: Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

</details>


### [134] [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579)
*Yun Tang,Cindy Tseng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Chunk SSL的新型自监督学习算法，可同时支持流式和离线语音预训练，在语音识别及翻译任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着语音技术的发展，对低延迟的人机语音交互需求骤增，现有自监督学习方法多假设整句输入，难以适配实际流式应用中常见的部分语句输入。需要通用方案兼顾流式与离线场景。

Method: 该文提出Chunk SSL算法：通过mask预测损失训练声学编码器，利用同一chunk及前面chunk的非mask帧恢复mask帧，并引入“复制与拼接”数据增强进行高效预训练。引入高分辨率有限标量量化（FSQ）模块离散输入特征，通过组mask预测损失缓解大词表带来的算力与内存消耗。

Result: 在语音识别（LibriSpeech）和语音翻译（Must-C）两个数据集及任务上验证，Chunk SSL在流式与离线两种模式下都达到了与当前先进方法相当甚至更优的效果。

Conclusion: Chunk SSL为流式和离线语音自监督预训练提供了统一且有效的解决方案，在语音转文本相关任务中表现突出，具有较强的实际应用价值。

Abstract: Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.

</details>


### [135] [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
*Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung*

Main category: cs.CL

TL;DR: 本文提出了一个新的经典逻辑推理评测基准DivLogicEval，用于更加准确、可靠地评估大语言模型在自然语言逻辑推理方面的能力，并引入新评测方法，以降低偏差和随机性带来的影响。实验对比了多种流行大模型的逻辑推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有的逻辑推理基准集合往往将多种推理技能混合评估，导致逻辑推理能力的评测结果不可信。同时，这些数据集语言多样性不足，与理想逻辑推理数据的分布存在偏差，带来了评价结果的偏见。

Method: 1. 构建了包含语言和陈述多样性的新逻辑推理数据集DivLogicEval，并设计题目弱化直觉，突出纯粹逻辑推理需求。2. 提出新的评估指标，减少大模型固有的随机性和偏见对评估结果的影响。3. 通过实验分析不同大模型在该基准下的推理表现。

Result: 实验显示，DivLogicEval中的问题确实显著依赖逻辑推理能力，并揭示了现有主流大语言模型在纯粹逻辑推理任务上的能力差异。

Conclusion: 本文工作为科学评估大语言模型在自然语言逻辑推理能力提供了更优质、更具挑战的数据集和更加公正的评测方法，对推动相关模型能力发展有重要意义。

Abstract: Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.

</details>


### [136] [SciEvent: Benchmarking Multi-domain Scientific Event Extraction](https://arxiv.org/abs/2509.15620)
*Bofu Dong,Pritesh Shah,Sumedh Sonawane,Tiyasha Banerjee,Erin Brady,Xinya Du,Ming Jiang*

Main category: cs.CL

TL;DR: 本文提出了SciEvent数据集，这是一个针对科学信息抽取（SciIE）的多领域基准，解决了现有方法过于狭窄和上下文理解能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的科学信息抽取方法主要局限于狭窄领域，难以适应跨学科研究，并且缺乏理解科学文本上下文的能力，导致信息碎片化甚至矛盾。

Method: 作者构建了SciEvent多领域基准数据集，包含五个不同领域的500篇科学摘要，采用统一的事件抽取（EE）标注体系。工作流程分为两步：首先将摘要分割为核心科学活动（背景、方法、结果和结论），然后提取相应的事件触发词和论元。作者还对细调事件抽取模型、大型语言模型（LLM）和人工标注人进行了实验对比。

Result: 实验结果显示，现有模型在社会学和人文等领域表现较差，人类标注者与模型之间的性能差距仍然明显。

Conclusion: SciEvent为科学信息抽取任务提供了有挑战性的多领域基准，并推动该领域向通用化和上下文感知能力更强的方向发展。

Abstract: Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.

</details>


### [137] [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
*Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的方法，实现大语言模型（LLM）层面的概念遗忘（Concept Unlearning, CU），不仅限于删除特定句子，还能精准去除更广泛的概念知识，同时尽量保留无关内容。


<details>
  <summary>Details</summary>
Motivation: 以往机器遗忘方法多只能移除具体句子，难以满足删除更大范围知识（如某人或某事件）等需求，这对于实际中的隐私和版权保护显得不足。

Method: 作者提出利用知识图谱表达LLM中的内部知识，把需要遗忘的内容建模为节点和关联边的删除。具体实现中，通过指令引导LLM生成目标相关的知识三元组和解释性句子，再对这些表示进行定向遗忘。

Result: 实验表明，该方法在真实和合成数据集上均可实现更精确且全面的概念级遗忘，同时最大限度减小对无关知识的影响。

Conclusion: 通过结合知识图谱与生成式策略，可以实现更直观、高效且细粒度的LLM概念遗忘操作，为应对隐私保护和知识订正等多种实际需求提供了有力工具。

Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

</details>


### [138] [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
*Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara*

Main category: cs.CL

TL;DR: 现有的大语言模型（LLMs）遗忘方法主要通过禁止生成不良输出来实现，但这种方式往往无法真正忘记内部知识。本文提出了一种直接干预模型内部激活的新型遗忘技术，在稀疏自编码器潜在空间中将目标实体的激活与“未知”实体对齐，从而实现真正的遗忘，避免模型崩溃，同时保留非目标知识。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类应用中的普及，隐私和版权问题引发了对更有效遗忘技术的需求。以往抑制输出的方法不能从根本上消除模型记忆，且容易导致模型性能崩溃。因此亟需一种能够真正让模型忘记特定知识，不仅仅是“禁止输出”的方法。

Method: 本文提出直接修改模型内部激活的遗忘方法。将“遗忘”定义为：被遗忘目标的激活与未知实体的激活无法区分。具体做法是在稀疏自编码器潜在空间中，使目标实体的激活远离已知实体并趋近于未知实体，实现对模型内部表征的重新调整。

Result: 实验表明，所提方法能有效将被遗忘目标的内部激活对齐到“未知”状态，这一效果是传统抑制法难以实现的。此外，在问答任务中，该方法显著降低了模型对目标知识的回忆能力，同时对非目标知识的损伤较小。

Conclusion: 本文方法能实质性地实现大语言模型对特定知识的遗忘，相比传统抑制法更有效且副作用更小，为隐私和版权合规提供了技术支撑。

Abstract: As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.

</details>


### [139] [Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation](https://arxiv.org/abs/2509.15640)
*Nhu Vo,Nu-Uyen-Phuong Le,Dung D. Le,Massimo Piccardi,Wray Buntine*

Main category: cs.CL

TL;DR: 本文评估了多种多语大型语言模型（LLM）在医学英语-越南语翻译上的表现，发现模型规模对翻译效果有决定性影响，但医学专有词典和检索增强策略能进一步提升专业领域任务的表现。


<details>
  <summary>Details</summary>
Motivation: 医学英语-越南语机器翻译对于越南医疗沟通至关重要，但越南语属于低资源语言，相关研究匮乏，亟需系统性探索适合该任务的高效翻译框架。

Method: 基于MedEV医学翻译数据集，系统性评估0.5B至9B参数规模的6个多语LLM，比较其零样本、少样本及利用医学词典（Meddict）增强的提示策略。考察模型规模、提示策略以及医学领域词汇检索等因素对性能的影响。

Result: 结果显示，大模型在零样本场景已能取得强劲效果，少样本提示带来提升有限。结合医学词典和利用嵌入检索方法的举例能够显著改善医学专有名词和领域特定翻译。

Conclusion: 多语LLM对医学英越翻译显示出巨大潜力，但仍存在局限。模型规模是提升效果的关键，领域专有名词表和嵌入检索等技术可补足当前LLM在专业领域的不足。

Abstract: Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.

</details>


### [140] [Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations](https://arxiv.org/abs/2509.15655)
*Linyang He,Qiaolin Wang,Xilin Jiang,Nima Mesgarani*

Main category: cs.CL

TL;DR: 本文系统性评估了基于Transformer的语音语言模型在编码句法和语义特征方面的能力，并发现这些模型对语法特征的编码明显优于概念特征。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer结构极大提升了神经语音识别与理解，但目前对于语音语言模型在编码细致语法及语义特征方面的能力尚不清楚。本文受大语言模型语言能力测评启发，填补该领域评估方法的空白。

Method: 作者设计了最小对比实验（minimal pair），并对包含S3M、ASR、编码器和AudioLLM等不同架构的SLMs，从层级和时间分辨角度，横跨71类不同语言任务，全面分析其对句法与概念特征的表征能力。

Result: 分析结果显示，所有类型的语音语言模型在编码语法特征（如语法结构）方面，一致优于对概念性特征（如语义或上下文知识）的编码。

Conclusion: Transformer类语音语言模型在对语法特征的表征方面比概念特征更稳健，提示未来模型设计和任务应用时需关注对深层语义编码能力的改进。

Abstract: Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.

</details>


### [141] [VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](https://arxiv.org/abs/2509.15667)
*Dimitrios Damianos,Leon Voukoutis,Georgios Paraskevopoulos,Vassilis Katsouros*

Main category: cs.CL

TL;DR: 本文提出了一种跨模态融合框架，通过结合大型语言模型（LLM）和声学编码-解码架构（如Whisper），实现了支持语音输入的LLM。实验取得了希腊语语音识别的最新效果，并引入了首个希腊语语音LLM—VoxKrikri。


<details>
  <summary>Details</summary>
Motivation: 为了让LLM具备处理语音能力，并推动多语种、低资源语言的语音识别任务，需要有效融合语音和文本模态的信息。以往直接使用音频嵌入方法局限较多，需要探索更高效的对齐机制。

Method: 本文提出以音频条件化文本空间为对齐机制，在Whisper解码器隐藏状态和LLM之间通过跨模态注意力进行融合，并在连续文本表示空间内工作，支持离线及流式两种模式。

Result: 提出的方案可有效对齐语音和文本模态表示，在希腊语自动语音识别任务上实现了约20%的相对性能提升，达到最优水平，并首次构建了希腊语语音LLM：VoxKrikri。

Conclusion: 连续空间融合是一种有效的跨模态对齐方法，特别适用于多语种和低资源语言场景，大幅提升语音识别性能，为后续Speech LLM和多模态模型发展提供了新路径。

Abstract: We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.

</details>


### [142] [Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment](https://arxiv.org/abs/2509.15701)
*Ke Wang,Wenning Wei,Yan Deng,Lei He,Sheng Zhao*

Main category: cs.CL

TL;DR: 本文探讨了利用大型多模态模型（LMMs）进行自动发音评估（APA）的效果，发现模型在单一粒度任务上表现优异，但细粒度（如音素级别）评估仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 自动发音评估对于计算机辅助语言学习非常关键，需要多粒度、多方面的评测。随着大规模多模态模型（LMMs）的发展，研究其在APA任务中的潜力成为迫切需求。

Method: 作者采用Speechocean762和一个私有语料库对LMMs进行微调，并评估了微调后LMM在多个粒度（如单词、句子、音素级别）的发音评测表现。

Result: 经过微调的LMM在零样本模式下表现显著提升，在单一粒度任务上可与公开或商用系统竞争，在单词和句子级别有良好效果，但音素级别评测依然困难。PCC达0.9而SCC仅0.6左右，提示SCC更好反映序一致性。

Conclusion: LMMs对APA有较大潜力，尤其在较粗粒度任务，但对细粒度问题还有待改进，未来应关注更精细的建模和排名感知评价方法。

Abstract: Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.

</details>


### [143] [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
*Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn*

Main category: cs.CL

TL;DR: 本文探索了通过高层次、认知启发式反馈辅助的互动学习，能否让语言模型在较少数据下获得更好的生成能力。研究发现，和传统的下一个词预测相比，这种方法在故事生成能力提升上更高效。


<details>
  <summary>Details</summary>
Motivation: 儿童通过与社会环境互动高效习得语言，而大多数大模型仅依赖大规模文本下的自动预测。作者受到这种对比的启发，思考能否借鉴认知学习机制，提升模型的数据利用效率。

Method: 作者提出用教师-学生模型结构：学生生成故事，教师模型从可读性、叙事连贯性和创造力等多维度对学生故事给予高层反馈。通过调整进入该反馈环节前的预训练词量，评估互动学习对模型语言能力提升的影响。

Result: 实验表明，通过仅100万词的高层交互学习，模型的故事生成能力可与传统下一个词预测方式下用4.1亿词的数据获得的提升相当。

Conclusion: 高层次、认知启发式反馈能显著提升语言模型训练的数据效率，对提升故事生成等复杂语言能力尤其有效。

Abstract: Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.

</details>


### [144] [REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting](https://arxiv.org/abs/2509.15723)
*Nannan Huang,Haytham M. Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种通过频率框架化提示（REFER）提高大语言模型（LLM）在观点摘要中公平性的新方法。实验证明，REFER能更好地代表多元观点，尤其在大型模型和强推理指令下效果更佳。


<details>
  <summary>Details</summary>
Motivation: 以往提升LLM观点摘要公平性的方法依赖超参数调整或要求用户提供真实分布信息，但实际操作中用户很少调整参数且难以获得准确分布数据。因此，亟需一种更实用、有效的方法提升模型公平性。

Method: 借鉴认知科学发现：基于频率的信息能显式呈现参考类别、减小认知负载，进而减少系统性偏见，作者将‘频率框架提示（REFER）’方法应用于LLM，通过系统实验比较频率型与概率型提示对LLM生成公平观点摘要的影响。

Result: 实验表明，采用REFER频率框架提示能显著提升LLM生成的观点摘要的公平性。尤其是在大型模型或者附加更强推理指令时，该方法的增强效果更为明显。

Conclusion: REFER方法能够在不依赖复杂调参或真实分布信息的前提下，有效提升LLM在多元意见信息摘要任务中的公平性。

Abstract: Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.

</details>


### [145] [Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics](https://arxiv.org/abs/2509.15739)
*Reza Sanayei,Srdjan Vesic,Eduardo Blanco,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在处理非线性推理任务中的能力，尤其是在辩论场景下如何模拟结构化推理，并评估了其与正式计算论证语义（QuAD）的表现一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在线性推理任务上表现优异，但在复杂、非线性结构（如实际辩论）中的能力尚未充分研究。作者希望了解LLM在不提供结构化论证图的情况下，能否仅根据对话内容，推断并评估论据的说服力。

Method: 作者以QuAD语义为评估标准，利用两个NoDE数据集中的辩论对话，要求LLM无需显式论证图、仅依据文本对话对论据进行排序。实验中比较了不同的高级提示方案，包括Chain-of-Thought和In-Context Learning，以观察对结果的影响。

Result: LLM与QuAD的排名存在一定一致性，但当输入变长或对话结构混乱时，模型表现下降。采用高级提示（如推理链、上下文学习）可部分减少因论据长度和位置而带来的偏差，从而提升稳定性。

Conclusion: LLM能够对一定程度上的结构化论证进行建模，但在处理更复杂的推理关系时存在局限。未来需要发展具备图结构感知能力的推理方法，以进一步提升LLM在正式论证语义建模中的表现。

Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.

</details>


### [146] [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://arxiv.org/abs/2509.15763)
*Chenlong Deng,Zhisong Zhang,Kelong Mao,Shuaiyi Li,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 该论文提出了一种名为UniGist的序列级长上下文压缩框架，可以在大语言模型处理长文本时高效压缩KV缓存，提升内存利用率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型支持更长上下文，KV缓存所需内存暴增，成为实际部署瓶颈。现有主流压缩方法难以在不损失重要上下文信息的情况下降低KV缓存大小。

Method: UniGist通过用称为gist的特殊压缩token替换原始token来精细压缩上下文信息，并引入了无分块训练和gist shift策略优化GPU训练效率。此外，压缩token可在推理时直接移除带来实时内存优化。

Result: 在多个长上下文任务上，UniGist相较其他方法有更好的压缩质量，尤其在细节回忆任务和长距离依赖建模方面表现突出。

Conclusion: UniGist能够有效解决长上下文处理过程中的KV缓存内存瓶颈，为大语言模型的实际部署提供了更优的压缩解决方案。

Abstract: Large language models are increasingly capable of handling long-context
inputs, but the memory overhead of key-value (KV) cache remains a major
bottleneck for general-purpose deployment. While various compression strategies
have been explored, sequence-level compression, which drops the full KV caches
for certain tokens, is particularly challenging as it can lead to the loss of
important contextual information. To address this, we introduce UniGist, a
sequence-level long-context compression framework that efficiently preserves
context information by replacing raw tokens with special compression tokens
(gists) in a fine-grained manner. We adopt a chunk-free training strategy and
design an efficient kernel with a gist shift trick, enabling optimized GPU
training. Our scheme also supports flexible inference by allowing the actual
removal of compressed tokens, resulting in real-time memory savings.
Experiments across multiple long-context tasks demonstrate that UniGist
significantly improves compression quality, with especially strong performance
in detail-recalling tasks and long-range dependency modeling.

</details>


### [147] [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
*Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出了一套全流程、可复现的多语种语料库构建方案，包含爬虫采集和文本对齐，并引入了新的段落对齐算法GAPA，构建了迄今为止最大、完全由人工翻译形成的公开平行语料库。


<details>
  <summary>Details</summary>
Motivation: 现有基于联合国文档建立的多语种语料库存在流程不透明、难以复现和规模受限等问题，亟需一种高质量、大规模且流程公开的平行语料建设方案。

Method: 提出了端到端的数据采集与对齐流程，包括网页爬取和文本对齐，其中核心创新为基于图的段落对齐算法GAPA，使对齐过程更高效灵活；整个过程可单机运行也支持分布式扩展。

Result: 构建的语料库包含超过7.13亿个英文词，规模比以往工作翻倍，实现了最大规模完全由人工翻译、非AI生成的公开平行语料库。

Conclusion: 本文方案大幅提升了数据集的规模和质量，并提供了公开、可复现的技术路线，相关代码及语料开放获取，有助于推动机器翻译等多语种NLP任务的发展。

Abstract: The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

</details>


### [148] [RAVE: Retrieval and Scoring Aware Verifiable Claim Detection](https://arxiv.org/abs/2509.15793)
*Yufeng Li,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出了一种结合证据检索与结构化信号的可验证主张检测框架RAVE，在多个测试集上相较现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息迅速传播，需要高效且可扩展的事实核查工具。主张检测（claim detection）作为关键环节，目前方法在面对模糊或多样化文本时效果不佳。

Method: 提出RAVE框架，同时利用证据检索、相关性和来源可信度等结构化信号，实现对多元化主张的检测。

Result: 在CT22-test与PoliClaim-test两个数据集上，RAVE在准确率与F1分数上均优于仅用文本或传统检索为基础的主张检测方法。

Conclusion: 将证据相关性和来源可信度纳入主张检测流程，能显著提升检测虚假信息主张的性能。

Abstract: The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.

</details>


### [149] [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
*Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz*

Main category: cs.CL

TL;DR: 论文提出了一种跨语言奖励模型，显著提升了多语言大模型的数学推理能力，揭示了多语言推理方法的新机会。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型推理能力不断发展，作者关注多语言模型在不同语言下推理能力的差异，以及不同语言产生的推理路径是否能互补，以期提升整体推理表现。

Method: 作者训练了一个跨语言奖励模型（reward model），该模型能够对不同语言生成的回答进行排名，并与只在单语奖励上的建模方法进行了对比测试。

Result: 跨语言奖励模型在数学推理任务上的表现优于单语言奖励模型，对于高资源语言同样有效。跨语言采样在低采样预算下对英语表现有显著提升。

Conclusion: 多语言大模型的推理能力可以通过跨语言互补得到提升。用不同语言的互补性提供了改善多语言推理能力的新途径。

Abstract: While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

</details>


### [150] [The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders](https://arxiv.org/abs/2509.15837)
*Adrian Sauter,Willem Zuidema,Marianne de Heer Kloots*

Main category: cs.CL

TL;DR: 该论文探讨了视觉信息在音频和文本深度学习模型中的训练对语言处理的影响，发现视觉语义的引入使语音与文本的内部表征更加一致，但主要提升了单词身份信息，而非词义信息。


<details>
  <summary>Details</summary>
Motivation: 随着多模态深度学习的发展，许多语言模型会在训练中引入视觉信息（如图像、视频）以改进语言理解。然而，视觉信息究竟如何影响语音和文本模型中的单词表征，目前还没有得到充分研究。作者希望深入剖析多模态视觉语义训练对不同类型语言模型（语音vs文本）的影响机理。

Method: 作者对比了经视觉语义训练的语音编码器和文本编码器。首先通过整体表征对齐分析，考察了视觉信息对语音与文本模型单词内部表征一致性的影响。接着，利用聚类分析，分别测量模型表征中语音层面（音素）和语义层面的可区分性。

Result: 实验表明：视觉语义训练能提升语音和文本模型间单词身份表征的对齐度，但这种提升主要体现在识别单词身份上，而非对词义的更好表示。此外，语音模型即使融合了视觉信息，其表征依然以音素（声音）为主，视觉信息并未改善其语义区分能力，而文本模型则能从视觉信息增强语义表征。

Conclusion: 该研究揭示了当前视觉语义方法对于语音模型语义能力提升有限，主要强化了单词身份而非词义表示，为后续发展更高效的语音-视觉联合建模方法提供了重要参考。

Abstract: How does visual information included in training affect language processing
in audio- and text-based deep learning models? We explore how such visual
grounding affects model-internal representations of words, and find
substantially different effects in speech- vs. text-based language encoders.
Firstly, global representational comparisons reveal that visual grounding
increases alignment between representations of spoken and written language, but
this effect seems mainly driven by enhanced encoding of word identity rather
than meaning. We then apply targeted clustering analyses to probe for phonetic
vs. semantic discriminability in model representations. Speech-based
representations remain phonetically dominated with visual grounding, but in
contrast to text-based representations, visual grounding does not improve
semantic discriminability. Our findings could usefully inform the development
of more efficient methods to enrich speech-based models with visually-informed
semantics.

</details>


### [151] [Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems](https://arxiv.org/abs/2509.15839)
*Zhongze Luo,Zhenshuai Yin,Yongxin Guo,Zhichao Wang,Jionghao Zhu,Xiaoying Tang*

Main category: cs.CL

TL;DR: 本文针对多模态大语言模型（MLLMs）在物理等科学领域应用中的评测短板，提出并发布了一个用于中文物理推理的新基准数据集Multi-Physics，涵盖更细致学科分层、过程评测和视觉作用评估。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测常因覆盖面窄、仅重结果、偏重英语且视觉信息利用有限，难以反映模型在科学领域的真实能力。

Method: 作者设计了涵盖11门高中物理科目、5种难度共1,412道含图像的多项选择题，并建立了双评价体系：既看最终答案对错，也评价推理链条（chain-of-thought）的完整性，还系统对比了输入模式与难度因子对模型表现的影响。

Result: 用该基准评测了20种MLLM，分别分析了在不同视觉输入与难度下的推理准确率和链条完整性。

Conclusion: Multi-Physics为多模态物理推理提供了细粒度的中文资源和标准化评测方法，为深入理解和提升MLLMs的多模态推理能力提供了新工具。数据集和代码已开源。

Abstract: While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.

</details>


### [152] [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
*Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级、高效的大模型任务适应方法Steering Vector Decoding (SVD)，通过引导解码过程直接调整输出分布，实现无需额外参数的高效下游任务适配。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）虽然比全量微调节省开销，但对于拥有十亿参数的大模型，适配下游任务依然代价高昂。研究者希望寻找一种理论支撑强、不依赖大量参数、适配效率高的新方法。

Method: 作者将任务适应重新表述为输出分布对齐，并提出SVD方法。具体做法为：先进行短暂微调，获取warm-start模型，通过计算该模型与预训练模型输出分布的KL散度梯度，提取任务相关的引导向量（steering vector），在推理过程中用该向量引导大模型输出分布朝向任务分布。理论上证明SVD等价于微调的一阶梯度步骤，并推导出该向量作用强度的最优解。该方法兼容多种PEFT方法，无需增添其它可训练参数。

Result: 在3类任务、9个基准上，SVD配合4种标准PEFT方法，将多项选择准确率提升最高达5分、开放性问题真实度提升2分，并在常识测试集上也有1-2分提升，且无需增加除PEFT适配器以外的可训练参数。

Conclusion: SVD为大型语言模型的下游任务适应提供了轻量级、理论基础扎实的新选择，能在不引入额外训练参数的情况下，有效提升各类任务表现。

Abstract: Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.

</details>


### [153] [The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection](https://arxiv.org/abs/2509.15896)
*Arghodeep Nandi,Megha Sundriyal,Euna Mehnaz Khan,Jikai Sun,Emily Vraga,Jaideep Srivastava,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文综述了当前自动化事实核查（fact-checking）的不足，指出有效打击网络虚假信息不仅仅依赖于事实准确性，还需结合人类心理和行为特征如认知偏见、社会动态和情绪反应。作者提出从“以人为本”的视角改进检测方法，并给出了结合神经行为学等新型研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前的虚假信息检测主要关注事实真伪，但虚假信息的危害远超于此，还影响人类认知、情绪和社会行为。为了更好地应对虚假信息传播，需要开发能兼容多维人类因素的新检测体系。

Method: 文章通过梳理并分析现有主流虚假信息检测系统，在此基础上引入认知科学与社会心理学的理论，如认知偏见、社会动态和情绪反应等，并探讨将这些心理行为因素整合进检测体系的可行性及必要性。

Result: 作者总结了现有检测系统在应对人类复杂心理和行为层面上的不足，识别并分析了如何借助心理和行为理论来增强虚假信息检测。文中还提出了包括神经行为学模型在内的多项潜在改进方向。

Conclusion: 有效遏制信息时代的虚假信息传播，仅依赖事实核查远远不够。结合人类心理与行为特征的检测框架，可为未来打击虚假信息提供更强的科学依据和社会适应性。

Abstract: Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.

</details>


### [154] [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
*Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了一种新的会议摘要方法FRAME，将摘要任务重新定义为语义增强流程，通过提取和组织关键信息来生成更准确、个性化的摘要，并引入创新的评价体系提升结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在会议摘要时常见内容幻觉、遗漏和无关信息等问题，因此需要一种新的方法来提升摘要质量，增强对关键信息的提炼和输出可控性。

Method: 提出模块化管线FRAME，将会议内容转化为事实点并按主题组织，最后形成摘要；引入SCOPE协议，通过让模型回答九个关键问题，构建推理轨迹以实现个性化抽取及生成；设计P-MESA评价框架，能够在无参考标准下多维度评估摘要效果并与人工评价对齐。

Result: 在QMSum和FAME数据集上，FRAME方法将幻觉和遗漏降幅达到2/5分（基于MESA评分）；SCOPE协议在知识契合和目标一致性方面超过基准prompt方法；P-MESA指标在标注一致性上达到≥89%，与人工严重性评级相关性r≥0.70。

Conclusion: 通过重新定义摘要任务，FRAME方法大幅提升了内容可信度、控制性和个性化能力，为会议摘要等任务的高质量生成提供了新的解决方案。

Abstract: Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

</details>


### [155] [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
*Ahmed Karim,Qiao Wang,Zheng Yuan*

Main category: cs.CL

TL;DR: 本文提出在自动作文评分系统中结合conformal prediction，生成带置信度且具备覆盖保证的评分集合，并首次引入UAcc评估模型的准确性和简洁性。实验表明，经过调整的开源大模型在三个不同数据集上达标，预测集合小，表明中等规模开源大模型已能用于实际教师辅助作文评分。


<details>
  <summary>Details</summary>
Motivation: 虽然现有自动作文评分系统在公开基准上几乎与人类一致，但实际应用受限，主要因系统仅输出单一分数，缺乏可信度或解释，难以用于高风险场景。作者希望通过加入置信度和覆盖保证，提高模型的可应用性和可信度。

Method: 将conformal prediction作为‘外包装’应用于开源大语言模型（Llama-3 8B和Qwen-2.5 3B），在三类作文数据集（ASAP, TOEFL11, Cambridge-FCE）上微调，采用90%置信度校准，利用UAcc衡量模型在准确性和预测集合简洁性上的表现。

Result: 经过conformal prediction校准后，模型在覆盖率始终达到目标要求，并且预测集合较为紧凑。所有实验的模型均表现出色，展现了中等规模开源大模型在自动作文评分中的潜力。

Conclusion: 结合Conformal prediction和UAcc评估，可有效增强开源大模型作文评分的可靠性和实用性。现有模型已能支持教师深度参与的自动评分，后续将致力于更大规模扩展及用户研究。

Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

</details>


### [156] [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
*Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard*

Main category: cs.CL

TL;DR: 本文提出了一种新的离散时间注意力模型localmax dynamics，它介于传统softmax和hardmax之间，并通过引入对齐敏感性参数，实现对邻域交互的可控松弛。


<details>
  <summary>Details</summary>
Motivation: 现有的softmax和hardmax机制在建模注意力机制时有各自的局限性：softmax过于平滑，hardmax过于激进，难以灵活调整邻居影响和系统收敛行为。因此需要一种介于两者之间的可调机制以刻画更复杂的系统动力学。

Method: 提出了localmax dynamics模型，能在softmax和hardmax之间插值。通过引入对齐敏感性参数调节行为，分析该模型在不同参数设定下的动力学性质，例如收敛到多面体、引入quiescent sets描述顶点附近行为并研究其渐近性质，还使用Lyapunov方法探讨稳定性。

Result: 证明了token状态的凸包最终收敛到一个凸多面体，并需引入quiescent sets描述邻顶点不变行为。系统在有消失/非零/时变参数下都不会有限步收敛，且在某些极限下会恢复hardmax的行为。分析还展示Lyapunov方法在非对称交互下的局限。

Conclusion: localmax dynamics丰富了注意力机制的理论基础，能介于softmax和hardmax之间灵活调节行为，对高阶系统动力学分析有新启示，并揭示未来研究潜在方向。

Abstract: We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

</details>


### [157] [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
*Baichuan Huang,Ananth Balashankar,Amir Aminifar*

Main category: cs.CL

TL;DR: 本文提出了一种高效选取fine-tune权重中偏置项的新方法，并在多个语言模型和任务上验证了该方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调（PEFT）技术中的偏置项微调方法，显示出卓越的参数效率和易用性。但不同偏置项（如query、key、value等）微调效果与下游任务表现缺乏明确关系，现有选择标准也不理想，因此亟需更有效的偏置项选择机制。

Method: 作者提出了一种系统性的方法，用于选择最适合fine-tune的偏置项（称为BEFT，Bias-Efficient Fine-Tuning），并将其与基于偏置变化幅度或经验Fisher信息等现有方法广泛对比。

Result: 在多种主流大模型（110M至6.7B参数，包含Encoder-only和Decoder-only架构）及多样下游任务（分类、多选、文本生成）上，作者的方法表现优于其它偏置选择方法。

Conclusion: 针对偏置微调的选择问题，BEFT方法不仅提供更优性能，也更具推广性和实用价值，有助于推动参数高效微调技术的发展。

Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

</details>


### [158] [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
*Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种创新的多模态基础模型方法，实现了口语评估过程中针对完整对话会话的单次评估，有效提升了对L2英语学习者口语能力的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着第二语言（L2）英语使用者数量的增加，亟需高效、可靠的口语评估方法服务于计算机辅助语言学习（CALL）。现有方法往往存在级联误差传播或无法捕捉话语级信息等不足。

Method: 提出将多目标学习与冻结的Whisper ASR模型结合，作为语音先验，实现声学感知校准。模型对整个答题会话进行联合处理，无需手工特征提取，端到端学习总体及细节能力目标。

Result: 在Speak & Improve基准测试上，所提方法超越了之前最优的级联系统，展现了出色的跨部分泛化能力，并构建了体量紧凑、易于部署的评分器。

Conclusion: 该方法为CALL应用带来更高准确性和实用性的口语评分工具，证明了多模态基础模型在口语评估场景中的有效性，并为后续相关研究提供了方向。

Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.

</details>


### [159] [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
*Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim*

Main category: cs.CL

TL;DR: 本文提出了Think-Verbalize-Speak框架，将文本推理与口语生成解耦，提升语音对话系统在自然性和简洁性上的表现，同时保持LLM推理能力。新方法包括高效的ReVerT口语化模块，经实验验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLM)具备出色推理能力，但直接用于口语交流时效果不佳，主要因文本和口语表达在最佳表现上的差异。现有让LLM生成更适合口语的输出方法，可能影响推理效果，因此需要一种兼顾推理与口语自然性的框架。

Method: 作者提出Think-Verbalize-Speak框架，将推理过程与口语化输出解耦，并设计了“Verbalizing”中间步骤，将推理结果转化为自然且适合语音输出的文本。同时，提出了ReVerT口语化器，该模块基于增量式和异步摘要，兼顾低延迟和效率。

Result: 在多个基准测试集上的实验结果显示，该方法能显著提升语音输出的自然度和简练程度，同时对模型推理能力影响极小。

Conclusion: 论文表明，将推理与口语输出解耦的方法能更好地发挥LLM在语音对话系统中的潜力，在语音自然性和推理能力之间取得平衡。作者也开放了数据集和代码以推动领域发展。

Abstract: Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

</details>


### [160] [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
*Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 该论文提出了一种新的LLM长文本答案评估框架DeCE，能更精准且可解释地评估如法律等高风险领域的生成答案。


<details>
  <summary>Details</summary>
Motivation: 现有BLEU、ROUGE等指标无法评估语义正确性，常用LLM评估器又只给出单一分数，难以反映答案细致维度，特别是精度与召回方面。因此需要新的方法解决高风险领域长文本生成评估的细粒度和可解释性问题。

Method: 提出DeCE框架，将答案质量分解为精度（事实准确性和相关性）及召回（涵盖必要概念），依据gold标准答案自动抽取实例相关标准，无需手动设计rubric，实现模型无关和领域广泛适用。随后在真实的跨法域法律问答任务中对DeCE进行实例验证。

Result: DeCE与专家评判的相关系数达0.78，显著高于传统方法与其他现代评估器。结果揭示了模型的召回与精度取舍差异；且仅约12%自动抽取标准需专家修订，显示DeCE具良好扩展性。

Conclusion: DeCE框架为高风险专家领域的LLM答案评估提供了高效、可解释且可扩展的新范式。

Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

</details>


### [161] [DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning](https://arxiv.org/abs/2509.16105)
*Sikai Bai,Haoxi Li,Jie Zhang,Zicong Hong,Song Guo*

Main category: cs.CL

TL;DR: 本文提出了一种新的MoE（专家混合）模型裁剪方法，名为DiEP，可以针对每层专家冗余的不同进行非均匀裁剪，在保证性能的同时显著减少模型规模和存储需求。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型规模的不断增大，其内存和存储需求也急剧上升，成为实际应用的重大挑战。现有均匀裁剪方法忽略了不同层之间专家冗余的差异，导致性能下降。

Method: 作者提出了DiEP方法，通过差分化方式自适应地学习并调整每一层的裁剪率，同时捕捉层间的重要性差异。该方法将全局的离散搜索空间转化为连续空间，使得可以用梯度优化实现高效非均匀裁剪。

Result: 在五种主流MoE模型和多个NLP任务上，DiEP显示出卓越的性能，特别是在Mixtral 8×7B上，仅保留一半专家即可保持约92%的原始性能，且在MMLU等任务上领先其他裁剪方法最高达7.1%。

Conclusion: DiEP方法有效地解决了MoE模型裁剪中冗余利用效率不足和性能损失的问题，有助于推动大规模MoE模型在实际中的落地应用。

Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.

</details>


### [162] [It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge](https://arxiv.org/abs/2509.16107)
*Lukas Ellinger,Georg Groh*

Main category: cs.CL

TL;DR: 本文评估了多语言大型语言模型（LLM）在多轮对话中解决歧义和使用常识的能力，并分析其对简化语言请求下的表现。当前主流模型难以有效解决歧义，往往只给出一种解释或覆盖所有可能结果，缺乏澄清能力；简化语言时常识利用和多样性策略进一步下降。通过先进微调方法可显著提升其歧义处理能力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中语义歧义（如模糊词和未具体指代）常见且需依赖常识和语境解决。了解现有LLM是否能借助常识处理歧义、以及在语言简化情况下的表现，有助于推动模型实用性提升和研究优化方向。

Method: 作者构建了多语言评测数据集，系统测试了DeepSeek v3、GPT-4o、Qwen3-32B、GPT-4o-mini、Llama-3.1-8B等模型，并结合LLM自动判别与人工标注分析表现。同时，通过对Llama-3.1-8B利用直接偏好优化微调，评估微调对歧义解决的作用。

Result: 主流LLM普遍难以高效解决对话歧义，常表现为直接单一解读或罗列所有可能，缺少主动请求澄清等灵活应对；在简化语言请求下，模型常识推理及响应策略多样性显著下降。对Llama-3.1-8B的微调显著提升了各类请求下的歧义处理能力。

Conclusion: 先进的模型可能在歧义处理和常识利用方面仍有较大提升空间，特别是在面对语言简化等实际场景时。需通过更优微调策略和评估机制，增强模型在多样沟通风格下的健壮性和灵活性。

Abstract: Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.

</details>


### [163] [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
*Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li*

Main category: cs.CL

TL;DR: 该论文提出了CodeRAG框架，通过改进检索与生成机制，提升了仓库级别的代码补全性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码补全方法依赖于大语言模型和检索机制，但存在检索查询构造不当、单路径检索及代码检索器与生成模型不匹配等问题，影响补全效果。

Method: 提出CodeRAG框架，包含三个核心组件：基于对数概率引导的查询构建、多路径代码检索，以及基于偏好重排序（BestFit reranking）提升检索结果质量。

Result: 在ReccEval和CCEval两个基准集上进行的大量实验表明，CodeRAG在各种评测指标上均显著优于目前的最新方法。

Conclusion: CodeRAG框架有效解决了仓库级代码补全中的主要难点，显著提升了代码补全性能，为该领域提供了可复现、效果优越的新方法。

Abstract: Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

</details>


### [164] [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
*Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma*

Main category: cs.CL

TL;DR: 本文提出了CultureScope框架，用于评估大语言模型（LLMs）的文化理解能力。该框架基于文化冰山理论，构建了多维度的文化知识分类标准，实现了包含3层、140个维度的文化知识本体，并由此自动生成不同语言和文化的文化知识库和评价数据集。实验表明，当前LLMs的文化理解能力不足，仅增加多语种数据无法有效提升其文化适应性。


<details>
  <summary>Details</summary>
Motivation: 现有评测大语言模型文化理解能力的基准存在适应性差、可扩展性差、缺少系统理论指导等问题，难以覆盖不同的文化背景并大规模自动化构建。为保证LLMs应用时的可信度与文化一致性，亟需科学且全面的评估方法。

Method: 受文化冰山理论启发，作者设计了包含3层、140维度的文化知识分类架构。据此自动化构建不同语言和文化背景下的知识库和评测数据集，并系统评估LLMs在多文化场景下的表现。

Result: 实验结果表明，该框架能够有效评估LLMs的文化理解能力。测试发现，现有主流LLMs的文化理解能力有限，且仅靠多语种数据的加入并不能明显提升文化适应能力。

Conclusion: CultureScope为LLMs的文化能力评测提供了科学详尽的方法工具，有助于推动可信赖且文化适应性强的AI系统发展。当前LLMs在文化理解上还有较大提升空间。

Abstract: As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture

</details>


### [165] [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
*Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的Repository Planning Graph（RPG）表示方法，显著提升了大模型生成完整代码仓库的能力，并开发了ZeroRepo框架进行验证，在实际数据集上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在函数级与文件级代码生成上表现优异，但从零生成完整代码仓库仍然极具挑战，原因在于需要跨层次、长距离的规划管理，而自然语言描述过于模糊且不精确，难以承载复杂的软件蓝图。

Method: 作者提出Repository Planning Graph（RPG），以图结构编码功能、文件结构、数据流等多种语义层级信息，替代自然语言进行规划，并据此构建三阶段ZeroRepo框架：包括方案规划、实现细化与图引导生成，辅以测试验证。并搭建了RepoCraft基准，包含六个真实项目和1,052个任务。

Result: 在RepoCraft基准上，ZeroRepo生成的仓库平均近36K行代码，是最佳对比（Claude Code）的3.9倍，是其他方法的64倍；功能覆盖率81.5%，测试通过率69.7%，均大幅领先于Claude Code（分别高出27.3和35.8个百分点）。

Conclusion: RPG显著提升了大模型跨阶段规划、依赖建模与理解代码仓库的能力；ZeroRepo实现了大规模、高质量的自动化仓库生成，对加速AI助手参与软件开发具有重要意义。

Abstract: Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [166] [DIPP: Discriminative Impact Point Predictor for Catching Diverse In-Flight Objects](https://arxiv.org/abs/2509.15254)
*Ngoc Huy Nguyen,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本研究提出一种使用四足机器人和篮筐捕捉飞行物体的方法，并建立了相关数据集与预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏多样化、包含复杂空气动力学飞行轨迹的数据集，且早期对飞行轨迹的撞击点预测难度大，限制了机器人对飞行物体的高效捕捉能力。

Method: 作者构建了包含20种物体、8,000条飞行轨迹的真实世界数据集，并提出了判别撞击点预测器（DIPP），包括判别特征嵌入（DFE）和撞击点预测器（IPP）。IPP有两种变体：一种通过神经网络加速度估计预测轨迹并推算撞击点，另一种直接输出撞击点。

Result: 实验发现本数据集比现有公开数据集更加多样和复杂；提出的方法在15种已见物体和5种新物体上的预测精度均优于现有方法，并且提升了早期阶段的预测，有效提高了仿真和真实环境下的成功抓捕率。

Conclusion: 本方法能更准确高效地预测飞行物体的着陆点，提升了四足机器人捕捉飞行物体的能力，对复杂空气动力学环境下的应用具有实际意义。

Abstract: In this study, we address the problem of in-flight object catching using a
quadruped robot with a basket. Our objective is to accurately predict the
impact point, defined as the object's landing position. This task poses two key
challenges: the absence of public datasets capturing diverse objects under
unsteady aerodynamics, which are essential for training reliable predictors;
and the difficulty of accurate early-stage impact point prediction when
trajectories appear similar across objects. To overcome these issues, we
construct a real-world dataset of 8,000 trajectories from 20 objects, providing
a foundation for advancing in-flight object catching under complex
aerodynamics. We then propose the Discriminative Impact Point Predictor (DIPP),
consisting of two modules: (i) a Discriminative Feature Embedding (DFE) that
separates trajectories by dynamics to enable early-stage discrimination and
generalization, and (ii) an Impact Point Predictor (IPP) that estimates the
impact point from these features. Two IPP variants are implemented: an Neural
Acceleration Estimator (NAE)-based method that predicts trajectories and
derives the impact point, and a Direct Point Estimator (DPE)-based method that
directly outputs it. Experimental results show that our dataset is more diverse
and complex than existing dataset, and that our method outperforms baselines on
both 15 seen and 5 unseen objects. Furthermore, we show that improved
early-stage prediction enhances catching success in simulation and demonstrate
the effectiveness of our approach through real-world experiments. The
demonstration is available at
https://sites.google.com/view/robot-catching-2025.

</details>


### [167] [GiAnt: A Bio-Inspired Hexapod for Adaptive Terrain Navigation and Object Detection](https://arxiv.org/abs/2509.15264)
*Aasfee Mosharraf Bhuiyan,Md Luban Mehda,Md. Thawhid Hasan Puspo,Jubayer Amin Pritom*

Main category: cs.RO

TL;DR: 本文介绍了一种受蚂蚁运动启发的新型六足机器人GiAnt，具备良好的地形适应性和成本优势。


<details>
  <summary>Details</summary>
Motivation: 传统轮式机器人在崎岖地形上的适应性有限，现有六足机器人成本和结构复杂。作者受蚂蚁出色地形适应能力启发，旨在设计更经济、适应性更强的机器人。

Method: 采用3D打印和激光切割制造轻质躯体，腿部用连杆-曲柄机构实现单自由度运动。控制系统基于Arduino，结合步态分析优化运动，并集成机器学习与图像处理，实现对象识别。

Result: GiAnt重量1.75kg，能轻松跨越8cm障碍，适应草地、岩石等复杂地形。能识别81种物体，并具备简便的手动操作。

Conclusion: GiAnt证明仿生六足机器人可在结构简单、低成本下，具备优异的复杂地形适应能力和一定的智能识别能力，适合用于科研、探测和巡检。

Abstract: This paper presents the design, development and testing of GiAnt, an
affordable hexapod which is inspired by the efficient motions of ants. The
decision to model GiAnt after ants rather than other insects is rooted in ants'
natural adaptability to a variety of terrains. This bio-inspired approach gives
it a significant advantage in outdoor applications, offering terrain
flexibility along with efficient energy use. It features a lightweight
3D-printed and laser cut structure weighing 1.75 kg with dimensions of 310 mm x
200 mm x 120 mm. Its legs have been designed with a simple Single Degree of
Freedom (DOF) using a link and crank mechanism. It is great for conquering
challenging terrains such as grass, rocks, and steep surfaces. Unlike
traditional robots using four wheels for motion, its legged design gives
superior adaptability to uneven and rough surfaces. GiAnt's control system is
built on Arduino, allowing manual operation. An effective way of controlling
the legs of GiAnt was achieved by gait analysis. It can move up to 8 cm of
height easily with its advanced leg positioning system. Furthermore, equipped
with machine learning and image processing technology, it can identify 81
different objects in a live monitoring system. It represents a significant step
towards creating accessible hexapod robots for research, exploration, and
surveying, offering unique advantages in adaptability and control simplicity.

</details>


### [168] [Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI](https://arxiv.org/abs/2509.15273)
*Fei Ni,Min Zhang,Pengyi Li,Yifu Yuan,Lingfeng Zhang,Yuecheng Liu,Peilong Han,Longxin Kou,Shaojin Ma,Jinbin Qiao,David Gamaliel Arcos Bravo,Yuening Wang,Xiao Hu,Zhanguang Zhang,Xianze Yao,Yutong Li,Zhao Zhang,Ying Wen,Ying-Cong Chen,Xiaodan Liang,Liang Lin,Bin He,Haitham Bou-Ammar,He Wang,Huazhe Xu,Jiankang Deng,Shan Luo,Shuqiang Jiang,Wei Pan,Yang Gao,Stefanos Zafeiriou,Jan Peters,Yuzheng Zhuang,Yingxue Zhang,Yan Zheng,Hongyao Tang,Jianye Hao*

Main category: cs.RO

TL;DR: 本文提出了一个名为Embodied Arena的评测平台，旨在系统化评估和推动Embodied AI的发展，通过统一的能力分类、标准化评测体系、自动化数据生成方法和公开排行榜，解决当前领域发展的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前Embodied AI的发展受限于三大挑战：缺乏对核心能力的系统性理解、缺少统一与标准化的评测体系，以及缺乏可扩展的自动化数据采集方法。这些问题使得领域进步缓慢，研究目标不明，无法进行有效的横向对比与大规模模型训练。

Method: 作者提出Embodied Arena评测平台，构建了包括感知、推理、任务执行三大层级的系统化能力分类，以及25个细分维度。该平台能统一纳管22个不同评测基准和30余种先进模型，支持2D/3D Embodied Q&A、导航、任务规划三大领域，并采用大模型驱动的数据自动生成流水线，实现多样且可持续进化的评测数据，同时在线发布三大榜单，综合展示模型能力。

Result: 通过该平台，作者在Embodied Arena排行榜上得出了九项关键发现，展示了各大模型在不同能力维度及任务上的表现与差距，为领域研究提供了清晰方向和问题定位。

Conclusion: Embodied Arena的提出为Embodied AI研究带来了系统化、标准化和可扩展的评测手段，有助于统一研究目标、克服发展瓶颈，加速领域进步。

Abstract: Embodied AI development significantly lags behind large foundation models due
to three critical challenges: (1) lack of systematic understanding of core
capabilities needed for Embodied AI, making research lack clear objectives; (2)
absence of unified and standardized evaluation systems, rendering
cross-benchmark evaluation infeasible; and (3) underdeveloped automated and
scalable acquisition methods for embodied data, creating critical bottlenecks
for model scaling. To address these obstacles, we present Embodied Arena, a
comprehensive, unified, and evolving evaluation platform for Embodied AI. Our
platform establishes a systematic embodied capability taxonomy spanning three
levels (perception, reasoning, task execution), seven core capabilities, and 25
fine-grained dimensions, enabling unified evaluation with systematic research
objectives. We introduce a standardized evaluation system built upon unified
infrastructure supporting flexible integration of 22 diverse benchmarks across
three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced
models from 20+ worldwide institutes. Additionally, we develop a novel
LLM-driven automated generation pipeline ensuring scalable embodied evaluation
data with continuous evolution for diversity and comprehensiveness. Embodied
Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task
Planning) with dual perspectives (benchmark view and capability view),
providing comprehensive overviews of advanced model capabilities. Especially,
we present nine findings summarized from the evaluation results on the
leaderboards of Embodied Arena. This helps to establish clear research veins
and pinpoint critical research problems, thereby driving forward progress in
the field of Embodied AI.

</details>


### [169] [Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound](https://arxiv.org/abs/2509.15325)
*Ryan S. Yeung,David G. Black,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: 本文提出一种改进远程超声成像中的力反馈模型，结合了点云和测量力的数据，显著提升了力反馈的准确性。


<details>
  <summary>Details</summary>
Motivation: 远程超声检查有助于偏远地区的诊断，但由于通信延迟，传统的力反馈方法无法有效实施，需要更高效准确的力估计方法。

Method: 作者首先用点云生成病人体表模型，并传输给超声医生，然后将其体素化并为每个体素分配势场值，用来估计超声探头与病人接触时的力与力矩。该势场通过结合空间拉普拉斯算子和实际测量到的力，用凸二次项优化计算得到。

Result: 在3名志愿病人上实验，结果显示引入实际测量力后，力幅值误差相比只用拉普拉斯方程降低了平均7.23N，力矢量夹角误差降低了9.37度。

Conclusion: 将测量力与内势场模型结合可大幅提高远程超声中的力反馈精度，对提升远程操作体验和成像质量有重要意义。

Abstract: Teleoperated ultrasound can improve diagnostic medical imaging access for
remote communities. Having accurate force feedback is important for enabling
sonographers to apply the appropriate probe contact force to optimize
ultrasound image quality. However, large time delays in communication make
direct force feedback impractical. Prior work investigated using point
cloud-based model-mediated teleoperation and internal potential field models to
estimate contact forces and torques. We expand on this by introducing a method
to update the internal potential field model of the patient with measured
positions and forces for more transparent model-mediated tele-ultrasound. We
first generate a point cloud model of the patient's surface and transmit this
to the sonographer in a compact data structure. This is converted to a static
voxelized volume where each voxel contains a potential field value. These
values determine the forces and torques, which are rendered based on overlap
between the voxelized volume and a point shell model of the ultrasound
transducer. We solve for the potential field using a convex quadratic that
combines the spatial Laplace operator with measured forces. This was evaluated
on volunteer patients ($n=3$) by computing the accuracy of rendered forces.
Results showed the addition of measured forces to the model reduced the force
magnitude error by an average of 7.23 N and force vector angle error by an
average of 9.37$^{\circ}$ compared to using only Laplace's equation.

</details>


### [170] [Trust-Aware Embodied Bayesian Persuasion for Mixed-Autonomy](https://arxiv.org/abs/2509.15404)
*Shaoting Peng,Katherine Driggs-Campbell,Roy Dong*

Main category: cs.RO

TL;DR: 本文提出了信任感知的贝叶斯说服（TA-EBP）框架，以提升自动驾驶车辆（AVs）与人驾车辆（HVs）在交通交互中的安全性与效率。通过引入信任参数和具体化说服信号，验证了相比传统方法，该框架能更好地影响人类驾驶员行为，提升交通安全和流畅度。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型在模拟自动驾驶和人驾车辆互动时，可能由于影响力衰减或被认为具有操控性，反而引发人类驾驶员的不信任乃至更危险行为。因此，亟需一种更透明、能维持信任且实际可行的交流和影响方法。

Method: 作者提出TA-EBP框架，利用贝叶斯说服理论对交通路口AC与HV的交流建模：1）将贝叶斯说服用于交通交互，提供对博弈论模型的透明替代方案；2）引入信任参数，推导影响对方所需的最低信任水平定理；3）将理论中抽象信号映射为具体的车辆推进动作，并给出最佳动作定理。

Result: 通过混合自治交通仿真实验，TA-EBP能够有效促使人驾车辆更加谨慎驾驶，消除碰撞事故并提升交通流畅性，优于忽略信任或缺乏交流的基线方法。

Conclusion: TA-EBP为自动驾驶车辆提供了一种透明、非操控性的影响方法，有助于提升在现实交通环境中与人类驾驶员的安全与效率互动。

Abstract: Safe and efficient interaction between autonomous vehicles (AVs) and
human-driven vehicles (HVs) is a critical challenge for future transportation
systems. While game-theoretic models capture how AVs influence HVs, they often
suffer from a long-term decay of influence and can be perceived as
manipulative, eroding the human's trust. This can paradoxically lead to riskier
human driving behavior over repeated interactions. In this paper, we address
this challenge by proposing the Trust-Aware Embodied Bayesian Persuasion
(TA-EBP) framework. Our work makes three key contributions: First, we apply
Bayesian persuasion to model communication at traffic intersections, offering a
transparent alternative to traditional game-theoretic models. Second, we
introduce a trust parameter to the persuasion framework, deriving a theorem for
the minimum trust level required for influence. Finally, we ground the abstract
signals of Bayesian persuasion theory into a continuous, physically meaningful
action space, deriving a second theorem for the optimal signal magnitude,
realized as an AV's forward nudge. Additionally, we validate our framework in a
mixed-autonomy traffic simulation, demonstrating that TA-EBP successfully
persuades HVs to drive more cautiously, eliminating collisions and improving
traffic flow compared to baselines that either ignore trust or lack
communication. Our work provides a transparent and non-strategic framework for
influence in human-robot interaction, enhancing both safety and efficiency.

</details>


### [171] [Sym2Real: Symbolic Dynamics with Residual Learning for Data-Efficient Adaptive Control](https://arxiv.org/abs/2509.15412)
*Easop Lee,Samuel A. Moore,Boyuan Chen*

Main category: cs.RO

TL;DR: 提出Sym2Real框架，通过约10条轨迹即可实现真实世界下四旋翼和赛车机器人的鲁棒控制，无需专家知识和高仿真调整。


<details>
  <summary>Details</summary>
Motivation: 当前低层自适应控制依赖大量数据、专家知识或高精仿真调参，限制其广泛应用。如何用极少数据实现真实场景下的鲁棒控制是瓶颈。

Method: 提出数据驱动方法Sym2Real，引入符号回归与低保真仿真数据、真实世界残差学习结合解决模型噪声、退化等问题。无需专家经验，通过原理性地设计数据高效的控制器。

Result: 在四旋翼和赛车平台实测，Sym2Real可跨六个出分布仿真场景快速适应，并在五种真实世界条件下实现sim2real迁移，均表现出色。

Conclusion: Sym2Real显著提升了机器人自适应控制器的数据效率，可用极少真实轨迹实现高性能控制，为低数据量下复杂机器人系统的落地提供新思路。

Abstract: We present Sym2Real, a fully data-driven framework that provides a principled
way to train low-level adaptive controllers in a highly data-efficient manner.
Using only about 10 trajectories, we achieve robust control of both a quadrotor
and a racecar in the real world, without expert knowledge or simulation tuning.
Our approach achieves this data efficiency by bringing symbolic regression to
real-world robotics while addressing key challenges that prevent its direct
application, including noise sensitivity and model degradation that lead to
unsafe control. Our key observation is that the underlying physics is often
shared for a system regardless of internal or external changes. Hence, we
strategically combine low-fidelity simulation data with targeted real-world
residual learning. Through experimental validation on quadrotor and racecar
platforms, we demonstrate consistent data-efficient adaptation across six
out-of-distribution sim2sim scenarios and successful sim2real transfer across
five real-world conditions. More information and videos can be found at at
http://generalroboticslab.com/Sym2Real

</details>


### [172] [Online Slip Detection and Friction Coefficient Estimation for Autonomous Racing](https://arxiv.org/abs/2509.15423)
*Christopher Oeltjen,Carson Sobolewski,Saleh Faghfoorian,Lorant Domokos,Giancarlo Vidal,Ivan Ruchkin*

Main category: cs.RO

TL;DR: 该论文提出了一种基于IMU和LiDAR的轻量化在线轮胎-路面摩擦系数(TRFC)估计方法，不依赖复杂轮胎模型或大量训练数据，能准确检测打滑及摩擦系数，且实时性能好。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶与自动赛车中，车辆需经常在摩擦极限下运行，然而TRFC既对安全至关重要，又无法直接用常规传感器测量；现有估计方法要么依赖参数不确定的模型要么依赖大量数据，应用受限。

Method: 该方法仅用IMU、LiDAR和控制命令，无需专项动力/轮胎模型、参数识别或训练数据；通过比较命令与实际运动检测打滑，在无打滑时直接由加速度观测估算TRFC。

Result: 基于不同摩擦的1:10自动赛车实验，结果显示该方法能精确且一致地检测打滑和估算摩擦系数，结果与真实值高度吻合。

Conclusion: 该方法简单、易部署、计算高效，适用于自动驾驶中实时打滑监测和摩擦系数估算。

Abstract: Accurate knowledge of the tire-road friction coefficient (TRFC) is essential
for vehicle safety, stability, and performance, especially in autonomous
racing, where vehicles often operate at the friction limit. However, TRFC
cannot be directly measured with standard sensors, and existing estimation
methods either depend on vehicle or tire models with uncertain parameters or
require large training datasets. In this paper, we present a lightweight
approach for online slip detection and TRFC estimation. Our approach relies
solely on IMU and LiDAR measurements and the control actions, without special
dynamical or tire models, parameter identification, or training data. Slip
events are detected in real time by comparing commanded and measured motions,
and the TRFC is then estimated directly from observed accelerations under
no-slip conditions. Experiments with a 1:10-scale autonomous racing car across
different friction levels demonstrate that the proposed approach achieves
accurate and consistent slip detections and friction coefficients, with results
closely matching ground-truth measurements. These findings highlight the
potential of our simple, deployable, and computationally efficient approach for
real-time slip monitoring and friction coefficient estimation in autonomous
driving.

</details>


### [173] [Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning](https://arxiv.org/abs/2509.15443)
*Xingyu Chen,Hanyu Wu,Sikai Wu,Mingliang Zhou,Diyun Xiang,Haodong Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种高效可扩展的人类动作到仿人机器人动作迁移方法（IKMR），实现了大规模、实时、物理可行的动作重定向。


<details>
  <summary>Details</summary>
Motivation: 现有的人类动作到机器人动作迁移方法多为逐帧重定向，缺乏可扩展性，因此难以将大规模人类动作转化为机器人可执行的动作。亟需一种既高效又考虑动力学约束的整体性方法。

Method: 提出了IKMR框架，综合了运动学和动力学。运动学方面，利用预训练运动拓扑特征和双编码-解码结构学习动作域映射；动力学方面，将模仿学习与重定向网络结合，提升轨迹的物理可行性。最终通过跟踪数据微调，实现实际机器人控制器的直接训练和部署。

Result: 在仿真和实体全尺寸仿人机器人上进行了大量实验。结果显示，该方法在大规模动作重定向的效率、物理可行性及实际部署效果方面表现优异。

Conclusion: IKMR框架能够实时地将大规模人类动作映射为仿人机器人可执行的轨迹，有效提升了动作迁移的性能和实用性。

Abstract: Human-to-humanoid imitation learning aims to learn a humanoid whole-body
controller from human motion. Motion retargeting is a crucial step in enabling
robots to acquire reference trajectories when exploring locomotion skills.
However, current methods focus on motion retargeting frame by frame, which
lacks scalability. Could we directly convert large-scale human motion into
robot-executable motion through a more efficient approach? To address this
issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel
efficient and scalable retargeting framework that considers both kinematics and
dynamics. In kinematics, IKMR pretrains motion topology feature representation
and a dual encoder-decoder architecture to learn a motion domain mapping. In
dynamics, IKMR integrates imitation learning with the motion retargeting
network to refine motion into physically feasible trajectories. After
fine-tuning using the tracking results, IKMR can achieve large-scale physically
feasible motion retargeting in real time, and a whole-body controller could be
directly trained and deployed for tracking its retargeted trajectories. We
conduct our experiments both in the simulator and the real robot on a full-size
humanoid robot. Extensive experiments and evaluation results verify the
effectiveness of our proposed framework.

</details>


### [174] [Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems](https://arxiv.org/abs/2509.15491)
*Reza Pirayeshshirazinezhad,Nima Fathi*

Main category: cs.RO

TL;DR: 本文提出了一种结合解释性AI的多智能体机器人监督控制框架，实现了安全可审计的模式切换、鲁棒的连续控制以及透明的性能预测，并在航天器编队和自主水下航行器(AUV)两个领域进行了验证，表现出良好的可移植性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体机器人系统在复杂环境下运行时，面临安全、鲁棒性、性能权衡等多种挑战。传统控制方法难以兼顾这些需求，且缺乏透明解释能力，限制了其在安全关键和资源受限场景的应用。作者希望通过引入解释性AI，提升系统的安全性、可控性与可解释性。

Method: 提出了一个三部分集成架构：(1) 基于时序自动机的监督器负责安全的模式切换；(2) 连续控制采用李雅普诺夫法的大角度机动和带边界层的滑模控制(SMC)以抵抗干扰和提高精度；(3) 解释性预测器根据任务上下文映射控制增益和预期性能。训练基于蒙特卡洛优化，能够支持实时透明的性能权衡。

Result: 在航天器编队和AUV领导-跟随两个领域进行实验证明：在航天器任务中，SMC控制器相较于PD控制可降低21.7%的跟踪误差及81.4%的能耗；AUV测试中SMC控制依然能在随机扰动下保持误差有界，体现了该方法在不同平台上的通用性和鲁棒性。

Conclusion: 该框架不仅具备良好的跨平台移植能力，还因为其可解释的控制与性能预测特性，非常适合应用于安全关键且资源受限的多机器人系统，在确保安全与性能的基础上，提升了决策的可透明性和可信赖性。

Abstract: We present an explainable AI-enhanced supervisory control framework for
multi-agent robotics that combines (i) a timed-automata supervisor for safe,
auditable mode switching, (ii) robust continuous control (Lyapunov-based
controller for large-angle maneuver; sliding-mode controller (SMC) with
boundary layers for precision and disturbance rejection), and (iii) an
explainable predictor that maps mission context to gains and expected
performance (energy, error). Monte Carlo-driven optimization provides the
training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation
flying and autonomous underwater vehicles (AUVs). Despite different
environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share
uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,
and tight tracking needs, making them representative of general robotic
systems. In the space mission, the supervisory logic selects parameters that
meet mission criteria. In AUV leader-follower tests, the same SMC structure
maintains a fixed offset under stochastic currents with bounded steady error.
In spacecraft validation, the SMC controller achieved submillimeter alignment
with 21.7% lower tracking error and 81.4% lower energy consumption compared to
Proportional-Derivative PD controller baselines. At the same time, in AUV
tests, SMC maintained bounded errors under stochastic currents. These results
highlight both the portability and the interpretability of the approach for
safety-critical, resource-constrained multi-agent robotics.

</details>


### [175] [STARC: See-Through-Wall Augmented Reality Framework for Human-Robot Collaboration in Emergency Response](https://arxiv.org/abs/2509.15507)
*Shenghai Yuan,Weixiang Guo,Tianxin Hu,Yu Yang,Jinyu Chen,Rui Qian,Zhongyuan Liu,Lihua Xie*

Main category: cs.RO

TL;DR: 论文提出了一种基于增强现实（AR）的人机协作系统STARC，通过融合地面机器人与救援人员装备的LiDAR传感器，实现室内环境下遮挡情况下的高效感知与可视化，提升救援效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 在紧急响应任务中，救援人员常需进入遮挡、视野受限的室内环境，无法直接识别被遮挡的受困人员或危险源，存在极大风险和挑战。因此亟需一种提升复杂环境下态势感知能力的技术方案。

Method: 该系统融合了地面机器人上的LiDAR惯性里程计（实现大范围探索及3D人体检测）与救援人员头盔或手持装置上的LiDAR数据，通过相对位姿估计将二者数据精准配准，从而在救援人员视野中以低延迟AR方式实时叠加隐藏目标与环境点云信息。

Result: 在仿真、实验室和实地战术测试中，该系统实现了稳健的位姿配准、可靠的人体检测及AR信息叠加，证实了其在实际复杂环境中的可用性和鲁棒性。

Conclusion: STARC系统显著提升了救援人员在安全关键场景下的态势感知与操作安全性，具有良好推广前景。系统代码与设计在论文被接收后将开源。

Abstract: In emergency response missions, first responders must navigate cluttered
indoor environments where occlusions block direct line-of-sight, concealing
both life-threatening hazards and victims in need of rescue. We present STARC,
a see-through AR framework for human-robot collaboration that fuses
mobile-robot mapping with responder-mounted LiDAR sensing. A ground robot
running LiDAR-inertial odometry performs large-area exploration and 3D human
detection, while helmet- or handheld-mounted LiDAR on the responder is
registered to the robot's global map via relative pose estimation. This
cross-LiDAR alignment enables consistent first-person projection of detected
humans and their point clouds - rendered in AR with low latency - into the
responder's view. By providing real-time visualization of hidden occupants and
hazards, STARC enhances situational awareness and reduces operator risk.
Experiments in simulation, lab setups, and tactical field trials confirm robust
pose alignment, reliable detections, and stable overlays, underscoring the
potential of our system for fire-fighting, disaster relief, and other
safety-critical operations. Code and design will be open-sourced upon
acceptance.

</details>


### [176] [Distribution Estimation for Global Data Association via Approximate Bayesian Inference](https://arxiv.org/abs/2509.15565)
*Yixuan Jia,Mason B. Peterson,Qingyuan Li,Yulun Tian,Jonathan P. How*

Main category: cs.RO

TL;DR: 本论文提出了一种新的数据关联框架，采用近似贝叶斯推断方法，有效应对环境中重复或对称信息导致的多解难题，提升机器人在同一或不同时间、不同机器人间操作时的数据匹配准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数据关联方法在面对高度歧义的场景时，因依赖最大似然估计或最大一致性，仅输出单一解答，容易因解空间多峰而失败。为克服这一缺陷，研究者迫切需要能捕捉多种可能解法的全局数据关联策略。

Method: 提出以粒子形式表示潜在解，每个粒子通过确定性或随机的更新规则演化，覆盖解空间多个模式。这一贝叶斯推断框架能结合数据关联中的优化约束，并利用GPU并行优化提升效率。

Result: 在大量模拟与实际数据集实验中，本方法在点云与物体地图配准任务中，对高度歧义的数据场景成功估算了转化分布，表现优于传统单解法。

Conclusion: 该方法能全面刻画全局数据关联问题下的解空间分布，有效避免在不确定情境下过早做出单一决策，为机器人感知和多主体协作中的数据关联任务提供了更稳健的解决方案。

Abstract: Global data association is an essential prerequisite for robot operation in
environments seen at different times or by different robots. Repetitive or
symmetric data creates significant challenges for existing methods, which
typically rely on maximum likelihood estimation or maximum consensus to produce
a single set of associations. However, in ambiguous scenarios, the distribution
of solutions to global data association problems is often highly multimodal,
and such single-solution approaches frequently fail. In this work, we introduce
a data association framework that leverages approximate Bayesian inference to
capture multiple solution modes to the data association problem, thereby
avoiding premature commitment to a single solution under ambiguity. Our
approach represents hypothetical solutions as particles that evolve according
to a deterministic or randomized update rule to cover the modes of the
underlying solution distribution. Furthermore, we show that our method can
incorporate optimization constraints imposed by the data association
formulation and directly benefit from GPU-parallelized optimization. Extensive
simulated and real-world experiments with highly ambiguous data show that our
method correctly estimates the distribution over transformations when
registering point clouds or object maps.

</details>


### [177] [Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios](https://arxiv.org/abs/2509.15582)
*Yuting Zeng,Zhiwen Zheng,You Zhou,JiaLing Xiao,Yongbin Yu,Manping Fan,Bo Gong,Liyong Ren*

Main category: cs.RO

TL;DR: 本文提出了一种动量约束的混合启发式轨迹优化框架（MHHTOF），结合启发式采样、优化和具有残差增强的深度强化学习，用于视障辅助导航，实验证明新方法在收敛速度、稳定性和安全性上优于基线。


<details>
  <summary>Details</summary>
Motivation: 视障人士在导航时需要更智能、更安全和高效的路径规划算法。传统方法在处理复杂动态环境时表现有限，难以兼顾轨迹的可行性、平滑性与实时性，也难以持续优化用户体验。本研究旨在填补这一空白，提升助残导航的智能化水平。

Method: 框架包含两阶段：第一阶段在Frenet坐标系中通过三次插值和五次多项式结合动量约束，生成启发式轨迹采样，并初步优化与评估；第二阶段在笛卡尔坐标系采用基于LSTM的残差增强深度强化学习网络，进一步细化轨迹决策，采用双阶段损耗建模并实现权重转移以优化人本需求。

Result: 所提出的LSTM-ResB-PPO方法相比PPO基线，训练收敛速度提升一倍，模型平均成本和成本方差分别降低30.3%和53.3%，自身体和避障风险降低77%以上。

Conclusion: 该框架有效提升了复杂环境中的轨迹规划任务的鲁棒性、安全性和实时性，是助残导航应用的可行且效果显著的方法。

Abstract: This paper proposes a momentum-constrained hybrid heuristic trajectory
optimization framework (MHHTOF) tailored for assistive navigation in visually
impaired scenarios, integrating trajectory sampling generation, optimization
and evaluation with residual-enhanced deep reinforcement learning (DRL). In the
first stage, heuristic trajectory sampling cluster (HTSC) is generated in the
Frenet coordinate system using third-order interpolation with fifth-order
polynomials and momentum-constrained trajectory optimization (MTO) constraints
to ensure smoothness and feasibility. After first stage cost evaluation, the
second stage leverages a residual-enhanced actor-critic network with LSTM-based
temporal feature modeling to adaptively refine trajectory selection in the
Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with
weight transfer aligns semantic priorities across stages, supporting
human-centered optimization. Experimental results demonstrate that the proposed
LSTM-ResB-PPO achieves significantly faster convergence, attaining stable
policy performance in approximately half the training iterations required by
the PPO baseline, while simultaneously enhancing both reward outcomes and
training stability. Compared to baseline method, the selected model reduces
average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle
risks by over 77%. These findings validate the framework's effectiveness in
enhancing robustness, safety, and real-time feasibility in complex assistive
planning tasks.

</details>


### [178] [Bench-RNR: Dataset for Benchmarking Repetitive and Non-repetitive Scanning LiDAR for Infrastructure-based Vehicle Localization](https://arxiv.org/abs/2509.15583)
*Runxin Zhao,Chunxiang Wang,Hanyang Zhuang,Ming Yang*

Main category: cs.RO

TL;DR: 本文提出了一个用于道路基础设施车辆定位的数据集，涵盖重复扫描和非重复扫描两种LiDAR，旨在比较和评估不同扫描方式下的定位表现，并为后续相关研究提供基线和资源。


<details>
  <summary>Details</summary>
Motivation: 当前大部分基于路侧LiDAR（激光雷达）的车辆定位研究集中在重复扫描LiDAR上，而非重复扫描在消除盲区和成本方面更具优势，但其在定位领域应用有限。为弥补这一空白，亟需对两种扫描模式的LiDAR进行系统对比，并提供支持相关研究的数据资源。

Method: 作者采集了包含重复扫描和非重复扫描LiDAR的车辆轨迹点云数据，涵盖八条多样化的车辆轨迹，总计5,445帧。基于这些数据，建立多种定位方法的基线，系统对比两种LiDAR扫描方式下的定位效果。

Result: 实验对比了两种不同LiDAR扫描模式在基础设施车辆定位任务中的表现，建立了主要方法的基线，并提供了性能分析。

Conclusion: 论文为基础设施车辆定位如何选择适合的LiDAR扫描模式提供了有益见解，所公开的数据集和源码将极大促进该领域的进一步研究和应用。

Abstract: Vehicle localization using roadside LiDARs can provide centimeter-level
accuracy for cloud-controlled vehicles while simultaneously serving multiple
vehicles, enhanc-ing safety and efficiency. While most existing studies rely on
repetitive scanning LiDARs, non-repetitive scanning LiDAR offers advantages
such as eliminating blind zones and being more cost-effective. However, its
application in roadside perception and localization remains limited. To address
this, we present a dataset for infrastructure-based vehicle localization, with
data collected from both repetitive and non-repetitive scanning LiDARs, in
order to benchmark the performance of different LiDAR scanning patterns. The
dataset contains 5,445 frames of point clouds across eight vehicle trajectory
sequences, with diverse trajectory types. Our experiments establish base-lines
for infrastructure-based vehicle localization and compare the performance of
these methods using both non-repetitive and repetitive scanning LiDARs. This
work offers valuable insights for selecting the most suitable LiDAR scanning
pattern for infrastruc-ture-based vehicle localization. Our dataset is a
signifi-cant contribution to the scientific community, supporting advancements
in infrastructure-based perception and vehicle localization. The dataset and
source code are publicly available at:
https://github.com/sjtu-cyberc3/BenchRNR.

</details>


### [179] [Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems](https://arxiv.org/abs/2509.15597)
*Yi Dong,Zhongguo Li,Sarvapali D. Ramchurn,Xiaowei Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种用于异构多机器人系统的分布式纳什均衡寻优算法，通过邻近机器人之间的信息共享，实现协同优化与控制。理论分析与仿真实验、实物测试均验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在实际应用中经常需要在不完全集中与信息受限的环境下，完成协同任务。当前面临的挑战包括各机器人能力差异与系统分布性，因而需要新的算法来保证高效协作并达成系统优化目标。

Method: 作者设计了一种分布式优化算法，每个机器人根据邻居提供的信息，计算自身适配的纳什均衡参考点，并结合输出控制律来跟踪该参考点。整个方法在博弈框架下实现系统聚合优化。

Result: 理论上证明了该算法的收敛性和高效性；通过数值仿真和真实机器人实验，表明算法在实际应用中同样有效。

Conclusion: 本文提出的分布式纳什均衡寻优方法能够有效支持异构多机器人系统在有限通信条件下实现高效协作，具有良好的实际应用前景。

Abstract: This paper develops a distributed Nash Equilibrium seeking algorithm for
heterogeneous multi-robot systems. The algorithm utilises distributed
optimisation and output control to achieve the Nash equilibrium by leveraging
information shared among neighbouring robots. Specifically, we propose a
distributed optimisation algorithm that calculates the Nash equilibrium as a
tailored reference for each robot and designs output control laws for
heterogeneous multi-robot systems to track it in an aggregative game. We prove
that our algorithm is guaranteed to converge and result in efficient outcomes.
The effectiveness of our approach is demonstrated through numerical simulations
and empirical testing with physical robots.

</details>


### [180] [ORB: Operating Room Bot, Automating Operating Room Logistics through Mobile Manipulation](https://arxiv.org/abs/2509.15600)
*Jinkai Qiu,Yungjun Kim,Gaurav Sethia,Tanmay Agarwal,Siddharth Ghodasara,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 该论文提出了一种用于医院手术室自动化物流的机器人系统ORB，通过集成多种先进感知与运动规划模块，实现了高效且可靠的物品递送和补货任务。实验证明ORB具有较高的成功率，显示出在自动化手术室物流领域的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 手术室物品的高效及时输送对患者安全至关重要，传统机器人仅能实现大宗搬运，难以胜任手术室独特的物品级物流需求。作者希望解决物品识别、操作效率和无菌性等自动化手术室物流中的关键难题。

Method: 提出ORB框架，采用层级行为树架构整合对象识别、场景理解及基于GPU的运动规划。具体方法包括：模块化软件架构、YOLOv7+SAM2+Grounded DINO的实时识别流水线，以及基于cuRobo优化的实时无碰撞轨迹规划，并在真实环境中进行性能验证。

Result: ORB在物资取用任务中的成功率达80%，补货任务成功率达96%，展示了系统的高效可靠。

Conclusion: ORB作为自动化手术室物流机器人，凭借其模块化设计和高度集成的感知与操作能力，在实际验证中表现优秀，有望成为手术室自主物流的可靠解决方案。

Abstract: Efficiently delivering items to an ongoing surgery in a hospital operating
room can be a matter of life or death. In modern hospital settings, delivery
robots have successfully transported bulk items between rooms and floors.
However, automating item-level operating room logistics presents unique
challenges in perception, efficiency, and maintaining sterility. We propose the
Operating Room Bot (ORB), a robot framework to automate logistics tasks in
hospital operating rooms (OR). ORB leverages a robust, hierarchical behavior
tree (BT) architecture to integrate diverse functionalities of object
recognition, scene interpretation, and GPU-accelerated motion planning. The
contributions of this paper include: (1) a modular software architecture
facilitating robust mobile manipulation through behavior trees; (2) a novel
real-time object recognition pipeline integrating YOLOv7, Segment Anything
Model 2 (SAM2), and Grounded DINO; (3) the adaptation of the cuRobo
parallelized trajectory optimization framework to real-time, collision-free
mobile manipulation; and (4) empirical validation demonstrating an 80% success
rate in OR supply retrieval and a 96% success rate in restocking operations.
These contributions establish ORB as a reliable and adaptable system for
autonomous OR logistics.

</details>


### [181] [PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models](https://arxiv.org/abs/2509.15607)
*Ruiqi Wang,Dezhong Zhao,Ziqin Yuan,Tianyu Shao,Guohua Chen,Dominic Kao,Sungeun Hong,Byung-Cheol Min*

Main category: cs.RO

TL;DR: 该论文提出了PRIMT框架，通过结合大语言模型和视觉语言模型，实现了多模态合成反馈和轨迹生成，从而显著提升了偏好强化学习的效率和表现。


<details>
  <summary>Details</summary>
Motivation: 偏好强化学习在无需奖励设计的情况下学习复杂行为，但通常需要大量人工输入，并难以解决查询歧义和奖励归因问题。

Method: 提出了PRIMT框架，采用大语言模型与视觉语言模型神经符号融合的方法，为机器人行为评估提供更完整可靠的反馈。引入前瞻性轨迹生成减少初期歧义，并利用后验轨迹增强和因果辅助损失，提升奖励归因能力。

Result: 在2个运动和6个操作任务、多个基准测试中，PRIMT均优于其他基于基础模型或脚本的基线方法，表现更优。

Conclusion: PRIMT框架有效减少了人工输入需求，提升了偏好强化学习的泛化性与表现，是机器人复杂行为学习的新进展。

Abstract: Preference-based reinforcement learning (PbRL) has emerged as a promising
paradigm for teaching robots complex behaviors without reward engineering.
However, its effectiveness is often limited by two critical challenges: the
reliance on extensive human input and the inherent difficulties in resolving
query ambiguity and credit assignment during reward learning. In this paper, we
introduce PRIMT, a PbRL framework designed to overcome these challenges by
leveraging foundation models (FMs) for multimodal synthetic feedback and
trajectory synthesis. Unlike prior approaches that rely on single-modality FM
evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy,
integrating the complementary strengths of large language models and
vision-language models in evaluating robot behaviors for more reliable and
comprehensive feedback. PRIMT also incorporates foresight trajectory
generation, which reduces early-stage query ambiguity by warm-starting the
trajectory buffer with bootstrapped samples, and hindsight trajectory
augmentation, which enables counterfactual reasoning with a causal auxiliary
loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6
manipulation tasks on various benchmarks, demonstrating superior performance
over FM-based and scripted baselines.

</details>


### [182] [Miniature soft robot with magnetically reprogrammable surgical functions](https://arxiv.org/abs/2509.15610)
*Chelsea Shan Xian Ng,Yu Xuan Yeoh,Nicholas Yong Wei Foo,Keerthana Radhakrishnan,Guo Zhan Lum*

Main category: cs.RO

TL;DR: 本文提出了一种磁性可编程毫米级软体机器人，具备全六自由度运动与多种外科手术功能，有望大幅提升微创手术的安全性与可行性。


<details>
  <summary>Details</summary>
Motivation: 现有磁驱动微型机器人在手术中的应用受限，主要因为其功能单一（最多两种功能）、运动自由度有限（多为5自由度）、以及需要近距离强磁场激励，缺乏实用性。因此，需要开发具备多样化手术功能、全六自由度运动，并能在弱均匀磁场下远程控制的新型机器人。

Method: 作者设计了一种毫米级软体机器人，通过可编程磁化配置，实现六自由度运动（包括磁矩绕自身轴旋转），并可在较弱均匀磁场下驱动。机器人可动态切换，实现药物释放、组织切割、夹持、样本存储和远程加热等五项手术功能。实验验证其能以滚动及'双锚点爬行'模式适应复杂生物组织环境。

Result: 实验证明，该机器人能在弱磁场下安全远程操控，穿越复杂非结构化环境，并顺利完成多项手术相关功能，且具备前所未有的功能集成度与六自由度灵活性，克服了传统磁控微型机器人具有限制的问题。

Conclusion: 本工作实现了磁性可编程软体微型机器人的重大突破，为构建功能多样、柔性高、安全且可控的无缆微创手术机器人提供了新思路，有望推动软体执行器和新一代微创医疗技术的发展。

Abstract: Miniature robots are untethered actuators, which have significant potential
to make existing minimally invasive surgery considerably safer and painless,
and enable unprecedented treatments because they are much smaller and dexterous
than existing surgical robots. Of the miniature robots, the magnetically
actuated ones are the most functional and dexterous. However, existing magnetic
miniature robots are currently impractical for surgery because they are either
restricted to possessing at most two on-board functionalities or having limited
five degrees-of-freedom (DOF) locomotion. Some of these actuators are also only
operational under specialized environments where actuation from strong external
magnets must be at very close proximity (< 4 cm away). Here we present a
millimeter-scale soft robot where its magnetization profile can be reprogrammed
upon command to perform five surgical functionalities: drug-dispensing, cutting
through biological tissues (simulated with gelatin), gripping, storing
(biological) samples and remote heating. By possessing full six-DOF motions,
including the sixth-DOF rotation about its net magnetic moment, our soft robot
can also roll and two-anchor crawl across challenging unstructured
environments, which are impassable by its five-DOF counterparts. Because our
actuating magnetic fields are relatively uniform and weak (at most 65 mT and
1.5 T/m), such fields can theoretically penetrate through biological tissues
harmlessly and allow our soft robot to remain controllable within the depths of
the human body. We envision that this work marks a major milestone for the
advancement of soft actuators, and towards revolutionizing minimally invasive
treatments with untethered miniature robots that have unprecedented
functionalities.

</details>


### [183] [Indoor Positioning Based on Active Radar Sensing and Passive Reflectors: Reflector Placement Optimization](https://arxiv.org/abs/2509.15613)
*Sven Hinderer,Pascal Schlachter,Zhibin Yu,Xiaofeng Wu,Bin Yang*

Main category: cs.RO

TL;DR: 本论文提出了一种基于FMCW雷达和被动反射器的室内定位系统，并通过多目标粒子群优化算法优化反射器的位置，实现了高精度且低成本的自动移动机器人定位。


<details>
  <summary>Details</summary>
Motivation: 在复杂室内环境中实现低成本且高精度的自动移动机器人定位是当前的重要挑战。传统室内定位方案成本高或精度有限，因此亟需新方法平衡成本与性能。

Method: 该方法采用单通道FMCW雷达结合简单的室内被动雷达反射器，通过检测反射信号进行定位。同时，提出多目标粒子群优化算法，自动优化反射器在二维平面内的布局以适应复杂房间。

Result: 实验结果表明，结合优化布局和雷达探测，该系统可在复杂室内环境下实现高定位精度，且硬件成本较低。

Conclusion: 结合被动反射器与FMCW雷达，并通过粒子群优化算法优化反射器摆放，能够有效提升AMR室内定位系统的性能并降低部署成本，具有实际应用前景。

Abstract: We extend our work on a novel indoor positioning system (IPS) for autonomous
mobile robots (AMRs) based on radar sensing of local, passive radar reflectors.
Through the combination of simple reflectors and a single-channel frequency
modulated continuous wave (FMCW) radar, high positioning accuracy at low system
cost can be achieved. Further, a multi-objective (MO) particle swarm
optimization (PSO) algorithm is presented that optimizes the 2D placement of
radar reflectors in complex room settings.

</details>


### [184] [Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion](https://arxiv.org/abs/2509.15673)
*Yinong Cao,Xin He,Yuwei Chen,Chenyang Zhang,Chengyu Pu,Bingtao Wang,Kaile Wu,Shouzheng Zhu,Fei Han,Shijie Liu,Chunlai Li,Jianyu Wang*

Main category: cs.RO

TL;DR: 本文提出了Omni-LIVO，一种多相机与广角LiDAR紧耦合的新型定位系统，显著提升了场景覆盖范围和定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大视场（FoV）LiDAR可在大范围环境下提供密集几何信息，但主流LiDAR-惯性-视觉里程计（LIVO）系统多只用单一相机，导致空间覆盖受限，鲁棒性下降。该研究旨在解决LiDAR与相机FoV不匹配的问题。

Method: Omni-LIVO采用多摄像头，通过Cross-View直接跟踪策略，实现多个非重叠视角间的光度一致性；同时扩展了误差状态迭代卡尔曼滤波器（ESIKF），支持多视角更新和自适应协方差加权。

Result: 系统在公开基准和自建数据集上测试，结果显示，Omni-LIVO在精度和鲁棒性方面优于现有LIVO、LIO和视觉-惯性基线方法。

Conclusion: Omni-LIVO能有效解决广角LiDAR与相机FoV不匹配的问题，在提升空间覆盖和鲁棒性的同时，实现了更高精度的定位与建图。

Abstract: Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large
environments, but most existing LiDAR-inertial-visual odometry (LIVO) systems
rely on a single camera, leading to limited spatial coverage and degraded
robustness. We present Omni-LIVO, the first tightly coupled multi-camera LIVO
system that bridges the FoV mismatch between wide-angle LiDAR and conventional
cameras. Omni-LIVO introduces a Cross-View direct tracking strategy that
maintains photometric consistency across non-overlapping views, and extends the
Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive
covariance weighting. The system is evaluated on public benchmarks and our
custom dataset, showing improved accuracy and robustness over state-of-the-art
LIVO, LIO, and visual-inertial baselines. Code and dataset will be released
upon publication.

</details>


### [185] [Imagination at Inference: Synthesizing In-Hand Views for Robust Visuomotor Policy Inference](https://arxiv.org/abs/2509.15717)
*Haoran Ding,Anqing Duan,Zezhou Sun,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 本论文提出了一种通过新颖的视角合成技术，让机器人在没有真实手持摄像头的情况下“想象”出手持视角图像，从而提升视觉运动控制策略的表现。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，手持视角信息对精准控制十分重要，但实际部署时增加手持摄像头会带来硬件、系统复杂度和成本等问题。

Method: 作者提出利用基于扩散模型（经过LoRA微调）的新颖视角合成技术（NVS），根据机器人和手持相机间的相对位姿，在推理阶段生成虚拟手持视角图像。具体上，将预训练的ZeroNVS模型通过LoRA微调适配到机器人操作领域。

Result: 实验在仿真数据集（RoboMimic和MimicGen）及真实世界（Unitree Z1机械臂采摘草莓任务）中进行，结果显示生成的虚拟手持视角明显提升了运动控制策略推理表现，弥补了未安装真实手持摄像头导致的性能损失。

Conclusion: 该方法为机器人部署带来了硬件友好、可扩展的解决方案，展示了利用“想象”视觉推理提升机器人任务表现的潜力。

Abstract: Visual observations from different viewpoints can significantly influence the
performance of visuomotor policies in robotic manipulation. Among these,
egocentric (in-hand) views often provide crucial information for precise
control. However, in some applications, equipping robots with dedicated in-hand
cameras may pose challenges due to hardware constraints, system complexity, and
cost. In this work, we propose to endow robots with imaginative perception -
enabling them to 'imagine' in-hand observations from agent views at inference
time. We achieve this via novel view synthesis (NVS), leveraging a fine-tuned
diffusion model conditioned on the relative pose between the agent and in-hand
views cameras. Specifically, we apply LoRA-based fine-tuning to adapt a
pretrained NVS model (ZeroNVS) to the robotic manipulation domain. We evaluate
our approach on both simulation benchmarks (RoboMimic and MimicGen) and
real-world experiments using a Unitree Z1 robotic arm for a strawberry picking
task. Results show that synthesized in-hand views significantly enhance policy
inference, effectively recovering the performance drop caused by the absence of
real in-hand cameras. Our method offers a scalable and hardware-light solution
for deploying robust visuomotor policies, highlighting the potential of
imaginative visual reasoning in embodied agents.

</details>


### [186] [GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation](https://arxiv.org/abs/2509.15733)
*Quanhao Qian,Guoyang Zhao,Gongjie Zhang,Jiuniu Wang,Ran Xu,Junlong Gao,Deli Zhao*

Main category: cs.RO

TL;DR: 本文提出GP3方法，通过多视角RGB图像推理场景3D几何特征，并结合语言指令实现高效灵巧的机器人操作。方法无需深度传感器，在仿真和现实世界均表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器人操作需要对场景的3D几何有精确理解，传统方法依赖深度传感器或环境建图，限制了泛化能力和适用范围。为此，作者希望借助多视角观测、仅基于RGB输入，构建一个紧凑高效的3D场景表示，提升操作能力并降低对硬件的依赖。

Method: 提出GP3方法：采用空间编码器从多视角RGB图像中推理稠密空间特征，进而估计深度和相机参数，生成适用于操作的3D场景表示。该表示结合语言指令输入，通过轻量化策略头输出连续操作动作。

Result: 在仿真基准测试上，GP3持续超越现有SOTA方法。在未使用深度传感器和预先建图的现实机器人平台中，仅需少量微调也能有良好迁移能力。

Conclusion: GP3方法以其实用、高效和传感器无关的特点，为3D场景感知和机器人智能操作提供了新的解决方案。

Abstract: Effective robotic manipulation relies on a precise understanding of 3D scene
geometry, and one of the most straightforward ways to acquire such geometry is
through multi-view observations. Motivated by this, we present GP3 -- a 3D
geometry-aware robotic manipulation policy that leverages multi-view input. GP3
employs a spatial encoder to infer dense spatial features from RGB
observations, which enable the estimation of depth and camera parameters,
leading to a compact yet expressive 3D scene representation tailored for
manipulation. This representation is fused with language instructions and
translated into continuous actions via a lightweight policy head. Comprehensive
experiments demonstrate that GP3 consistently outperforms state-of-the-art
methods on simulated benchmarks. Furthermore, GP3 transfers effectively to
real-world robots without depth sensors or pre-mapped environments, requiring
only minimal fine-tuning. These results highlight GP3 as a practical,
sensor-agnostic solution for geometry-aware robotic manipulation.

</details>


### [187] [SMART: Scalable Multi-Agent Reasoning and Trajectory Planning in Dense Environments](https://arxiv.org/abs/2509.15737)
*Heye Huang,Yibin Yang,Wang Chen,Tiantian Chen,Xiaopeng Li,Sikai Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为SMART的多车轨迹规划框架，结合分层优先级搜索和分布式优化，在复杂环境下大幅提升了多车同时规划的效率和成功率，并在大规模、多车实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多车轨迹规划面临的主要挑战是高密集环境中碰撞约束迅速增长，导致非凸性问题难以高效求解，难以满足实时、大规模协作的需求。

Method: 提出了SMART框架，采用分层结构：上层通过基于强化学习的优先级估计与大步长混合A*搜索探索多种交互模式，下层则采用可并行的凸优化进一步细化轨迹方案。空间分割及走廊构造方法将复杂非凸问题解耦为多个易于并行的凸子问题，从而提高规划效率和可行性。该框架支持车—车、车—基础设施协作。

Result: 在标准测试中，SMART支持25辆车时在50x50米场地1秒内达到90%以上成功率，明显优于传统方法；在100x100米场地，50辆车成功率达95%以上，且能扩展到90辆，运行速度比单纯优化方法快一个数量级。实际道路实验也验证了高效率和协作性。

Conclusion: SMART框架极大提升了多车轨迹规划的规模化和实时性，兼顾协作、可扩展性和安全，在理论和实际场景下均取得优异表现。

Abstract: Multi-vehicle trajectory planning is a non-convex problem that becomes
increasingly difficult in dense environments due to the rapid growth of
collision constraints. Efficient exploration of feasible behaviors and
resolution of tight interactions are essential for real-time, large-scale
coordination. This paper introduces SMART, Scalable Multi-Agent Reasoning and
Trajectory Planning, a hierarchical framework that combines priority-based
search with distributed optimization to achieve efficient and feasible
multi-vehicle planning. The upper layer explores diverse interaction modes
using reinforcement learning-based priority estimation and large-step hybrid A*
search, while the lower layer refines solutions via parallelizable convex
optimization. By partitioning space among neighboring vehicles and constructing
robust feasible corridors, the method decouples the joint non-convex problem
into convex subproblems solved efficiently in parallel. This design alleviates
the step-size trade-off while ensuring kinematic feasibility and collision
avoidance. Experiments show that SMART consistently outperforms baselines. On
50 m x 50 m maps, it sustains over 90% success within 1 s up to 25 vehicles,
while baselines often drop below 50%. On 100 m x 100 m maps, SMART achieves
above 95% success up to 50 vehicles and remains feasible up to 90 vehicles,
with runtimes more than an order of magnitude faster than optimization-only
approaches. Built on vehicle-to-everything communication, SMART incorporates
vehicle-infrastructure cooperation through roadside sensing and agent
coordination, improving scalability and safety. Real-world experiments further
validate this design, achieving planning times as low as 0.014 s while
preserving cooperative behaviors.

</details>


### [188] [FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication](https://arxiv.org/abs/2509.15807)
*Yuyang Zhang,Zhuoli Tian,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了FlyKites框架，实现了在受限通信条件下多机器人协同探索及与操作员的高效互动。该方法兼顾分布式探索、通信优化和人机在线协作，提升了任务效率。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人探索在极端或通信受限环境下（如地下通道、洞穴等）难以有效与人工操作员协同处理突发状况，亟需新的机制保证探索连续性和对人工协助请求的响应。

Method: FlyKites框架包括三个相互交织的模块：(I) 分布式探索与间断通信（spread mode），多机器人协作探索并本地交换信息；(II) 转为中继模式（relay mode）时，联合优化中继拓扑、操作员路径及机器人角色分派，保障协助需求带宽最小化延迟；(III) 在线人机协作，机器人自适应角色切换和与操作员互动。

Result: 通过多场景下的人类参与仿真和真实硬件实验，FlyKites框架展示了在人-机互动、多机器人探索以及受限通信环境下的有效性与稳定性。

Conclusion: FlyKites实现了在复杂、通信受限的任务环境下，多机器人协作与人工操作员之间高效、安全互动，具备良好的实际应用前景。

Abstract: Fleets of autonomous robots have been deployed for exploration of unknown
scenes for features of interest, e.g., subterranean exploration,
reconnaissance, search and rescue missions. During exploration, the robots may
encounter un-identified targets, blocked passages, interactive objects,
temporary failure, or other unexpected events, all of which require consistent
human assistance with reliable communication for a time period. This however
can be particularly challenging if the communication among the robots is
severely restricted to only close-range exchange via ad-hoc networks,
especially in extreme environments like caves and underground tunnels. This
paper presents a novel human-centric interactive exploration and assistance
framework called FlyKites, for multi-robot systems under limited communication.
It consists of three interleaved components: (I) the distributed exploration
and intermittent communication (called the "spread mode"), where the robots
collaboratively explore the environment and exchange local data among the fleet
and with the operator; (II) the simultaneous optimization of the relay
topology, the operator path, and the assignment of robots to relay roles
(called the "relay mode"), such that all requested assistance can be provided
with minimum delay; (III) the human-in-the-loop online execution, where the
robots switch between different roles and interact with the operator
adaptively. Extensive human-in-the-loop simulations and hardware experiments
are performed over numerous challenging scenes.

</details>


### [189] [Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for Energy-aware and Timely Operations](https://arxiv.org/abs/2509.15830)
*Chuhao Qin,Arun Narayanan,Evangelos Pournaras*

Main category: cs.RO

TL;DR: 本文提出一种通过多智能体深度强化学习优化的无人机多包裹配送方案，提高了能源效率、配送时效性及物流战略部署。


<details>
  <summary>Details</summary>
Motivation: 疫情期间凸显了医疗等急需包裹的及时递送需求，无人机为最后一公里配送带来了高速、安全和成本效益。为充分利用无人机在多包裹、多客户及时配送中的潜力，需要优化其航线和能源消耗。

Method: 问题被分解为三部分：1）利用K均值聚类优化仓库选址和服务范围；2）通过强化学习确定无人机最佳航程；3）新设计的方案选择方法规划多包裹递送路径。最后，通过基于actor-critic的多智能体深度强化学习算法将以上子问题整合起来。

Result: 使用真实配送数据集大量实验，结果显示所提算法在能源消耗、配送时延、整体执行时间等方面均表现突出。

Conclusion: 该研究为实际物流中无人机部署提供了经济高效、时效快速的解决方案，也为仓库战略布局提供了数据指导和优化参考。

Abstract: Drones have recently emerged as a faster, safer, and cost-efficient way for
last-mile deliveries of parcels, particularly for urgent medical deliveries
highlighted during the pandemic. This paper addresses a new challenge of
multi-parcel delivery with a swarm of energy-aware drones, accounting for
time-sensitive customer requirements. Each drone plans an optimal multi-parcel
route within its battery-restricted flight range to minimize delivery delays
and reduce energy consumption. The problem is tackled by decomposing it into
three sub-problems: (1) optimizing depot locations and service areas using
K-means clustering; (2) determining the optimal flight range for drones through
reinforcement learning; and (3) planning and selecting multi-parcel delivery
routes via a new optimized plan selection approach. To integrate these
solutions and enhance long-term efficiency, we propose a novel algorithm
leveraging actor-critic-based multi-agent deep reinforcement learning.
Extensive experimentation using realistic delivery datasets demonstrate an
exceptional performance of the proposed algorithm. We provide new insights into
economic efficiency (minimize energy consumption), rapid operations (reduce
delivery delays and overall execution time), and strategic guidance on depot
deployment for practical logistics applications.

</details>


### [190] [High-Bandwidth Tactile-Reactive Control for Grasp Adjustment](https://arxiv.org/abs/2509.15876)
*Yonghyeon Lee,Tzu-Yuan Lin,Alexander Alexiev,Sangbae Kim*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖触觉反馈的抓取调整算法，无需物体几何先验或准确初始抓取位姿，也能提高机械手抓取的稳定性。通过仿真和真实机器人实验验证该方法在存在感知误差时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉依赖型抓取系统受限于标定误差、传感器噪声及抓取位姿预测不准，导致最终抓取阶段存在不可避免的接触不确定性。如何在感知环节存在误差的情况下提升抓取鲁棒性是个重要挑战。

Method: 提出一种基于纯触觉反馈的抓取调整控制器。该控制器无需物体的几何信息或精准的初始抓取位姿，仅通过高带宽指尖触觉传感器(200Hz)获取反馈，动态修正抓取动作，从粗略、甚至不确定的初始配置出发，也能逐步优化到更稳定的抓取。

Result: 通过在15自由度(8自由度手)机械臂-机械手系统上的仿真和实物实验，证明所提出的方法显著提升了抓取的稳定性，在感知误差和接触不确定性较大的情况下表现优越。

Conclusion: 不依赖视觉和几何先验，纯触觉反馈抓取调整算法能够有效提升现实场景下机械手抓取的鲁棒性和稳定性，对接触不确定问题具有良好适应性。

Abstract: Vision-only grasping systems are fundamentally constrained by calibration
errors, sensor noise, and grasp pose prediction inaccuracies, leading to
unavoidable contact uncertainty in the final stage of grasping. High-bandwidth
tactile feedback, when paired with a well-designed tactile-reactive controller,
can significantly improve robustness in the presence of perception errors. This
paper contributes to controller design by proposing a purely tactile-feedback
grasp-adjustment algorithm. The proposed controller requires neither prior
knowledge of the object's geometry nor an accurate grasp pose, and is capable
of refining a grasp even when starting from a crude, imprecise initial
configuration and uncertain contact points. Through simulation studies and
real-world experiments on a 15-DoF arm-hand system (featuring an 8-DoF hand)
equipped with fingertip tactile sensors operating at 200 Hz, we demonstrate
that our tactile-reactive grasping framework effectively improves grasp
stability.

</details>


### [191] [Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder](https://arxiv.org/abs/2509.15880)
*An Dinh Vuong,Minh Nhat Vu,Ian Reid*

Main category: cs.RO

TL;DR: 本文提出了一种高效、具备三维空间理解能力的视觉编码器eVGGT，用于提升机器人模仿学习的表现，兼顾精度与部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的模仿学习方法多采用ResNet或ViT等视觉编码器，它们缺乏显式的三维推理能力，限制了机器人对空间结构的理解。新兴的几何感知视觉模型（如VGGT）虽然能增强空间理解，但运算代价高，难以实用。因此，研究如何在提升空间理解力的同时降低计算资源消耗成为关键。

Method: 本文将几何感知视觉编码器（如VGGT）集成到模仿学习框架（包括ACT和DP）中，比较其与传统编码器的性能差异。为解决高计算成本的问题，作者提出eVGGT，通过蒸馏方法从VGGT获得，显著提升推理速度并减少模型体积，同时保持较强的三维推理能力。

Result: 实验结果显示，在单手与双手操作任务、仿真与现实场景下，结合几何感知编码器的模仿学习框架比传统视觉编码器最高提升6.5%成功率。eVGGT比VGGT快近9倍、体积小5倍，仍能保持强三维理解能力。

Conclusion: 集成几何感知视觉编码器能够明显提升机器人操作性能。所提出的eVGGT兼具高效性与空间理解力，为几何感知机器人视觉在实际系统中的落地提供了可行路径。

Abstract: Existing RGB-based imitation learning approaches typically employ traditional
vision encoders such as ResNet or ViT, which lack explicit 3D reasoning
capabilities. Recent geometry-grounded vision models, such as
VGGT~\cite{wang2025vggt}, provide robust spatial understanding and are
promising candidates to address this limitation. This work investigates the
integration of geometry-aware visual representations into robotic manipulation.
Our results suggest that incorporating the geometry-aware vision encoder into
imitation learning frameworks, including ACT and DP, yields up to 6.5%
improvement over standard vision encoders in success rate across single- and
bi-manual manipulation tasks in both simulation and real-world settings.
Despite these benefits, most geometry-grounded models require high
computational cost, limiting their deployment in practical robotic systems. To
address this challenge, we propose eVGGT, an efficient geometry-aware encoder
distilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than
VGGT, while preserving strong 3D reasoning capabilities. Code and pretrained
models will be released to facilitate further research in geometry-aware
robotics.

</details>


### [192] [An MPC framework for efficient navigation of mobile robots in cluttered environments](https://arxiv.org/abs/2509.15917)
*Johannes Köhler,Daniel Zhang,Raffaele Soloperto,Andrea Carron,Melanie Zeilinger*

Main category: cs.RO

TL;DR: 本文提出了一种集成有限段最短路径规划器的模型预测控制(MPC)方法，实现移动机器人在复杂环境中的高效导航，同时保证避障和目标收敛，硬件实验表明效果优良。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在实际应用中需在障碍物密集环境下高效、可靠地导航，传统方法难以兼顾通用非线性动力学、动态目标调整和实时避障，因此亟需更优解决方案。

Method: 将有限段最短路径规划器嵌入模型预测控制的有限时域轨迹优化过程中，允许人在导航过程中动态分配目标点，该方法在MPC每次优化时更新路径，结合非线性动力学及避障约束，实现鲁棒和高效导航。

Result: 在实际小型地面机器人上进行硬件实验，人在过程中动态指定目标点，机器人能够成功穿越复杂障碍环境，在2-3秒内精确达到新目标。

Conclusion: 集成最短路径规划的MPC方案能有效实现机器人在复杂环境下的导航和避障，表现出较强的实时性和鲁棒性，具有实际应用潜力。

Abstract: We present a model predictive control (MPC) framework for efficient
navigation of mobile robots in cluttered environments. The proposed approach
integrates a finite-segment shortest path planner into the finite-horizon
trajectory optimization of the MPC. This formulation ensures convergence to
dynamically selected targets and guarantees collision avoidance, even under
general nonlinear dynamics and cluttered environments. The approach is
validated through hardware experiments on a small ground robot, where a human
operator dynamically assigns target locations. The robot successfully navigated
through complex environments and reached new targets within 2-3 seconds.

</details>


### [193] [A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning](https://arxiv.org/abs/2509.15937)
*Shaopeng Zhai,Qi Zhang,Tianyi Zhang,Fuxian Huang,Haoran Zhang,Ming Zhou,Shengzhe Zhang,Litao Liu,Sixu Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出了VLAC模型，一种基于视觉-语言-动作的通用奖励模型，显著提升了机器人在现实世界强化学习任务中的学习效率和成功率，尤其是在探索和泛化能力方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人视觉-语言-动作强化学习严重依赖稀疏且手工设计的奖励信号，导致探索效率低且泛化能力弱。因此，亟需一种无需专门奖励工程、能适应不同任务和环境的新型奖励机制。

Method: VLAC模型基于大规模异构数据集（包含视觉-语言数据和机器人/人类轨迹），结合InternVL进行训练，实现了密集进度奖励信号输出支持多个任务的转移。模型能够区分无关或退步情形，并通过prompt控制，统一奖励和动作生成。引入异步RL和分层人类介入协议，加速探索与学习。

Result: 在4个现实世界的操纵任务上，VLAC在200次交互内将成功率由约30%提升到约90% 。结合人类介入，样本效率提升50%，甚至实现了100%的最终成功率。

Conclusion: VLAC显著减少了奖励工程的工作量，提升了现实世界机器人学习的易用性、泛化性与效率，为基于视觉-语言-动作的强化学习在实际场景中大规模部署提供了有力手段。

Abstract: Robotic real-world reinforcement learning (RL) with vision-language-action
(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient
exploration. We introduce VLAC, a general process reward model built upon
InternVL and trained on large scale heterogeneous datasets. Given pairwise
observations and a language goal, it outputs dense progress delta and done
signal, eliminating task-specific reward engineering, and supports one-shot
in-context transfer to unseen tasks and environments. VLAC is trained on
vision-language datasets to strengthen perception, dialogic and reasoning
capabilities, together with robot and human trajectories data that ground
action generation and progress estimation, and additionally strengthened to
reject irrelevant prompts as well as detect regression or stagnation by
constructing large numbers of negative and semantically mismatched samples.
With prompt control, a single VLAC model alternately generating reward and
action tokens, unifying critic and policy. Deployed inside an asynchronous
real-world RL loop, we layer a graded human-in-the-loop protocol (offline
demonstration replay, return and explore, human guided explore) that
accelerates exploration and stabilizes early learning. Across four distinct
real-world manipulation tasks, VLAC lifts success rates from about 30\% to
about 90\% within 200 real-world interaction episodes; incorporating
human-in-the-loop interventions yields a further 50% improvement in sample
efficiency and achieves up to 100% final success.

</details>


### [194] [Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal](https://arxiv.org/abs/2509.15953)
*Chang Yu,Siyu Ma,Wenxin Du,Zeshun Zong,Han Xue,Wendi Chen,Cewu Lu,Yin Yang,Xuchen Han,Joseph Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: 该论文提出了Right-Side-Out框架，实现了服装正反翻转这个高度动态且被严重遮挡的复杂操作任务。该方法实现了高效、无需人工示范的零样本仿真到现实迁移，实现在真实硬件上的高成功率。


<details>
  <summary>Details</summary>
Motivation: 服装正反翻转任务具有高度动态性、接触变化快且容易被遮挡，传统方法很难应对，因此需要一种高效且鲁棒的解决方案。

Method: 作者将任务分解为两个阶段（拖/甩创建开口和插入/拉拽翻转衣物），每个阶段采用基于深度和关键点参数化的双臂运动基元，极大降低动作空间。数据由自研高保真MPM模拟器生成，该模拟器支持薄壳变形仿真以及高效接触处理，可批量高效生成多样化训练数据，无需人工标注。

Result: 在仅用单个深度摄像头的硬件系统上，完全用仿真训练的策略能够直接零样本部署于真实任务，最高达81.3%的成功率。

Conclusion: 基于任务分解与高保真仿真的零样本sim-to-real框架，可有效解决动态、被遮挡的柔性物体操作任务，且无需繁琐人工演示，具有良好的现实适用性和扩展性。

Abstract: Turning garments right-side out is a challenging manipulation task: it is
highly dynamic, entails rapid contact changes, and is subject to severe visual
occlusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that
effectively solves this challenge by exploiting task structures. We decompose
the task into Drag/Fling to create and stabilize an access opening, followed by
Insert&Pull to invert the garment. Each step uses a depth-inferred,
keypoint-parameterized bimanual primitive that sharply reduces the action space
while preserving robustness. Efficient data generation is enabled by our
custom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator
that models thin-shell deformation and provides robust and efficient contact
handling for batched rollouts. Built on the simulator, our fully automated
pipeline scales data generation by randomizing garment geometry, material
parameters, and viewpoints, producing depth, masks, and per-primitive keypoint
labels without any human annotations. With a single depth camera, policies
trained entirely in simulation deploy zero-shot on real hardware, achieving up
to 81.3% success rate. By employing task decomposition and high fidelity
simulation, our framework enables tackling highly dynamic, severely occluded
tasks without laborious human demonstrations.

</details>


### [195] [Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms](https://arxiv.org/abs/2509.15956)
*Alexandre Pacheco,Hanqing Zhao,Volker Strobel,Tarik Roukny,Gregory Dudek,Andreagiovanni Reina,Marco Dorigo*

Main category: cs.RO

TL;DR: 本论文提出了一种基于机器人群体的去中心化预言机系统Swarm Oracle，用于为区块链智能合约收集和验证现实世界数据，增强了区块链的可信性与安全性。


<details>
  <summary>Details</summary>
Motivation: 区块链共识机制受限于“不要信任，要验证”原则，无法直接访问或验证现实世界数据，现有预言机方案往往引入了新的信任假设或者降低了系统自治性和透明度。因此，亟需一种更加去中心化、安全且无需信任的预言机解决方案。

Method: 作者设计了Swarm Oracle，一个由多方持有的自治机器人组成的去中心化群体，机器人通过自身传感器采集现实数据，并通过点对点通信协作验证。系统采用拜占庭容错协议，兼顾数据一致性和防御多方攻击，并结合基于区块链代币的声誉系统提升抗攻击和自我恢复能力。

Result: 通过大量真实及仿真机器人实验，Swarm Oracle可在多数机器人受到攻击（含多种攻击类型）情境下达成环境数据共识，并借助声誉机制系统可自主从故障与攻击中恢复。

Conclusion: Swarm Oracle有效提升了区块链交互现实数据的安全性与自治性，展示了去中心化、抗攻击、可长期运行的预言机系统的可行性，为区块链与现实世界的连接提供了创新路径。

Abstract: Blockchain consensus, rooted in the principle ``don't trust, verify'', limits
access to real-world data, which may be ambiguous or inaccessible to some
participants. Oracles address this limitation by supplying data to blockchains,
but existing solutions may reduce autonomy, transparency, or reintroduce the
need for trust. We propose Swarm Oracle: a decentralized network of autonomous
robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer
communication to collectively verify real-world data and provide it to smart
contracts on public blockchains. Swarm Oracle leverages the built-in
decentralization, fault tolerance and mobility of robot swarms, which can
flexibly adapt to meet information requests on-demand, even in remote
locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates
robots from multiple stakeholders, protecting the system from single-party
biases but also introducing potential adversarial behavior. To ensure the
secure, trustless and global consensus required by blockchains, we employ a
Byzantine fault-tolerant protocol that enables robots from different
stakeholders to operate together, reaching social agreements of higher quality
than the estimates of individual robots. Through extensive experiments using
both real and simulated robots, we showcase how consensus on uncertain
environmental information can be achieved, despite several types of attacks
orchestrated by large proportions of the robots, and how a reputation system
based on blockchain tokens lets Swarm Oracle autonomously recover from faults
and attacks, a requirement for long-term operation.

</details>


### [196] [CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](https://arxiv.org/abs/2509.15968)
*Shiyu Fang,Yiming Cui,Haoyang Liang,Chen Lv,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出了一种名为CoReVLA的新型自主驾驶（AD）系统框架，专注于提升在稀有且关键安全场景（long-tail scenarios）下的表现，并在公开基准上超过了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前自主驾驶技术在常规场景下表现良好，但在稀有、易致事故的安全关键场景下仍易失效，成为事故主要来源。缺乏高质量数据和高效学习机制也是现有视觉-语言-动作（VLA）模型面临的主要制约。

Method: 提出CoReVLA框架，包括：1）在多种开源驾驶QA数据集上联合微调VLA模型，获得基础驾驶理解；2）在CAVE虚拟仿真平台中部署，采集人类接管数据，聚焦模型表现不足的long-tail场景；3）利用直接偏好优化（DPO）方法，以人类偏好数据精细优化模型，避免手工设计奖励的漏洞。

Result: CoReVLA在Bench2Drive基准的long-tail安全场景下取得了Driving Score 72.18、成功率50%的成绩，分别比最先进方法提升7.96和15个百分点。开放与封闭实验均证实了其精确感知与决策能力。案例分析亦表明其能从失败案例持续改进。

Conclusion: CoReVLA通过持续学习和人类偏好优化，显著提升了自主驾驶系统在稀有安全关键场景下的安全性和鲁棒性，对未来安全自主驾驶研究和实际部署具有重要推动意义。

Abstract: Autonomous Driving (AD) systems have made notable progress, but their
performance in long-tail, safety-critical scenarios remains limited. These rare
cases contribute a disproportionate number of accidents. Vision-Language Action
(VLA) models have strong reasoning abilities and offer a potential solution,
but their effectiveness is limited by the lack of high-quality data and
inefficient learning in such conditions. To address these challenges, we
propose CoReVLA, a continual learning end-to-end autonomous driving framework
that improves the performance in long-tail scenarios through a dual-stage
process of data Collection and behavior Refinement. First, the model is jointly
fine-tuned on a mixture of open-source driving QA datasets, allowing it to
acquire a foundational understanding of driving scenarios. Next, CoReVLA is
deployed within the Cave Automatic Virtual Environment (CAVE) simulation
platform, where driver takeover data is collected from real-time interactions.
Each takeover indicates a long-tail scenario that CoReVLA fails to handle
reliably. Finally, the model is refined via Direct Preference Optimization
(DPO), allowing it to learn directly from human preferences and thereby avoid
reward hacking caused by manually designed rewards. Extensive open-loop and
closed-loop experiments demonstrate that the proposed CoReVLA model can
accurately perceive driving scenarios and make appropriate decisions. On the
Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a
Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and
15% SR under long-tail, safety-critical scenarios. Furthermore, case studies
demonstrate the model's ability to continually improve its performance in
similar failure-prone scenarios by leveraging past takeover experiences. All
codea and preprocessed datasets are available at:
https://github.com/FanGShiYuu/CoReVLA

</details>


### [197] [Defining and Monitoring Complex Robot Activities via LLMs and Symbolic Reasoning](https://arxiv.org/abs/2509.16006)
*Francesco Argenziano,Elena Umili,Francesco Leotta,Daniele Nardi*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLMs）与自动规划的通用架构，实现了人类用自然语言指定高层活动，并能通过查询机器人实时监控其执行过程。该方法已在精准农业场景中实现并进行了实证评估。


<details>
  <summary>Details</summary>
Motivation: 当前工业和农业领域任务复杂多样且环境不可预知，许多活动包含多个原子任务的动态组合。虽然机器人能力提升，但人类仍需要有效监控高层任务的执行，特别是关系安全和正确性的关键流程。目前缺乏能支持自然语言交互和状态监控的通用系统。

Method: 作者提出了一种集成LLMs与自动规划的通用架构。该架构允许用户用自然语言描述高层任务，由系统解析并生成可执行计划，并支持用户以自然语言随时查询机器人的任务进展和状态。作者基于现有的先进技术实现了该系统，并在精准农业的真实场景下进行了量化评估。

Result: 在精准农业实际应用中，该架构能准确地将自然语言活动描述转化为机器人可执行任务，并支持有效的任务状态查询。实验结果表明，该系统具有较高的通用性和可用性。

Conclusion: 提出的架构有效结合了LLMs的自然语言理解与自动规划能力，实现了人机交互式的任务指定和执行监控，有助于推动机器人在非结构化复杂场景下的实际应用。

Abstract: Recent years have witnessed a growing interest in automating labor-intensive
and complex activities, i.e., those consisting of multiple atomic tasks, by
deploying robots in dynamic and unpredictable environments such as industrial
and agricultural settings. A key characteristic of these contexts is that
activities are not predefined: while they involve a limited set of possible
tasks, their combinations may vary depending on the situation. Moreover,
despite recent advances in robotics, the ability for humans to monitor the
progress of high-level activities - in terms of past, present, and future
actions - remains fundamental to ensure the correct execution of
safety-critical processes. In this paper, we introduce a general architecture
that integrates Large Language Models (LLMs) with automated planning, enabling
humans to specify high-level activities (also referred to as processes) using
natural language, and to monitor their execution by querying a robot. We also
present an implementation of this architecture using state-of-the-art
components and quantitatively evaluate the approach in a real-world precision
agriculture scenario.

</details>


### [198] [A Matter of Height: The Impact of a Robotic Object on Human Compliance](https://arxiv.org/abs/2509.16032)
*Michael Faber,Andrey Grishko,Julian Waksberg,David Pardo,Tomer Leivy,Yuval Hazan,Emanuel Talmansky,Benny Megidish,Hadas Erel*

Main category: cs.RO

TL;DR: 论文研究了机器人高度对人类顺从性的影响，发现与人类之间的作用机制恰好相反，较矮的机器人更易被顺从。


<details>
  <summary>Details</summary>
Motivation: 在人际互动中，身高较高者通常更具说服力，作者好奇这一特性在人-机器人互动中是否成立，尤其想探索高度对非类人型机器人影响。

Method: 设计了高度可调节的非类人型服务型机器人，通过实验让受试者面对不同高度（95cm和132cm）的机器人邀请他们自愿完成繁琐任务，比较两种条件下的顺从率。

Result: 实验发现，受试者对较矮机器人（95cm）请求的顺从率更高，与人际互动中“高者更具影响力”的结论相反。

Conclusion: 机器人高度对互动确实有显著影响，但与人类社交规律不同。机器人设计者不能直接照搬人类社会中有效的设计元素，需进行专门验证与测试。

Abstract: Robots come in various forms and have different characteristics that may
shape the interaction with them. In human-human interactions, height is a
characteristic that shapes human dynamics, with taller people typically
perceived as more persuasive. In this work, we aspired to evaluate if the same
impact replicates in a human-robot interaction and specifically with a highly
non-humanoid robotic object. The robot was designed with modules that could be
easily added or removed, allowing us to change its height without altering
other design features. To test the impact of the robot's height, we evaluated
participants' compliance with its request to volunteer to perform a tedious
task. In the experiment, participants performed a cognitive task on a computer,
which was framed as the main experiment. When done, they were informed that the
experiment was completed. While waiting to receive their credits, the robotic
object, designed as a mobile robotic service table, entered the room, carrying
a tablet that invited participants to complete a 300-question questionnaire
voluntarily. We compared participants' compliance in two conditions: A Short
robot composed of two modules and 95cm in height and a Tall robot consisting of
three modules and 132cm in height. Our findings revealed higher compliance with
the Short robot's request, demonstrating an opposite pattern to human dynamics.
We conclude that while height has a substantial social impact on human-robot
interactions, it follows a unique pattern of influence. Our findings suggest
that designers cannot simply adopt and implement elements from human social
dynamics to robots without testing them first.

</details>


### [199] [Learning Safety for Obstacle Avoidance via Control Barrier Functions](https://arxiv.org/abs/2509.16037)
*Shuo Liu,Zhe Huang,Calin A. Belta*

Main category: cs.RO

TL;DR: 本文提出了一种结合神经网络和高级控制屏障函数（CBF）的方法，实现了任意几何形状机器人在复杂障碍环境中的高效避障。通过数据驱动预测安全空间，兼顾了安全性和实时性，克服了传统CBF方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有CBF方法依赖于解析清除计算或多面体近似，对于几何形状复杂或未知姿态的机器人不适用。本研究旨在解决这些复杂几何体避障控制困难，兼顾安全、效率与扩展性问题。

Method: 本方法用残差神经网络在大量机器人-障碍物配置样本上训练，学习预测任意配置下的安全距离。以此距离构造Safety Ball，并将边界作为高阶离散时间CBF嵌入非线性优化框架，通过创新的约束松弛技术，确保机器人连续运动过程中无碰撞。

Result: 实验结果显示，该方法在处理任意（包括非凸）机器人几何体、生成动态可行且无碰撞轨迹方面效果显著，解算时间为毫秒级且预测精度高，优于现有CBF方法。

Conclusion: 提出的框架兼顾连续安全性、实用性和扩展性，实现复杂机器人在密集障碍环境中的高效安全导航，对实时机器人控制具有广泛应用潜力。

Abstract: Obstacle avoidance is central to safe navigation, especially for robots with
arbitrary and nonconvex geometries operating in cluttered environments.
Existing Control Barrier Function (CBF) approaches often rely on analytic
clearance computations, which are infeasible for complex geometries, or on
polytopic approximations, which become intractable when robot configurations
are unknown. To address these limitations, this paper trains a residual neural
network on a large dataset of robot-obstacle configurations to enable fast and
tractable clearance prediction, even at unseen configurations. The predicted
clearance defines the radius of a Local Safety Ball (LSB), which ensures
continuous-time collision-free navigation. The LSB boundary is encoded as a
Discrete-Time High-Order CBF (DHOCBF), whose constraints are incorporated into
a nonlinear optimization framework. To improve feasibility, a novel relaxation
technique is applied. The resulting framework ensure that the robot's
rigid-body motion between consecutive time steps remains collision-free,
effectively bridging discrete-time control and continuous-time safety. We show
that the proposed method handles arbitrary, including nonconvex, robot
geometries and generates collision-free, dynamically feasible trajectories in
cluttered environments. Experiments demonstrate millisecond-level solve times
and high prediction accuracy, highlighting both safety and efficiency beyond
existing CBF-based methods.

</details>


### [200] [Compose by Focus: Scene Graph-based Atomic Skills](https://arxiv.org/abs/2509.16053)
*Han Qi,Changhe Chen,Heng Yang*

Main category: cs.RO

TL;DR: 本文提出了一种基于场景图(scene graph)的技能学习框架，有效提升了机器人在复杂长时任务中的鲁棒性和组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要具备组合泛化能力，即能将原子技能组合解决复杂任务。然而，现有方法在序列化技能时容易因场景分布变化导致技能执行失败，缺乏对关键任务对象和关系的关注。

Method: 作者提出了基于场景图的表示方法，以关注与任务相关的对象和关系，降低对无关变化的敏感度。具体地，通过将图神经网络与基于扩散的模仿学习相结合，学习“聚焦”的场景图技能，并与基于视觉-语言模型的任务规划器组合，用以解决长时复杂任务。

Result: 在仿真和真实环境下的操作任务中，该方法成功率明显高于现有的最先进基线，展现了更强的鲁棒性和组合泛化能力。

Conclusion: 通过关注任务关键对象与关系、场景图技能学习及与视觉-语言任务规划结合，系统在复杂长时任务展现出卓越泛化能力与执行稳定性。

Abstract: A key requirement for generalist robots is compositional generalization - the
ability to combine atomic skills to solve complex, long-horizon tasks. While
prior work has primarily focused on synthesizing a planner that sequences
pre-learned skills, robust execution of the individual skills themselves
remains challenging, as visuomotor policies often fail under distribution
shifts induced by scene composition. To address this, we introduce a scene
graph-based representation that focuses on task-relevant objects and relations,
thereby mitigating sensitivity to irrelevant variation. Building on this idea,
we develop a scene-graph skill learning framework that integrates graph neural
networks with diffusion-based imitation learning, and further combine "focused"
scene-graph skills with a vision-language model (VLM) based task planner.
Experiments in both simulation and real-world manipulation tasks demonstrate
substantially higher success rates than state-of-the-art baselines,
highlighting improved robustness and compositional generalization in
long-horizon tasks.

</details>


### [201] [Latent Conditioned Loco-Manipulation Using Motion Priors](https://arxiv.org/abs/2509.16061)
*Maciej Stępień,Rafael Kourdis,Constant Roux,Olivier Stasse*

Main category: cs.RO

TL;DR: 现有的机器人控制方法多侧重于单一技能，效率低下，文中提出通过模仿学习训练通用运动策略，并在隐空间实现技能控制，更有效地解决复杂任务。该方法通过高质量模仿并引入约束提升安全性和表现，在仿真与实物机器人上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 控制多功能机器人（如类人和四足机器人）现有方法只针对单技能，难以应对涉及高层目标、物理限制及运动风格的复杂任务，因此亟需高效且可迁移的通用控制策略。

Method: 首先通过模仿学习训练获取包含多种低层技能的运动策略，利用隐空间调整执行细节；在此基础上扩展算法以处理安全性相关的约束，引入扩散判别器提升模仿质量；在仿真实验和真实硬件机器人（H1、Solo12）上验证方法。

Result: 新方法可在仿真环境的类人和四足机器人以及Solo12硬件平台上实现高效的多任务loco-manipulation，模仿质量和安全性优于传统深度强化学习方法。

Conclusion: 通过模仿多样运动并引入约束与高质量判别机制，所提通用控制策略能显著提升复杂机器人任务的效率与可靠性，有望推广于更多实际场景。

Abstract: Although humanoid and quadruped robots provide a wide range of capabilities,
current control methods, such as Deep Reinforcement Learning, focus mainly on
single skills. This approach is inefficient for solving more complicated tasks
where high-level goals, physical robot limitations and desired motion style
might all need to be taken into account. A more effective approach is to first
train a multipurpose motion policy that acquires low-level skills through
imitation, while providing latent space control over skill execution. Then,
this policy can be used to efficiently solve downstream tasks. This method has
already been successful for controlling characters in computer graphics. In
this work, we apply the approach to humanoid and quadrupedal loco-manipulation
by imitating either simple synthetic motions or kinematically retargeted dog
motions. We extend the original formulation to handle constraints, ensuring
deployment safety, and use a diffusion discriminator for better imitation
quality. We verify our methods by performing loco-manipulation in simulation
for the H1 humanoid and Solo12 quadruped, as well as deploying policies on
Solo12 hardware. Videos and code are available at
https://gepetto.github.io/LaCoLoco/

</details>


### [202] [DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation](https://arxiv.org/abs/2509.16063)
*Yue Su,Chubin Zhang,Sijin Chen,Liufan Tan,Yansong Tang,Jianan Wang,Xihui Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的策略架构DSPv2，用于提升全身移动操作机器人的模仿学习能力，在任务表现和泛化能力上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全身移动操作机器人的模仿学习在推广到不同环境和复杂任务时面临重大挑战，如复杂信息处理、泛化能力不足、动作连贯性差。

Method: 提出DSPv2策略架构，将三维空间特征与多视角二维语义特征进行有效对齐和融合，同时首次扩展Dense Policy范式到全身移动操作领域，实现更精细的感知与控制。

Result: 通过大量实验验证，该方法在任务完成率和泛化能力两方面显著超过现有方法。

Conclusion: DSPv2通过创新的特征融合与策略设计，有效推动了全身移动操作机器人模仿学习的发展，为机器人能力的广泛拓展奠定基础。

Abstract: Learning whole-body mobile manipulation via imitation is essential for
generalizing robotic skills to diverse environments and complex tasks. However,
this goal is hindered by significant challenges, particularly in effectively
processing complex observation, achieving robust generalization, and generating
coherent actions. To address these issues, we propose DSPv2, a novel policy
architecture. DSPv2 introduces an effective encoding scheme that aligns 3D
spatial features with multi-view 2D semantic features. This fusion enables the
policy to achieve broad generalization while retaining the fine-grained
perception necessary for precise control. Furthermore, we extend the Dense
Policy paradigm to the whole-body mobile manipulation domain, demonstrating its
effectiveness in generating coherent and precise actions for the whole-body
robotic platform. Extensive experiments show that our method significantly
outperforms existing approaches in both task performance and generalization
ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.

</details>


### [203] [I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models](https://arxiv.org/abs/2509.16072)
*Clemence Grislain,Hamed Rahimi,Olivier Sigaud,Mohamed Chetouani*

Main category: cs.RO

TL;DR: 该论文提出了一种用于检测语义未对齐错误（Semantic Misalignment Failures）的新型开源视觉-语言模型（VLM）框架I-FailSense。通过对现有数据集构建新的检测集，并在VLM不同层添加轻量分类头，I-FailSense在检测机器人任务执行中的语义错误上超越了现有方法，并在迁移到新场景时表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前VLM已显著提升了机器人空间推理与任务规划能力，但在检测自身任务失败、特别是语义未对齐错误方面存在显著不足。这类错误在实际环境中非常关键，因机器人可能执行了有意义但不符合指令的任务，亟需有效的检测方法。

Method: 方法一方面提出利用现有基于语言的操控数据集，构建专门用于语义未对齐检测的数据集；另一方面，开发了I-FailSense框架：在基础VLM上后训练，同时在VLM各层插入专用的轻量分类头（FS blocks），最后使用集成机制整合各层预测结果以提升准确率。

Result: 实验结果显示，I-FailSense在语义未对齐检测任务上优于现有同类和更大型VLM框架。此外，虽然仅在语义未对齐检测上训练，I-FailSense对更广泛的失败类别也有良好泛化能力，并能在其他仿真和真实环境下以零样本或极少调优实现迁移。

Conclusion: I-FailSense为机器人在真实世界中的失败检测（尤其是语义未对齐）提供了有效且易部署的解决方案，其泛化与迁移能力强，相关数据集和模型已开源，有望加速相关研究和实际落地。

Abstract: Language-conditioned robotic manipulation in open-world settings requires not
only accurate task execution but also the ability to detect failures for robust
deployment in real-world environments. Although recent advances in
vision-language models (VLMs) have significantly improved the spatial reasoning
and task-planning capabilities of robots, they remain limited in their ability
to recognize their own failures. In particular, a critical yet underexplored
challenge lies in detecting semantic misalignment errors, where the robot
executes a task that is semantically meaningful but inconsistent with the given
instruction. To address this, we propose a method for building datasets
targeting Semantic Misalignment Failures detection, from existing
language-conditioned manipulation datasets. We also present I-FailSense, an
open-source VLM framework with grounded arbitration designed specifically for
failure detection. Our approach relies on post-training a base VLM, followed by
training lightweight classification heads, called FS blocks, attached to
different internal layers of the VLM and whose predictions are aggregated using
an ensembling mechanism. Experiments show that I-FailSense outperforms
state-of-the-art VLMs, both comparable in size and larger, in detecting
semantic misalignment errors. Notably, despite being trained only on semantic
misalignment detection, I-FailSense generalizes to broader robotic failure
categories and effectively transfers to other simulation environments and
real-world with zero-shot or minimal post-training. The datasets and models are
publicly released on HuggingFace (Webpage:
https://clemgris.github.io/I-FailSense/).

</details>


### [204] [Real-Time Planning and Control with a Vortex Particle Model for Fixed-Wing UAVs in Unsteady Flows](https://arxiv.org/abs/2509.16079)
*Ashwin Gupta,Kevin Wolfe,Gino Perrotta,Joseph Moore*

Main category: cs.RO

TL;DR: 本文提出了一种可实时规划和控制的方法，能够考虑非定常空气动力学效应，通过轻量级涡粒子模型和并行GPU加速，实现对复杂空气流动影响的前瞻性优化，并在模拟和实物实验中验证了其对飞行器高机动动作性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有飞行器在执行敏捷机动和复杂环境下飞行时，往往难以有效处理非定常空气动力学效应，导致飞行性能下降。因此，开发能实时考虑非定常气动影响的规划与控制方法，成为提升飞行器适应复杂环境能力的重要需求。

Method: 方法基于一种轻量级的涡粒子模型，支持GPU并行加速，并结合采样式策略优化算法，将气动建模与控制决策有机结合，实现实时的预测与重规划能力。

Result: 在模拟和实际硬件实验中，使用所提出基于非定常气动模型的重规划方法，显著提升了飞行器在存在扰动时进行剧烈失速后机动动作时的性能。

Conclusion: 通过实时引入非定常气动模型与控制优化，能够有效提升飞行器在动态复杂环境中的性能与鲁棒性，具备实际应用前景。

Abstract: Unsteady aerodynamic effects can have a profound impact on aerial vehicle
flight performance, especially during agile maneuvers and in complex
aerodynamic environments. In this paper, we present a real-time planning and
control approach capable of reasoning about unsteady aerodynamics. Our approach
relies on a lightweight vortex particle model, parallelized to allow GPU
acceleration, and a sampling-based policy optimization strategy capable of
leveraging the vortex particle model for predictive reasoning. We demonstrate,
through both simulation and hardware experiments, that by replanning with our
unsteady aerodynamics model, we can improve the performance of aggressive
post-stall maneuvers in the presence of unsteady environmental flow
disturbances.

</details>


### [205] [Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors](https://arxiv.org/abs/2509.16122)
*Carter Sifferman,Mohit Gupta,Michael Gleicher*

Main category: cs.RO

TL;DR: 本文提出了一种基于机械臂安装的微型飞行时间（ToF）传感器进行周围物体检测和定位的方法，有效区分机械臂本体与外部物体，适用于避障和安全人机交互。


<details>
  <summary>Details</summary>
Motivation: 常规装在机械臂上的ToF传感器容易将机械臂自身误判为外物，导致检测不准确，严重影响避障和人机协作的安全性。现有方法对传感器布置灵活性有限，且计算开销大。

Method: 作者基于多组低分辨率、廉价ToF传感器，利用仅有机械臂本体存在时的传感器数据构建经验模型。运行时，通过对比实时数据和模型预测，检测和定位靠近机械臂的外来物体。此法避免了自检测问题，并扩展了传感器布置的灵活性。

Result: 该方法可有效检测靠近机械臂的小型物体，并能在机械臂连杆沿长度方向上对物体位置实现较高精度的定位。实验分析了不同物体类型、位置及环境光照条件下的性能，同时识别出因传感器原理导致的性能瓶颈。

Conclusion: 本文所提方法计算量小、布置灵活，可显著提升机械臂的周边物体检测能力，为避障及安全人机交互场景提供切实可行的技术路径。

Abstract: We provide a method for detecting and localizing objects near a robot arm
using arm-mounted miniature time-of-flight sensors. A key challenge when using
arm-mounted sensors is differentiating between the robot itself and external
objects in sensor measurements. To address this challenge, we propose a
computationally lightweight method which utilizes the raw time-of-flight
information captured by many off-the-shelf, low-resolution time-of-flight
sensor. We build an empirical model of expected sensor measurements in the
presence of the robot alone, and use this model at runtime to detect objects in
proximity to the robot. In addition to avoiding robot self-detections in common
sensor configurations, the proposed method enables extra flexibility in sensor
placement, unlocking configurations which achieve more efficient coverage of a
radius around the robot arm. Our method can detect small objects near the arm
and localize the position of objects along the length of a robot link to
reasonable precision. We evaluate the performance of the method with respect to
object type, location, and ambient light level, and identify limiting factors
on performance inherent in the measurement principle. The proposed method has
potential applications in collision avoidance and in facilitating safe
human-robot interaction.

</details>


### [206] [Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning](https://arxiv.org/abs/2509.16136)
*Changwei Yao,Xinzi Liu,Chen Li,Marios Savvides*

Main category: cs.RO

TL;DR: 本文提出了一种名为RE-GoT的新框架，将大语言模型（LLMs）、视觉语言模型（VLMs）和图结构推理相结合，实现了RL奖励函数的自动化设计和进化，并在多个复杂任务上显著优于现有方法和专家设计。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，奖励函数的设计非常关键，但通常需要大量人力和专业知识。虽然当前已有利用LLMs自动设计奖励的方法，但存在幻觉、对人工反馈的依赖以及难以处理多步复杂任务等问题。本文旨在解决这些自动化奖励设计过程中的核心挑战。

Method: 作者提出了Reward Evolution with Graph-of-Thoughts（RE-GoT）框架。首先，LLMs将任务分解为带文本属性的图结构，形成“思维图”进行深度分析和奖励函数生成。其次，RE-GoT结合VLMs进行自动化环境评估，该过程无需人工干预，通过视觉反馈不断迭代优化奖励函数。

Result: 在RoboGen的10个任务和ManiSkill2的4个任务上，RE-GoT均优于现有的LLM基线方法。在RoboGen上，平均任务成功率提升了32.25%，在多步复杂任务上表现尤为突出。在ManiSkill2上，RE-GoT平均成功率达到93.73%，显著超越之前方法，甚至超过专家手工设计的奖励。

Conclusion: 将LLMs和VLMs与图结构推理结合，可有效提升RL奖励函数的自动化设计能力，为复杂任务和高效RL训练提供了可扩展的解决思路。

Abstract: Designing effective reward functions remains a major challenge in
reinforcement learning (RL), often requiring considerable human expertise and
iterative refinement. Recent advances leverage Large Language Models (LLMs) for
automated reward design, but these approaches are limited by hallucinations,
reliance on human feedback, and challenges with handling complex, multi-step
tasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts
(RE-GoT), a novel bi-level framework that enhances LLMs with structured
graph-based reasoning and integrates Visual Language Models (VLMs) for
automated rollout evaluation. RE-GoT first decomposes tasks into
text-attributed graphs, enabling comprehensive analysis and reward function
generation, and then iteratively refines rewards using visual feedback from
VLMs without human intervention. Extensive experiments on 10 RoboGen and 4
ManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing
LLM-based baselines. On RoboGen, our method improves average task success rates
by 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,
RE-GoT achieves an average success rate of 93.73% across four diverse
manipulation tasks, significantly surpassing prior LLM-based approaches and
even exceeding expert-designed rewards. Our results indicate that combining
LLMs and VLMs with graph-of-thoughts reasoning provides a scalable and
effective solution for autonomous reward evolution in RL.

</details>


### [207] [Modeling Elastic-Body Dynamics of Fish Swimming Using a Variational Framework](https://arxiv.org/abs/2509.16145)
*Zhiheng Chen,Wei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于哈密顿原理的全身动力学模型，用于模拟仿鱼机器人游动，系统考虑了体弹性和流固耦合。通过初步参数研究，揭示了驱动频率和体刚度对速度与能耗的影响，为理解生物游动和软体仿生机器鱼设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有仿鱼机器人体内流固作用机制复杂，精准易于求解的动力学模型稀缺，而高效机器鱼的设计与控制离不开精确、可解释、且可计算的动力学建模。因此，需求建立能真实刻画柔性鱼体变形与流体耦合运动的模型，用于指导设计和优化。

Method: 采用哈密顿原理系统推导柔性鱼体大形变下的连续分布弹性动力学，同时引入流固耦合，实现自主推进模拟。通过变化驱动频率、体刚度和体长等参数，仿真分析其对游速与能耗的影响。

Result: 结果发现，尾部驱动频率提升会提高游动速度但降低能效（COT增大），速度和能耗随频率呈对立走势。体刚度和体长均存在优化值，能够达到速度和能耗的最佳平衡。

Conclusion: 提出的模型为分析复杂游动机制和软体仿生鱼机器人设计提供了理论工具，有助于揭示生物游动机理及提升仿生机器人性能。

Abstract: Fish-inspired aquatic robots are gaining increasing attention in research
communities due to their high swimming speeds and efficient propulsion enabled
by flexible bodies that generate undulatory motions. To support the design
optimizations and control of such systems, accurate, interpretable, and
computationally tractable modeling of the underlying swimming dynamics is
indispensable. In this letter, we present a full-body dynamics model for fish
swimming, rigorously derived from Hamilton's principle. The model captures the
continuously distributed elasticity of a deformable fish body undergoing large
deformations and incorporates fluid-structure coupling effects, enabling
self-propelled motion without prescribing kinematics. A preliminary parameter
study explores the influence of actuation frequency and body stiffness on
swimming speed and cost of transport (COT). Simulation results indicate that
swimming speed and energy efficiency exhibit opposing trends with tail-beat
frequency and that both body stiffness and body length have distinct optimal
values. These findings provide insights into biological swimming mechanisms and
inform the design of high-performance soft robotic swimmers.

</details>


### [208] [Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories](https://arxiv.org/abs/2509.16176)
*Yifan Lin,Sophie Ziyu Liu,Ran Qi,George Z. Xue,Xinping Song,Chao Qin,Hugh H. -T. Liu*

Main category: cs.RO

TL;DR: 本文提出了ACDC系统，实现了基于自然语言与无人机交流的自主航拍，无需专业知识即可获得优质无人机视频。


<details>
  <summary>Details</summary>
Motivation: 现有无人机航拍需要手动设定航点和视角，操作繁琐且效果不一致，限制了无人机在影视制作等领域的应用。

Method: 采用大语言模型（LLM）和视觉基础模型（VFM），把自然语言指令转化为可执行的无人机轨迹。方法包括视觉-语言检索选航点、基于偏好的贝叶斯优化调整姿态、以及运动规划生成安全轨迹。

Result: 通过模拟和硬件实验，验证了ACDC能在不同室内场景下，产生高质量、专业级无人机拍摄视频，无需用户具备机器人或影视知识。

Conclusion: ACDC证明了具身AI可通过对话指令实现全自主航拍作业，展示了语言与视觉模态结合在实际场景中闭环控制的潜力。

Abstract: We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic
Trajectories (ACDC), an autonomous drone cinematography system driven by
natural language communication between human directors and drones. The main
limitation of previous drone cinematography workflows is that they require
manual selection of waypoints and view angles based on predefined human intent,
which is labor-intensive and yields inconsistent performance. In this paper, we
propose employing large language models (LLMs) and vision foundation models
(VFMs) to convert free-form natural language prompts directly into executable
indoor UAV video tours. Specifically, our method comprises a vision-language
retrieval pipeline for initial waypoint selection, a preference-based Bayesian
optimization framework that refines poses using aesthetic feedback, and a
motion planner that generates safe quadrotor trajectories. We validate ACDC
through both simulation and hardware-in-the-loop experiments, demonstrating
that it robustly produces professional-quality footage across diverse indoor
scenes without requiring expertise in robotics or cinematography. These results
highlight the potential of embodied AI agents to close the loop from
open-vocabulary dialogue to real-world autonomous aerial cinematography.

</details>
