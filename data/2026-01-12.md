<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]
- [cs.CL](#cs.CL) [Total: 55]
- [cs.RO](#cs.RO) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 本文提出了一种新的分析框架BFD，深入揭示了视觉Transformer中自注意力机制在位置与内容信息上的交换方式，并分析了不同模型的注意力信息分布差异。


<details>
  <summary>Details</summary>
Motivation: 尽管自注意力机制是视觉Transformer的核心，但目前尚不清楚注意力在token间具体交流了哪些信息，尤其是区分位置和内容信息的作用。因此，作者希望深入剖析注意力层到底传递了什么，以及不同模型在信息交换机制上的差异。

Method: 提出双正交因子分解（BFD）分析框架，包括两个阶段：首先利用ANOVA方法将token激活分解为正交的位置和内容因子；其次对QK^T做SVD分解，分析查询和键之间的信息交互模式，以揭示注意力机制中不同信息因子的作用。该方法用于分析先进视觉Transformer，并定量对比无监督与有监督预训练模型的注意力特性。

Result: （1）注意力主要通过内容-内容交互实现信息传递，其次为内容-位置耦合，无监督模型（如DINOv2）在内容-位置耦合上分配更多计算能量并拥有更丰富的信息模式；（2）不同的注意力头以及头内特征呈现明显的“专门化”，分为内容-内容、内容-位置和位置-位置三类交互；（3）DINOv2在中间层表现出更好的形状整体表示能力，可同时保留位置信息并增强语义信息。

Conclusion: BFD首次系统量化了视觉Transformer中位置和内容信息因素对注意力机制的贡献，揭示了无监督方法如DINOv2在信息融合和表达上的优势，为设计和优化自注意力结构提供了新洞见。

Abstract: Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

</details>


### [2] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

TL;DR: 论文探索了视觉语言模型（VLMs）能否通过生成仿真代码，理解并再现图片中复杂系统的能力。结果表明VLMs能把握高层结构但难以还原细节。


<details>
  <summary>Details</summary>
Motivation: 机器智能能否像人类一样，通过图片构建世界的心理模型，并实现系统级理解，是人工智能中的核心挑战。本文希望评测目前顶尖VLMs在这方面的表现。

Method: 提出Im2Sim方法：先输入真实系统的图片给VLM，让其描述系统、生成能够模拟系统的代码，再由代码生成合成图片，并与原图对比，检测理解能力。测试对象涵盖来自自然与人造领域的各种复杂系统。

Result: 领先的VLM（如GPT、Gemini）能理解和建模多层次、跨领域复杂系统，但在还原精细结构及局部排列上表现有限。展现出模型对高层语义结构把握较好、细节认知不足的“非对称性”。

Conclusion: 当前VLMs已具有深入、高层图像理解和系统认知能力，但实际在细节拟合和微观仿真上仍有不少提升空间。

Abstract: The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

</details>


### [3] [STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs](https://arxiv.org/abs/2601.05364)
*Sudhakar Sah,Ravish Kumar*

Main category: cs.CV

TL;DR: 提出了两种新型轻量级神经网络STResNet（图像分类）和STYOLO（目标检测），在保持高精度的同时大幅优化延迟和内存消耗，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级网络在边缘设备上的应用仍需在精度和延时之间作权衡，且在微控制器和神经处理单元上的适用性有限。因此需要设计兼顾精度、效率与内存的新结构。

Method: 设计了STResNet和STYOLO网络系列，针对受限硬件平台，通过算法结构优化实现模型在准确率、计算效率和内存占用上的权衡优化。模型参数量均控制在较低范围，并于ImageNet和COCO等公开数据集上进行测试和对比。

Result: STResNet系列（Nano至Tiny）在四百万参数内取得有竞争力的ImageNet 1K准确率，其中STResNetMilli以三百万参数达到70.0% Top 1准确率，超越MobileNetV1和ShuffleNetV2。STYOLOMicro和STYOLOMilli在MS COCO数据集上分别达到30.5%与33.6%的mAP，优于YOLOv5n和YOLOX Nano。

Conclusion: STResNet和STYOLO在资源受限的边缘硬件环境下实现了高效、精确和小体积的模型，优于当前主流轻量级网络，为在微型硬件上部署深度学习应用提供了更优解决方案。

Abstract: Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.

</details>


### [4] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

TL;DR: 本文提出MOSAIC-GS方法，利用高斯涂抹（Gaussian Splatting），从单目视频中高效地重建高保真动态场景，兼具速度与质量，在多项实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 单目视频在动态场景三维重建领域存在固有难题，尤其缺乏多视角约束，导致几何恢复和时序一致性困难。现有方法在速度与精度间权衡不足，需新的高效高质量的重建方案。

Method: 利用多种几何线索（深度、光流、动态物体分割、点追踪），结合运动刚性约束，先在初始化阶段估算三维动态；之后运用光度优化提升精度，场景拆分为静态与动态，动态部分用随时间变化的Poly-Fourier曲线高效编码高斯的运动轨迹，实现紧凑表达、快速优化和实时渲染，并支持非刚性形变。

Result: MOSAIC-GS在多个标准单目动态场景基准测试中，优化与渲染速度显著快于现有方法，同时重建质量可与最新高水平方法媲美。

Conclusion: 该方法有效兼顾重建速度与精度，为单目动态场景三维重建提供了高效且精确的新思路。

Abstract: We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

</details>


### [5] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 该论文比较了放射组学、深度学习和集成方法在乳腺癌筛查中的诊断效果，发现集成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期诊断对于提高生存率至关重要。虽然放射组学和深度学习均能辅助影像医生早筛，但各方法在真实大规模数据集下的综合表现尚未充分评估。

Method: 作者采用两个独立数据集：RSNA 2023乳腺癌检测挑战数据集（11,913名患者）和墨西哥TecSalud数据集（19,400名患者）。使用ConvNeXtV1-small深度学习模型在RSNA上训练，在TecSalud上验证；放射组学模型用TecSalud开发并通过跨年度验证；集成方法将上述模型的输出进行组合和校准。

Result: 集成方法在乳腺癌诊断中AUC最高（0.87），高于单独深度学习模型（AUC=0.83）和放射组学模型（AUC=0.80）。

Conclusion: 将深度学习与放射组学预测结合的集成方法能显著提升基于乳腺X光片的乳腺癌诊断能力。

Abstract: Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

</details>


### [6] [EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning](https://arxiv.org/abs/2601.05379)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: 提出了EdgeLDR框架，将四元数神经网络的通道混合与块循环参数结构结合，通过FFT高效实现，显著提升了压缩和计算效率，在图像分类等数据集上取得了高压缩和优良准确率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在边缘设备上的部署受限于内存消耗和计算开销。四元数神经网络尽管参数高效，但权重仍为非结构化密集格式；结构化矩阵虽可提升计算效率，但多用于实数域。需要一种能兼具通道混合、参数压缩和高效计算的新方法。

Method: 提出EdgeLDR，将四元数块循环结构引入线性和卷积层，通过复数伴随表示实现基于FFT的高效运算，并与普通空间域方法进行了实现与对比。实验将其整合至紧凑CNN和Transformer架构，用于多个图像分类任务，细致评估了准确率与参数/延迟的权衡。

Result: FFT实现比空间域实现有明显加速，随着块规模增大，延迟基本稳定，实现了更大参数压缩因子的可行性。在标准数据集上，EdgeLDR层在保持竞争准确率的基础上，实现显著参数压缩，并在CPU/GPU上降低延迟。

Conclusion: EdgeLDR能够在不显著损失准确率的情况下减少参数量和计算延迟，适合于边缘设备高效部署，为四元数神经网络和结构化参数的新结合提供了实用方法。

Abstract: Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.

</details>


### [7] [Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation](https://arxiv.org/abs/2601.05394)
*Yuang Shi,Simone Gasparini,Géraldine Morin,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯表面（3DGS）表征的全新层次化自适应分类框架，将高斯分为描边和填色两类，提高了3D模型的渲染效率与质量。该方法无需外部3D线条，显著提升了模型压缩与流式传输能力。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯表征方法在高频（结构边界）和低频（纹理细节及体积）区域处理上方法单一，导致在模型压缩、流式传输和高效渲染时的结构表达与细节还原能力有限。作者发现高斯分布在三维渲染中有类比艺术绘画的作用——先勾勒再填色，由此提出了明确分工以提升表现力和效率。

Method: 方法创新地将高斯基元分为两类：描边高斯（Sketch Gaussians）专用于高频边界细节，补丁高斯（Patch Gaussians）负责低频平滑区域。提出了直接处理3DGS的新型层次化自适应分类框架，采用多指标密度聚类结合自适应质量优化，无需外部线条辅助。实现结构渐进式流式与更高效的参数编码。

Result: 在多种人造及自然3D场景测试中，新的高斯分层表示方法在等模型体积下，PSNR最大提升1.74 dB，SSIM提升6.7%，LPIPS降低41.4%。对于室内场景，仅保留0.5%原始模型体积即可保持优异视觉质量。

Conclusion: 该结构感知的高斯表示，有效提升了3D模型的自适应存储、渐进流式传输及高保真渲染能力，满足低带宽和受限设备的实际需求。

Abstract: We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.
  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.

</details>


### [8] [Multi-task Cross-modal Learning for Chest X-ray Image Retrieval](https://arxiv.org/abs/2601.05399)
*Zhaohui Liang,Sivaramakrishnan Rajaraman,Niccolo Marini,Zhiyun Xue,Sameer Antani*

Main category: cs.CV

TL;DR: 本文提出了一种多任务学习框架，对BiomedCLIP进行微调，以提升胸部X光片（CXR）跨模态检索的表现，并取得了比原始BiomedCLIP和CLIP模型更优的效果。


<details>
  <summary>Details</summary>
Motivation: CLIP和BiomedCLIP虽具备强大的跨模态嵌入能力，但未针对如通过CXR影像检索相关放射学报告等细粒度医学检索任务进行优化，因此需要改进以更好服务于临床实际需求。

Method: 以BiomedCLIP为基础，引入轻量级MLP投影头，使用多任务复合损失进行训练，包括：1）区分正常与异常CXR的二元交叉熵损失；2）强化类内一致性的有监督对比损失；3）保持跨模态对齐的CLIP损失。

Result: 微调后的模型在图像-文本和文本-图像双向检索任务上性能更均衡且更具临床意义。t-SNE可视化也显示正常与异常案例的语义聚类更加清晰，诊断灵敏度提升。

Conclusion: 领域自适应、多任务学习可有效提升医学领域的跨模态检索性能，对生物医学应用具有重要推动作用。

Abstract: CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.

</details>


### [9] [Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization](https://arxiv.org/abs/2601.05432)
*Yuxiang Ji,Yong Wang,Ziyu Ma,Yiming Hu,Hailang Huang,Xuecai Hu,Guanhua Chen,Liaoni Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了一种结合地图推理的图像地理定位方法，并通过强化学习与并行测试优化模型，在新构建的真实图片基准上取得显著突破。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型（LVLM）在图像地理定位任务中忽略了人类常用的“用地图思考”策略，仅依赖世界知识和推理能力。因此，需赋予模型地图推理能力，以提升定位效果。

Method: 作者将“用地图思考”能力形式化为“agent-in-the-map”循环，提出了两阶段优化方案：首先通过强化学习增强模型的选择与探索能力，再通过并行测试阶段让模型同时探索多条备选路径，从而提升地理定位准确性。并搭建了新的MAPBench基准以验证方法有效性。

Result: 实验结果显示，提出的方法在多个性能指标上优于现有开源和闭源模型。例如，Acc@500m指标从领先模型Gemini-3-Pro的8.0%提升到22.1%。

Conclusion: 结合地图推理与新型优化方案的图像地理定位方法能极大提升模型的实用性和准确性，并推动了该领域的基准建设。

Abstract: The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.

</details>


### [10] [TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection](https://arxiv.org/abs/2601.05446)
*Hongyang Xie,Hongyang He,Victor Sanchez*

Main category: cs.CV

TL;DR: 本文提出了一种新型的红外小目标检测网络TAPM-Net，通过显式建模目标诱导的特征扰动轨迹，实现了在复杂背景下更精确的小目标检测，实验结果达到最新最好水平。


<details>
  <summary>Details</summary>
Motivation: 红外小目标因信号对比度低、空间范围小且背景杂乱，在检测上长期存在挑战。现有的CNN和ViT方法虽有提升，但缺乏追踪小目标在特征空间中引发的有方向性的扰动机制，而这对区分信号与结构噪声至关重要。

Method: 本文提出了Trajectory-Aware Mamba Propagation Network (TAPM-Net)，核心包括扰动引导路径模块（PGM）和轨迹感知状态块（TASB）。PGM从多层特征构建扰动能量场并提取沿梯度的特征轨迹，TASB基于Mamba的状态空间单元，对每条轨迹进行速度约束扩散和语义对齐特征融合，实现方向性且具上下文感知的状态传播。

Result: 在NUAA-SIRST与IRSTD-1K红外小目标检测数据集上，TAPM-Net取得了优于当前所有方法的检测性能，验证了所提方法的有效性和优越性能。

Conclusion: 针对现有方法对特征扰动轨迹关注不足的问题，TAPM-Net通过创新性模块实现了小目标引发的扰动追踪，大幅提升了复杂背景下小目标的检测能力，具有理论创新和实际应用价值。

Abstract: Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.

</details>


### [11] [ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction](https://arxiv.org/abs/2601.05470)
*Tingwei Xie,Jinxin He,Yonghong Song*

Main category: cs.CV

TL;DR: 本文提出ROAP方法，通过建模文档阅读顺序和降低视觉噪声，提升视觉文档理解中的多模态transformer性能，无需修改主干模型架构，实验结果表现优秀。


<details>
  <summary>Details</summary>
Motivation: 多模态transformer在处理富含视觉信息的文档理解任务时存在两个主要问题：未显式建模逻辑阅读顺序，以及视觉token干扰文本语义关注。作者希望解决这两个限制，提升文档理解效果。

Method: 提出ROAP架构：1）使用Adaptive-XY-Gap(AXG-Tree)强健提取文档的分层阅读序列；2）将阅读序列信息通过Reading-Order-Aware Relative Position Bias(RO-RPB)引入注意力机制；3）采用Textual-Token Sub-block Attention Prior(TT-Prior)自适应降低视觉噪声，增强文本交互。整个方法无需改动预训练主干网络，只作为外部pipeline加入。

Result: 在FUNSD和CORD标准数据集上，ROAP在不改动主干的基础上显著提升了LayoutLMv3、GeoLayoutLM等主流骨干网络的性能。

Conclusion: 显式建模阅读顺序与合理调节多模态干扰对复杂布局文档理解至关重要。ROAP为此提供了轻量、可扩展的方案，对实际场景下的文档分析具有良好应用前景。

Abstract: The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.

</details>


### [12] [Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots](https://arxiv.org/abs/2601.05482)
*Shubham Agarwal,Ofek Nourian,Michael Sidorov,Sharon Chemweno,Ofer Hadar,Naftali Lazarovitch,Jhonathan E. Ephrath*

Main category: cs.CV

TL;DR: 该文提出了一种新型地下植物根系成像和增强系统，利用多视角和深度学习超分辨率方法，显著提升根系成像质量，为农业和生态领域的根系性状分析提供助力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于视觉的地下植物根系成像方法受根系遮挡、土壤水分变化及低对比度等不利条件影响，成像效果有限，因此亟需突破性的技术来提升根系成像质量，以推进土壤-植物相互作用等相关研究。

Method: 提出多视角成像系统，并基于深度学习的多图像超分辨（MISR）算法，充分利用多视角下的空间冗余信息，从而实现对地下根系的高分辨率重建。为训练与评测方法，构建了包含环境因素的合成地下成像数据集。

Result: 实验结果表明，所提MISR方法在BRISQUE指标上相较先进超分辨方法降低2.3%，图像质量显著提升且结构准确性更好，在CLIP-IQA分数不变的前提下，为根毛数量及密度等表型分析提供更准确的图像支持。

Conclusion: 该框架为地下根系成像与性状定量分析提供了稳定的自动化解决方案，对农业与生态学研究具有潜在推动作用。

Abstract: Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.

</details>


### [13] [Hippocampal Atrophy Patterns Across the Alzheimer's Disease Spectrum: A Voxel-Based Morphometry Analysis](https://arxiv.org/abs/2601.05494)
*Trishna Niraula*

Main category: cs.CV

TL;DR: 使用体素形态测量分析研究AD和MCI患者的灰质体积变化，揭示海马萎缩是AD进展的重要特征，并探索海马体积预测MCI发展为AD的潜力及APOE4基因影响。


<details>
  <summary>Details</summary>
Motivation: AD和MCI伴随有灰质流失，尤其在内侧颞叶结构，早期检测AD及评估不同风险因素（如遗传）对于理解和干预疾病进展非常重要。

Method: 对249名ADNI参与者的T1结构MRI影像采用CAT12/SPM12的体素基础形态学（VBM）分析，设置诊断组为主要自变量，校正年龄和颅内容积。统计图采用体素水平p<0.001阈值并以FWE方法进行簇水平多重校正。

Result: 与认知正常与MCI组相比，AD组出现显著的海马萎缩（Cohen's d=2.03, 1.61）。海马体积对MCI转化为AD有中等预测价值（AUC=0.66）。APOE4状态分层分析未发现其对基线海马体积有显著影响。

Conclusion: 内侧颞叶的神经变性是AD进展的关键生物标志。海马体积可作为MCI进展至AD的预测指标，遗传因素如APOE4对横断面海马体积影响有限。

Abstract: Alzheimer's disease (AD) and mild cognitive impairment (MCI) are associated with progressive gray matter loss, particularly in medial temporal structures. In this study, CAT12/SPM12 voxel-based morphometry was applied to baseline T1-weighted MRI scans from 249 ADNI participants (CN = 90, MCI = 129, AD = 30). Gray matter volume was analyzed using a general linear model, with the diagnostic group as primary predictor and age and total intracranial volume as covariates. Statistical maps were thresholded at p < 0.001 (voxelwise) and corrected for multiple comparisons at the cluster level using family-wise error (FWE) correction (p < 0.05). Significant hippocampal atrophy was observed in AD relative to CN and MCI (Cohen's d = 2.03 and 1.61, respectively). Hippocampal volume demonstrated moderate predictive value for conversion from MCI to AD (AUC = 0.66). Stratification by APOE4 status did not reveal significant genetic effects on cross-sectional hippocampal volume. These results support medial temporal degeneration as a key feature of AD progression and provide insights into predictive biomarkers and genetic influences.

</details>


### [14] [MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding](https://arxiv.org/abs/2601.05495)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出了一种新方法MMViR，以多模态和多粒度结构化方式提升长视频理解的效率与效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长视频内容复杂，现有多模态大模型难以有效处理其事件、场景和长距离依赖关系；直接编码视频计算量大，转文本冗余且碎片化，亟需新的高效理解方法。

Method: 开发了MMViR，通过检测关键转折点将长视频分段，并以三层结构描述视频，将全局叙事与细致视觉信息结合，支持高效检索与泛化能力。

Result: 在QA、摘要、检索三项任务中，MMViR相较最优基线，长视频理解提升19.67%，处理延迟降至原来的45.4%。

Conclusion: MMViR结构化表征能高效精准理解长视频，在多任务下通用性强，是长视频多模态理解的新方案。

Abstract: Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.

</details>


### [15] [FlyPose: Towards Robust Human Pose Estimation From Aerial Views](https://arxiv.org/abs/2601.05747)
*Hassaan Farooq,Marvin Brenner,Peter St\ütz*

Main category: cs.CV

TL;DR: 该论文提出FlyPose，一种专为无人机视角设计的轻量级人体姿态估计算法，有效提升了航拍环境下人体检测与姿态估计的精度与速度。


<details>
  <summary>Details</summary>
Motivation: 无人机越来越多地应用于与人类密切相关的场景，如快递、交通监控、基础设施检查等，对人体姿态与动作的精准感知要求更高。但当前方法在低分辨率、俯视角度和遮挡等无人机实际环境下表现不佳，且实时处理存在挑战。

Method: 提出FlyPose，一种面向航拍图像的轻量级top-down人体姿态估计流程；采用多数据集联合训练；针对无人机特殊视角及硬件条件优化，实现端到端高效推理。额外贡献了带有人工标注的航拍人体姿态新数据集FlyPose-104。

Result: 在人检任务上，FlyPose在Manipal-UAV、VisDrone、HIT-UAV及自建数据集上平均提升6.8 mAP；2D姿态估计任务上，在挑战性UAV-Human数据集提升16.3 mAP。系统在Jetson Orin AGX上推理延迟约20ms，并实现在无人机机载实时部署。

Conclusion: FlyPose有效解决了无人机航拍下人体姿态估计中低分辨率、俯视视角和遮挡等难题，实现了高精度、低延迟的姿态估计与检测，支持实际无人机应用，并对社区开放了新数据集。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.

</details>


### [16] [Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification](https://arxiv.org/abs/2601.05498)
*Samuel E. Johnny,Bernes L. Atabonfack,Israel Alagbe,Assane Gueye*

Main category: cs.CV

TL;DR: 本文提出了一种用于乳腺超声图像的多任务深度学习框架，能够同时进行病灶分割与分类，并在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像由于对比度低、斑点噪声大及病灶形态多样，导致肿瘤分割和分类仍面临挑战。

Method: 该研究基于Segment Anything Model（SAM）的视觉编码器，无需提示、采用全监督方式，将高维特征分别通过轻量卷积头或UNet型解码器实现像素级分割。分类分支结合基于掩码的注意力机制以突出病灶特征，抑制背景影响。

Result: 在PRECISE 2025乳腺超声数据集上，分割分支取得了0.887的Dice系数，分类准确率达92.3%，位列挑战赛排行榜前列。

Conclusion: 结合SAM特征和分割引导学习可显著提升乳腺超声病灶的分割与诊断预测能力。

Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.

</details>


### [17] [SceneFoundry: Generating Interactive Infinite 3D Worlds](https://arxiv.org/abs/2601.05810)
*ChunTeng Chen,YiChen Hsu,YiWen Liu,WeiFang Sun,TsaiChing Ni,ChunYi Lee,Min Sun,YuanFu Yang*

Main category: cs.CV

TL;DR: SceneFoundry提出了一种基于自然语言的扩散模型框架，能够自动生成具有可操作家具和多样化布局的3D室内环境，用于机器人学习和智能体研究。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景自动生成方法难以在真实世界室内场景中捕捉到功能复杂性，特别是包含可动部件的家具，这对机器人操控和导航很关键，因此亟需一种能生成功能性强、多样化布局的3D环境的新方法。

Method: 提出SceneFoundry框架，将LLM用于楼层布局生成，并通过扩散模型从3D资产库中采样、布置可动家具。为保障物理可用性，利用可微引导函数调控对象数量、避免部件碰撞、保证导航空间。整个流程支持从自然语言提示自动生成3D场景。

Result: SceneFoundry能高效生成结构合理、语义连贯且可供操控交互的多样化3D环境。实验表明该框架适用于多种场景类型和条件，生成环境具备较高功能性，对机器人训练和AI研究非常有用。

Conclusion: SceneFoundry显著提升了自动3D场景生成的功能复杂性和可交互性，支持可扩展的具身智能体AI研究和机器人应用。

Abstract: The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.

</details>


### [18] [Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors](https://arxiv.org/abs/2601.05508)
*Fuwen Luo,Zihao Wan,Ziyue Wang,Yaluo Liu,Pau Tong Lin Xu,Xuanjia Qiao,Xiaolong Wang,Peng Li,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法HieroSA，可以让多模态大模型（MLLMs）自动从象形文字位图中分析笔画结构，且无需人工设计数据，兼顾现代表意文字与古代象形文字的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前的大模型无论是处理文本还是图像，都难以捕捉象形文字内部的笔画与结构信息，且已有的结构分析方法依赖人工、依赖特定语言，不具备普适性。作者希望解决该类符号的自动结构建模问题。

Method: 提出Hieroglyphic Stroke Analyzer（HieroSA）框架，将字符图像转换为归一化坐标空间的线段（笔画）表示，不需专业人工标注。此外，方法支持现代与古代的多个表意文字体系，实现跨语种泛化。

Result: HieroSA可以有效抓取字符内部结构和语义特征，不依赖先验知识或语言特定信息，实验证明其在各种象形文字脚本上的有效性和通用性。

Conclusion: 该方法为多语种、多类型象形文字脚本的结构与语义分析提供了新的自动化工具，有助于进一步深入理解表意文字及象形文字系统。

Abstract: Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.

</details>


### [19] [Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals](https://arxiv.org/abs/2601.05848)
*Nate Gillman,Yinghua Zhou,Zitian Tang,Evan Luo,Arjan Chakravarthy,Daksh Aggarwal,Michael Freeman,Charles Herrmann,Chen Sun*

Main category: cs.CV

TL;DR: 本论文提出了Goal Force框架，使用户能以力向量和运动动态的方式设定视频生成模型的目标，从而实现物理任务的精准规划。该模型在基础物理合成数据上训练，但能在复杂实际场景中表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成世界模型很难通过文本或图片精确描述目标，特别是在涉及复杂物理动态的场景下，缺乏方便且物理直观的目标设定方式。

Method: 引入Goal Force框架，允许用户通过显式的力向量和中间动力学步骤设定目标，并基于合成基础物理数据（如弹性碰撞、多米诺骨牌倒塌等）训练视频生成模型，使其学会时空内力的传播。

Result: 模型仅在简单物理数据上训练，但能够零样本泛化到如工具操作、多物体因果链等复杂现实任务，展现出了高水平的物理决策能力。

Conclusion: 将物理相互作用作为基础，视频生成模型能够隐性地成为神经物理仿真器，实现无需外部引擎的精确物理规划，有望拓展机器人和规划领域应用。

Abstract: Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.

</details>


### [20] [GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting](https://arxiv.org/abs/2601.05511)
*Xuan Cheng,Jiahao Rao,Chengyang Li,Wenhao Wang,Weilin Chen,Lvqing Yang*

Main category: cs.CV

TL;DR: GaussianSwap提出了一种基于3D高斯点云(Gaussian Splatting)的人脸替换视频生成方法，实现了比现有像素级方法更高保真的“换脸”虚拟化身 (avatar)。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸替换方法仅能生成像素级别的人脸图像，无法进行动画化或交互操作，缺乏结构化和可控性。作者期望突破像素层面，生成具备身份迁移且可操控的高质量3D换脸视频化身。

Method: 方法首先对目标视频进行预处理，提取FLAME参数、相机位姿和分割掩码；然后将3D高斯点云与FLAME人脸模型配准，实现动态人脸控制；提出利用三种SOTA人脸识别模型融合的身份嵌入用于化身微调；最后渲染换脸化身于背景帧，合成最终视频。

Result: 实验结果显示，GaussianSwap在人脸身份保持、视觉清晰度和时序一致性方面优于现有技术，并支持以往无法实现的高质量交互式应用。

Conclusion: GaussianSwap实现了由像素生成向高保真的“换脸”数字虚拟化身的范式转变，在人脸换脸视频生成领域带来了更高水平的身份保真和可操控性，拓展了交互式应用场景。

Abstract: We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.

</details>


### [21] [SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances](https://arxiv.org/abs/2601.05535)
*Qiwei Yang,Pingping Zhang,Yuhao Wang,Zijing Gong*

Main category: cs.CV

TL;DR: 本文提出了一个适应尺度及整合形状先验的新型视频行人重识别方法SAS-VPReID，有效提升了极远距离与复杂背景下的识别准确率，在VReID-XFD基准上位居榜首。


<details>
  <summary>Details</summary>
Motivation: 极远距离下的视频行人重识别由于分辨率下降、视角变化大、外观噪声多，准确性大幅下降，亟需新颖方法提升特征表征能力。

Method: 方法包含三个关键模块：1)内存增强视觉主干（MEVB），借助CLIP视觉编码器和多代理记忆提升特征区分性；2)多粒度时序建模（MGTM），在多时间粒度下自适应挖掘运动信息；3)先验正则化形状动态（PRSD），捕捉身体结构动态。通过三者互补获得鲁棒表征。

Result: 在VReID-XFD基准上，各模块及整体方法实验结果突出，最终SAS-VPReID在该挑战排行榜取得第一。

Conclusion: 所提SAS-VPReID框架显著提升了极端条件下的视频行人重识别能力，展示出较强的实际应用前景。

Abstract: Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.

</details>


### [22] [DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion](https://arxiv.org/abs/2601.05538)
*Yiming Sun,Zifan Ye,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的DIFF-MF通道-空间状态空间模型，用于多模态图像融合。该方法通过差异驱动的特征提取和跨通道、跨空间的融合机制，在保证高效率的同时，有效提升了融合图像的质量。实验验证了其在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态图像融合方法在红外强度和可见光细节之间平衡不足，要么牺牲可见细节强调红外目标，要么反之，导致融合图像的信息损失。需要一种能够同时保留各模态优点的新方法。

Method: 提出DIFF-MF方法：基于差异驱动的通道-空间状态空间模型。克服现有方法的局限，融合流程包括特征差异图引导的特征提取、通道维度的交互增强（通过跨注意力和状态空间建模）、空间维度的跨模态融合（状态空间扫描），整体具备线性复杂度和全局建模能力。

Result: 在驾驶场景和低空无人机等公开数据集上，DIFF-MF方法在视觉质量和定量指标上均优于现有主流方法。具体实验结果证明了其融合效果和效率的提升。

Conclusion: DIFF-MF有效融合多模态图像的互补特征，避免了以往方法在红外与可见信息之间失衡的问题，兼顾了细节、目标显著性和计算效率，在实际应用中具有很强的竞争力。

Abstract: Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.

</details>


### [23] [MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation](https://arxiv.org/abs/2601.05546)
*Yanfeng Li,Yue Sun,Keren Fu,Sio-Kei Im,Xiaoming Liu,Guangtao Zhai,Xiaohong Liu,Tao Tan*

Main category: cs.CV

TL;DR: MoGen是一种多目标图像生成方法，通过区域语义锚点和自适应多模态引导模块，实现了基于语言描述的精细化和灵活控制的生成。


<details>
  <summary>Details</summary>
Motivation: 现有多目标图像生成方法难以实现依据语言描述的区域与语义精准对齐，常导致目标数量不一致及属性混淆。主流方法过度依赖外部控制信号，限制了输入的灵活性，难以适应多样化用户需求。

Method: 提出MoGen方法，设计区域语义锚点模块（RSA）用于将语言描述的短语单元准确锚定到图像区域，确保数量等约束；引入自适应多模态引导模块（AMG），可解析和集成多源控制信号，动态控制场景布局和对象属性。

Result: 实验显示，MoGen在生成质量、数量一致性和精细化控制上均显著优于现有方法，同时具有更强的易用性和灵活性。

Conclusion: MoGen实现了更精准、灵活和用户友好的多目标图像生成，为多约束条件下的文本到图像生成提供了有效方案。

Abstract: Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.

</details>


### [24] [VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck](https://arxiv.org/abs/2601.05547)
*Feiran Zhang,Yixin Wu,Zhenghua Wang,Xiaohua Wang,Changze Lv,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CV

TL;DR: 本文提出VIB-Probe，一种基于变分信息瓶颈（VIB）理论的视觉-语言模型（VLMs）幻觉检测与缓解方法，通过分析模型内部注意力头，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在多模态任务中表现卓越，但容易出现“幻觉”——生成的文本偏离实际视觉内容。现有检测方法多依赖输出logits或外部工具，缺乏对模型内部机制的理解，因此需要结构化、内在机制驱动的检测与干预新方法。

Method: 作者提出VIB-Probe框架，基于VIB理论，分析VLMs内部各层及注意力头的高维状态，提取辨别性强的特征并去除语义噪声。通过梯度分析识别与幻觉强相关的注意力头，并在推理阶段对这些头进行干预，有效检测并缓解模型幻觉。

Result: 在多个公开基准上，VIB-Probe在幻觉检测和缓解方面均显著超过当前最佳基线方法，验证了其优越性。

Conclusion: VIB-Probe利用信息瓶颈原理从模型内部机制入手，为VLMs幻觉检测与干预提供了创新且有效的新方案，有望提升多模态大模型生成内容的真实性。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.

</details>


### [25] [One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection](https://arxiv.org/abs/2601.05552)
*Bin-Bin Gao,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种极其简单、高效且通用的视觉异常检测方法UniADet，可以在无需数据集特定微调的情况下进行异常图片识别与分割，显著优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型的异常检测方法普遍依赖复杂的提示工程、适应模块和训练策略，导致其灵活性和通用性不足。

Method: 作者发现语言编码器用于异常分类和分割时其实并不必要，提出了完全解耦分类与分割及多层特征的极简方法，仅需独立学习各任务与层次的权重，显著简化了模型结构和参数量。

Result: UniADet方法结构简单、参数量极少（只有0.002M可学习参数）、适配多种基础模型，并在工业和医疗等14个真实异常检测基准上，突破性地大幅超越了现有零样本、少样本乃至全样本异常检测方法。

Conclusion: UniADet证明了极简方法同样能够实现通用且高效的异常检测，为实际应用提供了更简便和有效的新选择。

Abstract: Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.

</details>


### [26] [Semi-Supervised Facial Expression Recognition based on Dynamic Threshold and Negative Learning](https://arxiv.org/abs/2601.05556)
*Zhongpeng Cai,Jun Yu,Wei Xu,Tianyu Liu,Jianqing Sun,Jiaen Liang*

Main category: cs.CV

TL;DR: 本文提出了一种结合动态阈值调整（DTA）与选择性负学习（SNL）的半监督人脸表情识别算法，在RAF-DB与AffectNet数据集上取得了优异且领先的性能，甚至超过了部分全监督方法。


<details>
  <summary>Details</summary>
Motivation: 获取大量有标签的人脸表情数据成本高昂，因此亟需在标注数据有限的情况下，设计能有效利用未标注数据的半监督表情识别方法。

Method: （1）提出了特征提取阶段的局部注意力增强和特征图随机丢弃策略，以强化局部特征表示，并防止模型过拟合到局部区域；（2）提出动态阈值调整，以适应半监督学习框架下的表情识别需求；（3）通过选择性负学习策略，充分利用低置信度的未标注样本，挖掘补充性标签中的有效信息。

Result: 在RAF-DB和AffectNet这两个主流数据集上取得了最优（state-of-the-art）的性能，且即使未使用全部数据，也优于当前的全监督方法。

Conclusion: 所提出的半监督方法能充分利用有限的有标签和大量无标签数据，有效提升人脸表情识别的表现，具有良好的实际意义和推广价值。

Abstract: Facial expression recognition is a key task in human-computer interaction and affective computing. However, acquiring a large amount of labeled facial expression data is often costly. Therefore, it is particularly important to design a semi-supervised facial expression recognition algorithm that makes full use of both labeled and unlabeled data. In this paper, we propose a semi-supervised facial expression recognition algorithm based on Dynamic Threshold Adjustment (DTA) and Selective Negative Learning (SNL). Initially, we designed strategies for local attention enhancement and random dropout of feature maps during feature extraction, which strengthen the representation of local features while ensuring the model does not overfit to any specific local area. Furthermore, this study introduces a dynamic thresholding method to adapt to the requirements of the semi-supervised learning framework for facial expression recognition tasks, and through a selective negative learning strategy, it fully utilizes unlabeled samples with low confidence by mining useful expression information from complementary labels, achieving impressive results. We have achieved state-of-the-art performance on the RAF-DB and AffectNet datasets. Our method surpasses fully supervised methods even without using the entire dataset, which proves the effectiveness of our approach.

</details>


### [27] [What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews](https://arxiv.org/abs/2601.05563)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Dayang Li,Herun Wan,Wei Zhou,Min-Yen Kan*

Main category: cs.CV

TL;DR: 即使新闻预览内容本身真实，但在社交媒体上的标题配图可能因缺乏关键信息引发读者误解，与全文实际内容偏离，这种现象并不易被发现。本文提出了新的检测与纠正方法，并建立了相应基准和评测体系。


<details>
  <summary>Details</summary>
Motivation: 对比于明显虚假信息，新闻预览通过省略关键信息产生的潜在误导更难检测，学界对此关注不足。亟需系统性研究该类‘省略型’误导及其自动检测与修正手段。

Method: 作者设计了多阶段流程，区分并模拟新闻预览与全文理解之间的差异，推出了MM-Misleading基准数据集，评测多种开源大规模视觉语言模型（LVLMs）。提出了OMGuard系统，包括解读感知微调与基于推理的误导内容纠正两大技术。

Result: OMGuard系统使8B规模模型的检测能力提升至可与235B模型媲美，并且在端到端内容纠正上表现更优。分析发现，误导来源多为局部叙事信息缺失，部分场景仅靠文本难以修正，需视觉手段介入。

Conclusion: 省略型误导在社交媒体新闻中普遍而隐蔽，需要专门检测和纠正。OMGuard有效提升了模型检测及修正能力。未来需关注视觉与文本信息结合的干预手段，减少公众理解偏差。

Abstract: Even when factually correct, social-media news previews (image-headline pairs) can induce interpretation drift: by selectively omitting crucial context, they lead readers to form judgments that diverge from what the full article conveys. This covert harm is harder to detect than explicit misinformation yet remains underexplored. To address this gap, we develop a multi-stage pipeline that disentangles and simulates preview-based versus context-based understanding, enabling construction of the MM-Misleading benchmark. Using this benchmark, we systematically evaluate open-source LVLMs and uncover pronounced blind spots to omission-based misleadingness detection. We further propose OMGuard, which integrates (1) Interpretation-Aware Fine-Tuning, which used to improve multimodal misleadingness detection and (2) Rationale-Guided Misleading Content Correction, which uses explicit rationales to guide headline rewriting and reduce misleading impressions. Experiments show that OMGuard lifts an 8B model's detection accuracy to match a 235B LVLM and delivers markedly stronger end-to-end correction. Further analysis reveals that misleadingness typically stems from local narrative shifts (e.g., missing background) rather than global frame changes, and identifies image-driven scenarios where text-only correction fails, highlighting the necessity of visual interventions.

</details>


### [28] [Towards Generalized Multi-Image Editing for Unified Multimodal Models](https://arxiv.org/abs/2601.05572)
*Pengcheng Xu,Peng Tang,Donghao Luo,Xiaobin Hu,Weichu Cui,Qingdong He,Zhennan Chen,Jiangning Zhang,Charles Ling,Boyu Wang*

Main category: cs.CV

TL;DR: 本文针对统一多模态模型（UMMs）在多图像细节关联与一致性方面的局限，提出了两项核心方法，显著提升了多图像编辑任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前UMMs在处理多张输入图像、保持视觉一致性和细节消歧时存在困难，限制了多模态编辑体验，因此亟需改进。

Method: 提出了两项创新：（1）可学习的潜在分隔器，在潜空间中显式区分每张参考图片，实现精准而独立的条件控制；（2）正弦索引编码，为同一图片的视觉Token赋予连续正弦索引嵌入，实现图像身份区分，并对输入数量变化具备泛化能力。此外，构建了无伪影、可实现的高保真基准数据集用于训练与评测。

Result: 实验显示，在多图像编辑任务中，该方法在语义一致性、视觉保真度和跨图整合性方面均优于以往方法。

Conclusion: 该方法显著提升了UMMs在多图编辑中的一致性与泛化能力，为多模态生成和编辑任务带来有效进展。

Abstract: Unified Multimodal Models (UMMs) integrate multimodal understanding and generation, yet they are limited to maintaining visual consistency and disambiguating visual cues when referencing details across multiple input images. In this work, we propose a scalable multi-image editing framework for UMMs that explicitly distinguishes image identities and generalizes to variable input counts. Algorithmically, we introduce two innovations: 1) The learnable latent separators explicitly differentiate each reference image in the latent space, enabling accurate and disentangled conditioning. 2) The sinusoidal index encoding assigns visual tokens from the same image a continuous sinusoidal index embedding, which provides explicit image identity while allowing generalization and extrapolation on a variable number of inputs. To facilitate training and evaluation, we establish a high-fidelity benchmark using an inverse dataset construction methodology to guarantee artifact-free, achievable outputs. Experiments show clear improvements in semantic consistency, visual fidelity, and cross-image integration over prior baselines on diverse multi-image editing tasks, validating our advantages on consistency and generalization ability.

</details>


### [29] [Orient Anything V2: Unifying Orientation and Rotation Understanding](https://arxiv.org/abs/2601.05573)
*Zehan Wang,Ziang Zhang,Jiayang Xu,Jialei Wang,Tianyu Pang,Chao Du,HengShuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: Orient Anything V2是一种新的基础模型，能够从单张或多张图片中统一理解物体三维方向和旋转，尤其支持有多种旋转对称性的物体，并在多个基准任务上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统定向预测模型通常只考虑唯一的正面，难以处理具有对称性的物体，应用范围受到很大限制。此外，现有数据集也难以平衡多类别和多对称性物体，急需更通用和鲁棒的方法提升方向和旋转识别能力。

Method: 该论文提出了四项创新：（1）利用生成模型合成可扩展3D数据集，覆盖多类别并改善数据分布；（2）通过高效的模型辅助标注系统，自动识别每个物体的0到N个有效正面；（3）采用对称感知的周期分布拟合目标，有效捕捉所有可能的前向方向；（4）设计多帧架构，可直接预测物体的相对旋转。

Result: Orient Anything V2在11个广泛使用的基准数据集上，在方向估计、6DoF姿态估计和物体对称性识别等任务中均取得了最优的零样本性能，并展示出很强的泛化与适用性。

Conclusion: 该模型显著提升了物体三维方向与旋转理解的准确性与广泛性，为相关应用任务（如机器人、增强现实等）提供了更强有力的基础支持。

Abstract: This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.

</details>


### [30] [Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection](https://arxiv.org/abs/2601.05580)
*Hanyi Wang,Jun Lan,Yaoyu Kang,Huijia Zhu,Weiqiang Wang,Zhuosheng Zhang,Shilin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种三阶段持续学习框架，实现对不断演进的AI生成图像模型的连续适应，提升检测的泛化能力和准确性。通过大规模评测，方法在检测准确率上显著优于现有主流基线。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像的恶意使用和传播对信息真实性造成威胁，且检测方法难以适应不断涌现的新型生成模型。这使得现有检测技术在实际应用中易失效，因此亟需提升泛化与适应能力。

Method: 提出三阶段持续学习框架：(1) 采用高效参数微调，训练具备良好泛化性的离线检测模型；(2) 利用数据增强链与K-FAC近似Hessian，有效融入有限新样本，实现持续学习、减缓遗忘；(3) 采用线性模式连通性插值方法，进一步提取多生成模型间共性，提升整体检测性能。

Result: 建立了涵盖27种生成模型的基准数据集，并充分实验。离线检测模型mAP提升5.51%；持续学习平均准确率达92.2%，全面优于现有最优方法。

Conclusion: 该三阶段持续学习框架实现了对生成模型进化的强适应性与高检测准确率，为AI生成内容检测提供了有效实用的解决方案。

Abstract: The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.

</details>


### [31] [GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting](https://arxiv.org/abs/2601.05584)
*Nengbo Lu,Minghua Pan,Shaohua Sun,Yizhou Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GS-DMSR的新方法，用于3D动态场景重建，在加快模型收敛速度的同时提高渲染质量，特别适用于复杂动态场景。


<details>
  <summary>Details</summary>
Motivation: 当前3D动态场景重建领域存在模型收敛速度和渲染质量难以平衡的难题，尤其是在高精度建模复杂动态运动场景时问题更为突出。

Method: 提出GS-DMSR方法，通过定量分析高斯属性的动态演化，实现梯度自适应聚焦，动态识别模型中运动状态显著差异的部分，对不同显著程度的高斯模型采用差异化优化策略；引入多尺度流形增强模块，结合隐式非线性解码器和显式形变场进行协同优化，提高复杂形变场景建模效率。

Result: 在合成数据集上该方法达到96FPS的帧率，同时大幅减少存储开销与训练时间。

Conclusion: GS-DMSR有效平衡了模型收敛速度与渲染质量，提升了复杂动态场景重建的效率与实用性。

Abstract: In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.

</details>


### [32] [Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation](https://arxiv.org/abs/2601.05599)
*Takito Sawada,Akinori Iwata,Masahiro Okuda*

Main category: cs.CV

TL;DR: 本论文针对CNN对纹理偏倚影响提出了新衡量指标和高效适应方法，显著提升了对形状主导数据集的分类效果。


<details>
  <summary>Details</summary>
Motivation: CNN倾向于关注纹理而非全局形状，这在处理插画、素描等形状主导的数据时会导致性能下降，现有提升形状偏向的方法却缺乏衡量哪些数据集真正需要此改进的定量指标。

Method: 提出了一种基于结构相似性指数(SSIM)的新数据集形状-纹理平衡量化指标；基于该指标，设计了一种通过改变池化膨胀操作并保持卷积权重冻结，提升CNN形状偏倚的高效适应方法，仅需训练最终分类层。

Result: 所提方法在形状主导的数据集上，尤其是在样本较少的情况下，能稳定有效提升分类准确率，无需对全部网络进行微调。

Conclusion: 新指标可量化数据集是否偏向形状，所提适应策略简单高效，能在不同任务和数据规模下都增强CNN的形状识别能力。

Abstract: Convolutional Neural Networks (CNNs) are known to exhibit a strong texture bias, favoring local patterns over global shape information--a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this gap, we propose a data-driven metric that quantifies the shape-texture balance of a dataset by computing the Structural Similarity Index (SSIM) between each image's luminance channel and its L0-smoothed counterpart. Building on this metric, we further introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results show that this approach consistently improves classification accuracy on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.

</details>


### [33] [SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes](https://arxiv.org/abs/2601.05600)
*Chuhan Wang,Xintong Li,Jennifer Yuntong Zhang,Junda Wu,Chengkai Huang,Lina Yao,Julian McAuley,Jingbo Shang*

Main category: cs.CV

TL;DR: 该论文提出了SceneAlign框架，通过利用场景图进行结构化干预，有效提高了多模态大模型在复杂视觉推理场景中的答案准确率和推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在处理复杂视觉场景时，常常出现推理不真实的问题（如幻想实体、关系错误、步骤跳过等）。现有方法主要通过文本变体或基于答案的理由进行偏好学习，但无法防止模型利用语言偏见，绕过对视觉信息的真实依赖，因此需要更有效的对齐方法。

Method: 作者提出SceneAlign框架，利用场景图作为结构化视觉信息，通过识别推理关键节点并采用四种有针对性的扰动策略，模拟常见的视觉对齐失败情况，生成语言上自然但视觉上错误的对立推理对。这些对比样本结合直接偏好优化，用于训练模型实现更细粒度的结构化推理对齐。

Result: 在7个视觉推理基准测试上，SceneAlign方法显著提升了模型的答案准确率和推理忠实性。

Conclusion: SceneAlign证明了通过结构化的视觉信息干预，可以有效提升多模态大模型的推理能力和对视觉事实的忠实对齐，为多模态推理任务提供了新方法。

Abstract: Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.

</details>


### [34] [Learning Geometric Invariance for Gait Recognition](https://arxiv.org/abs/2601.05604)
*Zengbin Wang,Junjie Li,Saihui Hou,Xu Liu,Chunshui Cao,Yongzhen Huang,Muyi Sun,Siye Wang,Man Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种通过几何变换建立不同步态条件之间关系的新方法，实现更鲁棒的步态识别。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别研究大多依赖数据驱动方式隐式拉近不同步态条件之间的特征，很少有工作显式探讨不同条件之间的内在联系。作者希望通过更具解释性的方式提升步态识别系统的泛化能力。

Method: 作者提出将不同步态条件间的变化视为若干常见几何变换（反射、旋转、缩放）的组合。为此，设计了RRS-Gait框架，首先让卷积核可根据具体几何变化灵活调整，从而获得近似等变特征，随后通过全局池化实现特征不变性学习。

Result: 在四个主流步态数据集（Gait3D、GREW、CCPG、SUSTech1K）上进行了大量实验，结果显示该方法在多种步态情况下都有优越表现。

Conclusion: 通过将步态变化建模为几何变换并融合几何不变性学习，能够有效提升不同条件下的步态识别性能，为步态识别提供了新的视角和方法。

Abstract: The goal of gait recognition is to extract identity-invariant features of an individual under various gait conditions, e.g., cross-view and cross-clothing. Most gait models strive to implicitly learn the common traits across different gait conditions in a data-driven manner to pull different gait conditions closer for recognition. However, relatively few studies have explicitly explored the inherent relations between different gait conditions. For this purpose, we attempt to establish connections among different gait conditions and propose a new perspective to achieve gait recognition: variations in different gait conditions can be approximately viewed as a combination of geometric transformations. In this case, all we need is to determine the types of geometric transformations and achieve geometric invariance, then identity invariance naturally follows. As an initial attempt, we explore three common geometric transformations (i.e., Reflect, Rotate, and Scale) and design a $\mathcal{R}$eflect-$\mathcal{R}$otate-$\mathcal{S}$cale invariance learning framework, named ${\mathcal{RRS}}$-Gait. Specifically, it first flexibly adjusts the convolution kernel based on the specific geometric transformations to achieve approximate feature equivariance. Then these three equivariant-aware features are respectively fed into a global pooling operation for final invariance-aware learning. Extensive experiments on four popular gait datasets (Gait3D, GREW, CCPG, SUSTech1K) show superior performance across various gait conditions.

</details>


### [35] [LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction](https://arxiv.org/abs/2601.05611)
*Chengen Xie,Bin Sun,Tianyu Li,Junjie Wu,Zhihui Hao,XianPeng Lang,Hongyang Li*

Main category: cs.CV

TL;DR: 提出了一种无需语言注释的新型端到端自动驾驶模型LatentVLA，解决稀有场景泛化性弱和多步推理效率低等问题，实现了领先性能与高效实时性。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶模型对常见场景表现良好，但在稀有的长尾情况中泛化能力不足，部分原因是训练数据场景多样性有限。虽然最近基于视觉-语言-动作（VLA）的方法试图利用预训练模型的广泛知识进行提升，却带来了数值精度不足（离散化问题）、对语言注释依赖性高（导致偏见与负担）、以及多步推理计算效率低等新挑战。

Method: 提出了LatentVLA框架，采用自监督潜在动作预测来训练VLA模型，不需要语言注释，从无标签轨迹数据中学习丰富的驾驶表征。通过知识蒸馏，LatentVLA将VLA模型的泛化能力迁移给高效的纯视觉网络，实现了性能和实时效率的平衡。

Result: LatentVLA在NAVSIM基准集上取得了92.4的PDMS分数，刷新了最新最佳记录，并在nuScenes基准上展现了优秀的零样本泛化能力。

Conclusion: LatentVLA突破了以往VLA模型对语言注释的依赖，消除了语言偏见、减轻了注释负担，并通过自监督和知识蒸馏兼顾了泛化能力和部署效率，为自动驾驶场景的通用性和实用性带来了新进展。

Abstract: End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.

</details>


### [36] [Compressing image encoders via latent distillation](https://arxiv.org/abs/2601.05639)
*Caroline Mazini Rodrigues,Nicolas Keriven,Thomas Maugey*

Main category: cs.CV

TL;DR: 提出了一种通过简化蒸馏方法，将图像压缩模型的庞大编码器压缩为轻量化编码器，减少数据和训练时间，同时保证了重建质量，非常适合硬件受限环境。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像压缩模型虽然重建质量高，但模型复杂、庞大且对硬件资源、数据量和计算需求高，难以在受限设备上应用。因此需要有效方法降低模型规模和资源消耗。

Method: 采用简化的知识蒸馏策略，通过用较少的数据和更短的训练时间，将原始重型编码器的潜在空间映射知识转移到更小的编码器中，实现部分网络压缩，即“轻量编码器”。

Result: 在两种不同的架构和图片压缩任务上实验，结果显示：用该方法训练得到的轻量编码器较传统直接用原损失训练的小型编码器，能更好地保留重建质量和统计一致性。

Conclusion: 所提方法有效实现了图像压缩模型编码器的轻量化，为资源受限环境下应用高质量深度学习压缩模型提供了一种实用方案。

Abstract: Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.

</details>


### [37] [SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving](https://arxiv.org/abs/2601.05640)
*Jingyu Li,Junjie Wu,Dongnan Hu,Xiangkai Huang,Bin Sun,Zhihui Hao,Xianpeng Lang,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出了SGDrive框架，通过引入驾驶特有的场景-代理-目标分层结构，优化现有视觉语言模型（VLM）在自动驾驶中的空间-时间表示能力，提升了基于摄像头的自动驾驶系统的规划表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLM虽能提升自动驾驶场景下的规划能力，但普遍缺乏针对驾驶情境的空间推理和运动模式建模，难以构建对安全驾驶所需的几何关系和动态环境的有效表征。

Method: 提出SGDrive框架，将VLM的表示学习结构化为场景-代理-目标的知识分层。该层级模仿人类驾驶决策流程：先感知环境（场景）、关注关键行为体及其行为（代理），最后制定短时目标并执行行动，实现多层信息融合，助力轨迹规划。

Result: 在NAVSIM基准测试中，SGDrive在PDMS和EPDMS任务上，均取得了目前摄像头方法中的最优性能，显示出其结构化知识设计的有效性。

Conclusion: SGDrive通过引入驾驶知识分层，使通用VLM能够更加适配复杂的自动驾驶任务，证明了结构化空间-时间表达在提升轨迹规划表现中的重要作用。

Abstract: Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.

</details>


### [38] [SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More](https://arxiv.org/abs/2601.05688)
*Muye Huang,Lingling Zhang,Yifei Li,Yaqiang Wu,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了SketchVL模型，结合了新颖的细粒度信用分配RL算法（FinePO），实现了对自动图表理解中推理过程的精细优化，实验显示该方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动图表理解对于现有多模态大模型（MLLMs）来说是一个挑战，主要原因在于复杂推理过程中的正确与错误难以区分和精确奖励。传统基于RL的MLLMs在整个推理链的信用分配上存在不足，难以实现每一步的精细优化。作者意在突破这个瓶颈，提高模型在复杂、细致视觉推理场景下的表现。

Method: 本文提出SketchVL，一种通过可视化中间推理步骤，并送回模型自身进行多步推理的新型多模态大模型。其训练过程采用FinePO RL算法，该算法引入详细评分机制（FinePRM），对每一步画图操作进行信用分配，实现细粒度奖励或惩罚，强化正确行为，引导优化每个推理步骤。

Result: 实验结果表明，SketchVL在图表、自然图像和数学数据集上的平均性能较基础模型提升了7.23%，模型能够更好地对齐自己的推理行为与细粒度评估标准。

Conclusion: SketchVL和FinePO方法有效解决了自动图表理解中的细粒度推理优化难题，推进了多模态大模型在高密度视觉数据推理与分析中的能力，为后续相关模型的训练和应用提供了新方向。

Abstract: Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.

</details>


### [39] [Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation](https://arxiv.org/abs/2601.05722)
*Jin Wang,Jianxiang Lu,Comi Chen,Guangzheng Xu,Haoyu Yang,Peng Chen,Na Zhang,Yifan Xu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: RCM是一种创新的高质量图像到3D角色生成的方法，能够从单张或多张图像生成清晰的3D角色，并实现高分辨率、多角度的视图合成，效果超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像生成高质量3D角色极具挑战，主要难题在于复杂姿态和自身遮挡，现有方法在合成质量和多视角一致性上存在不足。

Method: 提出RCM，一种基于扩散模型的图像到视频框架，通过将任意姿态角色转化为规范姿态，实现一致的多角度合成。其特点包括高分辨率（1024x1024）、支持可控起拍摄视角和多视角输入（最多4张图像）。

Result: RCM在新视角生成和3D角色生成的实验中，全面优于最新主流方法，在分辨率、姿态处理和多视角条件下表现尤为突出。

Conclusion: RCM方法有效突破了单图高质量3D角色生成的传统难题，适应复杂姿态与多场景，具有很强的实际应用前景。

Abstract: Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.

</details>


### [40] [TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment](https://arxiv.org/abs/2601.05729)
*Jin Wang,Jianxiang Lu,Guangzheng Xu,Comi Chen,Haoyu Yang,Linqing Wang,Peng Chen,Mingtao Chen,Zhichao Hu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: 提出了TAGRPO，一种基于对比学习的图像到视频模型（I2V）后训练新方法，能显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 以往将Group Relative Policy Optimization（GRPO）用于I2V时常无法稳定获得奖励提升，需要新的优化方式提升I2V生成效果。

Method: 受对比学习启发，作者观察到同一噪声出发的rollout视频能更好引导优化。据此，提出了TAGRPO：在中间潜变量施加新颖的GRPO损失，使高奖励轨迹对齐、低奖励轨迹远离；引入rollout视频的记忆库增强多样性并减少计算。

Result: TAGRPO在I2V任务上显著优于DanceGRPO，带来稳定的奖励提升和生成质量改善。

Conclusion: TAGRPO是简单有效的I2V后训练方法，为I2V模型优化和应用拓展提供了新方案。

Abstract: Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.

</details>


### [41] [FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time](https://arxiv.org/abs/2601.05738)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 该论文提出了一个结合高效相机追踪和高质量特征丰富建图的实时SLAM系统，利用3D高斯斑点（3DGS），在标准数据集上达到了更低的位姿误差和更高的建图精度。


<details>
  <summary>Details</summary>
Motivation: 现有语义SLAM系统多依赖预定义类别标签，灵活性不足，且很难兼顾实时性、追踪稳定性与精细的语义建图。作者希望通过引入3DGS和视觉基础模型，提升SLAM系统在实时性、建图精度与语义丰富度方面的表现。

Method: 系统将密集特征光栅化与3DGS新视角合成结合，并用视觉基础模型对特征空间对齐，实现了语义增强的三维重建。其核心改进是用特征嵌入代替传统的预设标签，在追踪与建图过程中支持开放集分割，并且能够实现实时处理。

Result: 在标准评测数据集上，所提方法的实时追踪性能与当前顶尖系统持平，同时追踪更稳定，地图质感更高。实验结果显示，相比近期的定集SLAM系统，位姿误差降低9%，建图精度提升8%。语义与语言分割结果也与离线3DGS方法相当。

Conclusion: 该系统不仅提升了SLAM中追踪和建图的底层表现，也为下游任务如多视角、开放集分割等提供了基础，兼顾了实时性与精度，有望推动语义丰富、灵活SLAM的发展。

Abstract: We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.

</details>


### [42] [ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers](https://arxiv.org/abs/2601.05741)
*Guray Ozgur,Eduarda Caldeira,Tahar Chettaoui,Jan Niklas Kolf,Marco Huber,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出一种新的无训练方法ViTNT-FIQA，仅需一次前向传播即可评估人脸图像质量，并在多个数据集上达到与现有方法媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 当前人脸图像质量评估依赖最终层特征或需要多次计算，效率与灵活性都有限。人脸识别对图像质量敏感，因此亟需一种高效、泛用、无训练的人脸质量评估方法。

Method: ViTNT-FIQA利用预训练视觉Transformer（ViT）的中间层patch嵌入，在不同Transformer Block间计算L2归一化patch嵌入连续帧之间的欧氏距离，衡量特征演化稳定性，据此为每张图像生成质量分数，无需回传或改变网络结构。

Result: 在带有合成控制退化标签的数据集上实验证实，该分数与图像质量高度相关，并在八个人脸识别基准（LFW、AgeDB-30、CFP-FP等）上测试显示，与主流方法性能相当。

Conclusion: ViTNT-FIQA无需训练、一次前向即可适配任何ViT face model，兼具高效性与高精度，便于现有系统集成，在人脸图像质量评估领域具较大实用价值。

Abstract: Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.

</details>


### [43] [Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification](https://arxiv.org/abs/2601.05785)
*Quanjiang Li,Zhiming Liu,Tianxiang Xu,Tingjin Luo,Chenping Hou*

Main category: cs.CV

TL;DR: 本文提出了一种名为ADRL（Adaptive Disentangled Representation Learning）的多视角多标签学习新方法，有效应对特征缺失和标签不完整问题，并在多个公开数据集和实际应用中表现优越。


<details>
  <summary>Details</summary>
Motivation: 多视角多标签学习常面临特征缺失和标签不完整的问题，这主要是由于数据采集受限及标注成本高昂所致。现有方法在特征恢复、表征解耦和标签语义建模方面仍存在不足，迫切需要更健壮有效的解决方案。

Method: ADRL方法通过邻域感知机制在模态间传播特征亲和性实现视角补全，并采用随机掩码策略增强重构能力。通过标签的类别级相关性传播优化标签分布参数，使用互信息目标促进共享表征一致性并减少视角特有信息重叠。方法中还引入了原型特异性的特征选择及伪标签生成，通过伪标签空间结构特性指导多视角的判别性融合。

Result: 在多个公开数据集及实际场景应用上的大量实验表明，ADRL在特征恢复、标签推断及整体识别性能方面均优于当前主流方法。

Conclusion: ADRL为多视角多标签学习中存在的特征缺失与标签不全问题提供了高效、可泛化的解决方案，验证了其理论与实验上的有效性。

Abstract: Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.

</details>


### [44] [Boosting Latent Diffusion Models via Disentangled Representation Alignment](https://arxiv.org/abs/2601.05823)
*John Page,Xuesong Niu,Kai Wu,Kun Gai*

Main category: cs.CV

TL;DR: 本文提出了Send-VAE（一种语义解耦VAE），通过对齐预训练视觉基础模型（VFM）的语义层级，实现高效和结构化的属性编码，不仅提升了生成质量，还显著加快了训练速度，在ImageNet上取得了最新的生成效果。


<details>
  <summary>Details</summary>
Motivation: 以往VAE和LDM都采用同一VFM对齐目标，忽视了二者对表示空间需求的本质不同——LDM更需高层次语义保留，而VAE更需属性解耦。本研究希望解决VAE在语义解耦上的不足，使其能更结构化地编码属性信息，提高生成友好性。

Method: 提出Send-VAE，首先设计非线性映射器将VAE潜变量映射并对齐到VFM的语义层次，通过这种机制增强VAE的属性级解耦能力。采用线性探针进行属性预测任务，验证语义解耦能力。同时用Send-VAE训练flow-based的transformer模型SiTs，对比生成表现和训练速度。

Result: Send-VAE在属性预测任务中表现出良好的语义解耦能力，其解耦程度与生成质量显著相关。借助Send-VAE训练的SiTs，比现有方法显著加快了训练速度，在ImageNet 256x256数据集上（有无分类器自由引导）均取得了最新的FID成绩（1.21/1.75）。

Conclusion: 本文方法不仅解决了VAE在语义解耦和结构化属性表示上的短板，还有效提升了生成模型的效率与效果，推动了生成模型领域VAE设计和应用的进展。

Abstract: Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.

</details>


### [45] [GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras](https://arxiv.org/abs/2601.05839)
*Weimin Liu,Wenjun Wang,Joshua H. Meng*

Main category: cs.CV

TL;DR: GeoSurDepth是一种新颖的周视深度估计框架，通过几何一致性约束增强三维场景理解，显著提升自动驾驶系统的深度感知能力，并在主流数据集上达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有周视深度估计方法多侧重光度一致性，很少直接挖掘单目和周视图自身的丰富几何结构信息。这限制了三维场景理解的鲁棒性和精度，尤其是在自动驾驶等场景下，对更高可靠性的需求推动了研究者寻找更有效的几何建模方法。

Method: 提出了GeoSurDepth框架，核心在于以几何一致性为深度推断主要线索。通过利用foundation models提供伪几何先验和特征提升，引导网络在3D空间中保持表面法线一致性，并在2D中正则化物体和纹理的一致深度。同时引入创新的视角合成方法，利用稠密深度和空间偏移进行2D-3D提升，补偿单视角重建的不足。此外，提出自适应联合运动学习策略，增强网络提取关键空间几何线索的能力。

Result: GeoSurDepth在DDAD和nuScenes数据集上实现了SOTA（state-of-the-art）性能，显著优于以往方法，表明该方法在自监督多视角深度估计中的有效性。

Conclusion: 通过充分挖掘几何一致性和结构信息，GeoSurDepth框架为自动驾驶等场景的稳健多视角深度估计提供了新的解决思路和技术路径。

Abstract: Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.

</details>


### [46] [Kidney Cancer Detection Using 3D-Based Latent Diffusion Models](https://arxiv.org/abs/2601.05852)
*Jen Dusseljee,Sarah de Boer,Alessa Hering*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的新型 3D 肾脏异常检测方法，用于对比增强腹部 CT 扫描。该方法无需像以往那样进行切片级分析，而是直接处理图像体积，并利用仅有的病例级伪标签进行弱监督。实验显示，该方法在无需精细标注的情况下可行且有前景，但当前性能仍略逊于有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有腹部肾脏异常检测多依赖切片级分析和详细标注，实际中高质量标注成本高昂。为降低人工成本，提高模型泛化能力，亟需探索基于弱监督和生成式建模的方法，对复杂三维解剖结构实现高效异常检测。

Method: 方法整合了去噪扩散概率模型（DDPM）、去噪扩散隐式模型（DDIM）和矢量量化对抗生成网络（VQ-GAN），直接对三维 CT 图像体积进行处理。创新之处在于利用病例级伪标签实现弱监督训练，大大减少了对详细像素级标注的依赖。

Result: 将所提方法与当前主流有监督分割和检测模型进行对比，结果表明该方法在弱监督情境下能够有效进行 3D 肾脏异常检测，虽然与有监督基线尚有差距，但已具备可行性和发展潜力，尤其在提升重建精度和病灶定位方面显示出改进空间。

Conclusion: 本研究首次验证了 3D 潜在扩散模型在弱监督异检测中的可行性和应用前景，为构建高效、生成式的腹部复杂结构模型提供了重要基础，未来可进一步优化模型实现更优性能。

Abstract: In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.

</details>


### [47] [LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting](https://arxiv.org/abs/2601.05853)
*Yinghan Xu,John Dingliana*

Main category: cs.CV

TL;DR: 本文提出了一个新颖的3D多层人体重建框架，可以分别重建身体和服装层，使虚拟人像更加真实且可动画化。


<details>
  <summary>Details</summary>
Motivation: 现有的单层3D人体重建方法会将服装紧锁在某一身份上，缺乏灵活性；而多层方法又常因遮挡问题难以准确重建被遮挡区域。本文旨在突破这两个瓶颈，提高重建质量和虚拟服饰更换的真实感。

Method: 本方法将各个层（身体/衣物）编码为2D高斯分布集合，以提升几何与渲染效果，并用预训练的2D扩散模型结合SDS技术修补被遮挡区域。训练分为三步：1）单层粗重建服装；2）多层联合优化提取身体与衣物细节。

Result: 在4D-Dress与Thuman2.0数据集上，方法显著提升了渲染质量与分层重建效果，分层重组比现有SOTA更准确、真实，支持新角度下的虚拟试衣。

Conclusion: 该方法为虚拟人像与沉浸式应用的高保真3D资产创建提供了更好的解决方案。代码已开源，具有实际应用前景。

Abstract: We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS

</details>


### [48] [Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation](https://arxiv.org/abs/2601.05855)
*Kaiwen Huang,Yizhe Zhang,Yi Zhou,Tianyang Xu,Tao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双向通道选择语义交互（BCSI）框架，用于半监督医学图像分割，通过引入多种机制促进有标签和无标签数据的高效协同和特征交互，有效提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割主流方法（如mean teacher等）存在模型结构复杂、误差积累严重和有无标签数据流缺乏互动等问题。作者旨在解决这些不足，提升模型效果和泛化能力。

Method: 1）提出语义-空间扰动（SSP）机制，利用两种强增强操作与伪标签进行无监督学习，并通过强增强结果之间的一致性约束提升稳定性与鲁棒性；2）提出通道选择路由器（CR）组件，动态选择最相关的信息通道进行有无标签数据的特征交换，减少噪音干扰；3）采用双向通道交互（BCI）策略，补充语义信息并提升重要通道的表征能力。

Result: 在多个主流3D医学图像数据集上，所提出方法在分割性能上显著优于现有各种半监督方法。

Conclusion: BCSI框架通过一系列创新机制实现了有标签与无标签数据的高效特征交互，不仅增强了模型的稳定性和鲁棒性，还显著提升了分割准确率，能够更好地应用于有限标注的实际医学场景。

Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.

</details>


### [49] [Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection](https://arxiv.org/abs/2601.05861)
*Zhen-Xin Lin,Shang-Kuan Chen*

Main category: cs.CV

TL;DR: 本文提出了Phase4DFD, 一种结合相位信息、幅值信息和空间特征的深伪检测方法，通过引入相位感知注意力机制，有效提升了检测精度，且计算开销较低，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 大多数深伪检测方法仅关注频域中的幅值信息，忽略了相位信息的作用，而相位信息往往携带难以伪造的关键特征。鉴于此，本文旨在通过显式建模相位与幅值的相互关系，提高深伪检测的可靠性与鲁棒性。

Method: 提出Phase4DFD框架，将快速傅立叶变换（FFT）得到的幅值和相位、本地二值模式（LBP）等频域特征，与RGB图像信息结合，利用可学习的相位感知注意力模块，重点关注由伪造过程引入的相位不连续性，在主干特征提取前进行特征引导，并采用高效的BNext M骨干网络以及可选的通道空间注意力机制进一步细化特征。

Result: 在CIFAKE与DFFD深伪检测数据集上，Phase4DFD在检测准确率等指标上超越了现有空间域和频域方法，并保持较低的运算资源消耗。

Conclusion: 显式结合频域的相位信息可为深伪检测模型带来额外、非冗余的信息，提升检测性能。Phase4DFD验证了这种创新思路的有效性和先进性，为后续相关研究提供了重要参考。

Abstract: Recent deepfake detection methods have increasingly explored frequency domain representations to reveal manipulation artifacts that are difficult to detect in the spatial domain. However, most existing approaches rely primarily on spectral magnitude, implicitly under exploring the role of phase information. In this work, we propose Phase4DFD, a phase aware frequency domain deepfake detection framework that explicitly models phase magnitude interactions via a learnable attention mechanism. Our approach augments standard RGB input with Fast Fourier Transform (FFT) magnitude and local binary pattern (LBP) representations to expose subtle synthesis artifacts that remain indistinguishable under spatial analysis alone. Crucially, we introduce an input level phase aware attention module that uses phase discontinuities commonly introduced by synthetic generation to guide the model toward frequency patterns that are most indicative of manipulation before backbone feature extraction. The attended multi domain representation is processed by an efficient BNext M backbone, with optional channel spatial attention applied for semantic feature refinement. Extensive experiments on the CIFAKE and DFFD datasets demonstrate that our proposed model Phase4DFD outperforms state of the art spatial and frequency-based detectors while maintaining low computational overhead. Comprehensive ablation studies further confirm that explicit phase modeling provides complementary and non-redundant information beyond magnitude-only frequency representations.

</details>


### [50] [Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens](https://arxiv.org/abs/2601.05927)
*Yohann Perron,Vladyslav Sydorov,Christophe Pottier,Loic Landrieu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种适用于超高分辨率图像分割的新方法，通过多尺度视觉transformer架构，有效兼顾全局与局部信息，实验验证显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前超高分辨率图像分割常因滑动窗口方案而丢失全局语境，或因下采样损失细节，因此亟需一种结合全局感知和局部细节表达的高效Transformer方法。

Method: 作者设计了显式多尺度推理结构，包括局部分支（高分辨率小块）和全局分支（低分辨率大块）并行处理图像，通过少量可学习relay tokens在两分支间传递特征，且结构可直接集成于ViT和Swin等现有transformer主干网络中，新增参数极少。

Result: 在Archaeoscape、URUR、Gleason三大超高分辨率分割基准和Cityscapes常规数据集上进行广泛实验，均获得了稳定提升，最多提升15%的mIoU。

Conclusion: 所提方法在保持结构简洁和参数量低的同时，显著增强了超高分辨率图像分割的性能，在多个基准上均展现了优越性。

Abstract: Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .

</details>


### [51] [Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets](https://arxiv.org/abs/2601.05937)
*Pankaj Gupta,Priya Mudgil,Niharika Dutta,Kartik Bose,Nitish Kumar,Anupam Kumar,Jimil Shah,Vaneet Jearth,Jayanta Samanta,Vishal Sharma,Harshal Mandavdhare,Surinder Rana,Saroj K Sinha,Usha Dutta*

Main category: cs.CV

TL;DR: 本文提出并评估了一种基于Vision Transformer的深度学习模型，用于胰腺肿瘤超声内镜（EUS）图像的分割，旨在减少人为主观性并提升诊断准确性。模型在多个数据集上表现良好，但还存在一定局限性。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌恶性程度高，生存率低，早期诊断至关重要。EUS虽然是重要的检测手段，但评估依赖操作者经验，主观性强。开发自动化、客观的分割技术可辅助诊断，提高准确率和一致性。

Method: 使用USFM框架和Vision Transformer骨干网络，基于17,367张EUS公共数据集图像进行5折交叉验证训练，并在350张独立外部测试集数据进行验证。图像做了灰度、裁剪和归一化处理。评价指标包括DSC、IoU、敏感性、特异性和准确率。

Result: 模型在内部交叉验证中和外部测试集上分别取得了DSC约0.65、IoU约0.6，敏感性70%左右、特异性接近98%，一致性较好，但有9.7%的样本预测出错。

Conclusion: 提出的Vision Transformer模型在胰腺肿瘤EUS图像分割上具有良好表现，但受限于数据集异质性与外部验证有限，尚需进一步完善、标准化，并开展前瞻性研究。

Abstract: Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.

</details>


### [52] [Context-Aware Decoding for Faithful Vision-Language Generation](https://arxiv.org/abs/2601.05939)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: 本论文针对大型视觉语言模型（LVLMs）在图像描述和视觉推理等任务中出现幻觉（即生成与视觉输入不符的响应）的问题，提出了一种无须重新训练的缓解方法Context Embedding Injection（CEI），有效降低了模型的幻觉率。


<details>
  <summary>Details</summary>
Motivation: LVLMs在处理开放式视觉语言任务时，常常生成与图像内容不一致的幻觉答案。理解并减少这些幻觉行为，对于提升模型的实际应用和可靠性具有重要意义。

Method: 作者利用Logit Lens方法，分析了LVLMs在各解码层生成下一个token分布的过程，发现真实token与幻觉token在累积概率上的差异。基于此，提出了Context Embedding Injection（CEI）方法：在解码过程中将最后输入token的隐藏状态作为语境嵌入注入到模型中，作为视觉约束信号，用以指引生成过程远离幻觉。该方法无需重新训练，可直接应用于现有模型。

Result: 在CHAIR、AMBER、MMHal-Bench等多个基准测试上，CEI方法在三种主流LVLM上均优于当前SOTA基线，特别是其动态版本取得了最低的总体幻觉率。

Conclusion: 本研究通过对模型生成机制的深入分析，提出一种可扩展、无须训练的幻觉缓解方案，显著提升了LVLMs在保持视觉一致性上的表现，有助于推动其更广泛可靠的应用。

Abstract: Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.

</details>


### [53] [WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation](https://arxiv.org/abs/2601.05942)
*Chanchan Wang,Yuanfang Wang,Qing Xu,Guanxin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于小波分析的深度学习方法WaveRNet，用于提升视网膜血管分割的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割在自动眼科诊断中至关重要，但面对不同成像条件（如非均匀光照、对比度变化）容易出现领域迁移问题。同时，如何精准分割细小血管结构也是一大挑战。虽然SAM等大模型具备零样本能力，但目前的微调方法忽视了频域中的领域不变特征，导致泛化能力下降。

Method: 提出WaveRNet方法，结合小波分解和可学习的领域token用于光照鲁棒性和结构/边界的频率特征学习。通过Spectral-guided Domain Modulator (SDM)分离低频（大结构）和高频（细节边界）并生成领域特定特征，再通过Frequency-Adaptive Domain Fusion (FADF)模块融合测试时不同域的特征。最后引入Hierarchical Mask-Prompt Refiner (HMPR) 细化输出掩膜，克服SAM上采样导致细节丢失的问题。

Result: 在四个公开视网膜数据集上采用Leave-One-Domain-Out策略实验，WaveRNet在领域泛化性能上取得了当前最佳结果，优于现有的方法。

Conclusion: WaveRNet有效解决了照明、对比度变化引起的领域迁移问题，能更好地保留血管细节，显著提升视网膜血管分割模型的鲁棒性和泛化能力。

Abstract: Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.

</details>


### [54] [VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction](https://arxiv.org/abs/2601.05966)
*Longbin Ji,Xiaoxiong Liu,Junyuan Shang,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CV

TL;DR: VideoAR 是首个大规模视觉自回归（VAR）视频生成框架，通过多尺度的下一帧预测与自回归建模结合，提升了视频生成质量与效率，实现了更高的长期时序一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散和流匹配模型在视频生成领域取得高质量结果，但它们计算成本高、难以扩展。本研究希望通过自回归方式提升视频生成效率与可扩展性，同时保证高质量输出。

Method: 提出了 VideoAR 框架，将帧内VAR建模与因果的下一帧预测相结合，并使用3D多尺度分词器高效编码时空动态。为提升长期一致性，引入了多尺度时序RoPE、跨帧误差校正和随机帧掩码等机制，辅以多阶段预训练，实现分辨率与时长的逐步适应。

Result: VideoAR在UCF-101数据集上将FVD由99.5提升至88.6，推理步数减少10倍以上，在VBench测评中取得81.74分，性能接近日前主流扩散模型但模型规模更小。

Conclusion: VideoAR 有效弥补了自回归与扩散模型之间的性能差距，为视频生成提供了可扩展、高效且时序一致的新方向。

Abstract: Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.

</details>


### [55] [Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation](https://arxiv.org/abs/2601.05981)
*Yinsong Wang,Xinzhe Luo,Siyi Du,Chen Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种可适应的对比度无关可变形图像配准框架（AC-CAR），通过随机卷积对比度增强方案，实现对任意医学成像对比度的鲁棒配准，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多对比度医学图像间因非线性强度关系，配准极具挑战。传统优化方法计算耗时，现有学习方法则普遍局限于训练时见过的对比度，适用范围窄。

Method: 提出基于随机卷积的对比度增强数据生成方式，结合自适应条件特征调制器（ACFM）及潜在一致性正则，实现对比度无关的特征提取与配准，并引入方差网络测量配准不确定性。

Result: 在大量实验中，AC-CAR在配准准确性上优于主流基线，并在此前未见对比度的测试集上展现了良好泛化能力。

Conclusion: AC-CAR能够有效应对不同对比度医学图像的可变形配准任务，具备较强的泛化性和实际应用价值，提升了临床图像分析的鲁棒性和可靠性。

Abstract: Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.

</details>


### [56] [Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints](https://arxiv.org/abs/2601.05986)
*Adrian Serrano,Erwan Umlil,Ronan Thomas*

Main category: cs.CV

TL;DR: 本论文分析了深度伪造（deepfake）检测系统在真实环境下面对对抗性攻击的鲁棒性，发现常见对抗训练方法在跨数据集和有限攻击者知识下有效性有限。


<details>
  <summary>Details</summary>
Motivation: 深度伪造的检测系统在实际应用中易受到带有微小但难以察觉扰动的对抗性攻击。虽然对抗训练被广泛采用以提升系统安全性，但当前关于这种方法在现实中、尤其是攻击者信息有限及数据分布不匹配时效果的研究还很缺乏。因此，论文旨在弥补该领域的空白。

Method: 作者将DUMB和DUMBer评估框架扩展应用到deepfake检测领域，考察五种主流检测器（RECCE、SRM、XCeption、UCF、SPSL）、三种攻击方式（PGD、FGSM、FPBA）及两个主流数据集（FaceForensics++、Celeb-DF-V2），在攻击与防御者不同知识和数据分布设定下系统评测鲁棒性。

Result: 实验发现：对抗训练确实能增强模型在数据一致（in-distribution）情况下的鲁棒性，但在跨数据集（cross-dataset）场景下，不同策略可能导致鲁棒性下降。攻击策略、模型与数据集的组合对于最终性能影响很大。

Conclusion: 深度伪造检测领域，单一的对抗性训练方法并不能在所有现实应用场景有效，需要针对不同应用和数据分布制定更具针对性的防御策略，以提升实际部署下的安全性和鲁棒性。

Abstract: Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [57] [Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings](https://arxiv.org/abs/2601.05271)
*Xiran Fan,Zhimeng Jiang,Chin-Chia Michael Yeh,Yuzhong Chen,Yingtong Dou,Menghai Pan,Yan Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）和轻量级模型的混合框架，用于支付网络中的交易数据分析，通过LLM生成的语义嵌入初始化交易模型，提高理解能力并兼顾效率，实验表明在多个任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有交易分析模型将商户等分类字段转换为索引，导致文本语义损失，而LLM虽能缓解此问题但计算资源需求大，不适合实时金融场景，因此需找到兼顾语义理解与效率的方法。

Method: 该方法采用LLM生成语义嵌入，作为轻量级交易模型的初始化，同时通过多源数据融合丰富商户字段，并设计“一词约束”以确保各LLM架构间嵌入一致性。此外，引入降噪和上下文感知增强提升数据质量。

Result: 在大规模交易数据集上，所提出方法在多个交易理解任务上取得了显著性能提升，优于仅用传统索引表示的基线方法。

Conclusion: 利用LLM生成语义特征作为轻量化交易模型的启动点，既提升了对交易数据的理解力，又兼顾了实际部署的效率，为实时金融分析提供了一种实用且高效的解决方案。

Abstract: The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.

</details>


### [58] [The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques](https://arxiv.org/abs/2601.05358)
*Tim Menzner,Jochen L. Leidner*

Main category: cs.CL

TL;DR: 本论文提出了一套细致的句子级媒体偏见和宣传的分类法，由六类38种基本偏见类型组成，旨在超越左右翼标签，聚焦实际语言手法，并在对比现有主流体系下展现更全面和更清晰的归类。


<details>
  <summary>Details</summary>
Motivation: 现有关于媒体偏见的研究多关注媒体立场（左翼或右翼），而忽略了偏见更多通过具体语言策略表达，且常常跨越政治光谱。为了更精准地揭示和探查媒体偏见，需要关注和识别句子层面上具体如何表现偏见。

Method: 作者收集了26,464条来自新闻编辑室、用户投稿和自主浏览的句子，结合精读、跨学科理论和初步标注，逐步推导出一个细粒度的句子级偏见/宣传分类体系，并为每种类型提供定义、真实例子、认知和社会动因以及识别方法。

Result: 形成了一个两层级、包含6个功能分类和38种基础偏见类型的“媒体偏见元素表”；量化分析155句样本展现了不同类型的偏见分布，同时与最知名的NLP和传播学分类体系交叉比对，显示出本体系的覆盖率明显提高且更易判别。

Conclusion: 该研究为细致刻画和识别媒体偏见提供了全新且具实用价值的工具和资源，为后续自动化分析和理解媒体偏见现象奠定了坚实基础。

Abstract: Public debates about "left-" or "right-wing" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a "table of media-bias elements". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.

</details>


### [59] [Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models](https://arxiv.org/abs/2601.05366)
*Zheng Luo,T Pranav Kutralingam,Ogochukwu N Okoani,Wanpeng Xu,Hua Wei,Xiyang Hu*

Main category: cs.CL

TL;DR: 这篇论文提出了一个新的基准MLCL，系统评估了大型语言模型（LLMs）在中文、印地语和低资源语言伊博语下的多语言工具调用能力，发现语料语言与参数语言不一致是主要故障原因，并尝试多种推理策略进行改进。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在英文主导的测试中工具调用表现良好，但在多语言用户场景下的稳健性尚未充分研究。为推动多语言AI系统的可靠性，需要深入了解其在非英语环境下的失效原因与改进路径。

Method: 作者提出了MLCL基准，覆盖多种语言，通过细致错误分析和系统性实验，在中文、印地语和伊博语等多语言场景下，对LLM的工具调用功能进行评估，并测试了多种实时推理改进策略。

Result: 实验发现，虽然模型通常能正确理解意图和选择工具，但在参数值语言与调用标准不一致时常发生失效。采用多种推理策略后，语言诱发的错误显著减少，但仍未达到英语下的表现水平。

Conclusion: 尽管可以通过系统及推理层面减少多语言下的执行错误，但LLM多语言工具调用的稳健性仍显著逊于英文模式，需要进一步研究和技术突破。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.

</details>


### [60] [Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection](https://arxiv.org/abs/2601.05403)
*Zhiwei Liu,Yupen Cao,Yuechen Jiang,Mohsinul Kabir,Polydoros Giannouris,Chen Xu,Ziyang Xu,Tianlei Zhu,Tariquzzaman Faisal,Triantafillos Papadopoulos,Yan Wang,Lingfei Qian,Xueqing Peng,Zhuohan Xie,Ye Yuan,Saeed Almheiri,Abdulrazzaq Alnajjar,Mingbin Chen,Harry Stuart,Paul Thompson,Prayag Tiwari,Alejandro Lopez-Lira,Xue Liu,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文提出了一个用于评估大型语言模型在多语种金融虚假信息检测（MFMD）任务中行为偏见的综合性基准MFMDscen，并系统性分析了22个主流LLM的行为偏见。结果显示，无论是商业模型还是开源模型，行为偏见依然明显。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM广泛用于金融领域，但由于其训练数据来自人工语料，易遗传人类偏见，尤其是在处理复杂、高风险且有语境相关性的金融场景时，行为偏见可能影响决策，但目前相关研究还仅局限于简单或通用场景，缺乏面对现实复杂金融场景的系统研究。

Method: 作者与金融专家合作，设计了三类复杂金融场景（基于角色与人格、角色与地区、以及结合族裔和宗教信仰的角色场景），并构建覆盖英语、中文、希腊语和孟加拉语的多语种金融虚假信息数据集，通过将这些场景与虚假信息声称结合，建立MFMDscen基准框架，对22种LLM进行了系统性评估。

Result: 评估表明，无论是商业还是开源LLM，在多语种和多场景下都明显存在行为偏见。

Conclusion: 当前主流LLM在复杂金融虚假信息检测任务中普遍存在无法消除的行为偏见。MFMDscen为后续模型偏见诊断和改进提供了参考和工具。

Abstract: Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\mfmd). In this work, we propose \mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.

</details>


### [61] [Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts](https://arxiv.org/abs/2601.05411)
*Jan Černý,Ivana Kvapilíková,Silvie Cinková*

Main category: cs.CL

TL;DR: 本文提出使用信息熵指标评估文本可读性，通过可视化框架结合多种语言模型，以提升行政或官僚文本的清晰度与可读性。相关工具已开源。


<details>
  <summary>Details</summary>
Motivation: 行政或官僚文本常常晦涩难懂，因此需要有效方法量化其可读性，并帮助提升文本的易读性和表达明确性。

Method: 提出利用信息熵理论结合多种语言模型对文本进行可读性评估，并实现数据可视化，以便更直观地分析和优化文本。

Result: 开发了一套可视化框架和工具，能够以多模型方式近似估算文本信息熵，对可读性进行具体分析，成果已在GitHub开源。

Conclusion: 测量信息熵结合可视化框架可有效辅助行政文本的可读性提升，并为相关从业者提供了一套开源工具，实现了理论到实践的落地。

Abstract: This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.

</details>


### [62] [Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions](https://arxiv.org/abs/2601.05414)
*Minda Zhao,Yilun Du,Mengyu Wang*

Main category: cs.CL

TL;DR: 本论文对大型语言模型（LLMs）在概率分布采样能力上进行了大规模系统评测，发现其原生采样准确性较低，不适合直接作为需要统计保证应用的内部采样器。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育评测、合成数据等领域从简单对话工具转向关键的概率型组件，能否准确按指定概率分布采样成为实际需求。此前这一能力研究较少，存在空白。

Method: 作者对11种前沿LLMs在15种概率分布上的采样能力进行了系统测试，采用两个协议：一次生成1000个样本（Batch Generation）；独立调用1000次（Independent Requests），以此区分不同失效模式。并评估采样准确性的随分布复杂度和采样数变化情形，以及其对下游任务的影响。

Result: Batch采样通过率中位数仅为13%；而独立采样几乎全面失败（11个模型中10个分布全部不合格）。采样有效性随分布复杂性和采样数增加逐渐下降。下游任务如MCQ选项分布及特定属性的文本到图像生成中，错误会传播并导致统计属性目标无法被满足。

Conclusion: 当前前沿LLMs原生采样能力较差，不适合作为统计严谨任务的内部采样器，这类应用需依赖外部采样工具实现对概率分布的准确抽样。

Abstract: As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.

</details>


### [63] [Tracing Moral Foundations in Large Language Models](https://arxiv.org/abs/2601.05437)
*Chenxiao Yu,Bowen Yi,Farzan Karimi-Malekabadi,Suhaib Abdurahman,Jinyi Ye,Shrikanth Narayanan,Yue Zhao,Morteza Dehghani*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）内部是否具有真实的道德概念结构，或仅仅是表面模仿了人类的道德判断。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM往往能展现类人的道德判断，但是否基于真实的内部道德表示（而非表面模仿）尚不明确，因此需要深入剖析其内部机制。

Method: 利用道德基础理论（Moral Foundations Theory, MFT）作为分析框架，对Llama-3.1-8B-Instruct和Qwen2.5-7B-Instruct两种模型进行多层次分析：(1) 分析模型不同层对MFT概念的表征及其与人类判断的一致性；(2) 利用稀疏自动编码器（SAE）寻找支持道德概念的稀疏特征；(3) 通过密集和稀疏特征的因果操控，观察对模型道德输出的影响。

Result: 结果表明，两种模型以结构化并依赖不同层的方式区分和表征道德基础，这种结构与人类判断一致。SAE特征与具体道德基础有明确的语义联系。通过操控这些特征，可以因果性地改变模型的道德相关输出。

Conclusion: LLM中的道德概念是分布式、分层且部分可分离的，这表明多元的道德结构可以仅由语言规律自发形成。

Abstract: Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.

</details>


### [64] [Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction](https://arxiv.org/abs/2601.05459)
*Hongjin Kim,Jaewook Lee,Kiyoung Lee,Jong-hun Shin,Soojong Lim,Oh-Woog Kwon*

Main category: cs.CL

TL;DR: 本文主要研究了大语言模型（LLM）在韩语这类低资源语言中的推理和自我纠错能力，提出通过强化学习和联合特定神经元的微调，能显著提升模型的韩语推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在英语等高资源语言上推理能力很强，但在韩语等低资源语言上的表现却较差。研究者希望探索提升韩语推理与自我纠错能力的方法，并缩小与英语表现之间的差距。

Method: 首先用强化学习方法提升韩语推理能力，发现效果有限。随后提出对模型内部的韩语特定神经元（尤其是早期层）进行微调，并提出自我纠错式语码转换数据集对模型进行训练，从而更好地对齐模型推理过程。

Result: 通过新数据集和神经元微调，模型在韩语数学推理和自我纠错任务上取得了显著性能提升。实验表明，只有将模型内部推理机制与韩语输入深度对齐，强化学习才能有效发挥作用。

Conclusion: 提升多语种推理能力的关键不是添加新语言知识，而是激发并对齐已有的推理能力。本文展示了“内部翻译”和“神经元级调优”对多语种推理一致性的重要作用。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.

</details>


### [65] [Towards Valid Student Simulation with Large Language Models](https://arxiv.org/abs/2601.05473)
*Zhihao Yuan,Yunze Xiao,Ming Li,Weihao Xuan,Richard Tong,Mona Diab,Tom Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种用于教育场景的大语言模型（LLM）学生模拟的概念与方法框架，并分析了当前方法的核心局限，提出了更具认知真实性的模拟方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在模拟学生时，因模型本身知识全面，难以准确模仿知识有限的真实学习者，出现称为“能力悖论”的问题，导致错误模式与学习动态不真实。

Method: 作者将学生模拟视为受限生成任务，引入'认识状态规范（ESS）'，明确定义模拟学习者可访问的知识、易错点结构和认知状态随时间的演变。同时引入Goal-by-Environment框架以明确学生系统在不同目标和应用环境下的行为属性。

Result: 没有提出新系统或基准，而是对现有文献进行综合，规范关键设计维度，明确模拟学生的构建和应用面临的效度、评估、伦理等挑战。

Conclusion: 作者主张在模拟学生时，关注知识结构的真实性（epistemic fidelity）比表面上的行为仿真更为重要，这是让LLM模拟学生成为可靠科学和教育工具的前提。

Abstract: This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the "competence paradox" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.

</details>


### [66] [The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence](https://arxiv.org/abs/2601.05478)
*Herun Wan,Jiaying Wu,Minnan Luo,Fanxiao Li,Zhi Zeng,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文提出MisBelief框架，发现主流大模型虽然能抵抗明显的虚假信息，但对精心设计、难以直接证伪的误导性证据极为敏感。作者还提出了一种新机制DIS，可有效减缓模型的错误信念提升。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在面对直接伪信息时表现稳健，但现实中误导性信息常常以更隐蔽、逻辑有说服力的方式出现，影响AI助理的决策可信度。因此，有必要系统性地研究模型对复杂误导的脆弱性。

Method: 作者设计了MisBelief框架，通过多角色LLM多轮协作生成“难以证伪的虚假证据”，共生成4800个不同难度的误导性案例，并评测主流7种大模型的鲁棒性。此外，提出DIS机制，通过推断证据的欺骗意图，为模型决策提供预警。

Result: 实验发现，大模型对直接的虚假信息表现稳健，但遇到经过逻辑包装的微妙虚假证据时，信任错误信息的概率上升平均93%。DIS机制能显著抑制这种信念偏移，使模型更为谨慎地评估证据。

Conclusion: 大模型存在对复杂、难以证伪的误导性情景下信念易受影响的根本缺陷。引入欺骗意图预警等治理机制能提升AI对复杂虚假信息的防御能力。

Abstract: To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.

</details>


### [67] [MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards](https://arxiv.org/abs/2601.05488)
*Zhiyu Shen,Ziming Wu,Fuming Lai,Shaobing Lian,Yanghui Rao*

Main category: cs.CL

TL;DR: 该论文提出了MemBuilder框架，通过强化学习方法提升长对话中模型的一致性，在多个长对话基准上超过了当前主流闭源模型。


<details>
  <summary>Details</summary>
Motivation: 长对话中保持历史状态一致性是大语言模型尚未解决的关键难题，尤其是现有检索机制无法有效反映历史信息的时间演化，且基于记忆的框架依赖于静态的、闭源模型的提示方法或稀疏奖励导致训练效果有限。

Method: 作者提出MemBuilder强化学习框架，在模型训练中引入：1）通过合成生成会话级问题，实现稠密的中间奖励，解决稀疏奖励问题；2）提出贡献感知的梯度加权方法，根据个体的下游影响调整策略更新，从而实现多维记忆归因。

Result: 实验表明，基于MemBuilder训练的4B参数规模模型，在各项长对话基准测试中，性能优于当前最先进的闭源模型，表现出良好的泛化能力。

Conclusion: MemBuilder为长期对话中的一致性保持提供了有效的开源解决方案，通过稠密奖励和多维记忆归因，显著优于现有方法，有望推动对话系统实际应用的进展。

Abstract: Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.

</details>


### [68] [FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse](https://arxiv.org/abs/2601.05505)
*Yubo Hou,Zhisheng Chen,Tao Wan,Zengchang Qin*

Main category: cs.CL

TL;DR: FlashMem是一种提升大语言模型动态记忆能力的新框架，通过直接利用模型中已有的隐藏状态来高效合成记忆，无需冗余的重新编码，从而大大提高推理效率，并节省资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因为无状态架构，难以持续追踪对话或任务历史，导致需要重复处理全部历史内容，影响了长期推理效率与资源利用。现有‘潜在记忆’方法通过附加模块管理记忆，与主推理部分分离，带来性能和效率损耗。

Method: 作者提出FlashMem框架，核心思想是将最后的隐藏状态作为完整历史的充分统计量（即唯一的代表），直接用于生成记忆内容。通过‘Shared-KV Consolidator’组件处理隐藏状态缓存，无需额外参数化，从而提升效率。同时引入‘Cognitive Monitor’，一个基于注意力熵判定模型不确定性的方法，仅在模型面临高不确定性时进行记忆整合，从而进一步节省计算。

Result: 实验证明FlashMem在提升记忆能力的同时，与主流但资源消耗大的方案性能相当，但推理延迟减少了5倍，显著优化效率。

Conclusion: FlashMem有效解决了大模型动态记忆与推理效率的冲突，在保持认知持续性的同时显著提升了推理速度和资源利用率，为后续大模型记忆系统的设计提供了新范式。

Abstract: The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.

</details>


### [69] [CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems](https://arxiv.org/abs/2601.05520)
*Xuemei Tang,Chengxi Yan,Jinghang Gu,Chu-Ren Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CHisAgent的多智能体大语言模型框架，用于古代中国历史分类体系的自动构建，实现更好的结构性与覆盖性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在历史和文化推理（尤其是非英语语境如中国历史）方面能力有限，手动构建历史分类体系成本高且难以扩展。

Method: CHisAgent将历史分类体系构建分为三个阶段，每个阶段由不同角色的智能体完成：1）Inducer自底向上从原始历史文本中归纳初步层级；2）Expander结合大模型常识自顶向下补全缺失的中间概念；3）Enricher通过整合外部结构化历史资源提升体系的真实性和丰富度。

Result: 以《二十四史》为基础，自动构建了覆盖政治、军事、外交和社会生活的大规模历史事件分类体系。无参考和有参考的多种评估均表明新体系在结构一致性和覆盖性方面有明显提升，且有助于跨文化知识对齐。

Conclusion: CHisAgent能够自动、高效地为中国古代历史建构高质量分类体系，显著改进了大模型历史理解和多文化知识组织能力。

Abstract: Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.

</details>


### [70] [Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism](https://arxiv.org/abs/2601.05524)
*Yuhao Shen,Tianyu Liu,Junyi Shen,Jinyang Wu,Quan Kong,Li Huan,Cong Wang*

Main category: cs.CL

TL;DR: 本文提出了Double（Double Retrieval Speculative Parallelism）方法，通过创新性的同步机制解决了并行推测解码（PSD）中的加速上限和计算浪费问题，在大模型推理加速上达到了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统的推测解码（SD）和并行推测解码（PSD）在提升大模型推理速度时受限于草稿模型与目标模型的速度比，以及计算浪费和流水线阻塞等问题。需要新的方法突破推理加速的理论瓶颈，并提升效率和准确性。

Method: Double 框架结合了SD和PSD的优点，采用草稿模型迭代检索推测、目标模型权威检索多token指导，提出同步机制以提升精度与效率。无需重新训练模型，保持性能无损失。

Result: 在LLaMA3.3-70B模型上实现5.3倍速度提升，在Qwen3-32B模型上实现2.8倍提升，均显著优于需要大量训练的EAGLE-3先进方法。

Conclusion: Double方法突破了并行推测解码推理速度的理论上限，无需训练且损失无性能，能为大规模语言模型推理带来高效、先进的加速效果。

Abstract: Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\textbf{5.3}\times$ on LLaMA3.3-70B and $\textbf{2.8}\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.

</details>


### [71] [Closing the Modality Reasoning Gap for Speech Large Language Models](https://arxiv.org/abs/2601.05543)
*Chaoren Wang,Heng Lu,Xueyao Zhang,Shujie Liu,Yan Lu,Jinyu Li,Zhizheng Wu*

Main category: cs.CL

TL;DR: 本文提出了TARS强化学习框架，通过奖励机制对齐语音与文本语言模型的推理过程，从而显著缩小了二者在推理任务上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语音语言模型在推理能力上落后于文本模型，尤其在层级表征和长链推理过程中存在偏移，影响模型实际应用。解决这一模态推理差距成为提升语音LLM的重要方向。

Method: 提出TARS框架，基于强化学习，通过不对称奖励设计对齐文本与语音的推理轨迹。奖励信号包含：1）表征对齐，通过对比中间层隐藏状态相似性；2）行为对齐，评估输出与参考文本的语义一致性。

Result: 在MMSU和OBQA等高难度推理数据集上，TARS框架显著提升了语音LLM的推理表现，缩小了与文本LLM的差距，并在7B参数规模下取得了最优性能。

Conclusion: TARS框架证明通过有效对齐语音与文本轨迹，可以提升语音LLM的推理能力，为多模态大模型的发展提供了新思路。

Abstract: Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.

</details>


### [72] [Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring](https://arxiv.org/abs/2601.05545)
*Hongjin Kim,Jeonghyun Kang,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文针对自动作文评分系统（AES）和大语言模型（LLM）在检测有害作文方面的不足，提出了专门的基准数据集，并测试相关模型在识别与评分含有敏感内容（如种族主义、性别偏见）的能力。结果显示现有系统难以准确区分有害内容和正常议论文，呼吁开发更具道德敏感性的评分系统。


<details>
  <summary>Details</summary>
Motivation: 当前AES和LLMs虽然评分能力提升，但面临严重的问题——常常忽视作文中的伦理和道德风险，对传递有害观点的作文评分过高。这种现状亟需改进。

Method: 作者构建了Harmful Essay Detection (HED)基准，涵盖含有种族主义和性别偏见等敏感话题的作文，利用该数据集系统评估多种大语言模型在检测、评分有害内容的表现。

Result: 实验发现：1）LLMs尚不能有效区分有害作文与一般议论文；2）AES系统及LLMs在评分时普遍忽略内容的道德影响。

Conclusion: 目前AES及LLMs在识别和妥善处理有害内容上存在明显短板，今后的研究亟需提升其对道德伦理层面的敏感性，构建更健全、负责任的自动评分框架。

Abstract: This study addresses critical gaps in Automated Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.

</details>


### [73] [Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation](https://arxiv.org/abs/2601.05548)
*Jeonghyun Kang,Hongjin Kim,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文提出了KEEM数据集，一种旨在提升长对话系统中记忆更新质量的生成式数据集。KEEM通过动态生成整合性记忆，兼顾事实和情感，提高系统对用户状态的追踪与同理心。


<details>
  <summary>Details</summary>
Motivation: 现有长对话系统中的记忆更新方法（如简单累积或操作式方法）容易导致信息冲突，难以精确追踪用户状态，缺乏对情感和因果关系的捕捉，限制了系统的理解与响应能力。

Method: 设计KEEM数据集，通过动态生成性方法，将用户对话中的关键信息、情感上下文和因果关系综合为新的记忆内容，供对话系统实时更新记忆模块。

Result: KEEM所支持的系统能更全面地记忆和理解用户状态，不仅保存了关键事实信息，还能捕捉情感与因果线索，提高对用户意图和需求的追踪准确性。

Conclusion: 借助KEEM，长对话系统可更有同理心、更加精准地与用户互动，显著提升开放域场景下的对话体验和意义响应能力。

Abstract: In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.

</details>


### [74] [ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging](https://arxiv.org/abs/2601.05560)
*Junyao Yang,Chen Qian,Dongrui Liu,Wen Shen,Yong Liu,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReasonAny的新模型融合框架，能够有效结合推理与领域能力，克服以往方法在融合后推理或领域能力大幅下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在链式推理（chain-of-thought）上取得突出成果，但将推理能力赋予领域专用模型（如医疗、金融等）的“Reasoning + X”面临困难。传统的模型融合方法常导致推理深度和领域能力双双下降，急需新的解决方案。

Method: 作者发现推理能力主要存在于参数的低梯度敏感区域，而非高梯度敏感区域。根据这一发现，提出了Contrastive Gradient Identification方法，用于更精度地合成具备领域能力和推理能力的新模型，形成ReasonAny框架。

Result: 在安全、生物医学和金融等多个领域的实验表明，ReasonAny能显著优于现有模型融合方法，并同时保留强大的推理与领域特定能力。

Conclusion: ReasonAny为领域专用模型赋予强推理能力提供了有效路径，有望提升“Reasoning + X”模型的实用性和性能。

Abstract: Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as "Reasoning + X", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes "Reasoning + X" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.

</details>


### [75] [Can large language models interpret unstructured chat data on dynamic group decision-making processes? Evidence on joint destination choice](https://arxiv.org/abs/2601.05582)
*Sung-Yoo Lim,Koki Sato,Kiyoshi Takami,Giancarlos Parady,Eui-Jin Kim*

Main category: cs.CL

TL;DR: 本文探讨了利用大语言模型（LLMs）自动解析社交群体活动（如集体外出就餐）决策过程的可行性，通过设计特定提示框架，指导LLMs从非结构化群聊中提取关键决策因素，并与人工标注结果进行比较。


<details>
  <summary>Details</summary>
Motivation: 传统出行调查难以捕捉社交活动中复杂的群体决策过程，但新型非结构化数据（如聊天记录）为研究这些过程提供了可能。解析群体决策不仅需要提取显性因素，还包含大量隐性、受文化影响的内容，人工标注非常耗时，亟需自动化手段辅助。

Method: 以日本集体就餐活动群聊数据为例，设计一个仿知识获取过程的提示框架，引导LLMs依次抽取决策要素（如备选餐厅、最终选择、个体偏好及偏好驱动属性），将对话转为结构化表格数据，并通过人工标注的对比数据集进行定量和定性误差分析。

Result: LLMs可以较好识别对话中明确表达的决策因素，但对于隐晦、语境依赖的隐性决策动力捕捉较弱，需要人工辅助。论文还分析了哪些情景下可较放心依赖LLMs，哪些场景必须人工介入。

Conclusion: LLMs在解析社交群体非结构化决策数据上具有巨大潜力，可为社会活动研究引入新数据来源，但在处理隐性、细微的社会语境时存在局限，未来需结合人工力量以确保准确性。

Abstract: Social activities result from complex joint activity-travel decisions between group members. While observing the decision-making process of these activities is difficult via traditional travel surveys, the advent of new types of data, such as unstructured chat data, can help shed some light on these complex processes. However, interpreting these decision-making processes requires inferring both explicit and implicit factors. This typically involves the labor-intensive task of manually annotating dialogues to capture context-dependent meanings shaped by the social and cultural norms. This study evaluates the potential of Large Language Models (LLMs) to automate and complement human annotation in interpreting decision-making processes from group chats, using data on joint eating-out activities in Japan as a case study. We designed a prompting framework inspired by the knowledge acquisition process, which sequentially extracts key decision-making factors, including the group-level restaurant choice set and outcome, individual preferences of each alternative, and the specific attributes driving those preferences. This structured process guides the LLM to interpret group chat data, converting unstructured dialogues into structured tabular data describing decision-making factors. To evaluate LLM-driven outputs, we conduct a quantitative analysis using a human-annotated ground truth dataset and a qualitative error analysis to examine model limitations. Results show that while the LLM reliably captures explicit decision-making factors, it struggles to identify nuanced implicit factors that human annotators readily identified. We pinpoint specific contexts when LLM-based extraction can be trusted versus when human oversight remains essential. These findings highlight both the potential and limitations of LLM-based analysis for incorporating non-traditional data sources on social activities.

</details>


### [76] [ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue](https://arxiv.org/abs/2601.05589)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Qingyu Niu,Guoqing Ma,Yidan Liang,Jingjiang Liu,Yiling Wang,Shimin Di,Jiajie Xu*

Main category: cs.CL

TL;DR: 本论文提出了一种名为ACR的动态上下文重构框架，有效缓解了多轮对话中上下文拖延和状态漂移问题，提升了对话质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多轮对话中常常难以保持与早前内容的对齐，对多轮依赖的处理较弱，且对话越长越容易出现事实偏离。现有方法如扩展上下文窗口、引入外部记忆、上下文压缩等，都面临上下文惯性与状态漂移的局限。

Method: 提出了ACR（Adaptive Context Refactoring）框架，包含一套上下文重构操作库，并采用教师引导的自我进化式训练方法，让模型学会在何时以及如何主动干预和重构对话历史，将上下文管理与推理过程解耦。

Result: 在多轮对话任务上的大量实验表明，所提方法在提升对话表现、降低token消耗方面均显著优于现有主流基线方法。

Conclusion: ACR框架能有效克服多轮对话中常见的上下文惯性与状态漂移问题，为大语言模型的上下文管理提供了更优解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \textbf{contextual inertia} and \textbf{state drift}. To address these challenges, we propose the \textbf{A}daptive \textbf{C}ontext \textbf{R}efactoring \textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.

</details>


### [77] [Data Augmented Pipeline for Legal Information Extraction and Reasoning](https://arxiv.org/abs/2601.05609)
*Nguyen Minh Phuong,Ha-Thanh Nguyen,May Myo Zin,Ken Satoh*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLMs）进行数据增强的流程，有效减少法律领域信息抽取任务中的人工标注工作，并提升系统稳定性，该方法简单、有效且具有较强泛化性。


<details>
  <summary>Details</summary>
Motivation: 法律领域的信息抽取任务常因标注数据匮乏而发展受限，手动标注耗时耗力，因此亟需一种高效且可扩展的数据增强方案。

Method: 提出利用大语言模型生成数据，形成数据增强管道，用于提升信息抽取系统性能。该方法流程简单，并可推广至其他NLP任务。

Result: 实验表明，所提方法在减少人工标注量的同时，提升了信息抽取系统的鲁棒性。

Conclusion: 基于LLM的数据增强管道不仅减轻了人工负担，还提升了信息抽取模型的表现，且可泛化应用于其他NLP任务。

Abstract: In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.

</details>


### [78] [Text Detoxification in isiXhosa and Yorùbá: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages](https://arxiv.org/abs/2601.05624)
*Abayomi O. Agbeyangi*

Main category: cs.CL

TL;DR: 本文提出了一种新的混合方法，用于南非低资源语言（isiXhosa 和 Yorùbá）的自动文本去毒（将有害文本自动改写为中性文本），通过可解释性强的检测与可控重写实现高效去毒。


<details>
  <summary>Details</summary>
Motivation: 非洲语言缺乏强有力的有害文本检测与去毒工具，导致安全网络参与受阻。文章致力于弥补这一关键空白，帮助提升非洲语言社群的网络安全与包容性。

Method: 方法采用轻量且可解释的TF-IDF结合Logistic Regression模型进行毒性检测，并用受控的词典与token指导的规则方法进行重写。作者还构建了含有习惯用法、变音符号及语言切换的毒性-中性平行语料库以训练和评估模型。

Result: 毒性检测模型在isiXhosa和Yorùbá两种语言分别达到61-72%与72-86%的分层K折准确率，ROC-AUC最高达0.88。重写组件能完全去毒所有毒性句子，并保持所有非毒性句子无修改。

Conclusion: 本工作证明了可扩展且可解释的轻量检测器结合基于规则的去毒方法，可以为低资源非洲语言提供高效、文化适应性强的安全工具，树立了非洲语言文本风格迁移领域的新基准。

Abstract: Toxic language is one of the major barrier to safe online participation, yet robust mitigation tools are scarce for African languages. This study addresses this critical gap by investigating automatic text detoxification (toxic to neutral rewriting) for two low-resource African languages, isiXhosa and Yorùbá. The work contributes a novel, pragmatic hybrid methodology: a lightweight, interpretable TF-IDF and Logistic Regression model for transparent toxicity detection, and a controlled lexicon- and token-guided rewriting component. A parallel corpus of toxic to neutral rewrites, which captures idiomatic usage, diacritics, and code switching, was developed to train and evaluate the model. The detection component achieved stratified K-fold accuracies of 61-72% (isiXhosa) and 72-86% (Yorùbá), with per-language ROC-AUCs up to 0.88. The rewriting component successfully detoxified all detected toxic sentences while preserving 100% of non-toxic sentences. These results demonstrate that scalable, interpretable machine learning detectors combined with rule-based edits offer a competitive and resource-efficient solution for culturally adaptive safety tooling, setting a new benchmark for low-resource Text Style Transfer (TST) in African languages.

</details>


### [79] [GIFT: Games as Informal Training for Generalizable LLMs](https://arxiv.org/abs/2601.05633)
*Nuoyan Lyu,Bingbing Xu,Weihao Meng,Yige Yuan,Yang Zhang,Zhiyong Huang,Tat-Seng Chua,Huawei Shen*

Main category: cs.CL

TL;DR: 本文提出通过游戏环境为大型语言模型（LLM）提供非正式学习能力，并设计嵌套训练框架提升多任务学习表现，实现更广泛、类人化的智能泛化。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学、代码等正式学习任务上表现优异，但在人类认知中的实际智慧和社会推理等非正式学习能力上仍有较大差距。主要原因在于缺乏互动反馈为核心的非正式学习过程。

Method: 作者提出以游戏作为LLM非正式学习的主要环境，通过其固有奖励信号和复杂抽象性，培养模型多样能力。提出嵌套训练框架，用任务序列组合代替简单任务混合，使模型必须同时掌握多种能力以达最大化奖励，使用基于GRPO的强化学习训练，涵盖矩阵博弈、井字棋、谁是卧底等游戏。

Result: 实验证明，基于游戏的非正式学习不仅防止了多任务间的相互干扰，还大幅提升了模型在能力泛化基准测试中的表现。

Conclusion: 通过游戏环境与新型训练框架，可有效增强LLM的类人非正式智能泛化能力。相关方法和实现已公开。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the "practical wisdom" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit "OR" objective, our framework employs sequential task composition to enforce an explicit "AND" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.

</details>


### [80] [Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs](https://arxiv.org/abs/2601.05641)
*Alireza Dehghanpour Farashah,Aditi Khandelwal,Marylou Fauchard,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 本文研究了多语言大模型（以Aya-Expanse 8B为例）在数据撤回（unlearning）与概念撤回下的表现，并将相关基准测试扩展至十种不同语言。结果发现，高资源语言的撤回更稳定，不同语系间存在非对称知识转移，句法相似性是跨语种撤回效果的最强预测因子。


<details>
  <summary>Details</summary>
Motivation: 随着多语言大模型的广泛应用，如何在不同语言环境中保障模型的安全与公平成为重要难题。此前关于机器撤回的研究多集中在英语等单语场景，而多语言情境下，会有来自不同预训练与微调数据的偏见以及跨语言知识转移，对撤回带来新的挑战。

Method: 作者以Aya-Expanse 8B多语言大模型为研究对象，分别在数据撤回与概念撤回两个场景下实验，并将与事实性知识和刻板印象相关的基准测试数据通过翻译扩展到十种语言。这些语言覆盖五种语系、不同的资源丰富度。作者通过多语言评测，对不同语言间撤回产生的影响进行分析。

Result: 实验发现，在高资源语言上执行撤回一般更稳定。而在语系相关的语言之间，存在非对称的知识转移效应。进一步分析发现，语言间的句法相似性是预测跨语种撤回表现的关键因素。

Conclusion: 多语言大模型的撤回机制受资源量与语种间结构相似性显著影响。高资源语言的撤回效果好，语法相近的语言之间撤回影响更强。为提升多语言模型安全性和公平性，需考虑不同语言资源与语言结构间的关联。

Abstract: As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.

</details>


### [81] [A Framework for Personalized Persuasiveness Prediction via Context-Aware User Profiling](https://arxiv.org/abs/2601.05654)
*Sejun Park,Yoonah Park,Jongwon Lim,Yohan Jo*

Main category: cs.CL

TL;DR: 论文提出了一个上下文感知的用户画像方法，通过生成针对性的查询和摘要历史记录，从而提升说服性预测模型的效果。


<details>
  <summary>Details</summary>
Motivation: 在诸如推荐系统和大模型安全评估等应用中，准确预测信息的说服力十分重要；但目前尚缺乏一种系统性方法，能够系统化利用被说服者的历史活动（如对话）来提升模型预测性能。

Method: 作者提出了一个包含双可训练组件的框架：（1）查询生成器，用于提出最优问题，从用户历史记录中检索与说服相关的信息；（2）用户信息整合器，将检索到的内容摘要成画像，并输入说服性预测模型。整个方法在ChangeMyView Reddit数据集上进行了评估。

Result: 新方法在多个预测模型上都取得了比现有方法更好的效果，F1分数最高提升了13.77个百分点。进一步的分析表明，效果优异的用户画像需要根据任务和上下文动态调整，而不应仅依赖静态属性或表层相似度。

Conclusion: 面向任务、基于上下文的用户画像对于实现个性化的信息说服性预测十分关键，所提方法显著优于现有方案。

Abstract: Estimating the persuasiveness of messages is critical in various applications, from recommender systems to safety assessment of LLMs. While it is imperative to consider the target persuadee's characteristics, such as their values, experiences, and reasoning styles, there is currently no established systematic framework to optimize leveraging a persuadee's past activities (e.g., conversations) to the benefit of a persuasiveness prediction model. To address this problem, we propose a context-aware user profiling framework with two trainable components: a query generator that generates optimal queries to retrieve persuasion-relevant records from a user's history, and a profiler that summarizes these records into a profile to effectively inform the persuasiveness prediction model. Our evaluation on the ChangeMyView Reddit dataset shows consistent improvements over existing methods across multiple predictor models, with gains of up to +13.77%p in F1 score. Further analysis shows that effective user profiles are context-dependent and predictor-specific, rather than relying on static attributes or surface-level similarity. Together, these results highlight the importance of task-oriented, context-dependent user profiling for personalized persuasiveness prediction.

</details>


### [82] [Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat](https://arxiv.org/abs/2601.05657)
*Hao Yang,Hongyuan Lu,Dingkang Yang,Wenliang Yang,Peng Sun,Xiaochuan Zhang,Jun Xiao,Kefan He,Wai Lam,Yang Liu,Xinhua Zeng*

Main category: cs.CL

TL;DR: 本文提出了新一代逐步决策对话代理Stephanie2，有效提升了即时通讯对话的自然性和互动性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI分步式聊天系统缺乏主动等待机制，消息节奏不自然，影响了用户体验。

Method: Stephanie2通过主动等待和消息节奏适应机制，在每一步明确决策“发送消息”还是“等待”，将等待时间建模为思考时间和打字时间之和。此外，提出了基于时间窗口的双代理对话系统，用于生成伪对话历史用于人类与自动评测。

Result: Stephanie2在自然性、互动性等指标上明显优于上一代Stephanie1，并在角色识别图灵测试的人类评价中获得了更高通过率。

Conclusion: Stephanie2有效提高了AI聊天的节奏自然性和互动体验，为更自然流畅的社交聊天代理提供了新思路。

Abstract: Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.

</details>


### [83] [Afri-MCQA: Multimodal Cultural Question Answering for African Languages](https://arxiv.org/abs/2601.05699)
*Atnafu Lambebo Tonja,Srija Anand,Emilio Villa-Cueva,Israel Abebe Azime,Jesujoba Oluwadara Alabi,Muhidin A. Mohamed,Debela Desalegn Yadeta,Negasi Haile Abadi,Abigail Oppong,Nnaemeka Casmir Obiefuna,Idris Abdulmumin,Naome A Etori,Eric Peter Wairagala,Kanda Patrick Tshinu,Imanigirimbabazi Emmanuel,Gabofetswe Malema,Alham Fikri Aji,David Ifeoluwa Adelani,Thamar Solorio*

Main category: cs.CL

TL;DR: 本文提出了Afri-MCQA，这是首个涵盖15种非洲语言、涉及7500对问题答案的多语言文化问答基准，并对现有大模型表现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 非洲拥有全球三分之一以上的语言资源，但在人工智能领域严重缺乏代表性，因此亟需支持本地化和多模态AI基准，彰显文化和语言多样性。

Method: 作者构建了Afri-MCQA数据集，包含12个非洲国家、15种语言的问答对，涵盖文本和语音模态，且数据全部由母语者创建。同时评估了当前开源大型语言模型的在该数据集上的表现，并设计了分离语言能力和文化知识的对照实验。

Result: 在母语和语音任务上，已有开源大模型表现极差，接近于零的准确率，并且即使在语言能力而非文化知识相关任务中，模型在母语与英语之间仍有显著性能差距。

Conclusion: 当前多模态大模型对非洲本地语言及文化知识支持严重不足。应推动以语音为主、文化本地化预训练和跨语种迁移等方法，促进包容性更强的多模态AI系统发展。

Abstract: Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)

</details>


### [84] [Multimodal In-context Learning for ASR of Low-resource Languages](https://arxiv.org/abs/2601.05707)
*Zhaolin Li,Jan Niehues*

Main category: cs.CL

TL;DR: 本论文提出通过多模态上下文学习（MICL）结合语音大模型，在未见语言上也能提升自动语音识别（ASR）效果，避免对目标语言数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别仅覆盖很少一部分语言，主要因为高度依赖有标注的数据。以往利用大语言模型进行上下文学习只限于覆盖度高的语言和文本场景，远未解决低资源语言ASR难题。

Method: 本文探究了通过语音大模型Phi-4和Qwen3-Omni，采用多模态上下文学习（MICL）策略，将语音和文本两种模态共同用于未见语言ASR。实验还结合跨语言迁移学习，分析模型内部注意力机制，并基于MICL为声学模型假设选择提供辅助。

Result: 实验证明：1）MICL能有效提升未见语言的ASR表现；2）跨语言迁移学习可进一步提升效率且无需目标语言数据，其效果不逊色于已在语料库训练的语言模型；3）模型在内部表现出对文本上下文的偏好。

Conclusion: MICL是一种高效方法，能在缺乏标注语料的低资源、未见语言场景下利用语音大模型有效提升ASR，并可通过迁移学习进一步提升，无需额外目标语言数据。

Abstract: Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.

</details>


### [85] [Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging](https://arxiv.org/abs/2601.05713)
*Thomas Fabian*

Main category: cs.CL

TL;DR: 本文提出一种通过弥散张量成像（DTI）方法分析大语言模型（LLM）中的单词嵌入信息流的新工具，提升了对LLM中自然语言表达方式的理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析方法多关注单词嵌入的可视化和单词间的空间关系，忽略了单词在句子中的上下文和实际语境，导致对LLM内部信息流和语义表达理解有限。

Method: 作者引入医学中常用的弥散张量成像（DTI）方法，将其应用于LLM的单词嵌入，分析和可视化自然语言表达中的信息流动，从而追踪不同层级之间的信息流情况。

Result: 该方法能够揭示LLM各层嵌入间的信息传递路径，可对比不同模型结构，对未充分利用的层提出裁剪建议，并发现诸如指代消解、隐喻检测等任务的信息流差异。

Conclusion: 这种DTI方法带来了对LLM处理实际自然语言表达的新见解，弥补了仅靠单词嵌入分析的不足，从而提升了自然语言处理模型的可解释性。

Abstract: Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.

</details>


### [86] [Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns](https://arxiv.org/abs/2601.05751)
*Amalie Brogaard Pauli,Maria Barrett,Max Müller-Eberstein,Isabelle Augenstein,Ira Assent*

Main category: cs.CL

TL;DR: 本研究提出并验证了一个评估LLMs在劝说语言生成中受受众性别、发送者意图和输出语言影响的框架，发现LLMs在不同性别对象上的生成存在显著偏差，并与社会心理及社会语言学中的性别刻板印象相一致。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已广泛用于日常交流和劝说型信息撰写，先前研究表明它们在劝说方面极具能力，但目前对生成内容如何因用户指令和受众群体差异（如性别）发生变化的理解不足，迫切需要体系化的评估和理解。

Method: 作者提出一个评估框架，分别考察受众性别、发送意图和输出语言对生成劝说语句的影响。实际采用了13种主流LLM及16种语言，通过成对提示生成劝说文本，再结合社会心理学、传播学理论，用LLM判分方法对模型输出在19类劝说语言维度进行系统评估。

Result: 结果显示，在所有测试模型中，针对不同性别受众生成的劝说语言存在显著且一致的性别差异，这种差异与社会心理学、社会语言学中记录的性别刻板印象一致。

Conclusion: LLMs在劝说语言生成过程中存在性别偏见，这一现象涉及模型本身的语言倾向，应予以关注和改进，以避免在实际交流和应用中加剧社会刻板印象。

Abstract: Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.

</details>


### [87] [AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor](https://arxiv.org/abs/2601.05752)
*Shu Yang,Jingyu Hu,Tong Li,Hanqi Yan,Wenxuan Wang,Di Wang*

Main category: cs.CL

TL;DR: 该论文提出了AutoMonitor-Bench，这是首个系统性评估大语言模型(LLM)行为监测可靠性的基准，包括问题回答、代码生成与推理等多任务与多失误情境，测试了22种主流模型，并揭示了监测性能存在显著波动及安全性与实用性的权衡。


<details>
  <summary>Details</summary>
Motivation: LLM在实际部署中存在误判坏行为与过度敏感带来的安全与可用性问题，缺乏专门的、系统性的评估基准阻碍了LLM监测器的研发与改进。论文旨在填补这一空白，推动该领域的标准化与进步。

Method: 作者构建了3010个精细标注的测试样本，覆盖问答、代码、推理等任务，并分别包含恶意与良性实例，采用Miss Rate和False Alarm Rate两项指标全面评测监测器；此外，作者训练大规模样本并微调现有模型以检验在易构造和隐式恶意数据上的泛化能力。

Result: 实验证明，无论是开源还是专有LLM，监测性能差异较大，存在明显的漏检和误报权衡；单纯在已知、简单的数据上训练，仍难以提升对未知复杂失误的检测能力。

Conclusion: 论文强调了大模型行为监测的挑战和局限，认为需要针对任务的设计与更科学的训练才能提升监测器的可靠性和泛化性，推动更安全实用的LLM部署。

Abstract: We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.

</details>


### [88] [One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models](https://arxiv.org/abs/2601.05776)
*Benedikt Ebing,Lennart Keller,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文探讨了将脚本罗马化（script romanization）应用于多语种语言模型（mLM）预训练中的效果，结果表明对使用音段文字的语言几乎无性能损失，而对于中日等形音文字语言则有性能下降，但高保真罗马化可以缓解部分问题。


<details>
  <summary>Details</summary>
Motivation: 过去的罗马化转写研究主要集中在高资源拉丁文字到低资源非拉丁文字的跨语种迁移，或在异文字表亲缘语言间，但尚未系统分析罗马化是否适合用作通用mLM预训练的输入表示，尤其是其在高资源语言中的潜在性能损失。

Method: 作者从零开始分别用原文字与罗马化文本对六种类型各异的高资源语言进行编码器语言模型预训练，比较两种不同保真度的罗马化方法，并系统考察（1）脚本特有信息丢失，（2）子词重叠增多带来的跨语种负干扰。

Result: 对于音段文字（如英文等）罗马化几乎不会带来性能损失，且能提升编码效率，而对于形音文字（如中文、日文）则罗马化带来性能下降，高保真罗马化虽有缓解但不足以完全弥补。作者未发现子词重叠的增加会引发显著的跨语种负干扰。

Conclusion: 罗马化是一种可行且高效的多语种预训练表示选择，特别适用于音段文字系统；对于形音文字还需进一步优化转写方法。整体来看，罗马化在大多数高资源语言中可显著提升效率但需根据文字类型调整策略。

Abstract: Exposing latent lexical overlap, script romanization has emerged as an effective strategy for improving cross-lingual transfer (XLT) in multilingual language models (mLMs). Most prior work, however, focused on setups that favor romanization the most: (1) transfer from high-resource Latin-script to low-resource non-Latin-script languages and/or (2) between genealogically closely related languages with different scripts. It thus remains unclear whether romanization is a good representation choice for pretraining general-purpose mLMs, or, more precisely, if information loss associated with romanization harms performance for high-resource languages. We address this gap by pretraining encoder LMs from scratch on both romanized and original texts for six typologically diverse high-resource languages, investigating two potential sources of degradation: (i) loss of script-specific information and (ii) negative cross-lingual interference from increased vocabulary overlap. Using two romanizers with different fidelity profiles, we observe negligible performance loss for languages with segmental scripts, whereas languages with morphosyllabic scripts (Chinese and Japanese) suffer degradation that higher-fidelity romanization mitigates but cannot fully recover. Importantly, comparing monolingual LMs with their mLM counterpart, we find no evidence that increased subword overlap induces negative interference. We further show that romanization improves encoding efficiency (i.e., fertility) for segmental scripts at a negligible performance cost.

</details>


### [89] [Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs](https://arxiv.org/abs/2601.05794)
*Eilam Cohen,Itamar Bul,Danielle Inbar,Omri Loewenbach*

Main category: cs.CL

TL;DR: 本文通过对比微调和提示工程两种主流技术，评估了在文本简化任务上的效果，发现微调更适合结构性简化，人类评估更倾向于微调模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型文本生成能力强大，但在微调和提示工程之间如何取舍，是实际应用中重要的问题。特别是在文本简化领域，不同处理范式各有优劣，缺乏系统比较。

Method: 作者设计了名为Simplify-This的系统对比实验，选取了多组文本简化基准数据集，针对多种指标对比微调和提示工程方法在编码-解码大模型上的表现，包括结构简化能力、语义相似度等，并进行了人工评测。

Result: 实验显示，微调模型在结构简化方面有显著优势，而提示工程虽然语义相似度更高，但经常是拷贝输入。人工主观评测结果整体更偏向于微调模型输出。

Conclusion: 研究表明，对于文本简化任务，微调更能获得符合人类偏好的输出。作者还开源了代码、数据集和模型，利于复现和后续研究。

Abstract: Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.

</details>


### [90] [EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis](https://arxiv.org/abs/2601.05808)
*Xiaoshuai Song,Haofei Chang,Guanting Dong,Yutao Zhu,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 该论文提出了EnvScaler，一个通过程序化合成实现可扩展工具交互环境的自动化框架，有效提升了大模型在复杂环境下解决任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有让大模型作为agent学习真实环境工具交互面临环境难以访问、模拟环境幻觉与不一致、手工搭建沙盒难以扩展的问题，因此需要一种自动化、可扩展的环境合成方法。

Method: EnvScaler框架由SkelBuilder和ScenGenerator两部分组成。SkelBuilder通过主题挖掘、逻辑建模与质量评估自动构建多样化环境骨架；ScenGenerator为每个环境生成多任务场景与基于规则的轨迹验证函数，共合成了191个环境和约7千个场景。

Result: 将EnvScaler合成的环境与场景用于大模型Qwen3系列的有监督微调（SFT）和强化学习（RL），在三个基准任务上的实验结果显示，大模型在多回合、多工具交互等复杂任务环境下的任务解决能力显著提升。

Conclusion: EnvScaler能够通过自动化、程序化大规模合成环境，显著提升大模型复杂环境下agent能力，并公开了相关代码与数据。

Abstract: Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.

</details>


### [91] [LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public](https://arxiv.org/abs/2601.05821)
*Milad Alshomary,Grace Li,Anubhav Jangra,Yufang Hou,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 本文提出了一种将大型语言模型（LLM）训练成“科学新闻记者”的方法，帮助初级研究人员更好地向公众沟通其科研成果，并通过模拟与真人实验验证了效果优于通用LLM。


<details>
  <summary>Details</summary>
Motivation: 初级研究人员在将自己科研成果有效传达给公众方面存在困难，现有通用LLM虽然能协助，但并未针对该任务优化。该论文旨在提升LLM用于科学传播的专业性和沟通效果。

Method: 作者提出一种训练框架，使LLM能够模拟科学新闻记者角色，引导和提升初级研究者对科研成果的公共表达能力。通过模拟与真人研究者的交互实验，评估了这种LLM的实用性和表现。

Result: 实验发现，经过该框架训练的LLM在交流中能提出更贴合社会影响的问题，促使研究者进一步阐释和澄清观点。在用户调查中，大部分参与者更偏好这种定制化LLM新闻记者，而非通用型LLM。

Conclusion: 本研究验证了使用专门训练的LLM科学记者能够更有效地辅助科学家进行大众科学传播，对提升科研成果的社会影响力具有积极促进作用。

Abstract: The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose a framework for training LLMs to emulate the role of a science journalist that can be used by early-stage researchers to learn how to properly communicate their papers to the general public. We evaluate the usefulness of our trained LLM Journalists in leading conversations with both simulated and human researchers. %compared to the general-purpose ones. Our experiments indicate that LLMs trained using our framework ask more relevant questions that address the societal impact of research, prompting researchers to clarify and elaborate on their findings. In the user study, the majority of participants who interacted with our trained LLM Journalist appreciated it more than interacting with general-purpose LLMs.

</details>


### [92] [Peek2: A Regex-free implementation of pretokenizers for Byte-level BPE](https://arxiv.org/abs/2601.05833)
*Liu Zai*

Main category: cs.CL

TL;DR: 提出了一种新的Pretokenization实现Peek2，可替换现有如cl100k的方案，实现了性能提升且更安全。


<details>
  <summary>Details</summary>
Motivation: 现有的Byte-level BPE预分词器（如cl100k等）效率有限且依赖于正则表达式，正则处理可能效率低且有安全隐患。

Method: 设计了Peek2算法，无需正则表达式，保证预分词结果与原来一致。全部在CPU上运行，复杂度为O(n)。

Result: Peek2比Regex预分词器编码流程吞吐提升1.11倍。

Conclusion: Peek2在兼容原有分词效果的同时，大幅提升了性能和安全性，是现有预分词器的优质替代。

Abstract: Pretokenization is a crucial, sequential pass in Byte-level BPE tokenizers. Our proposed new implementation, Peek2, serves as a drop-in replacement for cl100k-like pretokenizers used in GPT-3, LLaMa-3, and Qwen-2.5. Designed with performance and safety in mind, Peek2 is Regex-free and delivers a $ 1.11\times $ improvement in overall throughput across the entire Byte-level BPE encoding process. This algorithm runs entirely on the CPU, has stable linear complexity $ O(n) $, and provides presegmentation results identical to those of the original Regex-based pretokenizer.

</details>


### [93] [Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation](https://arxiv.org/abs/2601.05835)
*Molly Kennedy,Ali Parker,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文研究了当前主流大语言模型（LLM）在新闻文本生成和总结任务中呈现的政治立场倾向，结果发现主流模型普遍倾向中立立场，即“中心塌缩”。Grok 4在立场表达上最为明显，而Claude Sonnet 4.5和Llama 3.1在偏见识别打分任务上表现最好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地参与新闻文本生成与改写，文本细微措辞对政治态度的“立场定向”影响受到关注，亟需检测和理解模型输出所带的潜在政治立场倾向。

Method: 作者在九个最新LLM上，先用少样本分类预测模型的意识形态（左/中/右）倾向，再通过设定不同立场导向提示（忠实、中立、左、右）引导模型生成摘要，并用同一立场打分器对输出统一评判。

Result: 实验结果表明，多数模型有明显的中立倾向，导致“中心塌缩”现象。在生成立场表达方面，Grok 4最能表达不同政治立场；在立场打分准确性方面，Claude Sonnet 4.5和Llama 3.1分别在商用与开源模型组中表现最佳。

Conclusion: 当前主流大语言模型在新闻文本生成任务中表现出普遍的政治中立（中心倾向），但各模型间在立场表达能力和偏见识别准确性存在差异，这对AI内容生成的新闻和社会影响需持续关注和深入研究。

Abstract: Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate "steered" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.

</details>


### [94] [Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs](https://arxiv.org/abs/2601.05847)
*Rafael Brens,Yuqiao Meng,Luoxi Tang,Zhaohan Xi*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义NLP的流程，用于将非结构化EHR文本转化为符合FHIR标准的数字孪生患者模型，并在MIMIC-IV数据库上表现出高效的结构化信息抽取能力。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在医疗领域有巨大的应用潜力，但由于电子健康记录（EHR）文本的非结构化和缺乏标准化映射，难以自动生成可互操作的患者数字孪生模型。本文旨在解决文档多样性和标准化不足带来的挑战。

Method: 作者设计了一个语义NLP驱动的处理流程，包括命名实体识别（NER）用于提取临床概念，实体标准化（映射至SNOMED-CT或ICD-10），以及关系抽取以结构化疾病、药物和观察间的关联。最终输出符合FHIR标准的数字孪生表示。

Result: 在MIMIC-IV Clinical Database Demo数据集上，针对MIMIC-IV-on-FHIR的标准映射进行了验证，结果显示实体及关系抽取的F1分数很高，同时在模式完整性和互操作性方面优于基线方法。

Conclusion: 该方法能够有效提升从自由文本EHR中抽取和映射结构化医疗信息的准确性，为生成标准化、可互操作的患者数字孪生模型提供了实用技术路径。

Abstract: Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.

</details>


### [95] [Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs](https://arxiv.org/abs/2601.05851)
*Sandeep Mishra,Devichand Budagam,Anubhab Mandal,Bishal Santra,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本论文提出并系统研究了多模态自动补全（MAC）任务，即结合视觉线索和部分文本预测实时对话中的下一个字符，显著提升了数字助理等场景下的用户体验和补全质量。


<details>
  <summary>Details</summary>
Motivation: 传统自动补全主要基于文本信息，难以完整捕捉用户意图，尤其在共享视觉上下文环境下（如设计工具、数字助手、智能医疗咨询等），对更智能、可感知视觉信息的补全系统需求强烈。

Method: 提出MAC任务，并改造MMDialog和ImageChat，构建了多模态自动补全基准数据集。评估了主流视觉-语言模型（VLM）和文本基线模型，引入Router-Suggest框架，根据对话上下文智能切换文本或视觉-语言模型，并推出资源受限轻量级变体。

Result: Router-Suggest框架在保证补全准确性的同时，比最佳VLM速度提升2.3-10倍。用户研究结果表明，VLM在用户满意度、减少输入量和提升多轮对话补全质量方面明显优于文本模型。

Conclusion: 论文证明多模态上下文对自动补全效果至关重要，Router-Suggest提供高效、智能补全方案，为开发更智能、用户感知的数字助手系统指明了方向。

Abstract: Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.

</details>


### [96] [CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning](https://arxiv.org/abs/2601.05858)
*Alexandra Dragomir,Florin Brad,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 本文通过将课程学习（curriculum learning）方法融入主流偏好优化算法，提出了一种新策略CLewR，用于提升大语言模型在零样本多语言机器翻译中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已在零样本多语言翻译取得了优异表现，并且通过偏好优化进一步提升，但训练过程中样本顺序对模型表现的影响尚未得到充分探索。

Method: 将课程学习方法与主流偏好优化算法结合，提出CLewR策略，即多次重复从易到难的学习顺序，以缓解容易样本遗忘问题，并在多种模型（Gemma2、Qwen2.5、Llama3.1）和优化算法上进行实验。

Result: 所提方法在不同模型和偏好优化技术的多语言机器翻译任务上均实现了持续性提升。

Conclusion: 结合课程学习与偏好优化，尤其是通过CLewR的多次“易到难”训练顺序，能够有效提升大语言模型在多语言机器翻译场景下的性能。

Abstract: Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.

</details>


### [97] [What do the metrics mean? A critical analysis of the use of Automated Evaluation Metrics in Interpreting](https://arxiv.org/abs/2601.05864)
*Jonathan Downie,Joss Moorkens*

Main category: cs.CL

TL;DR: 该论文评估了当前自动化传译质量测量方法的适用性，认为仅凭自动化指标难以充分评估真实语境下的传译质量。


<details>
  <summary>Details</summary>
Motivation: 随着远程传译、计算机辅助传译、自动语音翻译等技术的发展，社会急需高效快速地评估各种传译服务的质量。传统的人力评估方法耗时费力，因此推动了对自动化质量测量方法的需求。

Method: 文章系统检视和比较了近期提出的各类自动化传译质量评测方法，评述这些方法能否应用于真实语境下的人类或机器传译质量评价，特别关注它们对交流语境的处理能力。

Result: 研究发现，目前的自动化质量测量方法无法有效考量实际交流的语境因素，因而单独使用时难以准确反映传译工作的真实质量。

Conclusion: 作者认为，仅靠当前自动化指标还无法胜任高质量传译评估。语境因素对最终评判至关重要，未来需结合人工与自动评测手段，才能更全面和科学地评价传译质量。

Abstract: With the growth of interpreting technologies, from remote interpreting and Computer-Aided Interpreting to automated speech translation and interpreting avatars, there is now a high demand for ways to quickly and efficiently measure the quality of any interpreting delivered. A range of approaches to fulfil the need for quick and efficient quality measurement have been proposed, each involving some measure of automation. This article examines these recently-proposed quality measurement methods and will discuss their suitability for measuring the quality of authentic interpreting practice, whether delivered by humans or machines, concluding that automatic metrics as currently proposed cannot take into account the communicative context and thus are not viable measures of the quality of any interpreting provision when used on their own. Across all attempts to measure or even categorise quality in Interpreting Studies, the contexts in which interpreting takes place have become fundamental to the final analysis.

</details>


### [98] [FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG](https://arxiv.org/abs/2601.05866)
*Maxime Dassen,Rebecca Kotula,Kenton Murray,Andrew Yates,Dawn Lawrie,Efsun Kayi,James Mayfield,Kevin Duh*

Main category: cs.CL

TL;DR: 本文提出了一个名为FACTUM的新框架，用以分析和判定RAG模型中引文幻觉的产生机制，并显著提升了判别准确性。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）模型常见的引文幻觉问题，即模型错误地自信引用不支持其说法的来源。现有研究多将这一问题归因于对模型参数化知识的过度依赖，但这种理解过于简化。

Method: 作者提出FACTUM框架，基于四个机制性评分，分别测量模型注意力和前馈网络（FFN）路径的贡献及其相互之间的一致性。通过分析不同模型规模情况下这些机制的具体表现，深入揭示引文正确与否的内在机制特征。

Result: 研究发现正确引文通常伴随模型参数知识贡献更强，以及更多利用attention sink进行信息整合。而且这种特征会随模型规模不同出现变化。例如，Llama-3.2-3B模型表现为更高的路径对齐，而Llama-3.1-8B模型则表现为更低的对齐和更独立的信息贡献。FACTUM框架相较于SOTA基线AUC提升最高达37.5%。

Conclusion: “引文幻觉”是由模型内部多种机制、且与规模相关的复杂交互所致。FACTUM通过刻画其演变特征，为更细致和可靠的RAG系统设计和评价提供了新视角。

Abstract: Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.

</details>


### [99] [Continual-learning for Modelling Low-Resource Languages from Large Language Models](https://arxiv.org/abs/2601.05874)
*Santosh Srinath K,Mudit Somani,Varun Reddy Padala,Prajna Devi Upadhyay,Abhijit Das*

Main category: cs.CL

TL;DR: 本文针对多语言场景下将大模型适配至小模型时容易出现的灾难性遗忘问题，提出了结合词性（POS）代码切换与重放适配器的持续学习方法，并在视觉语言任务和语言建模任务中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 在多语言场景中，将大型语言模型（LLM）适配到低资源语种的小型模型（SLM）时，容易出现灾难性遗忘，导致原有知识丧失。该研究旨在解决SLM训练过程中灾难性遗忘的问题。

Method: 提出在训练SLM时，采用基于词性的代码切换和带重放机制的适配器相结合的持续学习策略，减缓遗忘问题。该策略通过代码切换让模型更好地保留原任务能力，通过重放适配器维持模型记忆。

Result: 在视觉问答等视觉语言任务以及语言建模任务上，通过实验验证了提出方法在缓解灾难性遗忘方面的有效性，与现有方法相比表现更优。

Conclusion: 结合词性代码切换和重放适配器的持续学习策略能够有效减缓SLM训练中的灾难性遗忘问题，有助于多语言低资源场景下模型能力的迁移与保持。

Abstract: Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.

</details>


### [100] [iReasoner: Trajectory-Aware Intrinsic Reasoning Supervision for Self-Evolving Large Multimodal Models](https://arxiv.org/abs/2601.05877)
*Meghana Sunil,Manikandarajan Venmathimaran,Muthu Subash Kavitha*

Main category: cs.CL

TL;DR: 本文提出iReasoner框架，通过强化多模态大模型（LMMs）在推理过程中的链式思考（CoT），提升其自我进化能力，实现了在无监督条件下更强的中间推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的自进化方法多只关注最终输出结果，忽视了中间推理的约束，而中间推理对视觉推理任务至关重要。

Method: iReasoner采用Proposer--Solver循环，在无标注图片上促使模型显式生成推理链（CoT），并对中间推理的一致性进行奖励，结合结果级和路径级奖励信号，无需标注或外部裁判即可分辨不同推理路径。

Result: 在Qwen2.5-VL-7B模型基础上，iReasoner在完全无监督后训练下，在多项多模态推理基准上带来最高+2.1分的提升。

Conclusion: iReasoner为多模态大模型纯无监督环境下的推理感知自我进化提供了新思路，有望推动LMMs推理能力的进一步提升。

Abstract: Recent work shows that large multimodal models (LMMs) can self-improve from unlabeled data via self-play and intrinsic feedback. Yet existing self-evolving frameworks mainly reward final outcomes, leaving intermediate reasoning weakly constrained despite its importance for visually grounded decision making. We propose iReasoner, a self-evolving framework that improves an LMM's implicit reasoning by explicitly eliciting chain-of-thought (CoT) and rewarding its internal agreement. In a Proposer--Solver loop over unlabeled images, iReasoner augments outcome-level intrinsic rewards with a trajectory-aware signal defined over intermediate reasoning steps, providing learning signals that distinguish reasoning paths leading to the same answer without ground-truth labels or external judges. Starting from Qwen2.5-VL-7B, iReasoner yields up to $+2.1$ points across diverse multimodal reasoning benchmarks under fully unsupervised post-training. We hope this work serves as a starting point for reasoning-aware self-improvement in LMMs in purely unsupervised settings.

</details>


### [101] [Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law](https://arxiv.org/abs/2601.05879)
*Jakub Harasta,Matej Vasina,Martin Kornel,Tomas Foltynek*

Main category: cs.CL

TL;DR: 研究分析了多种主流大语言模型（LLMs）在处理家庭法律场景下的性别偏见，发现部分模型在给出亲子分配建议时表现出性别依赖性，警示了普通人依赖LLM进行法律自助时的风险。


<details>
  <summary>Details</summary>
Motivation: 随着司法资源有限，越来越多普通人在法律问题上求助于大语言模型。但这些模型的输出可能影响使用者的期望，若存在偏见或错误，可能带来公正风险。研究动机在于评估LLM在实际且敏感法律场景下是否展现出性别偏见。

Method: 研究设计了基于捷克家庭法的专业离婚案例，并在完全零样本（zero-shot）条件下，让GPT-5 nano、Claude Haiku 4.5、Gemini 2.5 Flash 和 Llama 3.3这些LLM作答。案例分性别化姓名组和中性标签组，通过对九项法律相关因素进行条件变换，评估模型在拟议的亲子分配结果中的表现。

Result: 初步实验结果表明，不同模型间存在输出差异，部分模型对不同性别案例表现出显著的性别依赖分配模式。涉及的因素能影响亲子分配建议。结果主要为探索性和描述性，旨在揭示潜在偏差。

Conclusion: 部分LLM在现实法律场景下存在性别偏向，其建议可能受输入信息的表面特征影响。对于依赖这类工具获取法律建议的普通人而言，存在误导和不公的风险，强调了在敏感法律应用中评估与改进模型行为的迫切需求。

Abstract: Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.

</details>


### [102] [An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift](https://arxiv.org/abs/2601.05882)
*Constantinos Karouzos,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统研究了在领域迁移情况下偏好微调(preference tuning)的泛化能力，并分析了不同适应策略对这种迁移退化的缓解效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现，偏好微调方法在跨域（即领域迁移）时性能下降且可用性变差，但尚未系统研究不同适应策略能否缓解这一退化问题。

Method: 作者选取了五种主流对齐目标，以及领域内和跨领域的适应策略（包括目标域的有监督微调和伪标签策略），在摘要和问答等任务上评测和比较其泛化性能。

Result: 研究发现，不同对齐目标在领域迁移下的泛化能力存在系统性差异，基于伪标签的适应策略能显著减缓跨领域退化。

Conclusion: 本文工作揭示了偏好微调在领域迁移时的瓶颈，并指出通过伪标签等适应策略可提高其跨域泛化能力，对后续模型对齐和实际应用有重要参考价值。

Abstract: Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation

</details>


### [103] [HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search](https://arxiv.org/abs/2601.05903)
*Zihang Tian,Rui Li,Jingsen Zhang,Xiaohe Bo,Wei Huo,Xu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种创新的分层大语言模型路由框架 HAPS，能够同时优化模型的结构和参数配置，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）路由方法大多只关注模型结构的选择，忽视了参数配置对任务表现的重要性。为充分利用不同模型和参数的协同作用，亟需一种融合结构和参数搜索的统一框架。

Method: 提出 HAPS 分层路由框架：高层路由器用于选择候选的模型结构，低层路由器对所选结构进一步搜索最优参数。两层通过参数生成网络共享参数，提升协同能力，并在训练时引入带奖励信号的目标函数进行优化。

Result: 在两个通用基准数据集上，HAPS 框架在多项任务中稳定优于多个强力主流路由基线方法。

Conclusion: HAPS 有效结合了模型结构和参数的联合优化，有潜力成为提升大语言模型路由性能的强大范式。

Abstract: Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.

</details>


### [104] [Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency](https://arxiv.org/abs/2601.05905)
*Haoming Xu,Ningyuan Zhao,Yunzhi Yao,Weihong Xu,Hongru Wang,Xinle Deng,Shumin Deng,Jeff Z. Pan,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的评价大模型信念韧性的方法（NCB），并通过干扰实验验证了其有效性，还提出了提升信念结构稳健性的训练方法（SAT）。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的可信度主要依赖于单点置信度（如自洽性评分），但这容易掩盖模型信念在上下文轻微干扰下的脆弱性。为保证 LLMs 在实际部署中的可靠性，需要有更全面的鲁棒性检测方法。

Method: 提出 Neighbor-Consistency Belief (NCB)，用于衡量模型在概念邻域下的信念一致性；设计认知压力测试协议，对模型输出在上下文干扰下的稳定性进行验证；并引入结构感知训练（SAT）优化信念结构的鲁棒性。

Result: 经过各种干扰实验，高-NCB 数据表现出对干扰更强的抵抗力。结构感知训练方式（SAT）能将长尾知识脆弱性降低约30%。

Conclusion: NCB 可以有效作为衡量模型信念鲁棒性的结构性指标，并能通过结构优化训练大幅增强大模型的稳定性，对实际部署具有重要意义。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.

</details>


### [105] [Pantagruel: Unified Self-Supervised Encoders for French Text and Speech](https://arxiv.org/abs/2601.05911)
*Phuong-Hang Le,Valentin Pelloin,Arnault Chatelain,Maryem Bouziane,Mohammed Ghennai,Qianwen Guan,Kirill Milintsevich,Salima Mdhaffar,Aidan Mannion,Nils Defauw,Shuyue Gu,Alexandre Audibert,Marco Dinarelli,Yannick Estève,Lorraine Goeuriot,Steffen Lalande,Nicolas Hervé,Maximin Coavoux,François Portet,Étienne Ollion,Marie Candito,Maxime Peyrard,Solange Rossato,Benjamin Lecouteux,Aurélie Nardy,Gilles Sérasset,Vincent Segonne,Solène Evain,Diandra Fabre,Didier Schwab*

Main category: cs.CL

TL;DR: Pantagruel是一组专为法语文本和语音设计的自监督编码模型，通过学习特征空间中的上下文表示，跨多项任务优于现有基线，并能统一处理语音和文本输入。


<details>
  <summary>Details</summary>
Motivation: 现有的法语文本和语音模型 غالب采用针对具体模态的目标（如词元或音素），难以充分挖掘上下文特征，并且缺乏能同时处理多模态输入的统一模型。

Method: 分别对法语大规模文本和语音语料进行预训练，其中文本采用Wikipedia、OSCAR、CroissantLLM，语音采用MultilingualLibriSpeech、LeBenchmark及新引入的INA-100k（涵盖10万小时多样法语音频），采取在特征空间中学习目标表示，使模型抓取更丰富的语言和声学特征。

Result: 在多个文本和语音下游任务，包括标准法语基准（如FLUE、LeBenchmark）上，与CamemBERT、FlauBERT、LeBenchmark2.0等强基线相比，Pantagruel模型表现出有竞争力或更优的效果。

Conclusion: Pantagruel验证了基于特征空间的自监督目标对于法语表征学习的有效性，展现出成为多模态语音-文本理解基础模型的潜力。

Abstract: We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.

</details>


### [106] [Can We Predict Before Executing Machine Learning Agents?](https://arxiv.org/abs/2601.05930)
*Jingsheng Zheng,Jintian Zhang,Yujie Luo,Yuren Mao,Yunjun Gao,Lun Du,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种替代传统物理执行的预测机制，显著加速了自主机器学习智能体的科学探索效率，提升了效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化科学发现系统受限于必须通过高成本的物理执行进行假设验证，导致执行瓶颈，效率低下。作者希望通过引入预测机制减少或替代这些昂贵的执行操作。

Method: 作者提出了一种基于内化执行先验并借鉴World Models理念的方法，实现即时性预测推理，并构建了包含18438组配对比较的数据集。他们利用LLM在有经过验证的数据分析报告提示下进行预测，并提出了FOREAGENT，实现了“预测-验证”循环。

Result: LLM在数据分析报告引导下预测偏好，准确率达61.5%，置信校准能力强。FOREAGENT实现了6倍加速，同时在效果上超越了基于物理执行的基线，上升了6%。

Conclusion: 通过引入预测机制代替或辅助执行，大大提高了科学发现智能体的效率与效果，为后续相关研究提供了新范例。

Abstract: Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.

</details>


### [107] [Distilling Feedback into Memory-as-a-Tool](https://arxiv.org/abs/2601.05960)
*Víctor Gallego*

Main category: cs.CL

TL;DR: 本文提出了一种通过文件存储的记忆系统和智能体工具调用，将临时生成的模型自我反思转化为可检索指南，从而降低推理成本的框架。实验表明，该方法能以更低成本快速达到现有高性能管道的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型往往需要在推理时反复进行精细化评估及调整，导致成本高昂，缺乏高效性；因此，提出将中间生成的评语转为可被检索利用的格式，以提高推理效率并减少资源消耗。

Method: 构建一个文件式记忆系统，让模型在完成任务和自我批评后，将有用的反思和指南保存为外部可调用的内容，并通过代理控制工具的方式在未来推理中复用这些信息。方法在Rubric Feedback Bench数据集上进行验证。

Result: 实验结果显示，经过增强后的LLM可以迅速达到需要实时精细化管道的性能，同时大幅度降低了推理时的成本。

Conclusion: 该框架有效提升了推理效率并降低了资源消耗，为实际应用中的高成本LLM推理问题提供了有价值的解决方案。

Abstract: We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.

</details>


### [108] [The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.06002)
*Qiguang Chen,Yantao Du,Ziniu Li,Jinhao Liu,Songyao Duan,Jiarui Guo,Minghao Liu,Jiaheng Liu,Tong Yang,Ge Zhang,Libo Qin,Wanxiang Che,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文指出大语言模型（LLM）难以通过模仿教学学会有效的长链式思维（Long CoT）推理，提出了一种以类似分子结构为视角的新分析方法，并推出了提升长链式推理能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理任务中表现有限，尤其难以通过人类示范或简单模仿学习到高质量的长链推理轨迹。因此，理解并提升LLM长链式推理能力成为研究热点。

Method: 作者提出将长链推理轨迹视为分子结构，由深度推理（类共价键）、自我反思（类氢键）和自我探索（类范德华力）三种交互构成，并通过分析蒸馏后的推理轨迹，发现这些结构来源于专门的Long CoT微调，而非关键词模仿。同时，定义“有效语义异构体”，并观察到只有促进信息快速收敛的结构有助于稳定学习，结构竞争则有害。继而提出Mole-Syn方法，通过分布转移图，引导生成更有效的推理结构。

Result: Mole-Syn方法在多个基准任务中显著提升了模型的长链式推理表现和RL训练稳定性。

Conclusion: 以分子结构视角解析和引导长链思维推理，有助于提升LLM在复杂推理任务中的能力，为构建更强推理能力的模型提供了新理论框架和有效方法。

Abstract: Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.

</details>


### [109] [Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2601.06007)
*Elias Lumer,Faheem Nizar,Akshaya Jangiti,Kevin Frank,Anmol Gulati,Mandar Phadate,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLM）Agent工具调用场景下的Prompt缓存策略，并在多个主流提供商和多回合代理任务中验证其对于API成本和响应延时的节省效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM Agent越来越多应用于复杂、多轮的API工具调用场景，现有厂商虽有Prompt缓存机制，但其在此类高复杂性agentic负载下的作用和性能尚未被系统研究和量化。

Method: 作者针对OpenAI、Anthropic和Google三大LLM服务商，设计并实现了三种Prompt缓存策略（全上下文缓存、仅缓存system prompt、排除动态工具结果的缓存），在真实多轮任务基准DeepResearchBench中，通过超500场10,000-token会话实验，分别度量API成本及首Token响应时间。

Result: Prompt缓存可带来45-80%的API成本节省，以及13-31%的首Token延时改进。战略性Prompt块控制（如动态内容位置调整、避免动态传统函数调用、排除动态工具调用结果）较全上下文缓存更能稳定带来益处，且间接减少了延迟。各大厂商间缓存行为存在细致差异。

Conclusion: Prompt缓存是提升LLM多轮Agent效率的有效手段，策略性缓存配置可显著优化成本和响应延时。文中经验对生产环境下Agent系统具有实用参考意义。

Abstract: Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.

</details>


### [110] [Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards](https://arxiv.org/abs/2601.06021)
*Jiajie Zhang,Xin Lv,Ling Feng,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本论文提出了一种细致的奖励机制（CaRR），提升大模型驱动的深度搜索智能体的推理完整性和事实性，并提出结合CaRR的训练方式（C-GRPO），显著优于传统基于结果的强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型深度搜索智能体主要依靠二元奖励，难以全面评估推理过程的详实和事实依据，容易导致捷径、幻想等不良行为。为提升推理质量与可靠性，亟需更细致、可解释的奖励体系。

Method: 提出Citation-aware Rubric Rewards (CaRR)机制：将复杂问题拆解为可验证的单步标准，要求智能体显式识别潜在实体、提供正确引用，并构建完整证据链。再提出Citation-aware Group Relative Policy Optimization (C-GRPO)训练方法，将CaRR与一般结果奖励结合，优化智能体推理。

Result: 实验显示，C-GRPO方法在多个深度搜索基准上稳定优于传统RL方法，表现更全面的、基于证据的推理，极大抑制了捷径和幻想等行为。

Conclusion: CaRR和C-GRPO能够有效提升深度搜索智能体的推理严谨性和事实性，并有良好的泛化能力。该方法为LMM智能体推理质量提升提供了新思路，具有潜在实际应用价值。

Abstract: Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.

</details>


### [111] [AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs](https://arxiv.org/abs/2601.06022)
*Chengming Cui,Tianxin Wei,Ziyi Chen,Ruizhong Qiu,Zhichen Zeng,Zhining Liu,Xuying Ning,Duo Zhou,Jingrui He*

Main category: cs.CL

TL;DR: AdaFuse是一种自适应集成解码框架，可以根据生成上下文动态调整不同语言模型融合的粒度，显著提升大模型组合效能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型集成方式多数采用固定粒度进行融合，难以在生成中途灵活切换，也无法针对不同任务调整组合策略，导致性能受限。

Method: 提出AdaFuse框架，通过引入不确定性判据，判断每一步生成时是否需要进行模型集成。在自信的解码状态下，直接生成输出；而对于不确定性较高的状态，采用多样性增强的缩放策略生成更多备选项，再利用集成方式做决策。整个过程以词级为基本单元，对齐并动态适应集成行为。

Result: 在开放域问答、算术推理和机器翻译等任务上，AdaFuse相对传统集成基线平均提升6.88%，表现更为优越且稳定。

Conclusion: AdaFuse实现了生成过程中自适应、动态的多模型结合，突破了现有固定粒度集成的限制，为模型集成和实际应用提供了更有效的解决方案。

Abstract: Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [112] [Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models](https://arxiv.org/abs/2601.05336)
*Tracey Yee Hsin Tay,Xu Yan,Jonathan Ouyang,Daniel Wu,William Jiang,Jonathan Kao,Yuchen Cui*

Main category: cs.RO

TL;DR: 本文提出了一种结合凝视追踪和视觉语言模型的新型机器人控制系统GAMMA，实现了无需特定任务训练下的自主操控。


<details>
  <summary>Details</summary>
Motivation: 目前在机器人控制中，实现直观、高效的人机交互（特别是在助理护理场景）依然是个难题。眼动输入具备快速、非侵入和富含意图的优势，是表达用户目标的理想方式。

Method: GAMMA系统利用自视角的凝视追踪和视觉语言模型，通过将用户的凝视与场景语义关联，推理用户意图，自动完成机器人操作任务。与传统无需推理的视线控制进行对比评估，同时在多种台面操作任务中测试系统。

Result: 实验结果表明，与基础的凝视控制相比，GAMMA系统表现出更强的鲁棒性、直观性及泛化能力。

Conclusion: 将基础模型与凝视信息结合，为自然且可扩展的机器人自主控制提供了新的可能，对助理机器人具有重要意义。

Abstract: Designing intuitive interfaces for robotic control remains a central challenge in enabling effective human-robot interaction, particularly in assistive care settings. Eye gaze offers a fast, non-intrusive, and intent-rich input modality, making it an attractive channel for conveying user goals. In this work, we present GAMMA (Gaze Assisted Manipulation for Modular Autonomy), a system that leverages ego-centric gaze tracking and a vision-language model to infer user intent and autonomously execute robotic manipulation tasks. By contextualizing gaze fixations within the scene, the system maps visual attention to high-level semantic understanding, enabling skill selection and parameterization without task-specific training. We evaluate GAMMA on a range of table-top manipulation tasks and compare it against baseline gaze-based control without reasoning. Results demonstrate that GAMMA provides robust, intuitive, and generalizable control, highlighting the potential of combining foundation models and gaze for natural and scalable robot autonomy. Project website: https://gamma0.vercel.app/

</details>


### [113] [PRISM: Protocol Refinement through Intelligent Simulation Modeling](https://arxiv.org/abs/2601.05356)
*Brian Hsu,Priyanka V Setty,Rory M Butler,Ryan Lewis,Casey Stone,Rebecca Weinberg,Thomas Brettin,Rick Stevens,Ian Foster,Arvind Ramanathan*

Main category: cs.RO

TL;DR: PRISM是一种自动化实验方案设计与执行的系统，通过语言模型和仿真环境，实现无需人工干预的实验操作流程自动化。


<details>
  <summary>Details</summary>
Motivation: 实验协议的设计和执行自动化是实现自驱动实验室的关键难题，目前存在流程繁琐、需人工介入等瓶颈。

Method: PRISM框架依赖多智能体语言模型，自动收集实验流程，从网络提取操作步骤，通过多轮反馈与验证，生成结构化实验指令，并利用虚拟环境进行预验证，最终自动输出能直接由多台实验机器人协同执行的统一格式协议。

Result: 通过与单模型方案和不同提示方式的对比，PRISM多智能体方法在实验协议生成与执行方面表现更优；并在NVIDIA Omniverse数字孪生环境中验证了其安全性和准确性。以qPCR扩增和Cell Painting为例，实现了端到端自动化实验流程。

Conclusion: PRISM打通了从AI生成实验协议、仿真验证到机器人自动执行的流程，对提升自驱动实验室的自动化程度、减少人工干预有重要促进作用。

Abstract: Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.

</details>


### [114] [Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction](https://arxiv.org/abs/2601.05491)
*Luca Nunziante,Kentaro Uno,Gustavo H. Diaz,Shreya Santra,Alessandro De Luca,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出并验证了一种双臂机器人自主组装月球太阳能电池板系统，涵盖视觉、控制与硬件一体化方法。


<details>
  <summary>Details</summary>
Motivation: 随着人类再次计划重返月球并建设月球前哨站，机器人将成为重要的基础设施建造工具，如太阳能发电塔的组装。然而，月面复杂环境和任务需求对机器人感知、控制系统及硬件设计带来极大挑战。

Method: 本文围绕双臂机器人，设计了面向空间模块化组装的感知与控制流程，并开发了专用硬件，包括太阳能电池板模块样品和主动-被动对接连接件。通过硬件系统与控制流程集成，实地测试了机器人对电池板模块的抓取和连接能力。

Result: 实验结果表明，设计的双臂机器人能自主连接位置任意的太阳能电池板模块，视觉、控制与硬件系统实现了顺畅衔接，满足复杂空间组装任务需求。

Conclusion: 本文展示了双臂机器人在月面等类空间复杂场景下组装基础设施（如太阳能板）的可行性和有效性，验证了视觉与控制算法及硬件设计的优越性能。

Abstract: Since the successful Apollo program, humanity is once again aiming to return to the Moon for scientific discovery, resource mining, and inhabitation. Upcoming decades focus on building a lunar outpost, with robotic systems playing a crucial role to safely and efficiently establish essential infrastructure such as solar power generating towers. Similar to the construction of the International Space Station (ISS), shipping necessary components via modules and assembling them in situ should be a practical scenario. In this context, this paper focuses on the integration of vision, control, and hardware systems within an autonomous sequence for a dual-arm robot system. We explore a perception and control pipeline specifically designed for assembling solar panel modules, one of the benchmark tasks. Ad hoc hardware was designed and tested in real-world experiments. A mock-up of modular solar panels and active-passive connectors are employed, with the control of this grappling fixture integrated into the proposed pipeline. The successful implementation of our method demonstrates that the two robot manipulators can effectively connect arbitrarily placed panels, highlighting the seamless integration of vision, control, and hardware systems in complex space applications.

</details>


### [115] [TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds](https://arxiv.org/abs/2601.05499)
*Weishang Wu,Yifei Shi,Zhiping Cai*

Main category: cs.RO

TL;DR: 本文提出了一种面向任务的形状补全和灵巧抓取方法，能在严重部分观测下提升机器人对开放世界物体的抓取能力。


<details>
  <summary>Details</summary>
Motivation: 开放世界环境下，机器人操控经常只能获得目标物体的部分观测数据，传统的整体形状补全在数据缺失严重时表现不佳，极大限制了抓取任务的成功率。因此，亟需一种能在残缺观测下、专注于抓取相关部位补全的新方法。

Method: 提出了“面向任务的形状补全”任务，只补全与后续操作直接相关的接触区域。具体做法是：1）利用多个具备零样本功能理解能力的大模型生成多个面向任务的形状补全候选；2）提出三维判别自编码器对候选方案进行甄别与全局优化；3）开发条件流匹配模型FlowGrasp，从优化过的形状中生成面向任务的灵巧抓取动作。

Result: 方法在面向任务的灵巧抓取与形状补全两项任务上均取得了SOTA，抓取偏移量提升16.17%，Chamfer距离提升55.26%，尤其在严重缺失数据下抓取能力突出，并展现了良好的泛化性。

Conclusion: 本文方法突破了部分观测下的传统限制，显著提升了开放环境下机器人的操作能力，为实际应用提供了有效方案。

Abstract: Task-oriented dexterous grasping remains challenging in robotic manipulations of open-world objects under severe partial observation, where significant missing data invalidates generic shape completion. In this paper, to overcome this limitation, we study Task-Oriented Shape Completion, a new task that focuses on completing the potential contact regions rather than the entire shape. We argue that shape completion for grasping should be explicitly guided by the downstream manipulation task. To achieve this, we first generate multiple task-oriented shape completion candidates by leveraging the zero-shot capabilities of object functional understanding from several pre-trained foundation models. A 3D discriminative autoencoder is then proposed to evaluate the plausibility of each generated candidate and optimize the most plausible one from a global perspective. A conditional flow-matching model named FlowGrasp is developed to generate task-oriented dexterous grasps from the optimized shape. Our method achieves state-of-the-art performance in task-oriented dexterous grasping and task-oriented shape completion, improving the Grasp Displacement and the Chamfer Distance over the state-of-the-art by 16.17\% and 55.26%, respectively. In particular, it shows good capabilities in grasping objects with severe missing data. It also demonstrates good generality in handling open-set categories and tasks.

</details>


### [116] [Learning specifications for reactive synthesis with safety constraints](https://arxiv.org/abs/2601.05533)
*Kandai Watanabe,Nicholas Renninger,Sriram Sankaranarayanan,Morteza Lahijanian*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人模仿学习方法，通过建模潜在任务为概率型形式语言，并在学习过程中加入安全约束，实现了机器人在动态环境下自主执行复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习方法在面对动态环境和复杂任务时存在安全性不足和无法平衡用户偏好与机器人成本的问题，因此需要一种兼顾安全性和用户需求的新方法。

Method: 将任务建模为概率确定性有限自动机（PDFA），适配现有状态合并算法并引入安全约束；使用多目标反应式综合算法生成满足安全和任务需求的确定性策略，并通过博弈建模机器人与环境的交互，用值迭代算法求取帕累托最优策略集。

Result: 实验证明该算法适用于多种机器人和任务，学到的PDFA始终避免不安全行为，综合合成的策略可以同时满足用户偏好和机器人成本约束，较好地完成任务。

Conclusion: 本文方法有效提升了机器人在动态环境下复杂任务的安全自主执行能力，实现了任务完成、安全性、用户偏好和机器人成本之间的良好平衡。

Abstract: This paper presents a novel approach to learning from demonstration that enables robots to autonomously execute complex tasks in dynamic environments. We model latent tasks as probabilistic formal languages and introduce a tailored reactive synthesis framework that balances robot costs with user task preferences. Our methodology focuses on safety-constrained learning and inferring formal task specifications as Probabilistic Deterministic Finite Automata (PDFA). We adapt existing evidence-driven state merging algorithms and incorporate safety requirements throughout the learning process to ensure that the learned PDFA always complies with safety constraints. Furthermore, we introduce a multi-objective reactive synthesis algorithm that generates deterministic strategies that are guaranteed to satisfy the PDFA task while optimizing the trade-offs between user preferences and robot costs, resulting in a Pareto front of optimal solutions. Our approach models the interaction as a two-player game between the robot and the environment, accounting for dynamic changes. We present a computationally-tractable value iteration algorithm to generate the Pareto front and the corresponding deterministic strategies. Comprehensive experimental results demonstrate the effectiveness of our algorithms across various robots and tasks, showing that the learned PDFA never includes unsafe behaviors and that synthesized strategies consistently achieve the task while meeting both the robot cost and user-preference requirements.

</details>


### [117] [EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium](https://arxiv.org/abs/2601.05653)
*Phu-Hoa Pham,Chi-Nguyen Tran,Duy-Minh Dao-Sy,Phu-Quy Nguyen-Lam,Trung-Kiet Huynh*

Main category: cs.RO

TL;DR: 本文提出了一种新型自动驾驶交通仿真方法（EvoQRE），利用进化博弈和量化响应均衡，更真实地模拟受认知和感知约束的人类驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶交通仿真多基于完全理性假设（如模仿学习、博弈均衡），但实际人类驾驶行为受限于理性，无法做到完全最优，急需一种能捕捉触发安全风险的人类行为的新框架。

Method: EvoQRE将预训练生成模型与熵正则化的复制动态相结合，在通用和安全关键的马尔可夫博弈中，利用量化响应均衡（QRE）与进化博弈动力学，逼真建模具有限定理性的“真实”人类驾驶决策，并推广QRE到连续动作空间。理论上证明新方法能以显式速率收敛至Logit-QRE（在弱单调性下收敛速率为O(log k / k^{1/3})）。

Result: 在Waymo Open Motion和nuPlan数据集上的实验表明，EvoQRE实现了更真实的交通仿真、更优的安全性评价指标，并且能通过可解释的理性参数灵活生成多样化的安全关键场景。

Conclusion: EvoQRE突破了完全理性假设，实现了对人类有限理性驾驶行为的高仿真、安全和可控建模，有助于自动驾驶技术的测试和发展。

Abstract: Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative world model with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.

</details>


### [118] [Motion Compensation for Real Time Ultrasound Scanning in Robotically Assisted Prostate Biopsy Procedures](https://arxiv.org/abs/2601.05661)
*Matija Markulin,Luka Matijević,Luka Siktar,Janko Jurdana,Branimir Caran,Marko Švaco,Filip Šuligoj,Bojan Šekoranja*

Main category: cs.RO

TL;DR: 本文提出了一种用于前列腺超声辅助扫描的机器人系统，可实现自主扫描和三维重建，以辅助前列腺穿刺活检。实验表明该系统扫描和重建速度快，精确度和稳健性良好，并且对不同运动场景下的误差和延时进行了评估。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌诊断依赖穿刺活检，而其操作高度依赖于医生的手艺和经验，导致诊断结果存在较大的人为差异。因此，亟需自动化、高效、精准且易于推广的方法来辅助前列腺超声扫描和活检，提高诊断的一致性和效果。

Method: 研究团队搭建了实验平台，利用协作机器人自主扫描前列腺模型，并通过另一直连模型的医疗机械臂模拟患者运动。机器人保持超声探头与前列腺的相对位置恒定，获取多层超声图像，对每层切片分割，得到前列腺轮廓，进一步转换为三维点云，用于后续活检规划。系统在静止和多种运动（水平、垂直、组合）条件下进行实验，采用点云配准评估重建精度，并分析运动跟踪误差与延时。

Result: 系统平均扫描1次前列腺用时30秒，三维重建用时3秒。在不同运动场景下，点云配准的拟合度约80%~84%，均方根误差在0.35~0.37毫米之间；最大运动跟踪误差3毫米，最大运动补偿延时0.5秒。

Conclusion: 所开发的机器人系统能够在模拟患者多种动态状态下，保持较高的扫描和重建精度，有助于降低医生操作难度，提高前列腺穿刺活检的标准化和普及性。该系统的精度和响应速度已达临床前列腺活检要求。

Abstract: Prostate cancer is one of the most common types of cancer in men. Its diagnosis by biopsy requires a high level of expertise and precision from the surgeon, so the results are highly operator-dependent. The aim of this work is to develop a robotic system for assisted ultrasound (US) examination of the prostate, a prebiopsy step that could reduce the dexterity requirements and enable faster, more accurate and more available prostate biopsy. We developed and validated a laboratory setup with a collaborative robotic arm that can autonomously scan a prostate phantom and attached the phantom to a medical robotic arm that mimics the patient's movements. The scanning robot keeps the relative position of the US probe and the prostate constant, ensuring a consistent and robust approach to reconstructing the prostate. To reconstruct the prostate, each slice is segmented to generate a series of prostate contours converted into a 3D point cloud used for biopsy planning. The average scan time of the prostate was 30 s, and the average 3D reconstruction of the prostate took 3 s. We performed four motion scenarios: the phantom was scanned in a stationary state (S), with horizontal motion (H), with vertical motion (V), and with a combination of the two (C). System validation is performed by registering the prostate point cloud reconstructions acquired during different motions (H, V, C) with those obtained in the stationary state. ICP registration with a threshold of 0.8 mm yields mean 83.2\% fitness and 0.35 mm RMSE for S-H registration, 84.1\% fitness and 0.37 mm RMSE for S-V registration and 79.4\% fitness and 0.37 mm RMSE for S-C registration. Due to the elastic and soft material properties of the prostate phantom, the maximum robot tracking error was 3 mm, which can be sufficient for prostate biopsy according to medical literature. The maximum delay in motion compensation was 0.5 s.

</details>


### [119] [InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection](https://arxiv.org/abs/2601.05805)
*Simon Archieri,Ahmet Cinar,Shu Pan,Jonatan Scharff Willners,Michele Grimald,Ignacio Carlucho,Yvan Petillot*

Main category: cs.RO

TL;DR: 本文提出了InsSo3D，一种基于3D声纳与惯性导航系统联合进行大规模三维SLAM的高效精确方法。该方法可在浑浊水下环境中实现低误差的定位与建图。


<details>
  <summary>Details</summary>
Motivation: 传统声纳SLAM方法仅能获取二维数据，存在高程歧义，并且容易受到水下环境影响，导致定位与建图精度降低，难以适应大规模和复杂场景。为解决这一瓶颈，作者引入三维声纳以获得稠密3D点云，并结合惯性导航，提升整体鲁棒性与精度。

Method: 作者提出了基于3D声纳点云和INS惯导信息的现代SLAM框架。该方法用惯导信息提供先验，通过检测回环和位姿图优化，显著减少里程计漂移。系统在有真实轨迹的测试池和开阔露天采石场水域进行了实验验证。

Result: 实验结果表明，InsSo3D能将平均轨迹误差控制在21cm以内，50分钟任务期间能生成10m×20m区域、平均重建误差仅为9cm的高质量地图。系统可安全应用于复杂或浑浊水环境下的结构检测。

Conclusion: InsSo3D结合3D声纳和惯导，显著提升了大规模水下SLAM的精度与鲁棒性，适用于实际复杂水下环境，对于水下结构检测等任务具有广阔应用前景。

Abstract: This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS). Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity. We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation. We evaluated InsSo3D performance inside a test tank with access to ground truth data and in an outdoor flooded quarry. Comparisons to reference trajectories and maps obtained from an underwater motion tracking system and visual Structure From Motion (SFM) demonstrate that InsSo3D efficiently corrects odometry drift. The average trajectory error is below 21cm during a 50-minute-long mission, producing a map of 10m by 20m with a 9cm average reconstruction error, enabling safe inspection of natural or artificial underwater structures even in murky water conditions.

</details>


### [120] [Modular Autonomy with Conversational Interaction: An LLM-driven Framework for Decision Making in Autonomous Driving](https://arxiv.org/abs/2601.05806)
*Marvin Seegert,Korbinian Moller,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种将大型语言模型（LLM）与自动驾驶系统（ADS）集成的自然语言交互框架，显著提升了乘客通过自然语言控制和互动自动驾驶系统的能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统的用户交互方式较为死板，难以自然地映射复杂的人类语言至系统指令，限制了用户体验和系统可用性。作者希望利用LLM提升人与自动驾驶系统之间的自然互操作性。

Method: 作者设计了集成LLM的交互层与知名自动驾驶开源软件Autoware结合的框架。方法分为三部分：1）系统性梳理交互类别并建立分类，2）设计面向应用的领域专用语言（DSL）用于命令翻译，3）增加安全验证层。采用两阶段LLM架构，以提升透明度和及时反馈指令执行状态。

Result: 系统在时效性、指令翻译鲁棒性方面均表现良好。在仿真环境下，所有五类交互命令均被成功验证，展现了方法的有效性和通用性。

Conclusion: 该工作为模块化、注重安全的自动驾驶软件引入了可扩展的自然语言交互基础，有助于未来DSA系统交互方式的智能化和人性化。

Abstract: Recent advancements in Large Language Models (LLMs) offer new opportunities to create natural language interfaces for Autonomous Driving Systems (ADSs), moving beyond rigid inputs. This paper addresses the challenge of mapping the complexity of human language to the structured action space of modular ADS software. We propose a framework that integrates an LLM-based interaction layer with Autoware, a widely used open-source software. This system enables passengers to issue high-level commands, from querying status information to modifying driving behavior. Our methodology is grounded in three key components: a taxonomization of interaction categories, an application-centric Domain Specific Language (DSL) for command translation, and a safety-preserving validation layer. A two-stage LLM architecture ensures high transparency by providing feedback based on the definitive execution status. Evaluation confirms the system's timing efficiency and translation robustness. Simulation successfully validated command execution across all five interaction categories. This work provides a foundation for extensible, DSL-assisted interaction in modular and safety-conscious autonomy stacks.

</details>


### [121] [Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning](https://arxiv.org/abs/2601.05836)
*Sheng-Kai Chen,Jyh-Horng Wu*

Main category: cs.RO

TL;DR: 本文提出了一种融合模糊逻辑安全系统与强化学习算法的UR10机械臂奇异点检测与规避方法，实现了安全高效的路径规划。


<details>
  <summary>Details</summary>
Motivation: 机械臂在操作过程中可能因奇异点导致失控或设备损坏，亟需有效的检测与规避机制以提高作业安全性和系统可靠性。

Method: 采用基于操作性度量与条件数分析的实时奇异点检测，结合模糊逻辑进行安全判断，并将其与稳定的强化学习路径规划框架融合。系统通过PyBullet仿真进行训练数据收集，并可连接URSim实现真实部署。

Result: 实验表明，该系统在保证远离奇异配置的同时，实现了90%的到达目标成功率。

Conclusion: 所提出的混合方法在奇异点检测与安全规避方面表现优异，有效提高了UR10机械臂在实际应用中的安全性及路径规划鲁棒性。

Abstract: This paper presents a comprehensive approach to singularity detection and avoidance in UR10 robotic arm path planning through the integration of fuzzy logic safety systems and reinforcement learning algorithms. The proposed system addresses critical challenges in robotic manipulation where singularities can cause loss of control and potential equipment damage. Our hybrid approach combines real-time singularity detection using manipulability measures, condition number analysis, and fuzzy logic decision-making with a stable reinforcement learning framework for adaptive path planning. Experimental results demonstrate a 90% success rate in reaching target positions while maintaining safe distances from singular configurations. The system integrates PyBullet simulation for training data collection and URSim connectivity for real-world deployment.

</details>
