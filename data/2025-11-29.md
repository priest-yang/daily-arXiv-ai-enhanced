<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 117]
- [cs.CL](#cs.CL) [Total: 46]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?](https://arxiv.org/abs/2511.20710)
*David Amebley,Sayanton Dibbo*

Main category: cs.CV

TL;DR: 本文研究了多模态视觉-语言模型（VLMs）在黑盒隐私攻击（成员推断攻击）下的隐私保护能力，并提出了一种神经科学启发的拓扑正则化方法，有效提升了模型对隐私攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型（MMs）广泛应用，其数据泄露的隐私威胁增加。此前工作主要关注单模态模型的隐私攻击，尚缺乏对多模态模型特别是神经启发式改进后的模型在隐私防护上的系统研究。

Method: 提出并应用了一种神经科学启发的拓扑正则化（tau）方法对VLM进行训练，并在多模态成员推断攻击场景下，通过不同数据集和基线模型对比其抵抗隐私攻击的能力，评估包括BLIP、PaliGemma 2和ViT-GPT2三种模型。

Result: 实验证明，带有拓扑正则化（tau>0）的神经VLM在COCO数据集上能将成员推断攻击的成功率（ROC-AUC）降低24%，且生成性能（MPNet、ROUGE-2指标）基本不受影响，在其他模型和数据集上的结果也验证了结论的一致性。

Conclusion: 神经科学启发的方法能显著提升多模态大模型抵御隐私攻击的能力，且不损失模型效用，为多模态大模型的隐私防护研究提供了新方向和理论支持。

Abstract: In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.

</details>


### [2] [Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation](https://arxiv.org/abs/2511.20714)
*Inferix Team,Tianyu Feng,Yizeng Han,Jiahao He,Yuanyu He,Xi Lin,Teng Liu,Hanfeng Lu,Jiasheng Tang,Wei Wang,Zhiyuan Wang,Jichao Wu,Mingyang Yang,Yinghao Yu,Zeyu Zhang,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文介绍了一种新型世界模型推理引擎Inferix，利用优化的半自回归解码实现高质量、长时段、交互式视频生成，提升物理真实性和稳定性，推动视觉基础模型范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在视觉理解、推理和物理真实感方面存在局限，难以支持长时段、高互动性的世界模拟。大规模世界模型具备解锁新兴视觉能力的潜力，但对推理效率和生成质量提出了更高要求。

Method: 核心方法采用半自回归（block-diffusion）解码，将扩散模型与自回归模型优势结合，分块扩散并用KV Cache实现高效且可变长度的视频生成。Inferix专门针对世界模拟场景优化推理流程，并集成了LV-Bench基准用于分钟级视频生成评测。

Result: Inferix实现了长时段、高质量、可交互的视频流生成，相较传统视频扩散模型或高并发优化系统有独特优势，并支持实时互动和动态世界建模。

Conclusion: Inferix为世界模型社区提供了有力的推理平台，有望推动视觉基础模型从当前的LLM中心范式迈向新纪元，加强逼真世界模拟与交互，建议社区参与共建优化。

Abstract: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.

</details>


### [3] [Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?](https://arxiv.org/abs/2511.20716)
*Kun Guo,Yun Shen,Xijun Wang,Chaoqun You,Yun Rui,Tony Q. S. Quek*

Main category: cs.CV

TL;DR: 本文提出了一种结合本地跟踪与边缘检测的视频目标识别框架，并通过深度强化学习自适应选择检测方案，在多设备场景下利用联邦学习提升协作和泛化能力。实验结果显示该方法在Raspberry Pi等资源受限设备上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备（如监控摄像头）上，由于计算能力有限，实现快速且高精度的视频目标识别是一大挑战；而简单依赖边缘计算虽有助提升精度，但难以权衡延迟和本地计算负载。为此需要一种智能化策略，动态决定何时本地跟踪、何时边缘检测。

Method: 提出以深度强化学习为核心的LTED-Ada算法，结合视频帧间相关性及延迟/精度需求，自适应决定本地跟踪或边缘检测。单设备场景下直接训练， 多设备场景下引入联邦学习，通过设备间协作提升泛化和综合性能。

Result: 在Raspberry Pi 4B与边缘服务器搭建的硬件平台上进行了大量实验，LTED-Ada在不同帧率和性能要求下都优于传统固定决策方法，表现出更高的精度和更低的延迟。

Conclusion: 采用基于深度强化学习和联邦学习的自适应边缘视频识别方案，可有效兼顾识别精度与速度，适合部署到移动边缘环境，并具备良好的多设备扩展性和实际应用价值。

Abstract: Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.

</details>


### [4] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU,Lianming Huang,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: 本文提出 DeeAD 框架，通过早期退出方式加速视觉-语言动作（VLA）模型在自动驾驶中的推理过程，同时保持决策质量和安全性。


<details>
  <summary>Details</summary>
Motivation: VLA 模型集成了视觉理解、推理与轨迹生成，但深层 Transformer 网络带来推理延迟，影响自动驾驶实时性，亟需有效加速方法。

Method: 提出无须重新训练的 DeeAD 框架，根据中间轨迹与轻量规划先验（如导航或低精度规划）的一致性，及分数变化率，自适应层级跳跃或提前终止推理，减少不必要的网络计算。

Result: 在 Bench2Drive 基准测试中，DeeAD 可减少最多 28% 的 Transformer 层计算，降低 29% 的延迟，并保持原有的规划质量与安全水平。

Conclusion: DeeAD 框架为现有 VLA 模型提供了高效的加速方案，在无需额外训练的前提下，显著提升了推理速度并兼顾安全性，适合实际自动驾驶部署。

Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.

</details>


### [5] [Foundry: Distilling 3D Foundation Models for the Edge](https://arxiv.org/abs/2511.20721)
*Guillaume Letellier,Siddharth Srivastava,Frédéric Jurie,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种新的大模型压缩方法Foundation Model Distillation（FMD），并在3D点云领域首次实现这一技术，可有效压缩基础模型，在保持通用性的同时大幅降低计算与存储需求，为边缘设备应用带来更好的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前预训练基础模型虽具备强大通用特征提取能力，但因其体积庞大和高算力要求，难以在机器人、AR/VR等边缘设备部署；现有一般知识蒸馏方法虽可压缩模型，却通常牺牲了模型的通用性。为此，亟需一种新方法兼顾压缩效率与基础模型的下游通用泛化能力。

Method: 提出Foundation Model Distillation（FMD）新范式，并实现了第一个针对3D点云的实例Foundry。具体方法为：学生模型通过学习压缩的SuperTokens集合，能够重建教师模型的token级特征表示，从而形成其潜在空间的紧凑基底，实现模型体积和计算量大幅降低的同时，保持表征能力的泛化性。

Result: 单一蒸馏模型在分类、零样本学习与部件分割等多种下游任务中，展现出接近完整基础模型的迁移性能，同时所需token数量和FLOPs显著减少，更适合资源受限的硬件部署。

Conclusion: FMD为大规模自监督预训练模型的高效压缩与移植提供了新方案，特别适用于3D点云等需边缘部署场景，在保证通用表达能力的同时大幅提升了模型的实用性。

Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.

</details>


### [6] [DinoLizer: Learning from the Best for Generative Inpainting Localization](https://arxiv.org/abs/2511.20722)
*Minh Thong Doi,Jan Butora,Vincent Itier,Jérémie Boulanger,Patrick Bas*

Main category: cs.CV

TL;DR: 本文提出了DinoLizer，一种基于DINOv2的生成图像修复篡改区域定位模型，在多项基准测试上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成式修复（inpainting）技术日益成熟，常被用来篡改图像内容，导致视觉内容真实性受到挑战。现有检测方法在定位具体篡改区域、对不同生成模型和后处理操作的鲁棒性方面仍有不足，亟需更强大、泛化能力更强的篡改区域检测工具。

Method: 本方法基于在B-Free数据集上预训练的DINOv2模型，专注于检测合成图像。在ViT（视觉变换器）的patch embedding之上加上线性分类头来预测$14\times 14$分辨率的patch是否被篡改。训练时关注语义层面的篡改，对非语义编辑部分视为原内容。由于ViT输入尺寸固定，作者采用滑动窗口策略对大尺寸图片进行预测，并经过后处理生成精细的二值篡改掩码。

Result: 实验证明，DinoLizer在多种流行的修复数据集上超越了主流同类检测算法，对如缩放、加噪声、JPEG压缩等常见后处理操作表现出极高的鲁棒性。平均IoU指标比次优模型高12%，后处理后提升更明显。对比试验表明DINOv2的表现优于DINOv3。

Conclusion: DinoLizer展示了基于大规模自监督视觉变换器在本任务中的强大潜力，可为深度合成图像检测领域提供有力工具。论文承诺代码开源，便于后续研究发展。

Abstract: We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.

</details>


### [7] [CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design](https://arxiv.org/abs/2511.20737)
*Daeheon Jeong,Seoyeon Byun,Kihoon Son,Dae Hyun Kim,Juho Kim*

Main category: cs.CV

TL;DR: 本文提出了CANVAS基准，用于评测视觉语言模型（VLM）在界面设计软件中基于工具操作完成UI设计任务的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs具备工具调用能力，有望辅助设计师进行UI迭代设计，但当前缺乏针对工具操作设计性能的评测基准。因此，VLMs在这方面的实际能力尚不明确。

Method: 作者提出了CANVAS基准，包含598个与真实参考答案配对的工具操作设计任务，覆盖30种UI功能类别。任务分为两个类型：一是设计复刻，考察模型复原完整UI界面的能力；二是设计修改，考察对已有界面特定部分的修改能力。通过与设计软件联动实现逐步操作。

Result: 结果显示，领先的模型能进行更有策略性的工具调用，从而提升设计质量。研究还总结了模型较常见的错误类型。

Conclusion: CANVAS有助于衡量和提升VLM在真实设计场景中基于工具的协作和设计能力，为未来工具型AI设计协作研究提供重要基础。

Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.

</details>


### [8] [Text-Guided Semantic Image Encoder](https://arxiv.org/abs/2511.20770)
*Raghuveer Thirukovalluru,Xiaochuang Han,Bhuwan Dhingra,Emily Dinan,Maha Elbayad*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本引导型语义图像编码器（TIE），使得图像的编码能够根据输入文本查询进行调整，从而显著提升视觉-语言模型（VLMs）在多个基准任务上的表现，并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 目前主流视觉-语言模型中的图像编码器通常是独立预训练的，在下游任务和文本查询时并未根据具体需求调整。因此，图像编码器输出的特征对任务和查询缺乏针对性，影响模型表现。

Method: 作者提出了TIE，将输入文本查询作为条件信息引入图像编码过程，使生成的图像表示与文本语义关联。实验对比了TIE与传统编码器在1B和3B规模下九个图文任务上的表现，并分析了其泛化能力和注意力机制。

Result: TIE使VLMs在九个图像文本任务的平均成绩分别提升了1.5和1.3分，在DocVQA、InfoVQA等任务上提升可达6分。同时，TIE只需使用传统方法一半的图像token，推理效率明显提高。

Conclusion: TIE能有效增强模型对文本查询的感知，提升模型性能和效率，并具有良好的泛化能力和可解释性。

Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.

</details>


### [9] [One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues](https://arxiv.org/abs/2511.20784)
*Sindhuja Penchala,Gavin Money,Gabriel Marques,Samuel Wood,Jessica Kirschman,Travis Atkison,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 该论文提出了一种新模型SMARC，仅凭一小块图像即可实现表面重建与材料分类，并且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有材料表面识别与重建方法大多依赖全景或密集观测数据，无法适应视野受限或仅有部分可见的实际场景。因此急需在稀疏视觉输入下实现高效表面理解的新方法。

Method: 作者提出SMARC模型，在输入只有连续10%图像块时，可重建完整RGB表面并分类材料。SMARC基于Partial Convolutional U-Net结构，结合分类分支，实现空间补全与语义识别。

Result: 实验基于真实世界表面数据集Touch and Go，SMARC与五种主流模型（卷积自编码器、ViT、MAE、Swin Transformer、DETR）对比，在PSNR达17.55 dB，材料分类准确率达85.10%，均取得最佳性能。

Conclusion: SMARC方法在极端稀疏观测条件下，实现了高效的空间推断与语义理解，证明了部分卷积结构处理缺失数据的优势，为最小视觉表面理解奠定了基础。

Abstract: Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.

</details>


### [10] [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)
*Zuhao Yang,Sudong Wang,Kaichen Zhang,Keming Wu,Sicong Leng,Yifan Zhang,Chengwei Qin,Shijian Lu,Xingxuan Li,Lidong Bing*

Main category: cs.CV

TL;DR: 本论文提出LongVT框架，提升大规模多模态模型对长视频推理的能力，通过模拟人类观片方式，结合多模态链式工具思维，实现全局到局部的视频理解，并构建新数据集VideoSIAH验证效果，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在长视频理解任务表现有限，尤其当证据稀疏且分散时，常会产生幻觉（hallucination）。此外，长视频领域缺乏细粒度问答数据，限制了模型的训练和验证。

Method: 提出LongVT——一种端到端的agentic推理框架，采用交错的多模态链式工具思维。流程先进行全局浏览，再借助LMM的时序定位能力聚焦关键片段，反复细化直至找到有力证据；并构建包含多种训练及测试样本集的VideoSIAH数据集，实现三阶段训练策略，包括冷启动监督微调、agentic强化学习及对应微调。

Result: LongVT在四个长视频理解与推理挑战任务中，均显著优于当前主流强基线模型。数据与模型开放，便于社区复现和验证。

Conclusion: 长视频推理可借助人类一样的全局到局部思维方式，以多模态链式工具流程提升模型时序感与证据定位能力，结合新数据集和多阶段训练能极大改进当前大模型实际表现。

Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .

</details>


### [11] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta,Keshav Bulia,Neena S Nair*

Main category: cs.CV

TL;DR: 本文对KRISP模型进行轻量级复刻，参数量大幅减少，性能为原模型75%，并揭示出原模型设计上的一些问题，展示了在资源受限场景下知识增强VQA模型的可扩展性。


<details>
  <summary>Details</summary>
Motivation: KRISP原模型性能优异，但计算量大且依赖大规模骨干网络，不适合资源有限场景。本研究希望在大幅减少参数的情况下，探索知识增强VQA模型在边缘设备等受限环境中的表现。

Method: 以KRISP为基础，设计了轻量版复现模型，显著降低参数数量，并通过有约束的知识图谱域控制输出，防止AI幻觉。通过消融实验和在合成VQA数据及DAQUAR数据集上的评测，检验模型表现和可扩展性。

Result: 复现模型仅为原KRISP参数量的一小部分，性能能达到后者的75%。消融实验揭示了原设计中的潜在问题，证明了在有限资源下知识增强VQA模型的可用性和优势。

Conclusion: 轻量级KRISP不仅有助于避免AI幻觉及输出不可控的问题，还能在边缘设备等场合高效运行，推动知识增强视觉推理技术的实际应用。

Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.

</details>


### [12] [Intriguing Properties of Dynamic Sampling Networks](https://arxiv.org/abs/2511.20800)
*Dario Morle,Reid Zaffino*

Main category: cs.CV

TL;DR: 本文提出并分析了一个统一多种动态采样方法的新算子“warping”，揭示了其理论基础、训练稳定性的条件，并通过新颖的可视化方法加深了对动态采样网络学习行为的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管动态采样机制在计算机视觉深度学习模型中被广泛应用，但缺乏统一的理论分析。本研究旨在将现有不同动态采样方法进行关联与理论统一，阐明其共性与训练稳定性。

Method: 作者提出并分析了一个新算子“warping”，可以最小化动态采样结构并利于理论分析。通过该框架，重构了包括可变形卷积、主动卷积单元、空间变换网络等已存在结构。对该算子在输入为IID变量和齐次随机场两种情形下进行了统计分析，并研究了前向与反向传播中的独特不对称性。另外还分析了动态采样网络的离散化效应，并提出了一种利用梯度信息的新损失景观可视化方法。

Result: 理论和实证结果表明，“warping”代表了一类与经典卷积不同的正交操作，通过分析，明确了保证动态采样网络稳定训练的必要条件，且可视化方法帮助更深刻理解其学习行为。

Conclusion: 本文为动态采样机制提供了统一的理论基础，丰富了对该类方法特性的理解，并为其稳定应用及后续网络结构设计提供了指导与工具。

Abstract: Dynamic sampling mechanisms in deep learning architectures have demonstrated utility across many computer vision models, though the theoretical analysis of these structures has not yet been unified. In this paper we connect the various dynamic sampling methods by developing and analyzing a novel operator which generalizes existing methods, which we term "warping". Warping provides a minimal implementation of dynamic sampling which is amenable to analysis, and can be used to reconstruct existing architectures including deformable convolutions, active convolutional units, and spatial transformer networks. Using our formalism, we provide statistical analysis of the operator by modeling the inputs as both IID variables and homogeneous random fields. Extending this analysis, we discover a unique asymmetry between the forward and backward pass of the model training. We demonstrate that these mechanisms represent an entirely different class of orthogonal operators to the traditional translationally invariant operators defined by convolutions. With a combination of theoretical analysis and empirical investigation, we find the conditions necessary to ensure stable training of dynamic sampling networks. In addition, statistical analysis of discretization effects are studied. Finally, we introduce a novel loss landscape visualization which utilizes gradient update information directly, to better understand learning behavior.

</details>


### [13] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: 本文提出了$Δ$-NeRF，这是一种用于增量式NeRF细化的模块化残差框架，有效解决了传统NeRF在增量学习场景下灾难性遗忘和高训练成本的问题。实验显示，$Δ$-NeRF可大幅提升性能并降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在面对数据流式到来时，无法高效增量更新，只能全量重训，特别是在卫星地形分析等需要反复观测的领域，极大限制了应用。现有简单增量方法又存在灾难性遗忘和性能下降的问题。

Method: $Δ$-NeRF引入了几个创新技术：(1) 残差控制器模块，在不访问旧数据的情况下对冻结的基础NeRF逐层增量校正；(2) 不确定性感知门控机制，自适应融合基础与校正后的预测，避免过度修正；(3) 新颖的视图选择策略，在保持性能的同时减少47%的训练数据量；此外还结合知识蒸馏，将增强模型压缩成仅为原始系统20%大小的紧凑学生网络。

Result: 在卫星影像数据集上，$Δ$-NeRF的性能与全量联合训练相当，但训练时间减少30-42%，同时在PSNR等指标上，相比简单微调最多提升43.5%，一些指标上超越联合训练，并始终优于现有基线。

Conclusion: $Δ$-NeRF为增量NeRF学习提供了高效、有效的解决方案，显著提升了实际场景下模型的易用性与性能，为需要流式数据处理的3D重建领域带来新的技术突破。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [14] [Layer-Aware Video Composition via Split-then-Merge](https://arxiv.org/abs/2511.20809)
*Ozgur Kara,Yujia Chen,Ming-Hsuan Yang,James M. Rehg,Wen-Sheng Chu,Du Tran*

Main category: cs.CV

TL;DR: 该论文提出了Split-then-Merge (StM) 框架，通过分割视频为前景和背景层，并自我组合，实现了生成式视频合成的更强控制与数据高效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频合成方法依赖标注数据或手工规则，难以高效获得大规模、丰富交互的视频数据，且数据稀缺制约了模型表现。

Method: StM框架将大量无标注视频自动分割为动态前景与背景层，通过自我组合训练模型学习前景与不同场景的交互动态。方法包括变换感知的训练流程、多层融合与增强、以及保持前景一致性的身份保留损失。

Result: 实验表明，StM在定量指标和人类/VLLM定性评价中均优于当前最优方法（SoTA）。

Conclusion: StM框架不依赖标注数据，能高效学习复杂动态合成，为真实感视频生成带来新突破。

Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io

</details>


### [15] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam,Saksham Aggarwal,Justin Yang Chae,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文提出了Sphinx，这是一个用于视觉感知与推理的合成环境，能大规模生成涵盖核心认知能力的任务，用于评估和训练视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在视觉-语言推理上距离人类水平尚有较大差距，缺乏精确、可控、覆盖广的基准测试环境以系统评估和提升模型能力。

Method: 构建了Sphinx环境，能自动生成包含拼图、图表、符号、图标和几何原件等要素的多类推理任务，并提供真实可核查的标准答案。通过25种任务类型对包括GPT-5在内的视觉-语言大模型进行系统测试，并探讨了基于可验证奖励的强化学习方法对任务准确率的提升作用。

Result: 即使是当前最先进的GPT-5模型，在Sphinx测试中的准确率仅为51.1%，明显低于人类水平。采用可验证奖励的强化学习能大幅提升模型在这些任务以及外部视觉推理基准上的表现。

Conclusion: Sphinx为视觉-语言推理模型提供了扎实的测评和训练平台，强化学习等方法对能力提升有效，有助于推动多模态智能的发展。

Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.

</details>


### [16] [Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion](https://arxiv.org/abs/2511.20821)
*Samuele Dell'Erba,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的优化式视觉反演（OVI）方法，替代文本-图像扩散模型传统所依赖的扩散先验网络，并证明了该方法在提升视觉质量及效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本到图像生成任务中表现卓越，但高度依赖事先训练的扩散先验网络进行文本-视觉语义转换。这些先验模型训练成本高、依赖大数据集，存在效率与实用性的瓶颈。作者有意探索是否可以不依赖冗杂、昂贵的训练先验，实现高质量的文本制导图像生成。

Method: 作者提出了Optimization-based Visual Inversion（OVI），无需训练，假设初始潜变量为随机伪token，通过与输入文本嵌入的余弦相似度最大化直接优化视觉表示。同时设计了基于马氏距离的损失和最近邻损失，限制优化过程，使生成的视觉表示更接近真实图像分布。

Result: 通过在Kandinsky 2.2模型上的实验证明，纯文本嵌入即可取得很高的自动评测分数，但视觉质量不足。OVI方法在提升视觉质量同时，分数接近甚至优于目前最优的高效数据先验，尤其最近邻方法表现突出。

Conclusion: 无需训练的数据与先验，利用优化方式同样可实现合理的文本-图像生成效果，具有较好的视觉保真度。现有benchmark存在盲点，OVI方法为缩减训练与数据成本提供新方向，值得进一步研究。

Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.

</details>


### [17] [RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs](https://arxiv.org/abs/2511.20823)
*Roman Naeem,David Hagerman,Jennifer Alvén,Fredrik Kahl*

Main category: cs.CV

TL;DR: RefTr提出了一种高效而精确的3D血管树中心线提取方法，通过递归优化轨迹和参数高效的Producer-Refiner Transformer架构，在多个公开医学数据集上实现了高召回率和优异性能，适用于临床诊断与导航。


<details>
  <summary>Details</summary>
Motivation: 在临床中，血管和气道等管状树状结构的中心线检测对于诊断和手术导航至关重要，尤其是召回率不足时可能导致严重漏检，因此急需更高效且召回率更高的三维中心线提取方法。

Method: 提出了基于Transformer解码器的Producer-Refiner架构，Producer提出初步轨迹，Refiner递归细化生成最终轨迹，采用轨迹合流表示确保树状拓扑，并设计了空间树图去重算法提升精度。该方法参数量低、速度快，并能多步复用Refiner模块。

Result: 在多个公开医学中心线数据集上，RefTr达到了优于以往方法的召回率和相当的精度，显著减少了解码器参数量（2.4倍压缩）并提升了推理速度。

Conclusion: RefTr为三维医学图像中的血管树分析提供了一种精确高效的新框架，兼具较少参数和优异性能，有望成为临床中心线提取的新技术基准。

Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.

</details>


### [18] [MODEST: Multi-Optics Depth-of-Field Stereo Dataset](https://arxiv.org/abs/2511.20853)
*Nisarg K. Trivedi,Vinayak A. Belludi,Li-Yun Wang,Pardis Taghavi,Dante Lok*

Main category: cs.CV

TL;DR: 本文发布了第一个包含18000张高分辨率DSLR实拍立体图像的数据集，并覆盖不同焦距和光圈配置，填补了以往数据集依赖合成图像的不足。数据集支持多种任务的研究，如深度估计、浅景深渲染、去模糊、三维重建和新颖视角合成，并公开了标定文件及评测代码以支持可复现的真实环境研究。


<details>
  <summary>Details</summary>
Motivation: 现有的深度估计和景深渲染研究受限于高保真、真实相机光学数据集的缺乏，导致模型泛化能力有限，尤其是在合成数据上训练后在真实场景中的表现受影响。为此，作者致力于构建一个真实的、光学复杂的大规模DSLR立体数据集，提升相关模型的泛化和评测能力。

Method: 作者使用两套完全一致的DSLR相机，针对9个场景，系统性地改变焦距（28-70mm）和光圈（f/2.8-f/22），采集了每场景2000张、共50种光学配置的高分辨率图片。每种光学配置均配有专用的标定图像，场景涵盖了复杂的光学效果，如反射、镜面、玻璃、多尺度细节及多样自然/人工光照，并对数据集进行了光学和几何标定。

Result: 构建了内容丰富、光学和场景多样性强的数据集，具备真实相机成像特性。实验证明现有单目/立体深度估计和景深算法在该数据集上的表现存在明显挑战，显示了现实与合成数据集之间的真实性差距。

Conclusion: 该数据集为评估和推动深度估计、景深、去模糊等相关方向的研究提供了真实可靠的数据基础，有助于缩小合成与真实场景的现实鸿沟，并推动计算机视觉算法在现实光学条件下的泛化能力发展。

Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.

</details>


### [19] [Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries](https://arxiv.org/abs/2511.20854)
*Sree Bhattacharyya,Yaman Kumar Singla,Sudhir Yarram,Somesh Kumar Singh,Harini S,James Z. Wang*

Main category: cs.CV

TL;DR: 本论文提出了首个大规模的无监督视觉内容可记忆性数据集，并在相关任务上推动了模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉内容可记忆性研究受限于昂贵且有限的人工注释数据，难以扩展并捕捉更细致的记忆信号。大部分数据集只提供整体的记忆分数，缺乏自然语言中开放式回忆的细节表达。

Method: 作者通过从Reddit等平台收集tip-of-the-tongue（ToT，舌尖现象）召回查询，构建了包含8.2万余条视频及其描述性回忆数据的无监督数据集。同时，采用大模型微调和对比训练方法，探索其在召回生成和ToT检索等任务中的表现。

Result: 基于该数据集微调的大型视觉-语言模型，在生成开放式记忆性描述任务上超越了如GPT-4o等当前最先进模型。此外，首次实现了多模态ToT检索建模。

Conclusion: 该数据集和相关模型为视觉内容可记忆性研究开辟了新方向，有助于推动该领域在数据与方法上的进一步发展。

Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.

</details>


### [20] [Estimating Fog Parameters from a Sequence of Stereo Images](https://arxiv.org/abs/2511.20865)
*Yining Ding,João F. C. Mota,Andrew M. Wallace,Sen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可实时动态估计雾参数的新方法，并发布了一个真实雾环境下的高质量立体数据集SDIRF，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有雾参数估计方法多为依次估算，易引起误差累积，且难以处理实际中常见的非均匀雾分布。作者希望提出一种更稳健、适应性强的雾参数估计方案。

Method: 作者针对雾仅局部均匀的现实假设，提出同时估算所有雾模型参数的优化算法，能够动态更新参数。并开发了SDIRF数据集，其中包含高质量立体雾图及配对清晰图像和相机参数校准信息。

Result: 在合成雾及真实雾数据（SDIRF）上的实验验证了该方法较现有算法具有更高的估计精度，同时更适应真实、复杂多变的雾环境。

Conclusion: 所提方法实用性强，可作为现有视觉SLAM或里程计系统的插件模块，在真实雾环境下应用效果良好。开放的数据集和代码有望推动视觉感知在雾环境下的研究进展。

Abstract: We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.

</details>


### [21] [V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence](https://arxiv.org/abs/2511.20886)
*Jiancheng Pan,Runze Wang,Tianwen Qian,Mohammad Mahdi,Yanwei Fu,Xiangyang Xue,Xiaomeng Huang,Luc Van Gool,Danda Pani Paudel,Yuqian Fu*

Main category: cs.CV

TL;DR: 提出了一种名为V^2-SAM的新方法，用于跨视角目标对应任务，通过创新的提示生成模块和多专家机制，有效提升了跨视角目标关联性能，并在多个数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 在不同视角（例如第一人称与第三人称）下，物体由于外观和视角变化巨大，导致现有分割模型（如SAM2）难以直接用于跨视角目标关联。因此，亟需一种适合跨视角场景的目标对应方法。

Method: 提出V^2-SAM框架，通过两个互补的提示生成器（V^2-Anchor基于DINOv3几何特征生成锚点提示，实现SAM2在跨视角场景下的坐标提示；V^2-Visual通过新颖的视觉提示匹配器对齐外观特征），并采用多专家结构和循环一致性选择机制自适应输出最可靠结果。

Result: V^2-SAM在Ego-Exo4D、DAVIS-2017和HANDAL-X等跨视角或相关任务数据集上取得了比现有方法更优的效果，刷新了SOTA。

Conclusion: V^2-SAM有效突破了现有单视角分割模型难以直接应用于跨视角目标对应的问题，为该领域提供了一套通用、高效的技术方案。

Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).

</details>


### [22] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时对齐方法（Null-TTA），通过优化无条件文本嵌入，使扩散模型生成结果更加贴合特定奖励函数，同时避免了奖励操控（reward hacking）等问题，并在不更新模型参数的情况下实现了更好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐方法要么对奖励函数优化不足，要么容易过度优化（即reward hack），导致生成结果偏离语义。需要一种能够在保持语义合理的同时进行高效对齐的新方法。

Method: Null-TTA通过优化分类器无导引中的无条件文本嵌入（unconditional embedding），而不是直接操控潜变量或噪声变量。利用文本嵌入空间的结构化语义特性，使得对齐过程发生在语义连贯的流形上，自然规避了reward hacking问题。该方法不需对模型参数进行更新，直接引导模型分布朝向目标奖励。

Result: Null-TTA在多项基准任务中实现了最优的测试时对齐效果，同时保持了出色的跨奖励泛化能力，超过了现有方法。

Conclusion: Null-TTA开创性地将语义空间优化用于测试时对齐，证明了其有效性和理论合理性，推动了该领域的发展。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [23] [GaINeR: Geometry-Aware Implicit Network Representation](https://arxiv.org/abs/2511.20924)
*Weronika Jakubowska,Mikołaj Zieliński,Rafał Tobiasz,Krzysztof Byrski,Maciej Zięba,Dominik Belter,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了一种结合高斯分布与神经隐式表示的新方法GaINeR，实现了2D图像的连续、高保真表达，并支持局部灵活编辑与物理感知交互。


<details>
  <summary>Details</summary>
Motivation: 传统的隐式神经表示(INRs)虽能高质量重建图像，但缺乏几何结构信息，限制了其在局部编辑、与物理模拟结合以及交互式场景下的应用。

Method: 提出GaINeR框架，将可训练高斯分布与神经网络型INR结合。对每个图像坐标，模型检索最近K个高斯，利用距离加权聚合嵌入，再通过神经网络预测RGB值，实现可解释的几何表达和灵活的局部编辑。

Result: GaINeR可以实现连续的图像表达，提供可解释的几何结构，并显著提升图像的局部编辑和物理互动能力。官方代码已开源。

Conclusion: GaINeR弥补了传统INR在几何结构和交互性方面的不足，为基于物理和交互的图像操控提供了有效方法。

Abstract: Implicit Neural Representations (INRs) have become an essential tool for modeling continuous 2D images, enabling high-fidelity reconstruction, super-resolution, and compression. Popular architectures such as SIREN, WIRE, and FINER demonstrate the potential of INR for capturing fine-grained image details. However, traditional INRs often lack explicit geometric structure and have limited capabilities for local editing or integration with physical simulation, restricting their applicability in dynamic or interactive settings. To address these limitations, we propose GaINeR: Geometry-Aware Implicit Network Representation, a novel framework for 2D images that combines trainable Gaussian distributions with a neural network-based INR. For a given image coordinate, the model retrieves the K nearest Gaussians, aggregates distance-weighted embeddings, and predicts the RGB value via a neural network. This design enables continuous image representation, interpretable geometric structure, and flexible local editing, providing a foundation for physically aware and interactive image manipulation. The official implementation of our method is publicly available at https://github.com/WJakubowska/GaINeR.

</details>


### [24] [A deep learning model to reduce agent dose for contrast-enhanced MRI of the cerebellopontine angle cistern](https://arxiv.org/abs/2511.20926)
*Yunjie Chen,Rianne A. Weber,Olaf M. Neve,Stephan R. Romeijn,Erik F. Hensen,Jelmer M. Wolterink,Qian Tao,Marius Staring,Berit M. Verbist*

Main category: cs.CV

TL;DR: 本研究评估了一种深度学习模型在降低小脑桥脑角顺磁剂增强T1加权MRI对比剂用量下的应用效果，验证其能够恢复接近标准剂量影像质量。


<details>
  <summary>Details</summary>
Motivation: 对比剂在脑部MRI成像中提高诊断效能，但存在副作用与成本。若能有效降低对比剂用量且维持诊断质量，将大幅提升患者安全和经济性。

Method: 多中心回顾性研究，收集前庭神经鞘瘤患者MRI资料，人工模拟不同对比剂剂量。训练深度学习模型将低剂量图像恢复为标准剂量，并由专家多维度评价恢复影像质量与分割性能。

Result: DL模型在极低剂量（10%剂量）下即可显著提升结构相似性（SSIM）、峰值信噪比（PSNR）等指标，自动分割指标（Dice、Hausdorff距离等）均优于原始低剂量图像。专家认为低剂量恢复影像达到优秀诊断质量。

Conclusion: 深度学习模型能有效提升低剂量增强MRI影像质量，实现用10%-30%对比剂即可满足诊断与分割需求，有望降低对比剂相关风险与成本。

Abstract: Objectives: To evaluate a deep learning (DL) model for reducing the agent dose of contrast-enhanced T1-weighted MRI (T1ce) of the cerebellopontine angle (CPA) cistern. Materials and methods: In this multi-center retrospective study, T1 and T1ce of vestibular schwannoma (VS) patients were used to simulate low-dose T1ce with varying reductions of contrast agent dose. DL models were trained to restore standard-dose T1ce from the low-dose simulation. The image quality and segmentation performance of the DL-restored T1ce were evaluated. A head and neck radiologist was asked to rate DL-restored images in multiple aspects, including image quality and diagnostic characterization. Results: 203 MRI studies from 72 VS patients (mean age, 58.51 \pm 14.73, 39 men) were evaluated. As the input dose increased, the structural similarity index measure of the restored T1ce increased from 0.639 \pm 0.113 to 0.993 \pm 0.009, and the peak signal-to-noise ratio increased from 21.6 \pm 3.73 dB to 41.4 \pm 4.84 dB. At 10% input dose, using DL-restored T1ce for segmentation improved the Dice from 0.673 to 0.734, the 95% Hausdorff distance from 2.38 mm to 2.07 mm, and the average surface distance from 1.00 mm to 0.59 mm. Both DL-restored T1ce from 10% and 30% input doses showed excellent images, with the latter being considered more informative. Conclusion: The DL model improved the image quality of low-dose MRI of the CPA cistern, which makes lesion detection and diagnostic characterization possible with 10% - 30% of the standard dose.

</details>


### [25] [Smooth regularization for efficient video recognition](https://arxiv.org/abs/2511.20928)
*Gil Goldman,Raja Giryes,Mahadev Satyanarayanan*

Main category: cs.CV

TL;DR: 提出了一种通过高斯随机游走正则化中间层嵌入的平滑正则化方法，大幅提升了轻量级视频识别模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级视频识别模型在建模视频内在的时序相干性和复杂动态方面能力有限，缺乏有效的时序归纳偏置，导致准确率受限。作者希望通过一种新的正则化技术，增强模型对视频自然时序平滑性的捕捉能力，从而提升模型表现。

Method: 为实现时序平滑，作者提出对连续帧中间层嵌入的变化进行高斯随机游走建模，从而惩罚表征的突变，使得模型趋向于生成低加速度（即平滑变化）的特征。这一方法以正则化项的形式施加在网络训练中，重点针对轻量级模型。

Result: 该方法在Kinetics-600数据集上让模型准确率提升了3.8%—6.4%。其中MoViNets系列在相同FLOP限制下提升了3.8%—6.1%，MobileNetV3和MoViNets-Stream分别提升4.9%—6.4%。均显著优于同等资源约束下的现有最优模型。

Conclusion: 通过引入平滑正则化，显著提升了轻量级视频识别网络的建模能力，特别在资源受限场景下效果突出，为时序归纳偏置的设计提供了新的思路。代码和模型已公开。

Abstract: We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/gilgoldm/grw-smoothing.

</details>


### [26] [Open Vocabulary Compositional Explanations for Neuron Alignment](https://arxiv.org/abs/2511.20931)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉领域框架，无需人工标注即可为任意概念和数据集中的神经元生成组合式解释，利用开放词汇语义分割生成的掩码提高了解释的灵活性和适用性。


<details>
  <summary>Details</summary>
Motivation: 目前神经元组合式解释依赖人工标签数据集，导致其局限于特定领域和预定义概念，限制了其通用性和扩展性。作者希望打破这一限制，实现更普适、更开放的神经元解释框架。

Method: 论文提出一个三步框架：1）支持用户自由指定任意概念；2）通过开放词汇语义分割模型生成目标概念的分割掩码；3）利用这些掩码推导神经元激活的组合式解释。

Result: 实验证明，该框架不仅在定量指标上优于现有方法，在可解释性评价上也获得了更好的人类反馈。此外，作者还分析了用模型标注代替人工标注对于解释的影响。

Conclusion: 该框架显著提升了组合式解释的灵活性和适用范围，使用户能够针对不同任务和属性灵活开展神经元解释，有望推动AI可解释性发展。

Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.

</details>


### [27] [UruDendro4: A Benchmark Dataset for Automatic Tree-Ring Detection in Cross-Section Images of Pinus taeda L](https://arxiv.org/abs/2511.20935)
*Henry Marichal,Joaquin Blanco,Diego Passarella,Gregory Randall*

Main category: cs.CV

TL;DR: 本文提出并公开了UruDendro4数据集，一套包含102份手工标注年轮的松树（Pinus taeda L.）横截面图像数据集，用于促进树木年轮自动检测相关研究，并使用先进算法设立了性能基线。


<details>
  <summary>Details</summary>
Motivation: 树木年轮的测量对于森林管理和相关研究非常重要，但传统手动测量方法耗时且精度有限。目前公开的数据集数量少，且数据形式有限，影响了自动检测与建模方法的发展。因此亟需新的、质量高、标注详细的数据集以推进研究。

Method: 研究团队采集了102份不同高度松树横截面图像，并逐张人工标注所有年轮，建立UruDendro4数据集。随后使用多种先进算法对年轮自动检测进行了基准测试，并通过消融实验确定了最优超参数，同时应用本数据集提升现有模型的泛化能力。

Result: DeepCS-TRD算法在该数据集上取得了最佳表现，平均精度为0.838，召回率为0.782，适应性Rand误差为0.084。消融实验对参数配置进行了实证验证。将该数据集用于模型训练提升了检测任务的泛化能力。

Conclusion: UruDendro4数据集丰富了年轮自动检测研究的基础资源，具备高精度与多样性，可用于开发体积估算等新方法。论文也提供了当前尖端方法的性能基线，为进一步研究提供有力支持。

Abstract: Tree-ring growth represents the annual wood increment for a tree, and quantifying it allows researchers to assess which silvicultural practices are best suited for each species. Manual measurement of this growth is time-consuming and often imprecise, as it is typically performed along 4 to 8 radial directions on a cross-sectional disc. In recent years, automated algorithms and datasets have emerged to enhance accuracy and automate the delineation of annual rings in cross-sectional images.
  To address the scarcity of wood cross-section data, we introduce the UruDendro4 dataset, a collection of 102 image samples of Pinus taeda L., each manually annotated with annual growth rings. Unlike existing public datasets, UruDendro4 includes samples extracted at multiple heights along the stem, allowing for the volumetric modeling of annual growth using manually delineated rings. This dataset (images and annotations) allows the development of volumetric models for annual wood estimation based on cross-sectional imagery.
  Additionally, we provide a performance baseline for automatic ring detection on this dataset using state-of-the-art methods. The highest performance was achieved by the DeepCS-TRD method, with a mean Average Precision of 0.838, a mean Average Recall of 0.782, and an Adapted Rand Error score of 0.084. A series of ablation experiments were conducted to empirically validate the final parameter configuration. Furthermore, we empirically demonstrate that training a learning model including this dataset improves the model's generalization in the tree-ring detection task.

</details>


### [28] [BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model](https://arxiv.org/abs/2511.20956)
*Rawa Mohammed,Mina Attin,Bryar Shareef*

Main category: cs.CV

TL;DR: 本论文提出了一种无需配对的乳腺超声影像自动报告生成方法BUSTR，有效兼顾文本生成质量和临床指标，尤其在BI-RADS类别及病理关键指标提取方面表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺超声自动报告生成任务受限于缺乏成对的影像-报告数据集，以及大语言模型存在幻觉的风险，亟需发展无需配对且更具可解释性的生成方法。

Method: BUSTR为多任务视觉-语言框架，通过结构化描述符（如BI-RADS、病理、组织学）与影像组学特征构建报告，采用多头Swin编码器基于多任务损失联合学习描述符相关视觉表征，同时设计跨层次目标函数，结合了token级交叉熵和视觉-文本表征的余弦相似度对齐损失。无需配对影像-报告监督。

Result: 在BrEaST与BUS-BRA两大公开乳腺超声数据集上，BUSTR在通用自然语言生成指标与临床指标（尤其是BI-RADS类别与病理指标）上均显著优于基线方法。

Conclusion: 提出的BUSTR方法通过描述符驱动、联合损失训练，实现了无需配对数据的乳腺超声自动报告生成，提升了算法报告的自动质量与临床相关性，具有实用和推广价值。

Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR

</details>


### [29] [Beyond Realism: Learning the Art of Expressive Composition with StickerNet](https://arxiv.org/abs/2511.20957)
*Haoming Lu,David Kocharian,Humphrey Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像合成任务——表达性合成，强调风格多样性和用户意图而非仅追求真实感，并提出StickerNet系统，利用真实在线平台大数据模拟和学习用户实际的编辑行为。


<details>
  <summary>Details</summary>
Motivation: 传统图像合成侧重于真实感和语义合理性，但现实中用户编辑图片常常更注重创意、艺术性或互动性。因此，需重新定义任务，服务用户现实需求。

Method: 提出StickerNet，两阶段：首先判别合成类型，然后预测透明度、遮罩、位置、缩放等参数。数据集直接来源于180万条真实在线编辑记录，真实反映用户行为。

Result: StickerNet在用户研究和定量评测中均优于常见基线方法，合成结果与真实用户决策高度一致，能较好贴合实际编辑习惯。

Conclusion: 本研究提出并验证了一种以表达性和用户意图为核心的新型图像合成理念，展示了用真实编辑行为训练模型的有效性，为视觉理解开辟新方向。

Abstract: As a widely used operation in image editing workflows, image composition has traditionally been studied with a focus on achieving visual realism and semantic plausibility. However, in practical editing scenarios of the modern content creation landscape, many compositions are not intended to preserve realism. Instead, users of online platforms motivated by gaining community recognition often aim to create content that is more artistic, playful, or socially engaging. Taking inspiration from this observation, we define the expressive composition task, a new formulation of image composition that embraces stylistic diversity and looser placement logic, reflecting how users edit images on real-world creative platforms. To address this underexplored problem, we present StickerNet, a two-stage framework that first determines the composition type, then predicts placement parameters such as opacity, mask, location, and scale accordingly. Unlike prior work that constructs datasets by simulating object placements on real images, we directly build our dataset from 1.8 million editing actions collected on an anonymous online visual creation and editing platform, each reflecting user-community validated placement decisions. This grounding in authentic editing behavior ensures strong alignment between task definition and training supervision. User studies and quantitative evaluations show that StickerNet outperforms common baselines and closely matches human placement behavior, demonstrating the effectiveness of learning from real-world editing patterns despite the inherent ambiguity of the task. This work introduces a new direction in visual understanding that emphasizes expressiveness and user intent over realism.

</details>


### [30] [TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs](https://arxiv.org/abs/2511.20965)
*Md Adnan Arefeen,Biplob Debnath,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 提出了TrafficLens算法，用于高效处理多摄像头交通路口视频，显著加速视频到文本的转换并保准信息准确性。


<details>
  <summary>Details</summary>
Motivation: 面对多个交通摄像头产生的庞大视频数据，现有采用视觉-语言模型转文本并结合大模型分析的方式效率低下，处理速度不能满足实时分析需求。

Method: TrafficLens算法基于路口摄像头的重叠覆盖区域，采用序贯处理方式，并结合前一次输出作为当前输入提示。同时，通过对象级相似性检测，智能跳过冗余处理，从而高效生成文本描述。

Result: 在真实交通视频数据测试中，TrafficLens将视频到文本的转换速度提升最高4倍，同时保持了信息的准确性。

Conclusion: TrafficLens有效提升了多摄像头交通路口视频分析的速度和效率，对智能交通系统具有实际应用价值。

Abstract: Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.

</details>


### [31] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 本文提出了一种结合Vision Transformers（ViT）与同态加密（HE）的隐私保护型联邦学习框架，能在保护数据隐私的前提下，有效进行多机构间的病理图像分类协作。


<details>
  <summary>Details</summary>
Motivation: 在多医疗机构协作下，提升诊断准确率需要共享多样化数据集，但隐私法规（如HIPAA）限制了患者数据的直接共享。尽管联邦学习（FL）可避免原始数据交换，但模型梯度依然存在泄露敏感信息风险。需要新的方法实现更强数据隐私保护。

Method: 提出利用ViT的CLS token作为紧凑特征表示，并对这些token采用CKKS同态加密方案进行加密后聚合，取代传统的梯度共享，从而规避梯度重建攻击。同时在实际三客户端肺癌病理分类任务上进行评测。

Result: 实验表明，传统梯度易被反演重建高质量图像（PSNR: 52.26dB, SSIM: 0.999, NMI: 0.741），而该CLS+HE方案能有效防止此类攻击，并可直接在密文上推理，通讯量较梯度加密方法降低30倍，每轮仅需326KB数据。分类准确率未加密为96.12%，加密后依然有90.02%。

Conclusion: 该框架在显著降低通信量的同时，实现了高水平的隐私保护和可用性，为多机构医疗协作提供了可行且安全的技术路径。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [32] [Inversion-Free Style Transfer with Dual Rectified Flows](https://arxiv.org/abs/2511.20986)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong,Xucheng Yin*

Main category: cs.CV

TL;DR: 本文提出了一种基于双重修正流的无反演风格迁移框架，实现了高效、高保真地将视觉内容与艺术风格融合，用于图像编辑和创意设计。该方法避免了传统扩散模型中的复杂反演计算，提高了效率并减少了视觉失真。


<details>
  <summary>Details</summary>
Motivation: 目前主流扩散式风格迁移方法通常需要计算复杂的反演过程，效率低，并在反演不准确时易引入视觉失真。为了提升风格迁移的效率和视觉质量，本文提出一种无需反演的新架构。

Method: 提出了一种无反演的双重修正流架构，仅通过前向计算（forward pass）实现风格迁移。方法并行预测内容和风格路径，通过动态中点插值将两者的速度场融合，使生成的风格化图像兼具内容和风格特征。设计了联合建模和速度场以及注意力注入机制，增强了风格整合效果。

Result: 实验结果显示，该方法在不同内容和风格图像上表现出广泛的泛化能力，并且在视觉保真度、内容保存和计算效率上均优于传统方法。

Conclusion: 本文方法为风格迁移提供了高效且高质量的解决方案，摆脱了传统反演过程的缺陷，具有很好的实际应用价值。

Abstract: Style transfer, a pivotal task in image processing, synthesizes visually compelling images by seamlessly blending realistic content with artistic styles, enabling applications in photo editing and creative design. While mainstream training-free diffusion-based methods have greatly advanced style transfer in recent years, their reliance on computationally inversion processes compromises efficiency and introduces visual distortions when inversion is inaccurate. To address these limitations, we propose a novel \textit{inversion-free} style transfer framework based on dual rectified flows, which tackles the challenge of finding an unknown stylized distribution from two distinct inputs (content and style images), \textit{only with forward pass}. Our approach predicts content and style trajectories in parallel, then fuses them through a dynamic midpoint interpolation that integrates velocities from both paths while adapting to the evolving stylized image. By jointly modeling the content, style, and stylized distributions, our velocity field design achieves robust fusion and avoids the shortcomings of naive overlays. Attention injection further guides style integration, enhancing visual fidelity, content preservation, and computational efficiency. Extensive experiments demonstrate generalization across diverse styles and content, providing an effective and efficient pipeline for style transfer.

</details>


### [33] [RefOnce: Distilling References into a Prototype Memory for Referring Camouflaged Object Detection](https://arxiv.org/abs/2511.20989)
*Yu-Huan Wu,Zi-Xuan Zhu,Yan Wang,Liangli Zhen,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出了一种新的掩蔽物体检测方法，在推理阶段无需引用参考图像，依然能高效检测指定的伪装物体。


<details>
  <summary>Details</summary>
Motivation: 现有的Ref-COD方法需在推理阶段输入参考图像，限制了部署效率并增加了数据收集与推理延迟。该工作旨在解决这一实际应用瓶颈。

Method: 在训练阶段将类别原型以记忆方式蒸馏存储，通过EMA（指数移动平均）更新原型；推理时通过查询条件综合多个类别原型向量指导分割，无需参考图像。为弥合引用统计与伪装特征间的表示差异，引入了双向注意力对齐模块以自适应地调整查询特征及类别原型表示。

Result: 在R2C7K大规模基准上进行了实验证明，所提方法精度可与最新Ref-COD方法媲美，甚至在部分指标上超越。

Conclusion: 提出的方法为Ref-COD提供了一条高效、无需测试时参考的新路径，兼顾简单性和准确性，对实际部署有较高价值。

Abstract: Referring Camouflaged Object Detection (Ref-COD) segments specified camouflaged objects in a scene by leveraging a small set of referring images. Though effective, current systems adopt a dual-branch design that requires reference images at test time, which limits deployability and adds latency and data-collection burden. We introduce a Ref-COD framework that distills references into a class-prototype memory during training and synthesizes a reference vector at inference via a query-conditioned mixture of prototypes. Concretely, we maintain an EMA-updated prototype per category and predict mixture weights from the query to produce a guidance vector without any test-time references. To bridge the representation gap between reference statistics and camouflaged query features, we propose a bidirectional attention alignment module that adapts both the query features and the class representation. Thus, our approach yields a simple, efficient path to Ref-COD without mandatory references. We evaluate the proposed method on the large-scale R2C7K benchmark. Extensive experiments demonstrate competitive or superior performance of the proposed method compared with recent state-of-the-arts. Code is available at https://github.com/yuhuan-wu/RefOnce.

</details>


### [34] [Wavefront-Constrained Passive Obscured Object Detection](https://arxiv.org/abs/2511.20991)
*Zhiwen Zheng,Yiwei Ouyang,Zhao Huang,Tao Zhang,Xiaoshuai Zhang,Huiyu Zhou,Wenwen Tang,Shaowei Jiang,Jin Liu,Xingru Huang*

Main category: cs.CV

TL;DR: 提出了一种新的物理驱动型神经网络（WavePCNet），有效提升了在复杂环境下对视场外、被遮挡物体的定位和分割能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法对相干光传播的物理过程建模不足，尤其在低信噪比、多次散射等复杂情况下，容易产生不符合物理规律的结果，导致定位和分割精度不高。

Method: 提出WavePCNet网络，利用Tri-Phase Wavefront Complex-Propagation Reprojection（TriWCP）融入复幅传输算子，精确限制相干光传播，并引入动量记忆机制减少扰动累积，结合高频跨层补偿提高多尺度特征建模能力和模型稳健性。

Result: 在四个真实采集数据集上，WavePCNet在分割准确性和鲁棒性上均显著优于现有先进方法。

Conclusion: WavePCNet通过物理驱动建模和多项结构创新，提升了遮挡物体的检测分割能力，为复杂成像场景下提升光学感知系统的稳定性和可靠性提供了有效方法。

Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.

</details>


### [35] [GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision](https://arxiv.org/abs/2511.20994)
*Yuxiao Xiang,Junchi Chen,Zhenchao Jin,Changtao Miao,Haojie Yuan,Qi Chu,Tao Gong,Nenghai Yu*

Main category: cs.CV

TL;DR: 提出了一种名为GuardTrace-VL的模型，用于在多模态大模型的推理过程中检测不安全内容，显著提升了安全性的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLRM）虽然在视觉语言任务上表现优异，但其推理步骤中可能出现不安全内容，现有安全机制多忽略这一中间推理过程，从而存在安全隐患。

Method: 提出GuardTrace-VL安全审计器，通过联合图像和文本分析监控整个问题-推理-回答（QTA）流程，能在推理过程及时发现不安全内容。为训练和评估该模型，作者构建了GuardTrace数据集，并结合多轮人工及MLRM投票验证，采用三阶段递进训练方法提升模型对安全细微差异的识别能力。

Result: 在覆盖多领域的测试集上，GuardTrace-VL在不安全推理检测任务中取得了93.1%的F1分数，比现有最佳多模态安全方法提升了13.5%。

Conclusion: GuardTrace-VL能有效检测多模态推理过程中的不安全内容，大幅提升了现有安全防护能力，为多模态模型部署提供了更强有力的安全保障，相关代码将开源。

Abstract: Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.

</details>


### [36] [From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition](https://arxiv.org/abs/2511.20996)
*Jingxi Chen,Yixiao Zhang,Xiaoye Qian,Zongxia Li,Cornelia Fermuller,Caren Chen,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像分层分解方法，可对单幅图像进行前景与背景的分离，同时在图像擦除与遮挡恢复任务中表现优异。通过多模态上下文融合模块和轻量级微调，模型可实现细致分层及更灵活的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 当前图像只用单一层表示，导致编辑复杂且灵活性低。图像的前景和背景分解有利于独立编辑，但现有方法和数据有限，难以有效处理单幅图像的分层分解。

Method: 观察到图像分层与修复任务的相似性，作者提出用扩散式修补（inpainting）模型，并对其进行轻量级微调以实现分层分解。此外，提出了带有线性注意力复杂度的多模态上下文融合模块，以提升潜在空间细节保留。训练数据完全合成自开源资源。

Result: 模型在物体移除和遮挡恢复任务上取得优异表现，细节保留效果显著优于同类方法。

Conclusion: 利用扩散模型及创新的模块设计，可以高效地将单幅图像分解成分层成分，为后续编辑和创意应用带来更大灵活性和新可能性。

Abstract: Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.

</details>


### [37] [Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning](https://arxiv.org/abs/2511.21002)
*Xiaoxing You,Qiang Huang,Lingyu Li,Chi Zhang,Xiaopeng Liu,Min Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一个面向新闻图片自动生成说明的新方法MERGE，通过多模态、实体感知的检索增强生成模型提升图片描述的质量、事实覆盖和实体识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前新闻图片说明生成面临三大难题：信息覆盖不全、跨模态对齐弱、图片与实体关联不准确，现有方法难以满足新闻场景下对内容准确性和丰富性的高要求。

Method: 提出MERGE框架，构建实体中心的多模态知识库（EMKB），整合文本、图像与结构化知识实现背景信息检索；采用多阶段假说-说明机制提升跨模态对齐；通过图像引导的动态检索增强图片与实体的匹配。

Result: 在GoodNews和NYTimes800k两个数据集上，MERGE显著提升了描述质量（CIDEr指标分别提升6.84和1.16）和实体识别（F1分数分别提升4.14和2.64），且在未见过的Visual News数据集上也表现出很强的泛化能力（CIDEr提升20.17，F1提升6.22）。

Conclusion: MERGE有效解决了新闻图片说明生成中的关键难题，显著优于现有方法，并具备较强的鲁棒性与跨领域泛化能力，适用于实际新闻场景。

Abstract: News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.

</details>


### [38] [MetaRank: Task-Aware Metric Selection for Model Transferability Estimation](https://arxiv.org/abs/2511.21007)
*Yuhang Liu,Wenjie Zhao,Yunhui Guo*

Main category: cs.CV

TL;DR: 提出了一种通过语义信息自动选择最优MTE指标的方法，来帮助高效地在迁移学习中选择源模型。


<details>
  <summary>Details</summary>
Motivation: 在迁移学习中，选择合适的预训练源模型非常重要但计算开销大。虽然已有MTE方法通过指标来高效筛选模型，但选择哪一个指标却常常凭经验或简单基于历史表现，且不同任务下最优指标并不一致。

Method: 提出MetaRank元学习框架，将MTE指标选择视为学习排序问题，不再依赖传统meta特征，而是用预训练语言模型对数据集和MTE指标的文本描述进行编码，然后训练元预测器，离线学习数据集特征与指标机制的对应关系，并通过列表排序目标优化。上线时根据新任务的文本描述高效推荐最合适的MTE指标。

Result: 在11个预训练模型和11个目标数据集上进行了大量实验，验证了方法的有效性。

Conclusion: MetaRank能够自动且高效地依据新任务的文本描述选择最优的MTE指标，提升了迁移学习中的模型筛选效率。

Abstract: Selecting an appropriate pre-trained source model is a critical, yet computationally expensive, task in transfer learning. Model Transferability Estimation (MTE) methods address this by providing efficient proxy metrics to rank models without full fine-tuning. In practice, the choice of which MTE metric to use is often ad hoc or guided simply by a metric's average historical performance. However, we observe that the effectiveness of MTE metrics is highly task-dependent and no single metric is universally optimal across all target datasets. To address this gap, we introduce MetaRank, a meta-learning framework for automatic, task-aware MTE metric selection. We formulate metric selection as a learning-to-rank problem. Rather than relying on conventional meta-features, MetaRank encodes textual descriptions of both datasets and MTE metrics using a pretrained language model, embedding them into a shared semantic space. A meta-predictor is then trained offline on diverse meta-tasks to learn the intricate relationship between dataset characteristics and metric mechanisms, optimized with a listwise objective that prioritizes correctly ranking the top-performing metrics. During the subsequent online phase, MetaRank efficiently ranks the candidate MTE metrics for a new, unseen target dataset based on its textual description, enabling practitioners to select the most appropriate metric a priori. Extensive experiments across 11 pretrained models and 11 target datasets demonstrate the strong effectiveness of our approach.

</details>


### [39] [Structure-Aware Prototype Guided Trusted Multi-View Classification](https://arxiv.org/abs/2511.21021)
*Haojian Huang,Jiahao Shi,Zhe Liu,Harold Haodong Chen,Han Fang,Hao Sun,Zhongjiang He*

Main category: cs.CV

TL;DR: 本文提出了一种基于原型的可信多视图分类（TMVC）方法，通过引入原型表征各视图的邻居结构，提升多视图一致性和决策的可信性，并在多个公开数据集上取得了优越的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有TMVC方法主要依赖全局密集邻居关系来建模视图内的依赖，导致高计算成本且难以保证视图间的一致性。此外，传统方法多使用人工设定的权重聚合视图证据，无法确保学到的邻居结构在类别空间内一致，降低了分类决策的可信度。

Method: 提出利用原型来简化视图内邻居关系的学习，并支持动态对齐视图内和视图间结构，从而高效且一致地发现多视图共识。

Result: 在多个公开多视图数据集上的广泛实验证明，该方法在下游任务性能和分类鲁棒性上均优于主流TMVC方法。

Conclusion: 基于原型的TMVC框架有效提升了决策的可信性，增强了多视图分类的一致性和效率，是解决异构、多源信息决策问题的有力方案。

Abstract: Trustworthy multi-view classification (TMVC) addresses the challenge of achieving reliable decision-making in complex scenarios where multi-source information is heterogeneous, inconsistent, or even conflicting. Existing TMVC approaches predominantly rely on globally dense neighbor relationships to model intra-view dependencies, leading to high computational costs and an inability to directly ensure consistency across inter-view relationships. Furthermore, these methods typically aggregate evidence from different views through manually assigned weights, lacking guarantees that the learned multi-view neighbor structures are consistent within the class space, thus undermining the trustworthiness of classification outcomes. To overcome these limitations, we propose a novel TMVC framework that introduces prototypes to represent the neighbor structures of each view. By simplifying the learning of intra-view neighbor relations and enabling dynamic alignment of intra- and inter-view structure, our approach facilitates more efficient and consistent discovery of cross-view consensus. Extensive experiments on multiple public multi-view datasets demonstrate that our method achieves competitive downstream performance and robustness compared to prevalent TMVC methods.

</details>


### [40] [CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching](https://arxiv.org/abs/2511.21024)
*Qirui Yang,Yang Yang,Ying Zeng,Xiaobin Hu,Bo Li,Huanjing Yue,Jingyu Yang,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出了CameraMaster，一种统一的、具有相机感知能力的扩散模型框架，用于实现精确、物理一致的图像调色与编辑，并显著提升了参数控制的细致性及多参数组合能力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于文本指导的扩散模型极大推动了图像编辑和生成的发展，但要实现物理一致的精确参数化图像调色（比如曝光、白平衡、变焦等），仍然面临两个主要挑战：1）完全依赖模糊、耦合的文本提示导致参数控制不精确；2）为每个参数单独训练分支或权重不利于可扩展性和参数组合，也难以捕捉细微变化。

Method: CameraMaster通过显式解耦“摄影师指令（text prompt）”与“相机参数（如曝光等）”，并共同建模两者的信息。首先将相机参数嵌入用于调制指令和内容语义，再将调制后指令通过交叉关注注入内容特征，获得对相机高度敏感的语义上下文。同时，在扩散过程的每一层，将指令和参数作为时序嵌入的条件和门控信号，实现统一、分层的调制，确保语义与参数高度一致。

Result: CameraMaster在78K图像-指令-相机参数三元组的大规模数据集上训练与评估，显示对于参数变化其响应接近线性和单调，能够无缝支持多参数组合，且在各类基准测试中均明显优于现有方法。

Conclusion: CameraMaster有效提升了相机参数可控性、多参数协同和编辑的灵敏度，为基于扩散模型的高级图像调色与编辑设立了新基准。

Abstract: Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.

</details>


### [41] [CaptionQA: Is Your Caption as Useful as the Image Itself?](https://arxiv.org/abs/2511.21025)
*Shijia Yang,Yunong Liu,Bohan Zhai,Ximeng Sun,Zicheng Liu,Emad Barsoum,Manling Li,Chenfeng Xu*

Main category: cs.CV

TL;DR: 本文提出了一个以实际下游任务效用为核心的新型图片描述评价基准CaptionQA，显示现有模型生成的图片描述在支持下游多模态任务中仍有很大不足，并发布了相关数据集和代码工具。


<details>
  <summary>Details</summary>
Motivation: 虽然图片描述常作为多模态系统中图片的“替身”来用于检索、推荐或多步推理，但现有评测却鲜少考察这些描述在真实下游任务中的效用。因此，亟需一种能够直接反映描述是否足以替代原始图片的新型评价方法。

Method: 作者构建了CaptionQA基准，涵盖自然、文档、电商和智能体四大领域（细分为25大类、69子类），为每张图片密集设计了大量多选题（共33,027题，平均每图50.3题），所有问题都需要图片的视觉信息才能回答。然后，采用大语言模型（LLM）仅利用图片描述作答，来测评描述对下游LLM任务的支持能力。作者还将结果与传统多模态视觉-文本模型的表现进行了对比。

Result: 大量实验表明，当前多模态大模型生成的图片描述，在真实任务中对下游模型提供的信息量远低于图片原信息。即便是在传统评测中表现相近的模型，在描述支持下游推理任务时效用相差可达32%。

Conclusion: 该工作证明了现有图片描述生成系统与下游多模态推理任务之间存在显著效用缺口，并为后续研究图片描述与多模态推理的关系提供了新的评测工具和方向。研究成果及工具已开源，便于持续扩展。

Abstract: Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.

</details>


### [42] [FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation](https://arxiv.org/abs/2511.21029)
*Kaixing Yang,Xulong Tang,Ziqiao Peng,Xiangyue Zhang,Puwei Wang,Jun He,Hongyan Liu*

Main category: cs.CV

TL;DR: 本文提出了高效的音乐驱动舞蹈生成方法FlowerDance，实现了快速且高质量的动作生成，显著提升了生成速度与内存效率，并支持动作编辑。


<details>
  <summary>Details</summary>
Motivation: 现有音乐驱动舞蹈生成方法效率较低，难以兼顾高保真三维渲染，限制了在虚拟现实和数字娱乐等实际应用中的表现力。

Method: 提出FlowerDance方法，结合MeanFlow与物理一致性约束，实现以较少采样步骤生成高质量舞蹈动作；采用基于BiMamba的高效骨干网络与通道级跨模态融合，以非自回归方式提升速度，并支持用户交互式编辑舞蹈序列。

Result: 在AIST++与FineDance数据集上的大量实验表明，FlowerDance在动作质量和生成效率上均达到最新最优水平。

Conclusion: FlowerDance既提高了音乐驱动舞蹈生成的效率，也兼顾了舞蹈动作的物理合理性和艺术表现力，为实际应用如虚拟现实等领域提供了高性能解决方案。

Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.

</details>


### [43] [LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules](https://arxiv.org/abs/2511.21042)
*Cheng Yang,Hui Jin,Xinlei Yu,Zhipeng Wang,Yaoqun Liu,Fenglei Fan,Dajiang Lei,Gangyong Jia,Changmiao Wang,Ruiquan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种协作多智能体系统LungNoduleAgent，用于肺部CT扫描中结节的精确识别和恶性程度判定，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在肺结节形态学描述和医学知识结合方面存在不足，限制了其在实际临床中的可靠性与有效性；协作多智能体方法在医学病理领域尚未被充分探索。

Method: 设计了LungNoduleAgent系统，将诊断流程拆分为三个模块：结节定位（Nodule Spotter）、影像描述（Radiologist）、恶性程度推理（Doctor Agent System）；通过多智能体协作并结合病理知识库提升诊断精度。

Result: 在两个私有数据集和公开LIDC-IDRI数据集上，LungNoduleAgent在结节识别、描述和恶性判断等任务上均超越了主流视觉-语言模型、传统多智能体系统和医学专家模型。

Conclusion: 区域级语义对齐和多智能体协作机制对于提升肺结节诊断表现至关重要，LungNoduleAgent为临床分析提供了有价值的基础工具。

Abstract: Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.

</details>


### [44] [PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring](https://arxiv.org/abs/2511.21043)
*Hakki Motorcu,Mujdat Cetin*

Main category: cs.CV

TL;DR: 本文提出了一种结合生成模型与物理约束的新型图像去模糊方法，能够在严重模糊情况下实现更好的物理准确性与感知质量平衡。


<details>
  <summary>Details</summary>
Motivation: 现有空间变化图像去模糊方法要么高度依赖物理建模，会导致生成结果过于平滑有伪影，要么采用生成模型但往往忽视物理约束容易“幻觉化”细节，缺乏现实基础，因此需要一种能够兼顾物理约束与生成能力的新方法。

Method: 作者提出通过将生成模型（如扩散模型和ControlNet）与密集、显式的物理约束相结合的新框架。具体方法为不对退化场做过度简化，而是用高维压缩核的密集场描述复杂、微小的模糊变化，并以此作为条件输入引导扩散采样过程。

Result: 在多项实验中，该方法在复杂和严重模糊场景下，无论物理准确性还是感知质量均超过现有主流物理模型方法和生成模型基线。

Conclusion: 文中提出的方法在物理精确性与感知真实感之间取得优秀平衡，有效解决了复杂退化场景下的空间变化去模糊难题，优于当前主流方法。

Abstract: Spatially varying image deblurring remains a fundamentally ill-posed problem, especially when degradations arise from complex mixtures of motion and other forms of blur under significant noise. State-of-the-art learning-based approaches generally fall into two paradigms: model-based deep unrolling methods that enforce physical constraints by modeling the degradations, but often produce over-smoothed, artifact-laden textures, and generative models that achieve superior perceptual quality yet hallucinate details due to weak physical constraints. In this paper, we propose a novel framework that uniquely reconciles these paradigms by taming a powerful generative prior with explicit, dense physical constraints. Rather than oversimplifying the degradation field, we model it as a dense continuum of high-dimensional compressed kernels, ensuring that minute variations in motion and other degradation patterns are captured. We leverage this rich descriptor field to condition a ControlNet architecture, strongly guiding the diffusion sampling process. Extensive experiments demonstrate that our method effectively bridges the gap between physical accuracy and perceptual realism, outperforming state-of-the-art model-based methods as well as generative baselines in challenging, severely blurred scenarios.

</details>


### [45] [MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization](https://arxiv.org/abs/2511.21051)
*Yingjie Xia,Xi Wang,Jinglei Shi,Vicky Kalogeiton,Jian Yang*

Main category: cs.CV

TL;DR: MUSE是首个统一情感生成与编辑的图像情感合成框架，在无需额外训练和数据下，通过三大技术创新，提升了情感表达的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有图像情感合成方法将生成与编辑人为分开，效率低下且不适合生成与编辑自然交错的应用场景（如治疗或故事讲述），需要一种统一、高效的技术方案。

Method: 提出MUSE统一框架，借鉴Test-Time Scaling (TTS)思想，在现有扩散模型和情感分类器基础上：（1）利用基于梯度优化的情感token稳定引导情感合成；（2）用语义相似性信号确定最佳引入情感的时机；（3）采用多情感损失函数减少情感干扰。无需更新扩散模型或制作专门数据集。

Result: 实验结果显示，MUSE在情感生成和编辑方面均优于所有对比方法，提升了情感准确性、语义多样性，并保持了内容、文本提示和情感表达间的平衡。

Conclusion: MUSE框架为情感合成开辟了新范式，实现了情感生成与编辑的一体化，并大大拓展了其应用场景和效果表现。

Abstract: Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.

</details>


### [46] [Long-Term Alzheimers Disease Prediction: A Novel Image Generation Method Using Temporal Parameter Estimation with Normal Inverse Gamma Distribution on Uneven Time Series](https://arxiv.org/abs/2511.21057)
*Xin Hong,Xinze Sun,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的时序正态逆伽玛（T-NIG）分布模型，用于阿尔茨海默病影像预测，在不规则时间序列下，可更好保持和预测疾病相关特征。


<details>
  <summary>Details</summary>
Motivation: 以往的影像生成方法在处理预测阿尔茨海默病进展时，受限于时间不规则分布，导致疾病特征难以保留。本研究认为影像间分布的时间属性能够反映疾病进展特性，因此提出更好建模时间不均匀分布的方法。

Method: 提出T-NIG模型，在正态逆伽玛分布基础上引入时间参数，通过两时点大脑影像生成中间及未来影像，实现病情预测。同时，利用特征坐标邻域建模变化，并引入不确定性估计降低因时间信息不足导致的两类不确定性。

Result: T-NIG模型在数据集中的短期与长期预测任务上，实现了最先进的性能，能够更准确地预测和合成呈现疾病相关进展特征的影像。

Conclusion: T-NIG模型对于不规则时序影像数据能够保持并预测阿尔茨海默病相关特征，是有效的影像生成与病情预测工具。

Abstract: Image generation can provide physicians with an imaging diagnosis basis in the prediction of Alzheimer's Disease (AD). Recent research has shown that long-term AD predictions by image generation often face difficulties maintaining disease-related characteristics when dealing with irregular time intervals in sequential data. Considering that the time-related aspects of the distribution can reflect changes in disease-related characteristics when images are distributed unevenly, this research proposes a model to estimate the temporal parameter within the Normal Inverse Gamma Distribution (T-NIG) to assist in generating images over the long term. The T-NIG model employs brain images from two different time points to create intermediate brain images, forecast future images, and predict the disease. T-NIG is designed by identifying features using coordinate neighborhoods. It incorporates a time parameter into the normal inverse gamma distribution to understand how features change in brain imaging sequences that have varying time intervals. Additionally, T-NIG utilizes uncertainty estimation to reduce both epistemic and aleatoric uncertainties in the model, which arise from insufficient temporal data. In particular, the T-NIG model demonstrates state-of-the-art performance in both short-term and long-term prediction tasks within the dataset. Experimental results indicate that T-NIG is proficient in forecasting disease progression while maintaining disease-related characteristics, even when faced with an irregular temporal data distribution.

</details>


### [47] [MIRA: Multimodal Iterative Reasoning Agent for Image Editing](https://arxiv.org/abs/2511.21087)
*Ziyun Zeng,Hang Hua,Jiebo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态迭代推理代理（MIRA），能够更精准地遵循用户指令进行图像编辑，显著提升编辑的语义一致性和视觉质量，对比现有模型展现出更优的效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解释涉及组合关系、上下文线索或指代表达等复杂指令时存在较大困难，常导致语义偏离或未能正确反映用户意图。如何提高模型对复杂指令的理解和遵循能力，是图像编辑领域亟需解决的问题。

Method: 提出MIRA轻量级多模态推理代理，以迭代式感知-推理-行动环进行编辑。与一次性指令不同，MIRA逐步生成原子级编辑指令，并根据图像反馈调整决策。作者还构建了15万条多模态工具使用数据集（MIRA-Editing），并采用两阶段的监督微调（SFT）和GRPO训练流程。

Result: MIRA可以与多种开源图像编辑模型组合使用（如Flux.1-Kontext、Step1X-Edit、Qwen-Image-Edit），在复杂编辑指令下显著提升语义一致性和感知质量，超过或达到GPT-Image、Nano-Banana等闭源系统的水平。

Conclusion: MIRA为复杂场景下的指令引导图像编辑提供了高效可靠的解决方案，显著提升了模型对复杂、多层指令的理解与执行能力，在开源和商用系统中均具备较强的竞争力。

Abstract: Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.

</details>


### [48] [CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition](https://arxiv.org/abs/2511.21097)
*Geetanjali Sharma,Gaurav Jaswal,Aditya Nigam,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 本文提出了一种新颖且通用的虹膜认证匹配流程，结合3D-CNN和课程学习方式，有效增强对时空特征的建模能力，对旋转、缩放、反射和模糊有较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有虹膜识别方法面对图像旋转、缩放、反射和模糊时鲁棒性不足，大多仅依赖简单的点对点特征比较，未充分利用虹膜图案中的时空结构信息。为提升实际应用中的鲁棒性和判别能力，亟需建立更能表征虹膜时空结构的特征提取与匹配框架。

Method: 将每幅虹膜图像沿一维拆分成一系列子图像，输入3D卷积神经网络（3D-CNN），以捕捉空间及时空特征，并采用课程式训练方式强化特征时空动态的建模。整个流程基于triplet和ArcFace损失进行端到端训练，实现高判别性特征嵌入。

Result: 新方法在面对旋转、缩放、反射和模糊等常见难题时显示出更高的鲁棒性和泛化能力。

Conclusion: 提出的基于3D-CNN及课程式训练的虹膜认证方法有助于提升在实际复杂情境下的识别表现，具有广泛应用前景。

Abstract: Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems. However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur. In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns. To address these limitations, we propose a novel and generalized matching pipeline that learns rich spatio-spatial-temporal representations of iris features. Our approach first splits each iris image along one dimension, generating a sequence of sub-images that serve as input to a 3D-CNN, enabling the network to capture both spatial and spatio-spatial-temporal cues. To further enhance the modeling of spatio-spatial-temporal feature dynamics, we train the model in curriculum manner. This design allows the network to embed temporal dependencies directly into the feature space, improving discriminability in the deep metric domain. The framework is trained end-to-end with triplet and ArcFace loss in a curriculum manner, enforcing highly discriminative embeddings despite challenges like rotation, scale, reflections, and blur. This design yields a robust and generalizable solution for iris authentication.Github code: https://github.com/GeetanjaliGTZ/CLRecogEye

</details>


### [49] [Pygmalion Effect in Vision: Image-to-Clay Translation for Reflective Geometry Reconstruction](https://arxiv.org/abs/2511.21098)
*Gayoung Lee,Junho Kim,Jin-Hwa Kim,Junmo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种创新方法，将含有反射物体的图像转换为无反射“泥塑”形态，从而提升3D重建的几何准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 3D重建中，反射效应导致外观和几何纠缠，给准确重建带来长期挑战。现有方法在强反射情况下表现有限，因此亟需设计新的方法以解耦反射与几何特征，提高重建质量。

Method: 作者提出Pygmalion Effect in Vision框架，将反射性物体的图像通过图像-泥塑转换得到无反射的泥塑图像。设计了由基于BRDF的反射分支和泥塑引导分支组成的双分支网络，利用泥塑图像作为无反射监督信号，提高几何拟合的鲁棒性和法线恢复。

Result: 在合成与真实数据集上的实验表明，该方法在法线准确性和网格完整性上均优于现有反射处理方法。

Conclusion: 该方法不仅在技术上显著提升了含反射场景的3D重建质量，还揭示了“去除高光、以中性视角观察”可作为学习反射物体几何的新型强归纳偏置。

Abstract: Understanding reflection remains a long-standing challenge in 3D reconstruction due to the entanglement of appearance and geometry under view-dependent reflections. In this work, we present the Pygmalion Effect in Vision, a novel framework that metaphorically "sculpts" reflective objects into clay-like forms through image-to-clay translation. Inspired by the myth of Pygmalion, our method learns to suppress specular cues while preserving intrinsic geometric consistency, enabling robust reconstruction from multi-view images containing complex reflections. Specifically, we introduce a dual-branch network in which a BRDF-based reflective branch is complemented by a clay-guided branch that stabilizes geometry and refines surface normals. The two branches are trained jointly using the synthesized clay-like images, which provide a neutral, reflection-free supervision signal that complements the reflective views. Experiments on both synthetic and real datasets demonstrate substantial improvement in normal accuracy and mesh completeness over existing reflection-handling methods. Beyond technical gains, our framework reveals that seeing by unshining, translating radiance into neutrality, can serve as a powerful inductive bias for reflective object geometry learning.

</details>


### [50] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: 该论文提出了RadarFM，一种融合雷达传感和结构化空间语言的新型基础模型，用于实现跨任务的统一场景理解。通过创新性的结构化描述编码和哈希感知对比学习，提高了空间推理与泛化能力。利用CARLA仿真器生成大规模数据集，并设计新指标评价空间精确度。


<details>
  <summary>Details</summary>
Motivation: 当前雷达传感任务局限于特定任务和架构，缺乏通用模型，阻碍了多任务迁移和雷达感知潜力的充分挖掘。基础模型和空间语言的结合尚未在雷达领域深入探索，迫切需要统一框架提升雷达场景表征能力。

Method: 1) 提出结构化空间描述框架，将车辆分布以原生雷达坐标编码为场景级文本描述。
2) 引入哈希感知对比学习方法，以连续场景相似度实施训练，增强细粒度空间推理。
3) 利用CARLA仿真平台自动生成多样化驾驶场景的大规模雷达数据，并开发结合空间定位的评估指标。

Result: RadarFM在多个下游任务中展现了跨任务泛化和强空间推理能力。哈希感知对比学习促使模型能更精准区分不同空间分布的场景。在提出的新型空间准确性评价指标下，模型表现优异，超越传统检测方法。

Conclusion: RadarFM通过结构化空间语言和创新对比学习，实现了雷达感知的统一场景理解框架，为雷达感知模型在多任务间迁移和泛化能力的提升提供了新范式，对未来复杂驾驶场景中的高效感知有重要推动作用。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [51] [AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning](https://arxiv.org/abs/2511.21188)
*Zheng Li,Yibing Song,Xin Zhang,Lei Luo,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: AnchorOPT提出了一种动态锚点机制，通过任务自适应的锚点值和可学习的锚点与软token位置关系，优化基于CLIP的prompt learning，在多项实验中取得了优良性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的prompt learning方法使用固定的文本锚点，这种静态设置缺乏跨任务和阶段自适应能力，限制了泛化和灵活性。

Method: AnchorOPT在锚点值和锚点位置关系上引入动态性：（1）锚点token不再用手工指定的文本词，而是通过任务数据动态学习；（2）锚点与软token的位置由可学习的矩阵自适应调整。训练采用两阶段：先单独学习锚点token，再冻结锚点，优化软token和位置矩阵。

Result: 只用简单的可学习锚点和位置矩阵，AnchorOPT就能达到甚至超过使用额外模块或正则化的方法。实验覆盖多个数据集，结果显示性能提升一致。

Conclusion: AnchorOPT作为即插即用模块，可无缝集成到现有框架，对跨任务prompt learning有良好提升潜力。

Abstract: Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.

</details>


### [52] [EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens](https://arxiv.org/abs/2511.21106)
*Ze Feng,Sen Yang,Boqiang Duan,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出了EM-KD，一种改进高效多模态大语言模型（MLLMs）蒸馏效果的新范式，通过对视觉 token 进行空间对齐，并在此基础上设计了专门的蒸馏方法，有效提升了模型的理解能力和效率。


<details>
  <summary>Details</summary>
Motivation: 高效 MLLMs 通过压缩视觉 token 降低了算力消耗，但导致了视觉信息损失，影响模型理解能力。以往相关方法未能解决学生模型与教师模型视觉 token 不均衡带来的细粒度视觉理解差异。

Method: EM-KD 首先利用曼哈顿距离衡量学生和教师模型视觉 logits 的差异，并通过匈牙利匹配算法在空间维度上对齐视觉 token。对齐后，设计了两种知识蒸馏方法：一是计算文本 token 与对齐后视觉 token 的亲和矩阵，并最小化师生间的亲和矩阵差异（VLAD）；二是利用反向 KL 散度度量对齐后视觉 logits 在词汇空间分布的差异（VSD）。

Result: 在多个多模态任务基准上，采用 EM-KD 训练的高效 MLLMs 在准确率和效率上均大幅优于以往模型。相比于其他蒸馏方法，在公平对比条件下 EM-KD 提供了更好的性能。

Conclusion: EM-KD 成功解决了高效学生模型与标准教师模型在视觉 token 不均衡带来的匹配难题，并通过精心设计的蒸馏策略显著提升了多模态模型的综合表现。

Abstract: Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.

</details>


### [53] [Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis](https://arxiv.org/abs/2511.21397)
*Jiyun Bae,Hyunjong Ok,Sangwoo Mo,Jaeho Lee*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）在测试时加入干扰项（无关信息）对模型推理能力和表现的影响。通过构建带不同类型干扰项的视觉问答数据集，发现视觉干扰项和文本干扰项的影响机制有本质区别。提出了一种简单的提示策略来减轻模型的偏置预测。


<details>
  <summary>Details</summary>
Motivation: 已有研究发现，语言模型在有文本干扰项时，会出现逆向扩展（inverse scaling）现象，即推理路径变长但效果变差。该现象在多模态视觉-语言模型中是否同样存在尚未深入探索。论文旨在系统研究视觉干扰项对VLM推理和表现的影响机制。

Method: 作者设计了一个名为Idis的新视觉问答数据集，系统性地控制干扰项在语义、数量和空间等维度的变化。分析VLM在加入视觉干扰项后的表现，包括推理长度、准确率等，并结合Reasoning Trace追踪属性计数，理解干扰项与推理之间的交互。同时将结果拓展到已有视觉偏置基准如Waterbirds，并提出基于prompt的简单緩解策略。

Result: 实验表明，视觉干扰项虽引起逆向扩展（准确率下降），但推理长度并不会随着干扰项数量增加。通过Reasoning Trace分析发现，属性计数等特征可以有效揭示干扰项、推理长度和准确率三者间的联系。方法同样适用于Waterbirds等偏置数据集。提出的prompt策略可以缓解部分模型偏置。

Conclusion: 视觉干扰项对VLM表现的负面影响与文本干扰项机制存在关键差异。评估多模态模型推理时需特别注意干扰项设置和指标。提出的属性追踪分析及prompt策略为未来提升多模态推理效果及鲁棒性提供了新方向。

Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.

</details>


### [54] [FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain](https://arxiv.org/abs/2511.21113)
*YuAn Wang,Xiaofan Li,Chi Huang,Wenhao Zhang,Hao Li,Bosheng Wang,Xun Sun,Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D几何渲染系统（3DGS）与扩散模型的方法，实现了高保真且一致的驾驶场景重建与3D场景生成，在面对大视角变换时可维持真实的几何与视觉表现。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶场景重建与3D生成场景时，几何精度和生成外观之间存在权衡。3DGS关注几何，扩散模型擅长外观合成，两者结合时面临融合困难，尤其是在像素级和三维一致性编辑准则缺失导致恢复过度、几何漂移等问题。

Method: 提出FaithFusion框架，通过像素级期望信息增益（EIG）驱动。EIG作为统一策略引导扩散模型加权优化高不确定区域并将编辑蒸馏回3DGS，实现时空连续一致性。该方法为即插即用型，无需额外先验或结构修改。

Result: 在Waymo数据集上，方法在NTA-IoU、NTL-IoU和FID等指标取得最新最优成绩。即便车道横移6米，FID仍能维持107.47。

Conclusion: FaithFusion框架有效融合了3DGS与扩散模型优势，显著提升了大视角变换下的3D场景重建表现，无需附加先验或架构变更，实用性和泛化能力强。

Abstract: In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.

</details>


### [55] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: G$^2$VLM是一种结合3D几何学习和视觉-语言推理的新型模型，在三维空间重建和空间理解任务中具备较强表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在空间智能、尤其是空间理解和推理方面表现不佳，主要因为缺乏从2D图像到3D空间反演的几何学习能力。作者希望通过引入3D视觉几何特征，提升模型的空间理解和推理能力。

Method: 提出了G$^2$VLM，将视觉-语言模型与3D几何重建结合。该模型在训练时利用多视角图像和视频数据，通过学习3D视觉先验实现空间3D重建和空间推理，并可实现上下文学习和交错推理。设计能够扩展到大规模空间理解任务。

Result: 实验显示，G$^2$VLM能实现与现有最先进3D重建模型相当的性能，同时在空间理解和空间推理任务上取得了更优或相当的表现。

Conclusion: G$^2$VLM有效地将视觉-语言模型与低层级3D视觉任务统一，提升了空间智能表达能力。其作为新基准，有望推动如3D场景编辑等应用的发展。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [56] [Deformation-aware Temporal Generation for Early Prediction of Alzheimers Disease](https://arxiv.org/abs/2511.21114)
*Xin Honga,Jie Lin,Minghui Wang*

Main category: cs.CV

TL;DR: 本论文提出了Deformation-Aware Temporal Generative Network (DATGN)，一种能够生成符合阿尔茨海默病进展趋势的MRI时序图像的新方法，方案能够自动学习与疾病发展相关的脑部形态变化，实现了早期预测。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期预测有助于干预和减缓疾病进展。现有方法多依赖MRI图像的人工特征提取，存在效率低、主观性强等问题，因此需要发展能够自动学习脑部进展特征、提升预测准确性的模型。

Method: DATGN方法首先对存在缺失的MRI时序数据进行插值补齐，然后通过双向时序变形感知模块，生成与病变进展趋势一致的未来MRI图像。该模型在ADNI数据集上进行了训练和验证。

Result: 实验表明，DATGN生成的MRI图像在PSNR和MMSE等图像质量指标上具有竞争力。将DATGN生成的数据用于SVM、CNN、3DCNN分类时，AD vs. NC分类准确率提升6.21%到16%，AD vs. MCI vs. NC提升7.34%到21.25%。可视化结果也显示生成序列与脑萎缩趋势一致。

Conclusion: DATGN能够自动学习脑部形态的变化趋势，生成高质量的MRI序列，提升了阿尔茨海默病的早期预测能力，对相关疾病辅助诊断具有实际应用价值。

Abstract: Alzheimer's disease (AD), a degenerative brain condition, can benefit from early prediction to slow its progression. As the disease progresses, patients typically undergo brain atrophy. Current prediction methods for Alzheimers disease largely involve analyzing morphological changes in brain images through manual feature extraction. This paper proposes a novel method, the Deformation-Aware Temporal Generative Network (DATGN), to automate the learning of morphological changes in brain images about disease progression for early prediction. Given the common occurrence of missing data in the temporal sequences of MRI images, DATGN initially interpolates incomplete sequences. Subsequently, a bidirectional temporal deformation-aware module guides the network in generating future MRI images that adhere to the disease's progression, facilitating early prediction of Alzheimer's disease. DATGN was tested for the generation of temporal sequences of future MRI images using the ADNI dataset, and the experimental results are competitive in terms of PSNR and MMSE image quality metrics. Furthermore, when DATGN-generated synthetic data was integrated into the SVM vs. CNN vs. 3DCNN-based classification methods, significant improvements were achieved from 6. 21\% to 16\% in AD vs. NC classification accuracy and from 7. 34\% to 21. 25\% in AD vs. MCI vs. NC classification accuracy. The qualitative visualization results indicate that DATGN produces MRI images consistent with the brain atrophy trend in Alzheimer's disease, enabling early disease prediction.

</details>


### [57] [Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models](https://arxiv.org/abs/2511.21122)
*Changlin Li,Jiawei Zhang,Zeyi Shi,Zongxin Yang,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: EntPruner是一种针对大规模生成式视觉模型（如扩散与流模型）的自适应自动剪枝框架，通过熵引导的剪枝方法，实现在保持生成质量的同时，提升推理速度并减少参数冗余。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉生成模型在迁移到下游任务时存在显著参数冗余，亟需有效的模型剪枝方案，以便加快推理速度并降低计算消耗，但又不能损害生成输出的多样性和条件保真度。

Method: 论文提出了基于熵的块级重要性评估方法，使用条件熵偏差（CED）来衡量剪除某块对输出分布的影响。根据每个模块对特定下游任务的重要性进行动态、自适应剪枝，并设计零样本自适应剪枝框架，在训练时自动判断和调整剪枝程度，避免一次性剪枝导致的模式崩溃和性能下降。

Result: 在DiT和SiT模型上的实验表明，EntPruner在ImageNet及三个下游数据集上实现最高2.22倍推理加速，同时保持了与原模型相当的生成质量。

Conclusion: EntPruner能够高效自动地为扩散和流模型剪枝，在减少冗余、提升推理效率的同时保持优质的生成能力，适用于多种下游视觉生成任务。

Abstract: Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.

</details>


### [58] [CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion](https://arxiv.org/abs/2511.21129)
*Dianbing Xi,Jiepeng Wang,Yuanzhi Liang,Xi Qiu,Jialun Liu,Hao Pan,Yuchi Huo,Rui Wang,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种统一的扩散模型CtrlVDiff，通过融合图像的多种模态信息，实现对视频的理解和可控生成，并且在控制精度和生成一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统仅基于几何信息（如深度、边缘）的模型在视频编辑与生成中无法准确表达材质、光照等复杂外观信息，导致编辑灵活性有限，出现时间漂移等问题，因此需要引入更多语义和图形相关模态提升控制和理解能力。

Method: 提出CtrlVDiff扩散模型，采用Hybrid Modality Control Strategy（HMCS）融合和切换深度、法线、分割、边缘以及图形学内在属性（如反照率、粗糙度、金属度）等多模态特征，并能灵活应对模态缺失。同时构建了MMVideo数据集，实现多模态与文本的对齐。

Result: CtrlVDiff在视频理解和生成任务上，通过丰富模态输入实现了更精确的逐层编辑、外观调整，且在缺失部分模态的情况下表现依然稳健，控制性和生成质量均超过了主流方法。

Conclusion: 融合多种图像模态（特别是图形学内在属性）显著提升了可控视频生成和理解的能力，为实际应用中实现灵活、精准的视频编辑和合成提供了有效路径。

Abstract: We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.

</details>


### [59] [DeepRFTv2: Kernel-level Learning for Image Deblurring](https://arxiv.org/abs/2511.21132)
*Xintian Mao,Haofei Song,Yin-Nian Liu,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的去模糊方法Fourier Kernel Estimator (FKE)，通过在傅里叶空间估计模糊核，并联合解卷积网络训练，在特征层面上理解模糊过程，实现了更高效和物理一致性的去模糊效果，取得了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: 现有的图像去模糊深度网络大多停留在像素层面，要么端到端做像素恢复，要么伪核级去推断，不能让模型真正理解模糊的本质（即卷积核导致的模糊）。因此，亟需一种能让网络在模糊核级别学习模糊过程的新方法，以提升去模糊效果和泛化能力。

Method: 本文提出了Fourier Kernel Estimator (FKE)的方法：1）在傅里叶空间进行激活与估计，将空间域的卷积问题转为频域的乘法，简化问题复杂度；2）更改传统的“图像-卷积核”为“特征-估计核”卷积，利用特征的丰富语义结构信息促进核学习；3）引入解耦多尺度架构，多级子网络及可逆策略，实现多尺度高效表征并降低训练内存消耗；4）FKE与去模糊网络端到端联合优化，无需额外监督。

Result: 广泛实验表明，该方法在运动去模糊任务上取得了最新的SOTA性能，并有望用于处理其他与模糊核相关的图像修复问题。分析还显示，所学得的模糊核具有物理可解释性。

Conclusion: 该方法创新性地在特征层面和傅里叶空间高效学习模糊核，显著提升了去模糊表现和核的物理意义，为后续模糊相关问题的解决提供了新思路。

Abstract: It is well-known that if a network aims to learn how to deblur, it should understand the blur process. Blurring is naturally caused by the convolution of the sharp image with the blur kernel. Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance. But, current deep networks are still at the pixel-level learning stage, either performing end-to-end pixel-level restoration or stage-wise pseudo kernel-level restoration, failing to enable the deblur model to understand the essence of the blur. To this end, we propose Fourier Kernel Estimator (FKE), which considers the activation operation in Fourier space and converts the convolution problem in the spatial domain to a multiplication problem in Fourier space. Our FKE, jointly optimized with the deblur model, enables the network to learn the kernel-level blur process with low complexity and without any additional supervision. Furthermore, we change the convolution object of the kernel from ``image" to network extracted ``feature", whose rich semantic and structural information is more suitable to blur process learning. With the convolution of the feature and the estimated kernel, our model can learn the essence of blur in kernel-level. To further improve the efficiency of feature extraction, we design a decoupled multi-scale architecture with multiple hierarchical sub-unets with a reversible strategy, which allows better multi-scale encoding and decoding in low training memory. Extensive experiments indicate that our method achieves state-of-the-art motion deblurring results and show potential for handling other kernel-related problems. Analysis also shows our kernel estimator is able to learn physically meaningful kernels. The code will be available at https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur.

</details>


### [60] [Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning](https://arxiv.org/abs/2511.21136)
*Changlin Li,Jiawei Zhang,Shuhao Liu,Sihao Lin,Zeyi Shi,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出了一种高效训练人类视频生成扩散模型的新方法 Ent-Prog，显著提升训练速度并降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 人类视频生成需要处理高分辨率、多帧数据，对计算资源和显存的需求极高，限制了模型的实际应用。

Method: 1）引入条件熵膨胀（CEI）指标来评估模型各部分对条件生成任务的重要性，优先训练关键部分；2）采用自适应渐进式训练策略，根据模型的收敛效率动态调整计算复杂度。

Result: 在三个数据集上实验，Ent-Prog实现最多2.2倍训练加速、2.4倍显存降低，且生成质量无损失。

Conclusion: Ent-Prog有效提升了扩散模型在人类视频生成任务上的训练效率，为大规模视频生成模型训练提供了更优方案。

Abstract: Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\times$ training speedup and 2.4$\times$ GPU memory reduction without compromising generative performance.

</details>


### [61] [Referring Video Object Segmentation with Cross-Modality Proxy Queries](https://arxiv.org/abs/2511.21139)
*Baoli Sun,Xinzhu Ma,Ning Wang,Zhihui Wang,Zhiyong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProxyFormer的新型跨模态视频对象分割（RVOS）方法，通过引入代理查询（proxy queries）促进视觉和文本语义的融合，在多个阶段动态地聚焦目标对象，并建立帧间依赖，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法中，条件性查询缺乏帧间依赖和变化建模，目标追踪不够准确；此外，文本约束引入较晚，导致视频特征可能关注到非目标对象，影响性能。为解决这些问题，本文提出更紧密结合视觉和文本语义的机制。

Method: 提出ProxyFormer架构，核心为多组代理查询，逐步在视频特征编码器中进行语义更新与传播，强化与目标对象相关的特征；通过代理查询动态建立帧间依赖，提升追踪准确性和连贯性；为提升效率，将跨模态交互分解为时域与空域；同时，设计联合语义一致性（JSC）训练策略，使代理查询与视频-文本对达成语义共识。

Result: 在四个常用RVOS数据集上进行全面实验证明，ProxyFormer方法在分割精准度和追踪连续性等核心指标上均优于当前主流方法。

Conclusion: ProxyFormer能高效融合视觉与文本信息，实现帧间依赖建模，并有效提升视频对象分割的准确性与一致性，在RVOS领域具有显著的技术优势。

Abstract: Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.

</details>


### [62] [TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models](https://arxiv.org/abs/2511.21145)
*Jiaming He,Guanyu Hou,Hongwei Li,Zhicong Huang,Kangjie Chen,Yi Yu,Wenbo Jiang,Guowen Xu,Tianwei Zhang*

Main category: cs.CV

TL;DR: 本文提出了针对文本生成视频（T2V）模型的自动化红队测试框架TEAR，有效识别与视频时间动态相关的安全风险。TEAR能生成表面无害、但实际可能诱发违规输出的测试用例，在多种T2V系统上大幅提升攻破率至80%以上。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型虽能生成高质量视频，但随之带来复杂的安全挑战。以往的安全评估多针对静态图片和文本，难以覆盖视频生成中与时间动态相关的风险。因此亟需专门方法评估T2V模型的安全性。

Method: 作者提出了TEAR自动化框架，包括：1）采用两阶段优化的时间感知测试用例生成器（先训练生成器，再通过时间偏好在线学习提升）；2）循环优化方法以增强生成提示的隐蔽性和攻击效果。TEAR可以生成在文本上无害，但利用视频时间序列特性诱发违规输出的测试用例。

Result: TEAR在多个开源及商用T2V模型上进行了实验，结果显示成功攻破率超过80%，相比此前最佳方法（57%）有显著提升。

Conclusion: TEAR为T2V模型安全性评估提供了强有力工具，揭示了现有系统在时间动态诱导下的漏洞，并为未来T2V模型的防护与规范发展提供实践参考。

Abstract: Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.

</details>


### [63] [LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs](https://arxiv.org/abs/2511.21150)
*Shichu Sun,Yichen Zhang,Haolin Song,Zonghao Guo,Chi Chen,Yidan Zhang,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉压缩方法（Progressive Visual Compression, PVC），用于高效的多模态大语言模型架构，实现了在提升效率的同时维持竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型倾向于使用全局高分辨率视觉编码，比以往切片（slice-based）方法表现更好，但带来了计算负担。为了解决原生高分辨率编码的高计算开销，作者提出寻找高效视觉编码方案。

Method: 提出Progressive Visual Compression（PVC）方法，包含两个关键模块：（1）细化的patch嵌入，实现灵活的patch尺寸以支持细粒度视觉建模；（2）分层窗口token压缩，在Vision Transformer（ViT）各层渐进地聚合局部token表征。该方法能够无缝集成到标准ViT中，提升编码效率。

Result: 将PVC集成进ViT后，称为ViT-UHD，该模型在多个基准测试上性能与MoonViT相当，但TTFT（生成首token延迟）降低了2.4倍。在此基础上构建的LLaVA-UHD v3模型性能与Qwen2-VL相当，但TTFT进一步减少了1.9倍。

Conclusion: PVC方法允许ViT架构在维持性能的同时显著提升效率，为构建高效多模态大模型提供了可行路径。作者承诺开源以促进后续相关研究。

Abstract: Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.

</details>


### [64] [Progress by Pieces: Test-Time Scaling for Autoregressive Image Generation](https://arxiv.org/abs/2511.21185)
*Joonhyung Park,Hyeongwon Jang,Joowon Kim,Eunho Yang*

Main category: cs.CV

TL;DR: 本文提出了GridAR，一种改进视觉自回归（AR）模型的测试时扩展计算框架，在有限的计算量下，也能获得更高质量的图像生成和编辑结果。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型虽然有望实现出色的文本生成图像能力，但当前面临的测试时计算扩展（如Best-of-N采样）存在效率低和输出质量不佳的问题，尤其在生成全局一致的画面布局时更加突出。

Method: GridAR将画布划分为网格，在每个位置上生成多个部分候选，通过早期剪枝去除不合理候选，并将好的候选作为锚点引导后续生成。同时，引入布局指定的提示词重构策略，在生成过程中推断合理布局，从而缓解传统逐步扫描生成无法感知全局布局的问题。最后，框架提高了生成效率和内容一致性。

Result: 在T2I-CompBench++基准测试中，GridAR用N=4的采样方式超越了Best-of-N（N=8）的性能（提升14.4%），同时推理成本降低25.6%；在PIE-Bench图像编辑评测中，GridAR的编辑质量与更大N的基线相当，但在语义保持性上提升了13.9%。

Conclusion: GridAR能够在有限推理计算预算下，显著提升视觉自回归模型在文本生成图像和图像编辑任务中的表现，具有很好的泛用性和实用价值。

Abstract: Recent visual autoregressive (AR) models have shown promising capabilities in text-to-image generation, operating in a manner similar to large language models. While test-time computation scaling has brought remarkable success in enabling reasoning-enhanced outputs for challenging natural language tasks, its adaptation to visual AR models remains unexplored and poses unique challenges. Naively applying test-time scaling strategies such as Best-of-N can be suboptimal: they consume full-length computation on erroneous generation trajectories, while the raster-scan decoding scheme lacks a blueprint of the entire canvas, limiting scaling benefits as only a few prompt-aligned candidates are generated. To address these, we introduce GridAR, a test-time scaling framework designed to elicit the best possible results from visual AR models. GridAR employs a grid-partitioned progressive generation scheme in which multiple partial candidates for the same position are generated within a canvas, infeasible ones are pruned early, and viable ones are fixed as anchors to guide subsequent decoding. Coupled with this, we present a layout-specified prompt reformulation strategy that inspects partial views to infer a feasible layout for satisfying the prompt. The reformulated prompt then guides subsequent image generation to mitigate the blueprint deficiency. Together, GridAR achieves higher-quality results under limited test-time scaling: with N=4, it even outperforms Best-of-N (N=8) by 14.4% on T2I-CompBench++ while reducing cost by 25.6%. It also generalizes to autoregressive image editing, showing comparable edit quality and a 13.9% gain in semantic preservation on PIE-Bench over larger-N baselines.

</details>


### [65] [Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding](https://arxiv.org/abs/2511.21191)
*Yutao Tang,Cheng Zhao,Gaurav Mittal,Rohith Kukkala,Rama Chellappa,Cheng Peng,Mei Chen*

Main category: cs.CV

TL;DR: 本文提出了NDTokenizer3D，一种通用的三维视觉-语言模型(VLM)，通过创新的三阶段场景分词流程，实现对复杂3D场景的理解和人机交互，在3D相关任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉-语言模型在如何将3D场景高效转换为易于理解的整体Token，并在多种3D任务上泛化应用方面存在不足，限制了3D场景推理和与人类交互的能力。

Method: 提出基于多尺度正态分布变换(NDT)表示和多尺度NDT解码器(MSDec)的三阶段场景分词流程：首先将高分辨率点云转为包含全局和细粒度信息的多尺度NDT表示；然后多尺度特征融合，生成可供大型语言模型消化的整体Token；MSDec同时作为统一的人机交互与分割掩码解码接口，支持多任务处理。

Result: NDTokenizer3D在3D指向分割、3D视觉问答和3D密集标注等任务上取得了显著性能提升，验证了其方法的有效性和通用性。

Conclusion: 该模型以紧凑统一的设计，实现了对复杂3D场景的细粒度、通用性理解，拓展了3D视觉-语言模型在实际应用中的能力，具有较大潜力。

Abstract: Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.

</details>


### [66] [When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21192)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Qixin Zhang,Bingquan Shen,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本论文针对视觉-语言-动作（VLA）模型提出了一种通用并具可迁移性的对抗补丁攻击方法UPA-RFAS，并验证其在不同模型、任务和实际物理环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型在对抗攻击下存在脆弱性，而现有对抗补丁大多只能针对单一模型，缺乏可迁移性和在黑盒场景下的适用性。论文旨在填补这一研究空白，探索通用且能跨模型迁移的物理补丁攻击方法。

Method: 提出UPA-RFAS框架，包括：1）特征空间目标结合L1偏差和InfoNCE损失引导可迁移的表示扰动；2）两阶段极小极大过程增强鲁棒性，内循环学习不可见的样本扰动，外循环优化补丁使其能对抗这些扰动；3）两个面向VLA的专用损失函数：补丁注意力主导（劫持文本-视觉关注机制）和补丁语义错配（无标签诱导图文失配）。

Result: 在多种VLA模型、任务套件及实际物理操作中的实验表明，UPA-RFAS补丁能实现跨模型、跨任务的有效迁移攻击，暴露了实际可行的补丁对抗面。

Conclusion: UPA-RFAS成为VLA领域首个强力、通用和可迁移的补丁攻击基线，对今后的防御研究具有重要参考意义。

Abstract: Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.

</details>


### [67] [You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering](https://arxiv.org/abs/2511.21193)
*Hanyang Li,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 本文提出了一种名为DCBoost的无参数插件，可增强深度聚类模型的全局特征结构，并显著提升现有聚类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度聚类方法虽然在局部特征上展现出良好的类内一致性和紧凑性，但在全局特征上常出现类间边界交错、聚类分离度差的问题。为此，作者希望通过改进全局特征结构，进一步提升聚类效果。

Method: 作者提出DCBoost方法，利用自适应k近邻一致性过滤，筛选高置信度样本作为自监督的锚点，并利用这些样本计算判别损失，促进类内紧凑、类间分离，指导网络优化。该方法可以无缝嵌入现有深度聚类框架，无需额外参数。

Result: 在多个主流基准数据集上的实验表明，DCBoost可在多种现有深度聚类模型基础上显著提升聚类性能，对最先进基线如ProPos提升超过3%，Silhouette系数提升超过7倍。

Conclusion: DCBoost是一种有效且通用的深度聚类增强方法，易于集成并能大幅提升聚类模型的全局特征结构及聚类效果。

Abstract: Recent deep clustering models have produced impressive clustering performance. However, a common issue with existing methods is the disparity between global and local feature structures. While local structures typically show strong consistency and compactness within class samples, global features often present intertwined boundaries and poorly separated clusters. Motivated by this observation, we propose DCBoost, a parameter-free plug-in designed to enhance the global feature structures of current deep clustering models. By harnessing reliable local structural cues, our method aims to elevate clustering performance effectively. Specifically, we first identify high-confidence samples through adaptive $k$-nearest neighbors-based consistency filtering, aiming to select a sufficient number of samples with high label reliability to serve as trustworthy anchors for self-supervision. Subsequently, these samples are utilized to compute a discriminative loss, which promotes both intra-class compactness and inter-class separability, to guide network optimization. Extensive experiments across various benchmark datasets showcase that our DCBoost significantly improves the clustering performance of diverse existing deep clustering models. Notably, our method improves the performance of current state-of-the-art baselines (e.g., ProPos) by more than 3% and amplifies the silhouette coefficient by over $7\times$. Code is available at <https://github.com/l-h-y168/DCBoost>.

</details>


### [68] [BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data](https://arxiv.org/abs/2511.21194)
*Selene Cerna,Sara Si-Moussi,Wilfried Thuiller,Hadrien Hendrikx,Vincent Miele*

Main category: cs.CV

TL;DR: 本文提出了一种名为BotaCLIP的新方法，通过对地球观测基础模型DOFA进行轻量级的、多模态对比学习适配，使模型更好地服务于植物学领域，提升生态相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型虽具有泛化能力，但难以无缝注入特定领域知识，尤其在数据稀缺的实际生态学场景下，直接重训成本高昂，且容易遗忘原有知识。因此，迫切需要一种高效地将专业知识融入这些通用模型的方法。

Method: 提出了BotaCLIP框架，通过多模态对比学习，将高分辨率航空影像与植物学地面调查数据对齐，并采用正则化技术抑制灾难性遗忘，实现对已有地球观测大模型（DOFA）的高效适配，避免从头训练。

Result: 在植物存在性预测、蝴蝶出现建模和土壤营养级群落丰度估计三个生态学任务上，BotaCLIP生成的表征均优于直接采用DOFA和有监督基线方法。

Conclusion: BotaCLIP可有效将领域知识融入大模型，在数据匮乏环境下实现简约高效的表征学习，为基础模型的领域自适应提供了新思路，具有广泛生态实际应用潜力。

Abstract: Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevés. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.

</details>


### [69] [Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition](https://arxiv.org/abs/2511.21202)
*Baoli Sun,Yihan Wang,Xinzhu Ma,Zhihui Wang,Kun Lu,Zhiyong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的细粒度动作识别方法ART，能够发现和跟踪视频中动作的细节区域，有效区分相似动作，在多个基准上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度动作识别方法无法有效捕捉视频中随时间演化的局部细节，本研究旨在解决该难题，提升同类细微动作间的区分能力。

Method: 1）提出“动作-区域追踪”（ART）框架，通过查询-响应机制发现关键区域并追踪其动态；2）利用区域语义激活模块，用区分性文本语义作为查询，提取与动作相关的区域响应；3）通过语言模型中的文本分支提取动作标签描述信息，丰富语义信息；4）将区域响应组织为“动作追踪片段”（tracklets），表示动作的动态特性；5）采用多层级对比损失，优化空间和时间维度下的片段相关性；6）引入针对任务的微调机制，优化语言模型的表征以符合任务特性。

Result: 在多个主流动作识别数据集上进行了实验，ART框架在准确率等评测指标上均超越了现有主流细粒度动作识别方法。

Conclusion: ART框架能够显著提升细粒度动作识别的性能，特别是在需要辨别相似动作的场景下有明显优势，对实际应用具有推广价值。

Abstract: Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.

</details>


### [70] [From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting](https://arxiv.org/abs/2511.21215)
*Umang Agarwal,Rudraksh Sangore,Sumit Laddha*

Main category: cs.CV

TL;DR: 本文对DDPM、CFM和MeanFlow三种生成模型进行了系统比较，CFM在采样质量和速度上均优于DDPM，MeanFlow支持一步生成且推理速度显著提升，CFM拓展到图像修复效果出色。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型如扩散模型逐渐流行，但存在采样速度慢、模型结构复杂等问题。本研究希望全面比较主流生成范式的质量与效率，并探索如何实用化这些方法及其在下游任务（如图像修复）上的应用潜力。

Method: 采用统一且精简的TinyUNet（参数<1.5M）作为基座，对比DDPM、CFM和MeanFlow在CIFAR-10数据集上的生成质量（FID分数）及采样速度，并将CFM算法扩展到四种掩码类型的图像修复任务，通过引入inpainting-aware训练优化修复结果。

Result: 实验表明：CFM以50步采样达到24.15的FID，远超DDPM的402.98。MeanFlow仅用一步采样，FID为29.15，推理速度提升约50倍。在图像修复任务中，CFM经专门训练后，中心掩码条件下PSNR由4.95提升至8.57 dB，SSIM由0.289升至0.418，均有显著提升。

Conclusion: CFM在生成质量与效率上整体优于传统扩散模型，MeanFlow实现了快速一步采样。CFM扩展到图像修复任务表现突出，结合专门训练，可有效提升修复结果，说明该范式的实用性和应用前景。

Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.

</details>


### [71] [3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization](https://arxiv.org/abs/2511.21237)
*Shuhan Xia,Xuannan Liu,Xing Cui,Peipei Li*

Main category: cs.CV

TL;DR: 本文提出了T3-Tracer，这是首个结合帧、片段和全音频三个层次分析的部分音频伪造检测框架，能更全面地发现音频伪造痕迹，并在多个数据集上取得了最优检测效果。


<details>
  <summary>Details</summary>
Motivation: 部分音频伪造只篡改关键帧，维持整体听觉自然性，导致检测难度大。现有方法仅独立检测帧级伪造，无法对多时域层次异常进行有效建模，为此亟需一种多层次分析检测方法。

Method: 作者提出了T3-Tracer框架，包含两个核心模块：一是帧-音频特征聚合模块（FA-FAM），将帧级和音频级信息融合，实现帧级伪造检测及全局语义一致性建模；二是分段多尺度差异感知模块（SMDAM），采用双分支结构，多尺度窗口建模帧特征及相邻差异，突出伪造边界处的异常表达。该框架对帧、片段、音频三个层次联合分析。

Result: 在三个具有挑战性的数据集上，T3-Tracer在检测精度等各项指标上均达到了当前最优水平，显著优于已有部分音频伪造检测方法。

Conclusion: 本文的T3-Tracer通过多层次结构有效补足了现有检测方法在时域建模上的不足，能够更详尽捕捉和校正音频局部与全局伪造痕迹，对部分音频伪造检测领域具有重要推动作用。

Abstract: Recently, partial audio forgery has emerged as a new form of audio manipulation. Attackers selectively modify partial but semantically critical frames while preserving the overall perceptual authenticity, making such forgeries particularly difficult to detect. Existing methods focus on independently detecting whether a single frame is forged, lacking the hierarchical structure to capture both transient and sustained anomalies across different temporal levels. To address these limitations, We identify three key levels relevant to partial audio forgery detection and present T3-Tracer, the first framework that jointly analyzes audio at the frame, segment, and audio levels to comprehensively detect forgery traces. T3-Tracer consists of two complementary core modules: the Frame-Audio Feature Aggregation Module (FA-FAM) and the Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM). FA-FAM is designed to detect the authenticity of each audio frame. It combines both frame-level and audio-level temporal information to detect intra-frame forgery cues and global semantic inconsistencies. To further refine and correct frame detection, we introduce SMDAM to detect forgery boundaries at the segment level. It adopts a dual-branch architecture that jointly models frame features and inter-frame differences across multi-scale temporal windows, effectively identifying abrupt anomalies that appeared on the forged boundaries. Extensive experiments conducted on three challenging datasets demonstrate that our approach achieves state-of-the-art performance.

</details>


### [72] [FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision](https://arxiv.org/abs/2511.21245)
*Chen Ling,Henglin Shi,Hedvig Kjellström*

Main category: cs.CV

TL;DR: 本文提出了一种名为FIELDS的新方法，通过结合2D一致性线索与直接3D表情参数监督及情感识别辅助模块，实现了单张图片的人脸高保真3D重建，显著提升了情感表达的精细还原和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有3D人脸重建方法多依赖2D监督，缺乏3D真实表情数据，难以还原表情中的细微情感信息。因此，需要创新的方法来弥补2D/3D领域差异，并提升重建的人脸情感表达能力。

Method: 提出FIELDS模型，将自监督的2D图像一致性线索与直接3D表情参数监督相结合，并额外引入情感识别分支。模型的编码器由真实4D人脸扫描获取的表情参数引导，并借助表情强度感知的情感损失函数，准确刻画3D表情参数中的真实情感内容，避免情感夸大失真。通过这种双重监督，同时弥合2D和3D领域差异，减少表情强度偏差。

Result: FIELDS可从单张图片生成表情细致、极具情感真实感的3D人脸模型，在保持自然性的情况下，显著提升了野外场景下的面部表情识别表现。

Conclusion: FIELDS突破了现有方法在情感细节还原方面的局限，实现了更真实、更具表现力的3D人脸重建，为情感计算等相关应用提供了更优质的数据基础。

Abstract: Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.

</details>


### [73] [Shift-Equivariant Complex-Valued Convolutional Neural Networks](https://arxiv.org/abs/2511.21250)
*Quentin Gabot,Teck-Yian Lim,Jérémy Fix,Joana Frontera-Pons,Chengfang Ren,Jean-Philippe Ovarlez*

Main category: cs.CV

TL;DR: 本文提出将Learnable Polyphase up/downsampling（LPS）方法从实值神经网络扩展到复值神经网络，并引入新型投影层，系统性地提升卷积神经网络的平移等变与不变性，在多个计算机视觉任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络因下采样和上采样操作会破坏平移等变与不变性，这限制了模型的泛化性能和鲁棒性。现有方法多用数据增强来缓解，但并非理论上完整解决。为此，本文旨在设计能从理论上保证这两种属性的上/下采样层，进一步扩展到复值神经网络以应对极化SAR图像等更广泛场景。

Method: 本文基于Learnable Polyphase up/downsampling（LPS）机制，在上Gumbel Softmax操作前引入复数域到实数域的投影层，将该结构应用于复值神经网络架构。理论上推导其对平移等变性和不变性的影响，并在多个计算机视觉任务（包括分类、重建和语义分割）上，采用极化合成孔径雷达（SAR）图像测试了该方法。

Result: 实验结果显示，扩展后的复值LPS结构在分类任务中提升了平移不变性，在重建和分割任务中增强了平移等变性，并在极化SAR图像数据上展现出优越性能。

Conclusion: 将LPS系统性地引入复值神经网络，可通过理论保障的上/下采样层，实现平移等变与不变性，有效提高神经网络在各类计算机视觉任务中的鲁棒性和表现，特别适用于极化SAR等特殊数据场景。

Abstract: Convolutional neural networks have shown remarkable performance in recent years on various computer vision problems. However, the traditional convolutional neural network architecture lacks a critical property: shift equivariance and invariance, broken by downsampling and upsampling operations. Although data augmentation techniques can help the model learn the latter property empirically, a consistent and systematic way to achieve this goal is by designing downsampling and upsampling layers that theoretically guarantee these properties by construction. Adaptive Polyphase Sampling (APS) introduced the cornerstone for shift invariance, later extended to shift equivariance with Learnable Polyphase up/downsampling (LPS) applied to real-valued neural networks. In this paper, we extend the work on LPS to complex-valued neural networks both from a theoretical perspective and with a novel building block of a projection layer from $\mathbb{C}$ to $\mathbb{R}$ before the Gumbel Softmax. We finally evaluate this extension on several computer vision problems, specifically for either the invariance property in classification tasks or the equivariance property in both reconstruction and semantic segmentation problems, using polarimetric Synthetic Aperture Radar images.

</details>


### [74] [AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs](https://arxiv.org/abs/2511.21251)
*Shuhan Xia,Peipei Li,Xuannan Liu,Dongsen Zhang,Xinyu Guo,Zekun Li*

Main category: cs.CV

TL;DR: 本文提出了首个涵盖多类型、多层次音视频伪造检测的AVFakeBench基准，促进更全面的伪造检测能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有音视频伪造检测基准主要集中在以DeepFake为主的人脸伪造，且注释粒度单一，无法反映实际场景中更丰富、多样的伪造类型及复杂性。鉴于音视频伪造技术不断进化，亟需更真实、细致和多样化的评测基准以推动研究发展。

Method: 作者构建了AVFakeBench基准，覆盖人及非人主体，包含12K高质量音视频问题、7种伪造类型和4层注释，通过多阶段、混合式伪造生成框架统筹任务和高精度合成伪造内容。此外，建立了涵盖二分类判断、类型分类、细节选择和解释推理等多任务评估体系，并用11种AV大模型和2种主流检测方法进行测试。

Result: 测试发现，当前AV大模型虽展现出成为新型伪造检测工具的潜力，但在细粒度感知与推理能力上存在明显不足。

Conclusion: AVFakeBench填补了现有基准的空缺，推动了伪造检测研究走向更复杂和多样的应用场景，同时也揭示了AV大模型面临的新挑战。

Abstract: The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.

</details>


### [75] [LaGen: Towards Autoregressive LiDAR Scene Generation](https://arxiv.org/abs/2511.21256)
*Sizhuo Zhou,Xiaosong Jia,Fanrui Zhang,Junjie Li,Juyong Zhang,Yukang Feng,Jianwen Sun,Songbur Wong,Junqi You,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为 LaGen 的新颖模型，实现了基于 LiDAR 数据的长时序逐帧自回归生成，显著优于现有方法，尤其在后期帧表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前主流的 LiDAR 数据生成方法只能生成单帧，或者在预测多帧时需要多帧历史输入且结果缺乏交互性，难以支持长时序、可交互的生成任务。解决这些局限，有助于自动驾驶场景的智能建模和预测。

Method: 作者提出了 LaGen 框架，支持以单帧 LiDAR 数据和边界框为条件，自回归生成高保真的 4D 场景点云。该模型引入了“场景解耦估计模块”以提升物体级别的交互生成能力，以及“噪声调节模块”以减缓长时序生成中的误差累积。

Result: 基于 nuScenes 数据集设计了长时序 LiDAR 场景生成的评测协议。实验结果显示 LaGen 在多个评测指标上，尤其是在生成序列后期帧时，均优于当前最先进的 LiDAR 生成和预测模型。

Conclusion: LaGen 成功实现了支持长时序、逐帧、交互式 LiDAR 场景生成，并在实验中取得领先性能，为自动驾驶感知系统提供了更强大的生成建模工具。

Abstract: Generative world models for autonomous driving (AD) have become a trending topic. Unlike the widely studied image modality, in this work we explore generative world models for LiDAR data. Existing generation methods for LiDAR data only support single frame generation, while existing prediction approaches require multiple frames of historical input and can only deterministically predict multiple frames at once, lacking interactivity. Both paradigms fail to support long-horizon interactive generation. To this end, we introduce LaGen, which to the best of our knowledge is the first framework capable of frame-by-frame autoregressive generation of long-horizon LiDAR scenes. LaGen is able to take a single-frame LiDAR input as a starting point and effectively utilize bounding box information as conditions to generate high-fidelity 4D scene point clouds. In addition, we introduce a scene decoupling estimation module to enhance the model's interactive generation capability for object-level content, as well as a noise modulation module to mitigate error accumulation during long-horizon generation. We construct a protocol based on nuScenes for evaluating long-horizon LiDAR scene generation. Experimental results comprehensively demonstrate LaGen outperforms state-of-the-art LiDAR generation and prediction models, especially on the later frames.

</details>


### [76] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种改进3D Gaussian Splatting（3DGS）以生成几何精确、丰富多样训练数据的新方法MatchGS，从而显著提升零样本图像匹配算法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的图像匹配高度依赖海量、准确的训练数据。而3DGS虽可生成逼真新视角图像，但现有的几何不准确和深度偏差阻碍了其生成高质量配准标签数据，限制了用于匹配模型训练的潜力。

Method: 作者提出MatchGS，包括两部分：①对3DGS模型进行几何优化，精细其三维结构，生成高精度的像素配对标签，保证渲染保真并能涵盖极大视角变化；②提出2D-3D对齐策略，将3DGS的三维知识注入2D特征匹配网络，使2D匹配器可学习到更具视角鲁棒性的三维表达。

Result: MatchGS生成的标注数据，其本质的基准误差（epipolar error）比以往数据集低约40倍，并能在极端视角变化下产生稳定监督。此外，使用这些数据独立训练的SOTA匹配器，在公开基准上实现最高达17.7%的性能提升。

Conclusion: 经过精确几何优化的3DGS，可作为高质量、可扩展的结构化监督数据源，为打造新一代更强大的零样本图像匹配算法奠定基础。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [77] [Co-Training Vision Language Models for Remote Sensing Multi-task Learning](https://arxiv.org/abs/2511.21272)
*Qingyun Li,Shuran Ma,Junwei Luo,Yi Yu,Yue Zhou,Fengxiang Wang,Xudong Lu,Xiaoxing Wang,Xin He,Yushi Chen,Xue Yang,Junchi Yan*

Main category: cs.CV

TL;DR: 本论文提出了RSCoVLM，一种专为遥感多任务学习（MTL）设计的视觉语言模型基线，旨在统一并提升遥感图像理解、问答和检测等多种任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在遥感单任务上表现优异，但实际应用亟需能同时处理多种任务的统一模型，以提升泛化性和实用性。多任务学习方法相比单任务有更强的泛化能力与可扩展性。近期视觉语言模型展现出在多模态遥感任务上的潜力，因而亟需统一且高效的遥感多任务模型。

Method: 作者设计了RSCoVLM基线，包括：1）建立数据管理引擎，实现数据采集、离线处理整合与在线加载加权，适应复杂遥感环境，支持灵活视觉-语言对话；2）提出动态分辨率策略，涵盖不同尺度遥感图像；3）针对超高分辨率图像提出Zoom-in Chain机制并构建LRS-VQA-Zoom数据集，减轻计算压力；4）增强模型检测能力并设计新评测协议，实现与传统检测模型的公平比较。

Result: RSCoVLM在多项遥感任务上实现了SOTA（最优）性能，全面超越现有遥感视觉语言模型，部分任务表现可媲美专业检测模型。

Conclusion: RSCoVLM作为灵活的多任务遥感视觉语言模型基线，有效促进任务统一、提升模型泛化和应用能力。全套开源数据与工具有望推动通用遥感模型的进一步研究和发展。

Abstract: With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.

</details>


### [78] [PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery](https://arxiv.org/abs/2511.21298)
*Jules Decaestecker,Nicolas Vigne*

Main category: cs.CV

TL;DR: 本文提出PathMamba，一种结合了State Space Model（Mamba）和Vision Transformer的混合架构，有效提升了卫星图像中道路分割的准确性和拓扑连续性，同时保证了计算效率，在相关数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有道路分割方法大多依赖Vision Transformer，能捕捉全局信息但计算复杂度高，尤其不适用于资源受限的平台。而新兴的State Space Model（如Mamba）具有线性效率且善于建模长连续结构，非常适合道路形态。作者认为两者优势互补，亟需一种兼顾效率和拓扑表现的新方法。

Method: 作者提出了PathMamba——一种创新性的混合模型架构：通过Mamba块对道路网络的连续性进行建模，保持道路的拓扑结构；结合Transformer块对特征进行全局优化和精细化。该模型在整体结构上有策略性地布局两种模块，既提升了拓扑连贯性，又控制了计算消耗。

Result: PathMamba在DeepGlobe Road Extraction和Massachusetts Roads等数据集上创下新SOTA，尤其在APLS等拓扑连续性评估指标上有显著提升，同时保持了与Transformer模型相媲美的计算效率。

Conclusion: PathMamba模型充分融合了Sequential State Space Model与Transformer的优势，证明可以在保证高拓扑连续性和准确率的同时减少计算资源消耗，对实际道路抽取等应用有重要意义。

Abstract: Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive.

</details>


### [79] [CaliTex: Geometry-Calibrated Attention for View-Coherent 3D Texture Generation](https://arxiv.org/abs/2511.21309)
*Chenyu Liu,Hongze Chen,Jingzhi Bao,Lingting Zhu,Runze Zhang,Weikai Chen,Zeyu Hu,Yingda Yin,Keyang Luo,Xin Wang*

Main category: cs.CV

TL;DR: 现有3D纹理生成存在跨视角不一致，CaliTex通过引入几何校准注意力机制，提升了一致性和视觉效果。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在3D生成任务中取得巨大进展，但3D纹理通常在一个视角下看来合理，在其他视角下却出现明显错位和不一致，严重影响实际应用。

Method: 提出CaliTex框架，核心是几何校准注意力机制，包括两个模块：一是部分对齐注意力（Part-Aligned Attention），实现语义部分的空间对齐；二是条件路由注意力（Condition-Routed Attention），通过几何条件引导外观信息流动，从而保持空间一致性。同时采用两阶段扩散transformer模型实现纹理生成。

Result: CaliTex能生成无缝、跨视角一致、高质量的3D纹理效果，效果优于开源和商用主流对比方法。

Conclusion: 通过几何校准注意力机制，CaliTex使几何一致性成为网络内在属性，极大提升了3D纹理生成的实际表现和应用潜力。

Abstract: Despite major advances brought by diffusion-based models, current 3D texture generation systems remain hindered by cross-view inconsistency -- textures that appear convincing from one viewpoint often fail to align across others. We find that this issue arises from attention ambiguity, where unstructured full attention is applied indiscriminately across tokens and modalities, causing geometric confusion and unstable appearance-structure coupling. To address this, we introduce CaliTex, a framework of geometry-calibrated attention that explicitly aligns attention with 3D structure. It introduces two modules: Part-Aligned Attention that enforces spatial alignment across semantically matched parts, and Condition-Routed Attention which routes appearance information through geometry-conditioned pathways to maintain spatial fidelity. Coupled with a two-stage diffusion transformer, CaliTex makes geometric coherence an inherent behavior of the network rather than a byproduct of optimization. Empirically, CaliTex produces seamless and view-consistent textures and outperforms both open-source and commercial baselines.

</details>


### [80] [HTTM: Head-wise Temporal Token Merging for Faster VGGT](https://arxiv.org/abs/2511.21317)
*Weitian Wang,Lukas Meiner,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: 提出并加速了VGGT模型，使其能更高效地进行3D场景重建，实现最高7倍加速且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: VGGT模型虽然能共同推断3D场景的各类属性，但其全局关注机制导致在大尺度场景或长序列输入下存在推理延迟（瓶颈）。作者希望通过加速方法缓解这一问题。

Method: 提出了一种无需额外训练的3D Token合并方法——HTTM（head-wise temporal merging），它在多头粒度上进行Token合并，避免了传统方法中“合并后Token完全一致”导致的表达能力损失。同时，该方法利用了多头层面的空间局部性和时序对应关系，可以实现更高比例的Token合并且成本更低。

Result: 在GPU上进行推理时，HTTM加速VGGT模型最多能达7倍加速，而且性能几乎不受影响。

Conclusion: HTTM显著加速了VGGT在大规模3D重建任务中的推理过程，有效提升了实用性，并在性能损失极小的前提下推动了相关技术的落地应用。

Abstract: The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.

</details>


### [81] [The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment](https://arxiv.org/abs/2511.21331)
*Stefanos Koutoupis,Michaela Areti Zervou,Konstantinos Kontras,Maarten De Vos,Panagiotis Tsakalides,Grigorios Tsagatakis*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态对比融合（ConFu）方法，通过联合嵌入单一模态及其融合组合，提高了多模态学习在检索和分类等任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习方法大多关注于成对模态的对齐，难以充分捕捉多模态间的高阶依赖关系，限制了其在单模态任务上的效果。近期尽管有方法尝试捕捉多模态间更高阶的关系，但常常忽视或不完全保留模态之间的成对关系。

Method: 提出对比融合（Contrastive Fusion, ConFu）框架，在统一的表示空间同时对齐单个模态、模态对及其融合表示，将传统的成对对比目标扩展为添加了融合模态对比项，促使模态成对与第三模态进行联合嵌入，有效捕捉高阶互依关系。

Result: 在合成和真实多模态数据集上进行了广泛实验，ConFu在跨模态互补、高阶关系建模和扩展多模态复杂性方面表现出色，并在检索及分类任务中取得有竞争力的结果。支持统一的一对一和两对一检索。

Conclusion: ConFu在同时保持强成对对齐和有效捕捉高阶关系方面取得平衡，为多模态学习提供了更有效的表征方式。

Abstract: Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.

</details>


### [82] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee,Boris Bačić,Maryam Doborjeh*

Main category: cs.CV

TL;DR: 本文提出了SIFT-SNN框架，一种用于运输基础设施结构异常实时检测的低延迟神经形态信号处理管线，在实际数据集上取得了高准确率和低延迟，适合嵌入式低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积神经网络（CNN）的结构异常检测方法在解释性、透明性和低延迟低功耗部署上存在不足，难以满足交通基础设施监控的实时与可靠需求。作者希望设计一种兼具解释性、实时性和在实际嵌入式硬件上高效运行的新型检测系统。

Method: 方法将尺度不变特征变换（SIFT）用于空间特征编码，通过延迟驱动的脉冲转换层接口，将特征输入到LIF脉冲神经网络（SNN）进行分类。在Auckland Harbour Bridge数据集上，包含真实和增强的6000帧标签样本。

Result: 系统在测试中实现了92.3%（±0.8%）的分类准确率，单帧推理时间为9.5毫秒，平均只有8.1%的稀疏脉冲活动，远优于传统方法，在嵌入式设备上实现了实时、低功耗推理。

Conclusion: SIFT-SNN框架兼具空间特征可解释性、决策透明性、实时和低功耗部署优势，适用于可移动混凝土护栏等结构安全监控场景。虽增强数据提升了鲁棒性，但在未知现场环境下的泛化能力仍需进一步验证。

Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.

</details>


### [83] [SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding](https://arxiv.org/abs/2511.21339)
*Tae-Min Choi,Tae Kyeong Jeong,Garam Kim,Jaemin Lee,Yeongyoon Koh,In Cheul Choi,Jae-Ho Chung,Jong Woong Park,Juyoun Park*

Main category: cs.CV

TL;DR: 本文提出了SurgMLLMBench，这是一套专为外科场景多模态大模型设计和评估的统一基准，弥补了现有数据集在像素级分割和统一分类体系等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在医疗和外科领域具有巨大潜力，但现有外科数据集多为VQA形式、分类体系杂乱且缺乏像素级分割信息，导致评估和实际应用受限。

Method: 作者设计了SurgMLLMBench基准，包括新收集的MAVIS数据集，覆盖腹腔镜、机器人辅助手术和微创手术场景。集成了像素级器械分割掩膜和结构化VQA注释，并采用统一分类体系，拓展了传统VQA任务。

Result: 基于SurgMLLMBench的大量实验证明，单一模型可在不同外科领域实现一致表现，并能有效泛化到未见过的数据集。

Conclusion: SurgMLLMBench作为一个公开、统一的多模态外科基准，有助于推动可复现、交互式外科智能推理模型的研究与发展。

Abstract: Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.

</details>


### [84] [PFF-Net: Patch Feature Fitting for Point Cloud Normal Estimation](https://arxiv.org/abs/2511.21365)
*Qing Li,Huifang Feng,Kanle Shi,Yue Gao,Yi Fang,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 本文提出了一种融合多尺度特征的新方法，用于更加鲁棒地估算点云法线，显著提高了不同数据或几何形状下的效果，并减少了模型参数和计算消耗。


<details>
  <summary>Details</summary>
Motivation: 传统点云法线估计方法在选择局部邻域大小方面存在困难，且现有特征提取策略参数较多，难以高效、准确地应对多样化的数据和场景。

Method: 该方法通过多尺度特征融合，结合特征聚合与尺度补偿模块，自适应地从不同尺度的邻域提取和整合信息，拟合最优的几何描述以获得法线。聚合模块逐步聚合不同尺度的特征并收缩邻域，补偿模块增强大尺度特征的可用性和信息相关性。

Result: 大量实验表明，该方法在合成与真实数据集上均实现了更优的法线估计性能，同时大幅降低了网络参数量和运行时间。

Conclusion: 提出的多尺度特征聚合与补偿方法能有效适应不同局部尺度，提升点云法线估计的鲁棒性和效率，具备良好的实际应用前景。

Abstract: Estimating the normal of a point requires constructing a local patch to provide center-surrounding context, but determining the appropriate neighborhood size is difficult when dealing with different data or geometries. Existing methods commonly employ various parameter-heavy strategies to extract a full feature description from the input patch. However, they still have difficulties in accurately and efficiently predicting normals for various point clouds. In this work, we present a new idea of feature extraction for robust normal estimation of point clouds. We use the fusion of multi-scale features from different neighborhood sizes to address the issue of selecting reasonable patch sizes for various data or geometries. We seek to model a patch feature fitting (PFF) based on multi-scale features to approximate the optimal geometric description for normal estimation and implement the approximation process via multi-scale feature aggregation and cross-scale feature compensation. The feature aggregation module progressively aggregates the patch features of different scales to the center of the patch and shrinks the patch size by removing points far from the center. It not only enables the network to precisely capture the structure characteristic in a wide range, but also describes highly detailed geometries. The feature compensation module ensures the reusability of features from earlier layers of large scales and reveals associated information in different patch sizes. Our approximation strategy based on aggregating the features of multiple scales enables the model to achieve scale adaptation of varying local patches and deliver the optimal feature description. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets with fewer network parameters and running time.

</details>


### [85] [Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes](https://arxiv.org/abs/2511.21367)
*Yangle Liu,Fengze Li,Kan Liu,Jieming Ma*

Main category: cs.CV

TL;DR: 本文提出了一种用于内窥镜视频的4D高斯喷溅重建方法（Endo-G$^{2}$T），在动态场景下通过几何引导和时序感知提升重建的准确性和稳定性，并优于现有单目重建方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频常因反射、湿漉漉的表面及遮挡导致视角依赖严重，单纯光度监督与实际几何不符，造成早期几何漂移并难以修正。当前方法在动态场景下准确高效重建仍存难题。

Method: 1）使用几何引导的预训练，将置信度加权的单目深度转化为带尺度不变损失和深度梯度损失的几何监督，并采用渐进式注入先验方式防止过拟合。2）采用带时间嵌入的高斯场，通过旋转参数化建模XYZT中的动态，利用轻量正则实现时序一致和清晰边界。3）提出基于关键帧的流式优化策略，提高效率和长期稳定性。

Result: 在EndoNeRF和StereoMIS-P1两个公开动态内窥镜数据集上，Endo-G$^{2}$T在单目重建任务中实现了最优性能，超越了现有基线。

Conclusion: Endo-G$^{2}$T系统解决了动态内窥镜视频中的几何漂移等问题，在保持效率的同时提升了重建质量，对相关4D单目重建技术具有重要推动作用。

Abstract: Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.

</details>


### [86] [Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2511.21375)
*Xin Gu,Haoji Zhang,Qihang Fan,Jingxuan Niu,Zhipeng Zhang,Libo Zhang,Guang Chen,Fan Chen,Longyin Wen,Sijie Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法STVG-o1，实现了多模态大模型（MLLMs）在视频时空定位（STVG）任务上的新SOTA，显著提升了定位准确率，且无需改变现有大模型架构。


<details>
  <summary>Details</summary>
Motivation: 目前MLLMs在STVG任务上的表现不佳，主要因为训练目标不一致以及视觉编码器无法实现细粒度的文本-区域对齐。因此，作者希望利用现有强大的MLLMs，通过优化训练和推理机制，提升其在STVG任务上的表现。

Method: 提出STVG-o1，利用bounding-box chain-of-thought机制，在最终预测前显式地推理目标在时空中的位置。同时，设计了包含格式、连贯性、时序、空间及思维等多维奖励的强化学习函数，对大模型进行几何感知的强化微调。方法无需修改原有MLLM架构，直接以插件形式插入。

Result: 在HCSTVG-v1/v2及VidSTG数据集上，STVG-o1在HCSTVG-v1刷新SOTA，比原有最佳方法提升7.3%的m_tIoU，并在VidSTG上与专业模型表现持平，大幅超越所有基于MLLM的现有方法。同时展现了优秀的跨数据集泛化能力。

Conclusion: 本文证明，只需合理的方法设计和训练优化，现有MLLMs也能在STVG等细粒度多模态定位任务上达到甚至超越专用模型水平，为后续相关任务的研究和应用提供了强有力的新工具。

Abstract: Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.

</details>


### [87] [Monet: Reasoning in Latent Visual Space Beyond Images and Language](https://arxiv.org/abs/2511.21395)
*Qixun Wang,Yang Shi,Yifei Wang,Yuanxing Zhang,Pengfei Wan,Kun Gai,Xianghua Ying,Yisen Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态大语言模型训练框架Monet，可在潜在视觉空间内进行推理，通过生成连续嵌入作为中间视觉思考步骤，从而提升视觉推理能力。论文解决了潜在视觉推理高计算成本和监督不足问题，并提出了VLPO优化方法和Monet-SFT-125K数据集。模型在多个基准和抽象推理任务上表现优越，并分析了各训练组件的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有“以图像思考”的方法虽然促进了视觉推理，但由于需要依赖外部工具，灵活性有限，难以达到人类抽象视觉思考水平。因此，亟需提升多模态大模型在人类类似的潜在视觉空间中直接推理的能力。

Method: 提出了Monet训练框架，使多模态大模型能在潜在视觉空间通过生成连续嵌入进行推理。采用三阶段的基于蒸馏的有监督微调流程来解决高计算成本和嵌入监督不足的问题。发现传统GRPO方法主要优化文本推理，对潜在推理提升有限，因此创新性提出了VLPO（Visual-latent Policy Optimization）强化学习方法，直接将潜在嵌入纳入策略梯度更新，并构建了包含125K混合类型CoT的Monet-SFT-125K数据集支持训练。

Result: 所提出的Monet-7B模型在实际感知与推理基准上获得了持续提升，且在极具挑战性的抽象视觉推理任务中表现出色的泛化能力。实验证明了各训练组件的有效性，并分析了早期实验中的失败经验。

Conclusion: Monet框架有效提升了多模态模型在抽象视觉推理领域的能力，不仅在已有基准测试中表现优异，还在未见过的复杂领域展现了强泛化。方法和分析为未来潜在视觉推理的研究提供了有价值的参考和数据资源。

Abstract: "Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.

</details>


### [88] [DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models](https://arxiv.org/abs/2511.21415)
*Mingue Park,Prin Phunyaphibarn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiverseVAR的方法，在无需重新训练或大量增加计算开销的情况下，提高文本条件视觉自回归模型（VAR）的生成多样性，从而弥补现有VAR模型生成图片缺乏多样性的短板。


<details>
  <summary>Details</summary>
Motivation: 近年来VAR模型在图像生成领域表现突出，但在多样性方面存在不足，导致同一提示词生成的图片几乎相同。以往研究多注重图像质量，忽视了多样性问题。本文旨在在不影响图像质量的前提下提升VAR模型的生成多样性。

Method: 方法分两阶段：首先借鉴扩散模型的多样性增强技术，在文本嵌入中注入噪声，提升生成多样性；为避免图像质量大幅下降，进一步提出scale-travel的潜空间细化技术，通过多尺度自编码器提取粗粒度特征，使生成过程可在中间阶段继续。两者结合达到提升多样性的同时，限制对图像质量的影响。

Result: 实验表明，将文本嵌入噪声注入与scale-travel细化技术结合能大幅提升VAR生成结果的多样性且损失的图像质量极少，实现了多样性与质量权衡的新最优点。

Conclusion: DiverseVAR方法无需重新训练、微调或高计算成本，即可在提升文本条件VAR模型生成多样性的同时保持图像质量，为VAR模型实用性带来新的提升，并补足了以往关注点的空白。

Abstract: We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.

</details>


### [89] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合SAM基础模型与知识图谱的新方法，用于提升遥感变化描述任务的性能，并在多个数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统的遥感变化描述方法在捕捉变化区域和时序对齐方面存在不足，对区域感知较弱。为解决上述问题，作者希望通过引入SAM基础模型和知识图谱，提高模型对关注区域和变化类型的表达能力。

Method: 方法上，作者使用CNN/Transformer提取全局视觉特征，应用SAM模型分割出发生语义或运动变化的区域，并构建了知识图谱辅助对象知识提取。所有信息通过交叉注意力模块融合，最后由Transformer解码器生成自然语言描述。

Result: 实验显示，该方法在多个主流数据集上均达到了最新的最优性能。

Conclusion: 融合区域级表示、知识图谱和多模态特征，有效提升了遥感变化描述的精度和表现能力，为后续相关研究提供了新思路。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [90] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D拼接方法E-M3RF，融合点云的几何和颜色信息，显著提升了碎片重组的准确度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习3D拼接方法主要依赖几何特征，在几何信息不足或碎片对称、破损严重时表现受限，且难以避免拼接重叠。

Method: E-M3RF框架通过将点云的点位置信息（旋转等变编码）与颜色信息（transformer编码）融合，采用SE(3)流匹配预测碎片组装所需的变换，实现多模态拼接。

Result: 在四个数据集（两个合成、两个真实文化遗产）上测试，E-M3RF在RePAIR数据集上相比其他方法旋转误差降低23.1%、平移误差降低13.2%、Chamfer距离降低18.4%。

Conclusion: E-M3RF有效结合了几何与颜色信息，提升了3D碎片重组的准确率，特别适用于几何信息不足场景，在真实应用中表现出色。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [91] [From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings](https://arxiv.org/abs/2511.21428)
*Jiajie Zhang,Sören Schwertfeger,Alexander Kleiner*

Main category: cs.CV

TL;DR: 提出了一种全自动的无监督方法，用于从工业视频中提取可用于视觉-语言-动作（VLA）模型预训练的、有结构的人类动作数据。


<details>
  <summary>Details</summary>
Motivation: 工业环境下的大量无标注视频数据未被充分利用，限制了VLA模型在制造业领域的应用。本工作希望自动化地利用这些数据，为VLA模型预训练提供丰富的高质量样本。

Method: 该方法首先训练一个轻量级运动分词器对运动动态进行编码，然后基于新提出的“潜在动作能量”指标，实现无监督动作分割，从视频流中自动发现并划分语义上连贯的动作原语。最终输出分割后的视频片段及其潜在动作序列。

Result: 在公开基准和专有电机装配数据集上，有效分割出了关键任务动作，聚类和定量评估显示发现的动作原语在语义上高度一致。

Conclusion: 首次实现了将无结构工业视频自动转化为可用于VLA预训练的有结构数据，为制造业实现具身AI大规模落地提供了可扩展的新方案。

Abstract: We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.

</details>


### [92] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出一种基于超图引导的时空事件流补全机制，有效解决了事件相机由于空间稀疏导致的欠采样问题，并支持多模态信息的补全和融合。


<details>
  <summary>Details</summary>
Motivation: 当前事件相机产生的事件流虽时域密集但空间稀疏，主流表示方法（如帧、体素和张量）难以处理由此引发的欠采样问题，限制了事件表征的效果。

Method: 作者提出利用超图将不同时间和空间位置的事件令牌连接，通过上下文信息传递补全稀疏事件；框架还可灵活将RGB信息作为超图节点，实现多模态特征补全。之后，通过自注意力机制跨时刻聚合节点信息，加强多模态特征融合和学习。

Result: 在单标签和多标签事件分类任务上进行了大量实验，充分验证了所提框架的有效性。

Conclusion: 该方法不仅提升了稀疏事件流的表达和利用效率，还可兼容多模态信息，对下一步多数据源事件视觉任务有重要促进作用。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


### [93] [MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices](https://arxiv.org/abs/2511.21475)
*Shuai Zhang,Bao Tang,Siyuan Yu,Yueting Zhu,Jingfeng Yao,Ya Zou,Shanglin Yuan,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出了MobileI2V，一种面向移动设备、轻量级的图像到视频生成扩散模型，在保证质量的前提下实现了高速720p视频生成。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型虽然在图像到视频生成上取得了突破，但其高计算复杂度和慢速推理使其难以在移动端设备上实现实时、高分辨率应用。亟需开发兼顾效率与质量、适用于资源受限移动端的I2V生成方法。

Method: 1) 分析线性注意力与softmax注意力对移动端性能影响，提出一种高效混合架构去噪器；2) 设计时间步蒸馏策略，将采样步数从20+降至2步，速度提升10倍且无明显质量损失；3) 针对移动端优化注意力机制，实现推理阶段注意力操作2倍加速。整体模型规模仅270M。

Result: 首次实现了移动端720p图像到视频的高速生成，在保证现有模型质量的前提下，单帧生成速度小于100毫秒。在效率和效果上均优于此前同类方案。

Conclusion: MobileI2V模型显著推动了移动端图像到视频技术的发展，在移动设备上首次实现了高质量、实时的视频生成。该技术有望推动视频生成应用的普及和落地。

Abstract: Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.

</details>


### [94] [Frequency-Aware Token Reduction for Efficient Vision Transformer](https://arxiv.org/abs/2511.21477)
*Dong-Jae Lee,Jiwan Hur,Jaehyun Choi,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种频率感知的token压缩策略，有效减少了视觉Transformer的计算开销，同时在缓解rank collapsing和过度平滑问题的情况下保持甚至提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer因自注意力机制对token数量的二次复杂度，计算负担过重，实际部署受限。现有token压缩方法未充分考察自注意力的频率特性，尤其是rank collapsing和过度平滑问题，影响模型表达能力。作者希望在降低计算量的同时，缓解上述问题并保持性能。

Method: 方法将token分为高频和低频两类，高频token选择性保留，低频token通过聚合生成紧凑的直流（direct current）token，用以保留低频分量信息。通过频率维度的划分和处理，降低了原始token数量但保留了关键信息。

Result: 实验证明，该频率感知token压缩方法在降低计算成本的同时，准确率提升明显，并有效缓解了rank collapsing和过度平滑现象。分析也揭示了以往方法在频率特征方面的不足。

Conclusion: 频率感知token压缩不仅提升视觉Transformer的实用性与高效性，还为理解和优化自注意力机制中的频率特性（如rank collapsing等）提供了理论和实践参考。

Abstract: Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.

</details>


### [95] [Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning](https://arxiv.org/abs/2511.21490)
*Taehoon Kim,Donghwan Jang,Bohyung Han*

Main category: cs.CV

TL;DR: 提出了一种名为Merge-and-Bound (M&B)的新型连续学习训练方法，通过模型参数空间的合并和有界更新，有效缓解灾难性遗忘，并提升了CIL性能。


<details>
  <summary>Details</summary>
Motivation: 传统Class Incremental Learning(CIL)方法难以兼顾对新任务的学习与对旧知识的保留，容易出现灾难性遗忘。为此，研究者寻求更优的模型权重优化方法，以提高模型的稳定性和扩展性。

Method: M&B方法包括两种权重合并：任务间权重合并（对历史各阶段模型参数进行平均以整合旧知识）和任务内权重合并（在当前阶段对参数进行合并以促进新任务学习）；此外，提出了有界更新策略，限制参数更新幅度，减少对旧任务知识的干扰。整个方法可无缝集成到现有CIL框架中，无需修改模型结构或损失函数。

Result: 在标准CIL基准上广泛评测，M&B在性能上显著优于现有主流CIL方法。

Conclusion: M&B能有效融合历史知识与新任务能力，显著缓解灾难性遗忘，实现更优的连续学习表现。

Abstract: We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [96] [CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation](https://arxiv.org/abs/2511.21503)
*Shizhe Sun,Wataru Ohyama*

Main category: cs.CV

TL;DR: 本文提出了基于跨注意力机制的非局部知识蒸馏（CanKD）方法，通过让学生特征图中的每个像素动态考虑教师特征图中所有像素，实现更充分的像素关系学习，从而提升知识迁移和模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的知识蒸馏方法通常仅对齐教师与学生各自的特征图，未能充分捕捉两者间深层次的像素间关系，限制了蒸馏效能。因此，作者希望提出一种方法，能够让学生像素全局感知教师特征，提升知识蒸馏效果。

Method: 作者提出CanKD方法，基于跨注意力机制，让学生特征图的每个像素能动态关注教师特征图中所有像素，实现非局部的知识传递。该方法仅需引入一个新的损失函数即可与现有模型集成。

Result: 在目标检测与图像分割等任务上的实验表明，CanKD优于现有最先进的特征蒸馏及混合蒸馏方法，取得了更好的性能。

Conclusion: CanKD为注意力引导的知识蒸馏提供了新范式，在计算机视觉任务中表现出显著优势，有广泛的应用前景。

Abstract: We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD

</details>


### [97] [Generalized Design Choices for Deepfake Detectors](https://arxiv.org/abs/2511.21507)
*Lorenzo Pellegrini,Serafino Pandolfini,Davide Maltoni,Matteo Ferrara,Marco Prati,Marco Ramilli*

Main category: cs.CV

TL;DR: 本文系统性分析了影响深度伪造检测方法性能的各种实现细节，强调最佳实践的重要性，并提出了提升检测表现的设计建议。


<details>
  <summary>Details</summary>
Motivation: 当前多数深度伪造检测方法的实际效果，与核心模型设计的关系不如与数据预处理、增强策略及优化技术等实现细节密切，但这些因素却难以量化和公平比较。因此，明确这些细节对检测器性能的影响，对于推动领域进步和制定统一标准至关重要。

Method: 文章通过系统实验，独立考察训练、推理、增量更新等过程中的不同设计选择对模型准确率和泛化能力的影响，逐步剥离各因素作用，以找出真正提升性能的方法。

Result: 实验结果识别出一批可持续提升检测性能的设计细节，并在AI-GenBench基准测试上实现了SOTA（最优表现）。

Conclusion: 深度伪造检测模型的性能提升依赖于合理的设计选择，文章提出的架构无关型“最佳实践”为未来检测系统的开发提供了可靠指导。

Abstract: The effectiveness of deepfake detection methods often depends less on their core design and more on implementation details such as data preprocessing, augmentation strategies, and optimization techniques. These factors make it difficult to fairly compare detectors and to understand which factors truly contribute to their performance. To address this, we systematically investigate how different design choices influence the accuracy and generalization capabilities of deepfake detection models, focusing on aspects related to training, inference, and incremental updates. By isolating the impact of individual factors, we aim to establish robust, architecture-agnostic best practices for the design and development of future deepfake detection systems. Our experiments identify a set of design choices that consistently improve deepfake detection and enable state-of-the-art performance on the AI-GenBench benchmark.

</details>


### [98] [Self-Paced Learning for Images of Antinuclear Antibodies](https://arxiv.org/abs/2511.21519)
*Yiyang Jiang,Guangwu Qian,Jiaxin Wu,Qi Huang,Qing Li,Yongkang Wu,Xiao-Yong Wei*

Main category: cs.CV

TL;DR: 本论文提出了一种新的抗核抗体（ANA）检测框架，可自动处理复杂的多实例多标签（MIML）任务，直接利用原始显微镜图像实现端到端优化效果，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的ANA手工检测效率低且对操作人员要求高，且ANA的多样性导致机器自动检测面临多实例多标签的复杂挑战，现有方法不足以应对真实临床场景。

Method: 提出了一套新的MIML框架，利用实例采样器、概率伪标签分发器和自适应权重学习速率三大模块。该方法仿真人类标注过程，无需手工图像预处理，能够对显微镜原始图片中不同区域进行置信度建模和自适应标签分派，并利用“自适应学习进度”进行模型训练。

Result: 在自建ANA数据集和公开三大医学MIML基准数据集上，模型在F1-Macro、mAP、Hamming loss、one-error等重要指标上大幅优于先前最优方法，部分指标提升达7.0%、12.6%、18.2%和26.9%。

Conclusion: 该框架能有效自动处理ANA多标签检测问题，性能显著优于现有方法，为真实临床应用提供了新方向。代码已开源，有望推动医学图像分析自动化发展。

Abstract: Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.

</details>


### [99] [EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?](https://arxiv.org/abs/2511.21523)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的新方法，用于构建遥感基础模型，通过专家集成框架来提升模型的可扩展性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型领域在提升模型通用性和减少监督需求方面取得进展，但过度依赖大规模模型和数据集，导致极高的计算资源和碳排放，限制社区普适性和可持续性。遥感领域急需更高效、环保的基础模型研发方法。

Method: 提出了Ensemble-of-Specialists框架，将训练流程拆解为多个轻量级、任务专用的ConvNeXtV2专家模块，这些模块可以冻结重用，实现模块化管理。此外，该方法支持联合训练、剪枝及后续专家灵活集成。

Result: 该框架在效率、可解释性和可扩展性方面表现出显著优势，适用于合作和资源有限的场景，在构建遥感基础模型方面指明了新方向。

Conclusion: 论文提出的方法为遥感基础模型的可持续和高效开发提供了新范式，突破了大规模一体化模型的资源瓶颈，具有广泛的适用性和行业推广前景。

Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.

</details>


### [100] [The Age-specific Alzheimer 's Disease Prediction with Characteristic Constraints in Nonuniform Time Span](https://arxiv.org/abs/2511.21530)
*Xin Hong,Kaifeng Huang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合定量指标和年龄因素的序列化MRI图像生成方法，以更好预测阿尔茨海默病的进展阶段，并通过结构相似性指数验证了其合成图像的准确性。


<details>
  <summary>Details</summary>
Motivation: 及时发现阿尔茨海默病对于制定个体化治疗策略至关重要，然而利用不同时间间隔获取的序列影像进行有效病情预测存在挑战，尤其在合成能准确反映病变特征的图像方面。

Method: 研究提出了一种新颖的顺序化图像生成方法，利用定量指标指导图像生成过程，并引入年龄缩放因子以生成特定年龄段的MRI图像，同时采用基于像素的损失函数提升迭代生成效果。

Result: 消融实验显示，加入定量指标后，合成MRI图像的准确性明显提升，加入年龄缩放像素损失也进一步优化了生成图像质量。最终，合成图像的结构相似性指数（SSIM）达到了最高0.882。

Conclusion: 结合定量指标与年龄缩放的序列图像生成方法能有效提升阿尔茨海默病影像合成的准确性，为疾病分期预测和个性化治疗提供了有力工具。

Abstract: Alzheimer's disease is a debilitating disorder marked by a decline in cognitive function. Timely identification of the disease is essential for the development of personalized treatment strategies that aim to mitigate its progression. The application of generated images for the prediction of Alzheimer's disease poses challenges, particularly in accurately representing the disease's characteristics when input sequences are captured at irregular time intervals. This study presents an innovative methodology for sequential image generation, guided by quantitative metrics, to maintain the essential features indicative of disease progression. Furthermore, an age-scaling factor is integrated into the process to produce age-specific MRI images, facilitating the prediction of advanced stages of the disease. The results obtained from the ablation study suggest that the inclusion of quantitative metrics significantly improves the accuracy of MRI image synthesis. Furthermore, the application of age-scaled pixel loss contributed to the enhanced iterative generation of MRI images. In terms of long-term disease prognosis, the Structural Similarity Index reached a peak value of 0.882, indicating a substantial degree of similarity in the synthesized images.

</details>


### [101] [Video Generation Models Are Good Latent Reward Models](https://arxiv.org/abs/2511.21541)
*Xiaoyue Mi,Wenqing Yu,Jiesong Lian,Shibo Jie,Ruizhe Zhong,Zijun Liu,Guozhen Zhang,Zixiang Zhou,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的奖励反馈学习方法（PRFL），可在视频生成的潜在空间中进行高效优化，显著减少内存和训练成本，并提升与人类偏好的对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励反馈学习（ReFL）的方法已在图像生成中取得成效，但应用于视频生成时遇到显著瓶颈：因为视频奖励模型依赖于像素空间输入，导致内存开销大、训练时间长，以及优化仅在去噪末尾阶段，对关键的运动和结构缺乏早期监督。为克服这些问题，作者希望能在视频生成的潜在空间中高效完成偏好优化。

Method: 作者提出PRFL框架，在无需VAE解码至像素空间的情况下，直接在潜在空间完成人类偏好优化。利用预训练视频生成模型在任意时刻处理带噪潜在表示且保留时序信息的优势，实现全去噪链路的高效梯度回传。

Result: 实验表明，PRFL在减少内存消耗与训练时间的同时，较传统RGB空间ReFL能更好地提升模型输出与人类偏好的一致性。

Conclusion: PRFL不仅显著提升了视频生成模型与人类偏好的对齐能力，也大幅优化了训练效率，为奖励反馈学习在视频生成领域的应用提供了新的途径。

Abstract: Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.

</details>


### [102] [UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes](https://arxiv.org/abs/2511.21565)
*Kang Du,Xue Liao,Junpeng Xia,Chaozheng Guo,Yi Gu,Yirui Guan,Duotun Wang,ShengHuang,Zeyu Wang*

Main category: cs.CV

TL;DR: 该论文提出了UAVLight数据集，用于解决多视角3D重建中因光照变化带来的不一致问题，旨在推动光照鲁棒型3D重建技术的发展。


<details>
  <summary>Details</summary>
Motivation: 多视角3D重建受限于输入图像的光照一致性，实际无人机采集数据中由于光照方向、云层和阴影变化导致重构结果出现几何漂移、颜色不一致和阴影印记等问题。现有数据集要么忽略了光照多样性，要么因时间跨度过大引入了其他不可控变量，无法专门研究光照鲁棒性。

Method: 作者建立了UAVLight数据集，通过在固定的场景和一致的几何、校准条件下，沿可重复的航线在一天内不同固定时刻采集数据，实现了自然光照变化但场景保持不变。还制定了光照条件下的标准评测协议。

Result: UAVLight数据集弥补了现有数据在光照多样性与几何保持一致性的兼顾缺口。为真实室外环境下、具备良好一致性和可重光照的3D重建方法的研发与评测提供了标准化平台。

Conclusion: 该数据集为研究和提升3D重建在实际光照变化场景下的鲁棒性打下了基础，将推动室外真实场景的高质量重建算法和相关技术的发展。

Abstract: Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.

</details>


### [103] [Multimodal Robust Prompt Distillation for 3D Point Cloud Models](https://arxiv.org/abs/2511.21574)
*Xiang Gu,Liming Lu,Xu Zheng,Anan Du,Yongbin Zhou,Shuchao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的教师-学生框架MRPD，用于增强3D点云模型的抗对抗攻击能力。该方法在训练阶段结合多模态知识蒸馏，无需增加推理成本，显著提升了防御多种攻击的鲁棒性和原始性能。


<details>
  <summary>Details</summary>
Motivation: 面对对抗性攻击对3D点云模型带来的威胁，现有防御方法存在计算开销大或泛化能力差的问题。因此，作者希望提出一种高效、具备强泛化能力的新方法来提升3D点云模型的鲁棒性。

Method: 作者设计了一个多模态鲁棒提示蒸馏（MRPD）框架。具体来说，学生模型通过学习与三个不同教师（图像视觉模型、强3D模型、文本编码器）对齐的特征提示，通过置信度门控机制动态平衡多模态信息的贡献。此外，所有蒸馏过程均在训练阶段完成，推理时不引入额外开销。

Result: 大量实验显示，MRPD在对抗白盒和黑盒攻击时，比现有最先进防御方法具有更优表现，且在无攻击情况下模型的基础性能也得以提升。

Conclusion: 该方法为构建高鲁棒性的3D视觉系统提供了一种实用有效的新范式，充分利用了多模态知识且高效无推理开销。

Abstract: Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.

</details>


### [104] [Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss](https://arxiv.org/abs/2511.21575)
*Chou Mo,Yehyun Suh,J. Ryan Martin,Daniel Moyer*

Main category: cs.CV

TL;DR: 本文提出了一种结合2D/3D标志点配准的新型U-Net模型训练方法，用于提升骨盆荧光透视下自动标志点检测算法在不同体位或成像姿态下的鲁棒性，并通过对比实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的骨盆标志点自动检测方法多假设成像角度固定，但实际手术过程中成像视角常有变化，现有方法对变化的鲁棒性不足。

Method: 提出在U-Net标志点检测模型训练中引入2D/3D配准及姿势估计损失（Pose Estimation Loss），并通过基础U-Net、加姿势估计损失训练的U-Net以及微调后的U-Net模型对比，在模拟手术中不同体位下评估检测准确性。

Result: 引入姿势估计损失的U-Net模型在病人姿态变化条件下表现出更高的标志点检测精度，优于仅用常规损失训练的U-Net。

Conclusion: 将2D/3D标志点配准及姿势损失引入训练显著提升了模型对实际复杂临床场景下姿态变化的适应能力，提高了自动标志点检测的临床实用性。

Abstract: Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.

</details>


### [105] [Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy](https://arxiv.org/abs/2511.21579)
*Teng Hu,Zhentao Yu,Guozhen Zhang,Zihan Su,Zhengguang Zhou,Youliang Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: 该论文提出了Harmony框架，有效解决音视频合成中同步对齐难题，在生成质量与细粒度同步性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前开源的音视频合成模型普遍存在对齐不稳、全局注意力机制效率低以及现有无分类器引导（CFG）缺乏跨模态同步的不足，难以实现高质量音视频同步生成。

Method: 1）提出Cross-Task Synergy训练范式，通过音频驱动视频和视频驱动音频这两种任务的监督信号，减少对应偏移问题；2）设计Global-Local Decoupled Interaction Module，实现高效且精细的时序风格对齐；3）提出SyncCFG，在推理时显式隔离并增强音视频对齐信号。

Result: 大量实验证明，Harmony在生成保真度与音视频同步性两个方面均显著优于现有方法，达到了新的最先进水平。

Conclusion: Harmony机制性地强化了音视频同步，解决了协同扩散生成中的三大核心挑战，不仅生成质量更高，而且实现了细粒度的视听同步。

Abstract: The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.

</details>


### [106] [Deep Learning-Based Multiclass Classification of Oral Lesions with Stratified Augmentation](https://arxiv.org/abs/2511.21582)
*Joy Naoum,Revana Salama,Ali Hamdi*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多类别口腔病变分类模型，利用数据增强与过采样技术有效缓解数据不平衡问题，并在16种口腔病变上取得了较高准确率和精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 口腔癌常见且由于其病变与良性及癌前病变外观相似，早期难以诊断。现有辅助诊断系统尚存在数据不均衡、类别数量多等挑战，因此开发高效的多类别分类模型对于临床早期诊断非常迫切。

Method: 作者利用深度学习构建一个针对16类口腔病变的多类别分类器，采用分层数据切分、先进的数据增强及过采样手段，有效解决数据量有限和类别不均衡的问题。

Result: 实验结果显示，该模型在准确率、精度和召回率上分别达83.33%、89.12%、77.31%，整体优于同期主流方法，尤其在少数类别的识别性能上表现突出。

Conclusion: 本文提出的框架在数据不均衡的环境下展现出良好的分类能力，为临床口腔癌早期计算机辅助诊断系统的研究和应用提供了有力依据，具有实际落地的前景和价值。

Abstract: Oral cancer is highly common across the globe and is mostly diagnosed during the later stages due to the close visual similarity to benign, precancerous, and malignant lesions in the oral cavity. Implementing computer aided diagnosis systems early on has the potential to greatly improve clinical outcomes. This research intends to use deep learning to build a multiclass classifier for sixteen different oral lesions. To overcome the challenges of limited and imbalanced datasets, the proposed technique combines stratified data splitting and advanced data augmentation and oversampling to perform the classification. The experimental results, which achieved 83.33 percent accuracy, 89.12 percent precision, and 77.31 percent recall, demonstrate the superiority of the suggested model over state of the art methods now in use. The suggested model effectively conveys the effectiveness of oversampling and augmentation strategies in situations where the minority class classification performance is noteworthy. As a first step toward trustworthy computer aided diagnostic systems for the early detection of oral cancer in clinical settings, the suggested framework shows promise.

</details>


### [107] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN是一种提升视频生成运动质量的新方法，通过运动判别器和分布正则化增强时序一致性，无需奖励模型或人工偏好数据，在多个基准上实现了运动质量的大幅提升且无损视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型尽管在单帧视觉保真度上表现优异，但在运动连贯性和自然动态方面表现不佳，出现抖动、重影或不合理运动等问题。根本原因在于传统的去噪MSE目标未对时序一致性提供直接监督。

Method: 提出MoGAN，一种运动感知后训练框架。以蒸馏的视频扩散模型为基础，通过训练DiT结构的光流判别器区分真实与生成的运动，并结合分布匹配正则项保持视觉保真，无需人工偏好或奖励模型。

Result: MoGAN在Wan2.1-T2V-1.3B实验中显著提升了运动质量：在VBench上比teacher提升7.3%，比DMD模型提升13.3%；在VideoJAM-Bench上分别提升7.4%和8.8%。同时美学和图像质量分数有可比甚至更好表现。人类评测中，MoGAN运动质量优选比例大幅高于对比模型。

Conclusion: MoGAN能够在不损失视觉保真度或效率的前提下，显著提升视频生成模型的运动真实感，为高质量快视频生成提供了实际可行的新路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [108] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 作者提出了一种自驱动、点监督的框架，使通用分割模型SAM能更好地适应遥感影像，只需稀疏点注释即可显著提高分割性能。


<details>
  <summary>Details</summary>
Motivation: 尽管通用分割模型SAM在自然图像上表现优异，但由于领域差距和缺乏密集标注，在遥感影像上的表现较差。为解决这一问题，亟需一种能在无完整掩膜标注的情况下，使SAM适应遥感领域的新方法。

Method: 作者提出了Refine-Requery-Reinforce自驱动循环框架：首先用点标注生成伪掩膜（Refine），然后通过自构造的框提示进行改进（Requery），再对特征进行多轮对齐以减少确认偏差（Reinforce）；全程不依赖完整掩膜标注，实现模型的渐进式自适应。

Result: 在三个人工遥感数据集（WHU、HRSID、NWPU VHR-10）上，所提方法在分割效果上优于原始SAM及最新点监督分割方法。

Conclusion: 依靠自驱动提示和语义对齐机制，能高效地实现SAM等基础分割模型在遥感领域的点级自适应，对促进遥感影像分割具重要意义。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [109] [Active Learning for GCN-based Action Recognition](https://arxiv.org/abs/2511.21625)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 本文针对骨架动作识别中的图卷积网络（GCN）对大量标签数据的依赖问题，提出了一种新颖的高效利用少量标签的GCN方法，并在主流数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法在骨架动作识别中表现优秀，但高度依赖大规模有标签数据，现实中标注数据稀缺，限制了模型应用。

Method: 提出一种对代表性、多样性和不确定性均衡的新型样本获取函数，结合对抗策略选取少量有用样本标注；并设计了双向且稳定的GCN架构，实现数据空间与潜在空间的更有效映射。

Result: 在两个有挑战性的骨架动作识别基准数据集上，所提少标签高效GCN方法显著优于之前的工作。

Conclusion: 提出的方法能在有限标注数据条件下，提升GCN模型在骨架动作识别任务中的表现，具备实际应用价值。

Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.

</details>


### [110] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是Qwen系列最新的视觉-语言模型，在多模态任务中表现出色，原生支持长达256K token的多模态输入，并在多项基准测试中取得领先。


<details>
  <summary>Details</summary>
Motivation: 随着多模态应用的普及，现有模型在跨文本、图像、视频的理解和推理方面存在局限，尤其是在长上下文处理和高效推理能力方面的需求日益增长。Qwen3-VL旨在突破这些限制，提升模型处理多模态长文本与复杂推理的能力。

Method: Qwen3-VL包括多种规模和架构的模型（Dense与MoE），支持高达256K tokens的原生交错多模态输入。架构创新包括：改进的interleaved-MRoPE以增强时空建模能力，引入DeepStack融合多层ViT特征强化视觉-语言对齐，以及基于文本的时间对齐精确视频时序定位。

Result: Qwen3-VL在文本理解、长上下文保持与检索、多模态推理等方面显著超越同类模型，在MMMU、MathVista、MathVision等评测中成绩优异。Dense与MoE架构均展现出更佳表现，且在同等token与延迟下具更高效能。

Conclusion: Qwen3-VL不仅加强了多模态模型对长文本和复杂推理的能力，也可作为图像引导推理、智能决策和多模态代码智能的基础引擎，拓宽其在现实多模态工作流中的应用前景。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [111] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 本文提出了一种适用于资源受限设备的AI模型错误快速纠正系统，利用少量样本即可高效修正模型误判，且计算和存储消耗极低。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在日常设备中的广泛应用带来了预测错误问题，而现有方法多关注检测错误，缺乏高效且轻量的纠正机制，尤其不适合硬件资源有限的设备。

Method: 作者提出一种创新系统：利用服务器端基础模型训练和知识蒸馏，生成适用于设备的高效模型；设备端通过基于原型的分类方法和原型自适应实现错误快速修正，无需模型重训。

Result: 在Food-101与Flowers-102数据集上的图像分类和目标检测任务中，一次性场景下（one-shot）纠正率超过50%，遗忘率低于0.02%，几乎不增加额外计算负担，并在安卓端进行实际应用验证。

Conclusion: 该方案在资源受限设备上可行且高效，实现了低资源高准确度的模型误判修正，为实际应用AI模型提供了新的解决思路。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [112] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: 本文提出一种新的动作质量评估方法CaFlow，通过结合因果反事实去混淆和双向时序建模，有效提升了长时序动作评分的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 长时序动作质量评估（如花样滑冰、艺术体操）要求模型能够处理复杂、持续的动作变化，同时排除环境干扰。现有方法对标注依赖大或仅用单向时序建模，容易受到假相关影响，难以稳定学习长期动作动态。

Method: 提出CaFlow框架，包含因果反事实正则化（CCR）模块和双向时序流（BiT-Flow）模块。CCR模块通过自监督方式将因果特征和混淆特征解耦，并利用反事实干预增强模型的因果鲁棒性。BiT-Flow模块则采用双向（正、反）时序建模，并加入循环一致性约束得到平滑、连贯的特征表示。

Result: 在多个长时序动作质量评估基准数据集上，CaFlow都取得了目前最优的性能，显著优于现有方法。

Conclusion: CaFlow框架能够有效建模长时序动作动态并抵御环境混淆，实现更为鲁棒和准确的动作评分。其自监督设计与因果建模思路可为后续相关研究提供新范式。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [113] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: 本文提出了Multi-Crit基准，用于系统评测大型多模态模型（LMMs）在遵循多样评价标准和可靠输出细粒度判定上的能力，发现主流模型在多标准一致性、灵活切换和冲突偏好识别等方面尚存明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前多模态评测中普遍采用LMMs作为‘评判者’，但它们能否严格遵循多维、细粒度的评价标准尚未被深入检验。为支撑多样化、可控的AI评价体系，有必要开发针对LMMs判别能力的权威评测工具。

Method: 构建了Multi-Crit基准，涵盖开放式生成和可验证推理任务，通过多标准人工标注严选难度较大的模型输出对；设计三项新指标，分别评估模型对多元标准的遵从度、切换灵活性及识别标准冲突的能力，并对25个主流闭源及开源LMMs进行系统测评。

Result: 分析发现：1）闭源模型对多元标准的遵循仍存在一致性不足，尤其在开放式任务上；2）开源模型在灵活遵循不同标准上表现更差；3）针对总体判别信号的判官微调虽能提升视觉基础，但难以泛化到多标准细粒度判别。此外，还探讨了因推理微调、测试规模等因素带来的性能界限。

Conclusion: Multi-Crit为系统性评测LMMs多标准判别能力奠定了基础，并揭示当前主流模型在构建可靠、可控多模态AI评测系统上的显著挑战。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [114] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ADVLA的新型对抗攻击方法，只需在视觉特征投影到文本空间后加扰，既高效又低扰动，能极大降低VLA模型的下游动作预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型（VLA）的对抗攻击方法成本高且容易被察觉，难以在实际中实施高效且隐蔽的攻击。需要一种更高效、低可见度的新型对抗攻击方案。

Method: 作者提出ADVLA框架，直接在视觉编码器投影到文本特征空间后的特征上施加对抗扰动，并通过关注引导让扰动集中与稀疏，同时设计三种策略增强敏感性、强化稀疏性和集中扰动。

Result: 实验显示，在L_inf=4/255约束下，ADVLA结合Top-K masking仅修改不足10%的patch即可实现近100%的攻击成功率，扰动集中于关键区域且几乎不可见，单步迭代只需约0.06秒，远优于传统patch攻击。

Conclusion: ADVLA可在低幅值、稀疏条件下有效削弱VLA模型的动作预测性能，规避高昂训练成本和明显扰动，具有独特的攻击特性和实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [115] [Revolutionizing Glioma Segmentation & Grading Using 3D MRI - Guided Hybrid Deep Learning Models](https://arxiv.org/abs/2511.21673)
*Pandiyaraju V,Sreya Mynampati,Abishek Karthik,Poovarasan L,D. Saraswathi*

Main category: cs.CV

TL;DR: 本研究提出了一种结合U-Net分割和DenseNet-VGG分类网络、引入多头和空间-通道注意力机制的混合深度学习模型，用于3D MRI胶质瘤的自动分割与分级诊断，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤是一种具有高死亡率的脑肿瘤，早期且准确的诊断对治疗至关重要。传统方法在准确率和效率上存在不足，因此研究动机是提升基于MRI图像的自动诊断效率和准确性。

Method: 提出了一种混合深度学习框架：使用U-Net做3D MRI肿瘤精准分割，DenseNet和VGG结合注意力机制进行分类，模型前置数据进行归一化、重采样和数据增强。分类和分割均评估多项指标（Dice、IoU、准确率等）。

Result: 分割模型Dice系数达98%，分类准确率达99%，均优于传统CNN与无注意力机制方法。多头注意力机制提升了对关键肿瘤区域的关注、模型可解释性与分析精度。

Conclusion: 该混合模型有望为临床医师提供更高效、可靠的胶质瘤定位、诊断和分级工具，从而改进患者治疗方案的制定。

Abstract: Gliomas are brain tumor types that have a high mortality rate which means early and accurate diagnosis is important for therapeutic intervention for the tumors. To address this difficulty, the proposed research will develop a hybrid deep learning model which integrates U-Net based segmentation and a hybrid DenseNet-VGG classification network with multihead attention and spatial-channel attention capabilities. The segmentation model will precisely demarcate the tumors in a 3D volume of MRI data guided by spatial and contextual information. The classification network which combines a branch of both DenseNet and VGG, will incorporate the demarcated tumor on which features with attention mechanisms would be focused on clinically relevant features. High-dimensional 3D MRI data could successfully be utilized in the model through preprocessing steps which are normalization, resampling, and data augmentation. Through a variety of measures the framework is evaluated: measures of performance in segmentation are Dice coefficient and Mean Intersection over Union (IoU) and measures of performance in classification are accuracy precision, recall, and F1-score. The hybrid framework that has been proposed has demonstrated through physical testing that it has the capability of obtaining a Dice coefficient of 98% in tumor segmentation, and 99% on classification accuracy, outperforming traditional CNN models and attention-free methods. Utilizing multi-head attention mechanisms enhances notions of priority in aspects of the tumor that are clinically significant, and enhances interpretability and accuracy. The results suggest a great potential of the framework in facilitating the timely and reliable diagnosis and grading of glioma by clinicians is promising, allowing for better planning of patient treatment.

</details>


### [116] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本论文探讨仅凭摄像机轨迹（不依赖图像像素）来感知视频内容的可行性，提出并验证了CamFormer框架的有效性，显示了摄像机运动能揭示视频内容并具有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统视频内容理解依赖像素数据，但摄像机运动轨迹是否也能揭示内容尚未系统研究。作者旨在探索轨迹信息对视频理解的潜力，并寻求更轻量、鲁棒和多样的内容感知方式。

Method: 提出对比学习框架CamFormer，将摄像机轨迹编码为向量，并与自然语言描述对齐。通过训练该编码器，评估其在多种下游任务中的表现，并测试其对不同姿态估算法的鲁棒性。

Result: 实验表明，即使不依赖像素，仅凭摄像机轨迹也能有效识别视频内容。CamFormer嵌入在跨模态对齐、分类和时序分析等任务中表现出色，并且对各种姿态估算方法展现了鲁棒性。

Conclusion: 摄像机轨迹作为一种轻量、稳健且多样的模态渠道，可有效感知视频内容。CamFormer框架证实了摄像机运动信息的实际应用价值。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [117] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出了Canvas-to-Image框架，将多种控制需求（如文本描述、主体参照、空间布局、姿态约束等）统一到一个多模态画布输入，实现更高保真度的图像生成，对多控制任务具有更强表达力与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在高质量、多样性图像生成方面表现出色，但面对需要同时执行文本、参照、空间、姿态、布局等多种控制输入时，现有方法难以准确反映用户意图，控制力有限。

Method: 提出Canvas-to-Image统一框架，将各种异构控制信号编码为单一画布图像输入，由模型直接学习综合视觉-空间推理。针对多任务开发数据集，并设计多任务画布训练策略，让扩散模型在统一范式下共同理解和融合多种控制。

Result: 实验证明，Canvas-to-Image在身份保持和控制遵循方面，尤其在多人物合成、姿态控制、布局约束以及多控制场景下，显著优于现有先进方法。

Conclusion: Canvas-to-Image框架大幅提升了多模态、多控制输入下的图像生成能力，通过统一画布输入与联合多任务训练，实现了对复杂用户意图的高忠实还原。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [118] [Democratizing LLM Efficiency: From Hyperscale Optimizations to Universal Deployability](https://arxiv.org/abs/2511.20662)
*Hen-Hsen Huang*

Main category: cs.CL

TL;DR: 当前最有效的大模型提效方法（MoE、推测解码、复杂的检索增强生成）主要服务于大型科技公司，对资源有限的组织不适用。作者提出，后续研究应关注在有限资源和专业能力下仍具高效性和鲁棒性的简单方法，从而普惠AI技术。


<details>
  <summary>Details</summary>
Motivation: 现有高效技术对基础设施和专业能力要求高，导致大多组织无法受益。当前的效率提升手段加剧了技术不平等和碳排放问题。

Method: 提出新的研究方向，包括：（1）在无需重新训练的情况下对已有模型进行架构替换以提升效率；（2）发明轻量级微调方法以保持模型对齐和实用性；（3）优化长推理链下的经济性；（4）提供无需复杂RAG流程的动态知识管理方案；（5）提出“开销感知效率”(OAE)作为新标准，全面衡量效率。

Result: 提出了一套面向小型组织、低资源场景的效率提升策略和衡量标准，但未给出具体实验结果。

Conclusion: 如果以便于采用、可持续性和公平性重新定义效率，可以让更多普通组织受益于大模型，减少数字鸿沟和碳排放，实现AI民主化。

Abstract: Large language models (LLMs) have become indispensable, but the most celebrated efficiency methods -- mixture-of-experts (MoE), speculative decoding, and complex retrieval-augmented generation (RAG) -- were built for hyperscale providers with vast infrastructure and elite teams. Outside that context, their benefits collapse into overhead, fragility, and wasted carbon. The result is that a handful of Big Tech companies benefit, while thousands of hospitals, schools, governments, and enterprises are left without viable options. We argue that the next frontier is not greater sophistication at scale, but robust simplicity: efficiency that thrives under modest resources and minimal expertise. We propose a new research agenda: retrofitting pretrained models with more efficient architectures without retraining, inventing lightweight fine-tuning that preserves alignment, making reasoning economical despite long chains of thought, enabling dynamic knowledge management without heavy RAG pipelines, and adopting Overhead-Aware Efficiency (OAE) as a standard benchmark. By redefining efficiency to include adoption cost, sustainability, and fairness, we can democratize LLM deployment -- ensuring that optimization reduces inequality and carbon waste rather than amplifying them.

</details>


### [119] [Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology](https://arxiv.org/abs/2511.20665)
*Tcharlies Schmitz*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练、词表或随机参数的新型文本嵌入框架HTP，通过哈希性几何映射实现高效、可逆、可解释的文本表征，并在语义相似度任务上取得稳定表现。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入方法多基于神经网络，需要大量数据和计算，同时结果难以解释，并且往往不可逆。作者希望提出一种既高效、可逆又易于理解的新型文本嵌入方式。

Method: 提出Harmonic Token Projection (HTP) 框架，将每个文本token的Unicode整数按哈希方式映射为解析的谐波轨迹，实现离散符号与连续空间间的双射，完全无需训练。嵌入可逆且结构保持一致，可通过几何对齐测算语义相似度。

Result: 在STS-B及其多语言版本上，HTP在英语里的Spearman相关系数达到0.68，对十种语言表现稳定，计算效率极高，每句对延迟小于1毫秒。

Conclusion: HTP证明了确定性几何方法即可捕捉文本语义关系，结合高效性、可解释性和广泛适用性，是数据驱动方法的有力补充甚至替代。

Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.

</details>


### [120] [A centroid based framework for text classification in itsm environments](https://arxiv.org/abs/2511.20667)
*Hossein Mohanna,Ali Ait-Bachir*

Main category: cs.CL

TL;DR: 本文提出了一种面向层次化分类体系的双嵌入质心分类框架，在ITSM支持工单分类任务中取得了与传统SVM模型相当的准确率，但在训练速度和增量更新方面有显著提升，并具备良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: IT服务管理（ITSM）系统中，支持票据（工单）需要根据复杂的层次结构进行自动分类。现有分类方法面临准确率、速度和可解释性之间的权衡，实际业务场景强烈需求更高效且可解释的方法。

Method: 作者提出了一种双嵌入（语义和词表）质心分类框架：每个类别分别维护语义和词汇质心，并在推断阶段通过倒排融合方法（reciprocal rank fusion）进行结合，实现高效、可解释的分类。

Result: 在8968份ITSM工单、123个类别的实验中，新方法实现了与SVM相当的层次化F1分数（0.731 vs 0.727）；训练速度提升5.9倍，增量更新快152倍；去除嵌入计算后，在批次100-1000时，整体速度提升8.6-8.8倍。

Conclusion: 提出的方法在保持准确性和可解释性的前提下，大幅提升了实际运行效率，非常适合注重可解释性和运维效率的ITSM生产环境部署。

Abstract: Text classification with hierarchical taxonomies is a fundamental requirement in IT Service Management (ITSM) systems, where support tickets must be categorized into tree-structured taxonomies. We present a dual-embedding centroid-based classification framework that maintains separate semantic and lexical centroid representations per category, combining them through reciprocal rank fusion at inference time. The framework achieves performance competitive with Support Vector Machines (hierarchical F1: 0.731 vs 0.727) while providing interpretability through centroid representations. Evaluated on 8,968 ITSM tickets across 123 categories, this method achieves 5.9 times faster training and up to 152 times faster incremental updates. With 8.6-8.8 times speedup across batch sizes (100-1000 samples) when excluding embedding computation. These results make the method suitable for production ITSM environments prioritizing interpretability and operational efficiency.

</details>


### [121] [PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation](https://arxiv.org/abs/2511.20668)
*Yongfu Xue*

Main category: cs.CL

TL;DR: 该论文提出了一种新的训练范式PIRA，用于提升奖励模型（Reward Model）的数据效率和健壮性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型主要有两个问题：1）习惯性地将问答对直接拼接作为输入，导致数据效率低；2）奖励模型容易出现奖励过度优化现象，降低模型表现。

Method: 作者提出PIRA范式，包括三项关键策略：（1）将问答对重构为基于偏好的指令，明确任务；（2）聚合来自多种偏好任务的奖励，降低偏差、增强健壮性；（3）在不同dropout率下对value-head输出进行均值，提升奖励的稳定性。

Result: 通过大量实验证实，PIRA在数据效率、泛化能力和奖励的稳定性等方面均优于传统做法。

Conclusion: PIRA有效提高了奖励模型的表现，并缓解了原有的数据效率低和奖励过优化等问题。

Abstract: Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.

</details>


### [122] [Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data](https://arxiv.org/abs/2511.20669)
*Mann Khatri,Mirza Yusuf,Rajiv Ratn Shah,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）在法律任务上的表现，发现通过组织法律文档结构和定义专业术语能显著提升模型在新领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在常规推理任务中表现优异，但由于缺乏专业领域的预训练，对于如法律等复杂、长文本的领域知识掌握不足，因此需要探索提升其领域适应性的有效方法。

Method: 作者通过三种方式研究LLM在法律任务上的表现：（1）按修辞角色重组法律文档，分析结构化信息对模型处理长文本的影响；（2）向模型定义修辞角色，帮助其理解法律领域专业术语；（3）模拟法庭针对修辞角色的逐步推理过程，以提升模型的推理能力。在三个印度法律判决预测数据集上以零样本方式进行实验。

Result: 实验结果显示，整理文档结构或解释关键法律术语均能显著提升模型表现，F1分数相较基线模型最低提升约1.5%，最高提升4.36%。

Conclusion: 只需简单的方法，如结构重组和术语定义，即可增强LLM在法律等特殊领域的推理和理解能力，为将来提升通用模型专业适应性提供了有效路径。

Abstract: Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.

</details>


### [123] [MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data](https://arxiv.org/abs/2511.20672)
*Saad Mankarious,Ayah Zirikly,Daniel Wiechmann,Elma Kerz,Edward Kempa,Yu Qiao*

Main category: cs.CL

TL;DR: 本文介绍了一个大规模、经过严格清洗的社交媒体心理健康数据集MindSET，弥补现有基准数据集在数据规模和质量上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康研究难以捕捉用户的实时、真实情感和行为，而已有的社交媒体数据集存在数据陈旧、清洗能力不足、多语言和有害内容混杂的问题，亟需高质量的新数据集支持相关研究。

Method: 研究者从Reddit收集基于自述诊断的帖子，制作了涵盖七种心理健康状况、超过1300万条标注帖子的MindSET数据集。数据集制作过程中严格过滤语言、去除NSFW和重复内容，并用LIWC工具进行心理学术语频率分析。为展示数据集效能，采用微调语言模型和BoW特征进行二分类实验，并与先前基准数据集对比。

Result: 基于MindSET训练的模型在诊断检测任务上表现优于之前数据集，Autism检测任务F1值提升最高可达18分。

Conclusion: MindSET为研究社交媒体与心理健康之间关系提供了高质量数据基础，有助于推动风险早期检测和心理趋势深度分析。

Abstract: Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.

</details>


### [124] [Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation](https://arxiv.org/abs/2511.20673)
*Zheng Hui,Xiaokai Wei,Reza Shirkavand,Chen Wang,Weizhi Zhang,Alejandro Peláez,Michelle Gong*

Main category: cs.CL

TL;DR: 本文提出了针对生成式推荐的FlexCode框架，能够根据物品受欢迎程度自适应调整协同过滤和语义编码之间的比例，有效提升了推荐准确率和对长尾物品的推荐能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法通常用统一的编码方式对所有物品建模，没有考虑热门物品和长尾物品在协同信号和语义信息上的不平衡，限制了模型表现和泛化能力。

Method: FlexCode框架为相同的token预算，分别分配给协同过滤（CF）和语义codebook，通过轻量级的门控混合专家(MoE)机制动态平衡CF的精度和语义泛化能力，并设计了对齐与平滑目标函数，保证不同受欢迎度物品的表示连贯性。

Result: 在公开数据集和工业级数据集上，FlexCode在推荐准确性和长尾物品覆盖上均优于强基线方法。

Conclusion: FlexCode为生成式推荐模型中的token表征带来了新机制，不仅提升了整体和长尾物品的推荐效果，还在记忆能力与泛化能力权衡方面提出了新的视角。

Abstract: Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.

</details>


### [125] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed,May Alsofyani,Saad Almohaimeed,Mansour Al Ghanim,Liqiang Wang*

Main category: cs.CL

TL;DR: 本文提出了首个阿拉伯语跨领域、上下文相关的text-to-SQL数据集Ar-SParC，并通过大语言模型及创新方法提升了阿拉伯语text-to-SQL任务的表现。


<details>
  <summary>Details</summary>
Motivation: 由于text-to-SQL任务能够让用户用自然语言与数据库对话，广受关注。但现有数据集和研究绝大多数集中在英文和少量中文，阿拉伯语尚无相关研究。本文旨在填补阿拉伯语领域的空白。

Method: 1. 构建了名为Ar-SParC的阿拉伯语跨领域、上下文相关text-to-SQL数据集，包括3450条问答序列，总计10225个问题及对应SQL。2. 使用GPT-3.5-turbo和GPT-4.5-turbo两种大模型，结合10种提示工程技术（包括问题表述法和上下文学习法），共40组实验。3. 提出了新的GAT corrector方法，并与现有GAT verifier对比。4. 进行了消融实验分析新方法优势。

Result: GAT corrector方法在所有实验中均提升了性能：在零样本学习下，执行准确率和交互准确率平均提升1.9%，在上下文学习下分别提升1.72%和0.92%。消融实验揭示了新方法优于旧方法的原因，尤其在阿拉伯语场景下更为明显。

Conclusion: Ar-SParC填补了阿拉伯语text-to-SQL数据集的空白，新的GAT corrector方法能有效提升大模型在阿拉伯语text-to-SQL任务的表现，为后续多语言、多领域语义解析研究提供了参考。

Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.

</details>


### [126] [Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes](https://arxiv.org/abs/2511.20680)
*Matthew W. Kenaston,Umair Ayub,Mihir Parmar,Muhammad Umair Anjum,Syed Arsalan Ahmed Naqvi,Priya Kumar,Samarth Rawal,Aadel A. Chaudhuri,Yousef Zakharia,Elizabeth I. Heath,Tanios S. Bekaii-Saab,Cui Tao,Eliezer M. Van Allen,Ben Zhou,YooJung Choi,Chitta Baral,Irbaz Bin Riaz*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在肿瘤学决策支持中的推理错误，并提出了一个分层推理错误分类体系，发现推理失误会导致不安全的临床建议，强调了仅凭准确率评估模型存在局限。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如GPT-4）在临床基准测试上表现出色，但其推理过程可能存在错误，目前单靠准确率指标无法反映由推理缺陷导致的安全隐患，特别是在肿瘤学应用中。因此，亟需一种系统化方法识别和分析模型推理错误。

Method: 作者提出并开发了一个推理错误层级分类体系，通过人工标注和归纳真实肿瘤病历GPT-4推理链条中的错误，结合认知偏差理论，总结出三层分类法；以乳腺癌、胰腺癌和前列腺癌的实际病历数据验证推理错误的类型及其对临床建议的影响，并检验自动化评估方法的识别能力。

Result: 约23%的模型推理含有错误，错误类型以确认偏见和锚定偏见为主，推理错误显著关联于与指南不符且可能有害的临床推荐。现有自动化工具虽能检测到错误，但无法准确区分错误类型。

Conclusion: 大语言模型即使结论正确，其推理过程仍可能存在重大缺陷且威胁临床安全。引入系统化推理错误分类体系，有助于未来模型部署前的推理质量评估与改进，保障临床应用安全。

Abstract: Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.

</details>


### [127] [Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches](https://arxiv.org/abs/2511.20683)
*Bharadwaj Yadavalli*

Main category: cs.CL

TL;DR: 当前大语言模型普遍采用统一的提示模板，导致token效率低下，尤其是在输出token成本较高的情况下。本文提出动态模板选择（DTS）方法，根据查询复杂度自适应选择响应模板，减少token消耗，同时保持回答质量。通过实验，DTS方法在多个主流LLM供应商上通用，并显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛部署，输出token的费用显著高于输入token，且当前主流采用统一冗长的输出格式，无论任务简单或复杂，这导致资源浪费。研究动机在于提升token利用效率，降低商业部署的资金消耗，同时不牺牲模型的回答准确性。

Method: 提出了动态模板选择（DTS）系统，能够根据输入查询的复杂度匹配合适的响应模板。设计并比较了两种路由方法：基于预训练embedding的简单MLP和经过微调的复杂RoBERTa transformer，同时进行大规模实证对比（MMLU数据集）以及跨供应商实际生产API评测。

Result: MLP路由器在测试集上达到90.5%的路由准确率，比RoBERTa略高（89.5%），且参数量更少。路由决策在OpenAI GPT-4、Google Gemini和Anthropic Claude三大主流LLM之间具有良好泛化性，token消耗可减少32.6%~33.9%。

Conclusion: 动态模板选择（DTS）能够大幅减少大语言模型部署过程中的输出token消耗，而不会影响输出质量。路由算法具有高准确率且跨平台表现稳定，适用于多种主流供应商，为实际大规模模型应用带来可观的成本效益。

Abstract: Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.
  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.
  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.

</details>


### [128] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang,Yadong Yu,Wenqiang Kang,Jian Zhou,Dongyue Gao,Pan Xiang,Zhe Liu,Mengyan Dai,Zhonglu Guo,Zhimei Sun*

Main category: cs.CL

TL;DR: 二维材料因其独特的物理化学和电子特性，在能源存储与转化领域应用广泛。研究信息分散于各类论文之中，获取材料特性与制备方法信息困难。


<details>
  <summary>Details</summary>
Motivation: 解决二维材料相关信息分散、难以系统获取的问题，便于科研人员更高效利用已知材料知识。

Method: 收集、整合已发表论文中关于二维材料的性质和制备方法的信息，可能涉及知识库构建或自动化数据挖掘。

Result: 建立一种便于提取和使用二维材料关键信息的系统或数据库，提高信息利用效率。

Conclusion: 对二维材料信息的系统整合有助于推动相关领域的研究和应用发展。

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>


### [129] [Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models](https://arxiv.org/abs/2511.20799)
*Trung Cuong Dang,David Mohaisen*

Main category: cs.CL

TL;DR: 本文提出了一种新的判别大语言模型记忆内容的方法：多前缀记忆框架，并验证其可以有效区分模型是否真实记忆了训练数据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易死记硬背训练数据，带来隐私和版权风险。以往记忆检测的定义不足以全面捕捉这一现象，尤其是在经过“对齐”训练后的模型。因此需要更精确且鲁棒的记忆检测方法。

Method: 提出“多前缀记忆”框架。核心思想是：如果某段内容被模型真正记忆，则能通过大量不同的前缀将其激发出来。具体做法是，外部攻击者通过搜索，可以找到足够多的不同前缀触发该段内容，则认为其被记忆。与以往单一路径提取不同，强调了记忆的鲁棒性，并通过前缀多样性量化。

Result: 在多个开源及对齐后的聊天模型上验证，实验表明“多前缀记忆”定义可更可靠地区分模型是否记忆了训练数据，相比现有方法更实用也更稳健。

Conclusion: 所提方法为大模型数据泄露审计提供了有效、实用的工具，并推动了记忆检测方法的进步。

Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.

</details>


### [130] [SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models](https://arxiv.org/abs/2511.20820)
*Jiaojiao Han,Wujiang Xu,Mingyu Jin,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出SAGE方法，通过主动、基于实验的方法更好地解释稀疏自编码器在大语言模型中学到的特征，提高了解释的准确性与可用性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型内部机制不透明，如何解释模型内部特征成为安全与可信部署的难题。虽然稀疏自编码器有助于解构模型特征，但对这些特征的解释仍存在挑战。

Method: 提出SAGE（SAE AGentic Explainer）框架，将特征解释任务转变为主动的、迭代的过程。方法包括为每个特征系统性提出多种解释，设计针对性实验进行检验，并基于激活反馈不断优化解释。

Result: 实验证明，SAGE在多种LLM的SAE特征解释任务上，生成性与预测性准确率均明显优于现有主流方法。

Conclusion: SAGE为自动化、系统化解释大语言模型内部特征提供了更有效的工具，有助于促进大模型的可解释性与安全应用。

Abstract: Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.

</details>


### [131] [Structured Prompting Enables More Robust, Holistic Evaluation of Language Models](https://arxiv.org/abs/2511.20836)
*Asad Aali,Muhammad Ahmed Mohsin,Vasiliki Bikia,Arnav Singhvi,Richard Gaus,Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Yifan Mai,Jordan Cahoon,Michael Pfeffer,Roxana Daneshjou,Sanmi Koyejo,Emily Alsentzer,Percy Liang,Christopher Potts,Nigam H. Shah,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 本文提出并验证了一种结合DSPy结构化提示和HELM评测的新框架，可更准确和可扩展地基准评估大语言模型（LMs）的性能上限，比传统固定提示法更具代表性和决策价值。


<details>
  <summary>Details</summary>
Motivation: 现有如HELM的基准评测多采用固定提示，无法挖掘不同语言模型在提示设计上的最优表现，导致对模型性能估计偏低。这直接影响真实部署和模型选型的决策参考价值。因此，需开发更具泛化和可扩展性的测试方案。

Method: 引入DSPy声明式结构化提示框架，将其与HELM集成，设计四种提示方法，对四个前沿模型、七项通用及医学基准任务进行系统性评测，定量对比结构化提示和既有HELM固定提示的分数表现。

Result: 与结构化提示对比发现：(i) 传统HELM平均低估性能4%；(ii) 分数波动更大（标准差增加2%）；(iii) 排名误判，部分任务榜单前三翻转；(iv) 引入链式思维推理降低了对提示设计的敏感性。

Conclusion: 首次大规模实证表明，系统化引入结构化提示和性能上限估计能使语言模型基准更具代表性和实用价值。相关工具链已开源，促进社区采纳更科学的模型评估方法。

Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller Δ across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

</details>


### [132] [Length-MAX Tokenizer for Language Models](https://arxiv.org/abs/2511.20849)
*Dong Dong,Weijie Su*

Main category: cs.CL

TL;DR: 本文提出了一种新的Length-MAX分词器，可有效减少训练和推理过程中的分词数量，在多个语料库和词表规模下，相较主流BPE方法减少约14-18%的token数，提高了训练与推理效率，并在下游任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 目前主流的分词方法（如BPE）以频率为优化目标，导致同样文本中的token数量偏多，增加了模型训练和推理的计算量。为提升分词效率和资源利用率，需要优化分词算法以减少平均token数量，同时保持或提升模型效果。

Method: 作者将分词词表的选择建模为长度权重最大化的图划分问题，提出了一种贪心近似算法以优化平均token长度（而不只是出现频率）。该方法被称为Length-MAX分词器。

Result: 在FineWeb及多领域数据上，Length-MAX分词器在各种词表规模（1万至6.4万个）下比BPE分词器减少13-18%的token数。用其训练的GPT-2模型达到相同验证损失所需的步数下降逾17%，推理延迟降低约13%，同时提升下游任务（如LAMBADA和HellaSwag）的表现，词表覆盖率高达99.62%，OOV率仅0.12%。

Conclusion: 结合分词长度优化而非仅关注频率，可以显著提升分词效率，同时提升或保持下游任务表现。Length-MAX分词器与生产系统兼容，能降低推理时的内存消耗和延迟，对高效语言建模极具实际价值。

Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

</details>


### [133] [Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory](https://arxiv.org/abs/2511.20857)
*Tianxin Wei,Noveen Sachdeva,Benjamin Coleman,Zhankui He,Yuanchen Bei,Xuying Ning,Mengting Ai,Yunzhe Li,Jingrui He,Ed H. Chi,Chi Wang,Shuo Chen,Fernando Pereira,Wang-Cheng Kang,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 本文提出了Evo-Memory基准和框架，用于评估大语言模型（LLM）智能体在连续任务流中自我进化和记忆管理能力。作者还提出了一种新方法ReMem，并与多个记忆模块进行了综合评测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的记忆系统主要被动检索，缺少对记忆主动演化、经验复用能力的评估与研究，严重限制了其在动态、长时间任务中的应用潜力。现实任务需求LLM在处理连续交互时能积累、整合并更新经验。

Method: 1）构建Evo-Memory基准，涵盖10个多轮和单轮推理及问答数据集，每个数据集以任务流方式结构化，LLM需在每次交互后检索、适应并进化记忆；2）统一实现和评估10多种代表性记忆模块；3）提出基线方法ExpRAG以及创新性ReMem流程，将推理、任务操作与记忆更新紧密结合，实现持续自我提升。

Result: 实验在多种数据集和记忆模块下进行，验证了Evo-Memory基准对LLM经验积累与重用能力的有效评估。ReMem方法在持续改进、经验利用上表现优越，优于传统方法。

Conclusion: 本文的Evo-Memory弥补了LLM动态记忆评测的空白，对设计具备长远规划和持续学习能力的智能体具有重要推动意义。ReMem等新流程为LLM智能体的记忆管理与自我进化提供了强有力方案。

Abstract: Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.

</details>


### [134] [Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation](https://arxiv.org/abs/2511.20872)
*Ali Jahan,Masood Ghayoomi,Annette Hautli-Janisz*

Main category: cs.CL

TL;DR: 本文探讨了面向低资源语言的跨语言论证挖掘方法，并通过三种不同的训练场景进行实验，发现简单的跨语言训练策略优于依赖大语言模型生成数据的增强方法。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘在知识提取等领域有重要应用，但大多数方法需要大量标注数据，限制了其在低资源语言（如波斯语）上的应用。为了解决这个问题，研究人员探索如何利用高资源语言（如英语）的数据迁移到低资源语言。

Method: 本文设计三种训练方案：1）零样本迁移，仅用英语数据训练模型再在波斯语测试；2）通过大语言模型自动生成合成样本，增强英语训练集后再测试；3）将人工翻译的波斯语句子与原始英语数据联合训练并测试。模型以Microtext语料库及其波斯语平行翻译为实验基础，分别在英语和波斯语上测试评估。

Result: 零样本迁移F1分数为英语50.2%、波斯语50.7%；LLM增强带来改进，英语59.2%、波斯语69.3%；而跨语言联合训练的模型，在波斯语测试集上达到74.8%的F1分数，显著优于前两者。

Conclusion: 简单高效的跨语言训练方法（结合双语数据）不仅在低资源语言上超越更为复杂的增强技术，还为低资源语言论证挖掘任务提供了实用解决方案，缓解了数据短缺的问题。

Abstract: Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\% on the English test set and 50.7\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\% on English and 69.3\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.

</details>


### [135] [Emergence and Localisation of Semantic Role Circuits in LLMs](https://arxiv.org/abs/2511.20910)
*Nura Aljaafari,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 本研究提出并应用了一种新方法，揭示大语言模型内部用于语义结构建构的机制：这些机制分布集中、结构逐步细化，并在不同规模与架构间表现出部分转移能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型展现了语义理解能力，但其内部表征抽象语义结构的机制尚不清楚。作者希望揭示和理解这些模型如何具体实现语义角色等结构。

Method: 提出结合role-cross最小对、时序涌现分析与跨模型对比的方法，系统研究LLMs如何实现语义角色。具体分析了神经元归因集中度、结构演化过程以及不同模型间的机制保留情况。

Result: 结果显示：实现语义角色的神经回路高度集中（大部分归因聚集在28个节点以内）；结构演化是逐渐精细化而非突变的，大模型有时跳过局部回路；不同规模模型间有24-59%的组件重合且谱结构相似。

Conclusion: LLMs中用于抽象语义结构的机制是紧凑且因果隔离的，并能在不同规模和架构间部分转移，这为理解和解释LLMs的语义能力提供了结构性线索。

Abstract: Despite displaying semantic competence, large language models' internal mechanisms that ground abstract semantic structure remain insufficiently characterised. We propose a method integrating role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to study how LLMs implement semantic roles. Our analysis uncovers: (i) highly concentrated circuits (89-94% attribution within 28 nodes); (ii) gradual structural refinement rather than phase transitions, with larger models sometimes bypassing localised circuits; and (iii) moderate cross-scale conservation (24-59% component overlap) alongside high spectral similarity. These findings suggest that LLMs form compact, causally isolated mechanisms for abstract semantic structure, and these mechanisms exhibit partial transfer across scales and architectures.

</details>


### [136] [Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2511.20940)
*Reham Omar,Abdelghny Orogat,Ibrahim Abdelaziz,Omij Mangukiya,Panos Kalnis,Essam Mansour*

Main category: cs.CL

TL;DR: 本论文提出了一种新方法Chatty-KG，将检索增强生成（RAG）与知识图谱（KG）结构化查询结合，实现了高效准确的多轮对话式知识问答。


<details>
  <summary>Details</summary>
Motivation: 现有用于对话式知识图谱问答的方法存在显著局限：RAG方法处理结构化数据能力弱且多轮对话支持有限，传统KGQA方法高延迟且难以跟踪复杂对话背景。为此，需要一种方法综合两者优点，实现低延迟、结构化且支持多轮对话的KGQA。

Method: 作者提出Chatty-KG，一个模块化多智能体系统。各LLM代理负责不同任务，包括上下文理解、对话追踪、实体与关系链接、查询规划等。系统通过将自然语言问题转为SPARQL查询，实现结构化、可执行的问答流程，兼顾RAG检索和KG结构执行优势。

Result: 在多种大规模知识图谱以及单轮/多轮问答实验上，Chatty-KG取得了超过最先进基线的表现，F1和P@1评分更高。系统无需微调，也不依赖预处理，并适配多个商业及开源大模型，展现出广泛兼容性和稳定性能。

Conclusion: Chatty-KG有效融合了对话灵活性与知识图谱结构化支撑，具备可扩展性和实时性，为多轮对话式知识图谱问答提供了一种高效、可靠的新方案。

Abstract: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.

</details>


### [137] [TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models](https://arxiv.org/abs/2511.21006)
*Ioana Buhnila,Aman Sinha,Mathieu Constant*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在应对不同类型提问（如定义、举例、释义等）时的表现，并引入了新数据集RefoMed-EN用于相关实验。结果表明，LLMs擅长定义类问题，但在举例等非定义类问题上表现较差，且对热门知识更倾向于释义。


<details>
  <summary>Details</summary>
Motivation: 尽管人类可以轻松变换回答方式（如举例或释义），但LLMs在除定义类问题外的其他类型提问上表现欠佳。作者希望深入分析LLMs预训练数据如何影响其处理多样语言任务的能力。

Method: 作者利用TrackList工具，结合统计和语言学分析，对LLMs在针对不同类型医学术语查询（定义、举例、释义等）时的表现进行评估。并提出了RefoMed-EN数据集，含6170个人工注释医学术语及多种答案类型。同时用语法、语义相似度和统计方法测量模型输出质量。

Result: 研究发现，LLMs在定义类问题上的表现最好，在举例类问题上最差。对于热门或常见知识，LLMs更易产生释义，但对于冷门或技术性强的知识则不明显。

Conclusion: LLMs对多样化语用任务的处理能力有限，表现受知识频率影响。定义类表现较好，对稀有和专业知识则不足。在实际应用中需要结合多样性补足模型弱项。

Abstract: Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.

</details>


### [138] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在上下文学习（ICL）中是否能够覆盖或修改其预训练中学到的标签语义（label semantics），结论显示ICL更多是微调预训练语义而非完全重塑。


<details>
  <summary>Details</summary>
Motivation: 先前研究已经证实LLM通过少样本提示（few-shot prompting）能执行分类任务，但尚不清楚ICL在多大程度上能改变模型“内心”的语义标签定义。理解这一点对于提示工程和模型泛化能力等方向有重要意义。

Method: 作者引入‘自然’（正确标签）和‘反转’（标签全翻转）两种示例，并在8个分类任务、8个开源LLM（1～12B参数）上，设计了三种对齐指标（真值对齐、先验对齐、提示对齐）与语义覆盖率（正确率在标签翻转语义下的表现），比较模型表现。

Result: 实验发现：对于自然示例，ICL提升准确性的同时非常依赖于零样本（zero-shot）表现，即多数正确预测与预训练表现一致；对于反转示例，模型无法有效学习反语义分类器——提升提示对齐不可避免地导致准确率下降，语义覆盖率始终为零。

Conclusion: LLM的ICL难以彻底覆盖或重新定义标签语义，其主要作用为微调预训练时期学到的稳定语义方向。若想在1～12B参数量级模型中彻底改变标签语义，仅依赖ICL不足，需进一步的干预手段。

Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

</details>


### [139] [Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection](https://arxiv.org/abs/2511.21066)
*Michael Iskandardinata,William Christian,Derwin Suhartono*

Main category: cs.CL

TL;DR: 本文提出了一种结合检索式上下文信息来提升大语言模型（LLM）讽刺检测效果的方法，显著改善了不同数据集上的宏观F1分数。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测因语言复杂性和文化多样性，对当前的预训练语言模型（PLM）与大语言模型（LLM）依然是重大挑战。尤其是涉及特定词语、俚语或需要额外背景时，模型识别不稳定。为弥补这一不足，提高模型对讽刺的理解能力，有必要引入额外上下文。

Method: 基于前沿的PMP（Pragmatic Metacognitive Prompting）方法，本文提出两类检索感知策略：一是当模型背景知识不足时，利用基于网络的非参数知识检索，为目标文本检索并补充外部上下文；二是激发模型自身的内部知识，提升自我知识感知。将该流程应用于多个数据集并进行评估。

Result: 采用非参数检索方法，在Twitter Indonesia Sarcastic数据集上，宏观F1较原PMP方法提升9.87%；自我知识检索在Semeval和MUStARD数据集上宏观F1分别提升3.29%和4.08%。

Conclusion: 结合外部检索或自我检索的上下文能显著提升LLM在多文化、多语言环境下进行讽刺检测的能力，特别对涉及俚语、文化特定参考、未知词等情况有效。未来将进一步优化相关检索及其对模型表现的影响。

Abstract: Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.

</details>


### [140] [Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning](https://arxiv.org/abs/2511.21081)
*Thura Aung,Eaint Kay Khaing Kyaw,Ye Kyaw Thu,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 本文在缅甸语等低资源语言分类任务中，对比了新型 Kolmogorov-Arnold Networks（KANs）与传统多层感知机（MLP）作为分类头的性能，发现KANs表现优秀，是MLP可行且更高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如缅甸语）的分类任务常常只能微调最终分类层，预训练编码器通常被冻结；而广泛采用的MLP分类头在表达能力和计算成本上存在限制。因此，作者希望探索更高效、更具表达力的分类头方法。

Method: 将三类KAN变体（FourierKAN、EfficientKAN、FasterKAN）作为分类头，应用在不同类型的词嵌入（如TF-IDF、fastText、多语mBERT、Distil-mBERT）上，并与传统MLP分类头进行系统对比分析。

Result: 实验显示，KAN分类头多项指标优于或相当于MLP。其中，fastText+EfficientKAN组合获得最高F1分数（0.928）；FasterKAN在速度和准确率间表现出最佳权衡；在mBERT嵌入下，EfficientKAN与MLP持平或略优（F1=0.917）。

Conclusion: KANs作为分类头，兼具表达性和效率，是MLP在低资源语言分类任务中的有力替代方案。

Abstract: In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.

</details>


### [141] [Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models](https://arxiv.org/abs/2511.21086)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.CL

TL;DR: 本文系统评估了三大主流大语言模型在字符级约束文本生成任务上的表现，揭示了架构上的差异对任务成功率影响远大于模型规模，且模型在一些常见单词上的失败表明现有架构存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在受控文本生成中需满足严格的字形约束，但各架构间的系统性比较有限。论文意在填补不同模型架构在字符级约束任务表现上的系统评估空白，以指导更有效的模型设计。

Method: 作者选取了Qwen3、Claude Haiku-4.5和GPT-5-mini三个系列共28种模型配置，在包含58个涉及字符级约束的文字谜题上测试它们的表现，并分析了模型架构和规模、算力等因素对结果的影响，同时结合10000名人工解题者的数据进行对比。

Result: 不同架构间在约束满足能力上差别显著（最大可达2.2倍F1分数差距），规模扩展虽有提升但远低于架构带来的提升。对推理资源敏感度在大容量模型上明显提升，而中等规模模型则无明显收益或不升反降。对人工易难度标注的校准性表现中等，但所有模型在一些常见非典型拼写单词上普遍失败，错判率高（高达89-96%），即便人类准确率极高。

Conclusion: 模型能力提升不能单靠参数量扩展或增加算力，字符级约束满足任务可能需要在架构或训练目标上进行创新。现有模型对分布典型性的依赖导致其在异构/非典型单词处理上表现薄弱，需发展新型架构来提升约束推理能力。

Abstract: Large language models must satisfy hard orthographic constraints during controlled text generation, yet systematic cross-architecture evaluation remains limited. We evaluate 28 configurations spanning three model families (Qwen3, Claude Haiku-4.5, GPT-5-mini) on 58 word puzzles requiring character-level constraint satisfaction. Architectural differences produce substantially larger performance gaps (2.0-2.2x, F1=0.761 vs. 0.343) than parameter scaling within families (83% gain from eightfold scaling), suggesting that constraint satisfaction may require specialized architectural features or training objectives beyond standard language model scaling. Thinking budget sensitivity proves heterogeneous: high-capacity models show strong returns (+0.102 to +0.136 F1), while mid-sized variants saturate or degrade. These patterns are inconsistent with uniform compute benefits. Using difficulty ratings from 10,000 human solvers per puzzle, we establish modest but consistent calibration (r=0.24-0.38) across all families, yet identify systematic failures on common words with unusual orthography ("data", "poop", "loll": 86-95% human success, 89-96% model miss rate). These failures reveal over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns, suggesting architectural innovations may be required beyond simply scaling parameters or computational budgets.

</details>


### [142] [ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features](https://arxiv.org/abs/2511.21088)
*Ye Bhone Lin,Thura Aung,Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: 本文首次针对缅甸语低资源环境下，利用序列到序列Transformer模型进行自动语音识别（ASR）误差校正，通过集成IPA和对齐特征，有效提升了ASR输出的准确率。


<details>
  <summary>Details</summary>
Motivation: 缅甸语作为低资源语言，ASR系统的输出准确率较低，缺乏专门针对其误差校正的研究，亟需提升其ASR系统性能，并探索适用的特征融合策略。

Method: 作者评估了五种ASR主干网络，并提出了一种融合IPA和对齐特征的ASR误差校正（AEC）模型，采用序列到序列Transformer架构，在不同数据增强条件下比较了有无AEC的ASR输出性能。

Result: 融合IPA与对齐特征的AEC模型，将ASR平均词错误率（WER）从51.56降至39.82（未增强数据）和43.59（增强后），chrF++得分由0.5864提高到0.627，相比未校正输出均有明显提升。

Conclusion: AEC模型在低资源缅甸语ASR中表现出较强的鲁棒性，特征设计对于提升ASR输出至关重要，为后续低资源ASR研究提供了有效方案。

Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.

</details>


### [143] [MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing](https://arxiv.org/abs/2511.21101)
*Manish Jain,Satheesh Kumar Ponnambalam,Salman Faroz,Chandrakanth Lns,Vinay Sharma*

Main category: cs.CL

TL;DR: 本论文提出了MortgageLLM，一个专门针对抵押贷款金融领域的大语言模型，通过双专家架构解决了通用模型领域知识不足和多任务性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）虽然强大，但在抵押贷款等高度专业化领域表现有限，因为缺乏领域知识增强，并且通用模型难以兼顾结构化任务和对话任务表现。论文旨在解决如何增强行业特定能力同时保持指令跟随能力的问题。

Method: 作者以LLaMA-3.1-8B为基础，创新性地采用了双通道专家框架，分别针对结构化任务（通过SFT训练）和对话任务（通过DPO）各自优化，避免单一模型在多任务间性能权衡。通过“instruction residual”技术，在领域适配后恢复模型的指令跟随能力。同时，设计了智能任务路由机制，由专家模型之一通过少样本分类决定任务分配。

Result: 在特定的抵押贷款金融领域基准测试中，最终模型（MLM v2）在总结、问答和分类任务上都显著超越了基础的LLaMA-3.1-8B-Instruct，以及其他基线方法。在多个评分标准（LLM-as-a-Judge、BERTScore等）上都有全面提升。

Conclusion: 本论文验证了领域特定双专家LLM架构与指令残差技术在专业金融领域的有效性。新方法大幅提升了专业任务表现，为行业级专用大模型设计提供了新思路。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.

</details>


### [144] [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/abs/2511.21214)
*Yuhang Wang,Yanxu Zhu,Dongyuan Lu,Jitao Sang*

Main category: cs.CL

TL;DR: 本文提出了一种新的自适应安全对齐框架SGASA，有效增强大模型抵御恶意越狱攻击的能力，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前复杂推理型大模型易受到隐蔽且具欺骗性的越狱攻击，内置的安全机制常被绕过，导致危害内容生成，因此亟需更智能自适应的安全对齐策略。

Method: 提出SGASA框架，包括数据预合成（生成安全准则与增强提示）和对齐微调两个阶段，通过SFT（监督微调）和DPO（直接偏好优化）使模型内化这些安全准则。

Result: 在多个数据集上实验，SGASA显著提升了模型面对恶意提示时的安全性，同时减少了对正常请求的不必要拒绝。

Conclusion: SGASA框架自适应且具可扩展性，为提升推理模型的安全对齐能力提供了有效解决方案。

Abstract: Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.

</details>


### [145] [Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?](https://arxiv.org/abs/2511.21218)
*Steven Wang,Kyle Hunt,Shaojie Tang,Kenneth Joseph*

Main category: cs.CL

TL;DR: 本文探讨了将大语言模型（LLM）作为问卷和实验研究中人的替代者的可行性，尤其关注通过少量人类样本微调是否可缓解LLM与人类表现不一致的问题。结果显示微调后LLM在部分维度上表现提升，但仍无法替代人类数据用于正式推断分析。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在许多领域表现突出，但其能否在行为科学和市场等领域替代人类样本，尤其是在数据多样性和对少数群体对齐度等方面，引发了争议。本文动机在于检验通过微调能否提升LLM模拟人类行为数据的质量，降低其与真实人类数据的差距。

Method: 作者以信息披露行为实验为例，利用少量试点人类问卷数据对LLM进行微调，并比对人类与LLM的问卷结果，评估分布差异、亚群体对齐度、信念与行动一致性及回归系数等多维度表现。

Result: 微调后的LLM在异质性、亚群体对齐度和信念-行动一致性等方面均有较大提升，但在回归系数等关键统计量的还原上，仍与人类真实数据存在较大差距。

Conclusion: 即使经过微调，LLM生成的数据也不能完全替代人类样本，特别是在正式推断分析上有明显局限，提示在将LLM用于行为实验数据模拟时仍需谨慎。

Abstract: There is ongoing debate about whether large language models (LLMs) can serve as substitutes for human participants in survey and experimental research. While recent work in fields such as marketing and psychology has explored the potential of LLM-based simulation, a growing body of evidence cautions against this practice: LLMs often fail to align with real human behavior, exhibiting limited diversity, systematic misalignment for minority subgroups, insufficient within-group variance, and discrepancies between stated beliefs and actions. This study examines an important and distinct question in this domain: whether fine-tuning on a small subset of human survey data, such as that obtainable from a pilot study, can mitigate these issues and yield realistic simulated outcomes. Using a behavioral experiment on information disclosure, we compare human and LLM-generated responses across multiple dimensions, including distributional divergence, subgroup alignment, belief-action coherence, and the recovery of regression coefficients. We find that fine-tuning on small human samples substantially improves heterogeneity, alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the regression coefficients of the original study, suggesting that LLM-generated data remain unsuitable for replacing human participants in formal inferential analyses.

</details>


### [146] [Developing an Open Conversational Speech Corpus for the Isan Language](https://arxiv.org/abs/2511.21229)
*Adisai Na-Thalang,Chanakan Wittayasakpan,Kritsadha Phatcharoen,Supakit Buakaw*

Main category: cs.CL

TL;DR: 本论文介绍了首个开放的伊森语对话语音数据集，提供自然交流数据，旨在支持该地区语言研究和AI开发。


<details>
  <summary>Details</summary>
Motivation: 伊森语是泰国使用最广泛的地方方言，现有的语音语料库多基于朗读或脚本，缺乏能反映真实交流特征（如口语、语调、语音混杂等）的自然对话数据。此外，伊森语缺乏统一正字法，导致语料记录与处理复杂，对AI和语言研究提出了更高挑战。

Method: 收集包含自然口语特征（如自发语调、语音杂音、泰语与伊森语混用等）的对话语音数据，并针对缺乏统一正字法的问题，建立兼顾语言表现力与机器处理需求的实用转写方案。

Result: 建立并开放了首个伊森语自然对话语音数据集，同时制定了转写指南，有效管理了正字法不统一带来的问题。

Conclusion: 该开放数据集为包容性AI和欠研究语言的技术发展提供了基础，有助于相关领域的进一步研究和技术创新。

Abstract: This paper introduces the development of the first open conversational speech dataset for the Isan language, the most widely spoken regional dialect in Thailand. Unlike existing speech corpora that are primarily based on read or scripted speech, this dataset consists of natural speech, thereby capturing authentic linguistic phenomena such as colloquials, spontaneous prosody, disfluencies, and frequent code-switching with central Thai. A key challenge in building this resource lies in the lack of a standardized orthography for Isan. Current writing practices vary considerably, due to the different lexical tones between Thai and Isan. This variability complicates the design of transcription guidelines and poses questions regarding consistency, usability, and linguistic authenticity. To address these issues, we establish practical transcription protocols that balance the need for representational accuracy with the requirements of computational processing. By releasing this dataset as an open resource, we aim to contribute to inclusive AI development, support research on underrepresented languages, and provide a basis for addressing the linguistic and technical challenges inherent in modeling conversational speech.

</details>


### [147] [PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark](https://arxiv.org/abs/2511.21285)
*Robert Belanec,Branislav Pecher,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: 本文提出了PEFT-Bench，一个统一的端到端基准平台，用于在自回归大语言模型（LLM）上评测多种参数高效微调（PEFT）方法，同时开发了PSCP指标来综合评价方法的效能和资源占用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽性能强大，但在资源消耗和可获取性上存在局限。PEFT方法能减少可训练参数量，但相关评测工具和标准有限、难以复现，影响了方法的推广与对比。

Method: 作者构建了PEFT-Bench基准平台，支持对6种PEFT方法和27个NLP数据集的系统化、统一评测。同时提出PSCP指标，综合考虑可训练参数数、推理速度和训练内存等多方面性能。

Result: PEFT-Bench实现了对多种PEFT方法和数据集的高效评测验证。PSCP指标能够反映不同PEFT方法在资源和性能上的平衡，更全面评估实际适用性。

Conclusion: PEFT-Bench和PSCP为PEFT方法评价提供了统一、可复现的平台和多维度标准，有助于推动参数高效微调技术的研究和应用。

Abstract: Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.

</details>


### [148] [Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text](https://arxiv.org/abs/2511.21334)
*Kai Kugler*

Main category: cs.CL

TL;DR: 本文系统性地研究了神经语言模型训练过程中产生文本中马丁定律（词频与一词多义性间的经验规律）的演化变化。结果发现，模型符合马丁定律的程度并非随着训练持续上升，而是呈现先增强后下降的非单调趋势。特定阶段模型表现出多义性和词频的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 理解神经语言模型（如LLM）在训练过程中语言统计规律自发涌现的机制，有助于揭示语言能力的发展阶段，也为模型评价提供新角度。过去尚未有研究定量考察马丁定律在生成文本训练中的行为。

Method: 作者以Pythia系列四种规模模型为对象，在30个训练检查点抽取模型输出文本，用DBSCAN方法聚类单词的上下文嵌入以近似词义数，然后考察词频与聚类数量（多义性）的相关性变化，进而分析马丁定律的演变。

Result: 发现马丁定律的相关性在训练早期逐步出现，在第100-104个检查点达到峰值，之后下降。大模型（410M、1B参数）在后期表现为规律的'优雅退化'，而小模型（70M、160M）出现语义坍塌（多义性丧失）。词频-特异性权衡始终较为稳定（r≈-0.3）。

Conclusion: 语言模型对语言统计规律的符合度并非持续提升，而是在训练过程中呈现最佳窗口，随后退化。方法为分析模型训练中语义结构涌现提供了新途径，对模型调优、理解模型表达能力的演变有重要意义。

Abstract: We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.

</details>


### [149] [Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model](https://arxiv.org/abs/2511.21399)
*Joshua Fonseca Rivera*

Main category: cs.CL

TL;DR: 本文通过微调训练，让语言模型能够高可靠性地检测和报告人为注入的“思维”激活模式，实现了比零训练模型大幅提升的准确率，证明可通过直接训练诱导部分自省能力，有助于AI透明性。


<details>
  <summary>Details</summary>
Motivation: Lindsey（2025）发现，当前语言模型只能偶尔检测到被注入的激活模式，成功率很低。因此作者希望探究该能力能否通过直接训练获得，而不是依赖其自然涌现。这样做有助于减少不同模型间自省能力的差异，提高AI自身状态感知与透明性。

Method: 作者聚焦于注入“思维”后自我报告的任务，对7B参数量模型进行微调，使其在检测并报告单一token位置的注入“思维”时，能够从几乎不会检测（0.4%准确率，6.7%误报率）提升到85%准确率（α=40，0%误报）。同时，测试模型是否能泛化至未见过的概念向量。

Result: 训练后的模型能高准确率检测和报告瞬时注入“思维”，0误报。泛化测试中，对未见过的向量表现出7.5个百分点的准确率差距，体现出模型掌握了可转移的能力，而不仅仅是记忆。模型也满足了Lindsey提出的准确性、扎根性和内部性三个标准。

Conclusion: 论文证实，至少某些自省行为可以通过直接训练获得，为AI内在透明性和自省能力提供了一种现实可行的技术路径。

Abstract: Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected "thoughts" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at α=40, 0% false positives). Our model detects fleeting "thoughts" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether "training for introspection would help eliminate cross-model differences." We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.

</details>


### [150] [Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?](https://arxiv.org/abs/2511.21401)
*Antonín Jarolím,Martin Fajčík,Lucia Makaiová*

Main category: cs.CL

TL;DR: 本文关注于检测评论区中错误信息的细粒度证据抽取，特别是捷克语和斯洛伐克语场景，并提出了人工标注的新数据集，用于评估大语言模型的证据提取表现。


<details>
  <summary>Details</summary>
Motivation: 线上新闻评论区中常常传播错误信息，现有检测技术亟需提升。为了有效支持或反驳评论中的观点，需要精确定位支撑或反驳观点的文献及证据信息。捷克语和斯洛伐克语领域在这一细粒度任务中缺乏数据和系统性评测。

Method: 构建了基于人工双向标注的细粒度证据数据集，并选用多种主流大语言模型（如llama3.1:8b、gpt-oss-120b、qwen3:14b等）在该数据集上进行评测，分析模型输出与人工标注的一致性以及各模型的错误类型。

Result: 实验发现，大多数现有大模型未能很好地从原文本中逐字复制有效证据，导致输出无效。llama3.1:8b模型虽参数量小，但正确率较高；gpt-oss-120b虽大但表现反而较差；qwen3:14b、deepseek-r1:32b、gpt-oss:20b在模型规模和与人工标注一致性上表现平衡。

Conclusion: 当前大模型在细粒度事实证据抽取上存在较大改进空间，模型规模与能力未必成正比。为低资源语言提供人工细致标注数据集对于模型评估和优化具有重要意义。

Abstract: Misinformation frequently spreads in user comments under online news articles, highlighting the need for effective methods to detect factually incorrect information. To strongly support or refute claims extracted from such comments, it is necessary to identify relevant documents and pinpoint the exact text spans that justify or contradict each claim. This paper focuses on the latter task -- fine-grained evidence extraction for Czech and Slovak claims. We create new dataset, containing two-way annotated fine-grained evidence created by paid annotators. We evaluate large language models (LLMs) on this dataset to assess their alignment with human annotations. The results reveal that LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs. Error-rate analysis shows that the {llama3.1:8b model achieves a high proportion of correct outputs despite its relatively small size, while the gpt-oss-120b model underperforms despite having many more parameters. Furthermore, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate an effective balance between model size and alignment with human annotations.

</details>


### [151] [Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation](https://arxiv.org/abs/2511.21402)
*Zhifeng Hao,Qibin Song,Ruichu Cai,Boyan Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新型的Text-to-SQL推理框架DSR-SQL，通过双状态推理机制有效提升了大模型在复杂数据库上的SQL生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链(CoT)的分治方法在复杂企业数据库上的Text-to-SQL任务中，存在上下文容量不足、模式关联不稳定和数据库语义把握薄弱等问题。

Method: 提出了Dual-State Reasoning(DSR-SQL)框架：一方面通过自适应上下文状态，精炼并选取与查询相关的数据库结构，确保语义紧密且紧凑的环境；另一方面，在SQL合成过程中引入反馈引导的状态转换，使模型能自我纠错并更好对齐用户意图。该方法无需再训练或上下文示例。

Result: 在Spider 2.0-Snow数据集上达到35.28%的执行精度，在BIRD开发集上达到68.32%的精度，展现出有竞争力的性能。

Conclusion: DSR-SQL大幅提升了LLM在大规模与复杂数据库Text-to-SQL任务中的推理连贯性与执行准确率，为实际应用提供了新的有效方案，并计划开源实现。

Abstract: Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \textbf{D}ual-\textbf{S}tate \textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\% execution accuracy on Spider 2.0-Snow and 68.32\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.

</details>


### [152] [Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning](https://arxiv.org/abs/2511.21416)
*Kaifeng Hong,Yinglong Zhang,Xiaoying Hong,Xuewen Xia,Xing Xu*

Main category: cs.CL

TL;DR: 本文提出Odin及其轻量版本Light Odin，巧妙结合了图结构信息与Transformer的文本能力，在多个数据集上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本属性图（text-attributed graphs）处理方法存在不足：GNN易过平滑且依赖于节点间跳数，Transformer则忽视图结构。本文希望同时兼顾语义表达和结构推理能力。

Method: 作者提出Odin模型，将图结构信息有选择地注入到Transformer的特定层级，通过定向双模块机制实现多跳结构信息的层级融合，无需传统的消息传递。Odin采用全局[CLS]聚合方法，有效避免了过平滑，并能解耦结构抽象与邻域规模、图形拓扑的关系。Light Odin是其轻量版本，减少计算开销但保持核心抽象能力。

Result: 在多个富文本图的公开基准上，Odin实现了最新最优的准确率，Light Odin则以显著更低的计算成本实现了可观的性能表现。

Conclusion: Odin与Light Odin为图结构与文本信息的深度融合提供了一套统一、高效且无跳依赖的新范式，兼具性能与效率。代码已开源。

Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.

</details>


### [153] [A Systematic Study of Model Merging Techniques in Large Language Models](https://arxiv.org/abs/2511.21437)
*Oğuz Kağan Hitit,Leander Girrbach,Zeynep Akata*

Main category: cs.CL

TL;DR: 本论文对现有主流模型合并方法在大语言模型（LLM）上的有效性进行了大规模系统评估，发现多数方法无法带来性能提升，仅最简单的Task Arithmetic方法能稳定提升性能，指出需要专为LLM设计合并算法。


<details>
  <summary>Details</summary>
Motivation: 模型合并可以在无需额外训练的情况下，结合多个微调模型提升表现，对模型复用和开发效率意义重大。但之前的合并方法多在小模型或分类器上验证，其在LLM上的有效性尚不明确。

Method: 作者系统评估了六种先进的合并方法（包含最新子空间法等），在4个开源LLM上、每个模型12个微调版本、共16个标准LLM任务中，通过标准化基准测试，比较合并模型超过基础模型和最佳单模型的概率及增益。

Result: 实验结果显示，只有最老最简单的Task Arithmetic方法能在LLM上可靠提升性能，其他更复杂的合并（如干扰感知与子空间法）多数情况下会导致显著性能下降。

Conclusion: 目前模型合并原有的经典方法无法直接迁移应用到现代LLM，亟需开发专门针对LLM的合并算法与微调技术。代码将在论文接收后公开。

Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

</details>


### [154] [Hierarchical Ranking Neural Network for Long Document Readability Assessment](https://arxiv.org/abs/2511.21473)
*Yurui Zheng,Yijun Chen,Shaohong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于双向机制的可读性评估方法，同时结合了语境信息和标签的有序关系，在中英文数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来深度学习被应用于可读性评估，但现有方法普遍忽略了文本长度及可读性标签的顺序关系，导致评估准确性受限。

Method: 作者提出了一种双向可读性评估机制，结合上下文信息识别文本中语义丰富的部分，并对句子级别的可读性进行预测。这些句子级标签又辅助文档整体可读性预测。此外，引入了成对排序算法，用标签差建模可读性级别间的有序关系。

Result: 在中文和英文数据集上的实验结果显示，该方法取得了有竞争力的性能，明显优于常用基线模型。

Conclusion: 结合文本细粒度信息和标签顺序关系的方法能显著提升可读性评估表现，具有跨语言通用性。

Abstract: Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.

</details>


### [155] [Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation](https://arxiv.org/abs/2511.21517)
*Lina Conti,Dennis Fucci,Marco Gaido,Matteo Negri,Guillaume Wisniewski,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文探讨语音翻译模型在为指代说话人的词语分配性别时的决策机制，发现模型除了倾向于阳性偏见外，还会利用语音特征和结构信息来推断性别。


<details>
  <summary>Details</summary>
Motivation: 在由英语等有性别模糊表达的语言翻译成有语法性别要求的语言（如西、法、意）时，语音信号可能引入性别偏见，导致对说话人的性别识别错误，而目前对ST模型在性别决策过程的理解不足。

Method: 作者对三对语言（英/西、英/法、英/意）的ST模型进行分析，考察训练数据、内部语言模型（ILM）及声学信息是如何共同影响性别分配的。通过对频谱的对比特征归因，追踪模型在翻译过程中的性别线索获取。

Result: 研究发现ST模型不仅复制训练数据中的性别偏向，还学会了更广泛的阳性优先模式。尽管ILM本身存在较强的阳性偏误，但在有声学信息时，模型能调整这一倾向。性能更佳的模型采用了利用第一人称代词将有性别词语关联到说话人本身、并充分提取分布于频谱中的性别线索的新机制。

Conclusion: ST模型的性别分配决策比想象中复杂，既受训练数据影响，也可利用说话人语音特征纠正内部偏见。高性能模型会主动寻找并结合分布式声学线索和语法结构，以更准确地匹配说话人性别。这为未来减少语音翻译中的性别偏见提供了理论和方法支持。

Abstract: Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.

</details>


### [156] [Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects](https://arxiv.org/abs/2511.21533)
*Husne Ara Rubaiyeat,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文介绍了一个新的孟加拉手语翻译数据集IsharaKhobor及其两个子集，以支持该领域的研究，并探讨了数据集构建中的挑战及初步基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于孟加拉语手语资源稀缺，缺乏标准句子级数据集，极大限制了AI辅助工具在听障人群中的应用。创建标准数据集对于推动相关技术进步意义重大。

Method: 作者开发了IsharaKhobor数据集，并开放了两个子集。方法上，作者分析了数据收集和标注中的挑战，还基于姿态关键点（landmark）和RQE嵌入进行基准测试。此外，对词汇规范化限制进行了消融实验，进一步生成小规模和规范化的数据子集。

Result: 开发了IsharaKhobor及两个子集（IsharaKhobor_small与IsharaKhobor_canonical_small），并通过基准测试和消融实验展示了数据集在不同条件下的表现。

Conclusion: 文中数据集填补了孟加拉手语翻译数据资源的空白，为AI辅助工具开发和相关研究提供了重要支持，并公开发布以促进学界、工业界的进一步研究。

Abstract: Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].

</details>


### [157] [RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions](https://arxiv.org/abs/2511.21568)
*Minjoon Choi*

Main category: cs.CL

TL;DR: 本文提出了RoParQ基准，用于评估大语言模型（LLMs）在处理同义改写问题时的一致性，还提出了XParaCon指标量化模型的鲁棒性，并通过同义改写感知的SFT微调方法显著提升模型一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理同义改写问题时表现出不一致性，说明模型偏向表层模式识别而非语义理解，因此需要新的评估工具和训练方法提升同义语义的不变性。

Method: 1）构建RoParQ基准，利用专有模型生成并筛选问题同义改写例；2）提出XParaCon指标，用标准差衡量跨同义问题模型表现的波动；3）设计了基于推理、感知同义改写的SFT微调方法，调整模型更关注语义不变性。

Result: 实验显示所提出的SFT方法大幅提升了模型的一致性，微调后的轻量级模型在一致性表现上可媲美体量更大的预训练模型。

Conclusion: 本工作有效提升了LLMs对同义改写问题的鲁棒性，有助于减少模型对表层记忆的依赖，使其表现更加健壮可靠。

Abstract: Large Language Models (LLMs) often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding. To address this limitation, we introduce RoParQ, a benchmark specifically constructed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. This benchmark is derived from standard datasets by generating paraphrases via proprietary models and selectively retaining examples that elicit inconsistent confidence from a judge model. We further propose XParaCon, a novel evaluation metric that quantifies a model's robustness by measuring the standard deviation of accuracies across question variants. Additionally, we implement a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) strategy designed to align models toward semantic invariance. Our experiments demonstrate that this targeted alignment significantly enhances robustness. Notably, fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models. These results highlight the efficacy of our approach in mitigating superficial memorization and fostering more robust, reliable LLMs.

</details>


### [158] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了一种简便、通用的方法，能够识别大模型中特定技能的神经元，并通过与外部指标的相关性分析，实现对模型内部机制的解释。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出强大能力，但内部工作机制不透明。理解哪些神经元负责特定技能有助于模型可解释性与改进。

Method: 基于先前通过soft prompt训练发现技能神经元的方法，本文将分析扩展到包含多种技能的复杂场景。具体做法是通过神经元激活值与外部标签、模型置信度等辅助指标做相关性分析，从而无需人工聚合词元，也能发现具任务解释性的神经元行为。

Result: 通过实验证明，该方法不仅能识别驱动已知技能的神经元，还能揭示模型在BigBench算术推理任务上的新捷径。

Conclusion: 该方法能够在无需复杂人工参与的基础上，提升大语言模型内部机制的可解释性，并为后续技能神经元研究提供新工具。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [159] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文通过对不同类型元信息在大模型预训练中的作用进行系统分析，发现细粒度和高质量的元信息能显著提升训练效率，并提出了以元信息辅助多任务学习的新方法。


<details>
  <summary>Details</summary>
Motivation: 此前的研究仅发现URL作为元信息能够加速大模型训练，尚未充分探索其他元信息种类的潜力。为提高大模型预训练效率，需要系统性地研究更多元信息类型。

Method: 作者考察了多种文档级元信息（如文档质量指示器），比较了元信息前置与元信息追加的方法，并引入了预测元信息的辅助任务。同时，利用可学习的meta-token，通过mask loss训练方式引入质量感知的隐式结构，用探查方法分析潜在表征对学习过程的影响。

Result: 实验表明，细粒度、高质量的元信息（如文档质量指标），无论是前置还是通过辅助任务追加，都能显著提升大模型训练效率。可学习的meta-token也可部分复现加速效果。探查结果表明元信息提升了预训练过程表示的质量和学习效率。

Conclusion: 该研究为在大语言模型预训练中更有效地集成各类元信息提供了实践性指导，并证实利用细粒度元信息和元信息辅助任务能提升预训练效率和模型效果。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [160] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 该论文研究了AI生成与人类创作的捷克诗歌在母语者中的辨识度及审美评价，发现两者难以区分，并存在明显作者归因偏见。


<details>
  <summary>Details</summary>
Motivation: 目前关于AI诗歌的研究多集中于英语，尚缺乏对低资源语言（如捷克语）的相关研究。作者希望探究AI在复杂、低资源斯拉夫语言诗歌生成的表现以及人类对此的感知和审美。

Method: 让捷克母语者猜测诗歌的作者（AI或人类），并进行审美评分，采用逻辑回归分析评分与准确辨识之间的关系，同时统计受访者的文学背景。

Result: 受试者猜测作者的正确率接近随机（平均45.8%），表明AI诗歌与人类诗歌在捷克语中难以区分。当认为诗歌由AI创作时，审美评价会下降，但实际上AI诗歌的总体评分不逊于人类诗歌。喜好与判断准确性呈负相关，文学背景对识别无影响。

Conclusion: AI能够在捷克语等低资源语言中生成高质量诗歌，呈现出与英语类似的能力。同时，作者归因信念显著影响读者的审美评价。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [161] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的分布式多智能体合成数据框架，通过消息队列实现任务流转，显著提升了数据生成效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体合成数据框架通常依赖中心化调度，存在扩展性瓶颈或领域适应性差的问题。随着大模型数据需求增长，亟需更加高效灵活的多智能体协作解决方案。

Method: Matrix采用点对点消息队列通信方式定义任务控制与数据流，实现完全去中心化。每个轻量级agent独立推进任务，密集计算操作（如LLM推理）由分布式服务承担。框架基于Ray，具备高度模块化和可配置特点。

Result: 在协作对话、网页推理数据采集、工具轨迹生成等任务中，Matrix在相同硬件资源下数据生成吞吐量提升2—15倍，且输出质量不降低。

Conclusion: Matrix突破了中心式调度的效率与扩展瓶颈，能够大规模支持多智能体数据生成任务，适应多元应用场景。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [162] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 提出了ToolOrchestra方法，由小型编排器协调多个智能工具，能在提升智能上限的同时优化效率，效果超过现有主流大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型功能强大，但在处理高度复杂和深度的推理任务（如HLE）时，仍然面临概念难题和高昂的计算成本，亟需更高效的模型调度方法。

Method: 提出ToolOrchestra方法，利用小型模型（8B参数）作为‘编排器’，通过强化学习调度多种智能工具，奖励机制考虑任务结果、效率及用户偏好。

Result: Orchestrator比GPT-5准确率更高（HLE得分37.1%，高于GPT-5的35.1%），效率提升2.5倍；在tau2-Bench及FRAMES等基准超越GPT-5，成本仅为其约30%。综合分析表明性能与成本实现最佳平衡，并对未见过的工具有良好泛化能力。

Conclusion: 通过小型编排模型协调多工具，比单一大型模型更高效、有效，为可扩展的工具增强推理系统提供了新方向。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [163] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文针对大语言模型在不同难度任务上的泛化能力进行了系统性分析，发现单独使用简单或困难数据训练模型，无法实现对所有难度数据的泛化提升。


<details>
  <summary>Details</summary>
Motivation: 过去关于使用简单数据还是困难数据训练能否提升模型在不同难度测试集上的表现，结论不一。有效的数据选择和模型评价需明确模型在不同难度任务间的泛化能力。

Method: 作者基于六个数据集，利用成千上万的大语言模型输出及项目反应理论（IRT）来为样本评分难度，避免人为主观判断，从而开展更大规模、更精细的泛化分析。

Result: 实验证明，模型在难度跨域上的泛化能力有限。无论仅用简单还是困难的数据训练，都无法对所有难度范围的数据带来一致性的改进。

Conclusion: 在训练和评估大语言模型时，必须包含各类难度的样本，仅仅依赖部分难度的数据容易带来泛化风险。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [164] [OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping](https://arxiv.org/abs/2511.20841)
*Edmond Tong,Advaith Balaji,Anthony Opipari,Stanley Lewis,Zhen Zeng,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 提出了一种机器人可在零样本情况下基于任务与物体部件知识，利用大模型和视觉语言模型完成抓取任务的新方法（OVAL-Grasp），大幅提升了对新环境和新物体的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于几何的方法难以应对未知环境下物体的视觉特征复杂性，如遮挡、未见过的新物体，以及需要任务驱动选择物体部位等问题，因此需要一种更智能和泛化能力更强的抓取方法。

Method: OVAL-Grasp结合大语言模型（LLM）和视觉语言模型（VLM）。具体流程为：输入RGB图片和任务描述，由LLM判断要抓取或回避的物体部位，用VLM完成该部位的图像分割，并生成2D热力图定位可操作区域，使机器人可精确完成任务导向的抓取。

Result: 实验在20种家居物品、每种3种任务环境下，OVAL-Grasp在部件识别与分割准确率达到95%，实际抓取可操作区域的准确率为78.3%。在部分遮挡和杂乱场景中，选取正确部件成功率为80%。方法优于现有任务型抓取基线方法。

Conclusion: OVAL-Grasp不仅显著提升了机器人在实际复杂任务中的抓取效果，还能应对遮挡和视觉特征主导的场景。模块化设计和大模型的结合展示了很强的泛化和可解释性，是面向现实无结构环境抓取的有效方案。

Abstract: To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/

</details>


### [165] [NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities](https://arxiv.org/abs/2511.20848)
*Tasha Kim,Yingke Wang,Hanvit Cho,Alex Hodges*

Main category: cs.RO

TL;DR: 论文介绍了NOIR 2.0系统，这是一种通过脑电（EEG）信号控制机器人的智能接口，能够高效地把人类意图转化为机器人指令，实现日常任务的脑控操作。NOIR 2.0采用更快更准确的脑信号解码算法和少样本学习，显著提升了执行速度和用户适应性。


<details>
  <summary>Details</summary>
Motivation: 推动脑机接口在实际机器人控制中的应用，让人们能仅凭意念指挥机器人执行复杂日常任务；克服传统脑机接口解码慢、个性化适应难、训练样本需求大的难题。

Method: NOIR 2.0应用了先进的脑信号解码算法，提升了任务反应速度与准确性。采用few-shot learning及基础模型，实现机器人对个人用户意图的快速适应，通过15次示范完成模型学习（此前需单次）。

Result: 新系统缩短了任务完成时间达46%，整体人类操作时间减少65%，在少样本适应、新用户泛化等方面取得显著提升。

Conclusion: NOIR 2.0极大提升了脑机接口机器人的实用性和用户体验，在智能机器人、辅助设备等领域具有广阔应用前景。

Abstract: Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.

</details>


### [166] [ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation](https://arxiv.org/abs/2511.20887)
*Rui Yan,Jiajian Fu,Shiqi Yang,Lars Paulsen,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了ACE-F，一种兼具折叠性和力反馈的跨形态机器人远程操作系统，实现了高质量演示采集和支持多种机器人平台。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人远程操作平台存在缺乏力反馈、跨形态适应性差及系统便携性和易用性不足等问题，导致实际应用受限。

Method: 提出ACE-F系统，结合逆运动学和人机交互界面，实现便捷的演示数据采集。进一步，设计了集成PD控制与逆动力学的软控制管线，提升不同机器人类型的动作精度和安全性。通过将末端偏差解释为虚拟力信号，无需额外传感器实现跨平台的力反馈。

Result: 大量远程操作实验显示，ACE-F系统显著简化多种机器人操作，并使复杂操作如鼠标一般直观，还实现了高质量示范数据的高效采集。

Conclusion: ACE-F系统有效提升了远程操作平台的实用性和适用范围，为机器人演示学习和跨平台操作提供了新工具，且相关代码已开源。

Abstract: Teleoperation systems are essential for efficiently collecting diverse and high-quality robot demonstration data, especially for complex, contact-rich tasks. However, current teleoperation platforms typically lack integrated force feedback, cross-embodiment generalization, and portable, user-friendly designs, limiting their practical deployment. To address these limitations, we introduce ACE-F, a cross embodiment foldable teleoperation system with integrated force feedback. Our approach leverages inverse kinematics (IK) combined with a carefully designed human-robot interface (HRI), enabling users to capture precise and high-quality demonstrations effortlessly. We further propose a generalized soft-controller pipeline integrating PD control and inverse dynamics to ensure robot safety and precise motion control across diverse robotic embodiments. Critically, to achieve cross-embodiment generalization of force feedback without additional sensors, we innovatively interpret end-effector positional deviations as virtual force signals, which enhance data collection and enable applications in imitation learning. Extensive teleoperation experiments confirm that ACE-F significantly simplifies the control of various robot embodiments, making dexterous manipulation tasks as intuitive as operating a computer mouse. The system is open-sourced at: https://acefoldable.github.io/

</details>


### [167] [Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization](https://arxiv.org/abs/2511.20894)
*Vivek Pandey,Amirhossein Mollaei,Nader Motee*

Main category: cs.RO

TL;DR: 本文提出了两种快速且内存高效的特征选择算法，用于提升机器人视觉定位的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 面对视觉中大量冗余或低信息量的特征，全部处理会导致机器人定位过程中计算延迟严重，效率低下。因此有必要智能选择最有助于定位的特征，以提高运算效率和保持定位精度。

Method: 作者提出了两种新颖的特征选择算法，这些算法能够实时评估视觉特征的有用性，并减少计算和内存的开销。与现有方法相比，这些算法的时间和空间复杂度更低，能够动态筛选信息量大的特征参与定位计算。

Result: 所提方法在保证定位精度的情况下显著减少了特征处理的计算时间和内存消耗，实现了效率和性能的良好平衡。

Conclusion: 结果表明，所提算法能够有效提升机器人在未知环境下自主导航的实时定位效率和准确性，具有实际应用价值。

Abstract: Robot localization is a fundamental component of autonomous navigation in unknown environments. Among various sensing modalities, visual input from cameras plays a central role, enabling robots to estimate their position by tracking point features across image frames. However, image frames often contain a large number of features, many of which are redundant or uninformative for localization. Processing all features can introduce significant computational latency and inefficiency. This motivates the need for intelligent feature selection, identifying a subset of features that are most informative for localization over a prediction horizon. In this work, we propose two fast and memory-efficient feature selection algorithms that enable robots to actively evaluate the utility of visual features in real time. Unlike existing approaches with high computational and memory demands, the proposed methods are explicitly designed to reduce both time and memory complexity while achieving a favorable trade-off between computational efficiency and localization accuracy.

</details>


### [168] [Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy](https://arxiv.org/abs/2511.20906)
*Inkook Chun,Seungjae Lee,Michael S. Albergo,Saining Xie,Eric Vanden-Eijnden*

Main category: cs.RO

TL;DR: 该论文提出了一种难度感知的随机插值策略（DA-SIP），能让机器人控制器根据任务难度动态调整推理计算量，实现在保持性能的同时显著降低计算消耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流的方法在机器人操作和模仿学习任务中表现优异，但每一步都使用固定的计算预算，导致在简单任务上效率低下，在复杂任务上可能表现不足，因此需要一种能根据任务难度自适应调整计算量的策略。

Method: 提出了一种名为DA-SIP的框架，利用一个难度分类器分析当前观测，动态选择计算步数、求解器类型和微分方程的积分方式。该方法基于随机插值思想，统一了多种扩散和流策略的训练与推理模式。

Result: 在多个操控基准测试中，DA-SIP能够在保持任务成功率与高计算基线相当的情况下，实现2.6到4.4倍的总计算量减少。

Conclusion: 通过难度感知和动态计算分配，DA-SIP让生成式机器人控制系统更加高效、智能，能根据实际需求合理利用推理资源。

Abstract: Diffusion- and flow-based policies deliver state-of-the-art performance on long-horizon robotic manipulation and imitation learning tasks. However, these controllers employ a fixed inference budget at every control step, regardless of task complexity, leading to computational inefficiency for simple subtasks while potentially underperforming on challenging ones. To address these issues, we introduce Difficulty-Aware Stochastic Interpolant Policy (DA-SIP), a framework that enables robotic controllers to adaptively adjust their integration horizon in real time based on task difficulty. Our approach employs a difficulty classifier that analyzes observations to dynamically select the step budget, the optimal solver variant, and ODE/SDE integration at each control cycle. DA-SIP builds upon the stochastic interpolant formulation to provide a unified framework that unlocks diverse training and inference configurations for diffusion- and flow-based policies. Through comprehensive benchmarks across diverse manipulation tasks, DA-SIP achieves 2.6-4.4x reduction in total computation time while maintaining task success rates comparable to fixed maximum-computation baselines. By implementing adaptive computation within this framework, DA-SIP transforms generative robot controllers into efficient, task-aware systems that intelligently allocate inference resources where they provide the greatest benefit.

</details>


### [169] [AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios](https://arxiv.org/abs/2511.21053)
*Chenglizhao Chen,Shaofeng Liang,Runwei Guan,Xiaolou Sun,Haocheng Zhao,Haiyun Jiang,Tao Huang,Henghui Ding,Qing-Long Han*

Main category: cs.RO

TL;DR: 本文提出了AerialMind，这是首个面向无人机(UAV)场景的大规模基于自然语言的多目标跟踪（RMOT）基准数据集，并设计了创新的标注和跟踪方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的多目标跟踪研究主要局限于地面场景，限制了对大范围环境的感知和路径规划能力。随着无人机作为智能体平台的兴起，对具备自然语言交互能力的智能空中系统需求极大，但这方面缺乏公开数据集和方法。

Method: 1. 构建AerialMind数据集：研发半自动化协作标注助手COALA，提高标注效率并保证质量。2. 提出HawkEyeTrack(HETrack)方法，联合提升视觉-语言表征学习，增强无人机场景感知能力。

Result: 实验表明AerialMind数据集具有很高的挑战性，HETrack方法在该数据集上验证了其有效性，显著提升了基于自然语言的多目标跟踪表现。

Conclusion: AerialMind为UAV场景下的RMOT研究提供了标准化数据集和先进方法，为实现具备自然语言理解能力的空中智能体奠定了基础，对无人机智能感知和人机交互有重要推动作用。

Abstract: Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.

</details>


### [170] [Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry](https://arxiv.org/abs/2511.21083)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: cs.RO

TL;DR: 本文提出了一种通过轻量级强化学习（RL）策略减少视觉-惯性捆绑调整（VIBA）调用频率和强度的视觉-惯性里程计（VIO）方法，在保证精度的同时提升了效率与资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有VIO方法在效率和精度之间存在取舍：滤波方法高效但易漂移，优化方法精度高但计算资源消耗大，尤其VIBA对计算平台要求高。作者希望在不完全移除VIBA的情况下，降低其调用频率以适应资源受限环境。

Method: 方法创新点在于将视觉前端的激活时序及其输出权重设定视为序贯决策问题，并用轻量级RL代理进行求解。具体包含两个RL策略：Select Agent基于IMU数据智能决定VO流水线是否激活；Fusion Agent先以监督网络估算速度，再用RL策略自适应融合（p, v, q）状态。

Result: 在EuRoC MAV和TUM-VI数据集上，本文方法在精度-效率-内存消耗三者之间取得了优于现有GPU方案的综合表现：平均ATE最低、速度提升1.77倍、GPU内存消耗更小。同时，相较传统优化VIO，精度持平但计算负载明显降低。

Conclusion: 该方法证明通过引入RL调度，可以兼顾VIO系统的精度和计算资源开销，为移动端及资源受限平台的VIO应用提供了新的解决方案。

Abstract: Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.

</details>


### [171] [SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation](https://arxiv.org/abs/2511.21135)
*Ziyi Chen,Yingnan Guo,Zedong Chu,Minghua Luo,Yanfen Shen,Mingchao Sun,Junjun Hu,Shichao Xie,Kuan Yang,Pei Shi,Zhining Gu,Lu Liu,Honglin Han,Xiaolong Wu,Mu Xu,Yu Zhang*

Main category: cs.RO

TL;DR: 本文提出了SocialNav，一个具备强大社会意识的导航基础模型，利用新构建的SocNav大数据集和分阶段训练方案，实现了导航性能与社交合规性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管具身导航任务已取得一定进展，但如何让机器人等智能体遵守社会规范进行导航，仍然是未解决的重要难题。作者希望发展既能理解社会规范、又能执行合规路径的导航模型，提高现实场景中机器人的社会适应性。

Method: 提出SocialNav模型，采用层次化"脑-行为"结构；构建SocNav大数据集，包含700万样本，用于提供社会推理信号和多样专家导航轨迹；采用多阶段训练流程，先用模仿学习注入导航和社会规范知识，再用首个显式奖励社会合规行为的流式强化学习框架SAFE-GRPO精炼导航能力。

Result: SocialNav在多项标准上显著超越前沿方法，成功率提升38%，社会合规率提升46%，表现出色。

Conclusion: 利用大规模多样专家数据和创新强化学习方案，SocialNav显著提升了具身智能体的导航效果和社会合规性，对开放环境下机器人应用具有重要推动意义。

Abstract: Embodied navigation that adheres to social norms remains an open research challenge. Our \textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical "brain-action" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/

</details>


### [172] [Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning](https://arxiv.org/abs/2511.21149)
*Guoming Huang,Qingyi Zhou,Dianjing Liu,Shuai Zhang,Ming Zhou,Zongfu Yu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Maglev-Pentabot的磁悬浮系统，结合深度强化学习，实现了克重级非接触柔性操控，突破了现有主要局限于毫克级别的瓶颈，并具备良好的任务泛化和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前非接触操控技术多仅限于微观尺度和轻量目标，无法满足实际工业中对更大质量物体灵活操控的需求。本文旨在突破这一限制，扩展到克重级及更高的应用场景。

Method: 提出采用数值分析优化的电磁体排列方式，以拓展可控范围，并引入动作重映射方法以解决磁场强度非线性导致的样本稀疏问题，从而使深度强化学习控制器能够高效收敛。

Result: 实验结果显示，该系统可以灵活操控克重级物体，并能推广到未训练过的搬运任务。

Conclusion: Maglev-Pentabot为工业领域的非接触式大质量物体操控提供了新的参考范式，具备良好的灵活性、泛化性以及可扩展性，有望推动相关技术的实际应用。

Abstract: Non-contact manipulation has emerged as a transformative approach across various industrial fields. However, current flexible 2D and 3D non-contact manipulation techniques are often limited to microscopic scales, typically controlling objects in the milligram range. In this paper, we present a magnetic levitation system, termed Maglev-Pentabot, designed to address this limitation. The Maglev-Pentabot leverages deep reinforcement learning (DRL) to develop complex control strategies for manipulating objects in the gram range. Specifically, we propose an electromagnet arrangement optimized through numerical analysis to maximize controllable space. Additionally, an action remapping method is introduced to address sample sparsity issues caused by the strong nonlinearity in magnetic field intensity, hence allowing the DRL controller to converge. Experimental results demonstrate flexible manipulation capabilities, and notably, our system can generalize to transport tasks it has not been explicitly trained for. Furthermore, our approach can be scaled to manipulate heavier objects using larger electromagnets, offering a reference framework for industrial-scale robotic applications.

</details>


### [173] [MarketGen: A Scalable Simulation Platform with Auto-Generated Embodied Supermarket Environments](https://arxiv.org/abs/2511.21161)
*Xu Hu,Yiyang Feng,Junran Peng,Jiawei He,Liyi Chen,Chuanchen Luo,Xucheng Yin,Qing Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: 本文提出了MarketGen，一个可扩展的超市场景自动生成仿真平台，并提供了新的基准测试任务，以推动具身智能体在复杂商业环境中的研究。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人数据集和基准测试大多局限于家庭或台面环境，且任务周期较短，无法满足复杂商业环境下具身智能体的需求，因此需要更真实、多样和复杂的仿真平台及基准。

Method: MarketGen平台采用了新颖的基于智能体的程序化内容生成（PCG）框架，支持多模态输入（文本与参考图片），结合真实设计原则自动生成完整逼真的超市环境，并集成了1100+超市货品与可参数化设施的多样3D资产库。基于该平台，作者设计了两个长时序、复杂度高的基准任务：1）收银员的结账卸货（长时序台面任务）；2）导购员的货架取物（复杂移动操作任务）。

Result: 通过大量实验，包括模块化智能体系统的部署和仿真到现实的迁移，验证了平台和基准任务的有效性和可行性。

Conclusion: MarketGen为复杂商业场景下的具身AI研究提供了高度自动化且丰富的平台和全新基准，有助于加速相关领域的技术突破和应用落地。

Abstract: The development of embodied agents for complex commercial environments is hindered by a critical gap in existing robotics datasets and benchmarks, which primarily focus on household or tabletop settings with short-horizon tasks. To address this limitation, we introduce MarketGen, a scalable simulation platform with automatic scene generation for complex supermarket environments. MarketGen features a novel agent-based Procedural Content Generation (PCG) framework. It uniquely supports multi-modal inputs (text and reference images) and integrates real-world design principles to automatically generate complete, structured, and realistic supermarkets. We also provide an extensive and diverse 3D asset library with a total of 1100+ supermarket goods and parameterized facilities assets. Building on this generative foundation, we propose a novel benchmark for assessing supermarket agents, featuring two daily tasks in a supermarket: (1) Checkout Unloading: long-horizon tabletop tasks for cashier agents, and (2) In-Aisle Item Collection: complex mobile manipulation tasks for salesperson agents. We validate our platform and benchmark through extensive experiments, including the deployment of a modular agent system and successful sim-to-real transfer. MarketGen provides a comprehensive framework to accelerate research in embodied AI for complex commercial applications.

</details>


### [174] [Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation](https://arxiv.org/abs/2511.21169)
*Kaiyan Xiao,Zihan Xu,Cheng Zhe,Chengju Liu,Qijun Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的人形机器人联合位移操控方法，采用分离的三阶段训练策略，应对工业高负载场景下的灵巧性与力交互要求。


<details>
  <summary>Details</summary>
Motivation: 传统方法多关注灵巧操作，难以满足工业场景下对机器人灵巧性和主动力交互的双重需求。

Method: 提出了基于强化学习的训练框架，包括上半身策略、下半身策略和delta-command策略三阶段解耦训练。通过嵌入前向运动学先验的启发式奖励函数加速上半身策略收敛，并为下半身设计了基于力的课程学习策略以主动调节环境交互力。

Result: 上半身策略训练更快，性能更优；下半身能够更主动且有效地实现与环境的力交互，提高了机器人在工业高负载任务中的表现。

Conclusion: 所提方法有效提升了人形机器人在工业场景下的联合位移操控能力，兼顾了灵巧性与力学交互，具有实际应用潜力。

Abstract: Humanoid robots, with their human-like morphology, hold great potential for industrial applications. However, existing loco-manipulation methods primarily focus on dexterous manipulation, falling short of the combined requirements for dexterity and proactive force interaction in high-load industrial scenarios. To bridge this gap, we propose a reinforcement learning-based framework with a decoupled three-stage training pipeline, consisting of an upper-body policy, a lower-body policy, and a delta-command policy. To accelerate upper-body training, a heuristic reward function is designed. By implicitly embedding forward kinematics priors, it enables the policy to converge faster and achieve superior performance. For the lower body, a force-based curriculum learning strategy is developed, enabling the robot to actively exert and regulate interaction forces with the environment.

</details>


### [175] [Dual Preintegration for Relative State Estimation](https://arxiv.org/abs/2511.21189)
*Ruican Xia,Hailong Pei*

Main category: cs.RO

TL;DR: 该论文提出了一种双重预积分（dual preintegration）新方法，用于改善两移动体六自由度相对状态估计的精度，特别是在存在大旋转和大距离时表现更好。通过仿真和真实实验，验证其优于现有主流算法。


<details>
  <summary>Details</summary>
Motivation: 现有的相对状态估计方法在参照平台发生非线性大旋转和平台间距离较远时，精度严重下降，特别是在采用线性化运动模型时线性化误差积累，导致漂移。在虚拟现实（VR）等应用中表现为跟踪误差大。

Method: 基于IMU预积分思想，提出了“双重预积分”观测，将双平台的IMU预积分结果用作连续相对状态的运动学约束，支持高效地重新线性化。同时进行了状态可观测性分析，并将方法在仿真和真实场景下与SOTA算法做了对比。

Result: 结果表明，相较于现有SOTA算法，所提方法在多次大角度非线性旋转、长距离、带有偏置观测等场景下表现出更高的估计精度和鲁棒性。无论是仿真还是VR控制器现实跟踪测试中，性能均领先。

Conclusion: 双重预积分方法能有效抑制因线性化误差导致的漂移，提高6-DoF运动的相对状态估计精度，对需要高精度跟踪的VR等应用具有现实意义。

Abstract: Relative State Estimation perform mutually localization between two mobile agents undergoing six-degree-of-freedom motion. Based on the principle of circular motion, the estimation accuracy is sensitive to nonlinear rotations of the reference platform, particularly under large inter-platform distances. This phenomenon is even obvious for linearized kinematics, because cumulative linearization errors significantly degrade precision. In virtual reality (VR) applications, this manifests as substantial positional errors in 6-DoF controller tracking during rapid rotations of the head-mounted display. The linearization errors introduce drift in the estimate and render the estimator inconsistent. In the field of odometry, IMU preintegration is proposed as a kinematic observation to enable efficient relinearization, thus mitigate linearized error. Building on this theory, we propose dual preintegration, a novel observation integrating IMU preintegration from both platforms. This method serves as kinematic constraints for consecutive relative state and supports efficient relinearization. We also perform observability analysis of the state and analytically formulate the accordingly null space. Algorithm evaluation encompasses both simulations and real-world experiments. Multiple nonlinear rotations on the reference platform are simulated to compare the precision of the proposed method with that of other state-of-the-art (SOTA) algorithms. The field test compares the proposed method and SOTA algorithms in the application of VR controller tracking from the perspectives of bias observability, nonlinear rotation, and background texture. The results demonstrate that the proposed method is more precise and robust than the SOTA algorithms.

</details>


### [176] [Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching](https://arxiv.org/abs/2511.21203)
*Fuyuki Tokuda,Akira Seino,Akinari Kobayashi,Kai Tang,Kazuhiro Kosuge*

Main category: cs.RO

TL;DR: 本文提出了一种将织物片精确对齐叠放的新方法，结合了双臂机器人、灰度相机和基于Transformer的视觉控制技术，实现了高效、零样本的织物表面纹理对齐。


<details>
  <summary>Details</summary>
Motivation: 织物自动对齐与叠放是智能制造和柔性材料处理领域的挑战，传统方法在纹理变化、多样材质下难以适应。研究动机在于提升织物对齐的准确性与普适性，减小人工干预。

Method: 方法采用了双臂机器人的阻抗控制与Transformer视觉伺服相结合，引入了预训练骨干网络和新设计的DEAM（差分提取注意模块），并通过模拟图像进行训练，从而实现端到端的对齐控制。

Result: 实验结果显示，系统在无需预见具体纹理的情况下，能准确将不同纹理的织物片叠放且纹理精确对齐，展示了良好的实际泛化能力。

Conclusion: 所提出方法无需针对具体织物纹理进行训练，可直接应用于实际场景，表现出高度的普适性与有效性，为柔性物品的自动化处理提供了技术基础。

Abstract: In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.

</details>


### [177] [Sampling-Based Optimization with Parallelized Physics Simulator for Bimanual Manipulation](https://arxiv.org/abs/2511.21264)
*Iryna Hurova,Alinjar Dan,Karl Kruusamäe,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的优化框架，利用GPU加速的物理模拟器高效解决复杂的双臂操作任务，尤其针对有障碍的环境，并在多个具有挑战性的基准测试任务上取得了实时性能和较强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端学习成为双臂操作主流方法，但泛化能力较差，难以应对新颖或复杂场景（如杂乱环境），因此需要探索新的解决策略提升算法的适应性和实用性。

Method: 作者构建了一个采样优化框架，将自定义的模型预测路径积分控制（MPPI）算法与GPU加速的MuJoCo物理仿真结合，并设计针对具体任务的代价函数来指导算法，能够高效评估机器人与物体的交互效果。

Result: 该方法可以解决带静态障碍物的复杂双臂操作任务，在提升任务难度的PerAct^2基准上（如带障碍物的小球搬运），表现出实时性，并能顺利实现从仿真到真实机器的迁移。

Conclusion: 该方法不仅能在商品级GPU实现实时控制，还具备较高的采样效率和鲁棒性，为双臂操作在复杂环境下的泛化和实用化提供了一条可行路径。

Abstract: In recent years, dual-arm manipulation has become an area of strong interest in robotics, with end-to-end learning emerging as the predominant strategy for solving bimanual tasks. A critical limitation of such learning-based approaches, however, is their difficulty in generalizing to novel scenarios, especially within cluttered environments. This paper presents an alternative paradigm: a sampling-based optimization framework that utilizes a GPU-accelerated physics simulator as its world model. We demonstrate that this approach can solve complex bimanual manipulation tasks in the presence of static obstacles. Our contribution is a customized Model Predictive Path Integral Control (MPPI) algorithm, \textbf{guided by carefully designed task-specific cost functions,} that uses GPU-accelerated MuJoCo for efficiently evaluating robot-object interaction. We apply this method to solve significantly more challenging versions of tasks from the PerAct$^{2}$ benchmark, such as requiring the point-to-point transfer of a ball through an obstacle course. Furthermore, we establish that our method achieves real-time performance on commodity GPUs and facilitates successful sim-to-real transfer by leveraging unique features within MuJoCo. The paper concludes with a statistical analysis of the sample complexity and robustness, quantifying the performance of our approach. The project website is available at: https://sites.google.com/view/bimanualakslabunitartu .

</details>


### [178] [Improvement of Collision Avoidance in Cut-In Maneuvers Using Time-to-Collision Metrics](https://arxiv.org/abs/2511.21280)
*Jamal Raiyn*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度学习和碰撞时间（TTC）指标的新型避碰策略，用于自动驾驶汽车应对“插队”情景。该系统能够更有效地预测碰撞风险，并制定合理的规避措施。


<details>
  <summary>Details</summary>
Motivation: 插队情景极具挑战性，传统基于TTC的避碰系统在此类情况下表现有限，亟需提升自动驾驶车辆在复杂交通状况下的安全性和反应能力。

Method: 将深度学习方法与TTC计算融合，分析环境数据，预测可能的碰撞风险，并基于预测结果制定相应的规避动作。与传统单一TTC方法对比，探究其效果差异。

Result: 结合深度学习的TTC避碰系统在插队情景下能够更准确地预测碰撞，并作出更恰当的规避决策，优于传统方法。

Conclusion: 通过深度学习与TTC指标的结合，可显著提升自动驾驶车辆在复杂插队情景下的避碰能力，有助于自动驾驶技术的进一步发展。

Abstract: This paper proposes a new strategy for collision avoidance system leveraging Time-to-Collision (TTC) metrics for handling cut-in scenarios, which are particularly challenging for autonomous vehicles (AVs). By integrating a deep learning with TTC calculations, the system predicts potential collisions and determines appropriate evasive actions compared to traditional TTC -based approaches.

</details>


### [179] [Neural NMPC through Signed Distance Field Encoding for Collision Avoidance](https://arxiv.org/abs/2511.21312)
*Martin Jacquet,Marvin Harms,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经网络的非线性模型预测控制（NMPC）框架，使空中机器人能在未知环境下无地图自主避障导航。作者通过深度神经网络从单帧距离图像编码环境的有用信息，并用于碰撞规避，有效提升了在实际复杂环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 在无需构建地图、环境未知且传感受限的场景中，现有方法往往难以实现高效可靠的避障导航。因此，作者旨在开发一种新颖、能直接利用机载距离感知进行安全自主导航的方法，以解决传统基于地图或手工特征方法的局限。

Method: 作者提出了一套两级级联的神经网络结构：首阶段以卷积编码器将距离图像编码成低维潜在向量，次阶段以多层感知机（MLP）近似生成空间有符号距离函数（SDF），后者明确地用来表示避障约束；该约束整合进速度跟踪的NMPC控制器中，并分析其理论可行性与稳定性。随后，作者进行开环与闭环的仿真和实地实验，包括消融研究、与两种先进方法对比，以及对里程计漂移的鲁棒性评估。

Result: 仿真实验显示，该神经网络结构的避障能力与鲁棒性优于两种主流局部导航方法；现实世界森林环境实验中，该方法能有效处理杂乱场景里的主动避障问题，并且在有位置估计漂移等不利条件下依然稳定可靠。

Conclusion: 本文提出的方法可在未知、复杂环境下实现可靠的无地图空中机器人自主导航和实时避障，有效克服了位置信息漂移和环境干扰，展示了现实应用潜力。

Abstract: This paper introduces a neural Nonlinear Model Predictive Control (NMPC) framework for mapless, collision-free navigation in unknown environments with Aerial Robots, using onboard range sensing. We leverage deep neural networks to encode a single range image, capturing all the available information about the environment, into a Signed Distance Function (SDF). The proposed neural architecture consists of two cascaded networks: a convolutional encoder that compresses the input image into a low-dimensional latent vector, and a Multi-Layer Perceptron that approximates the corresponding spatial SDF. This latter network parametrizes an explicit position constraint used for collision avoidance, which is embedded in a velocity-tracking NMPC that outputs thrust and attitude commands to the robot. First, a theoretical analysis of the contributed NMPC is conducted, verifying recursive feasibility and stability properties under fixed observations. Subsequently, we evaluate the open-loop performance of the learning-based components as well as the closed-loop performance of the controller in simulations and experiments. The simulation study includes an ablation study, comparisons with two state-of-the-art local navigation methods, and an assessment of the resilience to drifting odometry. The real-world experiments are conducted in forest environments, demonstrating that the neural NMPC effectively performs collision avoidance in cluttered settings against an adversarial reference velocity input and drifting position estimates.

</details>


### [180] [Hybrid Control for Robotic Nut Tightening Task](https://arxiv.org/abs/2511.21366)
*Dmitri Kovalenko*

Main category: cs.RO

TL;DR: 该论文提出了一种用于串联机械臂的自主机器人拧螺母系统，能够更快且更安全地完成螺母紧固任务。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人螺母紧固方式在速度和安全性上存在瓶颈，且对初始条件变化的适应性较差，因此需要一种更高效且稳健的方法。

Method: 设计了一套基于分层运动原语的规划器以及力位切换控制策略，可以根据任务需求在位置控制和力控制间切换，适应不同的操作过程。

Result: 仿真实验表明，该系统对初始条件变化具有很强的鲁棒性。在实际拧螺丝任务中，系统的速度比基准方法快14%，且对操作物体的接触力小40倍。

Conclusion: 所提出的系统能够显著提升螺母紧固任务的效率与安全性，并且其方案已开源，便于研究社区进一步研究和应用。

Abstract: An autonomous robotic nut tightening system for a serial manipulator equipped with a parallel gripper is proposed. The system features a hierarchical motion-primitive-based planner and a control-switching scheme that alternates between force and position control. Extensive simulations demonstrate the system's robustness to variance in initial conditions. Additionally, the proposed controller tightens threaded screws 14% faster than the baseline while applying 40 times less contact force on manipulands. For the benefit of the research community, the system's implementation is open-sourced.

</details>


### [181] [$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion](https://arxiv.org/abs/2511.21542)
*Zhihao Zhan,Jiaying Zhou,Likui Zhang,Qinhan Lv,Hao Liu,Jusheng Zhang,Weizheng Li,Ziliang Chen,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的针对机器人感知、语言理解与控制生成的一体化模型E0，通过离散扩散框架提升操作的泛化性和精细控制表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在多任务、场景、摄像头视角变化下的泛化能力有限，且往往动作输出粗糙或不稳定。作者希望解决这些现有模型难以泛化和精细控制的问题。

Method: 作者提出E0模型，将动作生成问题表述为对量化动作tokens的逐步去噪过程，属于“离散扩散”范式。该方法一方面便于和预训练的视觉-语言（VLM/VLA）骨干网络结合，实现强语义条件控制；另一方面，离散动作更贴合真实机器人硬件对动作信号的离散性需求。此外，作者还提出摄像头扰动增强技术以提升模型对视角变化的鲁棒性。

Result: 在LIBERO、VLABench和ManiSkill等14个多样环境中，E0均取得了SOTA表现，平均超越主流强基线10.7%；在真实机器人（Franka机械臂）实验中，E0展现出高精度、强鲁棒性和良好可迁移性。

Conclusion: E0模型显著提升了VLA任务的泛化能力与动作精细控制，证明了离散扩散方法在通用化VLA策略学习上的潜力。

Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.

</details>


### [182] [VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation](https://arxiv.org/abs/2511.21557)
*Hui Zhou,Siyuan Huang,Minxing Li,Hao Zhang,Lue Fan,Shaoshuai Shi*

Main category: cs.RO

TL;DR: 本文提出了一种结合机械夹爪与真空吸盘的低成本硬件设计，实现了单一末端执行器的双模式操作能力，显著提升了机器人完成复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作（VLA）模型多采用并联两指夹爪作为操作器，但这种夹爪在擦拭玻璃、无把手抽屉开启等任务上表现有限，主要因其接触面积不足或缺乏附着力。为此，作者旨在突破传统夹爪的操作局限，提升机器人在现实世界任务中的适应性和能力。

Method: 作者设计了一种集成式末端执行器，将机械两指夹爪与真空吸盘结合，支持灵活切换或协同操作两种模式。该系统被集成于两套先进的VLA框架（DexVLA和Pi0）中，通过实验验证其效率与实用性。

Result: 实验结果表明，采用新型混合末端执行器的机器人，能够出色完成多项复杂而传统夹爪难以胜任的任务，展现了操作能力的大幅拓展。

Conclusion: 该集成化低成本硬件设计显著提升了VLA机器人在实际环境中解决多样复杂任务的能力，所有硬件与控制系统也将开源，促进后续研究和应用拓展。

Abstract: Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.

</details>


### [183] [Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving](https://arxiv.org/abs/2511.21584)
*Haohong Lin,Yunzhi Zhang,Wenhao Ding,Jiajun Wu,Ding Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种模型驱动的策略自适应(MPA)框架，通过生成多样化的对比轨迹和多步Q值评估，提升端到端自动驾驶模型在部署时的鲁棒性和安全性。实验表明，在nuScenes基准上该方法显著提升了模型在多种场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型虽然在开放环评估中表现良好，但在实际（闭环）部署中容易出现误差累积，泛化能力差，导致安全风险。这一现象需要通过提升模型在现实复杂场景下的稳健性和适应性来解决。

Method: 提出了MPA框架：1) 利用一致性仿真引擎生成多样的、真实几何约束下的对比轨迹，扩展训练数据；2) 基于这些数据，训练扩散模型为核心的策略自适应器优化预训练策略输出；3) 多步Q值模型用于评估各候选轨迹的长期效果，最终在推断阶段选择期望效用最高的轨迹。

Result: 在nuScenes基准和真实闭环仿真中，MPA在域内、域外和安全关键场景下均显著优于传统方法。同时，实验探究了生成对比数据规模和推理阶段指导策略对效果的影响。

Conclusion: MPA框架有效缓解了端到端自动驾驶中的串联误差与泛化问题，提升了模型的鲁棒性与安全性，并为未来自动驾驶系统的部署与性能优化提供了新的思路。

Abstract: End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.

</details>


### [184] [Uncertainty Quantification for Visual Object Pose Estimation](https://arxiv.org/abs/2511.21666)
*Lorenzo Shaikewitz,Charis Georgiou,Luca Carlone*

Main category: cs.RO

TL;DR: 本文提出了一种无需分布假设的姿态估计不确定性界限方法SLUE，可为单目对象姿态估计提供具有严格概率性保证的椭球形不确定性界，且经实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有姿态估计虽然广泛研究，但缺乏统计上严格、无特定分布假设的不确定性量化方法。而实际应用中的机器人控制与规划依赖于可靠的不确定度估计，因此亟需分布无关的方法。

Method: 提出SLUE（S-Lemma Uncertainty Estimation）方法，仅需2D语义关键点像素检测的高概率噪声界，通过S-lemma得到单一的椭球不确定性界，且无初值依赖。此外引入了sum-of-squares松弛序列，理论上可收敛至最小体积椭球界。

Result: 在两个姿态估计数据集及真实无人机跟踪场景中，SLUE获得的平移不确定性界明显小于现有方法，姿态界亦具竞争力。方法可分解得独立的平移与朝向不确定界，且实用性强。

Conclusion: SLUE为单目姿态估计提供了理论与实践均优的分布无关不确定性界限，显著提升了下游任务的安全与鲁棒性。

Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.

</details>


### [185] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: 本文提出了一个名为TraceGen的世界模型，通过将机器人的学习问题转化为符号化的3D空间轨迹（trace-space）表示，实现了跨平台、跨环境以及跨任务的数据迁移，显著提高了小样本场景下的机器人学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习在新平台、新场景及新任务下通常依赖大量示范数据。网络上虽然有许多不同主体（如人类或不同机器人）的视频，但因为具身差异、摄像机视角差异和环境差异，直接利用这些视频学习效果不佳，因此亟需一种能有效整合异构来源视频的统一表示方法。

Method: 作者提出一种统一的3D“trace-space”轨迹表示，将场景级操作转化为紧凑的空间符号轨迹，跨越具身、环境和任务的差异。基于此表示构建TraceGen模型，在轨迹空间预测未来移动行为，脱离了对外观的依赖，保留了几何结构。同时，设计了TraceForge数据管道，将异构的人类及机器人视频转为一致的3D轨迹，从而用于大规模预训练。最终使用仅少量目标机器人视频即可高效适应和学习新任务。

Result: 预训练后的TraceGen拥有良好的3D运动先验。在四项任务上，仅需5段目标机器人视频，即可获得80%的成功率，推理速度比主流视频世界模型快50-600倍。在极端情况下，只用5段通过手机手持拍摄的、未经校准的人类演示视频，TraceGen在实际机器人上仍可达到67.5%的成功率。

Conclusion: TraceGen通过抽象轨迹空间的表征与泛化预训练，显著解决了机器人在小样本、异构数据下的自适应学习难题，实现了高效、跨具身和跨环境的操作学习，并显著优于依赖像素空间的以往方法。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>
