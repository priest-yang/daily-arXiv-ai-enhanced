{"id": "2602.20200", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20200", "abs": "https://arxiv.org/abs/2602.20200", "authors": ["Zaijing Li", "Bing Hu", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Pengwei Xie", "Jianye Hao", "Liqiang Nie"], "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation", "comment": "17 pages, 8 figures", "summary": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86OptimusVLA\uff0c\u4e00\u79cd\u5177\u6709\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\uff08GPM\uff09\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\uff08LCM\uff09\u7684\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u9879\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5df2\u7ecf\u5360\u636e\u4e3b\u6d41\uff0c\u4f46\u5176\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u548c\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c1\uff09\u52a8\u4f5c\u751f\u6210\u7684\u521d\u59cb\u566a\u58f0\u5206\u5e03\u4e0e\u76ee\u6807\u5206\u5e03\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u5197\u957f\u4e14\u6613\u4ea7\u751f\u4e0d\u53ef\u884c\u52a8\u4f5c\uff1b2\uff09\u53ea\u8003\u8651\u5f53\u524d\u89c2\u6d4b\uff0c\u5ffd\u89c6\u5386\u53f2\u5e8f\u5217\u7ea6\u675f\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u4efb\u52a1\u8fdb\u5c55\u548c\u8f68\u8ff9\u4e00\u81f4\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51faOptimusVLA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1\uff09\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\uff08GPM\uff09\uff1a\u7528\u4ece\u8bed\u4e49\u76f8\u4f3c\u8f68\u8ff9\u4e2d\u68c0\u7d22\u7684\u4efb\u52a1\u5148\u9a8c\u66ff\u6362\u9ad8\u65af\u566a\u58f0\uff0c\u7f29\u77ed\u751f\u6210\u8def\u5f84\u5e76\u51cf\u5c11\u6a21\u578b\u6c42\u89e3\u6210\u672c\uff1b2\uff09\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\uff08LCM\uff09\uff1a\u52a8\u6001\u5efa\u6a21\u5df2\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff0c\u63a8\u65ad\u4efb\u52a1\u8fdb\u5c55\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u4fdd\u8bc1\u8f68\u8ff9\u7684\u65f6\u5e8f\u8fde\u8d2f\u4e0e\u5149\u6ed1\u3002", "result": "OptimusVLA\u5728\u4e09\u4e2a\u6a21\u62df\u57fa\u51c6\uff08LIBERO\u3001CALVIN\u3001RoboTwin 2.0 Hard\uff09\u4e0a\u5747\u5927\u5e45\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5728LIBERO\u4e0a\u5e73\u5747\u6210\u529f\u738798.6%\uff0c\u5728CALVIN\u4e0a\u6bd4pi_0\u63d0\u534713.5%\uff0c\u5728RoboTwin 2.0 Hard\u4e0a\u5e73\u5747\u6210\u529f\u738738%\u3002\u5728\u5b9e\u9645\u673a\u5668\u4eba\u8bc4\u6d4b\u4e2d\uff0cOptimusVLA\u5728\u6cdb\u5316\u53ca\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u5206\u522b\u6bd4pi_0\u9ad842.9%\u548c52.4%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.9\u500d\u3002", "conclusion": "OptimusVLA\u901a\u8fc7\u53cc\u8bb0\u5fc6\u6a21\u5757\u6709\u6548\u6539\u5584\u4e86\u5206\u5c42VLA\u6a21\u578b\u63a8\u7406\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u8303\u5f0f\u4e0e\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.20215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20215", "abs": "https://arxiv.org/abs/2602.20215", "authors": ["Jiyuan Zhao", "Zhengyu Shi", "Wentong Tian", "Tianliang Yao", "Dong Liu", "Tao Liu", "Yizhe Wu", "Peng Qi"], "title": "Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 3 figures, 3 tables", "summary": "Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u8f85\u52a9\u7ecf\u76ae\u51a0\u72b6\u52a8\u8109\u4ecb\u5165\u6cbb\u7597\uff08PCI\uff09\u8def\u5f84\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4e8c\u7ef4\u8840\u7ba1\u9020\u5f71\uff08DSA\uff09\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u8def\u5f84\u8bc6\u522b\u4e0e\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u4eba\u7f3a\u4e4f\u533b\u751f\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u57282D DSA\u56fe\u50cf\u4e0b\uff0c\u8def\u5f84\u89c4\u5212\u5bb9\u6613\u53d7\u6295\u5f71\u5931\u771f\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u8840\u7ba1\u5206\u53c9\u5904\u5bb9\u6613\u4ea7\u751f\u6a21\u7cca\u548c\u6b67\u4e49\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u7a7a\u95f4\u5750\u6807\u6ce8\u610f\u529b\u6b63\u5219\u5316\u7684UNet\uff08SCAR-UNet\uff09\u8fdb\u884c\u51a0\u72b6\u52a8\u8109\u8840\u7ba1\u5206\u5272\u5e76\u63d0\u53d6\u4e2d\u5fc3\u7ebf\u4e0e\u5206\u53c9\u70b9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u878d\u5408\u8840\u7ba1\u51e0\u4f55\u7279\u5f81\uff0c\u57fa\u4e8e\u8840\u7ba1\u56fe\u7ed3\u6784\u63a8\u7406\uff0c\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u533a\u5206\u771f\u5b9e\u5206\u53c9\u548c\u6295\u5f71\u4f2a\u4ea4\u53c9\u3002", "result": "\u5728\u4e34\u5e8aDSA\u6570\u636e\u96c6\u4e0a\uff0cSCAR-UNet\u5206\u5272Dice\u7cfb\u6570\u8fbe93.1%\uff1bGAT\u65b9\u6cd5\u8def\u5f84\u5224\u522b\u6210\u529f\u738795.0%\uff0c\u76ee\u6807\u5230\u8fbe\u738790.0%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6700\u77ed\u8def\u5f84\uff0860.0%\u548c55.0%\uff09\u53ca\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0875.0%\u548c70.0%\uff09\u3002\u673a\u5668\u4eba\u5e73\u53f0\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u5b9e\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u673a\u5668\u4ebaPCI\u8def\u5f84\u89c4\u5212\u4e2d\u5927\u5e45\u51cf\u5c11\u4e862D\u5f71\u50cf\u5f15\u8d77\u7684\u8def\u5f84\u6b67\u4e49\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u4e0e\u8def\u5f84\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff0c\u5177\u5907\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.20216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20216", "abs": "https://arxiv.org/abs/2602.20216", "authors": ["Hao Wang", "Tianliang Yao", "Bo Lu", "Zhiqiang Pei", "Liu Dong", "Lei Ma", "Peng Qi"], "title": "Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 5 figures, 1 table", "summary": "Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5728\u7ebf\u4e13\u5bb6\u6821\u6b63\u7684\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u5bfc\u7ba1\u7a7f\u8d8a\u8840\u7ba1\u5206\u53c9\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u5bfc\u822a\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5bfc\u7ba1\u81ea\u4e3b\u63a7\u5236\u4e2d\u9762\u4e34\u7a00\u758f\u5956\u52b1\u8bbe\u8ba1\u548c\u4f9d\u8d56\u9759\u6001\u8840\u7ba1\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u548c\u96be\u4ee5\u9002\u5e94\u624b\u672f\u4e2d\u7684\u53d8\u5316\u3002\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5bf9\u4e8e\u63d0\u5347\u81ea\u4e3b\u624b\u672f\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6240\u63d0\u6846\u67b6\u5305\u62ec\u4e09\u5927\u6a21\u5757\uff1a\uff081\uff09\u57fa\u4e8e\u5206\u5272\u7684\u5b9e\u65f6\u4f4d\u59ff\u4f30\u8ba1\u5b9e\u73b0\u7cbe\u786e\u53cd\u9988\uff1b\uff082\uff09\u57fa\u4e8e\u6a21\u7cca\u63a7\u5236\u5668\u7684\u5206\u53c9\u611f\u77e5\u5bfc\u7ba1\u65b9\u5411\u8c03\u6574\uff1b\uff083\uff09\u878d\u5408\u4e13\u5bb6\u5148\u9a8c\u7684\u5956\u52b1\u751f\u6210\u5668\uff0c\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002\u901a\u8fc7\u5f15\u5165\u5728\u7ebf\u4e13\u5bb6\u6821\u6b63\uff0c\u63d0\u5347\u4e86\u590d\u6742\u8840\u7ba1\u7ed3\u6784\u4e2d\u7684\u7b56\u7565\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u673a\u5668\u4eba\u5e73\u53f0\u548c\u900f\u660e\u8840\u7ba1\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700123\u8f6e\u8bad\u7ec3\u5373\u53ef\u6536\u655b\uff0c\u8bad\u7ec3\u6548\u7387\u6bd4\u57fa\u7ebf Soft Actor-Critic (SAC) \u7b97\u6cd5\u63d0\u534725.9%\uff0c\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u964d\u4f4e\u5230\u57fa\u7ebf\u768483.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u81ea\u4e3b\u5bfc\u7ba1\u7a7f\u884c\u590d\u6742\u8840\u7ba1\u5206\u53c9\u7ed3\u6784\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4e3a\u8fdc\u7a0b\u673a\u5668\u4eba\u4ecb\u5165\u624b\u672f\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6280\u672f\u8def\u5f84\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u89e3\u5256\u53d8\u5f02\u60c5\u51b5\u3002"}}
{"id": "2602.20219", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20219", "abs": "https://arxiv.org/abs/2602.20219", "authors": ["Guanting Shen", "Zi Tian"], "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction", "comment": "Preprint currently under revision", "summary": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u5c06\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3001\u8bed\u97f3\u5904\u7406\u4e0e\u6a21\u7cca\u903b\u8f91\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7cbe\u51c6\u3001\u9002\u5e94\u6027\u5f3a\u7684\u673a\u68b0\u81c2\u63a7\u5236\uff0c\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\u8fbe75%\u3002", "motivation": "\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u51c6\u786e\u7406\u89e3\u4eba\u7684\u610f\u56fe\u59cb\u7ec8\u662f\u4e00\u5927\u6311\u6218\uff0c\u4e5f\u662f\u5b9e\u73b0\u81ea\u7136\u534f\u4f5c\u7684\u5173\u952e\u3002\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u6cd5\u5728\u7406\u89e3\u8bed\u97f3\u4e0e\u89c6\u89c9\u573a\u666f\u3001\u89c4\u5212\u52a8\u4f5c\u7b49\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u547d\u4ee4\u7684\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001HRI\u7cfb\u7edf\uff0c\u5c06Florence-2\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0cLlama 3.1\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff0cWhisper\u7528\u4e8e\u8bed\u97f3\u8bc6\u522b\uff0c\u5e76\u878d\u5408\u6a21\u7cca\u903b\u8f91\u8fdb\u884c\u6307\u4ee4\u89e3\u6790\u4e0e\u4efb\u52a1\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u8bed\u97f3\u9a71\u52a8\u7684\u7269\u4f53\u64cd\u4f5c\u3002\u7cfb\u7edf\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\u8fbe\u523075%\uff0c\u663e\u793a\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u6240\u63d0\u51fa\u7684\u67b6\u6784\u4e0d\u4ec5\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u76f4\u89c2\u6027\uff0c\u8fd8\u4e3a\u4eca\u540e\u66f4\u590d\u6742\u7684\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u5960\u5b9a\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u57fa\u7840\uff0c\u4e3a\u8bed\u97f3\u4e0e\u89c6\u89c9-\u8bed\u8a00\u8054\u5408\u5904\u7406\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u9645\u8def\u5f84\u3002"}}
{"id": "2602.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20162", "abs": "https://arxiv.org/abs/2602.20162", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Phillip Miao", "Zilun Zhang", "Haozhan Shen", "Ruizhe Zhu", "Jianwei Yin"], "title": "Talking to Yourself: Defying Forgetting in Large Language Models", "comment": null, "summary": "Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.\n  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSA-SFT\u7684\u81ea\u589e\u5f3a\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u751f\u6210\u5bf9\u8bdd\u6570\u636e\uff0c\u4e0e\u4efb\u52a1\u6570\u636e\u6df7\u5408\u8bad\u7ec3\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u4efb\u52a1\u5185\u8868\u73b0\u3002\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u989d\u5916\u8c03\u53c2\uff0c\u7b80\u4fbf\u9ad8\u6548\u3002", "motivation": "\u5728\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bfc\u81f4\u6a21\u578b\u539f\u6709\u7684\u901a\u7528\u80fd\u529b\u53d7\u635f\u3002\u5f53\u524d\u5e38\u7528\u7684\u6280\u672f\uff08\u5982\u51bb\u7ed3\u5c42\u548c\u5916\u90e8\u6570\u636e\u6df7\u5408\uff09\u4ecd\u7136\u6709\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u3001\u66f4\u7b80\u6d01\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SA-SFT\u65b9\u6cd5\u8981\u6c42\u6a21\u578b\u5728\u5fae\u8c03\u524d\u5148\u81ea\u751f\u6210\u5927\u91cf\u81ea\u95ee\u81ea\u7b54\u5f62\u5f0f\u7684\u6570\u636e\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u81ea\u751f\u6210\u6570\u636e\u4e0e\u4efb\u52a1\u6570\u636e\u6df7\u5408\uff0c\u7528\u5e38\u89c4\u6d41\u7a0b\u5fae\u8c03\uff0c\u65e0\u9700\u6539\u53d8\u4f18\u5316\u7b97\u6cd5\u6216\u8bad\u7ec3\u65e5\u7a0b\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u6216\u7279\u6b8a\u8c03\u53c2\u3002", "result": "\u572850\u4e2a\u8bc4\u6d4b\u573a\u666f\u4e2d\uff0cSA-SFT\u7684\u8868\u73b0\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f73\u3002\u572840\u4e2a\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u5e38\u89c1\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u51bb\u7ed3\u5c42\u3001\u5916\u90e8\u6570\u636e\u6df7\u5408\u7b49\uff09\uff0c\u5728\u786e\u4fdd\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u3002\u7406\u8bba\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u707e\u96be\u6027\u9057\u5fd8\u90e8\u5206\u6765\u6e90\u4e8e\u98ce\u683c\u5f15\u8d77\u7684\u53c2\u6570\u6f02\u79fb\uff0cSA-SFT\u80fd\u591f\u6709\u6548\u4fee\u6b63\u3002", "conclusion": "SA-SFT\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\u3001\u5b9e\u73b0\u7b80\u4fbf\u3001\u6548\u679c\u663e\u8457\u7684\u81ea\u589e\u5f3a\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u9002\u5e94\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u7684\u7f13\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20165", "abs": "https://arxiv.org/abs/2602.20165", "authors": ["Dorsa EPMoghaddam", "Feng Gao", "Drew Bernard", "Kavya Sinha", "Mehdi Razavi", "Behnaam Aazhang"], "title": "VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography", "comment": "8 pages, 3 figures, 3 tabels", "summary": "Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5229\u7528AI\u548c\u5fc3\u5185\u8d85\u58f0\uff08ICE\uff09\u89c6\u9891\u81ea\u52a8\u5b9a\u4f4d\u5fc3\u5f8b\u5931\u5e38\u6765\u6e90\uff0c\u63d0\u5347\u901f\u5ea6\u5e76\u7b80\u5316\u64cd\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u5fc3\u5f8b\u5931\u5e38\u5b9a\u4f4d\u4f9d\u8d56\u9ad8\u5bc6\u5ea6\u6807\u6d4b\u548c\u672f\u524dCT/MRI\uff0c\u65f6\u95f4\u4e0e\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e9f\u9700\u66f4\u5feb\u3001\u66f4\u7ecf\u6d4e\u7684\u65b9\u6cd5\u3002AI\u5df2\u5728\u8d85\u58f0\u5f71\u50cf\u5206\u6790\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5f15\u53d1\u5c06\u5176\u5e94\u7528\u4e8e\u5fc3\u5f8b\u5931\u5e38\u5b9a\u4f4d\u7684\u9700\u6c42\u3002", "method": "\u5c06\u5fc3\u5f8b\u5931\u5e38\u6765\u6e90\u5b9a\u4f4d\u95ee\u9898\u5efa\u7acb\u4e3a\u4e09\u5206\u7c7b\u4efb\u52a1\uff1a\u533a\u5206\u6b63\u5e38\u7aa6\u5f8b\u3001\u5de6\u4fa7\u548c\u53f3\u4fa7\u5fc3\u5f8b\u5931\u5e38\u3002\u57fa\u4e8eICE\u89c6\u9891\u6570\u636e\uff0c\u5f00\u53d1\u4e863D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u4e0e\u5206\u7c7b\u3002\u901a\u8fc7\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5bf9\u56db\u540d\u65b0\u60a3\u8005\u6570\u636e\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523066.2%\uff0c\u8fdc\u9ad8\u4e8e\u968f\u673a\u57fa\u7ebf\uff0833.3%\uff09\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u5fc3\u5f8b\u5931\u5e38\u5b9a\u4f4d\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u7ed3\u5408ICE\u548c\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u81ea\u52a8\u5316\u5fc3\u5f8b\u5931\u5e38\u5b9a\u4f4d\uff0c\u53ef\u671b\u52a0\u5feb\u5fc3\u810f\u7535\u751f\u7406\u64cd\u4f5c\u3001\u51cf\u5c11\u6d88\u878d\u624b\u672f\u8d1f\u62c5\u3002\u540e\u7eed\u5c06\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u6a21\u578b\u7a33\u5065\u6027\u53ca\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2602.20220", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20220", "abs": "https://arxiv.org/abs/2602.20220", "authors": ["Yarden As", "Dhruva Tirumala", "Ren\u00e9 Zurbr\u00fcgg", "Chenhao Li", "Stelian Coros", "Andreas Krause", "Markus Wulfmeier"], "title": "What Matters for Simulation to Online Reinforcement Learning on Real Robots", "comment": null, "summary": "We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.", "AI": {"tldr": "\u672c\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86\u80fd\u8ba9\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5e94\u7528\u7684\u5177\u4f53\u8bbe\u8ba1\u9009\u62e9\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u7269\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4e3b\u6d41\u7684\u505a\u6cd5\u4e2d\u6709\u4e9b\u9ed8\u8ba4\u8bbe\u7f6e\u53cd\u800c\u4e0d\u5229\uff0c\u800c\u63a8\u8350\u7684\u505a\u6cd5\u66f4\u80fd\u7a33\u5b9a\u5b66\u4e60\u3002", "motivation": "\u5c3d\u7ba1\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8bb8\u591a\u8bbe\u8ba1\u7ec6\u8282\u88ab\u524d\u4eba\u5ffd\u7565\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7ecf\u9a8c\u6307\u5bfc\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u548c\u63a8\u5e7f\u3002", "method": "\u4f5c\u8005\u8de8\u4e09\u4e2a\u4e0d\u540c\u673a\u5668\u5e73\u53f0\uff0c\u8fdb\u884c\u4e86100\u6b21\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u5bf9\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u5b9e\u9a8c\u6d41\u7a0b\u4e2d\u7684\u5404\u4e2a\u8bbe\u8ba1\u9009\u62e9\u9010\u4e00\u6d88\u878d\u5206\u6790\uff0c\u603b\u7ed3\u6bcf\u4e2a\u56e0\u7d20\u5bf9RL\u6210\u6548\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e9b\u5e7f\u6cdb\u91c7\u7528\u7684\u9ed8\u8ba4\u9009\u62e9\u5b9e\u9645\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u6709\u5bb3\uff0c\u800c\u9075\u5faa\u63a8\u8350\u7684\u8bbe\u8ba1\u9009\u62e9\u5219\u80fd\u5728\u4efb\u52a1\u548c\u786c\u4ef6\u5c42\u9762\u4fdd\u8bc1\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u7528\u5927\u6837\u672c\u5b9e\u8bc1\u7814\u7a76\u4e86RL\u7cfb\u7edf\u6027\u8bbe\u8ba1\u9009\u62e9\uff0c\u4e3a\u884c\u4e1a\u5b9e\u8df5\u8005\u4f4e\u6210\u672c\u90e8\u7f72RL\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u5de5\u7a0b\u7ecf\u9a8c\u4e0e\u6307\u5bfc\u3002"}}
{"id": "2602.20164", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20164", "abs": "https://arxiv.org/abs/2602.20164", "authors": ["Sachin Gopal Wani", "Eric Page", "Ajay Dholakia", "David Ellison"], "title": "Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings", "comment": "16 pages, 5 figures, accepted at the the 2025 TPCTC Conference", "summary": "Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u7ecf\u8fc7\u77e5\u8bc6\u84b8\u998f\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u77e5\u8bc6\u84b8\u998f\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e14\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u540c\u7c7b\u4ea7\u54c1\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u5de8\u5927\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u53d7\u9650\u73af\u5883\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\u4f5c\u4e3a\u9ad8\u6548\u6784\u5efa\u5c0f\u578b\u3001\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u91cf\u5316\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u4e0e\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u5bf9\u6bd4\u5b9e\u9a8c\u65b9\u6cd5\uff1a\u5206\u522b\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\u540e\u6a21\u578b\u4e0e\u539f\u59cb\uff08vanilla\uff09\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6d88\u8017\uff1b\u91cd\u70b9\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u4e0b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u77e5\u8bc6\u84b8\u998f\u540e\u751f\u6210\u76848B\u53c2\u6570\u6a21\u578b\uff0c\u5176\u8bad\u7ec3\u6240\u9700\u8ba1\u7b97\u91cf\u6bd4\u539f\u59cb\u6a21\u578b\u5c112000\u500d\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4e0e\u66f4\u5927\u89c4\u6a21\u6807\u51c6\u6a21\u578b\uff08\u53c2\u6570\u91cf\u662f\u5176\u5341\u500d\uff09\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u4e0d\u4ec5\u662f\u4e00\u79cd\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u66f4\u5e94\u6210\u4e3a\u6784\u5efa\u9ad8\u6548AI\u7684\u4e3b\u6d41\u7b56\u7565\uff1b\u5176\u53ef\u4ee5\u6253\u9020\u9ad8\u6027\u80fd\u4e14\u66f4\u6613\u83b7\u5f97\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63a8\u52a8AI\u8fdb\u4e00\u6b65\u666e\u53ca\u548c\u843d\u5730\u3002"}}
{"id": "2602.20205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20205", "abs": "https://arxiv.org/abs/2602.20205", "authors": ["Xiwen Chen", "Wenhui Zhu", "Gen Li", "Xuanzhao Dong", "Yujian Xiong", "Hao Wang", "Peijie Qiu", "Qingquan Song", "Zhipeng Wang", "Shao Tang", "Yalin Wang", "Abolfazl Razi"], "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport", "comment": "Accepted by CVPR2026 (Findings). arXiv admin note: text overlap with arXiv:2503.02175 by other authors", "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOTPrune\u7684\u89c6\u89c9Token\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u5b9e\u73b0\u5206\u5e03\u5bf9\u9f50\uff0c\u52a0\u901f\u591a\u6a21\u6001\u5927\u6a21\u578b\u63a8\u7406\uff0c\u540c\u65f6\u517c\u987e\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u89c6\u89c9Token\u5197\u4f59\uff0c\u63a8\u7406\u6210\u672c\u9ad8\u3002\u5df2\u6709\u526a\u679d\u65b9\u6cd5\u5ffd\u7565\u4e86\u89c6\u89c9\u8868\u793a\u7684\u5206\u5e03\u7ed3\u6784\uff0c\u8fd9\u5f71\u54cd\u4e86\u526a\u679d\u7a33\u5b9a\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6539\u8fdb\u526a\u679d\u65b9\u6cd5\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "OTPrune\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5c06\u89c6\u89c9Token\u526a\u679d\u95ee\u9898\u5efa\u6a21\u4e3a\u5206\u5e03\u5bf9\u9f50\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u65b9\u6cd5\u6700\u5c0f\u5316\u5b8c\u6574\u4e0e\u526a\u679d\u540eToken\u5206\u5e03\u76842-Wasserstein\u8ddd\u79bb\uff0c\u4fdd\u6301\u5c40\u90e8\u591a\u6837\u6027\u548c\u5168\u5c40\u8868\u8fbe\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u53ef\u4f18\u5316\u7684\u6b21\u6a21\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u8be5\u76ee\u6807\u51fd\u6570\u5177\u6709\u5355\u8c03\u6027\u548c\u6b21\u6a21\u6027\uff0c\u4fdd\u969c\u526a\u679d\u7684\u9ad8\u6548\u548c\u7a33\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOTPrune\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86\u6bd4\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u66f4\u4f18\u7684\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u8bba\u6587\u7406\u8bba\u4e0e\u5b9e\u9a8c\u5747\u8868\u660e\uff0c\u5206\u5e03\u5bf9\u9f50\u89c6\u89d2\u4e0b\u7684Token\u526a\u679d\u80fd\u6709\u6548\u63d0\u5347MLLM\u63a8\u7406\u6548\u7387\u4e14\u6027\u80fd\u7a33\u5b9a\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2602.20225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20225", "abs": "https://arxiv.org/abs/2602.20225", "authors": ["Yichang Feng", "Xiao Liang", "Minghui Zheng"], "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators", "comment": null, "summary": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5FACTO\uff0c\u80fd\u9ad8\u6548\u5e94\u5bf9\u5177\u6709\u7ea6\u675f\u7684\u5355/\u591a\u673a\u68b0\u81c2\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89e3\u8d28\u91cf\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u591a\u673a\u68b0\u81c2\u548c\u590d\u6742\u7ea6\u675f\u65f6\u5b58\u5728\u89e3\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6025\u9700\u4e00\u79cd\u901a\u7528\u4e14\u6548\u7387\u66f4\u9ad8\u7684\u65b9\u6cd5\u63d0\u5347\u8f68\u8ff9\u53ef\u884c\u6027\u548c\u4f18\u5316\u6548\u679c\u3002", "method": "\u91c7\u7528\u6b63\u4ea4\u57fa\u51fd\u6570\u5bf9\u8f68\u8ff9\u8fdb\u884c\u53c2\u6570\u5316\u4f18\u5316\uff0c\u901a\u8fc7\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5e73\u6ed1\u5b50\u95ee\u9898\uff0c\u7528Levenberg-Marquardt\u7b97\u6cd5\u5728\u7ea6\u675f\u6d3b\u52a8\u96c6\u7684\u96f6\u7a7a\u95f4\u5185\u81ea\u9002\u5e94\u7ea6\u675f\u66f4\u65b0\uff0c\u6574\u4f53\u5728\u7cfb\u6570\u7a7a\u95f4\u76f4\u63a5\u4f18\u5316\u3002\u901a\u8fc7\u4e0e\u73b0\u6709\u4e3b\u6d41\u4f18\u5316\u548c\u91c7\u6837\u89c4\u5212\u5668\u5bf9\u6bd4\u8868\u73b0\u5176\u4f18\u8d8a\u6027\u3002", "result": "FACTO\u5728\u6709\u7ea6\u675f\u7684\u5355/\u591a\u673a\u68b0\u81c2\u8f68\u8ff9\u4f18\u5316\u573a\u666f\u4e0b\uff0c\u89e3\u8d28\u91cf\u548c\u53ef\u884c\u6027\u4f18\u4e8eCHOMP\u3001TrajOpt\u3001GPMP2\u7b49\u4f18\u5316\u578b\u4ee5\u53caRRT-Connect\u3001PRM\u7b49\u91c7\u6837\u578b\u89c4\u5212\u5668\u3002\u5b9e\u9645\u5728Franka\u673a\u68b0\u81c2\u4e0a\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u843d\u5730\u5e94\u7528\u3002", "conclusion": "FACTO\u80fd\u591f\u9ad8\u6548\u4e14\u7a33\u5065\u5730\u89e3\u51b3\u591a\u81c2\u534f\u4f5c\u4e0b\u590d\u6742\u7ea6\u675f\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5177\u5907\u826f\u597d\u7684\u5de5\u7a0b\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.20166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20166", "abs": "https://arxiv.org/abs/2602.20166", "authors": ["Yongda Yu", "Lei Zhang", "Xinxin Guo", "Minghui Yu", "Zhengqi Zhuang", "Guoping Rong", "Haifeng Shen", "Zhengfeng Li", "Boge Wang", "Guoan Zhang", "Bangyu Xiang", "Xiaobin Xu"], "title": "ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling", "comment": null, "summary": "In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ConceptRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u548c\u534f\u540c\u5b66\u4e60\uff0c\u6709\u6548\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u7b5b\u9009\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u601d\u6a21\u578b\u62e6\u622a\u865a\u5047\u544a\u8b66\u7684\u80fd\u529b\uff0c\u5e76\u5927\u5e45\u8d85\u8d8a\u4e86\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u4f53\u751f\u6210\u7684\u544a\u8b66\u8fc7\u91cf\u4e14\u591a\u4e3a\u8bef\u62a5\uff0c\u5bfc\u81f4\u7528\u6237\u8b66\u89c9\u75b2\u52b3\uff0c\u4ece\u800c\u9057\u6f0f\u5173\u952e\u544a\u8b66\u3002\u73b0\u6709\u8bad\u7ec3\u8fc7\u6ee4\u6a21\u578b\u65b9\u5f0f\u9762\u4e34\u6570\u636e\u6807\u6ce8\u566a\u58f0\u5927\u3001\u4eba\u5de5\u6e05\u6d17\u6210\u672c\u9ad8\u7684\u96be\u9898\u3002", "method": "\u63d0\u51faConceptRM\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u6270\u52a8\u751f\u6210\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u6570\u636e\u96c6\uff0c\u5e76\u7528\u534f\u540c\u6559\u5b66\uff08co-teaching\uff09\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5b66\u4e60\uff0c\u901a\u8fc7\u6a21\u578b\u5171\u8bc6\u5224\u5b9a\uff0c\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u7b5b\u9009\u51fa\u53ef\u9760\u8d1f\u6837\u672c\u7528\u4ee5\u8bad\u7ec3\u8fc7\u6ee4\u6a21\u578b\u3002", "result": "ConceptRM\u5728\u62e6\u622a\u865a\u5047\u544a\u8b66\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u4e3b\u6d41\u5927\u6a21\u578b\u57fa\u7ebf\uff0c\u5728\u57df\u5185\u548c\u8de8\u57df\u6570\u636e\u96c6\u5206\u522b\u63d0\u5347\u4e86\u6700\u591a53.31%\u548c41.67%\uff0c\u4e14\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "ConceptRM\u80fd\u4ee5\u6781\u4f4e\u7684\u6807\u6ce8\u6210\u672c\uff0c\u6709\u6548\u63d0\u5347\u53cd\u601d\u6a21\u578b\u7684\u544a\u8b66\u8fc7\u6ee4\u6548\u679c\uff0c\u5177\u6709\u5f88\u597d\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.20291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u8868\u89e3\u6790\u3001\u81ea\u52a8\u5206\u6790\u4e0e\u8fed\u4ee3\u6539\u8fdb\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u6709\u4f9d\u636e\u3001\u53ef\u64cd\u4f5c\u7684\u4f18\u5316\u5efa\u8bae\u3002\u7cfb\u7edf\u53ef\u81ea\u52a8\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5e76\u57fa\u4e8e\u53ef\u89c6\u5316\u539f\u5219\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\uff0c\u5b9e\u9a8c\u8868\u660e\u5bf91,000\u4e2a\u56fe\u8868\u80fd\u8f93\u51fa\u9ad8\u8d28\u91cf\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u636e\u53ef\u89c6\u5316\u5de5\u5177\u5e38\u56e0\u8bbe\u8ba1\u7f3a\u9677\u800c\u5bfc\u81f4\u7406\u89e3\u504f\u5dee\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u80fd\u7406\u89e3\u4e0a\u4e0b\u6587\uff0c\u4e5f\u4e0d\u80fd\u7ed9\u51fa\u5177\u4f53\u6539\u8fdb\u5efa\u8bae\uff1b\u800c\u76f4\u63a5\u7528LLMs\u53cd\u9988\u56fe\u8868\u8d28\u91cf\u53c8\u4e0d\u53ef\u9760\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u3001\u667a\u80fd\u4e14\u80fd\u4e3a\u7528\u6237\u63d0\u4f9b\u53ef\u843d\u5730\u6539\u8fdb\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7cfb\u7edf\uff1a\u5148\u901a\u8fc7\u56fe\u50cf\u91cd\u5efa\u56fe\u8868\u7ed3\u6784\uff0c\u7136\u540e\u7528\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u68c0\u6d4b\u8bbe\u8ba1\u7f3a\u9677\uff0c\u7ed3\u5408\u53ef\u89c6\u5316\u7814\u7a76\u539f\u5219\uff0c\u9010\u6761\u751f\u6210\u5177\u4f53\u4f18\u5316\u5efa\u8bae\u3002\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u6027\u5e94\u7528\u5efa\u8bae\uff0c\u7cfb\u7edf\u8fd8\u652f\u6301\u56fe\u8868\u518d\u6e32\u67d3\u4ee5\u5f62\u6210\u6301\u7eed\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728Chart2Code\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5bf91,000\u5f20\u56fe\u751f\u6210\u4e8610,452\u6761\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5f52\u7eb3\u523010\u4e2a\u6709\u4ee3\u8868\u6027\u7684\u6539\u8fdb\u7c7b\u522b\uff08\u5982\u5750\u6807\u8f74\u683c\u5f0f\u3001\u8272\u5f69\u65e0\u969c\u788d\u3001\u56fe\u4f8b\u4e00\u81f4\u6027\u7b49\uff09\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u9ad8\u8986\u76d6\u7387\u548c\u9488\u5bf9\u6027\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u63a8\u8350\u7cfb\u7edf\u53ef\u4e3a\u56fe\u8868\u8bbe\u8ba1\u63d0\u4f9b\u7ed3\u6784\u5316\u548c\u7406\u8bba\u652f\u6301\u7684\u53cd\u9988\uff0c\u63a8\u52a8\u667a\u80fd\u3001\u9ad8\u6548\u7684\u6570\u636e\u53ef\u89c6\u5316\u521b\u4f5c\u5de5\u5177\u7684\u53d1\u5c55\uff0c\u63d0\u5347\u53ef\u89c6\u5316\u7d20\u517b\u3002"}}
{"id": "2602.20231", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20231", "abs": "https://arxiv.org/abs/2602.20231", "authors": ["Manish Kumar Govind", "Dominick Reilly", "Pu Wang", "Srijan Das"], "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models", "comment": "https://manishgovind.github.io/unilact-vla/", "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\u7684\u6df1\u5ea6\u611f\u77e5\u6f5c\u5728\u52a8\u4f5c\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u4e30\u5bcc\u4ea4\u4e92\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ec5\u5229\u7528RGB\u89c6\u9891\u65e0\u76d1\u7763\u5b66\u4e60\u5f97\u5230\u7684\u6f5c\u5728\u52a8\u4f5c\u7f16\u7801\u7f3a\u4e4f\u663e\u5f0f\u7684\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\uff0c\u800c\u8fd9\u79cd\u7a7a\u95f4\u7ed3\u6784\u5bf9\u4e8e\u7cbe\u786e\u3001\u590d\u6742\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u975e\u5e38\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86UniLACT\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u6df1\u5ea6\u611f\u77e5\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u5f81\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7UniLARN\u6846\u67b6\u4f7f\u7528\u9006\u52a8\u529b\u5b66\u548c\u6b63\u5411\u52a8\u529b\u5b66\u76ee\u6807\uff0c\u7edf\u4e00\u5b66\u4e60RGB\u4e0e\u6df1\u5ea6\u7684\u8de8\u6a21\u6001\u5171\u4eab\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u3002\u8be5\u7a7a\u95f4\u53ef\u751f\u6210\u6a21\u6001\u7279\u5f02\u548c\u7edf\u4e00\u7684\u6f5c\u5728\u52a8\u4f5c\u4f2a\u6807\u7b7e\uff0c\u7528\u4e8e\u6df1\u5ea6\u611f\u77e5VLA\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6df1\u5ea6\u611f\u77e5\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u65b9\u6cd5\u5728\u57df\u5185\u3001\u57df\u5916\u9884\u8bad\u7ec3\u548c\u4e0d\u540c\u64cd\u4f5c\u4efb\u52a1\u4e0b\u5747\u4f18\u4e8e\u4ec5\u57fa\u4e8eRGB\u7684\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u4fe1\u606f\u7684\u6f5c\u5728\u52a8\u4f5c\u9884\u8bad\u7ec3\u4e3a\u673a\u5668\u4eba\u7406\u89e3\u548c\u6267\u884c\u590d\u6742\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7a7a\u95f4\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u7684\u64cd\u4f5c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.20294", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20294", "abs": "https://arxiv.org/abs/2602.20294", "authors": ["Yu Li", "Pranav Narayanan Venkit", "Yada Pruksachatkun", "Chien-Sheng Wu"], "title": "InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation", "comment": null, "summary": "Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u4e2a\u6027\u6a21\u62df\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u8fd8\u539f\u4e2a\u6027\u4e0a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u9488\u5bf9\u4e2a\u6027\u6a21\u62df\u7684\u8bc4\u4f30\u65b9\u6cd5\u591a\u57fa\u4e8e\u95ee\u5377\u6216AI\u8bbf\u8c08\u7b49\u95f4\u63a5\u6307\u6807\uff0c\u672a\u80fd\u76f4\u63a5\u8861\u91cf\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e2a\u6027\u7684\u8fd8\u539f\u80fd\u529b\uff0c\u56e0\u6b64\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u9a8c\u8bc1\u3002", "method": "\u4f5c\u8005\u91c7\u96c6\u4e861000\u4f4d\u516c\u4f17\u4eba\u7269\u3001\u603b\u8ba1670,000\u4f59\u5bf9\u95ee\u7b54\u7684\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u5185\u5bb9\u76f8\u4f3c\u5ea6\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u4e2a\u6027\u5951\u5408\u548c\u77e5\u8bc6\u4fdd\u6301\u56db\u9879\u6307\u6807\u7684\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u8bbf\u8c08\u68c0\u7d22\u589e\u5f3a\u548c\u6309\u65f6\u95f4\u987a\u5e8f\u63a8\u8fdb\u4e24\u79cd\u5efa\u6a21\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u6a21\u62df\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u7528\u4eba\u7269\u6863\u6848\u6216\u6a21\u578b\u5df2\u6709\u77e5\u8bc6\u7684\u65b9\u6cd5\uff1b\u8bbf\u8c08\u68c0\u7d22\u589e\u5f3a\u6cd5\u66f4\u64c5\u957f\u518d\u73b0\u4e2a\u6027\u98ce\u683c\u4e0e\u56de\u7b54\u8d28\u91cf\uff0c\u800c\u65f6\u5e8f\u6cd5\u66f4\u5229\u4e8e\u4e8b\u5b9e\u4e00\u81f4\u4e0e\u77e5\u8bc6\u4fdd\u5b58\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u6a21\u62df\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u548c\u65b9\u6cd5\u9009\u62e9\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u5e73\u8861\u4e2a\u6027\u8fd8\u539f\u548c\u4e8b\u5b9e\u4fdd\u6301\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u4e0e\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u53c2\u8003\u3002"}}
{"id": "2602.20312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86N4MC\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u968f\u65f6\u95f4\u53d8\u5316\u76844D\u7f51\u683c\u5e8f\u5217\u7684\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u65f6\u95f4\u5197\u4f59\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\u5728\u52a8\u753b\u3001\u865a\u62df\u73b0\u5b9e\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u6570\u636e\u91cf\u5de8\u5927\uff0c\u7ed9\u5b58\u50a8\u4e0e\u4f20\u8f93\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u4ec5\u5bf9\u6bcf\u4e00\u5e27\u72ec\u7acb\u5904\u7406\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u65f6\u95f4\u5197\u4f59\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\u7684\u538b\u7f29\u6548\u7387\uff0c\u5e76\u786e\u4fdd\u9ad8\u8d28\u91cf\u548c\u5b9e\u65f6\u89e3\u7801\u3002", "method": "N4MC\u501f\u92742D\u89c6\u9891\u7f16\u89e3\u7801\u4e2d\u7684\u5e27\u95f4\u538b\u7f29\u601d\u60f3\uff0c\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u673a\u5236\u9ad8\u6548\u538b\u7f29\u957f\u65f6\u5e8f\u7f51\u683c\u6570\u636e\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5c06\u8fde\u7eed\u7684\u4e0d\u89c4\u5219\u7f51\u683c\u5e27\u8f6c\u5316\u4e3a\u89c4\u6574\u76844D\u5f20\u91cf\uff0c\u5b9e\u73b0\u7edf\u4e00\u538b\u7f29\u8868\u793a\u3002\u968f\u540e\uff0c\u7528auto-decoder\u5bf9\u5f20\u91cf\u8fdb\u884c\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\u7684\u5197\u4f59\u6d88\u9664\u3002\u4e3a\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5f15\u5165\u4e86\u57fa\u4e8etransformer\u7684\u63d2\u503c\u6a21\u578b\uff0c\u901a\u8fc7\u4f53\u79ef\u4e2d\u5fc3\u7684\u6f5c\u5728\u5d4c\u5165\u9884\u6d4b\u4e2d\u95f4\u7f51\u683c\u5e27\uff0c\u6d88\u9664\u8fd0\u52a8\u6a21\u7cca\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cN4MC\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e864D\u7f51\u683c\u5e8f\u5217\u7684\u5b9e\u65f6\u89e3\u7801\u3002", "conclusion": "N4MC\u5145\u5206\u5229\u7528\u4e86\u65f6\u53d8\u6570\u636e\u7684\u65f6\u95f4\u5197\u4f59\uff0c\u5728\u9ad8\u6548\u538b\u7f29\u4e0e\u9ad8\u8d28\u91cf\u6062\u590d\u4e4b\u95f4\u83b7\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5bf9\u9700\u8981\u5904\u7406\u5927\u89c4\u6a214D\u7f51\u683c\u6570\u636e\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.20304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20304", "abs": "https://arxiv.org/abs/2602.20304", "authors": ["Onur Beker", "Andreas Ren\u00e9 Geist", "Anselm Paulus", "Nico G\u00fcrtler", "Ji Shi", "Sylvain Calinon", "Georg Martius"], "title": "Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation", "comment": null, "summary": "Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u521a\u4f53\u52a8\u529b\u5b66\u63a5\u89e6\u6a21\u62df\u6846\u67b6\uff0c\u517c\u987e\u4e86\u52a0\u901f\u3001\u53ef\u5411\u91cf\u5316\u548c\u53ef\u5fae\u5206\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u5224\u5b9a\u6548\u7387\uff0c\u5e76\u8d85\u8fc7\u4e86\u4e3b\u6d41\u6a21\u62df\u5668Mujoco XLA\u3002", "motivation": "\u73b0\u6709\u53ef\u5fae\u5206\u4eff\u771f\u7cfb\u7edf\u5728\u63a5\u89e6\u6d41\u5f62\u751f\u6210\u4e0a\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5e38\u7528\u6a21\u62df\u5668\u7684\u76f8\u5173\u6a21\u5757\u5e76\u975e\u4e3a\u5411\u91cf\u5316\u548c\u53ef\u5fae\u5206\u6027\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u4eff\u771f\u6548\u7387\u548c\u68af\u5ea6\u8ba1\u7b97\u53d7\u9650\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u5957\u5168\u65b0\u7684\u63a5\u89e6\u751f\u6210\u6846\u67b6\uff1a\uff081\uff09\u5b9a\u4e49\u4e86\u4e00\u7ec4\u5e73\u6ed1\u7684\u89e3\u6790\u6709\u7b26\u53f7\u8ddd\u79bb\u5143\uff0c\u7528\u4e8e\u9876\u70b9-\u9762\u78b0\u649e\u5224\u5b9a\uff1b\uff082\uff09\u63d0\u51fa\u4e86\u53ef\u5fae\u5206\u7684\u8fb9-\u8fb9\u78b0\u649e\u5b50\u7a0b\u5e8f\uff0c\u80fd\u591f\u8f93\u51fa\u6709\u7b26\u53f7\u8ddd\u79bb\u548c\u63a5\u89e6\u6cd5\u5411\u3002\u65b9\u6cd5\u517c\u987e\u4e86\u5e38\u89c4\u51f8\u5143\u9ad8\u6548\u4f46\u96be\u53ef\u5fae\u5206\u3001\u969c\u788d\u6cd5\u8ddd\u79bb\u51fd\u6570\u53ef\u5fae\u4f46\u4f4e\u6548\u7684\u4e24\u7c7b\u4e3b\u6d41\u601d\u8def\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u6559\u5b66\u7ea7\u5b9e\u9a8c\u548c\u4e0eMujoco XLA\u78b0\u649e\u68c0\u6d4b\u6a21\u5757\u8fdb\u884c\u57fa\u51c6\u5bf9\u6bd4\uff0c\u65b0\u65b9\u6cd5\u5728\u8fd0\u884c\u901f\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u51ed\u501f\u9488\u5bf9\u53ef\u5fae\u5206\u6027\u4e0e\u5411\u91cf\u5316\u4f18\u5316\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u672c\u6587\u63d0\u51fa\u7684\u521a\u4f53\u63a5\u89e6\u5224\u5b9a\u65b9\u6cd5\u80fd\u5927\u5e45\u63d0\u5347\u4eff\u771f\u6548\u7387\uff0c\u5e76\u517c\u5bb9\u81ea\u52a8\u5fae\u5206\uff0c\u53ef\u4e3a\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u7684\u5927\u89c4\u6a21\u53ef\u5fae\u4eff\u771f\u5e26\u6765\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2602.20300", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u67e5\u8be2\u672c\u8eab\u7684\u7279\u5f81\u4e5f\u4f1a\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ea7\u751f\u5e7b\u89c9\uff08hallucination\uff09\u7684\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u4e0d\u540c\u67e5\u8be2\u7279\u5f81\u4e0e\u5e7b\u89c9\u98ce\u9669\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u5c06LLM\u5e7b\u89c9\u4e3b\u8981\u5f52\u56e0\u4e8e\u6a21\u578b\u6216\u89e3\u7801\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u672c\u6587\u5219\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\uff0c\u63a2\u8ba8\u67e5\u8be2\u8868\u8fbe\u65b9\u5f0f\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u8bd5\u56fe\u63ed\u793a\u67d0\u4e9b\u67e5\u8be2\u7c7b\u578b\u4e3a\u4f55\u66f4\u6613\u8bf1\u53d1\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e8622\u7ef4\u7684\u67e5\u8be2\u7279\u5f81\u5411\u91cf\uff0c\u6db5\u76d6\u4ece\u53e5\u590d\u6742\u5ea6\u3001\u8bcd\u6c47\u7a00\u6709\u5ea6\u3001\u7167\u5e94\u3001\u5426\u5b9a\u3001\u53ef\u56de\u7b54\u6027\u5230\u610f\u56fe\u660e\u786e\u6027\u7b49\u5f71\u54cd\u7406\u89e3\u7684\u8bed\u8a00\u56e0\u7d20\u3002\u5728\u5927\u7ea637\u4e07\u4e2a\u771f\u5b9e\u67e5\u8be2\u4e0a\uff0c\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u4e0e\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u7279\u5f81\uff08\u5982\u6df1\u5c42\u4ece\u53e5\u5d4c\u5957\u3001\u4fe1\u606f\u6b20\u7f3a\uff09\u660e\u663e\u589e\u52a0\u5e7b\u89c9\u98ce\u9669\uff0c\u800c\u610f\u56fe\u660e\u786e\u3001\u53ef\u56de\u7b54\u6027\u5f3a\u7684\u67e5\u8be2\u5219\u5927\u5927\u964d\u4f4e\u5e7b\u89c9\u6982\u7387\u3002\u67d0\u4e9b\u7279\u5f81\u5982\u9886\u57df\u7279\u6b8a\u6027\u5219\u8868\u73b0\u51fa\u56e0\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0d\u540c\u800c\u5f02\u7684\u6df7\u5408\u6548\u5e94\u3002", "conclusion": "\u67e5\u8be2\u672c\u8eab\u7684\u8bed\u8a00\u7279\u5f81\u548c\u8868\u8fbe\u65b9\u5f0f\u4e0eLLM\u4ea7\u751f\u5e7b\u89c9\u6982\u7387\u663e\u8457\u76f8\u5173\uff0c\u4e3a\u540e\u7eed\u901a\u8fc7\u5f15\u5bfc\u5f0f\u67e5\u8be2\u91cd\u5199\u548c\u9488\u5bf9\u6027\u5e72\u9884\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2602.20328", "categories": ["cs.CV", "eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdr\u00f3n-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6210\u50cf\u9006\u95ee\u9898\u7684\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u5f81\uff08GSNR\uff09\uff0c\u901a\u8fc7\u52a0\u5f3a\u5bf9\u4e0d\u53ef\u89c1\u89e3\u7684\u5efa\u6a21\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u6210\u50cf\u9006\u95ee\u9898\u5e38\u56e0\u611f\u6d4b\u77e9\u9635\u7684\u975e\u5e73\u51e1\u96f6\u7a7a\u95f4\u5bfc\u81f4\u65e0\u7a77\u591a\u89e3\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4e00\u822c\u6027\u5148\u9a8c\uff08\u5982\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u6027\uff09\u7ea6\u675f\u6574\u4f53\u56fe\u50cf\uff0c\u4f46\u672a\u6709\u6548\u7ea6\u675f\u96f6\u7a7a\u95f4\uff0c\u53ef\u80fd\u5f15\u5165\u91cd\u5efa\u504f\u5dee\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u76f4\u63a5\u5efa\u6a21\u96f6\u7a7a\u95f4\u4fe1\u606f\u3002", "method": "\u4f5c\u8005\u63d0\u51faGSNR\u65b9\u6cd5\u30021\uff09\u901a\u8fc7\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6784\u5efa\u4ec5\u9488\u5bf9\u96f6\u7a7a\u95f4\u7684\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u7528\u4ee5\u63cf\u8ff0\u96f6\u7a7a\u95f4\u4fe1\u53f7\u4e2d\u76f8\u90bb\u50cf\u7d20\u7684\u76f8\u4f3c\u6027\uff1b2\uff09\u5229\u7528\u56fe\u7684\u6700\u4f4e\u9891\u8c31\u6a21\u5f0f\uff08p-\u6700\u5e73\u6ed1\u7684\u7279\u5f81\u5411\u91cf\uff09\u6784\u5efa\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635\uff0c\u4ece\u6d4b\u91cf\u4e2d\u9ad8\u6548\u9884\u6d4b\u548c\u8986\u76d6\u96f6\u7a7a\u95f4\u3002", "result": "GSNR\u88ab\u6574\u5408\u8fdb\u5178\u578b\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff08\u5982PnP\u3001DIP\u3001\u6269\u6563\u6cd5\uff09\u5e76\u5e94\u7528\u4e8e\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u989c\u8272\u91cd\u5efa\u3001\u8d85\u5206\u8fa8\u56db\u9879\u4efb\u52a1\uff0c\u5728PSNR\u4e0a\u76f8\u8f83\u4e8e\u57fa\u7840\u65b9\u6cd5\u63d0\u5347\u6700\u9ad84.3dB\uff0c\u76f8\u8f83\u4e8e\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u6700\u9ad81dB\u3002", "conclusion": "GSNR\u6709\u6548\u6539\u5584\u4e86\u9006\u95ee\u9898\u4e2d\u96f6\u7a7a\u95f4\u7684\u8868\u8fbe\uff0c\u7406\u8bba\u4e0e\u5b9e\u9a8c\u4e0a\u5747\u8868\u660e\u5176\u53ef\u63d0\u5347\u6536\u655b\u6027\u4e0e\u9884\u6d4b\u6027\uff0c\u5e76\u5e7f\u6cdb\u9002\u7528\u4e8e\u591a\u7c7b\u6210\u50cf\u91cd\u5efa\u4efb\u52a1\u3002"}}
{"id": "2602.20323", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20323", "abs": "https://arxiv.org/abs/2602.20323", "authors": ["Haoyang Li", "Yang You", "Hao Su", "Leonidas Guibas"], "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory", "comment": null, "summary": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPhysMem\u6846\u67b6\uff0c\u8ba9\u673a\u5668\u4eba\u5728\u4e0d\u66f4\u6539\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u4e3b\u4ea4\u4e92\u5b66\u4e60\u7269\u7406\u77e5\u8bc6\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u63d0\u5347\u8868\u73b0\u3002", "motivation": "VLM\u673a\u5668\u4eba\u89c4\u5212\u5668\u5bf9\u7269\u7406\u5c5e\u6027\u53ea\u80fd\u8fdb\u884c\u6cdb\u5316\u63a8\u7406\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5177\u4f53\u5b9e\u4f8b\u7684\u7269\u7406\u9884\u6d4b\u80fd\u529b\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u53ef\u4ece\u73b0\u573a\u4ea4\u4e92\u5feb\u901f\u8865\u5145\u548c\u9a8c\u8bc1\u7269\u7406\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "PhysMem\u6846\u67b6\u5c06\u673a\u5668\u4eba\u4ea4\u4e92\u7ecf\u9a8c\u5b58\u50a8\u4e3a\u5019\u9009\u7269\u7406\u5047\u8bbe\uff0c\u5e76\u8bbe\u8ba1\u201c\u5148\u9a8c\u8bc1\u540e\u5e94\u7528\u201d\u673a\u5236\uff1a\u5bf9\u6bcf\u4e2a\u5047\u8bbe\uff0c\u4ec5\u5728\u7528\u65b0\u89c2\u6d4b\u6570\u636e\u9a8c\u8bc1\u901a\u8fc7\u540e\uff0c\u624d\u7528\u4e8e\u6307\u5bfc\u540e\u7eed\u51b3\u7b56\uff0c\u4ece\u800c\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u907f\u514d\u7ecf\u9a8c\u8bef\u7528\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u4e09\u9879\u771f\u5b9e\u64cd\u4f5c\u4efb\u52a1\u548c\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPhysMem\u5728\u56db\u79cdVLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5b9e\u73b0\u63d0\u5347\u3002\u5c24\u5176\u5728\u7816\u5757\u63d2\u5165\u4efb\u52a1\u4e2d\uff0c\u62bd\u8c61\u539f\u5219\u5316\u65b9\u6cd5\u6210\u529f\u738776%\uff0c\u663e\u8457\u9ad8\u4e8e\u5355\u7eaf\u7ecf\u9a8c\u68c0\u7d22\u768423%\u3002\u77ed\u65f6\u73b0\u5b9e\u90e8\u7f72\uff0830\u5206\u949f\uff09\u540c\u6837\u89c2\u5bdf\u5230\u8868\u73b0\u63d0\u5347\u3002", "conclusion": "PhysMem\u80fd\u6709\u6548\u589e\u5f3aVLM\u673a\u5668\u4eba\u7684\u7269\u7406\u63a8\u7406\u4e0e\u573a\u666f\u9002\u5e94\u80fd\u529b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u548c\u5047\u8bbe\u9a8c\u8bc1\u514b\u670d\u4ec5\u51ed\u7ecf\u9a8c\u7684\u5c40\u9650\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u591a\u53d8\u73b0\u5b9e\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u8868\u73b0\u3002"}}
{"id": "2602.20332", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ea7\u751f\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86QueryBandits\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u81ea\u9002\u5e94\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7684\u7814\u7a76\u5927\u591a\u96c6\u4e2d\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\uff0c\u4e14\u5e38\u4f9d\u8d56\u540e\u5904\u7406\u6216\u53c2\u6570\u7f16\u8f91\u3002\u800c\u4e3b\u6d41\u7684\u95ed\u6e90\u6a21\u578b\u5e7f\u6cdb\u7528\u4e8e\u673a\u6784\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5374\u7f3a\u4e4f\u9488\u5bf9\u5e7b\u89c9\u95ee\u9898\u7684\u6709\u6548\u7814\u7a76\u548c\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQueryBandits\u6846\u67b6\uff0c\u5c06\u67e5\u8be2\u91cd\u5199\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5229\u7528\u7ecf\u9a8c\u9a8c\u8bc1\u4e14\u6821\u51c6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u7ebf\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u6700\u4f18\u7684\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u3002\u91cd\u70b9\u6d4b\u8bd5\u4e86Thompson\u91c7\u6837\u548c\u591a\u79cd\u9759\u6001\u7b56\u7565\uff0c\u5e76\u572816\u4e2a\u95ee\u7b54\u573a\u666f\u4e2d\u4e0e\u4e0d\u91cd\u5199\u548c\u9759\u6001\u91cd\u5199\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "QueryBandit\uff08Thompson Sampling\uff09\u76f8\u8f83\u4e0d\u91cd\u5199\u57fa\u7ebf\u63d0\u5347\u80dc\u7387\u81f387.5%\uff0c\u8d85\u8d8a\u9759\u6001\u7b56\u756542.6%\u548c60.3%\u3002\u6240\u6709\u4e0a\u4e0b\u6587bandit\u7b56\u7565\u5747\u4f18\u4e8e\u4f20\u7edfbandit\uff0c\u5355\u4e00\u91cd\u5199\u7b56\u7565\u65e0\u6cd5\u9002\u914d\u6240\u6709\u573a\u666f\uff0c\u90e8\u5206\u9759\u6001\u7b56\u7565\u751a\u81f3\u6bd4\u4e0d\u91cd\u5199\u66f4\u7cdf\u3002", "conclusion": "QueryBandits\u65e0\u9700\u5bf9\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u8c03\u6574\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ec5\u501f\u52a9\u524d\u5411\u4f20\u9012\u5373\u53ef\u63d0\u5347\u95ed\u6e90\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u65e0\u9700\u6a21\u578b\u7ed3\u6784\u53d8\u52a8\u5373\u53ef\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2602.20330", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20330", "abs": "https://arxiv.org/abs/2602.20330", "authors": ["Jingcheng Yang", "Tianhu Xiong", "Shengyi Qian", "Klara Nahrstedt", "Mingyuan Wu"], "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking", "comment": "To appear in the Findings of CVPR 2026", "summary": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u900f\u660e\u7535\u8def\u8ffd\u8e2a\u7684\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u5e76\u9a8c\u8bc1\u4e86VLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u878d\u5408\u4e0e\u53ef\u63a7\u6027\u3002", "motivation": "\u76ee\u524dVLMs\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u548c\u51b3\u7b56\u8fc7\u7a0b\u9ad8\u5ea6\u4e0d\u900f\u660e\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u573a\u5408\u7684\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3VLMs\u4f5c\u4e3a\u201c\u9ed1\u7bb1\u201d\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u53ef\u8ffd\u8e2a\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u7efc\u5408\u6027\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u5305\u62ec\u8f6c\u7801\u5668\u3001\u5f52\u56e0\u56fe\u8c31\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6280\u672f\uff0c\u5bf9VLMs\u8fdb\u884c\u7cfb\u7edf\u7684\u7535\u8def\u8ffd\u8e2a\u548c\u7279\u5f81\u5f52\u56e0\u5206\u6790\uff0c\u540c\u65f6\u901a\u8fc7\u7279\u5f81\u64cd\u63a7\u548c\u7535\u8def\u4fee\u8865\u5b9e\u9a8c\u9a8c\u8bc1\u6240\u5f97\u7ed3\u8bba\u7684\u56e0\u679c\u6027\u3002", "result": "\u4f5c\u8005\u53d1\u73b0VLMs\u5728\u591a\u6a21\u6001\u63a8\u7406\u65f6\u80fd\u591f\u5206\u5c42\u6574\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u89c6\u89c9\u7279\u5f81\u7535\u8def\u5b9e\u73b0\u6570\u5b66\u63a8\u7406\u548c\u8de8\u6a21\u6001\u5173\u8054\u3002\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u7535\u8def\u65e2\u5177\u6709\u56e0\u679c\u6027\uff0c\u4e5f\u53ef\u88ab\u4eba\u4e3a\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u7535\u8def\u8ffd\u8e2a\u6846\u67b6\u5b9e\u73b0\u4e86\u5bf9VLM\u5185\u90e8\u63a8\u7406\u673a\u5236\u7684\u53ef\u89e3\u91ca\u5316\u4e0e\u53ef\u63a7\u5316\uff0c\u4e3a\u4eca\u540e\u5f00\u53d1\u66f4\u900f\u660e\u3001\u66f4\u53ef\u9760\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.20362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20362", "abs": "https://arxiv.org/abs/2602.20362", "authors": ["Robin Jeanne Kirschner", "Anna Huber", "Carina M. Micheler", "Dirk M\u00fcller", "Nader Rajaei", "Rainer Burgkart", "Sami Haddadin"], "title": "Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates", "comment": "9 pages, 11 figures", "summary": "While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u63a5\u89e6\u4e0d\u53ef\u907f\u514d\u7684\u73b0\u5b9e\uff0c\u63d0\u51fa\u5e76\u6269\u5c55\u4e86\u673a\u5668\u4eba\u78b0\u649e\u4f24\u5bb3\u9632\u62a4\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u66f4\u591a\u590d\u6742\u7684\u78b0\u649e\u60c5\u666f\uff08\u5305\u62ec\u5207\u5411\u78b0\u649e\uff09\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u80fd\u91cf\u7684\u4f24\u5bb3\u9632\u62a4\u6570\u636e\u5e93\uff0c\u4e3a\u80fd\u91cf\u7ea6\u675f\u578b\u5b89\u5168\u63a7\u5236\u5668\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5b89\u5168\u6807\u51c6\u53ca\u53c2\u8003\u6570\u636e\u591a\u9488\u5bf9\u949d\u5668\u6b63\u649e\u7b49\u7b80\u5355\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u5c16\u9510\u3001\u5207\u5411\u78b0\u649e\u7b49\u590d\u6742\u548c\u5b9e\u9645\u63a5\u89e6\u60c5\u51b5\u7684\u4e34\u5e8a\u57fa\u7840\u6570\u636e\uff0c\u4ece\u800c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5168\u9762\u5b89\u5168\u9a8c\u8bc1\u548c\u9632\u62a4\u80fd\u529b\u3002", "method": "\u6269\u5c55\u4ee5\u5f80\u4f7f\u7528\u732a\u7ec4\u7ec7\u66ff\u4ee3\u7269\u7684\u78b0\u649e\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u65b0\u589e\u975e\u53d7\u9650\u7684\u5207\u5411\uff08shearing\uff09\u78b0\u649e\u5b9e\u9a8c\uff0c\u5c06\u5404\u7c7b\u78b0\u649e\u51e0\u4f55\u53ca\u89d2\u5ea6\u7684\u5b9e\u9a8c\u6570\u636e\u6574\u5408\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0d\u540c\u78b0\u649e\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u80fd\u91cf\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u975e\u53d7\u9650\u7684\u5207\u5411\u78b0\u649e\u6bd4\u6b63\u649e\u5bfc\u81f4\u7684\u4f24\u5bb3\u66f4\u5c11\uff0c\u78b0\u649e\u89d2\u5ea6\u5bf9\u4f24\u5bb3\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002\u6700\u7ec8\u6574\u5408\u6240\u6709\u5b9e\u9a8c\u6570\u636e\uff0c\u5efa\u7acb\u4e86\u8986\u76d6\u4e0d\u540c\u51e0\u4f55\u3001\u89d2\u5ea6\u3001\u78b0\u649e\u7c7b\u578b\u7684\u80fd\u91cf\u9608\u503c\u6570\u636e\u5e93\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5efa\u7acb\u4e86\u57fa\u4e8e\u80fd\u91cf\u7684\u673a\u5668\u4eba\u78b0\u649e\u4f24\u5bb3\u9632\u62a4\u6570\u636e\u5e93\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u79d1\u5b66\u3001\u53ef\u6269\u5c55\u7684\u80fd\u91cf\u9650\u503c\u4f9d\u636e\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u4e92\u52a8\u65f6\u7684\u5b89\u5168\u4fdd\u969c\u80fd\u529b\u3002"}}
{"id": "2602.20336", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20336", "abs": "https://arxiv.org/abs/2602.20336", "authors": ["Radoslaw Roszczyk", "Pawel Tecza", "Maciej Stodolski", "Krzysztof Siwek"], "title": "Natural Language Processing Models for Robust Document Categorization", "comment": "13 pages, 1 fiure, 5 tables", "summary": "This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.\n  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.\n  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e38\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8003\u5bdf\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u771f\u5b9e\u53ef\u7528\u7684\u6f14\u793a\u7cfb\u7edf\u3002", "motivation": "\u5728\u5b9e\u9645\u81ea\u52a8\u5316\u6d41\u7a0b\u4e2d\uff0c\u6587\u6863\u5206\u7c7b\u4efb\u52a1\u5e38\u8981\u6c42\u9ad8\u51c6\u786e\u7387\u540c\u65f6\u4e5f\u8981\u517c\u987e\u8ba1\u7b97\u8d44\u6e90\u4e0e\u8fd0\u884c\u6548\u7387\uff0c\u5c24\u5176\u5728\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6570\u636e\u65f6\u66f4\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u8fd9\u4e9b\u9700\u6c42\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u884c\u4e1a\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u53cc\u5411LSTM\uff08BiLSTM\uff09\u548c\u5fae\u8c03\u540e\u7684BERT\u4e09\u79cd\u6a21\u578b\uff0c\u5206\u522b\u6d4b\u8bd5\u5b83\u4eec\u5728\u6587\u6863\u81ea\u52a8\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u8017\u65f6\u548c\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u52a8\u8bf7\u6c42\u5206\u53d1\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u9a8c\u8bc1\u3002", "result": "BERT\u6a21\u578b\u51c6\u786e\u7387\u6700\u9ad8\uff08>99%\uff09\uff0c\u4f46\u8bad\u7ec3\u8017\u65f6\u548c\u8d44\u6e90\u6d88\u8017\u6700\u5927\uff1bBiLSTM\u5728\u51c6\u786e\u7387\uff08\u7ea698.56%\uff09\u548c\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u8fbe\u5230\u4e86\u826f\u597d\u5e73\u8861\uff1b\u6734\u7d20\u8d1d\u53f6\u65af\u5219\u8bad\u7ec3\u6700\u5feb\uff08\u6beb\u79d2\u7ea7\uff09\uff0c\u4f46\u51c6\u786e\u7387\u6700\u4f4e\uff08\u7ea694.5%\uff09\u3002\u6240\u6709\u65b9\u6cd5\u90fd\u53d7\u5230\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u7c7b\u8bc6\u522b\u4e0a\u3002", "conclusion": "BiLSTM\u6a21\u578b\u5728\u672c\u573a\u666f\u4e0b\u7efc\u5408\u8868\u73b0\u6700\u4f73\uff0c\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u4f18\u9009\u65b9\u6848\u3002\u6587\u4e2d\u8fd8\u6307\u51fa\u672a\u6765\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316Transformer\u7c7b\u6a21\u578b\u5e76\u6539\u5584\u5c11\u6570\u7c7b\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2602.20342", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20342", "abs": "https://arxiv.org/abs/2602.20342", "authors": ["Christos Maikos", "Georgios Angelidis", "Georgios Th. Papadopoulos"], "title": "Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques", "comment": "7 pages, 2 figures", "summary": "In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u5957\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5c06\u65e0\u4eba\u673a\u5b9e\u65f6\u89c6\u9891\u6d41\u5feb\u901f\u8f6c\u6362\u6210\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u7ed3\u679c\uff0c\u4e14\u5ef6\u8fdf\u6781\u4f4e\uff0c\u9002\u5408AR/VR\u7b49\u5b9e\u65f6\u4ea4\u4e92\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba13D Gaussian Splatting\uff083DGS\uff09\u7b49\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u5b9e\u65f6\u4e09\u7ef4\u91cd\u5efa\u9886\u57df\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u5728\u65e0\u4eba\u673a\u5b9e\u65f6\u91cd\u5efa\u4e0e\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8fd8\u4e0d\u5145\u5206\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u96c6\u6210\u73b0\u6709\u6280\u672f\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u7aef\u5230\u7aef\u4e09\u7ef4\u91cd\u5efa\u7684\u6548\u7387\u548c\u5b9e\u65f6\u6027\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u642d\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6548\u67b6\u6784\uff0c\u5c06RTMP\u5b9e\u65f6\u6d41\u3001\u540c\u6b65\u4f20\u611f\u5668\u878d\u5408\u3001\u6444\u50cf\u5934\u4f4d\u59ff\u4f30\u8ba1\u548c3DGS\u4f18\u5316\u6709\u673a\u7ed3\u5408\uff0c\u5b9e\u73b0\u6a21\u578b\u8fde\u7eed\u66f4\u65b0\uff0c\u5e76\u5728AR/VR\u73af\u5883\u4e2d\u4f4e\u5ef6\u8fdf\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u53ef\u89c6\u5316\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6e32\u67d3\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u66f4\u4f4e\uff1b\u91cd\u5efa\u7684\u7cbe\u5ea6\u4e0e\u79bb\u7ebf\u9ad8\u4fdd\u771f\u53c2\u8003\u76f8\u5dee\u4ec54-7%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u3001\u9ad8\u8d28\u91cf\u7684\u65e0\u4eba\u673a\u4e09\u7ef4\u91cd\u5efa\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u53ef\u6269\u5c55\u7684\u822a\u62cd\u589e\u5f3a\u611f\u77e5\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2602.20375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20375", "abs": "https://arxiv.org/abs/2602.20375", "authors": ["Jiashun Wang", "M. Eva Mungai", "He Li", "Jean Pierre Sleiman", "Jessica Hodgins", "Farbod Farshidian"], "title": "Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework", "comment": null, "summary": "Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u83b7\u5f97\u81ea\u7136\uff08\u5177\u6709\u4eba\u7c7b\u7279\u5f81\uff09\u7684\u8fd0\u52a8\u6280\u80fd\u548c\u9ad8\u5ea6\u9002\u5e94\u6027\uff0c\u907f\u514d\u4e86\u4ec5\u4f9d\u8d56\u53c2\u8003\u52a8\u4f5c\u6216\u7eaf\u4efb\u52a1\u9a71\u52a8RL\u7684\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u53d7\u9650\u4e8e\u53c2\u8003\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u800c\u7eaf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u7075\u6d3b\u4f46\u52a8\u4f5c\u8d28\u91cf\u901a\u5e38\u4e0d\u81ea\u7136\u3002\u4f5c\u8005\u5e0c\u671b\u7ed3\u5408\u4e24\u8005\u4f18\u70b9\uff0c\u5b9e\u73b0\u65e2\u6709\u4eba\u7c7b\u98ce\u683c\u3001\u53c8\u80fd\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u6846\u67b6\u8bad\u7ec3\u5355\u4e00\u7b56\u7565\u7f51\u7edc\uff0c\u540c\u65f6\u8fdb\u884c\u4e24\u4e2a\u4efb\u52a1\uff1a\u4e00\u662f\u4ee5\u53c2\u8003\u8f68\u8ff9\u4e3a\u5bc6\u96c6\u5956\u52b1\u6e90\u7684\u6a21\u4eff\u4efb\u52a1\uff08\u4f46\u8f68\u8ff9\u4e0d\u4f5c\u4e3a\u8f93\u5165\uff09\uff1b\u4e8c\u662f\u57fa\u4e8e\u968f\u673a\u76ee\u6807\u548c\u521d\u59cb\u72b6\u6001\u7684\u6cdb\u5316\u4efb\u52a1\uff0c\u4ec5\u6839\u636e\u76ee\u6807\u5b8c\u6210\u60c5\u51b5\u5956\u52b1\u3002\u7b56\u7565\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8fd9\u4e24\u79cd\u76ee\u6807\uff0c\u6c72\u53d6\u4eba\u7c7b\u52a8\u4f5c\u7279\u70b9\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u65b9\u6cd5\u65e0\u9700\u5bf9\u6297\u635f\u5931\u3001\u663e\u5f0f\u8ddf\u8e2a\u6216\u53c2\u8003\u4f9d\u8d56\u63a8\u7406\u3002", "result": "\u65b9\u6cd5\u5728\u590d\u6742\u7684\u7bb1\u5f0f\u8dd1\u9177\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5305\u62ec\u8df3\u8dc3\u3001\u6500\u722c\u7b49\u4e30\u5bcc\u8fd0\u52a8\u6280\u80fd\uff0c\u63a7\u5236\u5668\u5728\u8fdc\u79bb\u53c2\u8003\u5206\u5e03\u7684\u65b0\u4efb\u52a1\u4e2d\u4ecd\u4fdd\u6301\u81ea\u7136\u52a8\u4f5c\u8868\u73b0\u3002\u8fd8\u5c55\u793a\u4e86\u591a\u4e2a\u6280\u80fd\u7ec4\u5408\u751f\u6210\u957f\u65f6\u5e8f\u590d\u6742\u884c\u4e3a\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u517c\u987e\u4e86\u52a8\u4f5c\u7684\u81ea\u7136\u6027\u4e0e\u9002\u5e94\u6027\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u5404\u81ea\u7684\u74f6\u9888\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u7075\u5de7\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2602.20354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u89c6\u9891\u5373\u53ef\u81ea\u52a8\u8bc4\u4f30AI\u751f\u6210\u89c6\u9891\u771f\u5b9e\u6027\u7684\u65b0\u6846\u67b63DSPA\u3002\u8be5\u65b9\u6cd5\u7528\u7edf\u4e00\u7684\u7279\u5f81\uff0c\u7ed3\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u4fe1\u606f\u548c\u89c6\u89c9\u8bed\u4e49\u8bc4\u4ef7\u89c6\u9891\u771f\u5b9e\u6027\u4e0e\u7269\u7406\u5408\u7406\u6027\u3002\u5176\u7ed3\u679c\u4e0e\u4eba\u5de5\u8bc4\u5224\u9ad8\u5ea6\u4e00\u81f4\uff0c\u53ef\u66f4\u654f\u611f\u5730\u8bc6\u522b\u8fd0\u52a8\u4f2a\u5f71\u4e0e\u7269\u7406\u89c4\u5f8b\u8fdd\u80cc\u3002", "motivation": "\u76ee\u524dAI\u89c6\u9891\u751f\u6210\u6280\u672f\u8fdb\u6b65\u5f88\u5feb\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u6548\u7387\u4f4e\uff0c\u591a\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u5316\u3001\u901a\u7528\u4e14\u51c6\u786e\u7684\u8bc4\u4ef7\u4f53\u7cfb\u5341\u5206\u7f3a\u4e4f\u3002", "method": "\u63d0\u51fa3DSPA\uff083D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\uff09\uff0c\u5c063D\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548c\u8bed\u4e49\u7279\u5f81\u878d\u5408\uff0c\u6355\u6349\u7269\u4f53\u8fd0\u52a8\u53ca\u573a\u666f\u53d8\u5316\uff0c\u4e0d\u9700\u53c2\u7167\u89c6\u9891\u505a\u771f\u5b9e\u6027\u6253\u5206\u3002\u7cfb\u7edf\u80fd\u8bc6\u522b\u7269\u7406\u8fdd\u80cc\u3001\u8fd0\u52a8\u4f2a\u5f71\u7b49\u5f02\u5e38\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c3DSPA\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u654f\u611f\u4e8e\u89c6\u9891\u4e2d\u7684\u7269\u7406\u89c4\u5f8b\u51b2\u7a81\u4e0e\u8fd0\u52a8\u4f2a\u5f71\uff0c\u5176\u8bc4\u4ef7\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\u4eba\u7c7b\u5bf9\u89c6\u9891\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u5224\u65ad\u3002", "conclusion": "3DSPA\u8bc1\u660e\u4e86\u4e30\u5bcc\u4e09\u7ef4\u8bed\u4e49\u540e\u7684\u8f68\u8ff9\u8868\u793a\u5bf9\u89c6\u9891\u751f\u6210\u8bc4\u4ef7\u7684\u91cd\u8981\u6027\uff0c\u80fd\u81ea\u52a8\u63ed\u793a\u7269\u7406\u89c4\u5219\u51b2\u7a81\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b9a\u91cf\u57fa\u51c6\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.20497", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20497", "abs": "https://arxiv.org/abs/2602.20497", "authors": ["Peiliang Cai", "Jiacheng Liu", "Haowen Xu", "Xinyu Wang", "Chang Zou", "Linfeng Zhang"], "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration", "comment": null, "summary": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u9884\u6d4b\u6846\u67b6\uff08LESA\uff09\uff0c\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u52a0\u901fDiffusion Transformer\u6a21\u578b\uff08DiTs\uff09\uff0c\u5728\u591a\u9879\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6d88\u8017\u6781\u9ad8\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u548c\u5e94\u7528\u3002\u5df2\u6709\u52a0\u901f\u65b9\u6cd5\u5982feature caching\u5b58\u5728\u6cdb\u5316\u5dee\u3001\u8d28\u91cf\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u6269\u6563\u8fc7\u7a0b\u7684\u590d\u6742\u52a8\u6001\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86LESA\u9884\u6d4b\u5668\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528Kolmogorov-Arnold Network\uff08KAN\uff09\u5b66\u4e60\u65f6\u95f4\u7279\u5f81\u6620\u5c04\u3002\u63d0\u51fa\u591a\u9636\u6bb5\u3001\u591a\u4e13\u5bb6\u7ed3\u6784\uff0c\u4e3a\u4e0d\u540c\u566a\u58f0\u9636\u6bb5\u5206\u914d\u4e13\u7528\u7684\u9884\u6d4b\u5668\uff0c\u63d0\u9ad8\u7279\u5f81\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cLESA\u5728FLUX.1-dev\u3001Qwen-Image\u3001HunyuanVideo\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e865-6\u500d\u52a0\u901f\uff0c\u4e14\u51e0\u4e4e\u65e0\u8d28\u91cf\u635f\u5931\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u5927\u5e45\u8d85\u8d8a\u9886\u5148\u65b9\u6cd5TaylorSeer\uff0c\u5b9e\u73b0\u4e86SOTA\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6027\u80fd\u3002", "conclusion": "LESA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u7684\u8d28\u91cf\u635f\u5931\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u517c\u5177\u9ad8\u6548\u4e0e\u9ad8\u8d28\u91cf\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002\u4ee3\u7801\u5373\u5c06\u516c\u5f00\uff0c\u4fbf\u4e8e\u793e\u533a\u590d\u73b0\u548c\u5b8c\u5584\u3002"}}
{"id": "2602.20859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20859", "abs": "https://arxiv.org/abs/2602.20859", "authors": ["Zirui He", "Huopu Zhang", "Yanguang Liu", "Sirui Wu", "Mengnan Du"], "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction", "comment": "11 pages, 4 figures, 5 tables", "summary": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91d1\u878d\u6587\u672c\u9884\u6d4b\u7684\u65b0\u6846\u67b6\u2014\u2014FinAnchor\u3002\u5b83\u65e0\u9700\u5fae\u8c03\u4e0d\u540c\u7684LLMs\uff0c\u901a\u8fc7\u5bf9\u9f50\u548c\u805a\u5408\u591a\u6a21\u578b\u7684\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u91d1\u878d\u9884\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u957f\u6587\u672c\u4fe1\u606f\u590d\u6742\u3001\u5173\u952e\u4fe1\u53f7\u7a00\u758f\u4e14\u591a\u566a\u58f0\uff0c\u4e0d\u540c\u4efb\u52a1\u548c\u65f6\u671f\u5bf9LLM\u5d4c\u5165\u7684\u9700\u6c42\u5404\u5f02\uff0c\u5355\u4e00\u6a21\u578b\u5f88\u96be\u59cb\u7ec8\u8868\u73b0\u4f18\u5f02\u3002", "method": "FinAnchor\u901a\u8fc7\u9009\u62e9\u4e00\u4e2a\u951a\u70b9\u5d4c\u5165\u7a7a\u95f4\uff0c\u5c06\u5176\u4ed6\u6a21\u578b\u7684\u7279\u5f81\u7ecf\u8fc7\u7ebf\u6027\u53d8\u6362\u5bf9\u9f50\u5230\u8be5\u7a7a\u95f4\uff0c\u7136\u540e\u805a\u5408\u6240\u6709\u6a21\u578b\u7684\u5bf9\u9f50\u7279\u5f81\uff0c\u5f97\u5230\u7edf\u4e00\u8868\u8fbe\u7528\u4e8e\u540e\u7eed\u91d1\u878d\u9884\u6d4b\u4efb\u52a1\u3002\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u5bf9\u539f\u59cbLLM\u5fae\u8c03\u3002", "result": "FinAnchor\u5728\u591a\u4e2a\u91d1\u878dNLP\u4efb\u52a1\u4e2d\uff0c\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u5355\u6a21\u578b\u548c\u6807\u51c6\u96c6\u6210\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u901a\u8fc7\u5f02\u6784\u5d4c\u5165\u5bf9\u9f50\u5b9e\u73b0\u9c81\u68d2\u9884\u6d4b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u591a\u4e2aLLM\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u805a\u5408\u540e\uff0c\u80fd\u63d0\u5347\u91d1\u878d\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\uff1bFinAnchor\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u3001\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u591a\u6a21\u578b\u4f18\u52bf\u4e92\u8865\uff0c\u589e\u5f3a\u91d1\u878dNLP\u7cfb\u7edf\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.20556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20556", "abs": "https://arxiv.org/abs/2602.20556", "authors": ["Hanhui Li", "Xuan Huang", "Wanquan Liu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang", "Chenqiang Gao"], "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos", "comment": null, "summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.", "AI": {"tldr": "WildGHand\u662f\u4e00\u79cd\u4f18\u5316\u578b\u81ea\u9002\u5e943D\u624b\u90e8\u91cd\u5efa\u6846\u67b6\uff0c\u9488\u5bf9\u771f\u5b9e\u573a\u666f\u590d\u6742\u5e72\u6270\u5b9e\u73b0\u9ad8\u4fdd\u771f\u624b\u90e8\u6570\u5b57\u5316\uff0c\u5e76\u5f00\u6e90\u4e86\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u67093D\u624b\u90e8\u91cd\u5efa\u65b9\u6cd5\u591a\u4f9d\u8d56\u53d7\u63a7\u73af\u5883\u6570\u636e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u624b\u90e8\u4e0e\u7269\u4f53\u4ea4\u4e92\u3001\u6781\u7aef\u59ff\u52bf\u3001\u5149\u7167\u53d8\u5316\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u5f3a\u5e72\u6270\u4e0b\u8868\u73b0\u4e0b\u964d\uff0c\u96be\u4ee5\u6cdb\u5316\u81f3\u5b9e\u9645\u5e94\u7528\u3002", "method": "WildGHand\u6846\u67b6\u57fa\u4e8e3D\u9ad8\u65af\u70b9\u6e32\u67d3\uff0c\u6838\u5fc3\u5305\u62ec\uff1a\uff081\uff09\u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\uff0c\u5c06\u6270\u52a8\u663e\u5f0f\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5c5e\u6027\u7684\u65f6\u53d8\u504f\u5dee\uff1b\uff082\uff09\u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u751f\u6210\u5404\u5e27\u5404\u5411\u5f02\u6027\u52a0\u6743\u63a9\u7801\u4ee5\u6307\u5bfc\u4f18\u5316\uff0c\u4ece\u7a7a\u95f4\u4e0e\u65f6\u95f4\u7ef4\u5ea6\u8bc6\u522b\u5e76\u6291\u5236\u6270\u52a8\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5236\u4f5c\u4e86\u5305\u542b\u591a\u79cd\u771f\u5b9e\u5e72\u6270\u7684\u5355\u76ee\u624b\u90e8\u89c6\u9891\u6570\u636e\u96c6\u3002", "result": "WildGHand\u5728\u81ea\u5efa\u53ca\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cPSNR\u63d0\u5347\u9ad8\u8fbe15.8%\uff0cLPIPS\u964d\u4f4e23.1%\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "WildGHand\u6709\u6548\u63d0\u5347\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e0b\u5355\u76ee\u89c6\u9891\u624b\u90e8\u4e09\u7ef4\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2602.21082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21082", "abs": "https://arxiv.org/abs/2602.21082", "authors": ["Vishal Patil", "Shree Vaishnavi Bacha", "Revanth Yamani", "Yidan Sun", "Mayank Kejriwal"], "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification", "comment": null, "summary": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6df7\u5408\u6846\u67b6\uff0c\u81ea\u52a8\u8fdb\u884c\u5927\u89c4\u6a21\u5ba2\u6237\u8bc4\u4ef7\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9910\u996e\u884c\u4e1a\u5927\u6570\u636e\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u7528\u6237\u8bc4\u8bba\u6570\u91cf\u6fc0\u589e\uff0c\u4f01\u4e1a\u548c\u987e\u5ba2\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8fd9\u4e9b\u8bc4\u8bba\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u9ad8\u6548\u5206\u6790\u6570\u767e\u4e07\u6761\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u3002\u56e0\u6b64\uff0c\u4e0d\u65ad\u63a2\u7d22\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u517c\u5907\u7684\u81ea\u52a8\u5316\u5206\u6790\u65b9\u6cd5\u6210\u4e3a\u91cd\u8981\u8bfe\u9898\u3002", "method": "\u65b9\u6cd5\u9996\u5148\u5229\u7528ChatGPT\u4ece\u9910\u5385\u8bc4\u8bba\u62bd\u6837\u4e2d\u8bc6\u522b\u51fa\u7528\u9910\u4f53\u9a8c\u7684\u5173\u952e\u65b9\u9762\uff1b\u7136\u540e\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u8bc4\u8bba\uff0c\u8bad\u7ec3\u4f20\u7edf\u7684\u60c5\u611f\u5206\u7c7b\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u5230470\u4e07\u6761\u5927\u578b\u5728\u7ebf\u5e73\u53f0\u8bc4\u8bba\u6570\u636e\u3002\u6700\u540e\u901a\u8fc7\u56de\u5f52\u5206\u6790\u68c0\u9a8c\u673a\u5668\u6807\u6ce8\u7684\u5404\u65b9\u9762\u5bf9\u6574\u4f53\u8bc4\u5206\u7684\u89e3\u91ca\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u52a8\u5316\u673a\u5668\u6807\u6ce8\u7684\u7528\u9910\u65b9\u9762\u5728\u4e0d\u540c\u7528\u9910\u4f53\u9a8c\u7ef4\u5ea6\u3001\u83dc\u7cfb\u548c\u5730\u533a\u4e0b\uff0c\u5747\u80fd\u663e\u8457\u89e3\u91ca\u6574\u4f53\u9910\u5385\u8bc4\u5206\u7684\u65b9\u5dee\u3002\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bc4\u8bba\u60c5\u611f\u5206\u6790\u573a\u666f\u3002", "conclusion": "\u5c06LLM\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7ed3\u5408\uff0c\u517c\u987e\u4e86\u60c5\u611f\u5206\u6790\u7684\u7cbe\u5ea6\u4e0e\u6269\u5c55\u6027\uff0c\u4e3a\u9910\u996e\u548c\u5176\u4ed6\u670d\u52a1\u884c\u4e1a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5927\u89c4\u6a21\u5ba2\u6237\u53cd\u9988\u81ea\u52a8\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2602.20577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20577", "abs": "https://arxiv.org/abs/2602.20577", "authors": ["Jiaru Zhang", "Manav Gagvani", "Can Cui", "Juntong Peng", "Ruqi Zhang", "Ziran Wang"], "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion", "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u7684\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6269\u6563\u6846\u67b6MVLAD-AD\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3001\u8f68\u8ff9\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u65f6\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u3001\u52a8\u4f5c\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u89c4\u5212\u548c\u7406\u89e3\u7684\u9ad8\u6548\u7ed3\u5408\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Masked Vision-Language-Action Diffusion (MVLAD-AD) \u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4e0d\u5c06\u52a8\u4f5c\u5f3a\u884c\u6620\u5c04\u5230\u8bed\u8a00\u7a7a\u95f4\uff0c\u800c\u662f\u901a\u8fc7\u79bb\u6563\u52a8\u4f5c\u5206\u8bcd\u6cd5\u5c06\u53ef\u884c\u8f68\u8ff9\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u52a8\u4f5c\u7801\u672c\uff1b2\uff09\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u5b66\u4e60\u4ee5\u4fdd\u8bc1\u6f5c\u7a7a\u95f4\u4e2d\u5d4c\u5165\u4e0e\u7269\u7406\u51e0\u4f55\u76f8\u5173\u6027\uff1b3\uff09\u52a8\u4f5c\u4f18\u5148\u89e3\u7801\uff0c\u4f18\u5148\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "result": "\u5728nuScenes\u548c\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cMVLAD-AD\u5728\u63a8\u7406\u6548\u7387\u3001\u89c4\u5212\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4e3b\u6d41\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\u3002", "conclusion": "MVLAD-AD\u6709\u6548\u5e73\u8861\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c4\u5212\u6548\u7387\u4e0e\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.20583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20583", "abs": "https://arxiv.org/abs/2602.20583", "authors": ["Wonyong Seo", "Jaeho Moon", "Jaehyup Lee", "Soo Ye Kim", "Munchurl Kim"], "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models", "comment": "The first two authors contributed equally to this work (equal contribution)", "summary": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.", "AI": {"tldr": "PropFly\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u7f16\u8f91\u8bad\u7ec3\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5728\u7ebf\u751f\u6210\u7684\u76d1\u7763\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u9891\u7f16\u8f91\u8fc1\u79fb\u548c\u4f20\u64ad\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u4f20\u64ad\u7684\u89c6\u9891\u7f16\u8f91\u6280\u672f\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u914d\u5bf9\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u6210\u672c\u9ad8\u3001\u96be\u5ea6\u5927\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "PropFly\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574Classifier-Free Guidance\uff08CFG\uff09\u7cfb\u6570\uff0c\u65e0\u9700\u4e8b\u5148\u51c6\u5907\u914d\u5bf9\u6570\u636e\u96c6\u5373\u53ef\u5728\u7ebf\u751f\u6210\u4f4e\uff08\u6e90\uff09\u4e0e\u9ad8\uff08\u76ee\u6807\uff09CFG\u7684'\u6e90'\u4e0e'\u7f16\u8f91'\u8868\u5f81\u5bf9\uff0c\u5e76\u5f15\u5165\u989d\u5916\u9002\u914d\u5668\u548cGuidance-Modulated Flow Matching\uff08GMFM\uff09\u635f\u5931\u6307\u5bfc\u5b66\u4e60\u7f16\u8f91\u4f20\u64ad\u3002", "result": "\u5728\u591a\u9879\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u548c\u8bc4\u6d4b\u4e2d\uff0cPropFly\u4ea7\u751f\u7684\u7f16\u8f91\u7ed3\u679c\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u76ee\u524d\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u7f16\u8f91\u52a8\u6001\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PropFly\u8bc1\u660e\u65e0\u9700\u5927\u91cf\u914d\u5bf9\u6570\u636e\u96c6\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u7f16\u8f91\u4f20\u64ad\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u5e26\u6765\u6781\u5927\u7075\u6d3b\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.20608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20608", "abs": "https://arxiv.org/abs/2602.20608", "authors": ["Aihua Mao", "Kaihang Huang", "Yong-Jin Liu", "Chee Seng Chan", "Ying He"], "title": "VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos", "comment": null, "summary": "3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u5f15\u5bfc\u76843D\u7269\u4f53\u53ef\u4f9b\u6027\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u9759\u6001\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba-\u7269\u4ea4\u4e92\u63a5\u89e6\u533a\u57df\u7684\u8bc6\u522b\u6548\u679c\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u3002", "motivation": "\u73b0\u67093D\u7269\u4f53\u53ef\u4f9b\u6027\u5b9a\u4f4d\u65b9\u6cd5\u5927\u591a\u53ea\u4f9d\u8d56\u9759\u6001\u7684\u89c6\u89c9\u6216\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u80fd\u53cd\u6620\u53ef\u4f9b\u6027\u7531\u52a8\u6001\u52a8\u4f5c\u5b9a\u4e49\uff0c\u56e0\u6b64\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u5b9e\u9645\u4ea4\u4e92\u7684\u63a5\u89e6\u533a\u57df\u3002\u4f5c\u8005\u8ba4\u4e3a\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u548c\u6a21\u4eff\u52a8\u4f5c\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u7269\u54c1\uff0c\u56e0\u6b64\u5e94\u5f15\u5165\u52a8\u6001\u7684\u89c6\u9891\u7ebf\u7d22\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891\u5f15\u5bfc\u76843D\u53ef\u4f9b\u6027\u5b9a\u4f4d\u6846\u67b6VAGNet\uff0c\u5c06\u89c6\u9891\u4e2d\u7684\u4ea4\u4e92\u52a8\u4f5c\u7ebf\u7d22\u4e0e3D\u7ed3\u6784\u5bf9\u9f50\uff0c\u4ece\u800c\u89e3\u51b3\u7eaf\u9759\u6001\u65b9\u6cd5\u5b58\u5728\u7684\u6b67\u4e49\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u65b0\u6784\u5efa\u4e86PVAD\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5728\u914d\u5bf9\u7684\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\u4e0e3D\u6a21\u578b\u4e0a\u8fdb\u884c\u53ef\u4f9b\u6027\u6807\u6ce8\u3002", "result": "\u5728\u65b0\u63d0\u51fa\u7684PVAD\u6570\u636e\u96c6\u4e0a\uff0cVAGNet\u5728\u5b9a\u4f4d\u4eba-\u7269\u4ea4\u4e92\u63a5\u89e6\u533a\u57df\u7684\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u9759\u6001\u65b9\u6cd5\u663e\u8457\u66f4\u4f18\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u8868\u73b0\u3002", "conclusion": "\u5f15\u5165\u52a8\u6001\u89c6\u9891\u7ebf\u7d22\u80fd\u591f\u4e3a3D\u7269\u4f53\u53ef\u4f9b\u6027\u5b9a\u4f4d\u5e26\u6765\u66f4\u7cbe\u786e\u7684\u529f\u80fd\u6027\u76d1\u7763\uff0c\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002\u76f8\u5173\u7684\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u4e5f\u4f1a\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2602.20658", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20658", "abs": "https://arxiv.org/abs/2602.20658", "authors": ["Mohammad Sadra Rajabi", "Aanuoluwapo Ojelade", "Sunwook Kim", "Maury A. Nussbaum"], "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video", "comment": null, "summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ece\u89c6\u9891\u4e2d\u975e\u4fb5\u5165\u5f0f\u4f30\u7b97NIOSH\u4fee\u8ba2\u642c\u8fd0\u65b9\u7a0b\u4e2d\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u6444\u50cf\u89c6\u89d2\u4e0b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6d4b\u91cf\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u7684\u65b9\u6cd5\u9700\u4eba\u5de5\u6216\u4e13\u7528\u611f\u6d4b\u8bbe\u5907\uff0c\u5b9e\u9645\u5de5\u4f5c\u573a\u666f\u4e2d\u5e94\u7528\u56f0\u96be\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u7b80\u4fbf\u3001\u51c6\u786e\u3001\u975e\u4fb5\u5165\u5f0f\u7684\u65b0\u65b9\u6cd5\u63d0\u5347\u4eba\u4f53\u5de5\u5b66\u98ce\u9669\u8bc4\u4f30\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff1a\u4e00\u4e3a\u6587\u672c\u5f15\u5bfc\u7684\u68c0\u6d4b\u6d41\u7a0b\uff0c\u4e8c\u4e3a\u68c0\u6d4b\u52a0\u5206\u5272\u6d41\u7a0b\u3002\u5b83\u4eec\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u7528\u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u56de\u5f52\u6a21\u578b\u4f30\u7b97\u642c\u8fd0\u52a8\u4f5c\u8d77\u6b62\u65f6\u7684H\u548cV\u53c2\u6570\u3002\u901a\u8fc7\u4e03\u79cd\u6444\u50cf\u89c6\u89d2\u53ca\u8de8\u88ab\u8bd5\u9a8c\u8bc1\u8bc4\u4f30\u65b9\u6cd5\u8868\u73b0\u3002", "result": "\u5206\u5272\u4e3a\u57fa\u7840\u7684\u591a\u89c6\u89d2\u6d41\u7a0b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u6700\u4f18\uff0c\u6c34\u5e73\u8ddd\u79bb\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a6-8\u5398\u7c73\uff0c\u5782\u76f4\u8ddd\u79bb\u8bef\u5dee\u4e3a5-8\u5398\u7c73\u3002\u50cf\u7d20\u7ea7\u5206\u5272\u6bd4\u5355\u7eaf\u68c0\u6d4b\u6d41\u7a0b\u53ef\u5206\u522b\u5c06H\u548cV\u4f30\u7b97\u8bef\u5dee\u964d\u4f4e20-30%\u548c35-40%\u3002", "conclusion": "\u5229\u7528VLM\u7684\u591a\u9636\u6bb5\u89c6\u9891\u5206\u6790\u6d41\u7a0b\u80fd\u8f83\u4e3a\u51c6\u786e\u5730\u3001\u975e\u4fb5\u5165\u5f0f\u5730\u4f30\u7b97RNLE\u53c2\u6570\uff0c\u5177\u6709\u5728\u5b9e\u9645\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u573a\u666f\u4e2d\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.20664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20664", "abs": "https://arxiv.org/abs/2602.20664", "authors": ["Hailong Yan", "Shice Liu", "Tao Wang", "Xiangtao Zhang", "Yijie Zhong", "Jinwei Chen", "Le Zhang", "Bo Li"], "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?", "comment": "Tech Report", "summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAnimeAgent\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5b9a\u5236\u5206\u955c\u751f\u6210\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u81f4\u6027\u3001\u8868\u73b0\u529b\u53ca\u98ce\u683c\u5316\u6548\u679c\uff0c\u5e76\u6536\u96c6\u5efa\u7acb\u4e86CSG\u65b0\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u5b9a\u5236\u5206\u955c\u751f\u6210\u65b9\u6cd5\uff0c\u591a\u57fa\u4e8e\u9759\u6001\u6269\u6563\u6a21\u578b\uff0c\u65e0\u8bba\u5355\u6b21\u63a8\u7406\u8fd8\u662f\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u90fd\u5b58\u5728\u52a8\u6001\u8868\u8fbe\u529b\u5dee\u3001\u4e00\u81f4\u6027\u4e0d\u8db3\u53ca\u8bc4\u4f30\u6807\u51c6\u4e0d\u5065\u5168\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u8d28\u91cf\u3001\u591a\u89d2\u8272\u3001\u98ce\u683c\u5316\u6545\u4e8b\u53d9\u8ff0\u7684\u9700\u6c42\u3002", "method": "\u53d7\u8fea\u58eb\u5c3c\u201c\u9010\u5e27-\u5173\u952e\u5e27\u7ed3\u5408\u201d\u6d41\u7a0b\u542f\u53d1\uff0cAnimeAgent\u5c06\u56fe\u50cf\u5230\u89c6\u9891(I2V)\u6a21\u578b\u4f5c\u4e3a\u6838\u5fc3\uff0c\u5229\u7528\u5176\u9690\u5f0f\u8fd0\u52a8\u5148\u9a8c\u63d0\u5347\u5206\u955c\u95f4\u4e00\u81f4\u6027\u548c\u8868\u73b0\u529b\uff0c\u5e76\u5f15\u5165\u878d\u5408\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u7684\u8bc4\u5ba1\u673a\u5236\u652f\u6301\u8fed\u4ee3\u4f18\u5316\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u5206\u955c\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u65b9\u6cd5\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAnimeAgent\u5728\u89d2\u8272\u4e00\u81f4\u6027\u3001\u63d0\u793a\u543b\u5408\u5ea6\uff08prompt fidelity\uff09\u3001\u98ce\u683c\u5316\u7b49\u7ef4\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\uff08SOTA\uff09\u6027\u80fd\uff0c\u5e76\u79ef\u6781\u63a8\u52a8CSG\u4efb\u52a1\u6807\u51c6\u5316\u3002", "conclusion": "AnimeAgent\u4e3a\u52a8\u753b\u98ce\u683c\u5b9a\u5236\u5206\u955c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8868\u8fbe\u529b\u3001\u4e00\u81f4\u6027\u3001\u98ce\u683c\u5316\u53ca\u8bc4\u4f30\u6807\u6ce8\u65b9\u9762\u5747\u6709\u521b\u65b0\u7a81\u7834\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2602.21015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21015", "abs": "https://arxiv.org/abs/2602.21015", "authors": ["Yuhao Wu", "Maojia Song", "Yihuai Lan", "Lei Wang", "Zhiqiang Hu", "Yao Xiao", "Heng Zhou", "Weihua Zheng", "Dylan Raharja", "Soujanya Poria", "Roy Ka-Wei Lee"], "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "comment": "Work in processing. Website: https://social-ai-studio.github.io/CHAIN/", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCHAIN\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u68c0\u9a8c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7269\u7406\u7ed3\u6784\u7406\u89e3\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u76ee\u524d\u7684\u4e3b\u6d41\u6a21\u578b\u5728\u7ed3\u6784\u5316\u3001\u591a\u6b65\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u7136\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524dVLM\u7684\u8bc4\u4f30\u591a\u96c6\u4e2d\u5728\u5355\u8f6e\u3001\u4e0e\u7ed3\u6784\u65e0\u5173\u7684\u4efb\u52a1\u4e0a\uff0c\u65e0\u6cd5\u8861\u91cf\u6a21\u578b\u5bf9\u4e8e\u7269\u7406\u4e16\u754c\u590d\u6742\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\u7684\u63a8\u7406\u4e0e\u64cd\u4f5c\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u8003\u5bdf\u6a21\u578b\u7684\u771f\u5b9e\u7269\u7406\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86CHAIN\u57fa\u51c6\uff0c\u5305\u542b\u4ea4\u4e92\u5f0f3D\u7269\u7406\u73af\u5883\uff0c\u5982\u673a\u68b0\u62fc\u63a5\u3001\u7acb\u4f53\u5806\u53e0\u548c\u88c5\u7bb1\u7b49\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u7406\u89e3\u7269\u7406\u7ea6\u675f\u5e76\u5b8c\u6210\u7ed3\u6784\u5316\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u5bf9\u6700\u5148\u8fdb\u7684VLM\u4e0e\u6269\u6563\u6a21\u578b\u5728\u7edf\u4e00\u4ea4\u4e92\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6d4b\u8bd5\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u4f20\u7edf\u88ab\u52a8\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5185\u90e8\u5316\u7269\u7406\u7ed3\u6784\u3001\u56e0\u679c\u7ea6\u675f\u548c\u751f\u6210\u53ef\u9760\u957f\u5e8f\u5217\u8ba1\u5212\u65b9\u9762\u4ecd\u6709\u660e\u663e\u77ed\u677f\u3002\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u7a33\u5b9a\u5730\u5c06\u611f\u77e5\u5230\u7684\u7ed3\u6784\u8f6c\u5316\u4e3a\u6709\u6548\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u4e3b\u6d41VLM\u5728\u7ed3\u6784\u7406\u89e3\u4e0e\u957f\u65f6\u5e8f\u7269\u7406\u63a8\u7406\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\uff0cCHAIN\u57fa\u51c6\u4e3a\u6a21\u578b\u7684\u5b9e\u9645\u7269\u7406\u8ba4\u77e5\u4e0e\u56e0\u679c\u884c\u52a8\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6311\u6218\u6027\u7684\u53c2\u8003\u5de5\u5177\u3002"}}
