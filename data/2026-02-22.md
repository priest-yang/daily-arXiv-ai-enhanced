<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 本研究提出一种基于高斯分布渲染（GS）的数字孪生方法，用于高效精准地三维可视化土木基础设施损伤，并支持损伤随时间的动态更新。


<details>
  <summary>Details</summary>
Motivation: 现有土木工程基础设施检验对二维图像损伤识别依赖较多，难以进行精确的三维损伤可视化。传统三维重建方法（如光摄影测量）在渲染质量和无特征区域处理上存在局限。本文旨在提升数字孪生中的损伤三维表达能力，提高检测、监测和响应效率。

Method: 采用效率较高的高斯分布渲染（Gaussian Splatting, GS）来实现高质量三维重建，将二维损伤分割结果映射到三维模型上，并通过多尺度重建策略兼顾细节与效率。此外，方法支持数字孪生随损伤演化及时更新。

Result: 在开源的地震后基础设施损伤合成数据集上，方法实现了三维高质量的损伤可视化，有效减少了分割误差，并呈现出良好的效率与细节平衡。

Conclusion: 基于GS的数字孪生方法为基础设施的三维损伤可视化提供了有效技术支持，并具备动态适应损伤演化的能力，具有良好的工程应用前景。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 该论文提出了一个多维度的超大规模视频质量评价（VQA）数据集UltraVQA，并提出了新的评价方法ASO，实现了比现有模型更优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统VQA方法多只给出单一的平均主观评分，无法反映视频内容多维度的质量特性。作者希望通过更细致丰富的注释和新方法推动VQA的实际能力和解释性。

Method: 1）构建了覆盖五个重要质量维度（运动质量、运动幅度、美学质量、内容质量、清晰度质量）的UGC视频数据集UltraVQA。每个视频由3+人给出细致评分，并通过GPT生成解释理由。2）提出了分析分数优化（ASO）方法，从决策理论角度进行正则化，得到闭式解，可以更好模拟人的分级排序偏好，实现多维质量评分。

Result: 新方法在多项基准测试中优于多数闭源API和开源模型，在视频质量预测上显著降低平均绝对误差（MAE），验证了多维标签和强化对齐的有效性。

Conclusion: 多维、可解释的人类标注和基于强化对齐的新方法有助于提升VQA研究水平，为实际中更准确的视频质量评价奠定基础。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 本文提出了一种基于块离散扩散的新型视觉-语言模型DODO，用于加速光学字符识别（OCR）任务，在保证精度的同时大大提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉-语言模型（VLM）在OCR领域表现优异，但它们常用的自回归解码方式在处理长文档时效率低下、计算成本高。由于OCR输出严格由视觉输入决定（高确定性），理论上可以用扩散模型并行加速解码，因此作者希望突破自回归瓶颈，提高OCR推理速度。

Method: 提出DODO模型，首次在VLM中使用块离散扩散机制，将生成过程划分为多个块，以减缓全局扩散带来的同步误差，解决了以往扩散方法在OCR中的结构不稳定问题。模型既保证了精确输出需求，也实现解码的高并行度。

Result: DODO在达到接近SOTA（最优性能）的OCR识别准确率基础上，相比自回归基线方法，将推理速度提升至3倍。

Conclusion: DODO为OCR任务带来高效、准确的新解法，首次实现块离散扩散模型在该领域的应用，有效平衡精度和速度，具有较高实际应用价值。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 提出了一种新的适用于水下环境的立体匹配网络StereoAdapter-2，结合创新的卷积状态空间模型ConvSS2D和大规模水下合成数据集，实现了水下深度估计领域的领先性能。


<details>
  <summary>Details</summary>
Motivation: 水下机器人感知严重依赖于立体深度估计，但由于水下环境存在光线衰减、散射和折射，现有方法适应性较差，尤其在大视差和无纹理区域表现有限。因此亟需专为水下环境设计的立体深度估计方法，提升其精准性与适应性。

Method: 该方法提出用基于选择性状态空间模型的ConvSS2D操作器替换传统GRU更新器，采用四向扫描策略，与极线几何自洽，同时能捕捉垂直结构一致性，一步即可高效进行长距离信息传播。作者还构建了一个包含多种基线参数和漫射特性的8万对大规模水下合成双目数据集UW-StereoDepth-80K，利用语义风格迁移和视角新合成生成过程，并结合动态LoRA适应性强化模型泛化性。

Result: 在TartanAir-UW和SQUID两个公开水下数据集上分别取得17%和7.2%的零样本性能提升，并在BlueROV2实物平台上展示了方法的稳健性，达到最新最优水平。

Conclusion: StereoAdapter-2极大提升了水下立体匹配准确性与适应性，结合高效结构和合成数据集，在现实和公开基准中获得了显著进步，对水下机器人视觉感知具有重要推动作用。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 本文提出SemCovNet模型，针对现有数据集在语义覆盖方面存在的不平衡（SCI）问题，提出一系列新方法，显著提升了模型语义公平性，并用新指标量化该提升。


<details>
  <summary>Details</summary>
Motivation: 现代视觉模型不仅需要识别类别，还要理解丰富的语义和属性。现有数据集在语义层面存在长尾分布，导致视觉模型对少见语义表现差、偏差大。如何量化并消除这一语义覆盖不均是本文关注的问题。

Method: 提出了Semantic Coverage-Aware Network (SemCovNet)，包含三个关键组件：（1）Semantic Descriptor Map (SDM)用于学习语义表示；（2）Descriptor Attention Modulation (DAM)模块动态加权视觉和语义特征；（3）Descriptor-Visual Alignment (DVA)损失函数用于对齐视觉特征与语义描述。此外，提出Coverage Disparity Index (CDI)量化模型语义公平性。

Result: 在多个数据集上实验表明，SemCovNet方法显著改善了模型的可靠性并大幅度降低了CDI指标，提升了模型在少见语义上的表现和公平性。

Conclusion: SCI被确立为一种新型并可纠正的偏差。SemCovNet为提升视觉模型的语义公平性和可解释性提供了创新方法和理论基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: 本文提出了Xray-Visual，一个用于大规模图像与视频理解的统一视觉模型架构，基于海量社交媒体数据训练，具备高精度、多模态和良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前视觉模型在大规模多模态理解（图像与视频）上存在标签噪声高、语义多样性不足及训练效率挑战。因应产业级海量社交媒体数据的特性，亟需兼顾高效与准确的视觉理解统一方案。

Method: 1）构建包含150亿图像-文本对和100亿视频-标签对的大规模数据集；2）开发均衡与降噪的数据筛选流程；3）采用自监督MAE、半监督标签分类、CLIP风格对比学习三阶段联合训练图像和视频模态；4）以Vision Transformer主干并引入高效Token重组（EViT）以提升计算效率；5）集成大型语言模型做文本编码（LLM2CLIP），强化跨模态检索和泛化。

Result: 在ImageNet（图像分类）、Kinetics与HMDB51（视频理解）、MSCOCO（跨模态检索）等多个权威基准上取得最先进表现。模型对领域迁移和对抗干扰具有强鲁棒性。结合LLM2CLIP后，在实际场景检索与泛化更加突出。

Conclusion: Xray-Visual树立了可扩展多模态视觉模型的新标杆，兼顾高精度与高算效，对真实世界应用具有广泛的推动作用。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 本文提出了一种基于静止相机多通道NeRF的高通量高光谱三维重建系统（HSI-SC-NeRF），用于农产品的质量检测。新方法克服了复杂设备和低兼容性问题，实现了高效、精确的三维及高光谱重建。实验证实该方法在空间及光谱保真度上具有优异表现，对自动化农产品检测有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前农业中高光谱成像和三维重建技术虽能精确评价作物质量和性状，但二者的集成因硬件复杂和自动化兼容性差而受限。通过简化流程，实现高效、自动化、高通量的数据采集及分析，对推进农业可持续发展及育种研究具有重要意义。

Method: 构建了HSI-SC-NeRF系统：在定制特氟龙光照舱中，通过静止相机捕获旋转样品的多视角高光谱数据。用ArUco标记实现物体姿态估计，通过仿真将物体姿态转换到相机坐标系，实现NeRF训练。提出多通道NeRF结构，对全部高光谱波段联合优化引入复合光谱损失，并采用两阶段训练流程分离几何初始化与射线精细化。

Result: 在三种农产品样本上实验，HSI-SC-NeRF展示了高空间重建精度和出色的可见光至近红外光谱保真度，验证了新方法的有效性及其实用价值。

Conclusion: HSI-SC-NeRF系统能够高效实现高质量高光谱三维重建，适用于自动化农产品质量检测流程。为农业生产和研究中的高通量表型分析和质量评估提供了高效、兼容性强的技术方案。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 本文提出了一种动态分块（dynamic tokenization）技术，根据内容复杂度和去噪步数灵活调整图像/视频生成模型中的patch大小。在显著降低计算量的同时，保持生成质量不变。


<details>
  <summary>Details</summary>
Motivation: 目前DiT（Diffusion Transformers）虽在图像和视频生成领域表现出色，但其采用固定大小分块策略，导致整个去噪过程都存在高计算开销，效率低。作者希望通过动态调整分块，使之兼顾效率和效果。

Method: 作者提出：在推理（生成）阶段，动态调整patch大小。早期去噪步只需要较大patch（建模全局结构），后期逐步用更小patch（细化局部细节）。方案自动、逐步地优化patch分配，以适应内容和阶段需求。

Result: 动态分块方法在图像/视频生成任务上进行了大量实验，结果显示：在FLUX-1.Dev和Wan 2.1上分别获得了最高3.52倍和3.2倍的加速效果，且生成质量与提示词契合度未受影响。

Conclusion: 动态分块技术可明显降低生成任务推理成本，同时有效维持生成表现。该方法证明了分块大小的动态调整可在不损失质量前提下大幅提升效率，适合大规模部署。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: 本文提出了PRIMO模型，用于应对多模态数据中部分模态缺失的问题，并能量化缺失模态对预测的影响，在多种数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态数据经常存在缺失，如某些模态信息无法收集或只对部分样本可用，而现有多模态大模型主要假设数据完备，难以处理缺失情况。

Method: PRIMO是一种有监督潜变量插补模型，将缺失模态建模为潜变量，用于刻画该模态与已观测模态在预测任务下的关系。在推理阶段，从该潜变量的分布中采样，得到预测分布及缺失模态对预测的实例级影响。

Result: 在合成XOR、Audio-Vision MNIST及MIMIC-III等数据集上，PRIMO在完全缺失模态时表现与单模态方法相当，在模态完整时与多模态方法效果一致，并用方差度量实例级缺失模态影响。

Conclusion: PRIMO既能充分利用全部（完整及部分）训练样本，又能实例级量化模态缺失对预测的影响，方法有效并具解释性。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 本文提出了一种基于补丁的空间作者归属框架，用于区分人类与机器人在协同绘画中的创作贡献，并通过法医案例加以验证。该方法在补丁级和整体绘画级上均显示出较高准确率，还能量化人机融合创作的风格混合特征。


<details>
  <summary>Details</summary>
Motivation: 随着AI在创意生产中的主动参与度提升，记录和区分作者身份变得尤为重要，这不仅影响艺术本身，还涉及收藏与法律领域。人机协同创作下作者归属往往模糊，亟需可行的归属判定方法。

Method: 作者设计了基于扫描仪的补丁提取和分类框架，通过leave-one-painting-out交叉验证，在15幅人机协同抽象画中进行实验，对比纹理和预训练特征的多种基线，采用Shannon熵量化混合风格区块的不确定度。

Result: 方法在补丁级准确率达到88.8%，整画级多数票准确率86.7%，均优于基线（68.0%-84.7%）。对混合作品的分析显示，人工标注的混合区域有64%更高不确定度，说明模型能检测混合作者区域。

Conclusion: 本方案可针对特定人-机创作者对实现高效、样本数量要求低的作者归属，未来有望拓展至任意人机协同艺术场景，并为数据稀缺条件下的创意合作者归属提供方法论基础。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG提出了一种检索增强的单张图片3D生成方法，支持部件级结构建模和精细编辑，提升了多视图一致性和几何多样性。


<details>
  <summary>Details</summary>
Motivation: 当前单图3D生成难以处理多样和复杂的部件结构，同时缺乏高效准确的局部编辑能力。传统方法难以覆盖尾部部件几何，且多视图一致性弱，修改粒度不够细致。

Method: 方法上，PartRAG结合了外部部件数据库和diffusion transformer，将检索的真实部件示例引入生成流程。设计了层级对比检索模块，将2D图像patch与3D部件潜变量对齐，从1,236个部件注释模型中检索多样、真实的部件。此外，提出基于共享规范空间的部件级编辑器，能够在不重新生成整体的情况下对指定部件进行更换、属性优化等操作，且保持其他部件与多视图一致性。

Result: 在Objaverse、ShapeNet和ABO数据集上，PartRAG的Chamfer Distance从0.1726降至0.1528，F-Score从0.7472提升到0.844。具有38秒生成速度和5-8秒的交互编辑效率。生成结果部件边界更锐利、细结构保真度更高，对关节化物体表现鲁棒。

Conclusion: PartRAG显著提升了单图3D部件级建模的多样性、编辑性和一致性，在公开基准上取得竞争性结果，为3D生成系统带来更高的灵活性和实用性。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的模型压缩框架，可将高性能的60层双流MMDiT图文生成架构压缩为轻量级模型Amber-Image，显著降低计算和部署成本，同时维持或提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformer（DiT）架构虽推动了文生图技术进步，但因模型庞大、训练和推理资源消耗极高，导致实际应用和部署受阻，亟需压缩和高效化方案。

Method: 作者提出的压缩框架对Qwen-Image模型采用时间步敏感的深度剪枝策略、局部权重均值重初始化、层级蒸馏及全参数微调，先压缩为Amber-Image-10B。进一步通过深层双流结构至单流结构转化和渐进蒸馏，得到更小的Amber-Image-6B，两步均无需从零训练。整个流程低成本高效，无需大规模数据工程。

Result: 压缩后模型参数减少70%，全流程GPU时间不到2000小时。Amber-Image在标准基准测试（如DPG-Bench、LongText-Bench）上展现出与大模型相媲美的高保真度图像合成和文本渲染能力。

Conclusion: 提出的方法实现了高效压缩和训练，极大降低了文生图模型的资源门槛，使高保真、强文本理解的T2I模型更易于部署，具有较高的实际价值。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 提出了StructCore方法，通过结构化描述解决Max pooling在无监督异常检测中信息丢失的问题，显著提升图像级异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像级异常检测主要依赖Max pooling聚合像素异常分数。Max pooling只关注最大响应，丢弃了异常证据在图像中分布和结构的信息，导致正常与异常样本分数重叠、区分度下降。

Method: 提出无需训练的StructCore方法：对异常分数图计算低维结构描述符phi(S)，捕捉分布和空间特征，再通过在训练集的正常样本中估计的对角Mahalanobis校准优化图像级分数；不影响像素级定位。

Result: 在MVTec AD和VisA两个数据集上，StructCore分别获得了99.6%和98.4%的AUROC，优于以往的Max pooling方法，在图像级异常检测任务中表现出更强的鲁棒性和区分能力。

Conclusion: StructCore充分利用了异常分数图中的结构化信息，提高了异常检测的准确性和鲁棒性，是比传统聚合方法更优的方案。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 本文提出了Cholec80-port数据集，专注于腹腔镜手术中Trocar端口的高质量分割标注，并制定了严格的标准操作流程以提升后续几何应用的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Trocar端口在腔镜手术中常出现在视野中，因其镜面反射或纹理属性，容易聚集特征点，干扰下游如图像拼接、三维重建与视觉SLAM等任务。然而目前公开数据集中缺乏对Trocar端口的明确、几何一致的标注。

Method: 提出Cholec80-port数据集，基于Cholec80原始数据，建立了高保真度的Trocar端口分割支持，制定并遵循排除中央开口的标准SOP，对已有的公开数据按同标准进行清洗与统一。

Result: 通过实验表明，几何一致的标注方式能在不同数据集间显著提升模型的鲁棒性，这种提升超越了单纯扩大数据集规模带来的收益。

Conclusion: 精确且几何一致的Trocar端口标注对提升腹腔镜手术场景下的下游视觉任务具有重要价值，应推广统一的标注SOP。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: 该论文提出了CPL-VAD，一种基于双分支交叉伪标签的弱监督视频异常检测方法，在XD-Violence和UCF-Crime数据集上达到了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常检测仅用视频级标签，难以兼顾异常定位的时序精度与异常事件的语义区分。作者旨在设计能够发挥二者优势的高效架构。

Method: 提出CPL-VAD，包括异常检测和类别分类两个分支。异常检测分支进行时序片段异常定位，分类分支通过视觉-语言对齐实现异常类别识别。两分支通过交叉伪标签互补，实现优势互补。

Result: 在公开数据集XD-Violence和UCF-Crime上的实验显示，CPL-VAD在异常检测准确率和异常类别分类上均取得了最佳效果。

Conclusion: CPL-VAD利用交叉伪标签机制，有效结合了时序定位和语义判别的优点，在弱监督视频异常检测领域具有领先性能。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: 本文提出了ComptonUNet，一种混合深度学习框架，用于在低光子统计和高背景噪声下高效、精准地定位宇宙伽玛射线暴（GRBs）。该方法通过联合处理原始数据与图像重建，有效提升了在复杂环境中的GRB定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有探测手段在面对遥远、微弱的GRBs时受限于光子数量少与高背景噪声，导致定位精度和统计健壮性难以兼顾。针对这一难题，亟需一种既能抑制噪声又保证统计效率的新方法，以更好探测早期宇宙中的GRBs，为高能宇宙学研究提供支持。

Method: 作者提出了一种名为ComptonUNet的混合深度学习框架，将原始数据直接重建的统计优势与基于图像的去噪能力结合起来。通过在低地轨道典型环境下，利用逼真的GRB模拟数据详细测试了模型性能。

Result: 在多种低统计量和高背景噪声场景下，ComptonUNet在GRB定位精度上都显著优于传统方法，实现了更好的本底抑制与信号还原。

Conclusion: ComptonUNet为在恶劣探测环境中实现高精度GRB定位提供了有效手段，为未来宇宙高能瞬变事件的探测与研究带来重要推动。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 本论文提出将射频（RF）传感与3D高斯Splatting（GS）渲染结合，以提升在视角有限或视觉信息不可靠条件下的3D场景重建和渲染的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉为主的GS管道对摄像机视角数量有较高依赖，初始化与训练消耗高，且在恶劣天气、光照不足或遮挡情况下表现不佳。因此，作者探索能适应复杂环境且具备更强鲁棒性的方案。

Method: 该方法将RF传感（如汽车雷达）获得的稀疏深度信息与GS渲染结合，利用RF辅助深度预测生成高质量3D点云，以初始化不同GS架构下的高斯基元，实现多模态的3D场景重建。

Result: 数值实验表明，融合RF传感的数据能有效提升GS渲染的结构精度，兼具高保真度与高效率，且明显优于单纯视觉方式。

Conclusion: 将RF传感引入GS渲染框架，为复杂条件下3D重建与渲染提供更鲁棒且高效的多模态解决方案，展现了广阔应用前景。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: 本文提出了一种快速、无需重训练、无需相机和无需预定义标签的3D高斯Splatting（3DGS）交互式分割方法B3-Seg。该方法在速度和准确性上接近昂贵的有监督分割方法，可实现实时的3D场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS分割方法依赖预置视角、标签或代价高昂的重训练，难以满足影视、游戏等领域对实时交互编辑的需求。需要一种高效、无需预训练和专用数据的交互式分割新方法。

Method: 将3DGS分割问题转化为Beta-Bernoulli贝叶斯序列更新过程，并通过解析的期望信息增益（EIG）主动选择下一个视角，理论上实现了自适应单调性和次模性，保证采样策略的近似最优性。

Result: 在多个数据集上，B3-Seg分割效果与高成本的有监督方法相当，可在几秒钟内完成端到端分割，满足实时交互需求。

Conclusion: B3-Seg实现了高信息效率的3DGS实用交互分割，无需预设视角和重训练，适合影视、游戏等对实时资产编辑的场景。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BadCLIP++的新型多模态对比学习模型后门攻击方法，突破了隐蔽性与持久性两大难题，在极低投毒率下依然能维持超高攻击成功率，且对多种防御和微调具有强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对比学习的后门攻击面临隐蔽性不足及在持续微调或强检测下易失效的问题，主要由于跨模态触发器易被检测，以及低投毒率下触发梯度稀释导致攻击遗忘。针对这两个尚未充分建模和解决的问题，作者希望提出统一方案。

Method: 提出BadCLIP++框架，包括：（1）引入语义融合QR微触发器，将难以察觉的触发图案嵌入关键区域，保持干净数据统计特征，实现隐蔽与紧致分布；（2）目标对齐子集选择，增强低注入率下触发信号；（3）触发嵌入半径收缩与中心对齐，参数曲率控制加弹性权重固化，提升模型的攻击持久性。（4）首次分析和证明在置信域内，干净微调与后门目标梯度共向，理论支持攻击不易退化。

Result: 实验显示，在仅0.3%投毒率下，BadCLIP++在数字场景中的攻击成功率高达99.99%，比基线高11.4个百分点。在19种防御下，攻击成功率仍高于99.90%，且干净准确率损失小于0.8%。物理攻击成功率达65.03%，对水印去除等防御表现稳健。

Conclusion: BadCLIP++首次在多模态对比学习后门攻击中兼顾了隐蔽性与持久性，低投毒情境下可攻破各类防御，持久性强且影响正常功能极小，对未来相关模型安全评估和新防御设计具有重要参考价值。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯喷溅的单目非刚性内窥镜SLAM系统NRGS-SLAM，可更精准进行内窥场景下的运动与重建。该方法显著提升了相机定位和场景重建的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景常因软组织变形破坏刚性假设，导致相机运动与场景变形高度耦合，现有方法在解耦、场景表达保真度等方面存在局限，因而影响定位和重建质量。

Method: 1）提出使用3D高斯喷溅对场景进行高保真度建模。2）设计了变形感知的高斯图，将可学习的变形概率嵌入每个高斯基元，并通过贝叶斯自监督优化，无需外部变形标签。3）提出变形跟踪模块，优先估计低变形区域，实现粗到细位姿估计，再进行高效逐帧变形更新。4）变形建图模块逐步扩展和细化地图。5）引入稳健几何损失，融合几何先验以缓解问题不适定。

Result: 在多个公开内窥镜数据集上，NRGS-SLAM在相机位姿估计精度（RMSE最多下降50%）以及照片级真实感重建质量方面均显著优于最新方法。

Conclusion: NRGS-SLAM有效解耦了相机运动与软组织变形，提高了内窥镜SLAM的定位与重建表现，实验与消融研究证实了关键模块的有效性。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为VIG（视觉信息增益）的新指标，量化视觉输入对大模型推理的不确定性降低，促进视觉语言大模型去除语言偏见并加强视觉依据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言大模型虽然表现优异，但常常存在语言偏见，即仅凭语言生成答案而无需视觉证据。现有缓解方法缺乏细粒度度量视觉输入对训练样本或token的实际增益。

Method: 提出了VIG，一种以困惑度为基础的指标，用于衡量视觉输入带来的预测不确定性下降。并据此开展VIG指导的选择性训练，优先训练高VIG样本和token，以提升模型视觉依赖。

Result: VIG可在样本和token层面细粒度分析哪些信息受到视觉输入影响。在训练中应用后，该方法可提升视觉信息的使用，改善视觉基础能力，降低语言偏见，并在使用更少监督数据的情况下获得更优表现。

Conclusion: VIG为视觉语言模型提供了有效衡量和利用视觉信息的新手段，帮助训练更具视觉基础、降低语言偏见的模型。选择性训练可提升数据效率，为未来提升视觉语言模型的泛化能力提供新途径。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 本文提出了一种基于矩阵熵的新型多模态大模型视觉Token剪枝方法EntropyPrune，在加速推理的同时能最大限度保持模型性能，并显著优于现有剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型由于需要处理大量视觉tokens，推理成本极高。虽然token剪枝能加速推理，但现有方法多依赖静态经验决策，选择剪枝层次和位置缺乏理论依据、通用性差。为此，作者希望找到更有理论支撑的剪枝时机，并提升剪枝方法的泛化性和效率。

Method: 作者从矩阵熵出发，发现视觉表示的信息熵在某一特定层（称为“熵塌缩层”ECL）会发生剧烈且一致性的信息量骤降。据此提出EntropyPrune框架，不依赖注意力图，通过熵值评估每个视觉token的信息价值，剪除冗余token。此外，作者借助双Gram矩阵的谱等价性，大幅降低了熵计算复杂度。

Result: 在多个多模态基准上，EntropyPrune在准确率和效率上都优于SOTA剪枝方法。例如在LLaVA-1.5-7B模型上，FLOPs下降68.2%，但能保持96.0%的原始性能。此外，该方法在高分辨率和视频类多模态模型中也表现出良好泛化性和鲁棒性。

Conclusion: 作者提出的EntropyPrune为大模型视觉token剪枝提供了理论支撑和高效实用的方案，解决了剪枝层次选择的长期难题，取得了远超现有方法的加速与泛化能力，有望广泛用于实际多模态大模型部署和推理加速。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 提出了一种新方法GASS，通过分析生成的图像在CLIP特征空间中的几何分布，从prompt依赖（语义）和非依赖（背景等）两个正交方向提升T2I模型生成图像的多样性，同时保持生成质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像（T2I）模型虽然能较好地理解语义，但生成内容多样性较低，导致用户选择有限，甚至会强化社会偏见。需要在提升多样性的同时，不削弱图像质量和语义一致性。

Method: 方法创新点在于将多样性分解为与prompt相关和无关的两部分，在CLIP嵌入空间分别沿语义方向和正交方向进行多样性提升。具体做法是提出Geometry-Aware Spherical Sampling（GASS），沿两个正交轴增加嵌入的投影分布，引导扩散或流模型的生成过程并提升多样性。

Result: 在不同的冻结T2I架构和数据基准上验证，GASS方法能有效提升图像多样性，且对图像保真度和语义一致性的负面影响极小。

Conclusion: GASS方法能在不牺牲生成质量和语义对齐性的前提下，大幅增强T2I模型生成样本的多样性，是现有多样性提升方法的有效补充。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [HiMAP: History-aware Map-occupancy Prediction with Fallback](https://arxiv.org/abs/2602.17231)
*Yiming Xu,Yi Yang,Hao Cheng,Monika Sester*

Main category: cs.CV

TL;DR: 本文提出了HiMAP，无需依赖多目标追踪（MOT）即可进行准确的运动轨迹预测，即使在追踪失败时依然表现良好。其通过历史占用图和新型历史查询模块，保障了预测的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的多数轨迹预测方法依赖于MOT和ID关联，但追踪失败（如遮挡、ID交换、漏检等）会显著降低预测准确性，增加安全风险。需要一种在追踪失效时依然可靠的预测方法。

Method: HiMAP将历史检测结果转化为空间-时间不变的历史占用图，并引入历史查询模块，结合当前主体状态动态检索与主体相关的历史信息。这些历史数据通过时序嵌入和DETR风格解码器来预测多模态未来轨迹，无需对象ID。

Result: 在Argoverse 2数据集上，HiMAP在无ID情境中达到与追踪方法相近的表现，在无追踪设定下大幅超越主流方法（FDE提升11%，ADE提升12%，MR下降4%），同时实现了所有交通主体的稳定预测。

Conclusion: HiMAP解决了运动预测对MOT和ID依赖的问题，有效提升了无追踪或追踪失效场景下的鲁棒性，增强了自动驾驶系统的安全性和实用性。

Abstract: Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.

</details>


### [25] [Inferring Height from Earth Embeddings: First insights using Google AlphaEarth](https://arxiv.org/abs/2602.17250)
*Alireza Hamoudzadeh,Valeria Belloni,Roberta Ravanelli*

Main category: cs.CV

TL;DR: 本研究评估了AlphaEarth Embeddings（10米分辨率）在区域地表高程估算中的有效性，利用轻量级U-Net和U-Net++模型进行回归。结果表明，嵌入特征包含了可解码的高程信息，U-Net++在测试集上的泛化和鲁棒性优于U-Net，但仍面临泛化误差挑战。整体显示AlphaEarth Embeddings有助于深度学习高程映射，但需进一步降低偏差以提升区域适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统的地表高程测量现代化程度低、成本高，深度学习需要高质量和丰富信息指导。研究动机是验证地理空间多模态的Earth Embeddings能否为深度学习提供足够的信息，以便高效、自动化地实现区域高程估算。

Method: 采用AlphaEarth Embeddings（10米空间分辨率），结合高精度数字表面模型（DSM）作为参考，使用U-Net和U-Net++两种轻量级卷积神经网络架构，对地表高程进行回归建模，比较不同模型在训练集和测试集上的表现。

Result: 在训练阶段，两种网络R^2均达0.97。测试阶段由于分布差异，性能下降，U-Net++的R^2为0.84，中位差-2.62米，优于U-Net的R^2 0.78、中位差-7.22米。两种模型RMSE约16米，存在泛化误差和残差偏差。

Conclusion: AlphaEarth Embeddings可有效支持深度学习地表高程映射，卷积架构可提取与高程相关的地理信息。U-Net++泛化能力较强，但整体仍需解决泛化误差和偏差问题，以提升区域适应性。

Abstract: This study investigates whether the geospatial and multimodal features encoded in \textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.

</details>


### [26] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文提出并验证了一种基于基础设施的多模态货运车辆检测系统，集成了LiDAR和摄像头，能够实时准确感知车辆类型、位置和速度，有助于更好地实施货运信号优先策略。


<details>
  <summary>Details</summary>
Motivation: 为有效实施货运信号优先控制，亟需高精度且实时的货运车辆检测和运动估计手段，以便更好地管理和优化信号控制，提高道路通行效率。

Method: 设计并部署了结合激光雷达和摄像头的混合感知架构，包括交叉口和路段两大子系统，通过无线通信同步数据。感知管线结合了聚类方法、深度学习检测以及卡尔曼滤波追踪，实现了实时、稳定的车辆检测与追踪；激光雷达数据还进行了地理坐标校准，用于车道级定位。

Result: 现场测试结果显示，该系统能够以高时空分辨率可靠感知和追踪货运车辆，准确监测其运动，实现需求目标。

Conclusion: 所提出的基础设施感知系统在实际部署中表现可靠，为未来开发和推广用于货运信号优先的基础设施感知系统提供了实用的经验和参考。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [27] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: 该论文提出了EA-Swin模型，一种面向AI生成视频检测的高精度、强泛化能力的方法，并构建了包含13万视频的新数据集EA-Video，实验证明领先于以往方法。


<details>
  <summary>Details</summary>
Motivation: 随着Sora、Veo等底座级视频生成器的进步，生成视频越来越逼真，现有检测方法（如浅层特征/图像适应/大模型）已难以应对，迫切需要新的检测机制。

Method: 设计了一种Embedding-Agnostic Swin Transformer（EA-Swin），该方法直接在预训练特征基础上利用因式分解窗注意力建模时空依赖，与主流ViT类编码器兼容；同时推出了涵盖多种生成器和跨分布的EA-Video大规模基准数据集。

Result: EA-Swin在主要视频生成器上达到0.97-0.99的检测准确率，比SOTA方法高出5-20%，对未见分布同样表现出很强泛化能力。

Conclusion: EA-Swin为检测最新AI生成视频提供了准确且可扩展的解决方案，有效弥补了现有方法在实际应用中的不足。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [28] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种结合物理知识的深度学习超分辨方法（PESTGAN），显著提升了热带气旋卫星图像的结构和物理保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的卫星图像超分辨方法通常未将云层运动背后的大气物理规律纳入建模，导致超分结果虽视觉上逼真，但物理合理性不足。提升台风等极端天气的预测和分析能力，需要超分图像在物理结构上更加准确。

Method: 提出Physics Encoded Spatial and Temporal GAN（PESTGAN），设计解耦生成器结构，在生成器中集成了PhyCell模块，利用受涡度方程约束的卷积，作为隐式物理动态表征，与视觉纹理分离。还引入空间与时间双鉴别器，分别约束生成图像的空间和运动一致性。

Result: 在Digital Typhoon数据集上进行4倍超分实验，PESTGAN在结构保真度和感知质量上超过了现有方法；同时保持像素精度竞争力，在物理结构（如云层形态）重建上表现更优。

Conclusion: PESTGAN有效结合了物理先验和深度学习，提升了卫星图像超分的物理真实性和结构精度，为气象监测和预报提供了更有价值的数据基础。

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [29] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 本文提出使用 attachment anchors（附着锚点）来提升结直肠手术中组织抓取点的预测准确性，通过引入结构化的区域表征，提高了基于机器学习的自动组织操作能力，尤其可适用于复杂多变的手术环境。


<details>
  <summary>Details</summary>
Motivation: 结直肠等复杂外科手术中，组织操作难度大且时间长，现有研究中对此领域关注不足。同时，这类手术中组织重复抓取频繁，为基于机器学习的辅助操作提供了丰富的学习场景。主要挑战在于如何在变化多端的手术场景下准确预测抓取点。

Method: 提出了 attachment anchors 概念，将组织与其解剖附着关系以结构化形式编码，通过该表示对手术场景进行归一化，从而降低抓取点预测的不确定性。使用腹腔镜图像预测锚点，并将其集成进基于机器学习的抓取框架。

Result: 在90例结直肠手术数据集上进行实验，结果显示，基于 attachment anchors 的方法比仅基于图像的传统方法在抓取点预测上表现更好，尤其在未见过的手术类型和操作者等分布外场景下效果显著提升。

Conclusion: attachment anchors 为结直肠手术中的基于学习的组织操作提供了有效的中间表示，提升了抓取点预测的准确性，有望促进自动化手术操作的发展。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [30] [Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline](https://arxiv.org/abs/2602.17322)
*Mohamed Dhouib,Davide Buscaldi,Sonia Vanier,Aymen Shabou*

Main category: cs.CV

TL;DR: 本文提出了一种高质量篡改文档图像的生成方法，显著提升了篡改文本检测模型在真实场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有通过规则生成的篡改文档种类单一且容易留下明显伪造痕迹，导致检测模型泛化能力差，在真实数据上的表现有限。作者旨在解决高质量数据不足的问题，从而提升篡改检测的鲁棒性和实际适用性。

Method: 作者设计了两种辅助网络：一个用于对比学习，通过新的正负样本定义策略对比不同文本片段，另一个用于判断裁剪文本区域是否完整。结合上述辅助网络，设计了一条全新的数据生成流程，生成具有高度多样性和真实感的篡改文档图像。

Result: 作者采用相同训练流程，将基于新方法和现有方法生成的数据集分别用于模型训练。实验在多个开源数据集和不同架构模型上进行评估，结果显示新方法生成的数据显著提升了各类模型在真实场景下的检测表现。

Conclusion: 新提出的数据生成框架能生成高质量、具多样性的篡改文档，有效提升了检测模型的泛化能力和实际应用价值。

Abstract: Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.

</details>


### [31] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: 本文提出了Polaffini框架，一种以解剖结构为基础的医学影像配准新方法，利用深度学习分割模型获得特征点，实现高效准确的结构对齐，优于主流强度配准方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像配准多依赖强度为基础的方法，需要依赖间接的对齐质量度量。理论上更优的基于解剖特征的方法因特征提取困难已被边缘化。然而，深度学习显著提升了解剖特征分割的质量，因此可以重新探索基于特征的精确配准方法。

Method: 提出Polaffini方法，首先利用预训练的深度学习分割模型自动获得图像中解剖结构分割，再提取各结构的质心作为特征点，为图像之间建立一一对应关系，然后通过闭式解法实现局部和全局的仿射配准，并结合polyaffine变换及log-Euclidean框架，确保高自由度下的可微性和配准的平滑性。

Result: Polaffini相较流行的基于强度的方法（如常用配准工具），在结构对齐指标上取得了更优性能，并为后续的非线性配准提供了更优的初始化。同时，Polaffini具有运行速度快、鲁棒性高等优点。

Conclusion: Polaffini作为一种基于解剖特征的新型影像配准框架，不仅实现了精确、快速和稳健的结构对齐，且适用于独立配准及后续非线性配准预处理，易于集成到医学影像处理流程中，展现了优越性。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [32] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 本文提出了一种用于机器人视觉-语言-动作模型（VLA）的新基准与新方法，从根本上提高了这些模型根据语言指令的准确执行能力，尤其是在容易被数据集偏见误导的场景。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在理解并执行自然语言指令时，容易受到视觉上的捷径（如选择训练中常见的物体或动作）的影响，忽略了实际语言意图，导致“反事实失效”。

Method: 1）提出了LIBERO-CF，该基准通过视觉上实现可能但指令不同的任务，系统化评测VLA在反事实情景下的鲁棒性；2）提出了Counterfactual Action Guidance（CAG），该方法采用双分支推理：一条是标准的VLA决策分支，另一条则不包含语言条件，仅基于视觉-动作信息，通过两者对比选择更符合语言指令的动作，无需额外数据或模型结构修改。

Result: CAG方法可跨多种VLA模型直接集成，无需训练即可显著提升理解语言并执行任务的准确性。在LIBERO-CF基准下，CAG在跟随语言指令的准确率上提升了9.7%，在低观测任务上的成功率提升3.6%，结合VA模型最高提升分别达15.5%和8.5%。现实世界评测中，CAG可减少9.4%的反事实失效，并提升17.2%的任务成功率。

Conclusion: 当前VLA模型在十分关键的“反事实”情景下普遍存在语言理解缺陷，CAG为此提供了简单、有效且易于集成的解决方案，显著提升了模型的鲁棒性和实际应用价值。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [33] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 本文创建了南美首个10米分辨率的树作物分布图,利用深度学习处理卫星影像,提高了农林地识别精度,有助于更准确执行无毁林产品法规。


<details>
  <summary>Details</summary>
Motivation: 监测树作物扩张对实施“零毁林”政策(如欧盟无毁林产品法规EUDR)至关重要。但是,缺乏能区分多样农业系统与森林的高分辨率数据,影响了政策的有效落实。

Method: 研究采用多模态时空深度学习模型,以Sentinel-1与Sentinel-2卫星影像时间序列训练,生成南美区域10米分辨率的树作物地图。

Result: 地图揭示南美约有1100万公顷树作物,其中23%与2000-2020年森林覆盖损失有关。分析发现,官方用于EUDR的监管地图常将成熟农业用地(尤其是小农混农林业)误判为‘森林’,带来虚假毁林警报及对小农户的不公处罚风险。

Conclusion: 本文提供的高分辨率基线数据可以减少误判与不公平,为制定更有效、包容和公平的生态保护政策提供支持。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [34] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出了一种基于Retentive Networks（RetNet）的手写文本识别新模型DRetHTR，在推理速度和内存效率方面大幅超越传统Transformer，同时准确率保持领先。


<details>
  <summary>Details</summary>
Motivation: 当前主流的手写文本识别系统多基于Transformer结构，但其在解码过程中，随着输出序列增加，其KV缓存不断变大，导致推理速度慢、内存消耗高。研究者希望找到一种能在保证精度的同时，提高推理效率、降低内存消耗的方法。

Method: 本文提出使用RetNet结构（即保留网络），设计了一个仅包含解码器的模型DRetHTR。通过用无softmax的retention机制替代传统注意力机制，结合多尺度时序先验，避免了KV缓存随序列增长，并提出了层级gamma scaling机制，使浅层建模短程依赖，深层关注全局背景，弥补了去除softmax后模型灵活性的损失。

Result: 与等规模的传统解码器Transformer模型相比，DRetHTR推理速度提升1.6-1.9倍，内存消耗减少38-42%，且精度无损。在多个公开数据集（IAM-A、RIMES、Bentham和READ-2016）上，取得了目前已知最优的字符错误率（2.26%、1.81%、3.46%、4.21%）。

Conclusion: DRetHTR在不牺牲精度的前提下，显著提升了推理速度和内存效率，充分表明基于RetNet的Decoder-only结构能替代引Transformer，实现更高效的手写文本识别。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [35] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 本文提出了SpectralGCD方法，通过用CLIP跨模态图像-概念相似性构建统一表示，实现高效、语义对齐的通用类别发现（GCD），并在六个基准数据集上，以低计算成本达到或超过现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 传统GCD方法过度依赖视觉特征，导致对已知类别过拟合，新兴的多模态方案虽然引入文本信息但效率低、计算成本高，因此急需一种既高效又充分利用语义信息的GCD方法。

Method: SpectralGCD以CLIP生成的图像与大规模概念字典深度相似性为基础，将每张图片表示为统一的跨模态概念混合。提出光谱过滤，通过教师模型的跨模态协方差矩阵自动选取相关概念，并采用正逆知识蒸馏策略确保学生模型获得语义兼容、对齐的表征。

Result: 在六个基准测试集上，SpectralGCD的准确率与现有SOTA持平或显著更优，同时计算成本大幅降低。

Conclusion: SpectralGCD不仅能高效地发现未知类别，还能增强表征的语义质量和泛化性，提供了GCD任务的新标准。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [36] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文对基于无人机RGB相机进行的光学遥感研究进行了全面综述，涵盖了数据集、主要任务、能力及见解。


<details>
  <summary>Details</summary>
Motivation: 随着无人机广泛应用且普遍配备RGB相机，以及计算机视觉近期的快速进步，光学遥感领域取得巨大发展。作者发现现有文献缺乏一篇从整体角度梳理该领域的综述，对新入门研究者不够友好。

Method: 系统搜集、梳理并总结了基于RGB相机的光学遥感任务、能力、常用数据集和最新见解，以全面呈现该领域现状。

Result: 文中整理了光学遥感的主要任务类别、主流方法和代表性数据集，指出了现有研究的优缺点及未来发展方向。

Conclusion: 本综述为光学遥感新入门研究者提供了系统的领域知识和高层次见解，有助于他们快速定位自身研究兴趣，推进该领域发展。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [37] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: 该论文提出了一种无需微调的工业异常检测方法EAGLE，通过专家模型的输出来指导多模态大模型（MLLM）进行更准确的异常检测和可解释性描述。实验表明，该方法在多个基准集上取得了与微调方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测是智能制造的关键，但当前深度学习方法往往只给出二分类结果，且缺乏语义可解释性。多模态大模型具备语言表达能力，有望提升检测解释性，但其检测准确率和成本问题未被很好解决。

Method: 提出了一种专家增强注意力引导（EAGLE）框架，融合了专家模型对异常的检测结果，引导MLLM在检测与解释任务上更准确一致且具备可解释性，不需要额外微调。论文分析了EAGLE如何影响MLLM中间层对异常区域的注意力分布。

Result: 在MVTec-AD和VisA数据集的实验结果显示，该方法在多个多模态大模型上均提升了异常检测的性能，且无需更新参数，表现与微调主导的方法相当。发现EAGLE可以提升模型对异常区域的注意力集中度。

Conclusion: EAGLE框架无需微调即可提升多模态大模型在工业异常检测上的准确性和可解释性，为实际部署提供了低成本高性能的新方案。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [38] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于单目内窥镜视频的4D重建方法Local-EndoGS，能够应对内窥镜视角大幅变化及场景变形，实验效果优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于可变形手术场景重建的方法多依赖双目深度先验或准确的结构光流初始化，且往往要求内窥镜视角固定，限制了在实际临床中大幅相机运动与仅有单目视频场景的应用。

Method: 提出Local-EndoGS框架：采用滑动窗口的全局场景表示，将场景分解为多个局部可变形模型；引入多视角几何、跨窗口信息和单目深度先验，以粗到细的方式进行融合与优化；同时加入全局2D像素轨迹约束及物理运动先验，提升变形合理性和重建效果。

Result: 在三个公开可变形内窥镜视频数据集上，面对大幅相机运动，Local-EndoGS在外观质量和几何精度上均大幅超越当前最先进方法。消融实验也验证了关键模块的有效性。

Conclusion: Local-EndoGS为临床复杂可变形场景的单目4D重建提供了强大的新工具，具有出色的实用前景和性能优势。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [39] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 本论文提出了针对二维量子材料光学显微图像表征的物理感知多模态框架，综合改进数据生成、指令调优和特征融合，有效提升了模型泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 二维量子材料的层数表征在光学显微图像中微妙且依赖专家标注，且不同实验室和设备存在巨大差异，现有的视觉模型缺乏物理先验知识，难以泛化至新材料或新硬件。

Method: 1）开发物理驱动的合成数据生成器Synthia，模拟薄膜干涉下的真实光学响应，减少人工标注依赖。2）构建QMat-Instruct，多模态物理指导的指令数据集，通过问答形式提升多模态大模型对材料厚度与外观的理解。3）提出QuPAINT架构，引入Physics-Informed Attention模块，将视觉特征与光学物理先验有效融合。4）建立QF-Bench基准，覆盖多种材料、基底和成像条件，标准化评测流程。

Result: 新方法显著提升了二维量子材料光学图像的表征准确率和模型泛化能力。在多种材料与成像条件下都获得了更鲁棒和判别性更强的特征，且减少了对专家手动标注的依赖。

Conclusion: 物理感知与多模态融合为解决量子材料图像分析的瓶颈提供了新思路，为后续相关领域标准化和通用性模型提供了基础。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [40] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 该论文提出PixTrace和CopyNCE两大技术，实现了对图片复制检测任务中复杂内容编辑的更精细特征表达，显著提升了检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以处理复杂图片编辑，因为它们缺乏对细粒度对应关系的学习，导致应对多样编辑手段时表现不足。

Method: 1. 提出PixTrace模块，通过跟踪像素坐标，建立不同编辑变换下的显式空间映射。2. 提出CopyNCE损失函数，利用PixTrace获得的重叠比值引导对比损失，强化patch间的几何一致性。整体方法在SSL训练时通过像素级追踪与patch相似性学习紧密结合，以抑制噪声干扰。

Result: 在DISC21数据集上，用作匹配器时取得了88.7% uAP / 83.9% RP90、用作描述符时取得了72.6% uAP / 68.4% RP90，均为当前最优水平。此外，该方法在可解释性方面也优于现有方法。

Conclusion: 通过结合像素级可追溯性和patch级相似性学习，新方法在图片复制检测任务中达到了更高精度和更强解释能力，推动了自监督特征表征能力的进步。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [41] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度图和彩色姿态估计器的增强现实注册方案，通过非刚性迭代最近点（NICP）代替有限元（FE）方法，实现肝脏腔镜手术中肿瘤定位的自动化和简化。实验表明，方法精度高、开发门槛低。


<details>
  <summary>Details</summary>
Motivation: 现有增强现实配准多依赖器官轮廓及复杂有限元建模，增加工程难度与专业门槛；作者希望简化非刚性配准流程，提高注册效率和易用性。

Method: 作者将腹腔镜深度图与基础姿态估计模型结合，用于相机与肝脏的位姿估计，并用非刚性ICP算法取代有限元变形建模。

Result: 在实际病人数据上，方法取得了9.91 mm的平均配准误差，刚性+NICP注册方法优于传统刚性注册，NICP能有效替代有限元模型。

Conclusion: 所提流程具备临床相关精度，且模型轻量、工程友好，为FE模型提供了一种实用替代方案。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [42] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种新方法LATA（Laplacian-Assisted Transductive Adaptation），可提高医学视觉-语言模型在领域迁移和不确定性校准下的表现，在保证覆盖率的同时提升预测集效率与类别平衡，无需额外模型训练或标签。实验表明LATA在多个任务和模型上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型（VLMs）虽然在医学影像零样本识别上表现强劲，但其对领域变化的适应性和预测置信度的可靠性不足，尤其是在类间不平衡和样本极少的情况下。现有的分割合格预测（SCP）方法虽然保证有限样本覆盖率，但效率低且类别覆盖率失衡。因此需要方法在不增加计算负担和模型训练的情况下提高预测效率和可靠性。

Method: 提出LATA方法：利用图结构（image-image k-NN图）平滑模型概率输出，并通过CCCP均值场更新推理，提高SCP的效率和类别覆盖均衡性。LATA无需重新训练模型或使用标签（可以选择用校准边际分布一次性增强性能），是黑盒、低计算需求的推断时校准方法，并引入失败感知 conformal 分数融合于ViLU框架，实现对实例难度和标签可行性的细粒度调整。

Result: LATA在三种主流医学VLM和九个下游医学任务中，均显著减少了预测集的平均大小及类别条件覆盖差距，同时保持或提升整体覆盖率，超越了现有的转导基线，且仅需很少计算资源。此外，通过消融和定性实验验证了方法有效性。

Conclusion: LATA作为一种高效、通用且无需额外训练或标签的校准方法，能够提升医学视觉-语言模型迁移到新领域时的预测可靠性与实用价值，缩小了与有标签方法之间的性能差距，具有良好的实际应用前景。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [43] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker 通过构建事件级视频场景图和强化视觉定位，有效提升了视频推理中的因果理解能力，减少了推理幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态语言模型（MLLMs）在视频推理时对于事件间因果关系的理解不足，且往往依赖密集描述或摘要，容易导致推理幻觉，缺乏结构化的事件因果关联。

Method: 提出GraphThinker方法，采用强化学习微调模型，利用MLLM自动构建事件级视频场景图（EVSG），显式建模视频内外的事件关系，并将场景图作为模型的中间思考过程纳入推理流程。同时，引入视觉注意力奖励项，强化模型的视频真实对齐能力。

Result: 在RexTime和VidHalluc两个数据集上，GraphThinker在捕捉物体和事件关系以及事件归位上表现更佳，显著减少了视频推理中的幻觉现象，相比已有方法效果更优。

Conclusion: GraphThinker通过结构化场景图和强化视觉绑定，有效补足了MLLM在视频因果推理上的短板，对提升多模态推理的可靠性有重要意义。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [44] [RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward](https://arxiv.org/abs/2602.17558)
*Qiucheng Wu,Jing Shi,Simon Jenni,Kushal Kafle,Tianyu Wang,Shiyu Chang,Handong Zhao*

Main category: cs.CV

TL;DR: 该论文提出了RetouchIQ框架，利用强化学习驱动的多模态大模型作为智能代理，实现专业级的指令化图像编辑，并通过通用奖励模型提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型在视觉-语言推理与专业图像编辑间展现出很大潜力，但在创意编辑任务中缺乏可验证的奖励信号，导致强化学习训练过程仍然困难需要改进。

Method: 提出RetouchIQ框架，结合RL微调后的MLLM作为工具，通过“通用奖励模型”自动针对用户编辑意图评估结果并计算奖励信号，提升模型生成满足编辑指令的真实可执行参数；创新地超越了基于参考图与手工特征的奖励设计。构建了包含19万组指令推理对的数据集，建立新的基准。

Result: RetouchIQ在语义一致性和感知质量上，均优于以往基于MLLM及扩散模型的编辑系统。

Conclusion: 基于通用奖励模型驱动的多模态大模型作为智能图像编辑助手，具备灵活、可解释、可执行的优点，为专业图像编辑带来了新方向。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>


### [45] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 本文提出了直接由艺术作品生成音乐的方法和数据集，避免了传统的图像到文本再到音乐的间接路径，推动了视觉到音乐的跨模态生成研究。


<details>
  <summary>Details</summary>
Motivation: 目前的由图像生成音乐系统主要基于自然照片，难以捕捉艺术作品的丰富语义和风格。此外，多数方法需借助图像转文本的中间环节，限制了视觉信息的直接利用。为填补这些不足，作者探索直接用艺术作品生成音乐。

Method: 作者构建了ArtSound大规模艺术作品-音乐对数据集，并提出了ArtToMus框架，打通艺术作品到音乐的直接通路。该框架将图像嵌入映射到潜变量扩散模型的条件空间，无需经过文本描述直接生成音乐。

Result: 实验结果显示，ArtToMus生成的音乐在音乐性和风格一致性上表现良好，能够体现源艺术作品的视觉特征。虽然语义对齐分数低于文本条件系统（符合预期），但感知质量和跨模态对应性达到了较强的竞争力。

Conclusion: 本文奠定了直接视觉到音乐生成的基础，提出的数据集和方法为多媒体艺术、文化遗产保护以及AI辅助创作等应用带来支持，同时推动了相关领域的创新研究。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [46] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 该论文提出了一个统一的地理发现框架，结合主动学习、在线元学习与概念引导推理，以应对资源受限、数据稀疏和动态环境下的目标发现问题。


<details>
  <summary>Details</summary>
Motivation: 在环境监测、灾害响应和公共卫生等场景下，现实中的数据采集昂贵且局限，尤其是未观测区域的目标查找在资源有限的情况下极具挑战。此外，地理真值数据稀疏且有偏，使现有依赖大量数据的学习方法难以奏效。

Method: 作者提出结合主动学习、在线元学习和概念推理的新框架，创新点包括：（1）基于领域概念的相关性，设计了“概念加权的不确定性采样”策略，根据与目标相关的领域知识调整采样不确定性；（2）提出“相关性感知的元批策略”，增强在线元学习中的语义多样性以促进泛化。

Result: 在涉及致癌PFAS污染物的真实遥感数据集上进行测试，结果显示该方法在数据有限及环境不断变化时均能稳定有效地发现目标。

Conclusion: 所提出的方法能够在数据稀疏且动态的地理场景下，用极少的数据更高效、更可靠地发现隐藏目标，优于传统方法，具有实际应用价值。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [47] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散Transformer的新框架CORAL，通过显式对齐人物和服装的attention查询-键匹配，提升了虚拟试衣中的局部细节保留和全局形状迁移能力。新框架在各方面超过现有方法，尤其适用于无配对数据场景。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法难以在无配对数据下保留服装的细节，主要因为没有强制实现人-衣对应且无法解释Diffusion Transformer中对应性的产生。本文旨在解决这一分配与理解的难题。

Method: 分析了DiT中的3D全注意力机制，发现高质量的人-衣匹配依赖于精确的查询-键匹配。提出新框架CORAL，引入了外部对应引导的distillation损失和分布锐化的熵最小化损失，结合更贴合人类偏好的基于VLM的评价协议。

Result: CORAL框架相较于基线显著提升了全局形状迁移和局部细节保留的性能，通过消融实验证实各设计的有效性。

Conclusion: 显式对齐人-衣注意力查询-键，对提升虚拟试衣细节和形状表现至关重要。CORAL在各项指标上优于现有方案，为相关任务提供了新思路。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [48] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为IntRec的交互式目标检索框架，通过“意图状态”机制结合正负记忆集与对比对齐方法，能根据用户反馈逐步提升目标检索精度，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测器通常是一次性预测，无法根据用户的反馈逐步消歧，特别是在复杂和物体高度相似的场景中，容易出现模糊或选错目标的问题。因此需要一种可交互、能动态引入用户反馈以提升检索精度的方法。

Method: 作者提出IntRec框架，引入“意图状态（IS）”模块，用正向记忆集记录已确认目标，负向记忆集记录被排除的假设。通过对比对齐函数，根据用户反馈迭代优化候选目标的排序，使其更贴近正向线索、远离负向假设，实现精细化消歧。整个流程无需额外监督信号。

Result: 在LVIS数据集上，IntRec的AP为35.4，分别比OVMR、CoDet和CAKE高出2.3、3.7和0.5。在复杂的LVIS-Ambiguous基准上，用户仅反馈一次即可提升7.9 AP，每次交互仅增加不到30毫秒延迟，展现出高效精确的效果。

Conclusion: 通过引入交互式反馈与意图状态机制，IntRec在无需额外监督的前提下明显提升了复杂场景下的目标检索精度，证实了框架的实用价值和优越性。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [49] [Human-level 3D shape perception emerges from multi-view learning](https://arxiv.org/abs/2602.17650)
*Tyler Bonnen,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文提出了一种新的神经网络框架，该框架能够仅凭自然场景中的图像，实现与人类相当的三维形状推断能力。


<details>
  <summary>Details</summary>
Motivation: 长期以来，理解人类如何从二维视觉输入推断三维结构，并在计算模型中实现这一能力一直是视觉智能领域的重要目标，然而现有的方法在性能上仍落后于人类。

Method: 作者提出了一类利用视觉-空间目标对自然感知数据进行训练的神经网络模型。该模型依赖不同视角下的图像学习与空间相关的信息（如摄像机位置和视觉深度），且不依赖于对象相关的归纳偏置。同时，设计了zero-shot评测方法，将模型在标准三维感知任务中的表现与人类进行比较。

Result: 该模型无需针对任务训练或微调，首次在三维形状推断准确性上达到人类水平。此外，模型输出可独立预测人类行为中的细粒度指标（如错误模式和反应时间），显示出与人类感知的高度一致性。

Conclusion: 研究揭示，只需简单且可扩展的学习目标，基于自然视觉-空间数据的神经网络模型就能自发产生与人类相当的三维感知能力。

Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.

</details>


### [50] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: 论文提出了OpenEarthAgent，一个结合多模态推理与工具增强的地球观测智能体，能通过卫星影像和自然语言实现复杂的地理空间推理分析，并且在多个细分任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理虽有进展，但在遥感领域扩展困难，需要对复杂地理结构、多光谱指数进行多步逻辑推理，现有模型难以适应。亟需一种能在地理空间分析中有效进行多步推理并具备可解释性的智能体。

Method: 提出了一套统一的开发框架，通过集成卫星影像、自然语言及详细的推理记录，采用有监督微调，对多样化地理空间任务的多步推理轨迹进行训练。此外，构建了涵盖城市场景、环境、灾害与基础设施等领域的语料库，加入了多种遥感指数和GIS操作。

Result: 框架训练得出的OpenEarthAgent在结构化推理、空间理解及行为可解释性方面均有优异表现。在与强基线及当前开源、闭源模型的对比中，OpenEarthAgent持续取得性能提升并保持竞争力。

Conclusion: OpenEarthAgent有效填补了遥感多模态推理的空白，通过工具增强及推理轨迹监督，显著提升了地理空间分析的能力，为后续多模态地球观测智能体的发展奠定了基础。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出利用参考引导的大模型（LLM）评价器作为软“验证器”，用于缺乏真实验证的非可验证领域中实现有效的大模型对齐与自我提升。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习中的可验证奖励（RLVR）在推理任务中有效，但在如大模型对齐等缺乏真实验证者的非可验证领域无法直接应用，亟需找到新的对齐与评估方法。

Method: 设计了以参考输出为引导的LLM评估协议，通过实验评估了使用前沿模型与高质量（人工书写）参考输出提升弱/强LLM评判器的能力，并基于此实现参考引导的自我提升。

Result: 参考引导大幅提升了弱能力模型评判者的准确性，强LLM评判者在高质量参考引导下同样得到增强。在AlpacaEval和Arena-Hard两大任务上，提出方法分别获得显著优于传统SFT和无参考自我提升的方法的分数提升，接近强reward model的效果。

Conclusion: 参考引导的LLM-评价器为非可验证领域的大模型训练和后处理提供了切实可行的方法，有效提升了模型自我提升与对齐效果。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [52] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 该论文关注于提升低资源语言（如希腊语）下大型语言模型（LLM）在问答任务中的表现。主要贡献为构建了希腊语社交媒体问答数据集DemosQA，开发了高效评测框架，并对多种单语和多语LLM进行了全面评测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要针对高资源语言（如英语），在低资源语言的问答任务中存在文化、社会等表现不足。因此需要关注低资源语言（如希腊语）下LLM的有效性和公正性。

Method: （1）构建了希腊语社交媒体问答数据集DemosQA，突出希腊语的社会文化特色；（2）提出了内存高效的LLM评测框架，可适用多种QA数据集和语言；（3）对11个单语和多语LLM在6个人工整理的希腊语QA数据集上，采用3种提示策略进行评估。

Result: 系统比较了单语与多语LLM在希腊语问答任务上的性能，揭示了不同模型和提示策略的表现差异。

Conclusion: 单语LLM和多语LLM在低资源语言任务中表现各异，针对希腊语的专有数据集和评测工具对于促进模型公平性与能力提升非常重要。论文公开了全部代码与数据，有助于推动后续研究。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [53] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于流（flow-based）的语言模型，能够在生成速度和质量上超过现有的离散扩散（discrete diffusion）方法，实现了更快、更高质量的语言生成。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型被认为能够实现比自回归（autoregressive）模型更快的生成速度，但在实际使用中，在步数减少时会大幅降低生成质量，未完全兑现其潜力。作者试图通过新的建模方式解决这一问题。

Method: 作者基于对离散模态下流方法（flow）基础的重新理解，提出了一种基于欧几里得去噪（Euclidean denoising）的流式语言模型（FLM），并通过引入时间重参数化提升训练稳定性和生成质量。此外，通过模型蒸馏，得到可实现少步生成的FMLM。

Result: 在LM1B和OWT数据集上，所提FLM在生成质量上达到了当前离散扩散模型的最佳水平。FMLM在所有衡量指标上都优于最新的少步语言模型，单步生成的质量甚至超过了现有方法8步生成的质量。

Conclusion: 作者的工作质疑了“离散扩散是离散模态生成建模必需”的普遍假设，为大规模快速流式语言建模打开了新方向。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [54] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种面向治理的本地大语言模型，通过对保险理赔文本进行领域特定微调，实现了结构化纠正建议的自动生成，表现优于通用大模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽然强大，但在监管严格、数据敏感的保险等领域应用有限，需解决合规性、专业性和准确性需求。

Method: 利用历史质保理赔数据，对预训练大语言模型通过LoRA进行微调，限定模型用于理赔流程决策模块，并通过自动语义相似度与人工评估相结合的方法进行效果评估。

Result: 领域微调模型在约80%案例中生成的纠正建议与真实一致，效果显著优于商用通用型和仅依赖prompt的大模型。

Conclusion: 领域自适应微调显著提升模型在保险理赔场景中的可靠性和合规性，为大模型在受监管行业落地提供了理论和实证支持。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [55] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 该论文提出了BanglaSummEval，一个用于Bangla摘要事实一致性评价的无参考、问答式评测框架，方法高效、透明，并能提供细致诊断，且与人类专家评判高度相关。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医疗、新闻等，生成摘要的事实一致性评估至关重要。而Bangla作为一种广泛使用但资源稀缺的语言，目前缺乏有效的事实一致性评测方法，现有指标也普遍忽略了对Bangla的支持。

Method: 提出BanglaSummEval框架：不依赖参考摘要，自动从原文和摘要中生成问答，通过单一多语言指令微调模型完成问题生成、答案抽取及问句重要性加权，用BERTScore-Recall作语义一致性比对。该系统具备统一设计，降低了系统复杂度和计算成本。

Result: 在教育和医学领域共300篇人类撰写的摘要上实验，BanglaSummEval评测分数与专家人工评分有很强的相关性（Pearson r=0.694，Spearman ρ=0.763）。

Conclusion: BanglaSummEval能在低资源语言环境下，为Bangla摘要的事实一致性提供可靠、易解释且具备诊断功能的评测方案，具备实际应用价值。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [56] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文介绍了针对德国美因茨方言(Meenzerisch)的首个NLP研究，构建了词典数据集，并测试了主流大模型在该方言上的定义生成与词语生成能力，但表现极差，显示更多资源和研究有迫切需求。


<details>
  <summary>Details</summary>
Motivation: 美因茨方言作为狂欢节重要文化元素正在消亡，针对其缺乏NLP研究，亟需数字化资源和技术支持其保护和复兴。

Method: 1. 基于1966年Schramm资源构建标准德语与方言对照数字词典（2351条词条）；2. 使用LLM模型测试其对方言词义生成和反向生成能力；3. 尝试few-shot学习与规则引导提升效果。

Result: 主流LLM模型对定义生成准确率仅6.27%，对方言词生成准确率1.51%；即使通过few-shot学习和规则引导，准确率仍不足10%。

Conclusion: 现有大模型对资源极度稀缺的方言支持极差，说明需要建设更多数据资源，并加强相关领域的研究。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [57] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 本文提出了ConvApparel数据集及一套新的用户模拟器验证框架，旨在缩小大模型驱动的用户模拟器与真实用户互动之间的'现实差距'，提升对话式AI的现实适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的用户模拟器存在'现实差距'，导致优化出来的对话式AI系统难以在真实环境下表现良好，因此需要新的数据集和评测方案来缩小这一差距。

Method: 作者构建了ConvApparel数据集，采用“双代理”数据采集协议，分别模拟优秀与差劲的推荐者，收集丰富用户体验及第一人称满意度标注。并提出结合统计一致性指标、人类相似度得分与反事实验证的综合评测框架，测试用户模拟器的泛化能力。

Result: 实验显示，所有用户模拟器均存在现实差距，但数据驱动的用户模拟器在反事实验证下，尤其面对未见过的行为时，比提示工程基线适应性更强，反映出更为稳健但仍不完美的用户建模能力。

Conclusion: 提出的数据集和综合验证框架，有助于评估和缩小用户模拟器的现实差距；数据驱动模拟器比基线方案具有更好的泛化能力，但仍需进一步改进以实现更高真实度。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [58] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本文探究了跨语言中委婉语检测的迁移学习，发现即使语义重叠，正向迁移也不一定成立，尤其在低资源的土耳其语到英语任务中效果较差。


<details>
  <summary>Details</summary>
Motivation: 委婉语常常受文化和语境影响，导致跨语言建模难度增大。为提升多语言委婉语检测的表现，有必要研究不同语言之间的等价性影响。

Method: 作者将英语和土耳其语中的潜在委婉语术语（PETs）划分为重叠（OPETs）与非重叠（NOPETs）两类，根据其功能、语用和语义对齐情况进行分析，并进行迁移实验，探讨二者对于跨语言检测性能的影响。

Result: 发现语义重叠不能确保正向迁移效果，特别是在低资源土耳其语到英语方向，即使是重叠术语，其检测性能也可能下降。而采用非重叠术语训练，有时反而能提升效果。标签分布差异部分解释了这一反直觉结果。

Conclusion: 跨语言委婉语检测的迁移性能不仅依赖于语义重叠，还受制于标签分布等因素。领域对齐在类别层面有一定影响，但证据有限需进一步研究。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [59] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 本文提出了一个不确定性感知的计算框架，用于对波斯古典诗歌进行大规模诗人心理分析，引入自动化多标签注释、信心水平、弃权机制，并用多种方法量化诗人个性与情感结构。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌由于其丰富的隐喻、互文和修辞特性，难以大规模进行可重复的计算性心理分析。需要一种能兼顾细致解读与大规模定量对比的方法。

Method: 作者用自动化多标签注释为每句诗关联心理学概念，获取每个标签的置信分与弃权标记。然后将证据聚合到“诗人×概念”矩阵中，通过Jensen–Shannon散度和Kullback–Leibler散度衡量诗人个性。进一步基于信心加权的共现图定义“Eigenmood”情感嵌入，并用光谱分解来挖掘概念结构，进行敏感性分析和远-近读结合。

Result: 在10位诗人、61573句诗歌上测试，22.2%的诗句因分析不确定而弃权。框架可以量化诗人心理特征、呈现基于概念的情感结构，并实现诗句实例沿情感轴的筛选，表现了其敏感性与诊断能力。

Conclusion: 本工作实现了在保留诗歌解读审慎性的同时，实现了可扩展的数字人文学分析。通过不确定性的传播，使诗人层面心理分析更具可靠性，也增强了分析的透明性与可追溯性。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [60] [Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History](https://arxiv.org/abs/2602.17003)
*Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了Persona2Web，这是评估个性化网页智能体在真实开放网络上表现的首个基准，通过分析用户历史数据来实现个性化查询理解。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型驱动的网页智能体缺乏个性化功能，而用户又很少明确描述所有意图，所以需要智能体能根据用户历史主动推断需求。

Method: 构建了Persona2Web基准，包括（1）长期用户历史以隐式展示偏好；（2）需要智能体推断用户偏好的模糊查询；（3）支持精细化评测个性化能力的推理感知评价体系。

Result: 作者对不同智能体架构、主干模型、历史访问方式及不同模糊度查询进行了大量实验，揭示了个性化网页智能体面临的关键挑战。

Conclusion: Persona2Web为开发和评估真正具有推理和个性化能力的网页智能体提供了数据和工具，促进了该领域的发展。

Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.

</details>


### [61] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出了一种新方法Reasoning Inception (ReIn)，能在不修改大语言模型参数和提示词的情况下，有效帮助对话式智能体识别和恢复用户引起的意外对话错误，显著提升任务成功率和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有集成工具的大语言模型型对话体在固定任务数据集上表现优异，但对未预见的用户错误易出错。由于模型参数和prompt的修改成本高不可行，亟需寻找无需调整模型本身即可实现的高效错误恢复机制。

Method: 提出ReIn，一种推理植入（test-time intervention）方法：通过外部模块检测对话中的典型错误并生成恢复计划，把该推理过程动态嵌入智能体决策流程中，引导后续纠正行为，无需修改参数或系统prompt。用系统设计的对话失败场景（如用户请求含糊、缺乏支持）验证，各种agent模型及inception模块组合均被测试。

Result: ReIn在多agent和module组合下均大幅提升了任务完成率，对新型错误有较强泛化能力；其效果优于直接修改prompt的方案。分析显示，结合恢复工具与ReIn可安全高效提升对话体鲁棒性。

Conclusion: ReIn为现有大模型对话体提供了一种无需后端参数/prompt修改的即时错误恢复新范式，可安全快速提升复杂任务的对话成功率与系统韧性。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [62] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文提出了一种新的理论心智（ToM）测试任务，通过说服目标选择特定政策来评估人类与大语言模型（LLMs）的ToM能力，发现LLMs在公开信息下表现良好，但在需推断心智状态时表现不佳，而人类两种情况下都能较好完成任务。进一步实验表明LLMs即使缺乏显式ToM推理，也能有效说服人类。


<details>
  <summary>Details</summary>
Motivation: 现有评估人类及LLMs理论心智能力的方法多为静态问答任务，忽视了真实互动中的心智推理，尤其是通过互动和说服任务对ToM的考察，因此需要设计更贴合实际互动的ToM评估方法。

Method: 设计了一个新型ToM任务：说服者需向目标（有不同知识与动机状态）揭露信息，引导其选择特定政策。实验分为“信息揭示”和“信息隐藏”两种状态，前者直接提供目标心智状态，后者需说服者推断或提问。先后让人类与LLMs扮演说服者或者目标进行实验。

Result: 在目标心智状态已知时，LLMs表现优异；在需推断时，LLMs表现低于随机猜测，而人类两种条件下均表现适中。在后续面对真实人类目标的实验中，LLMs说服力超过人类说服者。

Conclusion: LLMs缺乏人类式的心智理论推理，但可通过修辞等非推理手段有效影响人类信念和行为。因此，不应轻易赋予LLMs人类的ToM能力，但需注意其现实影响力。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [63] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本文提出并实证分析了四种跨语言文本分类方法，用于从英语、日语、印地语、韩语等多语言社交媒体帖子中筛选出与氢能相关的讨论，并进行主题提取。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的全球性多语言舆论分析难度大，尤其是利用关键词收集的数据含有大量无关内容，因此需研究如何高效过滤和分析相关内容。

Method: 以2013-2022年间900多万条英语、日语、印地语与韩语推文为基础，分别使用（1）英文标注数据翻译后构建单语分类器，（2）多语言未标注数据翻译成英文后用英文模型分类，（3）多语言预训练transformer直接微调用于多语言分类，（4）混合上述两种翻译策略和多语言训练。比较四种方法对相关推文的筛选能力，并进行主题建模。

Result: 比较发现，不同方法在翻译成本、模型泛化能力、噪声处理等方面存在取舍，部分方法在准确性和效率之间有明显差异。主题分析有效发现了不同语言社交媒体中氢能讨论的核心主题。

Conclusion: 四种跨语言文本分类方法各有优劣，对优化大规模多语言社交媒体分析流程提供了实用参考；将多语言处理策略与翻译流程合理结合能够提升相关性过滤和主题识别效率。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [64] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: 本文介绍了ALPS（Arabic Linguistic & Pragmatic Suite），这是一个由专家精心打造的阿拉伯语深层语义与语用能力诊断测试集，旨在弥补现有大规模基准测试数据合成或翻译带来的不足。文中评估了多种模型，并揭示出AI模型在流畅性和深层语言理解间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语NLP基准测试主要关注规模和多任务能力，但往往依赖合成或翻译数据，存在语言深度理解和文化真实性不足的问题，需要有原生、经专家验证、专注语言深度的数据集。

Method: 研发了ALPS数据集，涵盖531个问题，横跨15个任务和47个子任务，为每个任务手工设计问题，充分利用阿拉伯语言学和文化知识，避免翻译伪影。随后，使用23个多样化模型（包括商业、开源和阿拉伯语原生模型）进行评估，并与人工基线和专家裁决的结果进行了比较分析。

Result: 发现主流模型在语言流畅性上表现良好，但在形态句法依赖（尤其是依赖音标的任务）上错误率较高（达36.5%），相较于组合语义任务更为薄弱。顶尖商业模型（如Gemini-3-flash）准确率为94.2%，超过一般人的平均水平（84.6%），但与专家裁决（99.2%）仍有差距。阿拉伯语原生最强模型Jais-2-70B得分为83.6%。

Conclusion: 当前AI模型在阿拉伯语深层语言理解上仍有明显短板，尤其是在细致的形态句法关系上。尽管大型商业模型取得进步，阿拉伯语专用模型与商业模型及人类表现仍有较大差距，ALPS为将来阿拉伯语NLP深层能力研究提供了权威工具和基准。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [65] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 本文提出了BankMathBench这一特定于银行领域的基准数据集，用于提升和评估大语言模型（LLMs）在真实银行场景下的数值推理能力。实验表明，经过该数据集训练的模型在银行相关计算任务上准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在处理银行常见的存款、贷款等产品咨询时，在核心数值计算和多步推理上表现不佳，容易出现公式套用错误或基本计算错误。现有数学和金融领域的数据集不能真实反映日常银行业务的问题场景，因此亟需一个针对银行业务数值推理的数据集。

Method: 提出了BankMathBench数据集，涵盖从基础到高级的银行业务推理场景，包括单产品推理、多产品比较及多条件计算。利用BankMathBench对开源LLMs进行训练，并通过工具增强微调方法提升模型能力。

Result: 经过BankMathBench训练和工具增强微调后，模型在单产品推理、中级及高级多条件推理任务上的准确率分别提升了57.6、75.1、62.9个百分点，明显优于现有零样本模型基线。

Conclusion: BankMathBench有效提升了LLMs在真实银行场景下的数值推理和公式生成准确率，是评估和推动LLMs金融推理能力的可靠基准。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [66] [Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests](https://arxiv.org/abs/2602.17108)
*Anton Dzega,Aviad Elyashar,Ortal Slobodin,Odeya Cohen,Rami Puzis*

Main category: cs.CL

TL;DR: 本文探讨了如何用SCORS-G量表，通过非语言模态，评估大型多模态模型（LMMs）的“人格特质”，并验证其在TAT心理测试下的表现与评估一致性。结果显示，大模型对人际关系和自我概念理解好，但在攻击性识别和调节上有明显不足。


<details>
  <summary>Details</summary>
Motivation: 传统TAT测试用于揭示人的无意识个性成分，但鲜有针对AI或LMMs在相似测试下的人格进行分析。作者希望了解LMMs是否能在类似给人做的投射性人格测试中表现出‘人格’，以及这些表现是否可以被系统量化和分析。

Method: LMMs在两种角色下工作：一是作为TAT主试，对图片生成故事，二是作为评估者，使用SCORS-G对这些故事进行评分。机器评估结果与人类专家进行对比，分析模型理解人际关系和人格各项维度的能力。

Result: LMM在理解和分析TAT故事上与人类评估高度一致，尤其在人际动态和自我概念维度表现良好，但在对攻击性内容的识别和调节上表现薄弱。先进或规模更大的模型性能优于旧或小模型。

Conclusion: 大型多模态模型可在一定程度上通过投射性测试展现并被评估‘人格’特质，其人格评估具备一致性，人际理解突出，但攻击性处理能力有限。模型大小与先进性会显著提升这类人格测试表现。

Abstract: Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.

</details>


### [67] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文提出一种新型评审方法，针对大型语言模型(LLMs)在多智能体系统及递归判定场景中的稳定行为特征进行量化与分析，从而保障AI治理与安全。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评测只关注任务准确率，难以识别模型训练与对齐期间沉淀下来的稳定行为偏向。本研究致力于捕捉这些深层、持久的“主导思维”，以应对AI多层嵌套部署造成的安全与治理隐患。

Method: 提出基于心理测量学理论的框架，采用强制选择、序数题干加语义无关扰动选项，并结合加密置换不变性，来评审主要LLM厂商模型的偏向（如优先优化、奉承性、现状正当化）。数据分析用混合线性模型(MixedLM)和组内相关系数(ICC)评定行为聚类及框架效力。

Result: 结果发现，尽管问题表述有很大方差影响，但“实验室信号”(即供应商特定行为模式)导致模型行为强烈聚类，显示这些偏向在封闭生态系统中具持续性。

Conclusion: 模型底层偏向并非偶发错误，而是能在多层AI架构中累积放大，存在构建意识形态‘回音室’的风险，需高度关注，为AI治理提供技术建议。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [68] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 本文分析了罗马尼亚文本型远程医疗中患者满意度的信号，通过机器学习方法发现交流方式特征与患者满意反馈的关联。


<details>
  <summary>Details</summary>
Motivation: 随着文本型远程医疗普及，医生需通过书面文字向患者有效传递建议。当前平台愈加依赖患者评分，医生面临维持满意度评分的压力，而这些评分常反映沟通质量多于医疗准确性。

Method: 采集77,334组匿名患者提问-医生回复对，基于患者反馈（点赞为正向，其余为负向）构建二分类模型。提取可解释、主要与语言无关的特征（如长度、结构、可读性），以及罗马尼亚LIWC心理语言特征、礼貌/缓和标记，并用基于时间分割的训练方式及SHAP分析特征贡献。

Result: 结果显示，患者和医生的历史特征对预测满意度起主导作用，但回复文本特征也提供了一定的可操作信号。分组相关性分析中，礼貌和缓和表达稳定正相关于患者满意度，词汇多样性则呈负相关。

Conclusion: 医生在书面回复中的表达风格，尤其是礼貌和缓和用语，有助于提升患者满意度评分，对优化远程医疗沟通和满意度具有指导意义。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [69] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本文提出了一种心理测量框架，量化并缓解LLM在自报告问卷评估中出现的社会期望性回答偏差（SDR），并用改进的问卷设计减小这种偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在NLP领域的广泛应用，研究人员越来越多地用自报告问卷对其一致性、安全性和偏见等指标进行评测。但这些问卷假设被评估者诚实作答，LLM却可能倾向于给出社会期望的答案，导致评测结果偏差。亟需建立有效框架识别和减弱这种偏差。

Method: 作者提出在两种指示下（“诚实”与“作假优答”）对LLM进行同一问卷测评，采用项目反应理论（IRT）计算标准化的SDR效应量，从而量化偏差。同时通过受控优化，从题库中选出匹配“期望性”的题对，设计分级强制选择（GFC）式Big Five问卷，减少LLM的SDR倾向。

Result: 在9个指令微调LLM和合成已知画像的数据集上，常用Likert量表均出现显著SDR偏差，而新设计的GFC问卷大幅降低了SDR，同时在一定程度上保留了对目标画像的恢复能力。

Conclusion: 问卷评估中的社会期望性回答对LLM测评存在显著影响，不同模型间存在SDR与画像恢复能力的权衡。建议在利用问卷对LLM进行基准和审计时，采取SDR敏感的设计和报告方法。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [70] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了X-Value跨语言价值观评测基准，用于评估大语言模型在内容价值观识别上的能力，涵盖18种语言、7大领域，实验发现现有主流大模型在此任务上表现不足、语言间差异明显。


<details>
  <summary>Details</summary>
Motivation: 当前大模型内容安全评测只关注明显危害内容，忽视了更深层、隐性的价值观维度；缺少能跨语言、反映全球多元价值观的评测数据。

Method: 作者构建了X-Value数据集，包含5000+QA对、18种语言，涉及7大基于Schwartz人类基本价值观理论的领域，分简单与困难级别。设计两阶段标注流程：第一步判断议题是全球共识还是多元，第二步多方评判内容隐含的价值观。

Result: 在X-Value上的系统实验显示，当前最优大模型在跨语言价值观评测上准确率低于77%，不同语言的表现差异大于20%。

Conclusion: 主流大模型在细腻、价值观敏感的内容评估方面能力有待提升。X-Value为全球多语多元价值观内容评测提供了急需的测试基准。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [71] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 该论文分析了Transformer在神经机器翻译中的表征坍缩问题，并提出通过角度分散正则化来缓解，提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然表现优异，但深层表征容易出现表征坍缩，使得表示能力受限；而此问题在连续输出的端到端翻译更为突出，有必要系统分析及缓解。

Method: 研究了离散与连续NMT Transformer在训练过程中各层的表征坍缩动态，融入基于角度分散的已有正则化方法，并评估其对模型多样性和性能的影响。另外也考察了量化模型的表征坍缩及正则化作用。

Result: 引入角度分散正则化后，不仅有效缓解了表征坍缩，还能提升翻译表现。实验证明正则化的益处在模型量化后依然显著。

Conclusion: 表征坍缩对Transformer机器翻译模型影响大，基于角度分散的正则化方法能有效缓解该问题，并提升翻译质量。该方法对量化模型同样有效。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [72] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在面对词汇和句法上的形式扰动时的表现，发现多数模型对词汇层面的变化极为敏感，评价结果及排名容易受影响，凸显了现有评测基准的局限。


<details>
  <summary>Details</summary>
Motivation: 虽然各类大语言模型广泛使用标准化基准进行评估，但近期发现其表现对输入提示的浅层变化异常敏感，导致模型间的比较可靠性受到质疑，所以需要系统性地研究这一现象。

Method: 研究者设计了两种基于语言学原理的扰动生成方式：一是同义词替换进行词汇扰动，二是利用依存句法分析实施句法结构扰动。分别在MMLU、SQuAD和AMEGA这三个评测集对23个主流LLM进行了测试。

Result: 词汇扰动会在几乎所有模型和任务上显著降低性能，变化具有统计学显著性；句法扰动的影响差异较大，有时甚至提升模型结果。两种扰动都会影响高难度任务上的模型排名。此外，模型规模与鲁棒性并不总是正相关，其鲁棒性高度依赖具体任务。

Conclusion: 现有LLM更依赖表层词汇模式，而非深入的抽象语言能力。建议未来LLM评测需将鲁棒性测试纳入标准流程，以提升评估的可信度。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [73] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 本文提出RPDR框架，通过挑选高质量、易学习的训练数据以增强稠密检索器，对长尾问答任务取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以掌握和召回罕见知识，稠密检索器也难以泛化到长尾知识，需要更有效的数据增强与检索方法。

Method: 提出RPDR数据增强框架，包括合成数据生成、借助Round-Trip预测选择易学习实例，并用这些实例训练检索器。同时提出动态路由机制，将查询分配给专门的检索模块。

Result: 在PopQA和EntityQuestion两个长尾检索基准上，RPDR在极端长尾类别上的表现优于BM25、Contriver等现有方法。

Conclusion: RPDR能够显著提升长尾检索性能，尤其是极端长尾类别，并指出了方法的优势和不足，为未来提升检索性能提出方向。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [74] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 本文探讨了在选择题答题中，“认知可得性”是否是有效的猜题策略，以及其对现有和未来学生行为建模的影响。


<details>
  <summary>Details</summary>
Motivation: 许多学生在不确定选择题答案时，会凭直觉猜测。认知心理学中的“可得性启发式”理论认为，大脑更倾向于选择最容易想到的选项。作者希望验证这种策略在实际多项选择题中的有效性。

Method: 作者提出了一种基于大型语料库（如维基百科）统计数据的计算方法，量化MCQ（多项选择题）中各选项的‘认知可得性’。通过三个大规模题库，比较正确和错误选项在语料库中的出现频率，并评估始终选择最高可得性选项的答题表现。

Result: 实验显示，无论题干如何，正确答案在语料库中的‘可得性’显著高于错误选项。使用“总是选择最可得性的答案”这一策略，答题得分比随机猜测高出13.5%至32.9%。此外，发现大型语言模型生成的题目，其选项的可得性分布与专家手工设计的题库相似。

Conclusion: ‘认知可得性’在MCQ中确实存在指导作用，应在未来关于学生答题行为的建模和研究中予以考虑。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [75] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出一种新的跨文档共指消解（CDCR）注释方案，增强模型在多样化新闻报道中识别不同表达的能力。


<details>
  <summary>Details</summary>
Motivation: 目前CDCR数据集大多聚焦于事件消解，且对共指定义狭窄，难以适应措辞多变、立场极化的新闻报道分析需求。

Method: 将共指链视为语篇要素（DEs），覆盖身份一致与近似一致表达，重注释NewsWCL50和ECB+子集，并用统一手册和词汇多样性指标进行评估。

Result: 重注释后数据集在词汇多样性和细粒度注释上取得平衡，性能介于原始ECB+和NewsWCL50之间，增强了对多样媒体 discourse 的适应性。

Conclusion: 新注释体系提升了CDCR在新闻领域的实用性，有助于更全面、客观地分析新闻报道中对事件与实体的表达与框架。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [76] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文比较了BLEU和ChrF++两种机器翻译评估指标在极低资源语言（ELRL）环境下的表现，发现BLEU依然能提供补充性的解释信息。


<details>
  <summary>Details</summary>
Motivation: 极低资源语言的机器翻译评估难以直接使用主流的BLEU指标得到准确反映，而ChrF++虽被普遍采用，但具体的优缺点未被充分研究。作者希望探索和对比这两种主流指标在ELRL下的适用性和局限性。

Method: 作者以Magahi、Bhojpuri和Chhattisgarhi三种极低资源语言为例，分析BLEU和ChrF++对翻译伪影（如幻觉、重复、照搬源文、变音符号等）的响应，评估LLMs和NMT系统输出。

Result: 结果表明，虽然BLEU分数整体偏低，但能够从词汇精度角度补充ChrF++的解释，使评估更具可解释性。

Conclusion: 在极低资源环境中，BLEU和ChrF++各有优势，结合两者可提高对翻译质量的理解和判读能力，BLEU作为补充性指标仍具有价值。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [77] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文提出了一个用于长文本大模型（LLMs）输出细粒度不确定性量化的新分类体系，并系统比较了不同方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLM幻觉检测（closed-book hallucination）的不确定性量化方法主要面向短文本输出，且在长文本生成上的泛化性能较差。

Method: 作者提出对长文本输出不确定性量化的方法进行三阶段（输出拆分、单元打分、整体聚合）分类，并形式化多类基于一致性、黑盒方式的量化评分器。同时，作者提出多种打分方法并进行综合实验。

Result: 1）声明-响应蕴含分析方法效果优越或不逊于更复杂的声明级打分器；2）声明级打分普遍优于句子级打分；3）不确定性感知的解码策略显著提升长文本输出的事实性。

Conclusion: 该框架厘清了现有方法的关系，为细粒度不确定性量化组件的选择提供了有效的对比和实际建议。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [78] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 本文提出了AIDG游戏理论框架，系统评测大语言模型在信息推理和防御上的能力，发现模型在信息防守上远强于主动推理。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准无法充分评估大语言模型的战略推理能力，需要通过动态多轮交互来挖掘模型在对话中的真实推理表现。

Method: 设计了AIDG框架，包含AIDG-I（评估社会推理策略）和AIDG-II（结构化'20问'任务，评估约束满足），在6个前沿LLM上展开439场对局，系统对比模型推理与防守能力。

Result: 模型在防守（信息保持）上的ELO评分比推理高出350分，确认式策略比盲目推理有效7.75倍，近一半推理失败源自对对话约束的遵循下降。

Conclusion: 当前LLM擅长局部防守和对话一致性，但在全局状态跟踪和战略主动推理上存在显著短板。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [79] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 本文发现，大型语言模型(LLM)在多项选择题评测中存在明显的标签和位置偏见，对LLM的能力评估带来干扰。作者提出了一种减少此类评测偏差的新协议，使评测结果更加稳健准确。


<details>
  <summary>Details</summary>
Motivation: 多项选择题评测常被用来衡量LLM推理与知识能力。但这种评测方式中，答案位置、选项标签及其排列可显著影响模型表现，掩盖真实能力。因此，亟需开发更公平、减少偏差的评估方法。

Method: 作者构建了合成的NonsenseQA基准，揭示了不同LLM在标签位置等方面存在的显著偏差。随后提出一种新评测协议，将所有选项标签用非序列、无序标签替代，并要求LLM选出完整答案。同时借助句子相似度模型进行判别，从而减少标签引入的人工偏见。

Result: 新协议下LLM对答案排列的鲁棒性显著增强，平均准确率方差减少了3倍，而模型整体性能几乎不受影响。消融实验进一步表明，在多种嵌入与相似度实现下，该方法都更为鲁棒。

Conclusion: 本文提出的无序标签评测协议能够更公正地评价LLM，极大减少让模型依赖无关偏置信息。该方法在不牺牲性能的前提下，提高了多项选择评测的鲁棒性和有效性。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [80] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 论文提出了一种基于熵的无监督数据选择(EUDS)框架，显著降低了Fine-tuning大语言模型时的算力和数据需求。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在微调时对算力和数据需求极高，实际应用中难以满足。数据选择方法虽能减少数据量，但常因高算力要求难以落地，亟需兼顾高效与效果的数据过滤机制。

Method: 提出EUDS框架，利用熵进行无监督的数据选择，在无需监督标签的情况下根据数据的不确定性过滤，形成计算高效的数据筛选机制。

Result: 在情感分析、主题分类和问答任务上进行实证，EUDS有效减少了所需数据量和训练计算成本，并提升了训练效率。理论与实验结果均支持方法有效性。

Conclusion: EUDS为在算力受限条件下高效微调大语言模型提供了创新性解决方案；该方法在提升训练效率与节约资源方面具有实际应用前景。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [81] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0是一种新工具，不仅能分析和解释仇恨言论，还能自动生成有证据支持的反击言论。


<details>
  <summary>Details</summary>
Motivation: 线上仇恨言论激增，虽然已能较好自动检测，但如何生成有效反击言论仍是难点。

Method: 提出PEACE 2.0工具，采用检索增强生成（RAG）方法：一方面为仇恨言论分析提供事实依据；另一方面自动生成以证据为基础的反击言论，并分析反击内容特征。

Result: PEACE 2.0实现了对显性和隐性仇恨言论的深入分析与自动反击生成。

Conclusion: 集成多项新功能后，PEACE 2.0可为仇恨言论的应对和理解提供支持，推动线上和谐环境建设。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [82] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本论文探讨了在跨语言情感分析中，AI与人类交流意图双向对齐面临的严重问题，重点分析了孟加拉语与英语间的语义错位现象及其造成的安全性和信任危机。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统日益融入多语言环境，但现有对齐范式在应对非主流语言（如孟加拉语）与主流语言（如英语）之间的情感表达一致性方面表现不佳，不仅影响系统安全，还最终削弱了人类对AI的信任。

Method: 作者对四类Transformer模型在跨语言情感对齐任务上的表现进行基准测试，深入分析了mDistilBERT等压缩模型情感极性逆转率、以及IndicBERT在消解孟加拉语方言差异（如Sadhu孟加拉语）时的表现。同时，考察了跨语言间“非对称共情”等人机信任相关的细微现象。

Result: 压缩模型mDistilBERT在情感极性判决中出现了高达28.7%的逆转率，即正向情感被错误解读为负向，反之亦然；IndicBERT在处理正式孟加拉语（Sadhu）时，对齐错误率提升了57%；不同模型在表达孟加拉语情感权重时表现出系统性偏差。

Conclusion: 当前“压缩—通用”对齐策略牺牲了低资源语言的情感保真度，危及人机互信。作者呼吁在对齐评测中加入“情感稳定性”指标，尤其在保护语言和方言多样性的同时，确保AI情感理解的准确与稳定。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [83] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究评估了“小型”大语言模型（约10亿参数）在医疗NLP任务中的表现，发现它们经过微调后能达到甚至超过大型模型的水平，并公开了多项意大利医疗数据集与模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗NLP任务表现突出，但高算力需求限制了其现实部署。本文关注体量更小、部署更灵活的LLM，探索其性能瓶颈与优化空间。

Method: 选取Llama-3、Gemma-3、Qwen3等三个大语言模型家族的“小型”模型，针对20项意大利医疗NLP任务，系统测试不同的适配和训练策略，包括推理时的few-shot prompt、约束解码，训练时的有监督微调和持续预训练，并与更大参数量基线比较。

Result: 小型LLM经过微调后可达到或超过大模型基线，Qwen3-1.7B平均分比Qwen3-32B高9.2。资源受限时，few-shot与约束解码结合也表现优良。此外，公开了全意大利语医疗NLP数据集和千万级词医护语料。

Conclusion: 经过合适调优后，小型LLM可在医疗NLP任务中媲美甚至优于更大模型，为其在医疗场景实际部署提供了新的可行路径，并丰富了医护NLP资源。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [84] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文主要介绍了在临床自由文本分节领域，通过新数据集构建、模型评估与对比，探讨了有监督和零样本大模型的分节表现，发现零样本方法在跨领域适应性上有优势。


<details>
  <summary>Details</summary>
Motivation: 临床文本包含大量重要患者信息，通常以分节形式出现。现有分节模型主要依赖于特定领域（如MIMIC-III），在其他领域如产科笔记的适用性有限。因此，推动跨领域分节、探索大模型无监督能力具有重要意义。

Method: 1）构建了去身份化并带分节标签的产科临床文本数据集；2）基于transformer的有监督模型在MIMIC-III内外（产科数据集）进行分节评估；3）首次对比有监督模型与零样本大语言模型在医学文本分节的表现。

Result: 有监督方法在来源领域（in-domain）表现优异，但在新领域（out-of-domain）表现明显下降；零样本大语言模型在修正虚构分节标题后，展现出良好的跨领域适应力。

Conclusion: 开发面向特定医疗领域的数据资源很重要，同时只要适当处理‘幻觉’问题，零样本大语言模型在医疗NLP文本分节方面极具前景，尤其适用于跨领域任务。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [85] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型自动标注编程任务中知识点(KC)正确性的框架，从而更精准地建模学生的知识掌握过程。


<details>
  <summary>Details</summary>
Motivation: 现实中很少有细粒度的知识点正确性标注，尤其在包含多个知识点的编程任务中仅用整体正确性会掩盖学生的部分掌握情况，导致学习曲线拟合不准确。

Method: 框架利用大语言模型自动分析学生代码，逐个判断每个知识点是否被正确应用，并引入时序上下文的Code-KC映射机制，提升知识点与代码的匹配度。通过学习曲线(如熟练度幂律、加性因子模型)拟合和预测性能评估自动化标注结果。

Result: 实验结果表明，该框架产生的学习曲线更符合认知理论，预测性能相比已有方法更佳。人类专家评估也显示大模型标注与专家高度一致。

Conclusion: 利用大语言模型进行细粒度知识点标注能够显著提升学生建模的科学性与自动化，为开放式编程任务的学习分析提供了更强的工具。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [86] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本文提出了一种自适应正则化的训练框架，用于在微调过程中持续保障大语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随类大语言模型虽然初始训练时注重安全，但在后续微调甚至对抗攻击下，安全性易下降，而已有防御手段在确保安全性和实用性间存在权衡，无法兼顾。

Method: 作者提出根据安全风险动态调整正则化强度。利用两种风险估算方式：（1）通过安全裁判分配潜在伤害分数；（2）使用模型中间激活训练轻量化分类器预测有害意图。这些风险信号用于约束高风险更新，不允许模型远离安全参考策略，低风险更新则正常训练。

Result: 实验证明：（1）有害意图可通过生成前的模型激活准确预测；（2）安全裁判得分能高召回地引导安全训练；（3）在多种模型和攻击场景下，所提自适应正则化显著降低攻击成功率，且不损失下游性能，无额外推断开销。

Conclusion: 作者验证了基于风险估算的自适应正则化机制既可维持模型安全性，又不牺牲实用性，是一种兼顾两者的有效解决方案。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [87] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 本文针对当前自主网页代理系统与用户协作过程中的人类介入行为建模，提出了任务和数据集，显著提升了代理系统的适应性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有自主网页代理系统缺乏对人类何时以及为何介入的系统性理解，容易做出过度自主或不必要请求确认等问题，因此需要建模人类介入以增强人机协作效率。

Method: 作者提出了建模人类介入的新任务，并收集了包含4200多个人类与代理交互动作的数据集CowCorpus，总计400条真实用户导航轨迹。通过分析，总结了四种交互模式，并利用这些 insights 训练语言模型预测用户何时会介入。最后将模型实际部署在网页导航代理中并进行用户研究。

Result: 基于用户交互风格预测介入点的新模型相比原始LMs在介入预测准确率上提升了61.4%-63.4%；在线部署后用户评分的代理有用性提升了26.5%。

Conclusion: 结构化地建模人类介入行为能让代理系统更适应用户需求，促进高效协作，改善用户体验。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [88] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 本文通过针对四种语音大模型（Speech LLM）和六种任务的统一回测，发现当前主流的语音大模型在多数场景下等价于传统的Whisper转录+LLM串联结构，而且在噪声环境下表现更差。


<details>
  <summary>Details</summary>
Motivation: 尽管语音大模型备受关注，但它们是否真的超越了传统的“转录+文本LLM”流程，还是在重复后者的功能，目前还不明确。因此作者希望以更科学的对比实验，探究两类方法的本质差异和表现。

Method: 作者通过控制实验（Matched-Backbone Testing），选用四个主流语音大模型，在六项任务上，与对应的Whisper→LLM级联结构进行对比。采用指标包括统计一致性度量（κ分数）、logit lens方法观察中间状态的文字显性、LEACE概念消融分析文字表征的因果作用等。

Result: Ultravox等语音大模型与串联结构在行为和机制上高度一致（κ=0.93）；分析表明其内部表现出明显的文字中间表征，并且文字表征对结果几乎是因果必要的。此外在噪声环境下，语音大模型比对应串联更差。例外是Qwen2-Audio，其机制不同，表明表现一致性与架构相关。

Conclusion: 当今大多数语音大模型在实际任务中只是成本更高的级联方案，且在嘈杂语音条件下更容易性能倒退。只有部分独特架构的模型才真正和级联不同。因此进一步研发要关注架构突破，不应迷信全部Speech LLM都优于传统方案。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [89] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 该论文提出了DivanBench基准，用于诊断波斯语大模型对迷信与习俗等社会规范的推理能力。结果显示现有模型存在文化推理与事实记忆的明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准虽然扩展到语用学和礼貌层面，但很少区分模型是否仅记忆文化事实还是能推理隐含社会规范，尤其是涉及难以用简单逻辑推断的习俗和迷信。

Method: 作者构建了名为DivanBench的诊断型基准，涵盖315个问题，分为三类任务：事实检索、情景配对验证和情境推理。同时评估了7个波斯语大模型在这些任务上的表现。

Result: 评估发现：（1）大多数模型倾向于顺从性偏见，能识别规范行为却难以拒绝明显违规行为；（2）持续波斯语预训练反而加重了上述偏见，降低辨别矛盾的能力；（3）模型在事实检索与实际情景应用上有21%的性能差距。

Conclusion: 当前模型对文化能力的提升不能仅靠扩大单语数据，因为它们只是模仿文化模式而未真正内化背后规则，未来需要新的方法帮助模型理解和推理文化规范。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [90] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本文探讨了语言模型（如GPT-2）在受控合成语料条件下能否学习并再现人类语言中差异论据标记（DAM）的类型学趋势。结果显示模型能学到部分人类偏好，但在DAM对象优先趋势方面未能对齐。


<details>
  <summary>Details</summary>
Motivation: 人类语言中存在跨语言的类型学规律，近期研究发现，基于合成语料训练的语言模型在句法现象（如词序）上能够表现出类似的偏好。本文关注差异论据标记（DAM）——一种语义许可驱动的形态现象，旨在检验语言模型是否同样表现出符合人类语言的DAM类型学倾向。

Method: 作者利用18套各异的合成DAM体系设计语料，对GPT-2模型进行受控训练，并通过极小配对句测试其推广能力，判断模型对DAM类型学维度的偏好。

Result: 实验发现，模型能够显现出类似人类语言的天然标记性倾向——更倾向于在语义上非典型的论元（如不常出现作主语或宾语的名词）上使用显式形态标记。但却未能再现人类语言中“更常为宾语而非主语添加明确形态标记”的偏好。

Conclusion: 不同的类型学趋势可能有不同的产生机制。语言模型在某些类型学倾向上可与人类语言一致，但也存在无法对齐的现象，提示其归纳机制或学习路径与人类仍有差异。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [91] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文提出了一种基于UnigramLM分词算法的高效语言识别方法UniLID，特别适合低资源和细粒度方言识别，在仅有少量标注样本下即可实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多语言处理流程中的语言识别对高资源语言表现接近完美，但在低资源和近似语言上性能较差，制约了大规模语言模型的应用与扩展。

Method: 基于UnigramLM分词算法，学习对不同语言有条件的unigram分布，采用共享词表但语言特定的分词策略，实现分布高效估计、推断和增量扩展。

Result: 与fastText、GlotLID、CLD3等主流方法对比，UniLID在低资源和方言识别任务上表现优越，在每种语言仅有5个标注样本时准确率超过70%。

Conclusion: UniLID方法不仅高效且易于集成，并能以极低成本适配新语言，在多语言和低资源环境下为语言识别任务提供了突破性进展。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [92] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型（DLMs）在推理时开销高的问题，提出了Sink-Aware Pruning方法，能更高效地裁剪模型，提高生成质量与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型需要多轮迭代去噪推理，导致推理成本高，因此需要高效的模型裁剪方法。传统裁剪方法来自自回归大模型，认为Attention Sink不可删，但此假设不一定适用于DLMs。

Method: 通过实证分析，发现DLM中的Attention Sink位置在生成过程中变化大，并不总是结构所必需。基于此，提出Sink-Aware Pruning方法，可以自动识别并裁剪不稳定sink，且无需重新训练。

Result: 在与现有方法等量计算消耗的条件下，提出的方法在质量与效率权衡上均优于之前的主流裁剪基线。

Conclusion: Sink-Aware Pruning针对DLM的特点显著提升了模型裁剪效果，对DLM高效部署具有实际意义，并打破了Attention Sink必须保留的传统假设。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [93] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 量子计算对传统加密算法（如RSA）构成威胁。本文提出结合传统与量子安全机制的混合型安全框架以应对后量子时代安全挑战。


<details>
  <summary>Details</summary>
Motivation: RSA等传统加密技术依赖于大数分解的高难度，而量子计算（如Shor算法）可高效破解。为了保障数据安全，需要设计能对抗量子攻击的新型安全架构。

Method: 提出一个概念性的混合安全框架，结合传统AES加密、BB84量子密钥分发、量子态比对认证与仿生免疫系统检测威胁。各部分协同，实现密钥安全、认证与攻击检测。

Result: 分析表明，RSA对Shor算法脆弱；BB84能在理想条件下完成密钥协商，并高效检测窃听；提出的模型兼顾经典与量子安全，具备扩展性和自适应能力。

Conclusion: 提出的框架为后量子时代数据保护提供了一种可行方案。实际实现、理论证明及实验验证有待后续研究进一步完善。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [94] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动叉车在斜面上无拖曳地卸载托盘的控制方法，通过点云配准追踪托盘和货叉相对姿态，实现货叉与斜面平行后撤叉完成卸载。方法在仿真及真实叉车实验中有效。


<details>
  <summary>Details</summary>
Motivation: 传统自动叉车在斜面上卸载托盘时，货叉撤出易拖曳托盘，造成安全或操作问题，亟需更智能、精准的卸载方法。

Method: 采用ICP（迭代最近点）算法实时处理托盘上方区域的点云数据，追踪托盘与货叉的相对位置和姿态夹角，根据姿态自动调整并平行对齐货叉与目标面，最后沿斜面方向撤叉完成卸载。

Result: 通过动态仿真和真实叉车在带斜坡卡车上的卸载实验，验证了新方法在跟踪精度和防拖曳卸载效果上的有效性。

Conclusion: 所提基于点云ICP的卸载控制方法能显著提升自动叉车在斜面作业时的安全性和卸载质量，具有良好的实际应用前景。

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [95] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: 本文提出了一种针对四自由度并联运动铣削机器人的平滑轨迹生成方法，结合B样条和四元数插值技术，实现了高效且无万向锁的路径和姿态同步插值，提升了轨迹精度和效率。


<details>
  <summary>Details</summary>
Motivation: 并联运动机器人在加工任务中对轨迹平滑性和精度要求极高，传统轨迹插值方法存在姿态同步难、速度波动大及计算负担重等问题，急需新方法提升轨迹生成性能。

Method: 该方法采用B样条和四元数插值实现位置和姿态的解耦管理，通过分段Bezier曲线拟合同步姿态与弧长参数化位置数据，并利用序列二次规划优化曲线，使路径长度与工具姿态间的非线性关系得到精确表达。利用Bezier曲线的凸包性质满足多机器人空间及时间分离约束，姿态插值采用单位四元数避免万向锁，位置插值用修正多项式实现。时间轨迹采用两阶段最优（最小跃度、时最优）Bezier曲线方法优化，并部署在低成本微控制器上实现。

Result: 实验表明，该方法在轨迹精度、速度波动减小和计算效率等方面均优于常规插值方法，能够有效实现复杂工艺需求。

Conclusion: 所提轨迹生成方法为并联机器人提供了平滑、精确、高效的轨迹规划新方案，便于实际应用中的运动控制优化，具有理论与工程推广意义。

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [96] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: 本文提出了一种名为RRT$^η$的采样运动规划框架，通过引入AGM鲁棒性度量，有效提升了满足信号时序逻辑（STL）任务在多约束场景下的效果。实验验证了在多种机器人系统中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有采样式运动规划结合STL多用极大极小鲁棒性度量，过于关注局部关键点，导致优化过程非平滑，决策边界尖锐，影响路径搜索效率。因此亟需更平滑、全球分析的方法来提升路径规划性能，尤其在复杂约束和信号稀疏情况下。

Method: 提出RRT$^η$框架，将算术-几何平均（AGM）鲁棒性度量纳入以评价在所有时刻与子公式的任务达成度。具体贡献包括：1）AGM鲁棒性区间语义，提高树构建时对部分轨迹的判定能力；2）高效的区间增量监控算法；3）结合满足度优先逻辑（FPL），优化多目标任务组合。

Result: RRT$^η$能在满足STL约束条件下合成动态可行的控制序列，并保持RRT$^*$的概率完备性与渐进最优性。实验在三种机器人上（双积分点机器人、单轮移动机器人、7自由度机械臂）对比传统方法，在多约束且缺乏强引导信息时表现出更高的规划成功率和鲁棒性。

Conclusion: RRT$^η$框架通过细致、平滑的鲁棒性度量提升了带复杂时空逻辑约束下的运动规划能力，为采样式路径规划与形式化任务描述的结合提供了新思路，具备现实应用推广价值。

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [97] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: 本文提出一种利用振动弦作为感测元件的声学触觉传感新方法，显著简化大面积分布式触觉感测系统的实现难度。


<details>
  <summary>Details</summary>
Motivation: 现有大面积排列的触觉传感器因布线复杂、成本高且易损，限制了其在大范围、实时、高动态环境下的应用，亟需一种可扩展、宽覆盖与高动态性能兼备的新型触觉感测方案。

Method: 提出Sound of Touch系统，将张紧的弦体用电磁方式持续激励，通过少量拾音器（接触麦克风）监测触碰引起的振动频谱变化。借助物理建模模拟接触对弦体振动模式的影响，开发实时信号解析流程以估测触碰位置、法向力并检测滑移事件。

Result: 实验结果表明，该系统实现了毫米级接触定位、准确的力量估测和实时滑移检测。

Conclusion: Sound of Touch方法具备轻量、易扩展和低成本特点，适合大面积机器人表面布设；物理基础仿真和数据解析流程有效提升了触觉感测能力，为分布式大面积触觉感知提供了新方案。

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [98] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: 随着自主机器人在公共空间的增多，针对其的研究方法也需创新。本文提出了一种“与机器人同行”(WawR)的新方法，对自主机器人现实运行下的互动与社会影响进行田野研究。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人，尤其是送货机器人已在现实世界中独立运行，传统受控实验及半自动观测方法已难以捕捉其在动态、不可控环境中的真实表现，迫切需要更合适的实地研究方法。

Method: 借鉴城市学、人文地理与社会学中的公共领域民族志，提出Walk-Along with Robots (WawR)方法：研究者在真实公共空间中以“同行者”角色，伴随机器人自然运行，观察、记录其交互和社会反应，详述研究步骤、要点及评价方式。

Result: 通过运用WawR方法，获得了机器人在实际公共空间运行下的独特洞察，可发现机器人与环境、行人互动的新模式及潜在社会影响，是传统方法难以获取的信息。

Conclusion: WawR为自主机器人研究提供了新视角和有效工具，有助推动公共空间中机器人研究方法的发展，并期待引发学界对此进一步讨论与完善。

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [99] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: SimToolReal提出了一种可泛化的工具操作策略，使机器人能在不同任务和工具上无需专门调优即可实现灵巧操作，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工具操作对于机器人扩展任务能力非常重要，但涉及到的细致操作和强力交互让收集人机操作数据变得困难。现有sim-to-real RL方法对每个对象和任务都需大量建模与奖励函数调优，实用性有限。为解决泛化性差和工程量大的问题，作者提出自动生成多种工具对象并一次性训练通用策略。

Method: 作者在模拟环境中程序化生成大量各异的工具类对象，并用统一目标（任意对象到随机目标姿态）训练一个通用的RL策略；测试时无需针对具体对象或任务的再训练。

Result: SimToolReal相比现有重新定位和固定抓取方法性能提升37%，且达到了针对具体任务训练的专用RL策略的表现。在120组真实世界实验（覆盖24任务、12对象、6类工具）中，展现了强大的零样本泛化能力。

Conclusion: SimToolReal实现了跨任务、跨对象工具操作的通用RL策略，极大提升机器人灵巧操作能力，降低了开发成本，为实际应用提供了可行路径。

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [100] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT是一个包含多种传感器、真实复杂驾驶环境下、多时序和多气候的自动驾驶数据集，便于现有定位与里程计算法的评价与对比。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集多集中于简单环境，难以评价算法在复杂气象、路况变化下的泛化能力。Boreas-RT旨在提供更有挑战性的环境，推动自动驾驶算法的稳健性与通用性发展。

Method: Boreas-RT数据集在9条真实路线、多种气象和交通状况下，重复采集60组（合计643公里）行驶数据，利用多种高精度传感器（包括摄像头、360°雷达、两种激光雷达、IMU、编码器）及厘米级真值（GNSS-INS），并发布高精度标定、配套开发工具包和结果榜单。

Result: 基准测试显示，许多主流定位与里程计算法在传统数据集上表现良好，但在Boreas-RT提供的复杂条件下表现明显下降，暴露其对简单环境的过拟合问题。

Conclusion: Boreas-RT数据集为多模态自动驾驶算法在多样路况下的统一评价提供了坚实基础，将有助于推动业界算法的泛化能力和实用性进步。

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [101] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: MALLVi系统通过多智能体配合，实现了基于LLM和视觉模型的机器人操作任务规划，支持环境反馈闭环控制，大幅提升了操作的健壮性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机器人任务规划方法多为开环控制，缺乏对环境反馈的实时响应，导致在动态环境中易失效，且通常需要专门模型或微调，通用性有限。

Method: 提出MALLVi框架，整合大语言模型与视觉语言模型，通过多个专用智能体（Decomposer、Localizer、Thinker、Reflector和可选Descriptor）分工协作，实现任务分解、感知、定位、推理、高层规划与视觉记忆，并利用Reflector agent实现针对性错误检测与局部恢复。机器人根据自然语言指令和环境图像生成原子动作，动作执行后VLM评估反馈，决定下一步。

Result: 在仿真与实际环境实验中，MALLVi展现出较以往方法更好的泛化能力和任务成功率，特别是在零样本操控任务上，闭环的多agent协同明显提升了执行效果。

Conclusion: MALLVi突破了开环LLM规划系统的局限，实现了更鲁棒、更具通用性的闭环机器人操作任务规划，为多智能体系统在复杂环境下的有效协作提供了新范式。

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [102] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: 本文提出了一种通过图结构表达场景和操作关系的方法，用于高效学习机器人在长时序操控任务中的目标。方法通过分割和聚合演示，将过程转为系列操作图，实现了跨全过程的物体交互建模，相比于以往方法覆盖更全面并更健壮。


<details>
  <summary>Details</summary>
Motivation: 机器人学习长期复杂任务时，直接在动作空间学习效率低且泛化难。因此，研究者希望借助对"做什么"的理解、而非"怎么做"，通过抽象表示方式提高任务泛化与学习效率。

Method: 通过将场景状态编码为物体关系的图，并结合演示分割与聚合，提取出遍历全过程的操作关系。方法还利用预训练视觉特征，提升了多演示下的物体匹配鲁棒性。

Result: 实验证明该方法在演示分割精度和基于多演示自动推断任务最小模型方面表现出色。最终，所得模型在仿真和真实机器人上都实现了稳定可靠地任务执行。

Conclusion: 该方法能有效、健壮地学习和表达复杂操作任务目标，可提升机器人多场景自适应执行能力。

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [103] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: 本文提出一个新型基准，用以评估三维重建结果对下游抓取任务（如6D姿态估计和抓取姿态生成）的实际影响，分析了三维模型质量对机器人操作表现的作用。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法在视觉和几何上表现出色，但传统的几何指标无法衡量其在实际机器人操作任务中的价值，导致感知成果与机器人应用间存在缺口。

Method: 作者构建了一个基于物理仿真的大规模基准，通过生成不同三维重建模型下的抓取姿态，并在真实（ground-truth）模型上复现这些抓取行动，从而系统性分析模型误差（包括姿态误差和几何不准确）对抓取成功率的影响。

Result: 结果发现，三维重建带来的几何伪影显著减少了可用的抓取候选数量，但在姿态估计准确时，对实际抓取成功率影响甚微。同时，抓成功率与姿态误差主要受空间误差主导，简单的平移误差对对称物体抓取效果有较强的解释力。

Conclusion: 本研究为三维感知系统在机器人操作中的实际作用提供了新认识，并指出三维模型几何误差对抓取任务影响有限，有助于指导感知与操作系统的集成设计。

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [104] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，可将刚性夹爪的抓取位姿映射到软体Fin-ray夹爪，实现更加高效和准确的抓取策略迁移。


<details>
  <summary>Details</summary>
Motivation: 当前抓取位姿合成方法多针对刚性夹爪，直接迁移到软体夹爪时因未考虑其柔顺性，导致模型数据需求大且准确性差。为解决这一“表征鸿沟”，需开发能准确、数据高效地实现抓取动作迁移的新方法。

Method: 作者利用生成模型Conditional Flow Matching（CFM），结合数据采集流程获得配对的刚性-软体夹爪抓取位姿。采用U-Net自编码器，将物体深度图的几何信息输入到CFM，实现从Anygrasp初始位姿到稳定Fin-ray夹爪位姿的连续映射。整个流程在7自由度机械臂上验证。

Result: 实验显示，软体夹爪基于CFM生成的抓取位姿在已经见过和未见过的物体上总体抓取成功率分别为34%和46%，远高于基线刚性位姿（6%和25%）。尤其对圆柱体和球体物体，模型在未见过物体上的泛化能力表现突出（分别达100%和31%成功率）。

Conclusion: CFM提供了一种数据高效、适用于软体机器人抓取策略迁移的方法，能够提升软体夹爪抓取准确性并具备良好泛化能力，为更广泛的软体机器人系统提供了可扩展的方法论。

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [105] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文提出了一种基于增强现实（AR）的物理人机交互框架，实现了对混合刚软机器人的直接遥操作，用于抓取等任务。通过AR头显，用户可操作集成物理引擎的机器人虚拟模型，在真实系统上叠加虚拟操作，进行仿真试验后再部署到现实中。引入真实-仿真参数识别流程，使虚实机器人行为一致。


<details>
  <summary>Details</summary>
Motivation: 刚软混合机器人兼有刚性操作臂的精确和软性臂的柔顺、适应性，适用于复杂环境下的灵巧抓取，但因模型、感知及不同域动力学协调难，实际控制始终存在挑战。

Method: 搭建了一个基于AR的人机交互系统，用户佩戴AR设备，通过物理引擎直接操作混合机器人，可以在虚拟空间预演操作。提出了参数识别流程，依据软机器人的结构性质，实现机器人静动态建模及控制系统响应的高精度识别。

Result: 该系统使用户无障碍地通过虚拟模型远程控制和操作混合机器人，基于参数识别达到了虚-实机器人的一致运动和响应，显著提升了机器人在实际任务中的操作性。

Conclusion: AR结合参数识别为混合刚软机器人提供了便捷且高保真的遥操作交互新方案，有效提升其控制准确性和实用性，对复杂环境下的智能机器人操作具有推动作用。

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [106] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: 本文提出了面向机器人、基于SO(3)的固定翼飞行器逆动力学坐标无关表述，推导了从飞行轨迹到飞控输入的解析解，为轨迹设计和可行性分析提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前飞行器逆动力学多依赖局部坐标和经验公式，难以适应机器人对灵活性与通用性的需求。作者希望用几何化、坐标无关的方法，提升逆动力学建模的一致性和准确性，并方便轨迹设计和自动化分析。

Method: 采用几何工具，将平移力平衡写在世界坐标系而转动动力学写在机体坐标系，定义气动方向为几何向量，避免依赖姿态欧拉角。通过强制协调飞行（无侧滑），推导出能直接从轨迹给出姿态、角速度和推力-攻角对的闭式解，并逐项恢复气动力矩系数。

Result: 针对绑缆圆周飞行推导所需滚转角的解析表达式，发现存在特定零滚转轨迹点，此时缆绳拉力和离心力完全平衡，揭示气动协调与表观重力的分离关系。简单升阻律下，最小推力攻角有闭式解。

Conclusion: 该框架把航空逆向仿真和机器人几何建模结合，可为轨迹生成和可行性检验提供理论基础和高效工具，适于自动化分析和机器人应用。

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [107] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: 该论文提出了一个用于无人机携带悬挂柔性缆绳的实时动力学控制框架，并以高保真PDE模型和降阶模型结合实现高效稳定控制。


<details>
  <summary>Details</summary>
Motivation: 灵活缆绳的动力学复杂且强非线性，为无人机操作带来挑战，现有方法在实时性和精确性难以兼顾，因此需要兼具高效与精度的控制新框架。

Method: 基于偏微分方程（PDE）建立高精度动力学模型，通过有限差分法离散化；利用正交分解法获得降低维的实时可控模型（ROM）；基于ROM设计非线性模型预测控制器（NMPC），以稳定缆绳动态并处理负载挂接/分离等混合过程。

Result: 仿真验证了ROM的稳定性、效率和鲁棒性，控制器能有效调节缆绳运动，支持负载切换等多种工况。进一步仿真表明该方法能实现受限环境下的柔性缆绳轨迹规划。

Conclusion: 提出的ROM与控制框架实现了无人机携带柔性缆绳的实时、动态感知和高效控制，具备稳定性、泛用性及轨迹规划能力。

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [108] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动系统在重复环境下实现高效映射与定位的新型多会话框架，重点解决了重复观测地区下地图生成和定位的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶、测绘及机器人等领域中，系统经常需要在已访问过的环境中反复运行，当前多采用单独重建地图并寻求其间的关联，累积误差大，全球一致性差。

Method: 提出一种基于地图定位的多会话框架，引入了拓扑结构与不确定性感知的决策机制，通过分析位姿图结构检测低连通区域，选择性地激活建图与回环检测模块，使边地图与位姿图平滑整合进已有模型。

Result: 在公开数据集的重叠序列和真实矿井环境下验证方法，有效提升了定位精度与地图一致性。

Conclusion: 新框架减少了累积误差，增强了地图的全球一致性，对重复环境下的自动系统具有实际应用价值。

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [109] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了FRAPPE方法，以提升视觉-语言-动作（VLA）模型对环境动态（world modeling）的预测能力，实现更强的机器人推理和泛化能力。通过改进训练目标和推理机制，有效解决了像素级重建过度和误差累积问题，在多个领域取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA world modeling方法在训练过程中过度关注像素级重建，限制了语义学习和泛化能力；此外，在推理时依赖未来观测的预测导致误差累积。这些问题限制了机器人策略的通用性和效率。

Method: 提出FRAPPE方法，采用“两阶段微调策略”：中期训练阶段让模型预测未来观测的潜在表征，后期训练阶段则在多个视觉基础模型间并行扩展计算，并对齐表征，从而提升泛化和效率。

Result: 在RoboTwin基准和真实世界任务的实验中，FRAPPE方法超越了当前最先进方法，在长时序和未见过的场景中表现出更强的泛化和效果。

Conclusion: FRAPPE显著提升了微调效率，减少了对动作注释数据的依赖，为通用机器人策略提供了更高效、可扩展的world modeling解决方案，并展示了优异的实验效果。

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [110] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 本论文提出了一种完全基于本体感知（不依赖相机或激光雷达）的腿式机器人状态估计方法，只需IMU与电机传感器即可实现对机器人位姿和速度的高精度估计。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在缺乏外部传感器（如摄像头或激光雷达）条件下的里程计精度受限，主要瓶颈为IMU漂移和关节速度传感噪声。因此，开发一种依赖本体传感器即可实现可靠状态估计的方法意义重大。

Method: 作者采用每个接触腿作为运动学锚点，结合基于关节扭矩的足部受力估计以检测可靠接触点，通过足落位置在世界坐标中的间断约束抑制长期漂移。针对足落高度漂移引入了基于聚类和时间衰减的平面修正法。采用逆运动学Cubature卡尔曼滤波直接滤波足端速度。对于偏航漂移，结合多接触点几何一致性及在IMU不可用时回退到运动学推算的朝向参考。

Result: 在四个平台（包括三台Astrall机器人和一台Unitree Go2 EDU）上闭环运行实验，表现为：如Astrall点足机器人在约200米水平环路上仅0.16米误差，15米垂直环路0.22米误差。带轮腿机器人在700米环路上7.68米误差，20米垂直环路0.54米误差。Unitree Go2 EDU在120米环路2.21米误差，8米垂直环路小于0.1米误差。

Conclusion: 该方法在不同类型平台和运动模式下均表现出较高鲁棒性和准确性，验证了完全本体输入（IMU+电机传感）下的状态估计可行性，有望应用于资源受限或无外部感知器件环境下的腿式机器人定位导航。

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [111] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: 本文提出了一种基于虚拟模型控制（VMC）的分散式、安全感知的人机协作控制框架，通过虚拟弹簧和阻尼模拟人机协作行为，实现更安全且无死锁的协作。


<details>
  <summary>Details</summary>
Motivation: 传统人机协作依赖于集中控制和明确轨迹规划，易产生死锁和安全风险，缺乏灵活性，难以扩展。研究动机是提升人机协作系统的安全性、灵活性和可扩展性。

Method: 将人和机器人置于同一虚拟组件形态的工作空间内，运动通过与虚拟弹簧和阻尼器的交互产生，而非显式规划轨迹。设计了基于力的分散式死锁检测器，并引入协商机制解决死锁，实现结构可扩展的分布式实现。

Result: 在实验中，机器人在积木任务中的卡死率从最高61.2%降至0；机器人与人安全协作，实体实验支持2人2机，仿真达4机，保持约20厘米安全距离，所有场景下均无死锁。

Conclusion: 该方法通过调整控制参数能直观地塑造机器人行为，实现各规模团队下的安全、无死锁人机协作，具有良好的扩展性和通用性。

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [112] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: 该论文提出了一种基于3D打印的软体光学传感器（SOLen），可实现高复杂度软体机器人的单材质一体化制造。通过引入3D打印透镜和特殊波导结构，提升了传感精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着增材制造的发展，软体机器人结构日趋复杂，对能与单步制造、单一材料兼容的传感方案有更高需求。现有光学传感器常因光信号泄露、散射等问题性能受限，而常规缓解手段需多材料组合，限制了一体化制造。

Method: 作者提出在Y型波导中集成3D打印透镜。传感原理为软体受力变形导致透镜旋转、焦斑位移，从而引导光能分配变化，输出包含运动方向与幅值的信息。通过调配高透性树脂、测量与仿真折射率，实现了精准设计并打印透镜结构。

Result: 成功制备了高保真透镜及Y型波导，实验证实分支信号可稳定切换并重复输出，单层光学测试优化了材料参数。

Conclusion: 该方法建立了软体机器人用光学传感器材料与结构一体化设计制造流程，为下一代软体机器人赋予新型感知功能奠定基础。

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [113] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: 本文提出一种经济实用的相机硬件解决方案，有效提升自动驾驶车辆在雨天环境下的感知性能，并显著提高了深度学习模型的行人检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有应对恶劣天气的感知硬件解决方案存在成本高、适用性有限的问题，限制了自动驾驶车辆的可靠部署。为了满足系统可持续性和提升雨天感知性能，亟需低成本且能兼容多相机的物理硬件方案。

Method: 开发了一种低成本、可同时兼容多相机的感知系统硬件。该系统可以与现有相机平台结合，无需额外高成本传感器或硬件替换，便于模块化升级和大规模部署。

Result: 在雨天条件下，该系统将深度学习模型的行人检测准确率从8.3%提升至41.6%。

Conclusion: 该硬件方案不仅提升了自动驾驶在恶劣天气下的可靠性，还助力运输系统的可持续发展，降低资源消耗和车辆排放，增强了自动驾驶技术的经济性和可扩展性。

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [114] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种新型光学传感电缆丝带致动器（OS-ERA），实现了对致动器复杂弯曲状态的高精度识别，为闭环控制奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有电缆丝带致动器（ERA）的嵌入式电容传感存在精度低的问题，影响了运动的准确控制。为实现闭环反馈和更精细的控制，需要更可靠的自感知解决方案。

Method: 将两个软性光学波导传感器集成进ERA，实现对致动器在运动过程中的复杂弯曲形态进行实时感知。训练分类器，将传感信号映射为八种不同的弯曲状态，并通过多组实验验证模型的准确性和重复性。

Result: 实验表明，感知信号与训练时的轨迹一致，分类器在速度和电压不同条件下依然能高精度辨识八种弯曲状态，表现出优良的速度与电压不变性。

Conclusion: OS-ERA实现了ERA复杂动态状态的快速、高保真、可重复分类，突破了传统电缆丝带致动器感知精度瓶颈，为其闭环控制和更复杂应用提供了技术支持。

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [115] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: 本研究探索了动力膝关节假肢动力部件上膝与下膝安装位置对行走能力的影响，初步发现上膝安装有助于提高步行速度和步频，同时保持良好的膝关节运动范围和控制策略的稳健性。


<details>
  <summary>Details</summary>
Motivation: 动力膝关节假肢能够帮助截肢者恢复部分行动能力，但动力系统增加的质量可能带来负面效应。因此，本研究关注于如何通过优化假肢质量分布（而非单纯减轻质量）来更好地平衡动力辅助带来的益处与质量增加的弊端。

Method: 在一项探索性研究中，作者比较了动力假肢动力系统安装于上膝位置与下膝位置，对一小组参与者进行步行测试。评估指标包括步行速度、步频、步态对称性、膝关节运动范围及在坡道和楼梯上的适应性等。

Result: 结果显示，上膝位置安装能够提升步行速度（提升9.2%）和步频（提升3.6%），但对步态对称性的影响不一。两种安装方式下膝关节运动范围和峰值速度相似。控制策略在多种地形下表现稳健。

Conclusion: 初步结果表明，将动力系统安装于膝关节上方在功能上是可行的，合理分布质量有助于维持动力辅助带来的益处并减轻负面影响。需进一步研究以验证这一趋势，为假肢设计和临床应用提供参考。

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [116] [RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments](https://arxiv.org/abs/2602.17515)
*Ziyi Zong,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: 提出了一种针对突发静态障碍物变为动态障碍物场景的风险感知导航框架RA-Nav，能够实时进行环境语义分割、障碍物分类与风险评估，并通过风险信息指导航迹规划，提高了安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有空中机器人导航系统难以应对原本静态障碍物突然运动带来的风险，缺乏针对障碍物运动状态突变的动态适应能力，需要融合环境语义信息以提升安全性。

Method: 采用轻量级多尺度语义分割网络对环境中的障碍物进行实时识别，将障碍物分为静止、暂时静止和动态三类；针对不同类别设计对应的风险评估函数，生成实时本地风险地图；在此基础上，设计风险感知路径搜索算法平衡路径效率和安全性，并进行轨迹优化保证轨迹的安全、平滑和动态可行。

Result: 在突发障碍物状态转换的仿真场景中，与其他基线方法对比，RA-Nav在任务成功率方面表现更优；并在真实世界数据的仿真中进一步验证了其有效性。

Conclusion: RA-Nav框架通过集成语义感知和风险评估，提升了空中机器人面对障碍物状态突变时的适应性和安全性，具有实际应用价值。

Abstract: Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.

</details>


### [117] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、高性能的智能机器人摄像系统IRIS，能够自主学习并重复执行复杂摄影运动。


<details>
  <summary>Details</summary>
Motivation: 当前工业级机器人摄像系统成本高、操作复杂，限制了其普及应用。为降低使用门槛，开发面向特定任务、性价比高的自主摄影机器人平台成为关键需求。

Method: 设计了一个6自由度、全3D打印的轻量机械臂，集成基于Transformer的动作分块模仿学习（ACT），通过人类演示直接学习对象感知与视觉平滑的摄影轨迹，无需明确的几何编程。

Result: 系统材料成本低于1000美元，负载能力达1.5公斤，重复精度约为1毫米。实验证明该系统能够精准跟踪轨迹，执行高可靠度摄影动作，并可泛化多样化摄影任务。

Conclusion: IRIS实现了低成本、高精度和自动化的机器人摄像运动，对拓展机器人视觉拍摄工具的普及应用具有重要意义。

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [118] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: 本文提出了首个专为急救人员通过手势控制无人地面车辆（UGV）设计的数据集FR-GESTURE，包含12种实用命令与3312组RGBD数据，并设定了评测方法，促进相关AI与机器人领域研究。


<details>
  <summary>Details</summary>
Motivation: 鉴于灾害频发且强度增加，急救人员在救援任务中面临更大难度。利用人工智能和机器人技术，尤其是手势控制，可减轻其工作负担。但目前缺乏针对急救人员手势控制UGV的专门数据集。本文旨在填补该空白。

Method: 1) 与经验丰富的急救人员合作，基于现有救援及战术手势，定义并完善12种控制命令；2) 采集2个视角和7种距离共3312对RGBD手势数据；3) 提出针对该数据集的评测协议；4) 进行基线实验并开放数据集。

Result: 成功构建了FR-GESTURE数据集，涵盖多角度多距离数据，提供了首套用于急救场景的手势控制UGV基线数据和评测方法，并通过公开实验展示了数据集初步效果。

Conclusion: FR-GESTURE数据集为急救人员手势控制UGV的研究提供了基础资源和评测标准，为人工智能、机器人与灾害应急领域搭建了桥梁，有助于未来研究和实际应用推广。

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [119] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: 该论文提出了一种结合混合zonotope与新型ADMM混合整数优化启发式算法，用于提升嵌入式混合系统的运动规划效率，并在自动驾驶场景中得到实验验证。


<details>
  <summary>Details</summary>
Motivation: 混合系统的嵌入式最优规划通常需要使用混合整数规划，但现有方法计算开销大且对数值实现很敏感，这限制了其实用性和效果。

Method: 提出将混合zonotope（一种高级集合表达方式）与新型ADMM混合整数规划启发式算法结合，详细分析分段仿射（PWA）系统的可达性，并将其用于最优规划问题中。提出的集合有更低的内存复杂度和更紧的凸松弛，与现有方法相比更优；ADMM启发式方法充分利用了混合zonotope结构。

Result: 用于混合zonotope建模的规划问题上，所提ADMM启发式在收敛速度上超过了现有的混合整数规划启发式算法，并且在嵌入式硬件上得到了实验验证。

Conclusion: 该方法提升了混合系统的嵌入式优化规划性能，降低了计算和存储成本，并在自动驾驶实验中表现出较优的实际效果。

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [120] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow是一种用于L4自动驾驶安全验证的无监督异常检测方法，通过对人类专家驾驶行为的概率密度建模，实现了对稀有高风险场景的精准检测，显示出优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在安全验证中面临瓶颈，尤其是极端或罕见风险场景难以用规则或启发式方法高效检测，因此需要一种能够全面刻画和发现安全关键异常的新方法。

Method: 提出Deep-Flow无监督框架，利用最优传输条件流匹配（OT-CFM）学习人类驾驶概率流，采用PCA约束降维，设计Transformer早期融合编码器并加入车道及意图条件，实现动力学平滑及稳定概率估计。对训练过程按动力学复杂度加权，引入跳跃连接维持最终意图。该方案在WOMD数据集上验证。

Result: Deep-Flow在WOMD数据集上得到0.766的AUC-ROC，能发现传统安全过滤遗漏的越界及非法路口行为，显著区分运动学危险与语义不合规，对异常行为检测能力优于现有方法。

Conclusion: Deep-Flow为自动驾驶安全验证提供了严谨的概率建模基础，实现了对长期低概率高风险场景的检测补全，为自动驾驶车辆安全部署提供了客观、可量化的数据驱动验证手段。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>


### [121] [Graph Neural Model Predictive Control for High-Dimensional Systems](https://arxiv.org/abs/2602.17601)
*Patrick Benito Eberhard,Luis Pabon,Daniele Gammelli,Hugo Buurmeijer,Amon Lahr,Mark Leone,Andrea Carron,Marco Pavone*

Main category: cs.RO

TL;DR: 提出了一种结合图神经网络（GNN）动力学建模与结构优化的模型预测控制（MPC）框架，实现了高维系统如软体机器人的实时高效控制。


<details>
  <summary>Details</summary>
Motivation: 高维复杂系统（尤其是软体机器人）动力学复杂且计算量大，现有方法难以兼顾建模精度与实时性。作者希望通过保留系统中稀疏结构和局部交互，以减少计算负担，实现高效实时控制。

Method: 将高维系统建模为图结构，采用GNN捕捉局部动力学，并利用特定的状态变量消元算法剥离MPC中的状态，提高推理速度。该消元算法计算复杂度与节点数线性相关，且支持GPU并行。

Result: 在仿真和真实软体机器人实验中，方法可实时闭环控制高达1,000节点系统，频率达100Hz，硬件上跟踪精度优于基线方法63.6%，同时能实现全身避障。

Conclusion: 结合GNN和结构优化的MPC能高效准确地控制高维系统，具备良好的扩展性和实时性。该方法可推广至其他大规模复杂系统控制。

Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.

</details>
