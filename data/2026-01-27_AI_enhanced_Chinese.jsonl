{"id": "2601.16985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16985", "abs": "https://arxiv.org/abs/2601.16985", "authors": ["Pierrick Lorang"], "title": "Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics", "comment": "IEEE ICRA 2025 Doctoral Consortium", "summary": "Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u67b6\u6784\uff0c\u5c06\u5c42\u6b21\u62bd\u8c61\u3001\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e2d\u7684\u5f00\u653e\u4e16\u754c\u73af\u5883\uff0c\u81ea\u52a8\u5316\u7cfb\u7edf\u7ecf\u5e38\u9047\u5230\u65e0\u6cd5\u9884\u89c1\u7684\u65b0\u60c5\u51b5\u3002\u6df7\u5408\u578b\u7684\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6709\u6548\uff0c\u4f46\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u9002\u5e94\u6162\u548c\u9057\u5fd8\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5c06\u5c42\u6b21\u62bd\u8c61\u3001\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u3002\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u7b26\u53f7\u63a8\u7406\u4e0b\u7684\u76ee\u6807\u5bfc\u5411\u5b66\u4e60\u548c\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\uff0c\u4f7f\u7cfb\u7edf\u80fd\u8fc5\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6df7\u5408\u578b\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u3001\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u7b49\u591a\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e0b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u62e5\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17219", "abs": "https://arxiv.org/abs/2601.17219", "authors": ["David Wireko Atibila", "Vineet R. Kamat", "Carol C. Menassa"], "title": "Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap", "comment": "73 pages, 8 figures", "summary": "The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5efa\u7b51\u4e1a\u4e2d\u4eba\u673a\u534f\u4f5c\u5373\u5174\u80fd\u529b\u7684\u5206\u7c7b\u4e0e\u53d1\u5c55\uff0c\u63d0\u51fa\u516d\u7ea7\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u805a\u7126\u8f83\u4f4e\u7ea7\u522b\uff0c\u6307\u51fa\u5411\u9ad8\u5c42\u6b21\u534f\u4f5c\u5373\u5174\u53d1\u5c55\u5b58\u5728\u6280\u672f\u4e0e\u65b9\u6cd5\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5efa\u7b51\u4e1a\u9762\u4e34\u751f\u4ea7\u529b\u505c\u6ede\u3001\u719f\u7ec3\u5de5\u4eba\u77ed\u7f3a\u548c\u5b89\u5168\u9690\u60a3\u3002\u5c3d\u7ba1\u5f15\u5165\u673a\u5668\u4eba\u81ea\u52a8\u5316\u6709\u4e00\u5b9a\u6210\u6548\uff0c\u4f46\u673a\u5668\u4eba\u96be\u4ee5\u9002\u5e94\u5efa\u7b51\u5de5\u5730\u7684\u52a8\u6001\u3001\u65e0\u7ed3\u6784\u73af\u5883\uff0c\u5c24\u5176\u662f\u5373\u5174\u9002\u5e94\u80fd\u529b\uff0c\u8fd9\u6210\u4e3a\u963b\u788d\u751f\u4ea7\u529b\u63d0\u5347\u7684\u5173\u952e\u74f6\u9888\u3002\u63a8\u52a8\u4eba-\u673a\u534f\u4f5c\u5373\u5174\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u5bf9\u6539\u5584\u5efa\u7b51\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b89\u5168\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u5bf92010-2025\u5e74\u95f4214\u7bc7\u76f8\u5173\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u5e76\u5e94\u7528\u4e00\u4e2a\u516d\u7ea7\u4eba-\u673a\u534f\u4f5c\u5373\u5174\u80fd\u529b\u5206\u7c7b\u4f53\u7cfb\uff1a\u4eba\u5de5\u64cd\u4f5c\u3001\u4eba\u63a7\u6267\u884c\u3001\u81ea\u9002\u5e94\u64cd\u4f5c\u3001\u6a21\u4eff\u5b66\u4e60\u3001\u4eba\u673a\u534f\u4f5cBIM\u6d41\u7a0b\u3001\u4e91\u7aef\u77e5\u8bc6\u6574\u5408\u548c\u771f\u6b63\u534f\u540c\u5373\u5174\u3002\u901a\u8fc7\u5bf9\u89c4\u5212\u3001\u8ba4\u77e5\u89d2\u8272\u3001\u7269\u7406\u6267\u884c\u3001\u5b66\u4e60\u80fd\u529b\u548c\u5373\u5174\u4e94\u7ef4\u80fd\u529b\u6f14\u5316\u7684\u96f7\u8fbe\u6846\u67b6\uff0c\u5168\u9762\u5206\u6790\u5404\u7ea7\u522b\u7814\u7a76\u73b0\u72b6\u53ca\u80fd\u529b\u63d0\u5347\u8def\u5f84\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u90e8\u5206\u673a\u5668\u4eba\u534f\u4f5c\u7814\u7a76\u505c\u7559\u5728\u8f83\u4f4e\u7ea7\u522b\uff0c\u7279\u522b\u662f\u5728\u7ecf\u9a8c\u5b66\u4e60\u548c\u534f\u540c\u5373\u5174\u65b9\u9762\u660e\u663e\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u5b58\u5728\u4e09\u5927\u969c\u788d\uff1a\u6280\u672f\u5c42\u9762\u5bf9\u5171\u4eab\u77e5\u8bc6\u4e0e\u5bf9\u8bdd\u63a8\u7406\u7684\u9650\u5236\u3001\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5373\u5174\u4e4b\u95f4\u7684\u6982\u5ff5\u5dee\u8ddd\u4ee5\u53ca\u65b9\u6cd5\u5b66\u7684\u6311\u6218\u3002\u63d0\u51fa\u4e86\u53ef\u89c6\u5316\u96f7\u8fbe\u6846\u67b6\u5c55\u793a\u4eba-\u673a\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\u5b9e\u73b0\u9ad8\u5c42\u6b21\u4eba\u673a\u534f\u540c\u5373\u5174\u4ecd\u9762\u4e34\u8bf8\u591a\u56f0\u96be\u3002\u672a\u6765\u5e94\u91cd\u70b9\u63d0\u5347\u4eba\u673a\u95f4\u6c9f\u901a\u80fd\u529b\uff0c\u5efa\u8bae\u7ed3\u5408\u589e\u5f3a/\u865a\u62df\u73b0\u5b9e\u754c\u9762\u3001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e91\u7aef\u77e5\u8bc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd9\u4e9b\u6280\u672f\u7a81\u7834\u66f4\u597d\u5730\u5b9e\u73b0\u5efa\u7b51\u673a\u5668\u4eba\u534f\u540c\u5373\u5174\uff0c\u91ca\u653e\u4ea7\u4e1a\u6f5c\u529b\u3002"}}
{"id": "2601.17227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17227", "abs": "https://arxiv.org/abs/2601.17227", "authors": ["Avraiem Iskandar", "Shamak Dutta", "Kevin Murrant", "Yash Vardhan Pant", "Stephen L. Smith"], "title": "Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization", "comment": null, "summary": "We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ee5\u6709\u9650\u9884\u7b97\u91c7\u96c6\u9ad8\u4ef7\u503c\u4fe1\u606f\uff0c\u663e\u8457\u964d\u4f4e\u76ee\u6807\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u867d\u6709\u5168\u5c40\u6027\u4fdd\u8bc1\uff0c\u4f46\u9700\u8981\u9884\u5148\u9009\u597d\u91c7\u6837\u70b9\uff0c\u5c40\u90e8\u8fde\u7eed\u8f68\u8ff9\u4f18\u5316\u5219\u5728\u969c\u788d\u7269\u591a\u7684\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e14\u5bf9\u521d\u59cb\u503c\u654f\u611f\u3002\u5b9e\u9645\u5e94\u7528\u9700\u8981\u517c\u987e\u5168\u5c40\u89c6\u91ce\u548c\u5c40\u90e8\u8def\u5f84\u7cbe\u7ec6\u6027\uff0c\u5e76\u4e14\u8981\u5728\u6709\u9650\u9884\u7b97\u4e0b\u9ad8\u6548\u83b7\u5f97\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u7684\u5206\u5c42\u6846\u67b6\uff1a(1) \u5148\u7528\u56fe\u641c\u7d22\u8fdb\u884c\u5168\u5c40\u8def\u5f84\u89c4\u5212\uff1b(2) \u7528\u51e0\u4f55\u548c\u6838\u51fd\u6570\u754c\u5bf9\u6bcf\u6bb5\u8def\u5f84\u5206\u914d\u9884\u7b97\uff1b(3) \u5728\u6bcf\u6bb5\u5185\u7528\u6837\u6761\u4f18\u5316\u5e76\u4e25\u683c\u6ee1\u8db3\u969c\u788d\u7ea6\u675f\u3002\u8fd9\u4e2a\u7b56\u7565\u7ed3\u5408\u4e86\u56fe\u65b9\u6cd5\u7684\u5168\u5c40\u89c6\u89d2\u548c\u8fde\u7eed\u65b9\u6cd5\u7684\u5c40\u90e8\u7cbe\u7ec6\u8c03\u6574\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u5b9e\u9645\uff08\u5982\u5317\u6781\u6570\u636e\u96c6\uff09\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u53d6\u5f97\u6bd4\u7eaf\u56fe\u65b9\u6cd5\u548c\u8fde\u7eed\u65b9\u6cd5\u66f4\u4f4e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e14\u8fd0\u7b97\u901f\u5ea6\u6700\u9ad8\u6bd4\u73b0\u6709\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\u5feb9-20\u500d\u3002", "conclusion": "\u8be5\u5206\u5c42\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u5168\u5c40\u5f15\u5bfc\u548c\u5c40\u90e8\u4f18\u5316\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u73af\u5883\u4e0b\u7684\u6709\u9650\u9884\u7b97\u4fe1\u606f\u91c7\u96c6\u95ee\u9898\uff0c\u517c\u5177\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2601.17231", "categories": ["cs.RO", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17231", "abs": "https://arxiv.org/abs/2601.17231", "authors": ["Tanmay Desai", "Brian Plancher", "R. Iris Bahar"], "title": "Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration", "comment": "8 pages, 5 figures", "summary": "Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728FPGA\u4e0a\u4f18\u5316\u4e86MPPI\u7b97\u6cd5\u5b9e\u73b0\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff08AMR\uff09\u5728\u5d4c\u5165\u5f0f\u7aef\u7684\u63a8\u7406\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u91c7\u6837\u7684MPC\uff08\u5982MPPI\uff09\u975e\u5e38\u9002\u7528\u4e8eAMR\u4e2d\u7684\u5b9e\u65f6\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u5e76\u4e14\u53ef\u5728GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002\u4f46\u5728\u7535\u6c60\u53d7\u9650\u3001\u80fd\u6548\u654f\u611f\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\uff0cGPU/CPU\u5b9e\u73b0\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u80fd\u8017\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u9488\u5bf9FPGA\u4f18\u5316\u7684MPPI\u63a7\u5236\u5668\u7ed3\u6784\uff0c\u5229\u7528\u6df1\u5ea6\u6d41\u6c34\u7ebf\u4e0e\u8de8\u7b97\u6cd5\u9636\u6bb5\u7684\u9ad8\u5ea6\u5e76\u884c\uff0c\u6d88\u9664\u540c\u6b65\u74f6\u9888\uff0c\u4ece\u800c\u6700\u5927\u5316\u786c\u4ef6\u5e76\u884c\u5ea6\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u4f18\u5316\u8fc7\u7684\u5d4c\u5165\u5f0fGPU\u548cCPU\u5b9e\u73b0\uff0c\u8be5FPGA\u65b9\u6848\u83b7\u5f97\u4e863.1\u20137.5\u500d\u7684\u901f\u5ea6\u63d0\u5347\u53ca2.5\u20135.4\u500d\u7684\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "FPGA\u4e3a\u8fb9\u7f18\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u4e14\u80fd\u6548\u4f18\u8d8a\u7684MPC\u52a0\u901f\u65b9\u6848\uff0c\u6709\u529b\u63a8\u52a8\u4e86\u80fd\u91cf\u654f\u611f\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u5e73\u53f0\u7684\u843d\u5730\u3002"}}
{"id": "2601.17027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17027", "abs": "https://arxiv.org/abs/2601.17027", "authors": ["Honglin Lin", "Chonghan Qin", "Zheng Liu", "Qizhi Pei", "Yu Li", "Zhanping Zhong", "Xin Gao", "Yanfeng Wang", "Conghui He", "Lijun Wu"], "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "comment": null, "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u7814\u7a76\u4e86\u79d1\u5b66\u56fe\u50cf\u5408\u6210\uff0c\u5305\u62ec\u751f\u6210\u8303\u5f0f\u3001\u8bc4\u4f30\u6307\u6807\u548c\u4e0b\u6e38\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u903b\u8f91\u9a71\u52a8\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6ImgCoder\u53ca\u65b0\u7684\u79d1\u5b66\u56fe\u50cf\u8bc4\u6d4b\u57fa\u51c6SciGenBench\u3002", "motivation": "\u5728\u6587\u672c\u9886\u57df\uff0c\u5408\u6210\u6570\u636e\u5df2\u88ab\u8bc1\u660e\u6709\u52a9\u4e8e\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u591a\u6a21\u6001\u63a8\u7406\u53d7\u9650\u4e8e\u7f3a\u4e4f\u79d1\u5b66\u4e25\u8c28\u7684\u5408\u6210\u56fe\u50cf\u3002\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08T2I\uff09\u5f80\u5f80\u751f\u6210\u8868\u9762\u5408\u7406\u4f46\u79d1\u5b66\u4e0a\u9519\u8bef\u7684\u56fe\u50cf\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0e\u903b\u8f91\u7684\u5206\u79bb\uff0c\u5f71\u54cd\u6a21\u578b\u4e0b\u6e38\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u7814\u7a76\u4e86\u4e24\u7c7b\u79d1\u5b66\u56fe\u50cf\u751f\u6210\u65b9\u5f0f\uff1a\u57fa\u4e8e\u50cf\u7d20\u7684\u76f4\u63a5\u5408\u6210\u548c\u7a0b\u5e8f\u5316\u5408\u6210\uff0c\u5e76\u63d0\u51fa\u903b\u8f91\u9a71\u52a8\u7684ImgCoder\u6846\u67b6\uff0c\u91c7\u7528\u201c\u7406\u89e3-\u8ba1\u5212-\u7f16\u7801\u201d\u7684\u663e\u5f0f\u6d41\u7a0b\u4ee5\u63d0\u5347\u7ed3\u6784\u7cbe\u5ea6\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4ef7\u65b9\u6cd5SciGenBench\uff0c\u4ece\u4fe1\u606f\u5b9e\u7528\u6027\u4e0e\u903b\u8f91\u6709\u6548\u6027\u65b9\u9762\u5bf9\u751f\u6210\u56fe\u50cf\u8fdb\u884c\u4e25\u8c28\u8bc4\u6d4b\u3002", "result": "\u8bc4\u6d4b\u53d1\u73b0\uff0c\u57fa\u4e8e\u50cf\u7d20\u7684\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63ed\u793a\u4e86\u8868\u73b0\u529b\u4e0e\u7cbe\u51c6\u6027\u7684\u57fa\u672c\u6743\u8861\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u4e25\u683c\u9a8c\u8bc1\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u5fae\u8c03LMM\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5448\u73b0\u4e0e\u6587\u672c\u9886\u57df\u7c7b\u4f3c\u7684\u53ef\u6269\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5ea6\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u53ef\u4ee5\u4f5c\u4e3a\u63d0\u5347\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\uff0cImgCoder\u548cSciGenBench\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u4e0e\u6807\u51c6\u3002"}}
{"id": "2601.16986", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16986", "abs": "https://arxiv.org/abs/2601.16986", "authors": ["Zihan Wang", "Cheng Tang", "Lei Gong", "Cheng Li", "Chao Wang", "teng wang", "Wenqi Lou", "Xuehai Zhou"], "title": "Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Crystal-KV\u6846\u67b6\uff0c\u6709\u6548\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u65f6\u7684KV\u7f13\u5b58\uff0c\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u4e0e\u51c6\u786e\u5ea6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u590d\u6742\u63a8\u7406\u65f6\u9700\u957f\u5e8f\u5217\u652f\u6301\uff0c\u5bfc\u81f4KV\u7f13\u5b58\u6781\u5ea6\u6d88\u8017\u5185\u5b58\uff0c\u800c\u4f20\u7edfKV\u538b\u7f29\u672a\u8003\u8651CoT\u4efb\u52a1\u4e2d\u6700\u7ec8\u7b54\u6848\u6bd4\u4e2d\u95f4\u63a8\u7406\u66f4\u91cd\u8981\u7684\u7279\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e3aCoT\u8bbe\u8ba1\u4e13\u95e8\u7684KV\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCrystal-KV\uff0c\u5c06\u5173\u6ce8\u91cd\u70b9\u653e\u5728\u6700\u7ec8\u7b54\u6848\uff0c\u5176\u6838\u5fc3\u5305\u62ec\uff1a\uff081\uff09\u63d0\u51fa\u7b54\u6848\u4f18\u5148\u539f\u5219\uff0c\u901a\u8fc7\u6620\u5c04\u7b54\u6848\u504f\u597d\u4e8e\u63a8\u7406\u9636\u6bb5\u7684\u6ce8\u610f\u529b\u56fe\u533a\u5206\u53ef\u4e22\u5f03\uff08SlipKV\uff09\u548c\u5fc5\u987b\u4fdd\u7559\uff08CrystalKV\uff09\u7684\u7f13\u5b58\uff1b\uff082\uff09\u57fa\u4e8e\u6ce8\u610f\u529b\u7684LRFU\u7b97\u6cd5\u53ca\u65f6\u5254\u9664\u65e0\u6548SlipKV\u6761\u76ee\uff1b\uff083\uff09\u81ea\u9002\u5e94KV\u9884\u7b97\u5206\u914d\u7b97\u6cd5\uff0c\u6839\u636e\u5173\u952eCrystalKV\u52a8\u6001\u8c03\u6574\u6bcf\u5c42/\u5934\u5206\u914d\uff0c\u6709\u6548\u5229\u7528\u7f13\u5b58\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1Crystal-KV\u53ef\u5927\u5e45\u63d0\u5347KV\u7f13\u5b58\u538b\u7f29\u7387\uff0c\u63d0\u5347\u541e\u5410\u91cf\u53ca\u54cd\u5e94\u901f\u5ea6\uff0c\u540c\u65f6\u7b54\u6848\u51c6\u786e\u5ea6\u4e0d\u964d\u53cd\u5347\uff0c\u8868\u73b0\u4e3a\u6700\u4f18\u538b\u7f29\u6548\u679c\u3002", "conclusion": "Crystal-KV\u4e3a\u5927\u8bed\u8a00\u6a21\u578bCoT\u63a8\u7406\u573a\u666f\u5e26\u6765\u4e86\u5b58\u50a8\u4e0e\u8ba1\u7b97\u6027\u80fd\u7684\u91cd\u5927\u63d0\u5347\uff0c\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2601.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17249", "abs": "https://arxiv.org/abs/2601.17249", "authors": ["Peter Bryan", "Rejin John Varghese", "Dario Farina"], "title": "Quantifying Ergonomics in the Elevate Soft Robotic Suit", "comment": "5 pages, 3 figures. Submitted to IEEE-EMBC 2026", "summary": "Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86Elevate\u67d4\u6027\u673a\u5668\u4eba\u5916\u9aa8\u9abc\u5728\u534f\u52a9\u80a9\u90e8\u62ac\u5347\u65f6\u7684\u8212\u9002\u6027\u548c\u4eba\u4f53\u5de5\u5b66\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u7a7f\u6234\u8212\u9002\u3001\u538b\u529b\u5408\u7406\uff0c\u4e3a\u540e\u7eed\u4e34\u5e8a\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u67d4\u6027\u673a\u5668\u4eba\u670d\u5177\u6709\u5eb7\u590d\u3001\u8f85\u52a9\u548c\u589e\u5f3a\u4eba\u4f53\u529f\u80fd\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u73b0\u8212\u9002\u3001\u7528\u6237\u5b9a\u5236\u7684\u4eba\u673a\u63a5\u53e3\u8bbe\u8ba1\u65b9\u9762\u8fd8\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u5e76\u5236\u4f5c\u4e86\u4e00\u6b3e\u57fa\u4e8e\u62c9\u7d22\u9a71\u52a8\u7684Elevate\u67d4\u6027\u673a\u5668\u4eba\u5916\u9aa8\u9abc\uff0c\u7528\u4e8e\u80a9\u90e8\u62ac\u5347\u7684\u8f85\u52a9\u3002\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u548c\u529b\u4f20\u611f\u5668\uff0c\u91cf\u5316\u8bc4\u4f30\u4e86\u7a7f\u6234\u6b64\u5916\u9aa8\u9abc\u5728\u62ac\u80a9\u81f370\u5ea6\u65f6\u7684\u4eba\u4f53\u5de5\u5b66\u4e0e\u8212\u9002\u6027\u3002\u5728\u4e00\u540d\u53d7\u8bd5\u8005\u8eab\u4e0a\u8fdb\u884c\u4e86\u4e24\u6b214\u5c0f\u65f6\u7684\u6d4b\u8bd5\uff0c\u62c9\u7d22\u5f20\u529b\u6700\u5927\u8fbe200N\u3002", "result": "\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u53d7\u8bd5\u8005\u62a5\u544a\u65e0\u4e0d\u9002\u611f\u3002\u5916\u9aa8\u9abc\u5728\u52a9\u529b\u8fd0\u52a8\u4e2d\u5bf9\u80a9\u90e8\u65bd\u52a0\u7684\u538b\u529b\u572869.1-85.1kPa\uff0c\u4e0e\u6b63\u5e38\u4eba\u624b\u6293\u63e1\u538b\u529b\u8303\u56f4\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u8eaf\u5e72\u548c\u4e0a\u81c2\u7684\u4f53\u79ef\u538b\u7f29\u7387\u5206\u522b\u4f4e\u4e8e3%\u548c8%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u521d\u6b65\u9a8c\u8bc1\u4e86Elevate\u5916\u9aa8\u9abc\u7684\u4eba\u4f53\u5de5\u5b66\u8bbe\u8ba1\uff0c\u4e3a\u65e5\u540e\u5728\u5b9e\u9645\u60a3\u8005\u7fa4\u4f53\u4e2d\u5f00\u5c55\u8fdb\u4e00\u6b65\u8bd5\u9a8c\u63d0\u4f9b\u4e86\u53ef\u9760\u4f9d\u636e\u3002"}}
{"id": "2601.17031", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17031", "abs": "https://arxiv.org/abs/2601.17031", "authors": ["Yunhao Xu", "Fuquan Zong", "Yexuan Xing", "Chulong Zhang", "Guang Yang", "Shilong Yang", "Xiaokun Liang", "Juan Yu"], "title": "Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection", "comment": null, "summary": "The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u91cd\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u7a7a\u95f4\u6d41\u5f62\u548c\u8bed\u4e49\u5bf9\u8c61\u6ce8\u5165\uff0c\u5e2e\u52a9\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u66f4\u9ad8\u6548\u5730\u5229\u7528\u6709\u9650\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5728\u9ad3\u819c\u7624\u7b49\u590d\u6742\u75c5\u7406\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6548\u679c\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u6570\u636e\u6709\u9650\uff0c\u590d\u6742\u75c5\u7406\uff08\u5982\u9ad3\u819c\u7624\uff09\u5206\u5272\u5bf9\u6570\u636e\u5229\u7528\u6548\u7387\u8981\u6c42\u9ad8\uff0c\u9700\u5145\u5206\u6316\u6398\u6709\u9650\u6570\u636e\u6f5c\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u7ed3\u6784\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u589e\u5f3a\u6709\u9650\uff0c\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u589e\u5f3a\u6846\u67b6\uff1a(1) \u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u5efa\u6a21\u8fde\u7eed\u901f\u5ea6\u573a\uff0c\u901a\u8fc7\u7ebf\u6027\u6df7\u5408\u6574\u5408\u53d8\u5f62\u573a\uff0c\u5b9e\u73b0\u89e3\u5256\u7ed3\u6784\u7684\u5e7f\u6cdb\u591a\u6837\u5316\u751f\u6210\uff1b(2) \u8bbe\u8ba1Sim2Real\u75c5\u7076\u6ce8\u5165\u6a21\u5757\uff0c\u5c06\u75c5\u53d8\u7eb9\u7406\u771f\u5b9e\u690d\u5165\u5065\u5eb7\u89e3\u5256\u80cc\u666f\uff0c\u5b9e\u73b0\u9ad8\u903c\u771f\u5ea6\u7684\u589e\u5f3a\uff0c\u5e76\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\uff08\u5982nnU-Net\uff0cU-Mamba\uff09\u5728\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u7684\u9ad8\u6027\u80fd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u75c5\u53d8\u7684\u7cbe\u786e\u5206\u5272\u3002"}}
{"id": "2601.16987", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16987", "abs": "https://arxiv.org/abs/2601.16987", "authors": ["Shunyang Luo", "Peibei Cao", "Zhihui Zhu", "Kehua Feng", "Zhihua Wang", "Keyan Ding"], "title": "Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions", "comment": "17 pages, 6 figures, 2 tables", "summary": "Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u6cdb\u5316\u8bc4\u4ef7\u6846\u67b6PMDC\uff0c\u901a\u8fc7\u52a8\u6001\u7b5b\u9009\u5206\u6b67\u6700\u5927\u7684\u6837\u672c\u5bf9\uff0c\u66f4\u6709\u6548\u8bc4\u4f30RM\u5728\u5f00\u653e\u57df\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u6d4b\u4f4e\u4f30\u4e86RM\u95f4\u7684\u5b9e\u9645\u5206\u6b67\u3002", "motivation": "\u73b0\u6709RM\u8bc4\u6d4b\u4f9d\u8d56\u9759\u6001\u6709\u6807\u6ce8\u6570\u636e\uff0c\u8986\u76d6\u6709\u9650\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5bf9\u65b0\u9896\u63d0\u793a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e0d\u80fd\u6709\u6548\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPairwise Maximum Discrepancy Competition\uff08PMDC\uff09\u6846\u67b6\uff0c\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u5f00\u653e\u57df\u63d0\u95ee\u6c60\u4e2d\u4e3b\u52a8\u6311\u9009\u4f7f\u4e24\u4e2aRM\u610f\u89c1\u5206\u6b67\u6700\u5927\u7684\u95ee\u7b54\u5bf9\u3002\u4eba\u5de5\uff08oracle\uff09\u5bf9\u8fd9\u4e9b\u9ad8\u4e89\u8bae\u6837\u672c\u88c1\u51b3\uff0c\u91c7\u7528Bradley--Terry\u6a21\u578b\u5bf9\u7ed3\u679c\u805a\u5408\uff0c\u5f97\u5230\u5168\u5c40\u6392\u540d\u548cRM\u95f4\u7684\u5bf9\u6297\u683c\u5c40\u3002", "result": "\u5bf910\u4e2a\u4ee3\u8868\u6027RM\u7528PMDC\u91cd\u65b0\u8bc4\u4f30\uff0c\u6392\u540d\u4e0e\u5e38\u89c4\u57fa\u51c6\u5927\u5e45\u53d8\u5316\uff0c\u53d1\u73b0\u5728\u66f4\u771f\u5b9e\u5206\u5e03\u4e0bRM\u6cdb\u5316\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002\u5b9a\u6027\u5206\u6790\u8fd8\u63ed\u793a\u4e86RM\u6cdb\u5316\u7cfb\u7edf\u6027\u95ee\u9898\u3002", "conclusion": "PMDC\u6846\u67b6\u80fd\u66f4\u5168\u9762\u5730\u63ed\u793a\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u77ed\u677f\uff0c\u5bf9\u63d0\u5347RM\u5b9e\u9645\u53ef\u9760\u6027\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2601.17251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17251", "abs": "https://arxiv.org/abs/2601.17251", "authors": ["Yunuo Chen", "Yafei Hu", "Lingfeng Sun", "Tushar Kusnur", "Laura Herlant", "Chenfanfu Jiang"], "title": "EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects", "comment": null, "summary": "Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206Material Point Method (MPM)\u6a21\u62df\u5668\u7684\u53d8\u5f62\u7269\u4f53\u5efa\u6a21\u4e0e\u4eff\u771f\u6846\u67b6EMPM\uff0c\u53ef\u9ad8\u6548\u3001\u7269\u7406\u771f\u5b9e\u5730\u6a21\u62df\u590d\u6742\u6750\u6599\u52a8\u6001\uff0c\u5e76\u5728\u89c6\u89c9\u6570\u636e\u9a71\u52a8\u4e0b\u5b9e\u73b0\u5728\u7ebf\u81ea\u9002\u5e94\u4f18\u5316\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u4f20\u7edf\u5f39\u7c27-\u8d28\u70b9\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5bf9\u53d8\u5f62\u7269\u4f53\u7279\u522b\u662f\u8fde\u7eed\u4ecb\u8d28\u7684\u5efa\u6a21\u5b58\u5728\u7269\u7406\u6027\u4e0d\u5f3a\u3001\u6cdb\u5316\u6027\u5dee\u3001\u6570\u636e\u9700\u6c42\u5927\u7b49\u95ee\u9898\u3002\u8bb8\u591a\u65b9\u6cd5\u6216\u5bf9\u590d\u6742\u52a8\u6001\u8fc7\u4e8e\u7b80\u5316\uff0c\u6216\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u5f71\u54cd\u5b9e\u9645\u6cdb\u5316\u548c\u843d\u5730\u5e94\u7528\u3002", "method": "\u63d0\u51faEMPM\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5faeMPM\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u4ece\u591a\u89c6\u56feRGB-D\u89c6\u9891\u91cd\u5efa\u7269\u4f53\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u5e76\u6700\u5c0f\u5316\u9884\u6d4b\u4e0e\u89c2\u6d4b\u6570\u636e\u4e4b\u95f4\u7684\u89c6\u89c9\u8bef\u5dee\uff0c\u5728\u7ebf\u4f18\u5316MPM\u53c2\u6570\uff0c\u4f7f\u5efa\u6a21\u80fd\u591f\u6839\u636e\u611f\u77e5\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "EMPM\u5728\u5b9e\u9a8c\u4e2d\u53ef\u6355\u6349\u590d\u6742\u6750\u6599\u52a8\u6001\uff0c\u5177\u6709\u81ea\u9002\u5e94\u3001\u5065\u58ee\u548c\u7269\u7406\u77e5\u8bc6\u611f\u77e5\u7b49\u4f18\u52bf\uff0c\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u5f39\u7c27-\u8d28\u70b9\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "EMPM\u4e3a\u53d8\u5f62\u7269\u4f53\u7269\u7406\u771f\u5b9e\u3001\u6cdb\u5316\u5f3a\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6848\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u590d\u6742\u53ef\u53d8\u5f62\u4f53\u62d3\u5c55\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2601.17032", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17032", "abs": "https://arxiv.org/abs/2601.17032", "authors": ["Wilkie Delgado-Font", "Miriela Escobedo-Nicot", "Manuel Gonz\u00e1lez-Hidalgo", "Silena Herold-Garcia", "Antoni Jaume-i-Cap\u00f3", "Arnau Mir"], "title": "Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images", "comment": null, "summary": "Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u5206\u6790\u7684\u81ea\u52a8\u5316\u7ea2\u7ec6\u80de(RBC)\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f85\u52a9\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7b49\u75be\u75c5\u7684\u8bca\u65ad\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u51c6\u786e\u7387\u9ad8\u3002", "motivation": "\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7b49\u75be\u75c5\u4f1a\u5bfc\u81f4\u7ea2\u7ec6\u80de\u53d8\u5f62\uff0c\u76ee\u524d\u8bca\u65ad\u4e0e\u76d1\u63a7\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u663e\u5fae\u955c\u89c2\u5bdf\uff0c\u8d39\u65f6\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u8bef\u5dee\u7387\u9ad8\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u5ba2\u89c2\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u5347\u8bca\u65ad\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528Chan-Vese\u6d3b\u52a8\u8f6e\u5ed3\u6a21\u578b\u5bf9\u8840\u6d82\u7247\u56fe\u50cf\u4e2d\u7684\u7ea2\u7ec6\u80de\u8fdb\u884c\u5206\u5272\uff0c\u7ed3\u5408\u5706\u5f62\u56e0\u5b50(CSF)\u548c\u692d\u5706\u5f62\u56e0\u5b50(ESF)\u7b49\u5f62\u72b6\u5206\u6790\u53c2\u6570\uff0c\u5bf9\u7ea2\u7ec6\u80de\u8fdb\u884c\u6b63\u5e38\u3001\u62c9\u957f\u6216\u5176\u4ed6\u53d8\u5f62\u7684\u5206\u7c7b\u3002\u5bf9\u4e8e\u90e8\u5206\u91cd\u53e0\u7684\u7ea2\u7ec6\u80de\uff0c\u91c7\u7528\u692d\u5706\u62df\u5408\u5206\u6790\u4ee5\u63d0\u9ad8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u8bca\u65ad\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u529e\u6cd5\uff0c\u6b63\u5e38\u7ea2\u7ec6\u80deF\u503c\u4e3a0.97\uff0c\u62c9\u957f\u7c7b\u4e3a0.95\uff0c\u591a\u5206\u7c7b\u8868\u73b0\u4e5f\u4f18\u5f02\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u8f85\u52a9\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u652f\u6301\uff0c\u63d0\u9ad8\u8bca\u65ad\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.16999", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16999", "abs": "https://arxiv.org/abs/2601.16999", "authors": ["Matthew Singer", "Srijan Sengupta", "Karl Pazdernik"], "title": "Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction", "comment": null, "summary": "Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u5e8f\u5217\u6807\u6ce8\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6a21\u578b\u751f\u6210\u5177\u5907\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u9884\u6d4b\u96c6\u7684\u65b0\u6846\u67b6\u3002\u9884\u6d4b\u96c6\u80fd\u4ee5\u6307\u5b9a\u7f6e\u4fe1\u5ea6\u5305\u542b\u6b63\u786e\u6807\u6ce8\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u6027\u4fdd\u8bc1\u3002\u65b9\u6cd5\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\uff0c\u5b9e\u73b0\u5728\u591a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u4e0e\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524dNER\u6a21\u578b\u53ea\u8f93\u51fa\u5355\u4e00\u6807\u7b7e\u5e8f\u5217\uff0c\u672a\u80fd\u8861\u91cf\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u4e0b\u6e38\u6a21\u5757\u5bb9\u6613\u53d7\u5355\u70b9\u9519\u8bef\u5f71\u54cd\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u4fdd\u969c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5171\u5f62\u9884\u6d4b\u7406\u8bba\uff0c\u4e3a\u73b0\u6709\u5e8f\u5217\u6807\u6ce8\u7c7bNER\u6a21\u578b\u9002\u914d\u201c\u9884\u6d4b\u96c6\u201d\u8f93\u51fa\u2014\u2014\u5373\u4e00\u7ec4\u5b8c\u6574\u53e5\u5b50\u7684\u6807\u7b7e\u5e8f\u5217\uff0c\u5176\u5305\u542b\u6b63\u786e\u6807\u7b7e\u5e8f\u5217\u7f6e\u4fe1\u5ea6\u7531\u7528\u6237\u6307\u5b9a\u3002\u8bbe\u8ba1\u9ad8\u6548\u7684\u975e\u4e00\u81f4\u6027\u8bc4\u5206\u51fd\u6570\uff0c\u652f\u6301\u65e0\u6761\u4ef6\u4e0e\u7c7b\u522b\u6761\u4ef6\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u8986\u76d6\uff0c\u5e76\u517c\u987e\u53e5\u957f\u3001\u8bed\u8a00\u3001\u5b9e\u4f53\u7c7b\u578b\u7b49\u5f02\u8d28\u6027\u5f71\u54cd\u3002", "result": "\u5728\u56db\u79cdNER\u6a21\u578b\u548c\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u9002\u7528\u6027\u3001\u7f6e\u4fe1\u5ea6\u8986\u76d6\u6709\u6548\u6027\u53ca\u9884\u6d4b\u96c6\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u826f\u3002", "conclusion": "\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86NER\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u6548\u4e3a\u4e0b\u6e38\u4efb\u52a1\u51cf\u5c11\u9519\u8bef\u4f20\u9012\u98ce\u9669\uff0c\u5e76\u5df2\u9a8c\u8bc1\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u80fd\u3002"}}
{"id": "2601.17287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17287", "abs": "https://arxiv.org/abs/2601.17287", "authors": ["Yanrong Chen", "Xihan Bian"], "title": "Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots", "comment": "6 pages, 6 figures", "summary": "As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eNAO\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u8bed\u97f3\u97f5\u5f8b\u4e0e\u5168\u8eab\u59ff\u6001\u5b9e\u65f6\u60c5\u611f\u540c\u6b65\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5728\u591a\u79cd\u670d\u52a1\u573a\u666f\u4e0b\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u60c5\u611f\u4e92\u52a8\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u578b\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u6a21\u6001\u60c5\u611f\u540c\u6b65\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u670d\u52a1\u3001\u6559\u80b2\u7b49\u573a\u666f\u4e0b\u7684\u5e94\u7528\u4e0e\u63a5\u53d7\u5ea6\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u8bed\u97f3\u8bed\u8c03\u4e0e\u8eab\u4f53\u52a8\u4f5c\u540c\u6b65\u534f\u540c\u8868\u793a\u60c5\u7eea\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a\uff081\uff09\u53cc\u901a\u9053\u60c5\u611f\u5f15\u64ce\uff0c\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u540c\u65f6\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6587\u672c\u548c\u5177\u5907\u751f\u7269\u529b\u5b66\u53ef\u884c\u6027\u7684\u52a8\u4f5c\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u5173\u8282\u52a8\u4f5c\u5e93\u7ea6\u675f\uff1b\uff082\uff09\u57fa\u4e8e\u6301\u7eed\u65f6\u957f\u611f\u77e5\u7684\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u7b97\u6cd5\uff0c\u786e\u4fdd\u8bed\u97f3\u8f93\u51fa\u4e0e\u8fd0\u52a8\u5173\u952e\u5e27\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\uff1b\uff083\uff09\u95ed\u73af\u53ef\u884c\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u5b9e\u65f6\u9002\u5e94\u52a8\u4f5c\u4ee5\u4fdd\u8bc1\u673a\u5668\u4eba\u7269\u7406\u5173\u8282\u9650\u5236\u5185\u7684\u52a8\u4f5c\u5b89\u5168\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u7684\u60c5\u611f\u540c\u6b65\u6c34\u5e73\u8f83\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u63d0\u5347\u4e8621%\uff0c\u7279\u522b\u662f\u5728\u534f\u8c03\u8bed\u97f3\u97f3\u9ad8\u4e0e\u624b\u81c2\u4e0a\u80a2\u52a8\u4f5c\u3001\u540c\u65f6\u4fdd\u6301\u4e0b\u80a2\u7a33\u5b9a\u6027\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u672c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u578b\u793e\u4ea4\u673a\u5668\u4eba\u5728\u591a\u53d8\u5e94\u7528\u573a\u666f\u4e0b\u7684\u60c5\u611f\u4e92\u52a8\u548c\u4f20\u9012\u80fd\u529b\uff0c\u4e3a\u5176\u5728\u4e2a\u6027\u5316\u5065\u5eb7\u7167\u62a4\u3001\u4ea4\u4e92\u5f0f\u6559\u80b2\u3001\u54cd\u5e94\u5f0f\u5ba2\u6237\u670d\u52a1\u7b49\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u6253\u4e0b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17037", "abs": "https://arxiv.org/abs/2601.17037", "authors": ["Aahana Basappa", "Pranay Goel", "Anusri Karra", "Anish Karra", "Asa Gilmore", "Kevin Zhu"], "title": "AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs", "comment": "Comments: 13 pages, 4 figures. Presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: NeurIPS 2025 VLM4RWD. Authors Aahana Basappa and Pranay Goel contributed equally to this work. Code: https://github.com/AahanaB24/AMVICC, Data: https://doi.org/10.5281/zenodo.17646068", "summary": "We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \\textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AMVICC\u57fa\u51c6\uff0c\u5bf9\u6bd4\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u5728\u89c6\u89c9\u63a8\u7406\u4e0a\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u8bc4\u4ef7\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u7c7b\u6a21\u578b\u5b58\u5728\u5171\u540c\u4ee5\u53ca\u7279\u6709\u7684\u5931\u8d25\u70b9\uff0c\u5c24\u5176IGMs\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u64cd\u63a7\u4e0a\u8868\u73b0\u8f83\u5f31\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u57fa\u672c\u89c6\u89c9\u63a8\u7406\u5982\u7269\u4f53\u671d\u5411\u3001\u6570\u91cf\u4e0e\u7a7a\u95f4\u5173\u7cfb\u7b49\u65b9\u9762\u4ecd\u5e38\u51fa\u73b0\u7406\u89e3\u548c\u751f\u6210\u9519\u8bef\uff0c\u8868\u660e\u8fd9\u7c7b\u6a21\u578b\u5728\u57fa\u7840\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u77ed\u677f\u3002", "method": "\u4f5c\u8005\u5c06MMVP\u57fa\u51c6\u95ee\u9898\u8c03\u6574\u4e3a\u663e\u5f0f\u548c\u9690\u5f0f\u63d0\u793a\uff0c\u6784\u5efa\u65b0\u7684AMVICC\u57fa\u51c6\uff0c\u5e76\u4ee5\u6b64\u6d4b\u8bd511\u4e2aMLLMs\u548c3\u4e2aIGMs\u5728\u4e5d\u7c7b\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u7cfb\u7edf\u6027\u6bd4\u8f83\u5176\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u548c\u6a21\u6001\u95f4\u5b58\u5728\u5171\u6027\u548c\u7279\u6709\u7684\u5931\u8d25\u6a21\u5f0f\uff0cIGMs\u5728\u5bf9\u63d0\u793a\u4e2d\u5177\u4f53\u89c6\u89c9\u7ec6\u8282\u7684\u64cd\u63a7\u4e0a\u8868\u73b0\u5c24\u4e3a\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u663e\u5f0f\u63d0\u793a\u4e0b\u53cd\u5e94\u8f83\u5dee\u3002", "conclusion": "AMVICC\u4e3a\u540e\u7eed\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u751f\u6210\u548c\u7406\u89e3\u4e2d\u5931\u8d25\u7684\u5171\u6027\u6839\u6e90\uff0c\u4e3a\u672a\u6765\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2601.17002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17002", "abs": "https://arxiv.org/abs/2601.17002", "authors": ["Ziyang Zhou", "Ziqi Liu", "Yan Wang", "Yiming Lin", "Yangbin Chen"], "title": "RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection", "comment": "12 pages, 4 figures, 6 tables, preprint", "summary": "Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAM-SD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u5347\u4e86\u53cd\u8bbd\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u53d6\u5f97\u4e86\u65b0SOTA\u6210\u7ee9\uff0c\u5e76\u5177\u5907\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u53cd\u8bbd\u68c0\u6d4b\u4f9d\u8d56\u590d\u6742\u8bed\u5883\u3001\u5e38\u8bc6\u548c\u591a\u7ef4\u5ea6\u8bed\u8a00\u7279\u5f81\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u4e0d\u540c\u53cd\u8bbd\u7c7b\u578b\u65f6\u4ec5\u7528\u4e00\u5957\u63a8\u7406\u7b56\u7565\uff0c\u96be\u4ee5\u7075\u6d3b\u5e94\u5bf9\u591a\u6837\u5206\u6790\u9700\u6c42\uff0c\u5bfc\u81f4\u6548\u679c\u53d7\u9650\u3002", "method": "RAM-SD\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\uff081\uff09\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u901a\u8fc7\u7c7b\u4f3c\u4e0e\u975e\u7c7b\u4f3c\u793a\u4f8b\u8fdb\u884c\u8bed\u5883\u951a\u5b9a\uff1b\uff082\uff09\u5143\u89c4\u5212\u5668\u5224\u65ad\u53cd\u8bbd\u7c7b\u578b\u5e76\u9009\u62e9\u6700\u4f18\u63a8\u7406\u8def\u5f84\uff1b\uff083\uff09\u591a\u667a\u80fd\u4f53\u4ece\u4e0d\u540c\u89c6\u89d2\u534f\u4f5c\u5206\u6790\uff1b\uff084\uff09\u96c6\u6210\u5668\u5c06\u591a\u667a\u80fd\u4f53\u5206\u6790\u7ed3\u679c\u6574\u5408\uff0c\u7ed9\u51fa\u6700\u7ec8\u53ef\u89e3\u91ca\u5224\u51b3\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6837\u5316\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\uff0c\u6709\u6548\u5e94\u5bf9\u53cd\u8bbd\u591a\u6837\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u53cd\u8bbd\u68c0\u6d4b\u57fa\u51c6\u4e0a\uff0cRAM-SD\u6846\u67b6\u5b9e\u73b0\u4e8677.74%\u7684Macro-F1\uff0c\u8f83GPT-4o+CoC\u57fa\u7ebf\u63d0\u53477.01\u5206\uff0c\u6253\u7834\u73b0\u6709\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "RAM-SD\u4e0d\u4ec5\u5728\u51c6\u786e\u7387\u4e0a\u5b9e\u73b0\u7a81\u7834\uff0c\u8fd8\u80fd\u8f93\u51fa\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u53cd\u8bbd\u8bc6\u522b\u80cc\u540e\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u4e3a\u5b9e\u9645\u843d\u5730\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u900f\u660e\u6027\u548c\u4fe1\u4efb\u57fa\u7840\u3002"}}
{"id": "2601.17404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17404", "abs": "https://arxiv.org/abs/2601.17404", "authors": ["Anke Fischer-Janzen", "Thomas M. Wendt", "Kristof Van Laerhoven"], "title": "Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms", "comment": "23 pages, 6 figures, publication in review process", "summary": "Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u4eba\u673a\u534f\u4f5c\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u4e25\u91cd\u80a2\u4f53\u6b8b\u75be\u8005\u80fd\u591f\u901a\u8fc7\u6ce8\u89c6\u64cd\u4f5c\u673a\u5668\u4eba\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u3002\u7cfb\u7edf\u7ed3\u5408\u4efb\u52a1\u56fe\u6807\u548c\u7279\u5f81\u5339\u914d\uff0c\u65e0\u9700\u5df2\u77e5\u7528\u6237\u4e0e\u7269\u4f53\u4f4d\u7f6e\u5173\u7cfb\uff0c\u5bf9\u76ee\u6807\u5bf9\u8c61\u4e0e\u4efb\u52a1\u7684\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\u8fbe97.9%\u3002\u5f00\u6e90\u6846\u67b6\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u57283D\u6ce8\u89c6\u70b9\u7cbe\u5ea6\u4e0e\u591a\u4efb\u52a1\u533a\u5206\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u4eba\u673a\u534f\u4f5c\uff0c\u5c24\u5176\u662f\u8f85\u52a9\u8eab\u4f53\u6b8b\u969c\u4eba\u7fa4\u4e2d\u7684\u5e94\u7528\u3002\u4f5c\u8005\u81f4\u529b\u4e8e\u4e3a\u4e25\u91cd\u80a2\u4f53\u6b8b\u75be\u8005\u63d0\u4f9b\u66f4\u72ec\u7acb\u7684\u4efb\u52a1\u64cd\u4f5c\u80fd\u529b\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u81ea\u9002\u5e94\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u4ee5\u773c\u52a8\u8ffd\u8e2a\u4e3a\u6838\u5fc3\uff0c\u7528\u6237\u901a\u8fc7\u6ce8\u89c6\u5e26\u6709\u56fe\u6807\u7684\u4efb\u52a1\u76ee\u6807\uff0c\u7cfb\u7edf\u5229\u7528\u56fe\u6807\u4f5c\u4e3a\u89c6\u89c9\u6807\u8bb0\u7ed3\u5408\u7279\u5f81\u5339\u914d\u7b97\u6cd5\uff0c\u5728eye-in-hand\uff08\u673a\u5668\u4eba\u7aef\u643a\u5e26\u89c6\u89c9\u7cfb\u7edf\uff09\u914d\u7f6e\u4e0b\u5b8c\u6210\u5bf9\u8c61\u8bc6\u522b\u4e0e\u4efb\u52a1\u5224\u522b\u3002\u6846\u67b6\u96c6\u6210\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u652f\u6301\u65b0\u4efb\u52a1\u4e0e\u65b0\u5bf9\u8c61\u7684\u6269\u5c55\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4efb\u52a1\u4e0e\u5bf9\u8c61\u7684\u8bc6\u522b\u4e0a\u51c6\u786e\u7387\u9ad8\u8fbe97.9%\u3002\u5728\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u5e76\u5b8c\u5584\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u548c\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u5171\u4eab\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u6613\u6269\u5c55\u3001\u9488\u5bf9\u6b8b\u75be\u4eba\u58eb\u53cb\u597d\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4e14\u7cfb\u7edf\u4e3a\u5f00\u6e90\uff0c\u53ef\u9002\u914d\u5230\u591a\u79cd\u4efb\u52a1\u548c\u5bf9\u8c61\uff0c\u5bf9\u8f85\u52a9\u6280\u672f\u548c\u4eba\u673a\u4ea4\u4e92\u5177\u6709\u73b0\u5b9e\u610f\u4e49\u548c\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2601.17038", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17038", "abs": "https://arxiv.org/abs/2601.17038", "authors": ["Obai Alashram", "Nejad Alagha", "Mahmoud AlKakuri", "Zeeshan Swaveel", "Abigail Copiaco"], "title": "Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification", "comment": null, "summary": "The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u7ba1\u9053\uff0c\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u65bd\u5de5\u4e0e\u62c6\u9664\u5783\u573e\u7684\u81ea\u52a8\u5316\u5206\u7c7b\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u4ea7\u751f\u5927\u91cf\u5783\u573e\uff0c\u5982\u4f55\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u5730\u5206\u7c7b\u7ba1\u7406\u8fd9\u4e9b\u5e9f\u5f03\u7269\u5bf9\u4e8e\u8d44\u6e90\u518d\u5229\u7528\u548c\u5783\u573e\u5904\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e861800\u5f20\u6765\u81ea\u771f\u5b9e\u5de5\u5730\u3001\u6db5\u76d64\u7c7b\u6750\u6599\u7684\u9ad8\u8d28\u91cf\u56fe\u7247\uff0c\u5e76\u91c7\u7528Xception\u7f51\u7edc\u8fdb\u884c\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u518d\u7ed3\u5408SVM\u3001kNN\u3001Bagged Trees\u7b49\u591a\u79cd\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7cfb\u7edf\u6027\u8bc4\u4f30\u5206\u7c7b\u8868\u73b0\u3002", "result": "\u7ed3\u5408Xception\u7279\u5f81\u4e0e\u7b80\u5355\u53cc\u673a\u5b66\u4e60\u5206\u7c7b\u5668\uff08\u5982Linear SVM\u3001kNN\u3001Bagged Trees\uff09\u53ef\u5f97\u5230\u9ad8\u8fbe99.5%\u7684\u51c6\u786e\u7387\u548cmacro-F1\u5206\u6570\uff0c\u8d85\u8d8a\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u7aef\u5230\u7aef\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u5b9e\u73b0\u73b0\u573a\u5783\u573e\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u4e0e\u5b9e\u7528\u6027\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u5e76\u4e3a\u672a\u6765\u4e0e\u673a\u5668\u4eba\u53ca\u73b0\u573a\u81ea\u52a8\u5316\u7cfb\u7edf\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2601.17132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17132", "abs": "https://arxiv.org/abs/2601.17132", "authors": ["Vigneshwaran Shankaran", "Gabriella Lapesa", "Claudia Wagner"], "title": "From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech", "comment": "Paper accepted to EACL Mains 2026", "summary": "Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears \"civiler\" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u68b3\u7406\u548c\u6574\u5408\u4e86\u5bf9\u201c\u6050\u60e7\u8a00\u8bba\u201d\u7684\u8de8\u5b66\u79d1\u7406\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u6050\u60e7\u8a00\u8bba\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u867d\u7136\u6050\u60e7\u4f5c\u4e3a\u4e00\u79cd\u9a71\u52a8\u793e\u4f1a\u4e0e\u8a00\u8bba\u7684\u529b\u91cf\u975e\u5e38\u5f3a\u5927\uff0c\u6050\u60e7\u8a00\u8bba\u5185\u5bb9\u5e7f\u6cdb\u4e14\u4f20\u64ad\u529b\u5f3a\uff0c\u4f46\u8be5\u73b0\u8c61\u5728\u8ba1\u7b97\u8bed\u8a00\u5b66\u9886\u57df\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u3001\u8de8\u5b66\u79d1\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6050\u60e7\u8a00\u8bba\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u68b3\u7406\u3002", "method": "\u4f5c\u8005\u5bf9\u5fc3\u7406\u5b66\u3001\u653f\u6cbb\u5b66\u3001\u4f20\u64ad\u5b66\u548c\u8bed\u8a00\u5b66\u4e2d\u7684\u6050\u60e7\u7406\u8bba\u8fdb\u884c\u5bf9\u6bd4\uff0c\u7cfb\u7edf\u56de\u987e\u65e2\u6709\u5b9a\u4e49\uff0c\u8c03\u7814\u76f8\u5173\u9886\u57df\u7684\u6570\u636e\u96c6\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u4e0d\u540c\u7ef4\u5ea6\u7684\u6050\u60e7\u8a00\u8bba\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u5f52\u7eb3\u603b\u7ed3\u4e86\u6050\u60e7\u8a00\u8bba\u7684\u7406\u8bba\u80cc\u666f\u3001\u6570\u636e\u96c6\u60c5\u51b5\u548c\u5b9a\u4e49\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fbf\u4e8e\u672a\u6765\u7814\u7a76\u7684\u6570\u636e\u96c6\u4e0e\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a\u5efa\u7acb\u66f4\u901a\u7528\u548c\u7cfb\u7edf\u7684\u6050\u60e7\u8a00\u8bba\u7814\u7a76\u6846\u67b6\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6050\u60e7\u8a00\u8bba\u76f8\u5173\u6570\u636e\u96c6\u7684\u521b\u5efa\u548c\u7814\u7a76\u7684\u6df1\u5165\u53d1\u5c55\u3002"}}
{"id": "2601.17412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17412", "abs": "https://arxiv.org/abs/2601.17412", "authors": ["Valerii Serpiva", "Artem Lykov", "Jeffrin Sam", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "DiffusionCinema: Text-to-Aerial Cinematography", "comment": null, "summary": "We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., \"orbit around me slowly from the right and reveal the background waterfall\"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the \"creative operator\" converting story intentions directly into aerial motion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u8f85\u52a9\u62cd\u6444\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6240\u9700\u955c\u5934\uff0c\u7cfb\u7edf\u5229\u7528\u6269\u6563\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6700\u4f73\u822a\u62cd\u8f68\u8ff9\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u7535\u5f71\u7ea7\u89c6\u9891\u5f55\u5236\u3002", "motivation": "\u7528\u6237\u624b\u52a8\u64cd\u4f5c\u65e0\u4eba\u673a\u8fdb\u884c\u590d\u6742\u3001\u5177\u5907\u827a\u672f\u6548\u679c\u7684\u822a\u62cd\u5b58\u5728\u9ad8\u5b66\u4e60\u95e8\u69db\u4e0e\u64cd\u4f5c\u8d1f\u62c5\uff0c\u7f3a\u5c11\u4fbf\u6377\u7684\u521b\u610f\u8868\u8fbe\u65b9\u5f0f\u3002\u672c\u6587\u65e8\u5728\u7528\u81ea\u7136\u8bed\u8a00\u7b80\u5316\u65e0\u4eba\u673a\u822a\u62cd\u6d41\u7a0b\uff0c\u8ba9\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u9ad8\u6548\u83b7\u5f97\u7406\u60f3\u955c\u5934\u3002", "method": "\u7cfb\u7edf\u5c06\u7528\u6237\u7684\u9ad8\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u65e0\u4eba\u673a\u6444\u50cf\u5934\u521d\u59cb\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u7531\u6269\u6563\u6a21\u578b\u91c7\u6837\u3001\u751f\u6210\u7b26\u5408\u573a\u666f\u51e0\u4f55\u4e0e\u955c\u5934\u8bed\u4e49\u7684\u65f6\u7a7a\u8fd0\u52a8\u8f68\u8ff9\uff0c\u7136\u540e\u65e0\u4eba\u673a\u6309\u6b64\u8f68\u8ff9\u81ea\u4e3b\u98de\u884c\u5b8c\u6210\u62cd\u6444\u3002\u540c\u65f6\u91c7\u7528NASA-TLX\u91cf\u8868\u4e0e\u4f20\u7edf\u9065\u63a7\u5668\u64cd\u4f5c\u65b9\u5f0f\u505a\u5bf9\u6bd4\u8bc4\u6d4b\u3002", "result": "\u65b0\u7cfb\u7edf\u7684NASA-TLX\u4efb\u52a1\u8d1f\u8377\u603b\u5206\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u9065\u63a7\u5668\uff0821.6 vs 58.1\uff09\uff0c\u5c24\u5176\u4f53\u73b0\u5728\u5fc3\u7406\u8d1f\u62c5\u53ca\u632b\u8d25\u611f\u663e\u8457\u4e0b\u964d\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u9a71\u52a8\u81ea\u6cbb\u98de\u884c\u7684\u4f7f\u7528\u4fbf\u5229\u6027\u4e0e\u7528\u6237\u4f53\u9a8c\u63d0\u5347\u3002", "conclusion": "\u6587\u672c\u5230\u822a\u62cd\u7684\u98de\u63a7\u65b0\u8303\u5f0f\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u201c\u521b\u610f\u64cd\u4f5c\u8005\u201d\u6865\u6881\uff0c\u76f4\u63a5\u5c06\u6545\u4e8b\u610f\u56fe\u8f6c\u5316\u4e3a\u822a\u62cd\u52a8\u4f5c\uff0c\u5b9e\u73b0\u4e86\u7b80\u5355\u3001\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u65e0\u4eba\u673a\u7535\u5f71\u5236\u4f5c\uff0c\u4e3a\u667a\u80fd\u822a\u62cd\u548c\u4eba\u673a\u4ea4\u4e92\u62d3\u5c55\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17039", "abs": "https://arxiv.org/abs/2601.17039", "authors": ["Junhyuk Heo", "Beomkyu Choi", "Hyunjin Shin", "Darongsae Kwon"], "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation", "comment": null, "summary": "Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MANGO\uff0c\u4e00\u4e2a\u6db5\u76d6\u5168\u7403124\u4e2a\u56fd\u5bb6\u3001\u5305\u542b42,703\u5bf9\u6807\u6ce8\u56fe\u50cf\u4e0e\u63a9\u819c\u5bf9\u7684\u5927\u89c4\u6a21\u7ea2\u6811\u6797\u9065\u611f\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u5206\u5272\u7b97\u6cd5\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7ea2\u6811\u6797\u9065\u611f\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\uff1a1\uff09\u591a\u4e3a\u5e74\u5c3a\u5ea6\u4ea7\u54c1\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5355\u65e5\u5f71\u50cf-\u63a9\u819c\u914d\u5bf9\uff1b2\uff09\u5730\u57df\u8986\u76d6\u6709\u9650\uff0c\u7f3a\u4e4f\u5168\u7403\u6027\u6570\u636e\uff1b3\uff09\u96be\u4ee5\u516c\u5f00\u83b7\u53d6\u3002\u56e0\u6b64\uff0c\u6025\u9700\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u5168\u7403\u5206\u5e03\u7684\u6570\u636e\u96c6\u63a8\u52a8\u81ea\u52a8\u5316\u7ea2\u6811\u6797\u76d1\u6d4b\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u5229\u75282020\u5e74Sentinel-2\u7684\u5168\u90e8\u53ef\u7528\u5f71\u50cf\uff0c\u5bf9\u5168\u7403\u7ea2\u6811\u6797\u5206\u5e03\u533a\u63d0\u53d6\u6700\u4f73\u5355\u65f6\u76f8\u5f71\u50cf\uff0c\u5e76\u7ed3\u5408\u5e74\u5ea6\u7ea2\u6811\u6797\u63a9\u819c\u3002\u91c7\u7528\u76ee\u6807\u68c0\u6d4b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u50cf\u7d20\u7ea7\u5750\u6807\u786e\u4fdd\u5f71\u50cf\u4e0e\u63a9\u819c\u7cbe\u51c6\u5339\u914d\uff0c\u6700\u7ec8\u83b7\u5f97\u9ad8\u8d28\u91cf\u56fe\u50cf-\u63a9\u819c\u5bf9\u3002\u6570\u636e\u96c6\u5728\u56fd\u5bb6\u65e0\u91cd\u53e0\u6761\u4ef6\u4e0b\u8bc4\u6d4b\u4e86\u591a\u79cd\u5206\u5272\u67b6\u6784\u3002", "result": "MANGO\u6570\u636e\u96c6\u6807\u6ce8\u4e8642,703\u5bf9\u6765\u81ea\u5168\u7403124\u56fd\u7684\u7ea2\u6811\u6797\u9065\u611f\u5f71\u50cf\uff0c\u5e76\u901a\u8fc7\u56fd\u5bb6\u7ea7\u72ec\u7acb\u5212\u5206\uff0c\u7ed9\u4e3b\u6d41\u8bed\u4e49\u5206\u5272\u67b6\u6784\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6027\u80fd\u57fa\u7ebf\u3002", "conclusion": "MANGO\u5f00\u521b\u4e86\u5168\u7403\u5927\u89c4\u6a21\u3001\u516c\u5f00\u53ef\u7528\u7684\u7ea2\u6811\u6797\u9065\u611f\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u7403\u7ea2\u6811\u6797\u667a\u80fd\u76d1\u6d4b\u7684\u57fa\u7840\uff0c\u4e3a\u76f8\u5173\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u548c\u7814\u7a76\u5e73\u53f0\u3002"}}
{"id": "2601.17152", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17152", "abs": "https://arxiv.org/abs/2601.17152", "authors": ["Miao Zhang", "Junsik Kim", "Siyuan Xiang", "Jian Gao", "Cheng Cao"], "title": "Dynamic Role Assignment for Multi-Agent Debate", "comment": null, "summary": "Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u89d2\u8272\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8fa9\u8bba\u673a\u5236\u4e3a\u591a\u667a\u80fd\u4f53\u5927\u6a21\u578b\u7cfb\u7edf\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\u62c5\u5f53\u7279\u5b9a\u89d2\u8272\uff0c\u4ece\u800c\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u57fa\u51c6\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u7edf\u4e00\u5206\u914d\u548c\u968f\u673a\u5206\u914d\u65b9\u5f0f\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5927\u6a21\u578b\uff08\u5982LLM\u548cVLM\uff09\u8fa9\u8bba\u7cfb\u7edf\u867d\u8bbe\u6709\u4e13\u4e1a\u5316\u89d2\u8272\uff0c\u4f46\u5728\u5206\u914d\u6a21\u578b\u65f6\u672a\u5145\u5206\u8003\u8651\u5404\u6a21\u578b\u7684\u7279\u957f\u4e0e\u9002\u914d\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u501f\u52a9\u52a8\u6001\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u4e0e\u89d2\u8272\u7684\u5339\u914d\u5ea6\u5e76\u4f18\u5316\u6574\u4f53\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u89d2\u8272\u5206\u914d\u7684\u5143\u8fa9\u8bba\u6846\u67b6\uff1a\u9996\u5148\u5728\u5019\u9009\u6a21\u578b\u4e2d\u542f\u52a8\u4e24\u9636\u6bb5\u5143\u8fa9\u8bba\u2014\u2014\u63d0\u6848\u9636\u6bb5\u7531\u5019\u9009\u6a21\u578b\u7ed9\u51fa\u9488\u5bf9\u7279\u5b9a\u89d2\u8272\u7684\u7406\u7531\uff0c\u8bc4\u5ba1\u9636\u6bb5\u5219\u6839\u636e\u6570\u636e\u548c\u89d2\u8272\u76f8\u5173\u6807\u51c6\u6253\u5206\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u89d2\u8272\u6311\u9009\u51fa\u6700\u4f73\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u65e0\u7f1d\u5d4c\u5165\u5230\u73b0\u6709\u8fa9\u8bba\u7cfb\u7edf\u4e4b\u4e0a\u3002", "result": "\u5728LLM\u95ee\u9898\u6c42\u89e3\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u8f83\u4e8e\u7edf\u4e00\u5206\u914d\uff08\u6240\u6709\u89d2\u8272\u7528\u540c\u4e00\u6a21\u578b\uff09\u63d0\u5347\u6700\u9ad8\u8fbe74.8%\uff0c\u76f8\u8f83\u968f\u673a\u5206\u914d\uff08\u4e0d\u8003\u8651\u9002\u914d\u6027\u76f2\u76ee\u5206\u914d\uff09\u63d0\u5347\u6700\u9ad8\u8fbe29.7%\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u5e26\u6765\u4e86\u65b0\u8303\u5f0f\uff0c\u5c06\u8fc7\u53bb\u9759\u6001\u3001\u9884\u8bbe\u4ee3\u7406\u89d2\u8272\u7684\u65b9\u5f0f\uff0c\u8f6c\u5411\u57fa\u4e8e\u6a21\u578b\u80fd\u529b\u52a8\u6001\u5206\u914d\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2601.17428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17428", "abs": "https://arxiv.org/abs/2601.17428", "authors": ["Ziming Li", "Chenhao Li", "Marco Hutter"], "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning", "comment": null, "summary": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u8fdb\u5c55\u7684\u81ea\u52a8\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08LP-ACRL\uff09\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u987b\u5148\u9a8c\u96be\u5ea6\u5206\u5e03\u4fe1\u606f\u7684\u81ea\u52a8\u8bfe\u7a0b\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u591a\u6837\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8bfe\u7a0b\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u3001\u591a\u6837\u7684\u4efb\u52a1\u7a7a\u95f4\u65f6\uff0c\u96be\u4ee5\u5408\u7406\u5212\u5206\u4efb\u52a1\u96be\u5ea6\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4efb\u52a1\u96be\u5ea6\u6392\u5e8f\u7684\u4f9d\u8d56\u6210\u4e3a\u6269\u5c55\u7684\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u63d0\u51faLP-ACRL\u6846\u67b6\uff0c\u5728\u7ebf\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u8fdb\u5c55\uff0c\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u4efb\u52a1\u91c7\u6837\u5206\u5e03\uff0c\u5b9e\u73b0\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u8bfe\u7a0b\u751f\u6210\uff0c\u9002\u7528\u4e8e\u96be\u5ea6\u7ed3\u6784\u4e0d\u6e05\u6670\u7684\u4efb\u52a1\u7a7a\u95f4\u3002", "result": "\u7528LP-ACRL\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u4f7fANYmal D\u56db\u8db3\u673a\u5668\u4eba\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e0b\u90fd\u80fd\u5b9e\u73b02.5 m/s\u7ebf\u901f\u5ea6\u548c3.0 rad/s\u89d2\u901f\u5ea6\u7684\u9ad8\u6548\u3001\u7a33\u5b9a\u8fd0\u52a8\uff0c\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u53ea\u80fd\u5728\u7b80\u5355\u6216\u7279\u5b9a\u5730\u5f62\u5b9e\u73b0\u9ad8\u901f\u5ea6\u3002", "conclusion": "LP-ACRL\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u73b0\u5b9e\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u3001\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u8bfe\u7a0b\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u529b\u57fa\u7ebf\u548c\u53c2\u8003\u3002"}}
{"id": "2601.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17040", "abs": "https://arxiv.org/abs/2601.17040", "authors": ["H Neji", "J Nogueras-Iso", "J Lacasta", "M\u00c1 Latre", "FJ Garc\u00eda-Marco"], "title": "FP-THD: Full page transcription of historical documents", "comment": "Figure 1: FP-THD architecture Overview: Layout Analysis and Masked Auto-encoder with Vision Trans- former", "summary": "The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u7559\u7279\u6b8a\u7b26\u53f7\u548c\u5b57\u7b26\u7684\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u8f6c\u5f55\u6d41\u7a0b\uff0c\u901a\u8fc7\u5e03\u5c40\u5206\u6790\u4e0eOCR\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u624b\u5199\u3001\u5370\u5237\u53ca\u591a\u8bed\u8a00\u6587\u732e\u7684\u9ad8\u6548\u6570\u5b57\u5316\u3002", "motivation": "15\u300116\u4e16\u7eaa\u7684\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u5305\u542b\u8bb8\u591a\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\uff0c\u8fd9\u4e9b\u5bf9\u6587\u672c\u610f\u4e49\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u8f6c\u5f55\u65b9\u6cd5\u65e0\u6cd5\u5f88\u597d\u5730\u4fdd\u7559\u8fd9\u4e9b\u7279\u5f81\uff0c\u5f71\u54cd\u6587\u672c\u7684\u5386\u53f2\u548c\u5b66\u672f\u4ef7\u503c\u3002", "method": "\u5c06\u5e03\u5c40\u5206\u6790\u4e0e\u73b0\u6709\u6587\u672c\u884c\u8bc6\u522b\uff08OCR\uff09\u65b9\u6cd5\u7ed3\u5408\uff0c\u5148\u7528\u5e03\u5c40\u5206\u6790\u6a21\u578b\u63d0\u53d6\u6587\u672c\u884c\uff0c\u518d\u7528OCR\u6a21\u578b\u8f6c\u5f55\uff0c\u6bcf\u4e00\u6b65\u90fd\u4fdd\u7559\u5fc5\u8981\u7684\u6587\u732e\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u6587\u732e\uff08\u624b\u5199\u3001\u5370\u5237\u3001\u591a\u8bed\u8a00\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6d41\u7a0b\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5177\u6709\u4e0d\u540c\u7279\u6027\u7684\u5386\u53f2\u6587\u732e\u9875\u9762\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u4e86masked autoencoder\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u8be5\u8f6c\u5f55\u6d41\u7a0b\u6709\u52a9\u4e8e\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u6570\u5b57\u5316\u4fdd\u5b58\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6587\u732e\u8f6c\u5f55\u8d28\u91cf\uff0c\u4fdd\u7559\u5386\u53f2\u6587\u732e\u539f\u6709\u7684\u98ce\u683c\u548c\u610f\u4e49\u3002"}}
{"id": "2601.17156", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17156", "abs": "https://arxiv.org/abs/2601.17156", "authors": ["Eduardo Sanchez-Karhunen", "Jose F. Quesada-Moreno", "Miguel A. Guti\u00e9rrez-Naranjo"], "title": "Interpretability of the Intent Detection Problem: A New Approach", "comment": "Accepted for publication in The European Journal on Artificial Intelligence (2026)", "summary": "Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.", "AI": {"tldr": "\u672c\u6587\u501f\u52a9\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\uff0c\u4ece\u51e0\u4f55\u89d2\u5ea6\u89e3\u91caRNN\u5728\u610f\u56fe\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5c5e\u6027\uff08\u5982\u7c7b\u522b\u4e0d\u5e73\u8861\uff09\u5982\u4f55\u5f71\u54cdRNN\u7684\u5185\u90e8\u72b6\u6001\u7a7a\u95f4\u7ed3\u6784\u53ca\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u5728\u610f\u56fe\u8bc6\u522b\u4efb\u52a1\u4e2d\u975e\u5e38\u6210\u529f\uff0c\u4f46RNN\u4e3a\u4f55\u80fd\u591f\u6709\u6548\u5b8c\u6210\u8be5\u4efb\u52a1\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7406\u89e3RNN\u5185\u90e8\u8868\u5f81\u8fc7\u7a0b\u5bf9\u4e8e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\uff0c\u5c06\u6587\u672c\u53e5\u5b50\u7684\u8868\u5f81\u89c6\u4e3aRNN\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\u3002\u4ee5\u5e73\u8861\u7684SNIPS\u6570\u636e\u96c6\u548c\u4e0d\u5e73\u8861\u7684ATIS\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u5206\u6790RNN\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5982\u4f55\u901a\u8fc7\u51e0\u4f55\u7ed3\u6784\uff08\u4f4e\u7ef4\u6d41\u5f62\u4e0e\u805a\u7c7b\uff09\u6765\u533a\u5206\u4e0d\u540c\u610f\u56fe\u3002\u8fdb\u4e00\u6b65\u6bd4\u8f83\u6570\u636e\u96c6\u5c5e\u6027\u53d8\u5316\uff08\u4e3b\u8981\u4e3a\u7c7b\u522b\u5206\u5e03\u5931\u8861\uff09\u5bf9\u8fd9\u79cd\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "result": "RNN\u5728\u5e73\u8861\u7684SNIPS\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u5b66\u5230\u7406\u60f3\u7684\u4f4e\u7ef4\u6d41\u5f62\uff0c\u6bcf\u7c7b\u610f\u56fe\u5728\u9690\u85cf\u7a7a\u95f4\u4e2d\u5f62\u6210\u5206\u660e\u805a\u7c7b\u3002\u800c\u5728\u4e0d\u5e73\u8861\u7684ATIS\u6570\u636e\u96c6\u4e0a\uff0c\u4f4e\u9891\u7c7b\u610f\u56fe\u7684\u805a\u7c7b\u7ed3\u6784\u88ab\u6270\u4e71\u548c\u9000\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u51fa\u51e0\u4f55\u5206\u79bb\u4e0e\u8bfb\u51fa\u5bf9\u9f50\u53ef\u5206\uff0c\u4e3a\u73b0\u5b9e\u6027\u80fd\u4e0d\u4e00\u81f4\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u7406\u89e3\u91ca\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u4e3a\u7406\u89e3RNN\u5185\u90e8\u52a8\u529b\u5b66\u548c\u51e0\u4f55\u7ed3\u6784\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u89e3\u91ca\u4e86\u6570\u636e\u96c6\u5c5e\u6027\uff08\u7279\u522b\u662f\u7c7b\u522b\u4e0d\u5e73\u8861\uff09\u5bf9RNN\u6a21\u578b\u8868\u73b0\u7684\u76f4\u63a5\u5f71\u54cd\uff0c\u4e5f\u4e3a\u6a21\u578b\u8c03\u4f18\u548c\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.17440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17440", "abs": "https://arxiv.org/abs/2601.17440", "authors": ["Xinru Cui", "Linxi Feng", "Yixuan Zhou", "Haoqi Han", "Zhe Liu", "Hesheng Wang"], "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes", "comment": "8 pages, 4 figures", "summary": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u6846\u67b6PILOT\uff0c\u901a\u8fc7\u878d\u5408\u611f\u77e5\u4e0e\u52a8\u4f5c\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u7cbe\u786e\u7684\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u5668\u7f3a\u4e4f\u5bf9\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u5176\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\u4efb\u52a1\u6267\u884c\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7ed3\u5408\u611f\u77e5\u548c\u52a8\u4f5c\u63a7\u5236\uff0c\u5e76\u9ad8\u6548\u5b8c\u6210\u672c\u4f53\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u4efb\u52a1\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86PILOT\uff0c\u4e00\u79cd\u57fa\u4e8e\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u8eab\u611f\u77e5-\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u878d\u5408\u9884\u6d4b\u6027\u672c\u4f53\u611f\u77e5\u7279\u5f81\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5916\u90e8\u611f\u77e5\u8868\u8fbe\u7684\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u4ee5\u589e\u5f3a\u5730\u5f62\u611f\u77e5\u548c\u811a\u6b65\u7cbe\u5ea6\uff1b2\uff09\u91c7\u7528Mixture-of-Experts\u7ed3\u6784\u63d0\u5347\u591a\u6837\u5316\u8fd0\u52a8\u80fd\u529b\uff0c\u5b9e\u73b0\u4e0d\u540c\u8fd0\u52a8\u6280\u80fd\u7684\u534f\u540c\u4e0e\u4e13\u7cbe\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cPILOT\u63a7\u5236\u6846\u67b6\u5728\u7a33\u5b9a\u6027\u3001\u547d\u4ee4\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u590d\u6742\u5730\u5f62\u9002\u5e94\u6027\u7b49\u591a\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5bf9\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PILOT\u6846\u67b6\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9ad8\u6027\u80fd\uff0c\u6709\u671b\u4f5c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5e95\u5c42\u63a7\u5236\u5668\u7684\u5f3a\u5927\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u590d\u6742\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u64cd\u4f5c\u63d0\u4f9b\u4fdd\u969c\u3002"}}
{"id": "2601.17041", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17041", "abs": "https://arxiv.org/abs/2601.17041", "authors": ["Ghadeer Alanazi", "Abir Benabid"], "title": "Arabic Sign Language Recognition using Multimodal Approach", "comment": null, "summary": "Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u901a\u8fc7\u878d\u5408Leap Motion\u548cRGB\u6444\u50cf\u5934\u4e24\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u63d0\u5347\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\u51c6\u786e\u7387\u7684\u65b9\u6cd5\u3002\u63d0\u51fa\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5bf918\u4e2a\u8bcd\u4e2d\u768413\u4e2a\u5b9e\u73b0\u4e86\u6709\u6548\u8bc6\u522b\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e3a78%\u3002", "motivation": "\u73b0\u6709\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u624b\u90e8\u59ff\u6001\u548c\u4e09\u7ef4\u52a8\u4f5c\uff0c\u51c6\u786e\u7387\u53d7\u9650\u3002\u591a\u6a21\u6001\u878d\u5408\u6709\u671b\u5f25\u8865\u5404\u81ea\u4f20\u611f\u5668\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u8bc6\u522b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u5b50\u7f51\u7edc\u7684\u7cfb\u7edf\u7ed3\u6784\uff1a\u4e00\u4e2a\u9488\u5bf9Leap Motion\u6570\u636e\u7684\u5b9a\u5236\u7a20\u5bc6\u795e\u7ecf\u7f51\u7edc\uff08\u542bdropout\u4e0eL2\u6b63\u5219\u5316\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eVGG16\u7684\u56fe\u50cf\u5b50\u7f51\u7edc\uff08\u5e26\u6570\u636e\u589e\u5f3a\uff09\u3002\u4e24\u8005\u7684\u7279\u5f81\u5728\u878d\u5408\u6a21\u578b\u4e2d\u62fc\u63a5\uff0c\u6700\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548cSoftMax\u4f5c\u6700\u7ec8\u5206\u7c7b\u3002\u7cfb\u7edf\u5728\u81ea\u5efa\u768418\u4e2aArSL\u8bcd\u6c47\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u3002", "result": "\u7cfb\u7edf\u5bf918\u4e2a\u8bcd\u6c47\u4e2d\u768413\u4e2a\u505a\u51fa\u6b63\u786e\u8bc6\u522b\uff0c\u603b\u4f53\u51c6\u786e\u7387\u4e3a78%\u3002", "conclusion": "\u591a\u6a21\u6001\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u5bf9\u63d0\u5347\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\u6709\u521d\u6b65\u6548\u679c\uff0c\u4f46\u8fd8\u6709\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6570\u636e\u96c6\u6269\u5c55\u7684\u7a7a\u95f4\u3002"}}
{"id": "2601.17172", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17172", "abs": "https://arxiv.org/abs/2601.17172", "authors": ["Tunazzina Islam"], "title": "Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9762\u5411\u7279\u5b9a\u4eba\u7fa4\u751f\u6210\u5b9a\u5411\u4fe1\u606f\u65f6\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\uff08\u5982\u6027\u522b\u3001\u5e74\u9f84\uff09\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u89c1\u3002", "motivation": "\u968f\u7740LLMs\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u4e2a\u6027\u5316\u3001\u6709\u8bf4\u670d\u529b\u7684\u6587\u672c\uff0c\u5176\u5728\u81ea\u52a8\u5316\u4ea4\u6d41\u4e2d\u7684\u504f\u89c1\u4e0e\u516c\u5e73\u6027\u95ee\u9898\u53d8\u5f97\u7a81\u51fa\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793aLLMs\u5728\u4eba\u7fa4\u5b9a\u5411\u4fe1\u606f\u751f\u6210\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u523b\u677f\u5370\u8c61\u548c\u4e0d\u516c\u5e73\u73b0\u8c61\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53d7\u63a7\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5GPT-4o\u3001Llama-3.3\u548cMistral-Large 2.1\u4e3a\u4ee3\u8868\uff0c\u5bf9\u6bd4\u4e86\u201c\u72ec\u7acb\u751f\u6210\u201d\u548c\u201c\u5bcc\u4e0a\u4e0b\u6587\u751f\u6210\u201d\u4e24\u79cd\u73af\u5883\u4e0b\u7684\u4fe1\u606f\u751f\u6210\u6548\u679c\uff0c\u4ece\u8bcd\u6c47\u3001\u8bed\u8a00\u98ce\u683c\u3001\u8bf4\u670d\u6846\u67b6\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4ef7\uff0c\u805a\u7126\u4e8e\u6c14\u5019\u4f20\u64ad\u573a\u666f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u6027\u522b\u548c\u5e74\u9f84\u4e0a\u5177\u6709\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\uff1a\u9488\u5bf9\u7537\u6027\u548c\u9752\u5e74\u7684\u4fe1\u606f\u66f4\u5f3a\u8c03\u4e3b\u5bfc\u6027\u3001\u521b\u65b0\u548c\u81ea\u4fe1\uff0c\u800c\u9488\u5bf9\u5973\u6027\u548c\u8001\u5e74\u4eba\u7684\u4fe1\u606f\u66f4\u7a81\u51fa\u6e29\u6696\u3001\u5173\u6000\u548c\u4f20\u7edf\u3002\u52a0\u4e0a\u4e0b\u6587\u540e\uff0c\u8fd9\u4e9b\u5dee\u5f02\u8fdb\u4e00\u6b65\u52a0\u5267\uff0c\u9752\u5e74\u548c\u7537\u6027\u5b9a\u5411\u4fe1\u606f\u7684\u8bf4\u670d\u529b\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "LLMs\u5728\u5b9a\u5411\u751f\u6210\u4e2d\u4f1a\u5448\u73b0\u5e76\u653e\u5927\u4eba\u53e3\u523b\u677f\u5370\u8c61\uff0c\u4f5c\u8005\u5f3a\u8c03\u4e9f\u9700\u5f00\u53d1\u5177\u5907\u504f\u89c1\u610f\u8bc6\u7684\u751f\u6210\u7ba1\u9053\u53ca\u900f\u660e\u7684\u5ba1\u8ba1\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u793e\u4ea4\u654f\u611f\u5e94\u7528\u9886\u57df\u7684\u4eba\u53e3\u7279\u5f81\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2601.17486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17486", "abs": "https://arxiv.org/abs/2601.17486", "authors": ["Zhiyuan Zhang", "Yu She"], "title": "EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds", "comment": "Project website: https://ZhangZhiyuanZhang.github.io/equiform-website/ Code will be released", "summary": "Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4e09\u7ef4\u70b9\u4e91\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b0\u65b9\u6cd5EquiForm\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u566a\u58f0\u53ca\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u70b9\u4e91\u7684\u6a21\u4eff\u5b66\u4e60\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u3001\u4f4d\u59ff\u6270\u52a8\u548c\u906e\u6321\u5bfc\u81f4\u7684\u4f2a\u5f71\u975e\u5e38\u654f\u611f\uff0c\u96be\u4ee5\u4fdd\u8bc1\u4e09\u7ef4\u7ed3\u6784\u7684\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002\u591a\u6570\u7b49\u53d8\u65b9\u6cd5\u4ec5\u5728\u795e\u7ecf\u7ed3\u6784\u4e2d\u7f16\u7801\u5bf9\u79f0\u6027\u7ea6\u675f\uff0c\u672a\u80fd\u6709\u6548\u5904\u7406\u566a\u58f0\u5e26\u6765\u7684\u51e0\u4f55\u5931\u771f\uff0c\u4e5f\u672a\u5bf9\u8868\u5f81\u7684\u7b49\u53d8\u4e00\u81f4\u6027\u8fdb\u884c\u660e\u786e\u5f3a\u5316\u3002", "method": "1\uff09\u63d0\u51faEquiForm\uff0c\u7ed3\u5408SE(3)\u7b49\u53d8\u673a\u5236\uff1b2\uff09\u5f15\u5165\u529b\u5b66\u51e0\u4f55\u53bb\u566a\u6a21\u5757\uff0c\u6709\u6548\u6062\u590d\u53d7\u566a\u58f0\u548c\u7f3a\u5931\u5f71\u54cd\u7684\u70b9\u4e91\u4e09\u7ef4\u7ed3\u6784\uff1b3\uff09\u8bbe\u8ba1\u7b49\u53d8\u5bf9\u6bd4\u8868\u5f81\u5bf9\u9f50\u76ee\u6807\uff0c\u4f7f\u7279\u5f81\u5728\u521a\u6027\u53d8\u6362\u548c\u566a\u58f0\u6270\u52a8\u4e0b\u4fdd\u6301\u4e00\u81f4\u6027\uff1b4\uff09\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7ed3\u5408\uff0c\u5f62\u6210\u901a\u7528\u3001\u9c81\u68d2\u7684\u7b56\u7565\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "\u572816\u4e2a\u4eff\u771f\u548c4\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0cEquiForm\u5bf9\u6bd4\u73b0\u6709\u6700\u4f18\u70b9\u4e91\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u4eff\u771f\u63d0\u534717.2%\uff0c\u771f\u5b9e\u4efb\u52a1\u63d0\u534728.1%\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6297\u566a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EquiForm\u80fd\u591f\u6709\u6548\u89e3\u51b3\u70b9\u4e91\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u566a\u58f0\u548c\u7ed3\u6784\u5931\u771f\u96be\u9898\uff0c\u6781\u5927\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2601.17042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17042", "abs": "https://arxiv.org/abs/2601.17042", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Bin Yan"], "title": "Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective", "comment": "8 pages with 6 figures", "summary": "Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between \"membership matrix\" and \"subspace matrix U\" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the \"membership matrix\" and \"subspaces U\" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u7f16\u7801\u7387\u51cf\u5c11\uff08MCR2\uff09\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u9ad8\u6548\u6027\u7edf\u4e00\u7684\u767d\u76d2Transformer\u65b9\u6cd5\u3002\u901a\u8fc7\u89e3\u8026MCR2\u4e2d\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4\u77e9\u9635U\u201d\u7684\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfc\u51fa\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u5b9e\u73b0\u7b97\u6cd5\u9ad8\u6548\u4e14\u6613\u4e8e\u89e3\u91ca\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u4e0e\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709MCR2-T\u65b9\u6cd5\u4e2d\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4\u77e9\u9635U\u201d\u8026\u5408\u7d27\u5bc6\uff0c\u5bfc\u81f4token\u6295\u5f71\u4e0d\u51c6\u786e\u65f6\u4f1a\u4ea7\u751f\u5197\u4f59\u7f16\u7801\uff0c\u5f71\u54cd\u6a21\u578b\u6548\u7387\u4e0e\u89e3\u91ca\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u8fd9\u4e8c\u8005\u5173\u7cfb\u3001\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u4e0e\u9ad8\u6548\u6027\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u4ece\u8f93\u5165\u5b66\u4e60\u6210\u5458\u77e9\u9635\uff0c\u518d\u4ece\u5168\u96c6S\u4e2d\u6d3e\u751f\u7a00\u758f\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u8fc7\u7684MCR2\u76ee\u6807\u7684\u68af\u5ea6\u5c55\u5f00\uff0c\u63a8\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\u2014\u2014\u89e3\u8026\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\uff08DMSA\uff09\u3002\u52a0\u5165Token Statistics Transformer\u5f62\u6210DMST\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0c\u5c06Token Statistics Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3aDMSA\u540e\uff08\u5373DMST\uff09\uff0c\u7f16\u7801\u7387\u4e0b\u964d\u66f4\u5feb\uff0c\u5e76\u4e14top-1\u51c6\u786e\u7387\u63d0\u9ad81.08%-1.45%\u3002\u76f8\u8f83\u4e8e\u539f\u751fTransformer\uff0cDMST\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u89e3\u8026MCR2\u76ee\u6807\u4e2d\u7684\u6210\u5458\u77e9\u9635\u4e0e\u5b50\u7a7a\u95f4\u77e9\u9635\uff0c\u80fd\u591f\u8bbe\u8ba1\u51fa\u65e2\u9ad8\u6548\u53c8\u6613\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u89c6\u89c9\u6a21\u578b\u5e26\u6765\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u53cc\u63d0\u5347\u3002"}}
{"id": "2601.17173", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17173", "abs": "https://arxiv.org/abs/2601.17173", "authors": ["Parth Bhalerao", "Diola Dsouza", "Ruiwen Guan", "Oana Ignat"], "title": "Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content", "comment": null, "summary": "Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86MentorQA\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u957f\u671f\u89c6\u9891\u5185\u5bb9\u63d0\u95ee\u7684\u3001\u591a\u8bed\u8a00\u3001\u4e13\u6ce8\u4e8e\u6307\u5bfc\u6027\u56de\u7b54\u7684\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u6846\u67b6\uff0c\u5f3a\u8c03\u8d85\u8d8a\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u591a\u7ef4\u5ea6\u8bc4\u4ef7\u3002\u4f5c\u8005\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540c\u95ee\u7b54\u6a21\u578b\u67b6\u6784\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u590d\u6742\u8bdd\u9898\u53ca\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u4f20\u7edf\u95ee\u7b54\u7cfb\u7edf\u4ec5\u5173\u6ce8\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u6559\u80b2\u53ca\u804c\u4e1a\u6307\u5bfc\uff09\u9700\u8981\u5177\u5907\u6307\u5bfc\u6027\u548c\u53cd\u601d\u6027\u7684\u56de\u7b54\u3002\u73b0\u6709\u95ee\u7b54\u57fa\u51c6\u5728\u591a\u8bed\u8a00\u548c\u957f\u6587\u672c\u573a\u666f\u4e0b\uff0c\u6781\u5c11\u8bc4\u4f30\u6307\u5bfc\u80fd\u529b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5efa\u7acb\u65b0\u7684\u8bc4\u6d4b\u4f53\u7cfb\u3002", "method": "\u63d0\u51faMentorQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u8bed\u8a00\u3001\u8fd19000\u5bf9\u95ee\u7b54\u5bf9\uff08\u622a\u81f3180\u5c0f\u65f6\u89c6\u9891\uff09\uff0c\u5e76\u8bbe\u7acb\u53cd\u6620\u6307\u5bfc\u6027\u7684\u65b0\u8bc4\u6d4b\u7ef4\u5ea6\uff08\u5982\u6e05\u6670\u5ea6\u3001\u5bf9\u9f50\u5ea6\u4e0e\u5b66\u4e60\u4ef7\u503c\uff09\u3002\u5bf9\u6bd4\u4e86\u5355\u667a\u80fd\u4f53\u3001\u53cc\u667a\u80fd\u4f53\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u548c\u591a\u667a\u80fd\u4f53\u7b49\u67b6\u6784\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u540c\u65f6\u8bc4\u6d4b\u4e86\u7528\u5927\u6a21\u578b\u81ea\u52a8\u8bc4\u4ef7\u7684\u53ef\u9760\u6027\u3002", "result": "\u591a\u667a\u80fd\u4f53\u95ee\u7b54\u6d41\u7a0b\u5728\u6307\u5bfc\u6027\u56de\u7b54\u8d28\u91cf\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u590d\u6742\u4e3b\u9898\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u63d0\u5347\u663e\u8457\u3002\u81ea\u52a8\u5316\u8bc4\u6d4b\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u5b58\u5728\u5bf9\u9f50\u5dee\u5f02\u3002", "conclusion": "\u672c\u5de5\u4f5c\u9996\u6b21\u7cfb\u7edf\u5316\u63d0\u51fa\u6307\u5bfc\u578b\u95ee\u7b54\u4e3a\u72ec\u7acb\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u6559\u80b2AI\u7684\u667a\u80fd\u4f53\u67b6\u6784\u4e0e\u8bc4\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u3002MentorQA\u6570\u636e\u96c6\u548c\u6846\u67b6\u53ef\u652f\u6301\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2601.17507", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17507", "abs": "https://arxiv.org/abs/2601.17507", "authors": ["Yutong Shen", "Hangxu Liu", "Kailin Pei", "Ruizhe Xia", "Tongtong Feng"], "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions", "comment": "8 pages, 4 figures, Submitted to ICLR 2026 World Model Workshop", "summary": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eff\u4eba\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u63a7\u7684\u65b0\u65b9\u6cd5MetaWorld\uff0c\u901a\u8fc7\u5206\u5c42\u4e16\u754c\u6a21\u578b\u5c06\u8bed\u4e49\u8ba1\u5212\u4e0e\u7269\u7406\u63a7\u5236\u7ed3\u5408\uff0c\u5b9e\u73b0\u6548\u7387\u66f4\u9ad8\u4e0e\u6cdb\u5316\u6027\u66f4\u597d\u7684\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u73b0\u6709\u4eff\u4eba\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u63a7\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u4f4e\u3001\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u5f31\uff0c\u4ee5\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7269\u7406\u4e0d\u4e00\u81f4\u3002\u4e9f\u9700\u4e00\u4e2a\u9ad8\u6548\u3001\u6cdb\u5316\u597d\u3001\u7269\u7406\u4e00\u81f4\u6027\u5f3a\u7684\u6574\u4f53\u6846\u67b6\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u4e0a\u5c42\u7531VLM\u9a71\u52a8\u8fdb\u884c\u8bed\u4e49\u5206\u89e3\uff0c\u5e95\u5c42\u5229\u7528\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u7684\u52a8\u529b\u5b66\u6a21\u578b\u5b9e\u73b0\u7269\u7406\u63a7\u5236\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u56fe\u4e13\u5bb6\u9009\u62e9\u4e0e\u52a8\u4f5c\u5148\u9a8c\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408\u591a\u4e13\u5bb6\u7b56\u7565\u5e93\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u548c\u9ad8\u6548\u5728\u7ebf\u81ea\u9002\u5e94\u3002VLM\u5c06\u81ea\u7136\u6307\u4ee4\u76f4\u63a5\u6620\u5c04\u5230\u53ef\u6267\u884c\u6280\u80fd\uff0c\u8df3\u8fc7\u7b26\u53f7\u8bed\u4e49\u7ed1\u5b9a\u3002", "result": "\u5728Humanoid-Bench\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cMetaWorld\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u4ee5\u4e16\u754c\u6a21\u578b\u4e3a\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u8fd8\u662f\u52a8\u4f5c\u8fde\u8d2f\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "MetaWorld\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u4eba\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u63a7\u4e2d\u8bed\u4e49\u4e0e\u7269\u7406\u95f4\u7684\u8131\u8282\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u673a\u5668\u4eba\u667a\u80fd\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u4e0e\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17046", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17046", "abs": "https://arxiv.org/abs/2601.17046", "authors": ["Matan Leibovich", "Mai Tan", "Adria Marcos-Morales", "Sreyas Mohan", "Peter A. Crozier", "Carlos Fernandez-Granda"], "title": "Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning", "comment": null, "summary": "We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u6df1\u5ea6\u4f30\u8ba1\u8f6c\u5316\u4e3a\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u4ece\u51e0\u4f55\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u56fe\u50cf\u4e2d\u63d0\u53d63D\u539f\u5b50\u7ea7\u4fe1\u606f\uff0c\u5728CeO2\u7eb3\u7c73\u9897\u7c92\u7684\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u53d6\u5f97\u4e86\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u7ed3\u679c\u3002", "motivation": "\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u4eceTEM\u56fe\u50cf\u4e2d\u63d0\u53d6\u4e09\u7ef4\u539f\u5b50\u7ea7\u4fe1\u606f\u4e00\u76f4\u662f\u96be\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u4e0d\u591f\u7cbe\u786e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5c06\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u5efa\u6a21\u4e3a\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u52a0\u5165\u5408\u6210\u566a\u58f0\u7684\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u751f\u6210\u9010\u50cf\u7d20\u7684\u6df1\u5ea6\u5206\u5272\u56fe\u3002\u65b9\u6cd5\u968f\u540e\u5728CeO2\u7eb3\u7c73\u9897\u7c92\u7684\u6a21\u62df\u53ca\u5b9e\u9645TEM\u6570\u636e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u7ec4\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c\u4e0d\u4ec5\u51c6\u786e\uff0c\u8fd8\u5177\u6709\u826f\u597d\u7684\u6821\u51c6\u6027\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5c06\u6df1\u5ea6\u4f30\u8ba1\u8f6c\u5316\u4e3a\u8bed\u4e49\u5206\u5272\u5e76\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u6709\u6548\u63d0\u5347\u4e86TEM\u56fe\u50cf3D\u539f\u5b50\u7ea7\u4fe1\u606f\u63d0\u53d6\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9ad8\u566a\u58f0\u6570\u636e\u4e0b\u7684\u4e09\u7ef4\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.17181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17181", "abs": "https://arxiv.org/abs/2601.17181", "authors": ["Doreen Osmelak", "Yang Xu", "Michael Hahn", "Kate McCurdy"], "title": "Systematicity between Forms and Meanings across Languages Supports Efficient Communication", "comment": null, "summary": "Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u8bed\u8a00\u4e2d\u52a8\u8bcd\u548c\u4ee3\u8bcd\u5982\u4f55\u8868\u8fbe\u6709\u9650\u7684\u8bed\u6cd5\u610f\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u590d\u6742\u5ea6\u7684\u65b0\u6d4b\u91cf\u65b9\u6cd5\u6765\u89e3\u91ca\u8bed\u8a00\u5f62\u5f0f\u7684\u7cfb\u7edf\u89c4\u5f8b\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u8ba4\u4e3a\u8bed\u8a00\u7684\u8bcd\u4e49\u2014\u8bcd\u5f62\u6620\u5c04\u662f\u4e3a\u4e86\u9ad8\u6548\u4ea4\u6d41\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bcd\u5f62\u5185\u90e8\u7684\u7cfb\u7edf\u6027\u5173\u7cfb\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u91ca\u8bed\u8a00\u5f62\u5f0f\u4e2d\u7684\u7cfb\u7edf\u89c4\u5f8b\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u89c4\u5f8b\u5982\u4f55\u670d\u52a1\u4e8e\u9ad8\u6548\u4ea4\u6d41\u3002", "method": "\u4f5c\u8005\u8c03\u67e5\u4e86\u8de8\u8bed\u8a00\u7c7b\u578b\u591a\u6837\u7684\u52a8\u8bcd\u548c\u4ee3\u8bcd\u5728\u8868\u8fbe\u7279\u5b9a\u8bed\u6cd5\u610f\u4e49\uff08\u5982\u4eba\u79f0\u3001\u6570\u7b49\uff09\u65f6\u7684\u5f62\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u53ef\u5b66\u4e60\u6027\u201d\u7684\u65b0\u590d\u6742\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4ee5\u91cf\u5316\u8bcd\u5f62\u7cfb\u7edf\u7684\u89c4\u5f8b\u6027\u548c\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u5bf9\u6bd4\u73b0\u6709\u8bcd\u5f62\u7cfb\u7edf\u4e0e\u672a\u89c1\u7cfb\u7edf\uff0c\u5206\u6790\u5e76\u6a21\u62df\u4e86\u4e0d\u540c\u4ea4\u6d41\u538b\u529b\uff08\u7b80\u5316 vs. \u51c6\u786e\u6027\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff0c\u52a8\u8bcd\u548c\u4ee3\u8bcd\u7684\u5f62\u5f0f\u786e\u5b9e\u53d7\u9650\u4e8e\u7b80\u5316\uff08\u7c7b\u522b\u6700\u5c0f\u5316\uff09\u548c\u51c6\u786e\u6027\uff08\u6b67\u4e49\u89c4\u907f\uff09\u7684\u5171\u540c\u538b\u529b\u3002\u800c\u65b0\u63d0\u51fa\u7684\u590d\u6742\u5ea6\u91cf\u6307\u6807\u53ef\u4ee5\u66f4\u7ec6\u81f4\u5730\u533a\u5206\u5df2\u89c2\u5bdf\u5230\u548c\u672a\u88ab\u91c7\u7528\u7684\u8bed\u8a00\u7cfb\u7edf\uff0c\u63ed\u793a\u51fa\u8bed\u8a00\u5f62\u5f0f\u7684\u7cfb\u7edf\u6027\u7279\u5f81\u3002", "conclusion": "\u4f5c\u8005\u7684\u65b0\u6a21\u578b\u4e3a\u9ad8\u6548\u4ea4\u6d41\u7406\u8bba\u4e0e\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u7cfb\u7edf\u6027\u89c4\u5f8b\u95f4\u5efa\u7acb\u4e86\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u4e3a\u4ec0\u4e48\u4f1a\u5f62\u6210\u73b0\u5728\u8fd9\u6837\u7684\u8bcd\u5f62\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u91ca\u548c\u5de5\u5177\u3002"}}
{"id": "2601.17550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17550", "abs": "https://arxiv.org/abs/2601.17550", "authors": ["Deepak Singh", "Shreyas Khobragade", "Nitin J. Sanket"], "title": "AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation", "comment": "8 pages, 10 figures, Published in IEEE Robotics And Automation Letters", "summary": "Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7edd\u5bf9\u9ed1\u6697\u73af\u5883\u4e0b\u81ea\u4e3b\u5bfc\u822a\u7684\u5fae\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u76ee\u7ea2\u5916\u76f8\u673a\u7ed3\u5408\u5927\u5149\u5708\u7f16\u7801\u900f\u955c\u548c\u7ed3\u6784\u5149\u5b9e\u73b0\u5bfc\u822a\uff0c\u65e0\u9700GPS\u6216\u5916\u90e8\u5b9a\u4f4d\u8bbe\u5907\u3002\u7cfb\u7edf\u91c7\u7528\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578bAsterNet\uff0c\u80fd\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u5bf9\u672a\u77e5\u969c\u788d\u7269\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u5bfc\u822a\u3002", "motivation": "\u707e\u540e\u641c\u6551\u5e38\u56e0\u65ad\u7535\u9677\u5165\u7edd\u5bf9\u9ed1\u6697\uff0c\u5fae\u578b\u65e0\u4eba\u673a\u867d\u9002\u5408\u6b64\u573a\u666f\uff0c\u4f46\u73b0\u6709\u6280\u672f\u8d44\u6e90\u6709\u9650\uff0c\u96be\u4ee5\u5b89\u5168\u5bfc\u822a\u5bfb\u627e\u5e78\u5b58\u8005\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u65e0\u9700\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u3001\u80fd\u5728\u9ed1\u6697\u4e2d\u72ec\u7acb\u5bfc\u822a\u7684\u53ef\u9760\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5229\u7528\u7ea2\u5916\u5355\u76ee\u76f8\u673a\u914d\u5927\u5149\u5708\u7f16\u7801\u900f\u955c\u4e0e\u7ed3\u6784\u5149\u7167\u660e\uff0c\u901a\u8fc7\u70b9\u6269\u6563\u83b7\u53d6\u4e0d\u540c\u6df1\u5ea6\u4fe1\u606f\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1AsterNet\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u5e76\u5728\u6a21\u62df\u6570\u636e\u751f\u6210\u4e0b\u8bad\u7ec3\uff0c\u5b9e\u73b0\u65e0\u987b\u5fae\u8c03\u5373\u53ef\u8fc1\u79fb\u81f3\u771f\u5b9e\u73af\u5883\uff0c\u4e14\u7b97\u6cd5\u5728\u673a\u5668\u4eba\u7aef\u5b9e\u65f6\u63a8\u7406\uff0c\u9002\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "result": "\u6240\u63d0\u7cfb\u7edf\uff08AsterNav\uff09\u5728\u591a\u7ec4\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u4ec5\u4f9d\u9760\u673a\u8f7d\u4f20\u611f\u548c\u8ba1\u7b97\u5b9e\u73b0\u9ed1\u6697\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u5305\u62ec\u5bf9\u6697\u8272\u54d1\u5149\u969c\u788d\u7269\u548c\u7ec6\u7ef3\uff08\u76f4\u5f846.25mm\uff09\uff0c\u603b\u6210\u529f\u7387\u8fbe95.5%\uff0c\u5bf9\u969c\u788d\u7269\u5f62\u72b6\u3001\u4f4d\u7f6e\u3001\u6750\u6599\u672a\u77e5\u4e5f\u80fd\u9c81\u68d2\u901a\u8fc7\u3002", "conclusion": "\u672c\u5de5\u4f5c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5355\u76ee\u7ed3\u6784\u5149\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u7edd\u5bf9\u9ed1\u6697\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u8bc1\u660e\u4e86\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u9ad8\u53ef\u9760\u6027\uff0c\u4e3a\u707e\u540e\u65e0\u4eba\u673a\u6551\u63f4\u63d0\u4f9b\u4e86\u5168\u65b0\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.17047", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17047", "abs": "https://arxiv.org/abs/2601.17047", "authors": ["Yuanjie Gu", "Yiqun Wang", "Chaohui Yu", "Ang Xuan", "Fan Wang", "Zhi Lu", "Biqin Dong"], "title": "A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities", "comment": null, "summary": "Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce \"Noisomics\", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cNoisomics\u201d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Contrastive Pre-trained (CoP)\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u6210\u50cf\u566a\u58f0\u4ece\u201c\u5e72\u6270\u201d\u8f6c\u53d8\u4e3a\u53ef\u89e3\u7801\u7684\u4fe1\u606f\u8d44\u6e90\u3002\u8be5\u65b9\u6cd5\u7528\u6781\u5c11\u6570\u636e\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4f20\u7edf\u5927\u89c4\u6a21\u6709\u76d1\u7763\u57fa\u7ebf\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u50cf\u566a\u58f0\u89e3\u7801\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6210\u50cf\u566a\u58f0\u5efa\u6a21\u4e25\u91cd\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u548c\u8bbe\u5907\uff0c\u566a\u58f0\u5e38\u88ab\u89c6\u4e3a\u5e72\u6270\u800c\u975e\u6709\u7528\u4fe1\u606f\uff0c\u96be\u4ee5\u5229\u7528\u5176\u6df1\u5c42\u6f5c\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u7684\u89e3\u7801\u65b9\u6848\uff0c\u5c06\u566a\u58f0\u4f5c\u4e3a\u4fe1\u606f\u8d44\u6e90\uff0c\u66f4\u597d\u5730\u8f85\u52a9\u6210\u50cf\u8bca\u65ad\u3002", "method": "\u63d0\u51faNoisomics\u6846\u67b6\u548cCoP\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u5408\u6210\u566a\u58f0\u57fa\u56e0\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u57fa\u4e8e\u6d41\u5f62\u5047\u8bf4\u5b9e\u73b0\u8bed\u4e49\u4fe1\u53f7\u4e0e\u968f\u673a\u6270\u52a8\u7684\u89e3\u8026\u3002\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5916\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "CoP\u53ea\u9700100\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u8d85\u8d8a\u970010\u4e07\u6837\u672c\u7684\u6709\u76d1\u7763\u57fa\u7ebf\u3002\u8de812\u4e2a\u9886\u57df\u5916\u6570\u636e\u96c6\u83b7\u5f97\u5f3a\u9c81\u68d2\u6027\uff0c\u566a\u58f0\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e63.8%\uff0c\u51b3\u5b9a\u7cfb\u6570\u63d0\u534785.1%\u3002", "conclusion": "CoP\u53ef\u9ad8\u6548\u89e3\u7801\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u7684\u566a\u58f0\u4fe1\u606f\uff0c\u5b9e\u73b0\u8bbe\u5907\u65e0\u5173\u7684\u6210\u50cf\u8bca\u65ad\uff0c\u4e3a\u566a\u58f0\u5efa\u6a21\u548c\u6210\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u6570\u636e\u548c\u8bbe\u5907\u6807\u5b9a\u7684\u4f9d\u8d56\u3002"}}
{"id": "2601.17197", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17197", "abs": "https://arxiv.org/abs/2601.17197", "authors": ["Seyyed Saeid Cheshmi", "Hahnemann Ortiz", "James Mooney", "Dongyeop Kang"], "title": "Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u6b65\u6846\u67b6\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u591a\u6a21\u6001\u9690\u55bb\u8bed\u8a00\uff08\u5982\u8bbd\u523a\u3001\u5e7d\u9ed8\u3001\u6bd4\u55bb\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u8f6c\u79fb\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u79cd\u9690\u55bb\u98ce\u683c\u4e0b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u64c5\u957f\u5904\u7406\u76f4\u767d\u7684\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u4f46\u5728\u7406\u89e3\u8bbd\u523a\u3001\u5e7d\u9ed8\u3001\u6bd4\u55bb\u7b49\u9690\u55bb\u6027\u8bed\u8a00\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e9b\u8bed\u8a00\u98ce\u683c\u6d89\u53ca\u6df1\u5c42\u610f\u56fe\u548c\u60c5\u611f\uff0c\u540c\u65f6\u53d7\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u56fe\u7247\uff09\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u6a21\u578b\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u6b65\u6846\u67b6\uff1a\uff081\uff09\u89e3\u91ca\u591a\u6a21\u6001\u9690\u55bb\u8bed\u8a00\uff1b\uff082\uff09\u751f\u6210\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b\uff083\uff09\u5b9e\u73b0\u591a\u9690\u55bb\u98ce\u683c\u7684\u6cdb\u5316\u3002\u8bbe\u8ba1\u5b9e\u9a8c\u5bf9\u56db\u79cd\u98ce\u683c\u5206\u522b\u53ca\u8054\u5408\u8bad\u7ec3\uff0c\u5e76\u68c0\u9a8c\u63a8\u7406\u80fd\u529b\u53ca\u98ce\u683c\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\uff081\uff09\u63a8\u7406\u8fc7\u7a0b\u7684\u5f15\u5165\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9690\u55bb\u8bed\u8a00\u7684\u7406\u89e3\uff1b\uff082\uff09\u7279\u5b9a\u98ce\u683c\u4e0b\u7684\u63a8\u7406\u53ef\u4ee5\u8fc1\u79fb\u5230\u76f8\u5173\u98ce\u683c\uff08\u5982\u8bbd\u523a\u4e0e\u5e7d\u9ed8\uff09\uff1b\uff083\uff09\u591a\u98ce\u683c\u8054\u5408\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\uff0c\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u5927\u89c4\u6a21\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u5177\u5907\u53ef\u9a8c\u8bc1\u63a8\u7406\u80fd\u529b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5bf9\u591a\u6a21\u6001\u9690\u55bb\u6027\u8bed\u8a00\u7684\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5177\u5907\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u76f8\u5173\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17556", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17556", "abs": "https://arxiv.org/abs/2601.17556", "authors": ["Ulices Santa Cruz", "Mahmoud Elfar", "Yasser Shoukry"], "title": "Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models", "comment": null, "summary": "We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53ef\u4e3a\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5173\u952e\u573a\u666f\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ed3\u679c\u6b63\u786e\u6027\u7684\u53ef\u8bc1\u660e\u4fdd\u8bc1\uff0c\u8fd9\u5bf9\u5b89\u5168\u76f8\u5173\u5e94\u7528\uff08\u5982\u81ea\u4e3b\u7cfb\u7edf\uff09\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5728\u4f30\u8ba1\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7269\u7406\u5efa\u6a21\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u5b89\u5168\u8ba4\u8bc1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u51e0\u4f55\u751f\u6210\u6a21\u578b\uff08GGM\uff09\uff0c\u5176\u53c2\u6570\u7531\u76f8\u673a\u89c2\u6d4b\u5e73\u9762\u76ee\u6807\u7269\u7684\u6210\u50cf\u8fc7\u7a0b\u63a8\u5bfc\uff0c\u5229\u7528\u8be5\u6a21\u578b\u53ef\u8bad\u7ec3\u5177\u6709\u8bef\u5dee\u8ba4\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u4f4d\u59ff\u4f30\u8ba1\u5668\u3002\u9996\u5148\u5728\u65e0\u5e72\u6270\u73af\u5883\u4e0b\u5b9e\u73b0\uff0c\u518d\u501f\u52a9\u53ef\u8fbe\u6027\u5206\u6790\u65b9\u6cd5\u6269\u5c55\u81f3\u6709\u5e72\u6270\u73af\u5883\uff0c\u6700\u540e\u5c06\u8ba4\u8bc1\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ba4\u8bc1\u4f4d\u59ff\u4f30\u8ba1\u7ed3\u5408\uff0c\u5f62\u6210\u591a\u9636\u6bb5\u611f\u77e5\u7ba1\u9053\u3002\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\uff08\u5305\u62ec\u4e8b\u4ef6\u76f8\u673a\u91c7\u96c6\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5404\u79cd\u5e73\u9762\u76ee\u6807\u7269\u4f53\u5408\u6210\u53ca\u771f\u5b9e\u56fe\u7247\uff08\u5982\u4ea4\u901a\u6807\u5fd7\uff09\u6d4b\u8bd5\u4e2d\uff0c\u6240\u8bad\u7ec3\u7684\u795e\u7ecf\u7f16\u7801\u5668\u80fd\u5728\u5df2\u8ba4\u8bc1\u8bef\u5dee\u754c\u4e0b\u51c6\u786e\u4f30\u8ba1\u76ee\u6807\u4f4d\u59ff\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u4e0e\u5b89\u5168\u4fdd\u8bc1\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5177\u8ba4\u8bc1\u4fdd\u969c\u7684\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u5728\u5173\u952e\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u66f4\u5177\u5b89\u5168\u6027\u4e0e\u53ef\u7528\u6027\uff0c\u540c\u65f6\u5177\u5907\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.17048", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17048", "abs": "https://arxiv.org/abs/2601.17048", "authors": ["Jing Jie Tan", "Rupert Schreiner", "Matthias Hausladen", "Ali Asgharzade", "Simon Edler", "Julian Bartsch", "Michael Bachmann", "Andreas Schels", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum"], "title": "SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis", "comment": null, "summary": "Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSiMiC\u65b9\u6cd5\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u81ea\u52a8\u5206\u6790\u7845\u5fae\u7ed3\u6784SEM\u56fe\u50cf\uff0c\u5b9e\u73b0\u591a\u7c7b\u522b\u7ed3\u6784\u7684\u5206\u7c7b\u4e0e\u5c3a\u5bf8\u9884\u6d4b\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\u3002", "motivation": "\u7845\u5fae\u7ed3\u6784\u7684\u7cbe\u786e\u5b9a\u91cf\u5bf9\u4e8e\u5fae\u5c3a\u5ea6\u5236\u9020\u548c\u5668\u4ef6\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u7684SEM\u56fe\u50cf\u4eba\u5de5\u5206\u6790\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u7845\u573a\u53d1\u5c04\u5c16\u7aef\u7684\u4e13\u7528\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u81ea\u5b9a\u4e49CNN\u6a21\u578b\uff0c\u53ef\u4eceSEM\u56fe\u50cf\u4e2d\u63d0\u53d6\u5f62\u6001\u7279\u5f81\u5e76\u8fdb\u884c\u591a\u7c7b\u522b\u5206\u7c7b\u53ca\u5c3a\u5bf8\u9884\u6d4b\u3002", "result": "\u4e0e\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u5bf9\u6bd4\uff0cSiMiC\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u4e0e\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SiMiC\u4e3a\u7845\u5fae\u7ed3\u6784\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4fc3\u8fdb\u7ed3\u6784\u4e0e\u53d1\u5c04\u6027\u80fd\u7684\u5173\u8054\u7814\u7a76\uff0c\u5e76\u4e3a\u51b7\u9634\u6781\u4e0eSEM\u7535\u5b50\u6e90\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17203", "abs": "https://arxiv.org/abs/2601.17203", "authors": ["Scott Friedman", "Sonja Schmer-Galunder", "Anthony Chen", "Jeffrey Rye"], "title": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "comment": "7 pages, 5 figures. Presented at the First Workshop on Gender Bias in Natural Language Processing (GeBNLP 2019)", "summary": "Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u5206\u6790\u8bcd\u5d4c\u5165\u4e2d\u6027\u522b\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u8be5\u65b9\u6cd5\u5173\u8054\u5e76\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u5404\u9886\u57df\u7684\u6027\u522b\u5dee\u8ddd\uff0c\u901a\u8fc7\u63a8\u6587\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3b\u6d41NLP\u6a21\u578b\u5e38\u56e0\u8bad\u7ec3\u6587\u672c\u5b58\u5728\u6027\u522b\u4e0e\u79cd\u65cf\u504f\u89c1\u800c\u5907\u53d7\u4e89\u8bae\uff0c\u800c\u8fd9\u4e9b\u504f\u89c1\u672c\u8eab\u6216\u53ef\u53cd\u6620\u771f\u5b9e\u793e\u4f1a\u7684\u6027\u522b\u5dee\u8ddd\u3002\u672c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u8bcd\u5d4c\u5165\u4e2d\u7684\u504f\u89c1\uff0c\u6d1e\u5bdf\u80cc\u540e\u7684\u6587\u5316\u4e0e\u793e\u4f1a\u73b0\u8c61\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u8bcd\u5d4c\u5165\u4e2d\u6027\u522b\u504f\u89c1\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5c06\u83b7\u5f97\u7684\u504f\u89c1\u6307\u6807\u4e0e\u6559\u80b2\u3001\u653f\u6cbb\u3001\u7ecf\u6d4e\u3001\u5065\u5eb7\u7b49\u73b0\u5b9e\u9886\u57df\u7684\u6027\u522b\u5dee\u8ddd\u8fdb\u884c\u7edf\u8ba1\u5173\u8054\u3002\u7814\u7a76\u57fa\u4e8e2018\u5e74\u6db5\u76d6\u7f8e\u56fd51\u4e2a\u5730\u533a\u548c\u5168\u740399\u4e2a\u56fd\u5bb6\u7684\u63a8\u7279\u6570\u636e\uff0c\u7edf\u8ba1\u8bcd\u5d4c\u5165\u504f\u89c1\uff0c\u5e76\u5c06\u5176\u4e0e18\u9879\u56fd\u9645\u6027\u548c5\u9879\u7f8e\u56fd\u672c\u5730\u6027\u522b\u5dee\u8ddd\u6307\u6807\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u8bba\u6587\u53d1\u73b0\u8bcd\u5d4c\u5165\u4e2d\u7684\u6027\u522b\u504f\u89c1\u4e0e\u591a\u4e2a\u5b9e\u9645\u6027\u522b\u5dee\u8ddd\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u8fd9\u4e9b\u6307\u6807\u5728\u4e0d\u540c\u56fd\u5bb6\u548c\u5730\u533a\u5177\u6709\u8f83\u5f3a\u7684\u89c4\u5f8b\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u672c\u65b9\u6cd5\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63ed\u793a\u548c\u7406\u89e3\u8bad\u7ec3\u6587\u672c\u4e2d\u8574\u542b\u7684\u6027\u522b\u504f\u89c1\uff0c\u8fd8\u80fd\u7528\u6765\u91cf\u5316\u3001\u8868\u5f81\u5e76\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u5728\u6587\u5316\u80cc\u666f\u5efa\u6a21\u4e0e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17812", "categories": ["cs.RO", "cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17812", "abs": "https://arxiv.org/abs/2601.17812", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Bernardo Noronha", "Domenico Campolo"], "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction", "comment": null, "summary": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u5ef6\u8865\u507f\u521a\u5ea6\u4f30\u7b97\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u8fdc\u7a0b\u7269\u7406\u6cbb\u7597\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5728\u73b0\u6709\u4f30\u7b97\u57fa\u7840\u4e0a\u878d\u5408\u7f51\u7edc\u5ef6\u8fdf\u8865\u507f\u548c\u52a8\u6001\u504f\u5dee\u6ee4\u9664\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u7684\u8fdc\u7a0b\u4eba-\u4eba\u4ea4\u4e92\u7528\u4e8e\u7269\u7406\u6cbb\u7597\uff0c\u4f46\u56e0\u7f51\u7edc\u5ef6\u8fdf\u5bfc\u81f4\u89e6\u89c9\u53cd\u9988\u4fe1\u53f7\u9519\u4f4d\uff0c\u4f7f\u5f97\u611f\u77e5\u60a3\u8005\u521a\u5ea6\u53d8\u5f97\u56f0\u96be\u3002\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u65f6\u5ef6\uff0c\u521a\u5ea6\u4f30\u7b97\u8bef\u5dee\u968f\u5ef6\u8fdf\u52a0\u5927\uff0c\u4e9f\u9700\u514b\u670d\u8be5\u6311\u6218\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51c6\u9759\u6001\u5e73\u8861\u7684\u4ee3\u6570\u521a\u5ea6\u4f30\u7b97\u5668\uff0c\u5e76\u663e\u5f0f\u5bf9\u4e13\u5bb6\u8f93\u5165\u548c\u65b0\u4eba\u54cd\u5e94\u7684\u4fe1\u53f7\u8fdb\u884c\u65f6\u5e8f\u5bf9\u9f50\u3002\u968f\u540e\u7528\u5f52\u4e00\u5316\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08NWLS\uff09\u6ee4\u9664\u52a8\u6001\u504f\u5dee\uff0c\u63d0\u9ad8\u4f30\u7b97\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u91c7\u7528H-MAN\u5eb7\u590d\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u591a\u79cd\u65f6\u5ef6\u6761\u4ef6\u4e0b\u80fd\u660e\u663e\u4f18\u4e8e\u6807\u51c6\u521a\u5ea6\u4f30\u7b97\u5668\uff0c\u4f30\u7b97\u7cbe\u5ea6\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8fdc\u7a0b\u7269\u7406\u6cbb\u7597\u573a\u666f\u4e2d\u9ad8\u4fdd\u771f\u89e6\u89c9\u611f\u77e5\u548c\u53ef\u9760\u521a\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5bf9\u7f51\u7edc\u4e0b\u7684\u8fdc\u7a0b\u4eba-\u4eba\u4ea4\u4e92\u5eb7\u590d\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17049", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17049", "abs": "https://arxiv.org/abs/2601.17049", "authors": ["Christina Garcia", "Nhat Tan Le", "Taihei Fujioka", "Umang Dobhal", "Milyun Ni'ma Shoumi", "Thanh Nha Nguyen", "Sozo Inoue"], "title": "Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support", "comment": "14 pages, 7 figures, 3 tables. Summary paper for a coding challenge hosted in ISAS 2025", "summary": "This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86ISAS 2025\u4e3e\u529e\u7684\u201c\u8bc6\u522b\u672a\u77e5\u884c\u4e3a\uff1a\u57fa\u4e8e\u59ff\u6001\u6570\u636e\u7684\u5f02\u5e38\u884c\u4e3a\u8bc6\u522b\u6311\u6218\u8d5b\u201d\uff0c\u805a\u7126\u4e8e\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u81ea\u52a8\u68c0\u6d4b\u7279\u6b8a\u4eba\u7fa4\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "motivation": "\u5728\u53d1\u5c55\u969c\u788d\u4eba\u58eb\u7684\u7167\u62a4\u573a\u6240\uff0c\u81ea\u52a8\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\u5bf9\u4e8e\u4fdd\u969c\u5b89\u5168\u548c\u63d0\u5347\u7ba1\u7406\u6548\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u5982\u4f55\u57fa\u4e8e\u59ff\u6001\u6570\u636e\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u524d\u63d0\u4e0b\uff0c\u51c6\u786e\u68c0\u6d4b\u4e0d\u5e38\u89c1\u7684\u3001\u7a81\u53d1\u7684\u5f02\u5e38\u884c\u4e3a\u4ecd\u7136\u6781\u5177\u6311\u6218\u3002", "method": "\u8d5b\u4e8b\u63d0\u4f9b\u4e86\u4ece\u6a21\u62df\u573a\u666f\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u9aa8\u67b6\u5173\u952e\u70b9\u6570\u636e\uff0c\u8981\u6c42\u53c2\u8d5b\u961f\u4f0d\u6839\u636e\u8fd9\u4e9b\u6570\u636e\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38\u884c\u4e3a\uff0c\u5e76\u91c7\u7528LOSO\uff08\u7559\u4e00\u53d7\u8bd5\u8005\u6cd5\uff09\u5b9e\u73b0\u6cdb\u5316\u8bc4\u4f30\u3002\u56e2\u961f\u4f7f\u7528\u4e86\u4ece\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6837\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528macro-F1\u4f5c\u4e3a\u4e3b\u8981\u8bc4\u4f30\u6307\u6807\u4ee5\u5e94\u5bf9\u7c7b\u522b\u5931\u8861\u3002", "result": "\u6311\u6218\u8d5b\u663e\u793a\uff0c\u9762\u5bf9\u566a\u58f0\u5927\u3001\u7ef4\u5ea6\u4f4e\u7684\u6570\u636e\uff0c\u5c24\u5176\u7f55\u89c1\u4e14\u7a81\u53d1\u884c\u4e3a\u7684\u5efa\u6a21\u975e\u5e38\u56f0\u96be\u3002\u53c2\u8d5b\u65b9\u6cd5\u5728\u6548\u679c\u4e0a\u6709\u660e\u663e\u5dee\u8ddd\uff0c\u6574\u4f53\u6027\u80fd\u53d7\u9650\u4e8e\u6570\u636e\u672c\u8eab\u7684\u590d\u6742\u6027\u548c\u4e0d\u5e73\u8861\u6027\u3002", "conclusion": "\u672c\u6b21\u6311\u6218\u5f3a\u8c03\u5728\u884c\u4e3a\u5efa\u6a21\u4e2d\u6355\u6349\u65f6\u95f4\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5728\u533b\u7597\u5065\u5eb7\u548c\u884c\u4e3a\u76d1\u6d4b\u7b49\u793e\u4f1a\u8d23\u4efb\u578bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u7ecf\u9a8c\u548c\u6d1e\u89c1\u3002"}}
{"id": "2601.17212", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17212", "abs": "https://arxiv.org/abs/2601.17212", "authors": ["Saadat Hasan Khan", "Spencer Hong", "Jingyu Wu", "Kevin Lybarger", "Youbing Yin", "Erin Babinsky", "Daben Liu"], "title": "DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation", "comment": "Accepted to Findings of EACL 2026", "summary": "Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DF-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u68c0\u7d22\u73af\u8282\u5f15\u5165\u591a\u6837\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u95ee\u9898\u4e0a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRAG\u6280\u672f\u5728\u9762\u5bf9\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u95ee\u9898\u65f6\u8868\u73b0\u53d7\u9650\uff0c\u5e38\u89c4\u7684\u68c0\u7d22\u65b9\u5f0f\uff08\u5982\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff09\u867d\u7136\u80fd\u627e\u5230\u76f8\u5173\u5185\u5bb9\uff0c\u4f46\u5bb9\u6613\u5bfc\u81f4\u68c0\u7d22\u5185\u5bb9\u91cd\u590d\u3001\u4fe1\u606f\u56de\u6536\u7387\u964d\u4f4e\uff0c\u4ece\u800c\u5f71\u54cd\u95ee\u7b54\u80fd\u529b\u3002", "method": "\u672c\u6587\u521b\u65b0\u6027\u5730\u5c06\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\uff08Maximal Marginal Relevance, MMR\uff09\u5f15\u5165\u68c0\u7d22\u73af\u8282\uff0c\u786e\u4fdd\u6bcf\u6b21\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u7247\u6bb5\u65e2\u4e0e\u95ee\u9898\u9ad8\u5ea6\u76f8\u5173\uff0c\u53c8\u80fd\u5728\u5f7c\u6b64\u4e4b\u95f4\u4fdd\u6301\u591a\u6837\u6027\u3002DF-RAG\u80fd\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u591a\u6837\u6027\u7684\u7a0b\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u5148\u9a8c\u4fe1\u606f\u3002", "result": "DF-RAG\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578bQA\u57fa\u51c6\u4e0a\uff0c\u5c06F1\u5206\u6570\u63d0\u9ad8\u4e864-10\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u4f20\u7edfRAG\u4ee5\u53ca\u5176\u4ed6\u6210\u719f\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0cDF-RAG\u53d6\u5f97\u4e86\u7406\u8bba\u6700\u9ad8F1 18\u4e2a\u767e\u5206\u70b9\u589e\u76ca\u4e2d\u768491.3%\u3002", "conclusion": "\u5f15\u5165\u591a\u6837\u6027\u540e\u7684\u68c0\u7d22\u673a\u5236\u80fd\u591f\u6781\u5927\u63d0\u5347RAG\u7cfb\u7edf\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0cDF-RAG\u4e3a\u63a8\u7406\u578b\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u63d0\u5347\u65b9\u6848\u3002"}}
{"id": "2601.17815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17815", "abs": "https://arxiv.org/abs/2601.17815", "authors": ["Yves Inglin", "Jonas Frey", "Changan Chen", "Marco Hutter"], "title": "Less Is More: Scalable Visual Navigation from Limited Data", "comment": null, "summary": "Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u51e0\u4f55\u89c4\u5212\u5668\u751f\u6210\u7684\u5408\u6210\u8f68\u8ff9\u4e0e\u4e13\u5bb6\u793a\u8303\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\uff08LiMo\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u7684\u76ee\u6807\u5bfc\u5411\u89c6\u89c9\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d7\u9650\u4e8e\u9ad8\u6210\u672c\u548c\u4f4e\u591a\u6837\u6027\u7684\u4eba\u5de5\u793a\u8303\u6570\u636e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u7528\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u5f0f\u63d0\u5347\u5bfc\u822a\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9075\u5b88\u4eba\u7c7b\u504f\u597d\u548c\u793e\u4f1a\u89c4\u8303\u7684\u573a\u666f\u4e0b\u3002", "method": "\u5229\u7528\u7ecf\u5178\u51e0\u4f55\u89c4\u5212\u5668\u81ea\u52a8\u751f\u6210\u5408\u6210\u5bfc\u822a\u8f68\u8ff9\uff0c\u8865\u5145\u6709\u9650\u7684\u4e13\u5bb6\u793a\u8303\u6570\u636e\uff0c\u5c06\u8fd9\u4e9b\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565LiMo\uff0c\u4ece\u5355\u5f20RGB\u56fe\u50cf\u9884\u6d4b\u76ee\u6807\u5bfc\u5411\u7684SE(2)\u8f68\u8ff9\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u548c\u591a\u7ef4\u8bc4\u4f30\u5206\u6790\u3002", "result": "\u5728\u6709\u9650\u4e13\u5bb6\u793a\u8303\u57fa\u7840\u4e0a\uff0c\u7ed3\u5408\u5408\u6210\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5bfc\u822a\u7b56\u7565\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u6700\u7ec8\u6548\u679c\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u4e5f\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u5bfc\u822a\u4e0d\u5728\u4e8e\u5355\u7eaf\u589e\u52a0\u793a\u8303\u6570\u91cf\uff0c\u800c\u662f\u7b56\u7565\u6027\u5730\u589e\u52a0\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u3002\u901a\u8fc7\u53ef\u6269\u5c55\u3001\u5177\u8eab\u6027\u5f3a\u7684\u51e0\u4f55\u76d1\u7763\uff0c\u662f\u63d0\u5347\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u6570\u636e\u6548\u7387\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.17050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17050", "abs": "https://arxiv.org/abs/2601.17050", "authors": ["Hongjun An", "Yiliang Song", "Jiawei Shao", "Zhe Sun", "Xuelong Li"], "title": "Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence", "comment": "Initial Version, Pending Updates. We welcome any feedback and suggestions for improvement. Please feel free to contact us at an.hongjun@foxmail.com", "summary": "Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSP-VLM\uff08\u5355\u50cf\u7d20\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u9690\u79c1\u654f\u611f\u73af\u5883\u4e0b\u7684\u73af\u5883\u5b89\u5168\u76d1\u6d4b\uff0c\u5728\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u5bf9\u5f02\u5e38\u884c\u4e3a\u7684\u68c0\u6d4b\u548c\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u76d1\u63a7\u5728\u5982\u66f4\u8863\u5ba4\u3001\u536b\u751f\u95f4\u7b49\u9690\u79c1\u573a\u6240\u53d7\u5230\u6cd5\u5f8b\u548c\u9053\u5fb7\u9650\u5236\uff0c\u65e0\u6cd5\u5e94\u5bf9\u9738\u51cc\u3001\u9a9a\u6270\u7b49\u5371\u5bb3\u4e2a\u4f53\u548c\u516c\u5171\u5b89\u5168\u7684\u884c\u4e3a\u3002\u9700\u6c42\u8feb\u5207\uff0c\u4f46\u4fdd\u62a4\u9690\u79c1\u4e0e\u5b89\u5168\u76d1\u63a7\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u5229\u7528\u4f4e\u7ef4\u5355\u50cf\u7d20\u611f\u77e5\u65b9\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u5e76\u7406\u89e3\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\u3002\u5728\u91c7\u6837\u7387\u4e34\u754c\u70b9\u4ee5\u4e0b\uff0c\u5931\u771f\u4e25\u91cd\u5230\u65e0\u6cd5\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\uff0c\u4f46\u4ecd\u53ef\u63d0\u53d6\u884c\u4e3a\u8bed\u4e49\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u3001\u4eba\u6570\u7edf\u8ba1\u548c\u6d3b\u52a8\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u5355\u50cf\u7d20\u611f\u77e5\u4e0b\uff0c\u4eba\u8138\u8bc6\u522b\u5f7b\u5e95\u5931\u6548\uff0c\u5b9e\u73b0\u4e86\u672c\u5f81\u9690\u79c1\u4fdd\u62a4\uff1b\u540c\u65f6\uff0cSP-VLM\u80fd\u591f\u7ef4\u6301\u9ad8\u6c34\u5e73\u7684\u884c\u4e3a\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002\u4f18\u5316\u540e\uff0c\u627e\u5230\u8eab\u4efd\u53ef\u4fdd\u62a4\u4e14\u53ef\u884c\u7684\u91c7\u6837\u7387\u533a\u95f4\u3002", "conclusion": "SP-VLM\u53ef\u4f5c\u4e3a\u9690\u79c1\u654f\u611f\u73af\u5883\u4e0b\u7684\u5b89\u5168\u884c\u4e3a\u76d1\u6d4b\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b89\u5168\u9700\u6c42\u5e73\u8861\uff0c\u4e3a\u5408\u4e4e\u4eba\u6743\u7684\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.17223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17223", "abs": "https://arxiv.org/abs/2601.17223", "authors": ["Massimiliano Pronesti", "Anya Belz", "Yufang Hou"], "title": "Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning", "comment": null, "summary": "Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVerifiable Process Reward Models\uff08VPRMs\uff09\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u76d1\u7763\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5728\u533b\u5b66\u8bc1\u636e\u5408\u6210\u4e2d\u7684\u504f\u501a\u8bc4\u4f30\u4efb\u52a1\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709LLM\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u4ec5\u5728\u6700\u7ec8\u7ed3\u679c\u5c42\u9762\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u800c\u4e2d\u95f4\u8fc7\u7a0b\u7684\u76d1\u7763\u65b9\u5f0f\u4f9d\u8d56\u795e\u7ecf\u8bc4\u5224\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u6613\u53d7\u504f\u89c1\u548c\u5956\u52b1\u89c4\u907f\u5f71\u54cd\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\uff0c\u5f00\u53d1\u4e00\u79cd\u53ef\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u8fdb\u884c\u786e\u5b9a\u6027\u3001\u89c4\u5219\u5316\u9a8c\u8bc1\u7684\u673a\u5236\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51faVPRMs\uff0c\u901a\u8fc7\u5c06\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7528\u53ef\u6267\u884c\u89c4\u5219\uff08\u5982\u6d41\u7a0b\u56fe\u6216\u51c6\u5219\uff09\u7a0b\u5e8f\u5316\u6821\u9a8c\uff0c\u5b9e\u73b0\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u94fe\u5f0f\u76d1\u7763\u3002\u4ee5\u533b\u5b66\u8bc1\u636e\u5408\u6210\u7684\u504f\u501a\u98ce\u9669\u8bc4\u4f30\u4e3a\u4f8b\uff0c\u4f9d\u636e\u9886\u57df\u6307\u5357\u5b9a\u4e49\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u8f6c\u6362\u4e3a\u53ef\u68c0\u9a8c\u7684\u51b3\u7b56\u8def\u5f84\uff0c\u8bad\u7ec3LLM\u4f9d\u636e\u8fd9\u4e9b\u8def\u5f84\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cVPRMs\u751f\u6210\u7684\u63a8\u7406\u4e25\u683c\u7b26\u5408\u9886\u57df\u89c4\u5219\uff0c\u6b65\u9aa4\u51b3\u7b56\u4e0e\u6700\u7ec8\u6807\u7b7e\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002F1\u5206\u6570\u6700\u9ad8\u8f83SOTA\u63d0\u534720%\uff0c\u6bd4\u4ec5\u7528\u7ed3\u679c\u9a8c\u8bc1\u7684RL\u65b9\u6cd5\u9ad8\u51fa6.5%\uff1b\u8bc1\u636e\u57fa\u7840\u6027\u4e0e\u903b\u8f91\u4e00\u81f4\u6027\u4e5f\u6709\u660e\u663e\u589e\u957f\u3002", "conclusion": "VPRMs\u4e3aLLM\u63a8\u7406\u76d1\u7763\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u900f\u660e\u5ea6\u3001\u9ad8\u4e00\u81f4\u6027\u7684\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u6fc0\u52b1\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u51b3\u7b56\u8def\u5f84\u660e\u786e\u3001\u53ef\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17991", "abs": "https://arxiv.org/abs/2601.17991", "authors": ["Roman Akinshin", "Elizaveta Lopatina", "Kirill Bogatikov", "Nikolai Kiz", "Anna V. Makarova", "Mikhail Lebedev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Valerii Kangler"], "title": "NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408sEMG\u4e0e\u89c6\u89c9\u6ce8\u89c6\u7cfb\u7edf\u7684\u795e\u7ecf\u5f62\u6001\u4e0a\u80a2\u5047\u80a2\u63a7\u5236\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u808c\u7535\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u7684\u4e0a\u80a2\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u5728\u80fd\u6548\u3001\u51c6\u786e\u6027\u4e0e\u9002\u5e94\u590d\u6742\u751f\u6d3b\u73af\u5883\u65b9\u9762\u4ecd\u6709\u9650\u3002\u5982\u4f55\u63d0\u9ad8\u5047\u80a2\u63a7\u5236\u7684\u667a\u80fd\u6027\u3001\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\uff0c\u662f\u8be5\u9886\u57df\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5229\u7528\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\uff08AltAi\uff09\u7684\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u5b9e\u65f6\u5206\u7c7bsEMG\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u4e0e\u89c6\u89c9\u76f8\u673a\u8bc6\u522b\u7528\u6237\u5173\u6ce8\u7684\u76ee\u6807\u7269\u4f53\u3002\u5f53\u68c0\u6d4b\u5bf9\u8c61\u540e\uff0c\u7cfb\u7edf\u4ec5\u4ece\u4e0e\u5f53\u524d\u7269\u4f53\u76f8\u5173\u7684\u4e09\u79cd\u624b\u52bf\u4e2d\u8fdb\u884c\u8bc6\u522b\u51b3\u7b56\u3002\u8be5\u65b9\u6848\u4e0e\u65e2\u6709GPU\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u529f\u8017\u5927\u5e45\u964d\u4f4e\u3002", "result": "\u5728\u516d\u79cd\u5047\u80a2\u4f7f\u7528\u8005\u7684\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u8fbe\u5230\u4e86\u4e0e\u76ee\u524d\u4e3b\u6d41\u808c\u7535\u63a5\u53e3\u76f8\u5f53\u7684\u7a33\u5065\u8bc6\u522b\u7387\u3002\u5c06\u8bc6\u522b\u8303\u56f4\u9650\u5236\u4e3a\u4e0e\u7269\u4f53\u76f8\u5173\u7684\u4e09\u79cd\u624b\u52bf\u540e\uff0c\u51c6\u786e\u7387\u63d0\u5347\u81f3\u7ea695%\uff0c\u540c\u65f6\u6392\u9664\u5371\u9669\u6216\u4e0d\u5408\u7406\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u795e\u7ecf\u5f62\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u5177\u5907\u80fd\u6548\u9ad8\u3001\u53ef\u7a7f\u6234\u3001\u53ef\u9760\u6027\u597d\u7684\u4f18\u70b9\uff0c\u6709\u671b\u63d0\u5347\u4e0a\u80a2\u5047\u80a2\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.17053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17053", "abs": "https://arxiv.org/abs/2601.17053", "authors": ["Shuhao Que", "Dieuwke van Dartel", "Ilse Heeringa", "Han Hegeman", "Miriam Vollenbroek-Hutten", "Ying Wang"], "title": "Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults", "comment": "This paper has been submitted to Nordic Conference on Digital Health and Wireless Solutions 2026, currently under review", "summary": "Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u7cfb\u7edf\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u8001\u5e74\u9acb\u90e8\u9aa8\u6298\u5eb7\u590d\u60a3\u8005\u7684\u6d3b\u52a8\u7c7b\u578b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u6d3b\u52a8\uff08\u5982\u4f53\u4f4d\u8f6c\u79fb\uff09\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u4e34\u5e8a\u5bf9\u8001\u5e74\u9acb\u90e8\u9aa8\u6298\u5eb7\u590d\u671f\u7684\u8eab\u4f53\u6d3b\u52a8\u91cf\u7f3a\u4e4f\u91cf\u5316\uff0c\u73b0\u6709\u5546\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u4e3b\u8981\u9488\u5bf9\u4e2d\u9752\u5e74\uff0c\u96be\u4ee5\u5e94\u5bf9\u8001\u5e74\u4eba\u6b65\u6001\u7f13\u6162\u548c\u591a\u53d8\u7684\u7279\u70b9\uff0c\u5bfc\u81f4\u76d1\u6d4b\u51c6\u786e\u6027\u4f4e\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e8624\u540d80\u5c81\u4ee5\u4e0a\u5065\u5eb7\u8001\u5e74\u4eba\uff0c\u5728\u4f69\u6234\u8170\u90e8\u548c\u5927\u817f\u5904\u52a0\u901f\u5ea6\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u62df\u65e5\u5e38\u751f\u6d3b\u73af\u5883\uff0c\u5b8c\u6210\u591a\u79cd\u6d3b\u52a8\uff0c\u5e76\u91c7\u7528leave-one-subject-out\u4ea4\u53c9\u9a8c\u8bc1\u6765\u8bc4\u4f30\u7cfb\u7edf\u9c81\u68d2\u6027\u3002\u5f15\u5165\u4e86\u5408\u6210\u6570\u636e\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u7279\u5f81\u5e72\u9884\u6a21\u578b\uff08FIM\uff09\u3002", "result": "FIM\u7cfb\u7edf\u5728\u591a\u9879\u6d3b\u52a8\u8bc6\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6b65\u884c\u3001\u7ad9\u7acb\u3001\u5750\u4e0b\u3001\u8eba\u4e0b\u53ca\u4f53\u4f4d\u8f6c\u79fb\u4e94\u7c7b\u6d3b\u52a8\u4e2d\uff0cF1\u5f97\u5206\u5206\u522b\u4e3a0.896\u30010.927\u30010.997\u30010.937\u548c0.816\u3002\u7279\u522b\u662f\u5728\u4f53\u4f4d\u8f6c\u79fb\u68c0\u6d4b\u4e0a\uff0cFIM\u8f83\u65e0\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u521d\u6b65\u8bc1\u5b9e\u4e86\u5728\u8001\u5e74\u4eba\u4e2d\u5b9e\u73b0\u9ad8\u9c81\u68d2\u6027\u6d3b\u52a8\u8bc6\u522b\u7684\u53ef\u884c\u6027\uff0c\u4f46\u8fd8\u9700\u5728\u5b9e\u9645\u9acb\u90e8\u9aa8\u6298\u60a3\u8005\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.17226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17226", "abs": "https://arxiv.org/abs/2601.17226", "authors": ["David Y. Liu", "Xanthe Muston", "Aditya Joshi", "Sebastian Sequoiah-Grayson"], "title": "Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation", "comment": "8 Pages, 6 figures", "summary": "Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4ee5\u5f3a\u5316\u5b66\u4e60\uff08d-RLAIF\uff09\u66ff\u4ee3\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u63d0\u5347\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u7684\u591a\u6837\u6027\u4e0e\u4e0e\u4eba\u7c7b\u53d9\u4e8b\u4e60\u60ef\u7684\u5951\u5408\u5ea6\u3002", "motivation": "\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u6709\u9650\u7684\u2018\u6807\u51c6\u7b54\u6848\u2019\uff0c\u96be\u4ee5\u5168\u9762\u8861\u91cf\u548c\u751f\u6210\u66f4\u8d34\u8fd1\u4eba\u7c7b\u521b\u4f5c\u7684\u6545\u4e8b\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u5408\u9002\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6a21\u578b\u8f93\u51fa\u3002", "method": "\u8bba\u6587\u57fa\u4e8eTodorov\u53d9\u4e8b\u5747\u8861\u7406\u8bba\uff0c\u63d0\u51fa\u4e00\u5957\u8bc4\u5224\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u8d28\u91cf\u7684\u539f\u5219\u3002\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff087B\u548c14B\uff09\u4f5c\u4e3a\u8bc4\u5ba1\uff0c\u901a\u8fc7\u8fd9\u4e9b\u539f\u5219\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u5956\u52b1\uff0c\u5e76\u91c7\u7528Gemini-3-Flash\u5bf9\u6a21\u578b\u6545\u4e8b\u4e0e\u4eba\u7c7b\u6545\u4e8b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cd-RLAIF\u65b9\u6cd5\u751f\u6210\u7684\u6545\u4e8b\u66f4\u52a0\u591a\u6837\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u53d9\u4e8b\u89c4\u8303\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u6d4b\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u4e3b\u89c2\u6027\u8f83\u5f3a\u7684\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u524d\u666f\u7684\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u8f93\u51fa\u66f4\u5177\u4eba\u7c7b\u98ce\u683c\u7684\u6587\u672c\u3002"}}
{"id": "2601.18121", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18121", "abs": "https://arxiv.org/abs/2601.18121", "authors": ["Byeonggyeol Choi", "Woojin Oh", "Jongwoo Lim"], "title": "Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization", "comment": "13 pages, 7 figures", "summary": "Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.\n  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.\n  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u624b\u90e8\u64cd\u4f5c\u6570\u636e\u7269\u7406\u53ef\u6267\u884c\u6027\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u73b0\u6709\u89c6\u89c9\u5bf9\u9f50\u8f68\u8ff9\u7684\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u624b\u90e8\u64cd\u4f5c\u6570\u636e\u96c6\u5982DexYCB\u548cHO3D\u5728\u89c6\u89c9\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u751f\u6210\u7684\u4ea4\u4e92\u8f68\u8ff9\u5728\u7269\u7406\u4eff\u771f\u4e2d\u5e38\u8868\u73b0\u51fa\u7a7f\u900f\u3001\u5931\u7a33\u7b49\u4e0d\u5408\u73b0\u5b9e\u7684\u60c5\u51b5\uff0c\u5f71\u54cd\u4e86\u4e0b\u6e38\u5f3a\u5316\u5b66\u4e60\u7b49\u4efb\u52a1\u3002\u6025\u9700\u628a\u8fd9\u4e9b\u89c6\u89c9\u6f14\u793a\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86simulation-in-the-loop\uff08\u4eff\u771f\u95ed\u73af\uff09\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u4fee\u6b63\u95ee\u9898\u5efa\u6a21\u4e3a\u9ed1\u7bb1\u4f18\u5316\uff0c\u5c06\u624b\u90e8\u8fd0\u52a8\u53c2\u6570\u5316\u4e3a\u4f4e\u7ef4\u6837\u6761\u66f2\u7ebf\u5e76\u7ed3\u5408\u7a00\u758f\u5173\u952e\u5e27\uff0c\u901a\u8fc7CMA-ES\u8fd9\u4e00\u5f3a\u5927\u7684\u65e0\u68af\u5ea6\u4f18\u5316\u5668\uff0c\u76f4\u63a5\u4ee5\u9ad8\u4fdd\u771f\u7269\u7406\u5f15\u64ce\u53cd\u9988\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u517c\u987e\u7269\u7406\u6210\u529f\u4e0e\u4e0e\u539f\u59cb\u793a\u8303\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u4e0eMANIPTRANS\u7b49\u5f53\u524d\u6280\u672f\u76f8\u6bd4\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u91cd\u653e\u65f6\u83b7\u5f97\u66f4\u4f4e\u7684\u624b\u548c\u7269\u4f53\u59ff\u6001\u8bef\u5dee\uff0c\u66f4\u51c6\u786e\u5730\u8fd8\u539f\u4e86\u624b-\u7269\u7269\u7406\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u7528\u4e14\u9ad8\u6548\u5730\u5c06\u89c6\u89c9\u5bf9\u9f50\u7684\u624b\u90e8\u793a\u8303\u6570\u636e\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u6267\u884c\u8f68\u8ff9\uff0c\u4e3a\u9ad8\u4fdd\u771f\u6570\u636e\u7684\u751f\u6210\u548c\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17056", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17056", "abs": "https://arxiv.org/abs/2601.17056", "authors": ["Zahra Vaseqi", "James Clark"], "title": "Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring", "comment": null, "summary": "Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faEgo4OOD\u6570\u636e\u96c6\u4f5c\u4e3a\u5934\u6234\u89c6\u89d2\u89c6\u9891\u9886\u57df\u6cdb\u5316\u7684\u57fa\u51c6\uff0c\u805a\u7126\u4e8e\u5ea6\u91cf\u53ef\u63a7\u7684\u5171\u53d8\u91cf\u53d8\u5316\uff0c\u5e76\u51cf\u5c11\u6982\u5ff5\u6f02\u79fb\uff0c\u4ee5\u4fbf\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u5171\u53d8\u91cf\u53d8\u5316\u6307\u6807\u53ca\u72ec\u7acb\u4e8c\u5206\u7c7b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5bf9\u5171\u53d8\u91cf\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8f83\u4f4e\u53c2\u6570\u91cf\u4e0b\u8fbe\u5230\u4e0e\u4e3b\u6d41\u65b9\u6cd5\u76f8\u5f53\u7684\u8868\u73b0\u3002", "motivation": "\u5934\u6234\u89c6\u89d2\u89c6\u9891\u7684\u52a8\u4f5c\u8bc6\u522b\u5728\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u540c\u4e00\u7c7b\u522b\u5185\u7a7a\u95f4-\u65f6\u95f4\u53d8\u5316\u5927\u3001\u7279\u5f81\u5206\u5e03\u6781\u5ea6\u4e0d\u5747\u8861\u4ee5\u53ca\u52a8\u4f5c\u4e0e\u73af\u5883\u95f4\u5f3a\u76f8\u5173\u6027\u3002\u73b0\u6709\u57fa\u51c6\u6df7\u5408\u4e86\u5171\u53d8\u91cf\u53d8\u5316\u4e0e\u6982\u5ff5\u6f02\u79fb\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1\uff09\u6784\u5efaEgo4OOD\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d68\u4e2a\u5730\u7406\u57df\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5206\u7c7b\u4ee5\u51cf\u5c0f\u6982\u5ff5\u6f02\u79fb\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e\u805a\u7c7b\u7684\u5171\u53d8\u91cf\u53d8\u5316\u91cf\u5316\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u57df\u95f4\u96be\u5ea6\uff1b3\uff09\u63d0\u51fa\u4e00\u5bf9\u591a\u4e8c\u5206\u7c7b\u76ee\u6807\uff0c\u5c06\u591a\u7c7b\u522b\u8bc6\u522b\u8f6c\u4e3a\u4e00\u7cfb\u5217\u72ec\u7acb\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u4ee5\u964d\u4f4e\u7c7b\u522b\u95f4\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u91c7\u7528\u6240\u63d0\u51fa\u7684\u4e8c\u5206\u7c7b\u8bad\u7ec3\u65b9\u6cd5\u548c\u8f7b\u91cf\u4e24\u5c42\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u5728Argo1M\u548cEgo4OOD\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u53ef\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\u3001\u65e0\u9700\u591a\u6a21\u6001\u4fe1\u606f\u3002", "conclusion": "\u53d7\u63a7\u5316\u7684\u57fa\u51c6\u548c\u91cf\u5316\u7684\u57df\u96be\u5ea6\u6307\u6807\u5bf9\u5934\u6234\u89c6\u89d2\u89c6\u9891\u7684\u8de8\u57df\u6cdb\u5316\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51fa\u7684Ego4OOD\u548c\u8bad\u7ec3\u7b56\u7565\u80fd\u66f4\u51c6\u786e\u8ba4\u8bc6\u57df\u95f4\u5171\u53d8\u91cf\u6f02\u79fb\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4fc3\u8fdbOOD\uff08\u5206\u5e03\u5916\uff09\u6cdb\u5316\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17230", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17230", "abs": "https://arxiv.org/abs/2601.17230", "authors": ["Akshith Reddy Putta", "Jacob Devasier", "Chengkai Li"], "title": "CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval", "comment": null, "summary": "Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCaseFacts\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u9a8c\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u6a21\u578b\u5728\u7f8e\u6700\u9ad8\u6cd5\u9662\u5224\u4f8b\u6cd5\u5f8b\u9886\u57df\u7684\u8868\u73b0\uff0c\u5176\u4efb\u52a1\u662f\u9a8c\u8bc1\u4ee5\u901a\u4fd7\u8868\u8ff0\u63d0\u51fa\u7684\u6cd5\u5f8b\u4e3b\u5f20\u7684\u771f\u5b9e\u6027\u3002\u4f5c\u8005\u53d1\u73b0\uff0c\u5f53\u524d\u5927\u6a21\u578b\u5bf9\u6b64\u7c7b\u590d\u6742\u6cd5\u5f8b\u95ee\u9898\u4ecd\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u4ee5\u5f80\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7814\u7a76\u96c6\u4e2d\u5728\u9759\u6001\u9886\u57df\u7684\u5e38\u8bc6\u6838\u67e5\uff0c\u5ffd\u89c6\u4e86\u5982\u6cd5\u5f8b\u8fd9\u6837\u9ad8\u98ce\u9669\u3001\u77e5\u8bc6\u4e0d\u65ad\u6f14\u5316\u4e14\u6280\u672f\u95e8\u69db\u9ad8\u7684\u9886\u57df\u3002\u6cd5\u5f8b\u9886\u57df\u4e8b\u5b9e\u6838\u67e5\u5c24\u5177\u6311\u6218\uff0c\u56e0\u4e3a\u9700\u8981\u8de8\u8d8a\u901a\u4fd7\u8868\u8fbe\u548c\u4e13\u4e1a\u6cd5\u7406\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5e76\u51c6\u786e\u5904\u7406\u5224\u4f8b\u65f6\u6548\u6027\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86CaseFacts\u6570\u636e\u96c6\uff0c\u5305\u542b6294\u6761\u6cd5\u5f8b\u4e3b\u5f20\uff08\u5206\u4e3a\u5df2\u652f\u6301\u3001\u88ab\u9a73\u56de\u3001\u5df2\u63a8\u7ffb\u4e09\u7c7b\uff09\uff0c\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4e13\u5bb6\u5224\u4f8b\u6458\u8981\u4e2d\u81ea\u52a8\u751f\u6210\u4e3b\u5f20\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u542f\u53d1\u65b9\u6cd5\u9ad8\u6548\u7b5b\u67e5\u590d\u6742\u7684\u63a8\u7ffb\u6848\u4f8b\uff0c\u8fd8\u5206\u6790\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u542f\u7528\u7f51\u9875\u68c0\u7d22\u540e\u7684\u6838\u67e5\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u6cd5\u5f8b\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u7136\u6709\u9650\u3002\u5f53\u589e\u52a0\u4e0d\u53d7\u9650\u7684\u7f51\u9875\u68c0\u7d22\u8f85\u52a9\u65f6\uff0c\u6a21\u578b\u6548\u679c\u53cd\u800c\u4e0b\u964d\uff0c\u56e0\u4e3a\u68c0\u7d22\u5230\u7684\u8d44\u6599\u542b\u6709\u566a\u58f0\u4e14\u7f3a\u4e4f\u6743\u5a01\u6027\u3002", "conclusion": "CaseFacts\u4e3a\u6cd5\u5f8b\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u91cd\u8981\u8bc4\u6d4b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u8de8\u8bed\u4e49\u5c42\u3001\u65f6\u95f4\u654f\u611f\u7684\u6cd5\u5f8b\u4e3b\u5f20\u6838\u67e5\u5bf9\u73b0\u6709\u6a21\u578b\u7684\u6311\u6218\u3002\u8be5\u6570\u636e\u96c6\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.18289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18289", "abs": "https://arxiv.org/abs/2601.18289", "authors": ["Jialong Li", "Zhenguo Wang", "Tianci Wang", "Maj Stenmark", "Volker Krueger"], "title": "Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation", "comment": "HRI 2026", "summary": "Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports \"Side-by-Side\" and \"Mirror\" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.", "AI": {"tldr": "Quest2ROS2\u662f\u4e00\u4e2a\u5f00\u6e90\u7684ROS2\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u624b\u8fdc\u7a0b\u64cd\u4f5c\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u76f8\u5bf9\u8fd0\u52a8\u63a7\u5236\u7a81\u7834\u7a7a\u95f4\u5c40\u9650\uff0c\u63d0\u5347\u6570\u636e\u91c7\u96c6\u6548\u7387\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709VR\u63a7\u5236\u673a\u5668\u4eba\u6846\u67b6\u5982Quest2ROS\u5728\u64cd\u4f5c\u8303\u56f4\u548c\u4f7f\u7528\u4fbf\u6377\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u6570\u636e\u91c7\u96c6\u548c\u53cc\u624b\u64cd\u4f5c\u573a\u666f\u4e2d\uff0c\u96be\u4ee5\u6ee1\u8db3\u7075\u6d3b\u3001\u76f4\u89c2\u548c\u9ad8\u6548\u7684\u64cd\u4f5c\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u59ff\u6001\uff0c\u5229\u7528VR\u624b\u67c4\u59ff\u6001\u53d8\u5316\u6765\u9a71\u52a8\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u76f4\u89c2\u4e14\u4e0d\u53d7\u7a7a\u95f4\u9650\u5236\u7684\u64cd\u4f5c\u3002\u540c\u65f6\uff0c\u6846\u67b6\u96c6\u6210\u4e86\u5b9e\u65f6\u53ef\u89c6\u5316\u3001\u4fbf\u6377\u5939\u722a\u63a7\u5236\u548c\u6682\u505c/\u91cd\u7f6e\u7b49\u529f\u80fd\uff0c\u5e76\u652f\u6301\u201c\u5e76\u6392\u201d\u548c\u201c\u955c\u50cf\u201d\u4e24\u79cd\u64cd\u4f5c\u6a21\u5f0f\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "result": "\u65b0\u6846\u67b6\u7a81\u7834\u4e86\u64cd\u4f5c\u7a7a\u95f4\u7684\u9650\u5236\uff0c\u901a\u8fc7\u5404\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7075\u6d3b\u90e8\u7f72\u548c\u9ad8\u6613\u7528\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u591a\u5e73\u53f0\u3001\u591a\u573a\u666f\u4e0b\u7684\u53cc\u624b\u64cd\u4f5c\u9700\u6c42\u3002", "conclusion": "Quest2ROS2\u5927\u5e45\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8fdc\u7a0b\u53cc\u624b\u64cd\u4f5c\u7684\u76f4\u89c2\u6027\u3001\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u6570\u636e\u91c7\u96c6\u7b49\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4ee5\u5f00\u6e90\u65b9\u5f0f\u53d1\u5e03\uff0c\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.17062", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17062", "abs": "https://arxiv.org/abs/2601.17062", "authors": ["Robert M. Belcher", "Brendan C. Degryse", "Leonard R. Kosta", "Christopher J. Lowrance"], "title": "A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing", "comment": "Presented at the 2025 MIT Undergraduate Research Technology Conference (URTC)", "summary": "Adjusting rifle sights, a process commonly called \"zeroing,\" requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7aef\u5230\u7aef\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5c04\u51fb\u201c\u5f52\u96f6\u201d\u8fc7\u7a0b\u4e2d\u7684\u81ea\u52a8\u5f39\u5b54\u68c0\u6d4b\u548c\u8fed\u4ee3\u8ffd\u8e2a\uff0c\u6781\u5927\u63d0\u5347\u68c0\u6d4b\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5c04\u51fb\u5f52\u96f6\u9700\u8981\u4eba\u5de5\u8bc6\u522b\u4e0d\u540c\u8fed\u4ee3\u7684\u5f39\u5b54\uff0c\u8fc7\u7a0b\u6162\u4e14\u6613\u9519\uff0c\u5e0c\u671b\u901a\u8fc7\u81ea\u52a8\u5316\u624b\u6bb5\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u4eba\u5de5\u5931\u8bef\u3002", "method": "\u7cfb\u7edf\u91c7\u7528YOLOv8\u8fdb\u884c\u5c0f\u7269\u4f53\u68c0\u6d4b\uff0c\u5229\u7528IoU\u5206\u6790\u533a\u5206\u4e0d\u540c\u8fed\u4ee3\u4ea7\u751f\u7684\u5f39\u5b54\uff0c\u5e76\u901a\u8fc7\u5220\u51cf\u76ee\u6807\u7684\u65b0\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6a21\u62df\u771f\u5b9e\u5c04\u51fb\u5e8f\u5217\u3002\u540c\u65f6\uff0c\u501f\u52a9ORB\u7b97\u6cd5\u7684\u89c6\u89d2\u6821\u6b63\u6807\u51c6\u5316\u76ee\u6807\u56fe\u50cf\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "result": "\u5f39\u5b54\u68c0\u6d4b\u5e73\u5747\u7cbe\u5ea6\u8fbe\u523097.0%\uff0c\u5f39\u5b54\u5f52\u5c5e\u8fed\u4ee3\u51c6\u786e\u7387\u4e3a88.8%\u3002", "conclusion": "\u672c\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u5c04\u51fb\u5f52\u96f6\u8fc7\u7a0b\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\uff0c\u5e76\u6709\u671b\u5e94\u7528\u4e8e\u9700\u533a\u5206\u65f6\u95f4\u5e8f\u5217\u76f8\u4f3c\u7269\u4f53\u7684\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2601.17232", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17232", "abs": "https://arxiv.org/abs/2601.17232", "authors": ["Jacob Devasier", "Akshith Putta", "Qing Wang", "Alankrit Moses", "Chengkai Li"], "title": "Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data", "comment": null, "summary": "Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u7684\u6838\u67e5\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7684\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u5c0f\u578b\u3001\u4eba\u5de5\u6574\u7406\u7684\u8868\u683c\uff0c\u800c\u5ffd\u89c6\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u9ad8\u4f53\u91cf\u7ed3\u6784\u5316\u6570\u636e\u6838\u67e5\u7684\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e434\u4e2a\u590d\u6742OECD\u8868\u683c\uff0c\u5229\u7528\u516d\u79cd\u8bed\u4e49\u6846\u67b6\u7a0b\u5e8f\u5316\u9009\u53d6\u5173\u952e\u6570\u636e\u70b9\uff0c\u81ea\u52a8\u751f\u6210\u4e8678,503\u4e2a\u5408\u6210\u4e3b\u5f20\uff0c\u8986\u76d6\u82f1\u6587\u3001\u4e2d\u6587\u3001\u897f\u73ed\u7259\u6587\u548c\u5370\u5730\u8bed\u3002\u901a\u8fc7\u77e5\u8bc6\u63a2\u67e5\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u4e8b\u5b9e\u672a\u88ab\u5927\u6a21\u578b\u8bb0\u5fc6\uff0c\u9700\u7cfb\u7edf\u771f\u6b63\u68c0\u7d22\u4e0e\u63a8\u7406\u3002\u8fd8\u63d0\u4f9b\u4e86SQL\u751f\u6210\u7684\u57fa\u7ebf\u7cfb\u7edf\u8bc4\u4f30\u4efb\u52a1\u96be\u5ea6\u3002", "result": "\u5206\u6790\u8868\u660e\u8be5\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4ece\u8d85\u5927\u8868\u683c\u4e2d\u68c0\u7d22\u8bc1\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8bc1\u636e\u68c0\u7d22\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u6240\u63d0\u6570\u636e\u96c6\u4e3a\u771f\u5b9e\u4e16\u754c\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e8b\u5b9e\u6838\u67e5\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.18323", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18323", "abs": "https://arxiv.org/abs/2601.18323", "authors": ["Weishi Mi", "Yong Bao", "Xiaowei Chi", "Xiaozhu Ju", "Zhiyuan Qin", "Kuangzhi Ge", "Kai Tang", "Peidong Jia", "Shanghang Zhang", "Jian Tang"], "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion", "comment": null, "summary": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTool-Centric Inverse Dynamics Model\uff08TC-IDM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u89c6\u89c9\u2014\u8bed\u8a00\u2014\u884c\u52a8\u8303\u5f0f\u7684\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4ee5\u5de5\u5177\u8f68\u8ff9\u4e3a\u6838\u5fc3\u4e2d\u95f4\u8868\u5f81\uff0c\u6709\u6548\u8fde\u63a5\u4e86\u89c6\u89c9\u89c4\u5212\u4e0e\u5b9e\u9645\u7269\u7406\u63a7\u5236\uff0c\u5728\u591a\u6837\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8f83\u5f3a\u6cdb\u5316\u6027\u548c\u8f83\u9ad8\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u2014\u8bed\u8a00\u2014\u884c\u52a8\uff08VLA\uff09\u8303\u5f0f\u5f3a\u5927\u4f46\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6570\u636e\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u800c\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u867d\u7136\u53ef\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u652f\u6301\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u50cf\u7d20\u7ea7\u8ba1\u5212\u96be\u4ee5\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u5b58\u5728\u8868\u793a\u9e3f\u6c9f\u3002\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u96be\u9898\uff0c\u4f5c\u8005\u8bd5\u56fe\u5bfb\u627e\u4e00\u79cd\u66f4\u52a0\u5065\u58ee\u4e14\u5177\u5907\u6cdb\u5316\u80fd\u529b\u7684\u4e2d\u95f4\u8868\u5f81\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u9002\u7528\u8303\u56f4\u3002", "method": "\u4f5c\u8005\u63d0\u51faTC-IDM\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u89c6\u9891\uff0c\u5229\u7528\u5206\u5272\u4e0e3D\u8fd0\u52a8\u4f30\u8ba1\u4ece\u4e2d\u63d0\u53d6\u5de5\u5177\u70b9\u4e91\u8f68\u8ff9\uff0c\u4f5c\u4e3a\u6865\u6881\u8fde\u63a5\u89c6\u89c9\u89c4\u5212\u4e0e\u7269\u7406\u52a8\u4f5c\u3002\u6a21\u578b\u7ed3\u6784\u9488\u5bf9\u4e0d\u540c\u5de5\u5177\u5c5e\u6027\u8bbe\u8ba1\u89e3\u8026\u7684\u52a8\u4f5c\u5934\uff0c\u5c06\u8ba1\u5212\u8f68\u8ff9\u6620\u5c04\u4e3a6\u81ea\u7531\u5ea6\u672b\u7aef\u6267\u884c\u5668\u52a8\u4f5c\u53ca\u5bf9\u5e94\u63a7\u5236\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u201c\u8ba1\u5212\u2014\u7ffb\u8bd1\u201d\u8303\u5f0f\uff0c\u517c\u5bb9\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\uff0c\u5e76\u63d0\u5347\u4e86\u89c6\u89d2\u4e0d\u53d8\u6027\u53ca\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "result": "TC-IDM\u5728\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523061.11%\uff0c\u5176\u4e2d\u5e38\u89c4\u7b80\u5355\u4efb\u52a1\u4e3a77.7%\uff0c\u96f6\u6837\u672c\u53ef\u53d8\u5f62\u7269\u4f53\u4efb\u52a1\u4e3a38.46%\u3002\u6574\u4f53\u6548\u679c\u663e\u8457\u8d85\u8d8a\u73b0\u6709VLA\u8303\u5f0f\u7aef\u5230\u7aef\u65b9\u6cd5\u548c\u5176\u4ed6\u9006\u52a8\u529b\u5b66\u6a21\u578b\u3002", "conclusion": "TC-IDM\u6709\u529b\u586b\u8865\u4e86\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u89c6\u89c9\u89c4\u5212\u4e0e\u7269\u7406\u63a7\u5236\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4e0e\u65e0\u5148\u9a8c\u7684\u573a\u666f\u4e0b\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u63a8\u52a8\u901a\u7528\u578b\u5b9e\u4f53\u667a\u80fd\u4f53\u7684\u7814\u7a76\u548c\u5e94\u7528\u5e26\u6765\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17067", "abs": "https://arxiv.org/abs/2601.17067", "authors": ["Luozhou Wang", "Zhifei Chen", "Yihua Du", "Dongyu Yan", "Wenhang Ge", "Guibao Shen", "Xinli Xu", "Leyi Wu", "Man Chen", "Tianshuo Xu", "Peiran Ren", "Xin Tao", "Pengfei Wan", "Ying-Cong Chen"], "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics", "comment": null, "summary": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5168\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u8be5\u9886\u57df\u672a\u6765\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u7269\u7406\u4e00\u81f4\u6027\uff0c\u88ab\u8ba4\u4e3a\u662f\u6f5c\u5728\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u5b58\u5728\u201c\u65e0\u72b6\u6001\u201d\u89c6\u9891\u67b6\u6784\u4e0e\u4f20\u7edf\u201c\u6709\u72b6\u6001\u201d\u4e16\u754c\u6a21\u578b\u7406\u8bba\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4ee5\u72b6\u6001\u6784\u5efa\u4e0e\u52a8\u529b\u5b66\u5efa\u6a21\u4e3a\u6838\u5fc3\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u72b6\u6001\u6784\u5efa\u5206\u4e3a\u9690\u5f0f\u8303\u5f0f\uff08\u4e0a\u4e0b\u6587\u7ba1\u7406\uff09\u4e0e\u663e\u5f0f\u8303\u5f0f\uff08\u6f5c\u53d8\u91cf\u538b\u7f29\uff09\uff0c\u5e76\u5bf9\u52a8\u529b\u5b66\u5efa\u6a21\u4ece\u77e5\u8bc6\u6574\u5408\u548c\u67b6\u6784\u91cd\u5851\u89d2\u5ea6\u5206\u6790\u3002\u4f5c\u8005\u8fd8\u4e3b\u5f20\u8bc4\u4f30\u6807\u51c6\u5e94\u4ece\u89c6\u89c9\u903c\u771f\u5ea6\u8f6c\u5411\u7269\u7406\u6301\u7eed\u6027\u4e0e\u56e0\u679c\u63a8\u7406\u7b49\u529f\u80fd\u6027\u57fa\u51c6\u3002", "result": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u529f\u80fd\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u4e14\u660e\u786e\u4e86\u6301\u4e45\u6027\uff08\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u8bb0\u5fc6\u548c\u538b\u7f29\u4fdd\u771f\u5ea6\uff09\u548c\u56e0\u679c\u6027\uff08\u901a\u8fc7\u6f5c\u56e0\u5b50\u89e3\u8026\u548c\u63a8\u7406\u5148\u9a8c\u96c6\u6210\uff09\u662f\u4eca\u540e\u7814\u7a76\u7684\u5173\u952e\u524d\u6cbf\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u6307\u660e\u65b9\u5411\uff0c\u5373\u7531\u8ffd\u6c42\u89c6\u89c9\u903c\u771f\u8f6c\u5411\u6784\u5efa\u5f3a\u5065\u3001\u901a\u7528\u7684\u4e16\u754c\u6a21\u62df\u5668\u3002"}}
{"id": "2601.17277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17277", "abs": "https://arxiv.org/abs/2601.17277", "authors": ["Mohammad Rifqi Farhansyah", "Hanif Muhammad Zhafran", "Farid Adilazuarda", "Shamsuddeen Hassan Muhammad", "Maryam Ibrahim Mukhtar", "Nedjma Ousidhoum", "Genta Indra Winata", "Ayu Purwarianti", "Alham Fikri Aji"], "title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues", "comment": "preprint", "summary": "Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPingPong\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u771f\u5b9e\u591a\u65b9\u8bed\u7801\u8f6c\u6362\u5bf9\u8bdd\uff0c\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u7ec4\u5408\uff0c\u5f3a\u8c03\u5176\u81ea\u7136\u6027\u548c\u7ed3\u6784\u591a\u6837\u6027\uff0c\u5e76\u7528\u4e8e\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u8bc4\u6d4b\u73b0\u6709\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5168\u7403\u591a\u8bed\u4f7f\u7528\u8005\u5e7f\u6cdb\u8fdb\u884c\u8bed\u7801\u8f6c\u6362\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u73b0\u5b9e\u4ea4\u6d41\u7684\u590d\u6742\u6027\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5\u79cd\u8bed\u8a00\u7ec4\u5408\uff08\u542b\u90e8\u5206\u4e09\u8bed\u7ec4\u5408\uff09\u7684\u4eba\u7c7b\u64b0\u5199\u591a\u65b9\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u5bf9\u8bdd\u6d89\u53ca2-4\u4f4d\u53c2\u4e0e\u8005\uff0c\u62e5\u6709\u771f\u5b9e\u591a\u7ebf\u7a0b\u7ed3\u6784\u3002\u5bf9\u6bd4\u673a\u751f\u6210\u6570\u636e\uff0c\u5c55\u793a\u8be5\u6570\u636e\u96c6\u5728\u6d88\u606f\u957f\u5ea6\u3001\u8bf4\u8bdd\u4eba\u4e3b\u5bfc\u6027\u3001\u56de\u590d\u8ddd\u79bb\u7b49\u65b9\u9762\u66f4\u81ea\u7136\u548c\u591a\u6837\u3002\u57fa\u4e8e\u6b64\uff0c\u5b9a\u4e49\u4e86\u95ee\u7b54\u3001\u6458\u8981\u548c\u4e3b\u9898\u5206\u7c7b\u4e09\u9879\u4efb\u52a1\uff0c\u5e76\u7528SOTA\u6a21\u578b\u5728\u65b0\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u8bba\u662f\u4eba\u7c7b\u8fd8\u662fSOTA\u6a21\u578b\uff0c\u5728PingPong\u5bf9\u8bdd\u96c6\u4e0a\u7684\u4ee3\u7801\u5207\u6362\u8f93\u5165\u4e0b\u8868\u73b0\u90fd\u8f83\u4e3a\u6709\u9650\uff0c\u8868\u660e\u73b0\u6709NLP\u7cfb\u7edf\u5728\u5904\u7406\u771f\u5b9e\u591a\u8bed\u8bed\u7801\u8f6c\u6362\u95ee\u9898\u4e0a\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u663e\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u5e94\u5bf9\u591a\u8bed\u8bed\u7801\u8f6c\u6362\u65f6\u7684\u5c40\u9650\u6027\uff0c\u547c\u5401\u53d1\u5c55\u66f4\u5f3a\u5065\u7684NLP\u6a21\u578b\u4ee5\u9002\u5e94\u73b0\u5b9e\u591a\u8bed\u4ea4\u6d41\u7684\u590d\u6742\u9700\u6c42\u3002"}}
{"id": "2601.18442", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18442", "abs": "https://arxiv.org/abs/2601.18442", "authors": ["Hongyi Zhao", "Shuo Wang", "Qijie He", "Ziyuan Pu"], "title": "SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation", "comment": null, "summary": "Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578bSG-CADVLM\uff0c\u80fd\u57fa\u4e8e\u78b0\u649e\u62a5\u544a\u548c\u9053\u8def\u56fe\u81ea\u52a8\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u78b0\u649e\u4e8b\u4ef6\uff09\u6781\u4e3a\u7f55\u89c1\u4e14\u9a8c\u8bc1\u6210\u672c\u9ad8\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u591a\u6837\u6027\uff0c\u57fa\u4e8e\u5bf9\u6297\u7684\u751f\u6210\u65b9\u6cd5\u53c8\u5b58\u5728\u7269\u7406\u4e0d\u771f\u5b9e\u7684\u95ee\u9898\u3002\u5982\u4f55\u771f\u5b9e\u3001\u9ad8\u6548\u5730\u4ea7\u751f\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u7684\u9ad8\u98ce\u9669\u4ea4\u901a\u573a\u666f\uff0c\u662f\u6025\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSG-CADVLM\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u4e0e\u591a\u6a21\u6001\u8f93\u5165\uff08\u78b0\u649e\u62a5\u544a\u548c\u9053\u8def\u7f51\u56fe\uff09\u7ed3\u5408\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u51cf\u8f7b\u6a21\u578b\u5e7b\u89c9\u53ca\u5931\u771f\uff0c\u81ea\u52a8\u751f\u6210\u9053\u8def\u7ed3\u6784\u53ca\u8f66\u8f86\u8f68\u8ff9\u3002", "result": "SG-CADVLM\u751f\u6210\u5b89\u5168\u5173\u952e\u9ad8\u98ce\u9669\u573a\u666f\u7684\u6210\u529f\u7387\u8fbe84.4%\uff0c\u8fdc\u9ad8\u4e8e\u57fa\u51c6\u65b9\u6cd5\u768412.5%\uff0c\u63d0\u5347\u4e86469%\uff1b\u5e76\u80fd\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u7684\u6848\u4f8b\u3002", "conclusion": "SG-CADVLM\u6781\u5927\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6d4b\u8bd5\u4e2d\u9ad8\u98ce\u9669\u573a\u666f\u7684\u751f\u6210\u6548\u7387\u548c\u771f\u5b9e\u6027\uff0c\u6210\u4e3a\u5229\u7528\u4e8b\u6545\u62a5\u544a\u6a21\u62df\u548c\u9a8c\u8bc1\u667a\u80fd\u8f66\u8f86\u5b89\u5168\u6027\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002"}}
{"id": "2601.17071", "categories": ["cs.CV", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.17071", "abs": "https://arxiv.org/abs/2601.17071", "authors": ["Jisui Huang", "Andreas Alpers", "Ke Chen", "Na Lei"], "title": "Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances", "comment": "34 pages, 11 figures", "summary": "We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u5c42\u805a\u7c7b\u6846\u67b6\uff0c\u5728\u5b58\u5728\u660e\u663e\u975e\u5747\u5300\u6027\u7684\u60c5\u51b5\u4e0b\u4f9d\u7136\u8868\u73b0\u4f18\u79c0\u3002\u6838\u5fc3\u662f\u5c06\u50cf\u7d20\u5148\u5206\u7ec4\u4e3a\u8d85\u50cf\u7d20\uff0c\u7136\u540e\u7528\u4f18\u5316\u7684\u5206\u5e03\u95f4\u8ddd\u79bb\u8fdb\u884c\u5bf9\u8c61\u7ea7\u5408\u5e76\uff0c\u5b9e\u73b0\u7cbe\u51c6\u4e14\u9ad8\u6548\u7684\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u50cf\u975e\u5747\u5300\u6027\u548c\u590d\u6742\u5206\u5e03\u65f6\u6548\u679c\u6709\u9650\uff0c\u4e14\u5e38\u7528\u7684\u8d85\u50cf\u7d20\u5408\u5e76\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u989c\u8272\u5747\u503c\u8ddd\u79bb\uff0c\u96be\u4ee5\u523b\u753b\u5206\u5e03\u7279\u5f81\uff0c\u7cbe\u5ea6\u53d7\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u66f4\u5f3a\u8868\u5f81\u529b\u7684\u5206\u5e03\u8ddd\u79bb\u514b\u670d\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u63d0\u9ad8\u5206\u6790\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u7b2c\u4e00\u6b65\uff0c\u5c06\u50cf\u7d20\u901a\u8fc7\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u5206\u914d\u95ee\u9898\u5206\u7ec4\u6210\u8d85\u50cf\u7d20\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u770b\u4f5c\u79bb\u6563\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u95ee\u9898\u7684\u7279\u4f8b\uff1b\u7b2c\u4e8c\u6b65\uff0c\u5229\u7528\u8d85\u50cf\u7d20\u7684\u7ecf\u9a8c\u5206\u5e03\u95f4\u7684\u4e8c\u9636Wasserstein\u8ddd\u79bb\uff0c\u5728\u5168\u5c40\u8303\u56f4\u5185\u8d2a\u5a6a\u5408\u5e76\u8d85\u50cf\u7d20\u3002\u4e0d\u540c\u4e8e\u53ea\u8003\u8651\u989c\u8272\u5747\u503c\uff0c\u672c\u6587\u91c7\u7528\u5206\u5e03\u6027OT\u8ddd\u79bb\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u7ed3\u6784\u590d\u6742\u6216\u5b58\u5728\u5f3a\u975e\u5747\u5300\u6027\u7684\u56fe\u50cf\u4e0a\u5206\u5272\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u80fd\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e86\u5206\u5e03\u6027\u4e0e\u9ad8\u6548\u6027\u7684\u77db\u76fe\uff0c\u8bc1\u660e\u4e86\u5206\u5e03OT\u8ddd\u79bb\u5728\u56fe\u50cf\u5206\u5272\u805a\u7c7b\u4e2d\u7684\u4ef7\u503c\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.17284", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17284", "abs": "https://arxiv.org/abs/2601.17284", "authors": ["Yaokun Liu", "Yifan Liu", "Phoebe Mbuvi", "Zelin Li", "Ruichen Yao", "Gawon Lim", "Dong Wang"], "title": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering", "comment": "Accepted at The Web Conference 2026 (WWW 2026)", "summary": "The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided \"Clarify-Before-Answer\" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCV-MedBench\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u7814\u7a76\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u8f93\u5165\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u9ad8\u6548\u7684\u6f84\u6e05\u673a\u5236\uff0c\u63d0\u9ad8\u95ee\u7b54\u51c6\u786e\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u95ee\u7b54\u9886\u57df\u53d7\u5230\u8f93\u5165\u6b67\u4e49\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u56de\u7b54\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u964d\u4f4e\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u95ee\u9898\u7a81\u51fa\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5bf9\u5e94\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u4e3b\u52a8\u5e94\u5bf9\u8f93\u5165\u6b67\u4e49\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u533b\u7597\u667a\u80fd\u95ee\u7b54\u3002", "method": "\u4f5c\u8005\u5c06\u8f93\u5165\u6b67\u4e49\u4e0e\u4e0d\u53ef\u7ea6\u5316\u4e0d\u786e\u5b9a\u6027\uff08Aleatoric Uncertainty, AU\uff09\u76f8\u8054\u7cfb\uff0c\u5e76\u9996\u6b21\u63d0\u51faCV-MedBench\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8861\u91cf\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u8f93\u5165\u6b67\u4e49\u3002\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\uff0c\u53d1\u73b0AU\u53ef\u4ee5\u7ebf\u6027\u7f16\u7801\u5728LLM\u4e2d\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86AU-guided\u201c\u5148\u6f84\u6e05\u518d\u7b54\u590d\u201dFramework\uff0c\u52a0\u5165AU-Probe\u6a21\u5757\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u9690\u85cf\u72b6\u6001\u63a2\u6d4b\u8f93\u5165\u6b67\u4e49\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u591a\u6b21\u524d\u5411\u63a8\u7406\uff0c\u9ad8\u6548\u63d0\u5347\u95ee\u7b54\u6d41\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u5f00\u6e90LLM\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b0\u6846\u67b6\u5b9e\u73b0\u4e86\u5e73\u57479.48%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u533b\u7597\u95ee\u7b54\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u3001\u6846\u67b6\u548c\u6a21\u5757\u4e3a\u8f93\u5165\u6b67\u4e49\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u63d0\u5347\u533b\u7597\u76f8\u5173AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002\u76f8\u5173\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2601.18492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18492", "abs": "https://arxiv.org/abs/2601.18492", "authors": ["Zijun Li", "Shijie Li", "Zhenxi Zhang", "Bin Li", "Shoujun Zhou"], "title": "DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u65b9\u6cd5DV-VLN\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1\u8303\u5f0f\u63d0\u5347\u5bfc\u822a\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bed\u8a00\u9a71\u52a8\u7684\u5bfc\u822a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684VLN\u65b9\u6cd5\u591a\u91c7\u7528\u5355\u6b65\u51b3\u7b56\uff0c\u5bb9\u6613\u7531\u4e8e\u5c40\u90e8\u504f\u5dee\u548c\u4e2d\u95f4\u63a8\u7406\u4e0d\u5b8c\u5584\u5bfc\u81f4\u5bfc\u822a\u9519\u8bef\uff0c\u5e76\u4e14\u5728\u590d\u6742\u548c\u672a\u77e5\u73af\u5883\u4e2d\u53ef\u9760\u6027\u8f83\u4f4e\u3002", "method": "DV-VLN\u91c7\u7528\u751f\u6210-\u9a8c\u8bc1\uff08generate-then-verify\uff09\u8303\u5f0f\uff1a\u9996\u5148\u5bf9LLaMA-2\u4e3b\u5e72\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\uff0c\u8f93\u51fa\u7ed3\u6784\u5316\u7684\u5bfc\u822a\u601d\u7ef4\u94fe\uff1b\u7136\u540e\u901a\u8fc7True-False Verification\uff08TFV\uff09\u548cMasked-Entity Verification\uff08MEV\uff09\u4e24\u79cd\u4e92\u8865\u7684\u9a8c\u8bc1\u624b\u6bb5\u8bc4\u4f30\u5907\u9009\u52a8\u4f5c\uff0c\u5e76\u6574\u5408\u591a\u6b21\u6837\u672c\u7684\u9a8c\u8bc1\u7ed3\u679c\u8fdb\u884c\u6700\u7ec8\u52a8\u4f5c\u9009\u62e9\u548c\u6392\u5e8f\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728R2R\u3001RxR\uff08\u82f1\u8bed\u5b50\u96c6\uff09\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDV-VLN\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u76f4\u63a5\u9884\u6d4b\u548c\u4ec5\u91c7\u6837\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8bed\u8a00\u9a71\u52a8VLN\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u90e8\u5206\u6307\u6807\u4e0a\u63a5\u8fd1\u8de8\u6a21\u6001\u7cfb\u7edf\u3002", "conclusion": "DV-VLN\u6709\u6548\u514b\u670d\u4e86\u5355\u6b65\u51b3\u7b56\u5e26\u6765\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u5347\u4e86VLN\u5728\u672a\u77e5\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u662f\u5f53\u524d\u8bed\u8a00\u9a71\u52a8\u5bfc\u822a\u9886\u57df\u7684\u6709\u529b\u8fdb\u5c55\u3002"}}
{"id": "2601.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17088", "abs": "https://arxiv.org/abs/2601.17088", "authors": ["Rui-Yang Ju", "Jen-Shiun Chiang"], "title": "GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars", "comment": "IEEE VR 2026 Poster", "summary": "Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GlassesGB\uff0c\u4e00\u79cd\u80fd\u57283D\u5934\u50cf\u4e0a\u5b9e\u73b0\u4e2a\u6027\u5316\u773c\u955c\u5b9a\u5236\u7684\u65b0\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u57282D\u6216\u6a21\u677f\u57fa\u7840\u4e0a\u64cd\u4f5c\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u6234\u7cfb\u7edf\u4ec5\u80fd\u5728\u9884\u5b9a\u4e49\u6a21\u677f\u57fa\u7840\u4e0a\u4fee\u6539\uff0c\u7f3a\u4e4f\u7cbe\u7ec6\u3001\u7528\u6237\u9a71\u52a8\u76843D\u4e2a\u6027\u5316\u5b9a\u5236\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u5b9e\u73b0\u66f4\u8d34\u5408\u7528\u6237\u9700\u6c42\u76843D\u773c\u955c\u5b9a\u5236\uff0c\u4e3aVR\u865a\u62df\u5316\u8eab\u63d0\u4f9b\u66f4\u591a\u4e2a\u6027\u53ef\u80fd\u3002", "method": "\u8bba\u6587\u7ed3\u5408\u4e86GlassGAN\u76842D\u4e2a\u6027\u5316\u773c\u955c\u751f\u6210\u80fd\u529b\u4e0e3D Gaussian Blendshapes\u7684\u5934\u90e8\u91cd\u5efa\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86GlassesGB\u3002\u8be5\u6846\u67b6\u80fd\u591f\u5c062D\u751f\u6210\u7ed3\u679c\u4e0e3D\u5934\u6a21\u65e0\u7f1d\u7ed3\u5408\uff0c\u5b9e\u73b0\u53ef\u5b9a\u5236\u76843D\u773c\u955c\u6e32\u67d3\u3002", "result": "GlassesGB\u53ef\u4ee5\u57283D\u865a\u62df\u5934\u50cf\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u7684\u773c\u955c\u5efa\u6a21\uff0c\u5e76\u517c\u5bb9VR\u5e94\u7528\u573a\u666f\uff0c\u6548\u679c\u4f18\u4e8e\u53ea\u652f\u6301\u6a21\u677f\u4fee\u6539\u6216\u4ec52D\u5c55\u793a\u7684\u73b0\u6709\u6280\u672f\u3002\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "GlassesGB\u6846\u67b6\u6210\u529f\u5c062D\u751f\u6210\u4e0e3D\u91cd\u5efa\u878d\u5408\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u53ef\u5b9a\u5236\u76843D\u865a\u62df\u8bd5\u6234\uff0c\u6781\u5927\u62d3\u5c55\u4e86\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u4e2d\u4e2a\u6027\u5316\u88c5\u5907\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.17312", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17312", "abs": "https://arxiv.org/abs/2601.17312", "authors": ["Hugo Silva", "Mateus Mendes", "Hugo Gon\u00e7alo Oliveira"], "title": "Meta-Judging with Large Language Models: Concepts, Methods, and Challenges", "comment": null, "summary": "Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff08LLM-as-a-Judge\uff09\u5b58\u5728\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u68b3\u7406\u4e86LLM\u4f5c\u4e3a\u5143\u8bc4\u5ba1\u5458\uff08LLM-as-a-Meta-Judge\uff09\u9886\u57df\u7684\u53d1\u5c55\u548c\u672a\u6765\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5ba1\u5458\u8fdb\u884c\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u8bc4\u4f30\uff0c\u4f46\u5df2\u88ab\u53d1\u73b0\u5b58\u5728\u8bf8\u5982\u63d0\u793a\u654f\u611f\u6027\u3001\u7cfb\u7edf\u6027\u504f\u5dee\u3001\u5197\u4f59\u3001\u4ee5\u53ca\u7406\u7531\u4e0d\u53ef\u9760\u751a\u81f3\u51fa\u73b0\u5e7b\u89c9\u7b49\u95ee\u9898\u3002\u4e3a\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u7a33\u5065\u6027\uff0c\u4e1a\u754c\u4e9f\u9700\u66f4\u4e3a\u5f3a\u5065\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u4e86LLM-as-a-Meta-Judge\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4ee5\u516d\u5927\u5173\u952e\u89c6\u89d2\uff08\u6982\u5ff5\u57fa\u7840\u3001\u5143\u8bc4\u5ba1\u673a\u5236\u3001\u5bf9\u9f50\u8bad\u7ec3\u65b9\u6cd5\u3001\u8bc4\u4f30\u3001\u5c40\u9650\u6027\u53ca\u672a\u6765\u65b9\u5411\uff09\u4e3a\u6846\u67b6\uff0c\u5bf9\u76f8\u5173\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\u5206\u7c7b\u4e0e\u603b\u7ed3\u3002", "result": "\u6587\u7ae0\u63ed\u793a\u4e86LLM-as-a-Judge\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u603b\u7ed3\u5206\u6790\u4e86LLM\u5143\u8bc4\u5ba1\uff08Meta-Judging\uff09\u53d6\u5f97\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u6280\u672f\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u8be5\u65b9\u5411\u5728\u81ea\u52a8\u5316\u8bc4\u6d4b\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u4fe1\u5ea6\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LLM-as-a-Meta-Judge\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5e26\u6765\u66f4\u9ad8\u7a33\u5b9a\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6210\u672c\u3001\u63d0\u793a\u654f\u611f\u6027\u548c\u6a21\u578b\u5171\u6027\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u8bc4\u6d4b\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2601.18537", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18537", "abs": "https://arxiv.org/abs/2601.18537", "authors": ["Linyong Gan", "Zimo Li", "Wenxin Xu", "Xingjian Li", "Jianhua Z. Huang", "Enmei Tu", "Shuhang Chen"], "title": "SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction", "comment": null, "summary": "Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u6761\u4ef6\u7684\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u5168\u5c40\u8bed\u4e49\u51b3\u7b56\u548c\u5c40\u90e8\u8fd0\u52a8\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u95f4\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u65b9\u5411\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8fd8\u539f\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u8239\u8236\u8f68\u8ff9\u7684\u957f\u65f6\u9884\u6d4b\u5b58\u5728\u7531\u4e8e\u590d\u6742\u822a\u884c\u884c\u4e3a\u4e0e\u73af\u5883\u56e0\u7d20\u5e26\u6765\u7684\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u5168\u7403\u65b9\u5411\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u504f\u79bb\u6216\u4e0d\u5408\u7406\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u66f4\u9c81\u68d2\u3001\u5177\u5907\u8bed\u4e49\u7ea6\u675f\u7684\u957f\u65f6\u95f4\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u4ee5\u672a\u6765\u8f68\u8ff9\u53d7\u9ad8\u5c42\u5bfc\u822a\u610f\u56fe\uff08Next Key Point, NKP\uff09\u6761\u4ef6\u5f71\u54cd\u4e3a\u6838\u5fc3\u521b\u65b0\u70b9\uff0c\u901a\u8fc7\u5c06\u957f\u65f6\u9884\u6d4b\u5206\u89e3\u4e3a\u5168\u7403\u8bed\u4e49\u51b3\u7b56\uff08NKP\u9009\u62e9\uff09\u548c\u5c40\u90e8\u8fd0\u52a8\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3-\u5fae\u8c03\u7b56\u7565\u4ece\u5386\u53f2\u89c2\u6d4b\u9ad8\u6548\u4f30\u7b97NKP\u5148\u9a8c\u3002", "result": "\u5728\u771f\u5b9eAIS\u6570\u636e\u4e0a\uff0c\u63d0\u51fa\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u3001\u65b9\u5411\u7cbe\u5ea6\u53ca\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u4e0a\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e00\u81f4\u6027\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8eNKP\u7684\u8bed\u4e49\u6761\u4ef6\u8f68\u8ff9\u5efa\u6a21\u6709\u6548\u7ea6\u675f\u4e86\u672a\u6765\u8f68\u8ff9\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u8239\u8236\u9884\u6d4b\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17089", "abs": "https://arxiv.org/abs/2601.17089", "authors": ["Qigan Sun", "Chaoning Zhang", "Jianwei Zhang", "Xudong Wang", "Jiehui Xie", "Pengcheng Zheng", "Haoyu Wang", "Sungyoung Lee", "Chi-lok Andy Tai", "Yang Yang", "Heng Tao Shen"], "title": "GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing", "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRASP\u7684\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u80cc\u666f\u566a\u58f0\u6216\u5ffd\u7565\u76ee\u6807\u7ec6\u8282\uff0c\u539f\u56e0\u5728\u4e8e\u9065\u611f\u56fe\u50cf\u672c\u8eab\u5b58\u5728\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u76ee\u6807\u7a00\u758f\u548c\u533a\u57df\u8bed\u4e49\u590d\u6742\u7b49\u7279\u70b9\uff0c\u5bfc\u81f4\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51faGRASP\uff08Guided Region-Aware Sparse Prompting\uff09\u65b9\u6cd5\u3002GRASP\u901a\u8fc7\u4e3a\u7a7a\u95f4\u7ed3\u6784\u5757\u5f15\u5165\u8f6f\u63d0\u793a\uff08soft prompts\uff09\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684\u89c6\u89c9token\u7f51\u683c\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u95ee\u9898\u5f15\u5bfc\u7684\u7a00\u758f\u878d\u5408\u673a\u5236\uff0c\u5c06\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u52a8\u6001\u805a\u5408\u6210\u7d27\u51d1\u7684\u5168\u5c40\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u5173\u952e\u533a\u57df\uff0c\u51cf\u5c11\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0cGRASP\u5728\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387\u7684\u57fa\u7840\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u53ef\u6bd4\u73b0\u6709\u5fae\u8c03\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "conclusion": "GRASP\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u8868\u73b0\uff0c\u662f\u9488\u5bf9\u9065\u611f\u4efb\u52a1\u9ad8\u6548\u548c\u7cbe\u786e\u5fae\u8c03\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17344", "abs": "https://arxiv.org/abs/2601.17344", "authors": ["Chen Chen", "Kim Young Il", "Yuan Yang", "Wenhao Su", "Yilin Zhang", "Xueluan Gong", "Qian Wang", "Yongsen Zheng", "Ziyao Liu", "Kwok-Yan Lam"], "title": "The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents", "comment": "21 pages, 11 figures", "summary": "Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8bc4\u6d4b\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u68c0\u9a8c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u73b0\u5b9e\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u56fa\u6709\u4ef7\u503c\u504f\u5dee\uff08Intrinsic Value Misalignment, VM\uff09\u98ce\u9669\uff0c\u5e76\u7528\u8be5\u6846\u67b6\u6d4b\u8bd5\u4e86\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u56fa\u6709\u4ef7\u503c\u504f\u5dee\u5728\u73b0\u6709LLM\u4ee3\u7406\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4e14\u73b0\u6709\u9632\u62a4\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u8bc4\u4f30\u591a\u805a\u7126\u5728\u5bf9\u660e\u786e\u6709\u5bb3\u8f93\u5165\u7684\u54cd\u5e94\u6216\u7cfb\u7edf\u9c81\u68d2\u6027\u4e0a\uff0c\u4f46\u5e76\u672a\u5145\u5206\u8003\u5bdf\u5728\u65e0\u5bb3\u3001\u73b0\u5b9e\u548c\u81ea\u4e3b\u60c5\u5883\u4e0b\uff0cLLM\u4e5f\u53ef\u80fd\u51fa\u73b0\u4ef7\u503c\u504f\u5dee\uff08\u5373\u6a21\u578b\u884c\u4e3a\u504f\u79bb\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u4f26\u7406\u51c6\u5219\uff09\uff0c\u8fd9\u4e00\u201c\u56fa\u6709\u4ef7\u503c\u504f\u5dee\u201d\u5b58\u5728\u672a\u77e5\u5b89\u5168\u98ce\u9669\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u6d4b\u6846\u67b6\u6765\u586b\u8865\u6b64\u9886\u57df\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\u4e86\u201c\u5931\u63a7\u98ce\u9669\u201d\u548c\u201c\u56fa\u6709\u4ef7\u503c\u504f\u5dee\u201d\u3002\u968f\u540e\u63d0\u51faIMPRESS\u8bc4\u6d4b\u6846\u67b6\uff0c\u57fa\u4e8e\u4e00\u7ec4\u8be6\u7ec6\u523b\u753b\u73b0\u5b9e\u3001\u65e0\u5bb3\u3001\u5177\u6709\u4ee3\u7406\u80fd\u529b\u7684\u60c5\u5883\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6a21\u578b\u751f\u6210\u548c\u4e25\u683c\u8d28\u91cf\u7ba1\u63a7\uff0c\u6784\u5efa\u4e86\u5bf9\u8fd9\u4e9b\u98ce\u9669\u7684\u7cfb\u7edf\u6027\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5e76\u5bf921\u4e2a\u5148\u8fdbLLM\u4ee3\u7406\u8fdb\u884c\u4e86\u7cfb\u7edf\u6d4b\u8bd5\u3002\u540c\u65f6\u8fd8\u52a0\u5165\u4e86\u4eba\u5de5\u9a8c\u8bc1\u548c\u9632\u62a4\u65b9\u6cd5\uff08\u5982\u5b89\u5168\u63d0\u793a\u3001\u62a4\u680f\u673a\u5236\uff09\u7684\u8003\u67e5\u3002", "result": "\u8bc4\u6d4b\u53d1\u73b0\uff1a\u51e0\u4e4e\u6240\u6709\u4e3b\u6d41LLM\u4ee3\u7406\u90fd\u666e\u904d\u5b58\u5728\u56fa\u6709\u4ef7\u503c\u504f\u5dee\u98ce\u9669\uff0c\u4e14\u8be5\u98ce\u9669\u968f\u4efb\u52a1\u52a8\u673a\u3001\u98ce\u9669\u7c7b\u578b\u3001\u6a21\u578b\u4f53\u91cf\u3001\u6a21\u578b\u67b6\u6784\u6709\u660e\u663e\u5dee\u5f02\u3002\u89e3\u7801\u7b56\u7565\u548c\u8d85\u53c2\u6570\u5f71\u54cd\u5f88\u5c0f\uff0c\u4f46\u60c5\u5883\u8bbe\u7f6e\u548c\u4efb\u52a1\u8868\u8ff0\u5bf9\u4ef7\u503c\u504f\u5dee\u5f71\u54cd\u5f88\u5927\u3002\u73b0\u6709\u7684\u5b89\u5168\u9632\u62a4\u7b56\u7565\uff08\u5982\u5b89\u5168\u63d0\u793a\u3001\u62a4\u680f\uff09\u8868\u73b0\u4e0d\u7a33\u5b9a\u4e14\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u56fa\u6709\u4ef7\u503c\u504f\u5dee\u662f\u5f53\u524dLLM\u4ee3\u7406\u4e2d\u7684\u5e7f\u6cdb\u98ce\u9669\uff0c\u9700\u5f15\u8d77\u5173\u6ce8\u3002IMPRESS\u6846\u67b6\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u3001\u73b0\u5b9e\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u6d4b\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6301\u7eed\u76d1\u63a7\u548c\u6539\u8fdbAI\u4ee3\u7406\u7684\u4ef7\u503c\u5bf9\u9f50\u3002\u516c\u5f00\u6570\u636e\u548c\u5de5\u5177\u4e5f\u5c06\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.18548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18548", "abs": "https://arxiv.org/abs/2601.18548", "authors": ["Yulin Li", "Zhiyuan Song", "Yiming Li", "Zhicheng Song", "Kai Chen", "Chunxin Zheng", "Zhihai Bi", "Jiahang Cao", "Sylvain Calinon", "Fan Shi", "Jun Ma"], "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field", "comment": null, "summary": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff08GCDF\uff09\u7528\u4e8e\u9ad8\u6548\u3001\u7cbe\u51c6\u5730\u5904\u7406\u6709\u79fb\u52a8\u5e95\u5ea7\u673a\u68b0\u81c2\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u6574\u4f53\u5f69\u8ff9\u4f18\u5316\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6b64\u7684\u9ad8\u6027\u80fd\u4f18\u5316\u5668\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u9690\u5f0f\u7ea6\u675f\u4e0b\u7684\u5feb\u901f\u78b0\u649e\u63a8\u7406\u548c\u5feb\u901f\u91cd\u89c4\u5212\u3002", "motivation": "\u79fb\u52a8\u673a\u68b0\u81c2\u80fd\u591f\u901a\u8fc7\u534f\u8c03\u5e95\u5ea7\u548c\u673a\u68b0\u81c2\u52a8\u4f5c\uff0c\u5b9e\u73b0\u66f4\u52a0\u7075\u6d3b\u548c\u8fdc\u8ddd\u79bb\u7684\u64cd\u4f5c\u3002\u7136\u800c\uff0c\u5728\u72ed\u7a84\u3001\u62e5\u6324\u7a7a\u95f4\u4e2d\u7684\u5168\u8eab\u8f68\u8ff9\u4f18\u5316\u975e\u5e38\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u9ad8\u7ef4\u975e\u51f8\u6027\u548c\u5bf9\u5feb\u901f\u3001\u7cbe\u51c6\u78b0\u649e\u68c0\u6d4b\u7684\u9700\u6c42\u3002\u76ee\u524d\u7684\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a\uff08CDF\uff09\u65b9\u6cd5\u53ea\u9002\u7528\u4e8e\u56fa\u5b9a\u5e95\u5ea7\u7684\u673a\u68b0\u81c2\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u79fb\u52a8\u673a\u68b0\u81c2\u590d\u6742\u7684\u8026\u5408\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u6269\u5c55\u5230\u79fb\u52a8\u5e73\u53f0\u3001\u89e3\u51b3\u78b0\u649e\u68c0\u6d4b\u548c\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u6269\u5c55\u4e86\u901a\u7528\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a\uff08GCDF\uff09\u7684\u65b9\u6cd5\uff0c\u5c06\u539f\u672c\u9002\u7528\u4e8e\u56fa\u5b9a\u8f74\u673a\u68b0\u81c2\u7684CDF\u6269\u5c55\u5230\u80fd\u540c\u65f6\u5904\u7406\u5e73\u79fb\u3001\u65cb\u8f6c\u5173\u8282\u53ca\u65e0\u754c\u5de5\u4f5c\u7a7a\u95f4\u7684\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\u3002\u8bc1\u660e\u4e86GCDF\u7684\u5c40\u90e8\u8ddd\u79bb\u4fdd\u6301\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u751f\u6210\u548c\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6d41\u7a0b\uff0c\u83b7\u5f97\u53ef\u9ad8\u6548\u6279\u91cf\u67e5\u8be2\u7684\u8fde\u7eed\u795e\u7ecfGCDF\u6a21\u578b\u3002\u57fa\u4e8eGCDF\u78b0\u649e\u63a8\u7406\uff0c\u4f5c\u8005\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86\u9ad8\u6027\u80fd\u5e8f\u8d2f\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u652f\u6301\u795e\u7ecf\u7ea6\u675f\u5728\u7ebf\u6307\u5b9a\u3001\u5e76\u884c\u6279\u91cf\u7ea6\u675f\u8bc4\u4f30\u548c\u9ad8\u6548\u589e\u91cf\u5f0f\u7ea6\u675f\u7ba1\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684GCDF\u80fd\u591f\u51c6\u786e\u53cd\u6620\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u5168\u8eab\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u5728\u5904\u7406\u5927\u91cf\u9690\u5f0f\u7ea6\u675f\u65f6\uff0c\u4f18\u5316\u5668\u5177\u5907\u4f18\u79c0\u7684\u6269\u5c55\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u573a\u666f\u53d8\u5316\u4e0b\u7684\u5feb\u901f\u91cd\u89c4\u5212\u3002", "conclusion": "GCDF\u6709\u6548\u6269\u5c55\u4e86\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a\u7684\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u53d7\u9650\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u6574\u4f53\u5f69\u8ff9\u4f18\u5316\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u7406\u8bba\u4e0e\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2601.17095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17095", "abs": "https://arxiv.org/abs/2601.17095", "authors": ["Xusheng Du", "Athiwat Kongkaeo", "Ye Zhang", "Haoran Xie"], "title": "LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation", "comment": "10 pages, 5 figures, Proceedings of CAADRIA 2026", "summary": "For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u81ea\u52a8\u5316LoD\u8349\u56fe\u63d0\u53d6\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ec6\u8282\u5efa\u7b51\u6a21\u578b\u5230\u591a\u7ea7\u522b\u7ec6\u8282(LoD)\u4e00\u81f4\u6027\u6a21\u578b\u7684\u81ea\u52a8\u8f6c\u6362\u3002", "motivation": "\u591a\u7ea7\u522b\u7ec6\u8282(LoD)\u7684\u8868\u8fbe\u5bf9\u5efa\u7b51\u8bbe\u8ba1\u975e\u5e38\u5173\u952e\uff0c\u76ee\u524d\u4e3b\u6d41\u624b\u5de5\u64cd\u4f5c\u65b9\u6cd5\u7e41\u7410\u6613\u9519\uff0c\u4e14\u751f\u6210\u5f0fAI\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6210\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u7f3a\u4e4f\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u751f\u6210\u5f0fAI\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9010\u7ea7\u7b80\u5316\u9ad8\u7ec6\u8282\u5efa\u7b51\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u4e0e\u8bed\u4e49\u5c42\u6b21\u6e05\u6670\u7684\u591a\u7ea7LoD\u8868\u793a\uff0c\u81ea\u52a8\u63d0\u53d6\u5e76\u8f6c\u6362\u7ec6\u8282\u5c42\u7ea7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728LoD\u7b80\u5316\u8fc7\u7a0b\u4e2d\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0cSSIM\u5206\u522b\u8fbe\u5230\u4e860.7319\uff08LoD3\u2192LoD2)\u548c0.7532\uff08LoD2\u2192LoD1\uff09\uff0c\u5f52\u4e00\u5316Hausdorff\u8ddd\u79bb\u53cd\u6620\u4e86\u53d7\u63a7\u7684\u51e0\u4f55\u504f\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u4fdd\u8bc1\u7ed3\u6784\u4e0e\u8bed\u4e49\u6e10\u8fdb\u7b80\u5316\u7684\u4e00\u81f4\u6027\uff0c\u4e3aAI\u9a71\u52a8\u7684\u591a\u5c42\u7ea7\u5efa\u7b51\u751f\u6210\u548c\u5206\u5c42\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2601.17363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17363", "abs": "https://arxiv.org/abs/2601.17363", "authors": ["Michael Farrell"], "title": "Do readers prefer AI-generated Italian short stories?", "comment": "7 pages", "summary": "This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u610f\u5927\u5229\u8bfb\u8005\u5bf9AI\u751f\u6210\u77ed\u7bc7\u5c0f\u8bf4\u4e0e\u77e5\u540d\u4f5c\u5bb6\u4f5c\u54c1\u7684\u504f\u597d\uff0c\u53d1\u73b0AI\u6587\u672c\u7565\u53d7\u9752\u7750\u4e14\u8bfb\u8005\u7684\u80cc\u666f\u56e0\u7d20\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u5185\u5bb9\u5728\u6587\u5b66\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u4eba\u4eec\u666e\u904d\u8ba4\u4e3a\u4eba\u7c7b\u4f5c\u5bb6\u7684\u4f5c\u54c1\u8d28\u91cf\u66f4\u9ad8\u3002\u4f5c\u8005\u5e0c\u671b\u68c0\u9a8c\u5728\u4e0d\u544a\u77e5\u4f5c\u8005\u8eab\u4efd\u7684\u60c5\u51b5\u4e0b\uff0c\u8bfb\u8005\u662f\u5426\u771f\u80fd\u504f\u597d\u4eba\u7c7b\u4f5c\u8005\u7684\u4f5c\u54c1\uff0c\u5e76\u63a2\u8ba8\u5f71\u54cd\u8fd9\u4e00\u504f\u597d\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u76f2\u6d4b\u65b9\u6cd5\uff0c\u8ba920\u540d\u53c2\u4e0e\u8005\u9605\u8bfb\u5e76\u8bc4\u4ef7\u4e24\u7bc7\u7531ChatGPT-4o\u751f\u6210\u548c\u4e00\u7bc7\u7531\u610f\u5927\u5229\u4f5c\u5bb6Alberto Moravia\u521b\u4f5c\u7684\u77ed\u7bc7\u5c0f\u8bf4\u3002\u6536\u96c6\u53c2\u4e0e\u8005\u7684\u9605\u8bfb\u4e60\u60ef\u3001\u5e74\u9f84\u3001\u6027\u522b\u3001\u6559\u80b2\u6c34\u5e73\u548c\u6bcd\u8bed\u7b49\u6570\u636e\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u53d8\u91cf\u4e0e\u504f\u597d\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "AI\u521b\u4f5c\u7684\u6587\u672c\u83b7\u5f97\u4e86\u7565\u9ad8\u7684\u5e73\u5747\u8bc4\u5206\uff0c\u5e76\u4e14\u5728\u504f\u597d\u9009\u62e9\u4e0a\u66f4\u53d7\u6b22\u8fce\u3002\u4f46\u4e24\u8005\u5dee\u5f02\u4e0d\u5927\u3002\u53c2\u4e0e\u8005\u7684\u504f\u597d\u4e0e\u5e74\u9f84\u3001\u6027\u522b\u3001\u6559\u80b2\u3001\u6bcd\u8bed\u53ca\u9605\u8bfb\u4e60\u60ef\u7b49\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u7ed3\u679c\u6311\u6218\u4e86\u4eba\u4eec\u5bf9\u4eba\u7c7b\u6587\u5b66\u4f5c\u54c1\u4f18\u8d8a\u6027\u7684\u5047\u8bbe\uff0c\u540c\u65f6\u8868\u660e\u5728\u65e0\u4f5c\u8005\u4fe1\u606f\u7684\u6761\u4ef6\u4e0b\uff0cAI\u751f\u6210\u6587\u672c\u5728\u6587\u5b66\u573a\u666f\u4e2d\u65e0\u9700\u8fc7\u5ea6\u7f16\u8f91\uff0c\u4e5f\u80fd\u83b7\u5f97\u8bfb\u8005\u8ba4\u53ef\u3002"}}
{"id": "2601.18569", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18569", "abs": "https://arxiv.org/abs/2601.18569", "authors": ["Seokju Lee", "Kyung-Soo Kim"], "title": "Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation", "comment": "8 pages, 6 figures, Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u589e\u5f3a\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08AttenNKF\uff09\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u7b97\uff0c\u80fd\u6709\u6548\u8865\u507f\u7531\u4e8e\u8db3\u90e8\u6253\u6ed1\u5e26\u6765\u7684\u8bef\u5dee\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6613\u6253\u6ed1\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u8db3\u90e8\u6253\u6ed1\u4f1a\u5bfc\u81f4\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u7b97\u51fa\u73b0\u8f83\u5927\u8bef\u5dee\uff0c\u4e3b\u8981\u56e0\u4e3a\u8fd0\u52a8\u5b66\u89c2\u6d4b\u5728\u6253\u6ed1\u65f6\u8fdd\u80cc\u4e86\u65e0\u6ed1\u79fb\u5047\u8bbe\uff0c\u9020\u6210\u6ee4\u6ce2\u66f4\u65b0\u6709\u504f\u3002\u4e3a\u63d0\u5347\u4f30\u7b97\u7cbe\u5ea6\uff0c\u9700\u8bbe\u8ba1\u9ad8\u6548\u7684\u6253\u6ed1\u8865\u507f\u65b9\u6cd5\u3002", "method": "\u5728\u7ecf\u5178\u7684\u975e\u9f50\u6b21\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\u57fa\u7840\u4e0a\uff0c\u4f5c\u8005\u5f15\u5165\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u8865\u507f\u5668\u3002\u8865\u507f\u5668\u6839\u636e\u6253\u6ed1\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5bf9\u6ee4\u6ce2\u5668\u66f4\u65b0\u540e\u72b6\u6001\u8fdb\u884c\u8bef\u5dee\u4f30\u8ba1\u548c\u8c03\u6574\u3002\u795e\u7ecf\u7f51\u7edc\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\uff0c\u964d\u4f4e\u5bf9\u539f\u59cb\u8f93\u5165\u5c3a\u5ea6\u654f\u611f\u6027\uff0c\u786e\u4fdd\u8865\u507f\u7ed3\u6784\u826f\u597d\u540c\u65f6\u4e0d\u5f71\u54cd\u6ee4\u6ce2\u9012\u5f52\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0AttenNKF\u5728\u8db3\u90e8\u6253\u6ed1\u9891\u7e41\u7684\u5de5\u51b5\u4e0b\uff0c\u72b6\u6001\u4f30\u7b97\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u7b97\u5668\u3002", "conclusion": "\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u8865\u507f\u5668\u80fd\u6709\u6548\u6291\u5236\u7531\u8db3\u90e8\u6253\u6ed1\u5f15\u5165\u7684\u4f30\u7b97\u8bef\u5dee\uff0c\u5728\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u7b97\u9886\u57df\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17103", "abs": "https://arxiv.org/abs/2601.17103", "authors": ["Pascaline Andr\u00e9", "Charles Heitz", "Evangelia Christodoulou", "Annika Reinke", "Carole H. Sudre", "Michela Antonelli", "Patrick Godau", "M. Jorge Cardoso", "Antoine Gilson", "Sophie Tezenas du Montcel", "Ga\u00ebl Varoquaux", "Lena Maier-Hein", "Olivier Colliot"], "title": "Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals", "comment": null, "summary": "Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u4e2d\u7f6e\u4fe1\u533a\u95f4\uff08CI\uff09\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u4e0d\u540cCI\u65b9\u6cd5\u5728\u5404\u79cd\u5b9e\u9a8c\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\u4e0e\u7cbe\u5ea6\u5dee\u5f02\uff0c\u63d0\u51fa\u4e94\u5927\u5173\u952e\u7ed3\u8bba\uff0c\u4e3a\u672a\u6765\u6807\u51c6\u5236\u5b9a\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u76ee\u524d\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u7814\u7a76\u6709\u9650\uff0c\u5c24\u5176\u5bf9\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u884c\u4e3a\u53ca\u5176\u9002\u7528\u6027\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4e86\u89e3\uff0c\u5f71\u54cd\u4e86\u6027\u80fd\u7ed3\u679c\u7684\u53ef\u9760\u9a8c\u8bc1\u4e0e\u4e34\u5e8a\u8f6c\u5316\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u793e\u533a\u5bf9\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u79cd\u7c7b\u3001\u884c\u4e3a\u53ca\u5176\u9002\u5e94\u573a\u666f\u8ba4\u77e5\u7684\u7a7a\u767d\u3002", "method": "\u5bf924\u4e2a\u5206\u5272\u4e0e\u5206\u7c7b\u4efb\u52a1\u3001\u6bcf\u7ec419\u4e2a\u6a21\u578b\uff0c\u91c7\u7528\u591a\u79cd\u5e38\u7528\u6027\u80fd\u6307\u6807\u3001\u805a\u5408\u7b56\u7565\u53ca\u5e38\u89c1CI\u7b97\u6cd5\uff0c\u5f00\u5c55\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002\u7cfb\u7edf\u8bc4\u4f30\u5404\u79cd\u60c5\u666f\u4e0b\u5404CI\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff08\u8986\u76d6\u7387\uff09\u4e0e\u7cbe\u5ea6\uff08\u5bbd\u5ea6\uff09\uff0c\u5206\u6790\u5176\u5bf9\u6837\u672c\u91cf\u3001\u4efb\u52a1\u7c7b\u578b\u3001\u6307\u6807\u3001\u805a\u5408\u65b9\u5f0f\u7b49\u56e0\u7d20\u7684\u4f9d\u8d56\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a\uff081\uff09\u6240\u9700\u53ef\u9760CI\u7684\u6837\u672c\u91cf\u89c6\u7814\u7a76\u53c2\u6570\u4ece\u51e0\u5341\u5230\u6570\u5343\u75c5\u4f8b\u4e0d\u7b49\uff1b\uff082\uff09CI\u8868\u73b0\u5f3a\u4f9d\u8d56\u6027\u80fd\u6307\u6807\u9009\u62e9\uff1b\uff083\uff09\u805a\u5408\u7b56\u7565\u663e\u8457\u5f71\u54cdCI\u53ef\u9760\u6027\uff0c\u5982\u5b8f\u89c2\u805a\u5408\u9700\u66f4\u591a\u6837\u672c\uff1b\uff084\uff09\u5206\u5272/\u5206\u7c7b\u4efb\u52a1\u5dee\u5f02\u5f71\u54cdCI\u884c\u4e3a\uff1b\uff085\uff09\u4e0d\u540cCI\u65b9\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u53ef\u9760\u6027\u548c\u7cbe\u5ea6\u5e76\u4e0d\u76f8\u540c\u3002", "conclusion": "\u4e3a\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u65b9\u6cd5\u5236\u5b9a\u672a\u6765\u6807\u51c6\u548c\u6307\u5357\uff0c\u63d0\u4f9b\u4e86\u91cd\u8981\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5f3a\u8c03\u5e94\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u5e76\u5145\u5206\u62a5\u544a\u76f8\u5173\u53c2\u6570\u3002"}}
{"id": "2601.17364", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17364", "abs": "https://arxiv.org/abs/2601.17364", "authors": ["Mohammed Fasha", "Bassam Hammo", "Bilal Sowan", "Husam Barham", "Esam Nsour"], "title": "Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws", "comment": "5 pages, resources at: https://github.com/msfasha/Research-Resources/tree/main/ArabicLegalLLM", "summary": "This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u6cd5\u5f8b\u95ee\u7b54\u9886\u57df\uff0c\u57fa\u4e8e\u7ea6\u65e6\u6cd5\u5f8b\u6570\u636e\uff0c\u5fae\u8c03\u4e86Llama-3.1\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u591a\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u5bf9\u963f\u62c9\u4f2f\u8bed\u53ca\u6cd5\u5f8b\u9886\u57df\u7684\u9002\u914d\u6027\u8f83\u5dee\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5fae\u8c03\uff0c\u9a8c\u8bc1Llama-3.1\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u3001\u56db\u4f4d\u91cf\u5316\u6761\u4ef6\u4e0b\u80fd\u5426\u63d0\u5347\u963f\u62c9\u4f2f\u8bed\u6cd5\u5f8b\u95ee\u7b54\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u9002\u5408\u8be5\u9886\u57df\u7684\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u9009\u7528Llama-3.1-8B\u4e24\u79cd\u57fa\u5ea7\u6a21\u578b\uff0c\u5728\u91c7\u7528PEFT\uff08\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff09\u548cLoRA\u9002\u914d\u5668\u3001Unsloth\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u4ee5\u56db\u4f4d\u91cf\u5316\u6a21\u578b\u8fdb\u884c\u52a0\u901f\u548c\u8282\u7701\u8d44\u6e90\u8bad\u7ec3\u3002\u81ea\u5efa\u4e866000\u4e2a\u7ea6\u65e6\u6cd5\u5f8b\u95ee\u7b54\u5bf9\u6570\u636e\u96c6\uff0c\u6309\u7ed3\u6784\u5316\u63d0\u793a\u683c\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002\u6700\u7ec8\u901a\u8fc7BLEU\u548cROUGE\u6307\u6807\u8bc4\u4f30\u5fae\u8c03\u524d\u540e\u7684\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u548c\u95ee\u7b54\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u7531\u4e8e\u91c7\u7528\u91cf\u5316\u548c\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u5229\u7528\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660eLlama-3.1\u7b49\u5927\u6a21\u578b\u7ecf\u8fc7\u4f18\u5316\u540e\u80fd\u66f4\u597d\u9002\u5e94\u963f\u62c9\u4f2f\u8bed\u6cd5\u5f8b\u4efb\u52a1\uff0c\u4e14\u76f8\u5173\u7684\u9ad8\u6548\u5fae\u8c03\u548c\u91cf\u5316\u6280\u672f\u4e3a\u9886\u57df\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2601.18629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18629", "abs": "https://arxiv.org/abs/2601.18629", "authors": ["Yiming Wang", "Ruogu Zhang", "Minyang Li", "Hao Shi", "Junbo Wang", "Deyi Li", "Jieji Ren", "Wenhai Liu", "Weiming Wang", "Hao-Shu Fang"], "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection", "comment": null, "summary": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u91c7\u96c6\u4e0e\u8fc1\u79fb\u6846\u67b6ExoGS\uff0c\u901a\u8fc7\u4eba\u7a7f\u6234\u7684\u5916\u9aa8\u9abc\u88c5\u7f6e\u91c7\u96c6\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u5e76\u5c06\u5176\u9ad8\u6548\u8f6c\u79fb\u5230\u4eff\u771f\u73af\u5883\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u548c\u6cdb\u5316\u3002", "motivation": "\u4f20\u7edf\u7684Real-to-Sim-to-Real\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5728\u89c6\u89c9\u5c42\u9762\u7684\u73af\u5883\u8fc1\u79fb\uff0c\u5ffd\u7565\u4e86\u4eba\u4e0e\u7269\u4f53\u3001\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u8fc1\u79fb\u3002\u800c\u8fd9\u4e9b\u52a8\u6001\u4ea4\u4e92\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u63a5\u89e6\u573a\u666f\u4e0b\uff0c\u5355\u9760\u4eff\u771f\u96be\u4ee5\u9ad8\u6548\u83b7\u5f97\u3002\u4f5c\u8005\u5e0c\u671b\u7834\u89e3\u771f\u5b9e\u4e16\u754c\u4eba\u7c7b\u64cd\u4f5c\u4ea4\u4e92\u7684\u6570\u636e\u91c7\u96c6\u4e0e\u8fc1\u79fb\u96be\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u7814\u7684\u65e0\u673a\u5668\u4eba\u88ab\u52a8\u5916\u9aa8\u9abcAirExo-3\u91c7\u96c6\u88c5\u7f6e\uff0c\u901a\u8fc7\u8be5\u88c5\u7f6e\u53ef\u83b7\u5f97\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e00\u81f4\u3001\u9ad8\u7cbe\u5ea6\u7684\u4eba\u7c7b\u6f14\u793a\u6570\u636e\uff0c\u5e76\u540c\u6b65\u83b7\u53d6RGB\u89c6\u89c9\u4fe1\u606f\u3002\u5c06\u673a\u5668\u4eba\u3001\u7269\u4f53\u3001\u73af\u5883\u91cd\u5efa\u4e3a\u53ef\u7f16\u8f91\u76843D Gaussian Splatting\u8d44\u4ea7\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u7684\u56de\u653e\u548c\u5927\u89c4\u6a21\u6570\u636e\u6269\u589e\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684Mask Adapter\u63d0\u5347\u7b56\u7565\u7684\u8bed\u4e49\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e0b\uff0cExoGS\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u4e0a\u660e\u663e\u4f18\u4e8e\u57fa\u4e8e\u8fdc\u7a0b\u64cd\u4f5c\u7684\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u3002", "conclusion": "ExoGS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u91c7\u96c6\u548c\u8fc1\u79fb\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5927\u5e45\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002\u4ee3\u7801\u548c\u786c\u4ef6\u4e5f\u5df2\u5f00\u653e\u5171\u4eab\u3002"}}
{"id": "2601.17107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17107", "abs": "https://arxiv.org/abs/2601.17107", "authors": ["Qinkai Yu", "Chong Zhang", "Gaojie Jin", "Tianjin Huang", "Wei Zhou", "Wenhui Li", "Xiaobo Jin", "Bo Huang", "Yitian Zhao", "Guang Yang", "Gregory Y. H. Lip", "Yalin Zheng", "Aline Villavicencio", "Yanda Meng"], "title": "StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors", "comment": "15 pages,7 figures. Accepted to IEEE Transactions on Image Processing (TIP) 2026", "summary": "Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u533b\u7597\u5206\u5272\u6a21\u578b\u6c34\u5370\u65b9\u6cd5StealthMark\uff0c\u901a\u8fc7\u5fae\u5999\u8c03\u63a7\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e0d\u5f71\u54cd\u5206\u5272\u7ed3\u679c\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6a21\u578b\u6240\u6709\u6743\u7684\u9ed1\u76d2\u9a8c\u8bc1\uff0c\u4e14\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u4e14\u65e0\u635f\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u5148\u8fdb\u7684\u533b\u7597AI\u5206\u5272\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u4e14\u654f\u611f\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u4e14\u8fd9\u4e9b\u6a21\u578b\u5c5e\u4e8e\u91cd\u8981\u77e5\u8bc6\u4ea7\u6743\uff0c\u9700\u8981\u6709\u6548\u7684\u4fdd\u62a4\u4e0e\u9a8c\u8bc1\u673a\u5236\u3002\u76ee\u524d\u6a21\u578b\u4fdd\u62a4\u65b9\u6cd5\u591a\u9488\u5bf9\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5206\u5272\u6a21\u578b\u7684\u6240\u6709\u6743\u4fdd\u62a4\u65b9\u6cd5\u9c9c\u6709\u7814\u7a76\u3002", "method": "\u63d0\u51faStealthMark\uff0c\u901a\u8fc7\u8c03\u8282\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408LIME\u7b49\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728\u4e0d\u6539\u53d8\u5206\u5272\u8f93\u51fa\u60c5\u51b5\u4e0b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u63ed\u793a\u6c34\u5370\u3002\u6c34\u5370\u8bbe\u8ba1\u4e3aQR\u7801\uff0c\u4fbf\u4e8e\u8bc6\u522b\u4e0e\u9a8c\u8bc1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u548c\u4e94\u79cd\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u4e0a\u5927\u91cf\u5b9e\u9a8c\uff0cStealthMark\u80fd\u4fdd\u8bc1\u6240\u6709\u6743\u9a8c\u8bc1\u6210\u529f\u7387\uff08ASR\uff09\u9ad8\u4e8e95%\uff0c\u5206\u5272\u6027\u80fd\u635f\u5931\u5c0f\u4e8e1%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u540e\u95e8\u7684\u6c34\u5370\u65b9\u6cd5\u3002", "conclusion": "StealthMark\u4e3a\u533b\u7597\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u9690\u853d\u3001\u65e0\u635f\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u65b0\u65b9\u6848\uff0c\u517c\u5177\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.17367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17367", "abs": "https://arxiv.org/abs/2601.17367", "authors": ["Zecheng Tang", "Quantong Qiu", "Yi Yang", "Zhiyi Hong", "Haiya Xiang", "Kebin Liu", "Qingqing Dang", "Juntao Li", "Min Zhang"], "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "comment": null, "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f39\u6027\u6ce8\u610f\u529b\uff08Elastic Attention\uff09\u7684\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\u6765\u63d0\u5347\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u6548\u7387\u4e0e\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\u65b9\uff0c\u963b\u788d\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u6df7\u5408\u7a00\u758f\u4e0e\u5168\u6ce8\u610f\u529b\u65b9\u6848\u91c7\u7528\u9759\u6001\u6bd4\u4f8b\uff0c\u65e0\u6cd5\u9002\u914d\u4e0d\u540c\u4efb\u52a1\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u7a00\u758f\u5ea6\u7684\u591a\u6837\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5728\u539f\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u96c6\u6210\u4e00\u4e2a\u8f7b\u91cf\u7ea7Attention Router\uff0c\u901a\u8fc7\u8be5\u8def\u7531\u5668\u52a8\u6001\u5206\u914d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u8ba1\u7b97\u6a21\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u6a21\u578b\u6839\u636e\u8f93\u5165\u7075\u6d3b\u8c03\u6574\u7a00\u758f\u5ea6\u3002\u8be5\u65b9\u6cd5\u53ea\u970012\u5c0f\u65f6\u8bad\u7ec3\uff088\u5f20A800\u663e\u5361\uff09\uff0c\u4fbf\u80fd\u591f\u6536\u655b\u3002", "result": "\u5728\u4e09\u4e2a\u4eba\u5de5\u667a\u80fd\u957f\u6587\u672c\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4f5c\u8005\u65b9\u6cd5\u5728\u591a\u4e2a\u4e3b\u6d41LLM\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u4e0e\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u5f39\u6027\u6ce8\u610f\u529b\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u8fd8\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u9002\u914d\u4e0d\u540c\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u4f18\u4e8e\u56fa\u5b9a\u6bd4\u4f8b\u7684\u6df7\u5408\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5bf9\u957f\u6587\u672c\u5904\u7406\u573a\u666f\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18639", "abs": "https://arxiv.org/abs/2601.18639", "authors": ["Ojasva Mishra", "Xiaolong Wu", "Min Xu"], "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation", "comment": null, "summary": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\u03c4=1.0$~s, $\u0394t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5b9e\u9645\u673a\u5668\u4eba\u4e2d\u7684\u9971\u548c\u79bb\u6563\u65f6\u95f4\u5173\u8282\u63a7\u5236\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u5b9e\u9645\u5b9e\u73b0\u56e0\u7d20\u7684\u5206\u6790\u548c\u8c03\u53c2\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5b89\u5168\u7b5b\u9009\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684PI/PID\u63a7\u5236\u5668\u8c03\u6574\u3002", "motivation": "\u5c3d\u7ba1PID\u63a7\u5236\u5e7f\u6cdb\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u7b49\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0\u4e2d\u5b58\u5728\u79bb\u6563\u6267\u884c\u3001\u6267\u884c\u5668\u9971\u548c\u3001\u5c0f\u5ef6\u8fdf\u53ca\u6d4b\u91cf\u4e0d\u5b8c\u7f8e\u7b49\u5e38\u89c1\u95ee\u9898\uff0c\u5bfc\u81f4\u7406\u8bba\u4e0e\u5b9e\u9645\u8868\u73b0\u4e0d\u7b26\uff0c\u4e9f\u9700\u4e00\u5957\u517c\u987e\u7406\u8bba\u4e0e\u5b9e\u73b0\u7ec6\u8282\u7684\u5206\u6790\u548c\u8c03\u6574\u65b9\u6cd5\u3002", "method": "(1) \u5229\u7528Jury\u5224\u636e\uff0c\u63a8\u5bfc\u4e86Euler\u548cZOH\u79bb\u6563\u5316\u4e0bPI\u63a7\u5236\u5668\u7684\u7a33\u5b9a\u533a\u57df\uff1b(2) \u5728\u9971\u548c\u4e3b\u5bfc\u573a\u666f\u4e0b\uff0c\u8bc4\u4f30\u4e86\u79bb\u6563\u53cd\u8ba1\u7b97\u6297\u79ef\u5206\u9971\u548c\u7ed3\u6784\uff1b(3) \u63d0\u51fa\u6df7\u5408\u8ba4\u8bc1\u8d1d\u53f6\u65af\u4f18\u5316\u6d41\u7a0b\uff0c\u5148\u5254\u9664\u4e0d\u7a33\u5b9a\u6216\u4e0d\u5b89\u5168\u53c2\u6570\uff0c\u518d\u9488\u5bf9\u9c81\u68d2\u6027\u80fd\u6307\u6807\uff08IAE\uff09\u53ca\u8f6f\u60e9\u7f5a\uff08\u5982\u8d85\u8c03/\u9971\u548c\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u4e00\u7cfb\u5217\u4eff\u771f\uff08\u542b\u4e0d\u786e\u5b9a\u6027\u3001\u5ef6\u8fdf\u3001\u566a\u58f0\u3001\u91cf\u5316\u53ca\u66f4\u5f3a\u9971\u548c\uff09\u663e\u793a\uff0c\u5728\u9c81\u68d2\u8c03\u53c2\u540e\uff0c\u4e2d\u503cIAE\u4ece0.843\u964d\u81f30.430\uff0c\u8d85\u8c03\u4fdd\u6301\u57282%\u4ee5\u4e0b\u3002\u901a\u8fc7\u4eff\u771f\u9636\u6bb5\u8ba4\u8bc1\u7b5b\u966411.6%\u7684\u4e0d\u5408\u683c\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u6837\u672c\u5229\u7528\u7387\uff0c\u65e0\u9700\u786c\u4ef6\u5b9e\u9a8c\u3002", "conclusion": "\u672c\u6587\u6240\u63d0\u6d41\u7a0b\u5728\u6599\u60f3\u4e0d\u786e\u5b9a\u548c\u9971\u548c\u7b49\u5b9e\u9645\u5de5\u7a0b\u7ea6\u675f\u4e0b\uff0c\u63d0\u5347\u4e86\u79bb\u6563\u65f6\u95f4PI/PID\u63a7\u5236\u5668\u7684\u7a33\u5065\u6027\u4e0e\u8c03\u4f18\u6548\u7387\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u7b49\u5b9e\u9645\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8c03\u53c2\u4e0e\u5b89\u5168\u4fdd\u969c\u624b\u6bb5\u3002"}}
{"id": "2601.17124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17124", "abs": "https://arxiv.org/abs/2601.17124", "authors": ["Bin Lin", "Zongjian Li", "Yuwei Niu", "Kaixiong Gong", "Yunyang Ge", "Yunlong Lin", "Mingzhe Zheng", "JianWei Zhang", "Miles Yang", "Zhao Zhong", "Liefeng Bo", "Li Yuan"], "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code", "comment": "Technical Report", "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6709\u9650\u6807\u91cf\u91cf\u5316\u65b9\u6cd5iFSQ\uff0c\u901a\u8fc7\u4e00\u884c\u4ee3\u7801\u66ff\u6362FSQ\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4e0a\u6700\u4f73\u7684\u4fe1\u606f\u5229\u7528\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4fc3\u8fdb\u4e86\u79bb\u6563\u4e0e\u8fde\u7eed\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7edf\u4e00\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u9886\u57df\u5728\u79bb\u6563\uff08\u81ea\u56de\u5f52\u6a21\u578b\uff09\u548c\u8fde\u7eed\uff08\u6269\u6563\u6a21\u578b\uff09\u65b9\u6cd5\u95f4\u5b58\u5728\u5206\u6b67\uff0c\u4e24\u8005\u95f4\u7f3a\u4e4f\u7edf\u4e00\u7684\u6a21\u578b\u4e0e\u516c\u5e73\u7684\u57fa\u51c6\u3002\u4f20\u7edfFSQ\u5728\u91cf\u5316\u7cbe\u5ea6\u548c\u4fe1\u606f\u5229\u7528\u7387\u95f4\u96be\u4ee5\u4e24\u5168\uff0c\u6025\u9700\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5728FSQ\u4e2d\u5f15\u5165\u65b0\u7684\u5206\u5e03\u5339\u914d\u6620\u5c04\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u9690\u53d8\u91cf\u5206\u5e03\u66f4\u52a0\u5747\u5300\uff0c\u786e\u4fdd\u6bcf\u4e2a\u91cf\u5316\u6876\u90fd\u88ab\u6709\u6548\u5229\u7528\uff0c\u540c\u65f6\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7b80\u5355\uff0c\u4ec5\u9700\u4fee\u6539\u4e00\u884c\u4ee3\u7801\uff0c\u5e76\u8bbe\u8ba1\u63a7\u5236\u5b9e\u9a8c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0ciFSQ\u65e2\u53ef\u4fdd\u8bc1\u9ad8\u6548\u7684\u4fe1\u606f\u5229\u7528\uff0c\u4e5f\u80fd\u83b7\u5f97\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002\u5bf9\u6bd4\u5206\u6790\u4e2d\u53d1\u73b0\uff1a\uff081\uff09\u57284 bits/dimension\u5904\u79bb\u6563\u4e0e\u8fde\u7eed\u8868\u793a\u8fbe\u5230\u6700\u4f18\u5e73\u8861\uff1b\uff082\uff09AR\u6a21\u578b\u6536\u655b\u5feb\u4f46\u6269\u6563\u6a21\u578b\u6700\u7ec8\u6548\u679c\u66f4\u4f73\uff0c\u5e8f\u5217\u6027\u53ef\u80fd\u9650\u5236AR\u6a21\u578b\u751f\u6210\u4e0a\u9650\u3002", "conclusion": "iFSQ\u4e3a\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u7edf\u4e00\u7684\u5e73\u53f0\u3002\u7814\u7a76\u6210\u679c\u5efa\u8bae\u672a\u6765\u6a21\u578b\u5728\u4fe1\u606f\u91cf\u4e0e\u6a21\u578b\u7ed3\u6784\u95f4\u5e94\u5bfb\u6c42\u65b0\u5e73\u8861\uff0c\u540c\u65f6\u4e3a\u7edf\u4e00\u8bc4\u6d4b\u4e0e\u7ed3\u6784\u521b\u65b0\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2601.17377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17377", "abs": "https://arxiv.org/abs/2601.17377", "authors": ["Kiyotada Mori", "Shohei Tanaka", "Tosho Hirasawa", "Tadashi Kozuno", "Koichiro Yoshino", "Yoshitaka Ushiku"], "title": "WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews", "comment": null, "summary": "The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u79d1\u5b66\u8bc4\u5ba1\u8bc4\u8bba\u4e2d\u2018\u4e3b\u5f20\u2019\u4e0e\u2018\u8bc1\u636e\u2019\u4e4b\u95f4\u7684\u903b\u8f91\u63a8\u7406\uff0c\u81ea\u52a8\u5224\u65ad\u8bc4\u8bba\u89c2\u70b9\u662f\u5426\u6709\u636e\u53ef\u4f9d\uff0c\u5e76\u53d6\u5f97\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u66f4\u9ad8\u7684\u4eba\u7c7b\u8bc4\u5ba1\u5206\u6570\u76f8\u5173\u6027\u3002", "motivation": "\u968f\u7740\u88ab\u6295\u7a3f\u8bba\u6587\u6570\u91cf\u6025\u5267\u589e\u957f\uff0c\u79d1\u5b66\u9886\u57df\u7684\u540c\u884c\u8bc4\u5ba1\u9762\u4e34\u4eba\u529b\u8d44\u6e90\u77ed\u7f3a\u3002\u5229\u7528\u8bed\u8a00\u6a21\u578b\u534f\u52a9\u8bc4\u5ba1\u88ab\u89c6\u4e3a\u51cf\u8f7b\u4eba\u529b\u8d1f\u62c5\u7684\u65b9\u5411\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u81ea\u52a8\u8bc4\u5224\u8bc4\u5ba1\u610f\u89c1\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u81ea\u52a8\u62bd\u53d6\u79d1\u5b66\u8bc4\u5ba1\u610f\u89c1\u4e2d\u7684\u2018\u4e3b\u5f20\u2019\u548c\u2018\u8bc1\u636e\u2019\uff0c\u8fdb\u800c\u4e0d\u4ec5\u5224\u65ad\u2018\u8bc1\u636e\u2019\u662f\u5426\u5b58\u5728\uff0c\u8fd8\u8bc4\u4f30\u2018\u4e3b\u5f20\u2019\u4e0e\u2018\u8bc1\u636e\u2019\u95f4\u7684\u903b\u8f91\u63a8\u7406\u5173\u7cfb\uff0c\u5e76\u636e\u6b64\u91cf\u5316\u2018\u4e3b\u5f20\u2019\u88ab\u8bc1\u636e\u652f\u6301\u7684\u7a0b\u5ea6\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u8bc4\u5ba1\u8bc4\u5206\u7684\u76f8\u5173\u6027\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u66f4\u8d34\u8fd1\u4eba\u5de5\u8bc4\u5ba1\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347\u540c\u884c\u8bc4\u5ba1\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u4eba\u529b\u6548\u7387\uff0c\u80fd\u66f4\u597d\u5730\u652f\u6301\u8bc4\u5ba1\u6d41\u7a0b\u3002"}}
{"id": "2601.18692", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18692", "abs": "https://arxiv.org/abs/2601.18692", "authors": ["Wei Wu", "Fan Lu", "Yunnan Wang", "Shuai Yang", "Shi Liu", "Fangjing Wang", "Qian Zhu", "He Sun", "Yong Wang", "Shuailei Ma", "Yiyu Ren", "Kejia Zhang", "Hui Yu", "Jingmei Zhao", "Shuai Zhou", "Zhenqi Qiu", "Houlong Xiong", "Ziyu Wang", "Zechen Wang", "Ran Cheng", "Yong-Lu Li", "Yongtao Huang", "Xing Zhu", "Yujun Shen", "Kecheng Zheng"], "title": "A Pragmatic VLA Foundation Model", "comment": "Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/", "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LingBot-VLA\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u4ee3\u7801\u3001\u57fa\u7ebf\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u5df2\u5f00\u653e\uff0c\u65e8\u5728\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u80fd\u591f\u8de8\u4efb\u52a1\u3001\u8de8\u5e73\u53f0\u6cdb\u5316\u4e14\u9ad8\u6548\u9002\u5e94\u7684\u65b0\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u5728\u6cdb\u5316\u80fd\u529b\u6216\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86LingBot-VLA\u6a21\u578b\uff0c\u57fa\u4e8e2\u4e07\u4e2a\u5c0f\u65f6\u30019\u79cd\u53cc\u81c2\u673a\u5668\u4eba\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u3002\u57283\u4e2a\u5e73\u53f0\u4e0a\u7cfb\u7edf\u8bc4\u6d4b\uff0c\u6bcf\u4e2a\u5e73\u53f0\u6267\u884c100\u9879\u4efb\u52a1\uff0c\u6bcf\u9879\u4efb\u52a1130\u6b21\u8bad\u7ec3\u540e\u6d4b\u8bd5\uff0c\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u540c\u65f6\u5f00\u53d1\u9ad8\u6548\u4ee3\u7801\u5e93\uff0c\u5b9e\u73b0\u5355GPU\u6bcf\u79d2261\u6837\u672c\u30018GPU\u8bad\u7ec31.5~2.8\u500d\u63d0\u901f\u3002", "result": "LingBot-VLA\u5728\u5404\u5e73\u53f0\u548c\u4efb\u52a1\u8bc4\u6d4b\u4e2d\u5747\u5927\u5e45\u8d85\u8d8a\u7ade\u54c1\uff0c\u9a8c\u8bc1\u4e86\u5176\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u4e0e\u6027\u80fd\u3002\u65b0\u4ee3\u7801\u5e93\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "LingBot-VLA\u65e2\u5f3a\u5316\u6a21\u578b\u6cdb\u5316\u548c\u6548\u7387\uff0c\u4e5f\u4e3a\u5b9e\u9645\u90e8\u7f72\u505a\u597d\u4e86\u51c6\u5907\u3002\u5f00\u653e\u8d44\u6e90\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u66f4\u52a0\u6807\u51c6\u5316\u3001\u6311\u6218\u6027\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17151", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17151", "abs": "https://arxiv.org/abs/2601.17151", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Yu Gu", "Ying Jin", "Sam Preston", "Yanbo Xu", "Sid Kiblawi", "Wen-wai Yim", "Tim Ossowski", "Tristan Naumann", "Mu Wei", "Hoifung Poon"], "title": "Scaling medical imaging report generation with multimodal reinforcement learning", "comment": null, "summary": "Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u6846\u67b6UniRG\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6307\u6807\uff0c\u5728\u6743\u5a01\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u867d\u80fd\u7406\u89e3\u81ea\u7136\u8bed\u8a00\uff0c\u4f46\u5728\u533b\u5b66\u7b49\u9ad8\u4ef7\u503c\u9886\u57df\u7684\u591a\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u4fbf\u662f\u5178\u578b\u4f8b\u5b50\uff0c\u4e14\u4f20\u7edf\u6709\u76d1\u7763\u5fae\u8c03\u6613\u9677\u5165\u6a21\u677f\u5316\u8fc7\u62df\u5408\uff0c\u5f71\u54cd\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u901a\u7528\u62a5\u544a\u751f\u6210\u6846\u67b6UniRG\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u7edf\u4e00\u673a\u5236\uff0c\u76f4\u63a5\u9488\u5bf9\u4e0b\u6e38\u5e94\u7528\u8bc4\u6d4b\u6307\u6807\u8fdb\u884c\u4f18\u5316\u3002\u4ee5\u80f8\u90e8X\u5149\u6570\u636e\u4e3a\u4f8b\uff0c\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u5e76\u8bbe\u8ba1\u591a\u79cd\u4e25\u683c\u8bc4\u6d4b\u573a\u666f\u3002", "result": "\u5728\u6743\u5a01\u7684ReXrank\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u57fa\u51c6\u4e0a\uff0cUniRG-CXR\u663e\u8457\u8d85\u8d8a\u6240\u6709\u5df2\u53d1\u8868\u65b9\u6cd5\uff0c\u5237\u65b0\u4e86\u6574\u4f53\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "UniRG\u6846\u67b6\u80fd\u5728\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u9886\u57df\u53d6\u5f97\u4f18\u8d8a\u4e14\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u884c\u4e1a\u6709\u8f83\u5927\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.17387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17387", "abs": "https://arxiv.org/abs/2601.17387", "authors": ["Toshiki Nakai", "Varsha Suresh", "Vera Demberg"], "title": "Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis", "comment": "8 pages for the main text, 51 figures, 1 table", "summary": "Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u8bed\u97f3\u6587\u672c\u57fa\u7840\u6a21\u578b\uff08\u5982SeamlessM4T v2\uff09\u5728\u5904\u7406\u4e0d\u540c\u6a21\u6001\uff08\u8bed\u97f3\u4e0e\u6587\u672c\uff09\u65f6\uff0c\u5185\u90e8\u662f\u5426\u5b58\u5728\u4e00\u81f4\u7684\u8bed\u8a00\u8868\u8fbe\u65b9\u5f0f\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5185\u90e8\u5b58\u5728\u4e0d\u5b8c\u5168\u7684\u6a21\u6001\u4e0d\u53d8\u6027\uff0c\u5c24\u5176\u5728\u4ece\u8bed\u97f3\u5230\u6587\u672c\u8f6c\u6362\u65f6\uff0c\u8bed\u8a00\u4fe1\u606f\u7684\u6062\u590d\u53d8\u5f97\u66f4\u96be\u3002\u67d0\u4e9b\u5173\u952e\u795e\u7ecf\u5143\u5bf9\u6a21\u6001\u548c\u8bed\u8a00\u7684\u533a\u5206\u5177\u6709\u9ad8\u5ea6\u9009\u62e9\u6027\uff0c\u8fd9\u79cd\u96c6\u4e2d\u6fc0\u6d3b\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8de8\u6a21\u6001\u548c\u8bed\u8a00\u65f6\u8868\u73b0\u66f4\u8106\u5f31\u3002", "motivation": "\u591a\u8bed\u8a00\u8bed\u97f3-\u6587\u672c\u57fa\u7840\u6a21\u578b\u7684\u76ee\u6807\u662f\u5728\u591a\u79cd\u8bed\u8a00\u4e0e\u6a21\u6001\u4e4b\u95f4\u5b9e\u73b0\u7edf\u4e00\u5904\u7406\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u6a21\u578b\u5185\u90e8\u662f\u5426\u80fd\u5728\u8bed\u97f3\u4e0e\u6587\u672c\u95f4\u4fdd\u6301\u4e00\u81f4\u7684\u8bed\u8a00\u8868\u793a\uff0c\u5bf9\u7406\u89e3\u5176\u6cdb\u5316\u548c\u5f31\u70b9\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u65b9\u6cd5\u5206\u6790SeamlessM4T v2\uff1a1\uff09\u5229\u7528\u5e73\u5747\u7cbe\u5ea6\u6392\u5e8f\u8bc6\u522b\u5bf9\u8bed\u8a00\u548c\u6a21\u6001\u654f\u611f\u7684\u795e\u7ecf\u5143\uff1b2\uff09\u5728\u63a8\u7406\u65f6\u91c7\u7528\u4e2d\u4f4d\u6570\u66ff\u6362\u5e72\u9884\uff0c\u6d4b\u8bd5\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u89e3\u7801\u7684\u56e0\u679c\u5f71\u54cd\uff1b3\uff09\u5206\u6790\u4e0d\u540c\u8bed\u8a00\u3001\u6a21\u6001\u4e0b\u6fc0\u6d3b\u5f3a\u5ea6\u7684\u4e0d\u5747\u8861\u7a0b\u5ea6\u3002", "result": "\u53d1\u73b0\u7f16\u7801\u5668\u8868\u793a\u867d\u8d8b\u4e8e\u53bb\u8bed\u8a00\u5316\uff0c\u4f46\u8fd9\u79cd\u538b\u7f29\u5bfc\u81f4\u89e3\u7801\u5668\u96be\u4ee5\u91cd\u5efa\u539f\u8bed\u8a00\uff0c\u7279\u522b\u662f\u4ece\u8bed\u97f3\u5230\u6587\u672c\u65f6\u3002\u6b64\u5916\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u4e2d\u5b58\u5728\u9ad8\u5ea6\u5c40\u90e8\u5316\u7684\u6a21\u6001\u9009\u62e9\u7279\u5f81\uff0c\u8bed\u97f3\u6761\u4ef6\u4e0b\u53ca\u975e\u4e3b\u5bfc\u6587\u5b57\u7cfb\u7edf\u7684\u89e3\u7801\u6fc0\u6d3b\u5206\u5e03\u66f4\u96c6\u4e2d\uff0c\u610f\u5473\u7740\u6a21\u578b\u66f4\u4f9d\u8d56\u5c11\u6570\u795e\u7ecf\u5143\u3002", "conclusion": "\u591a\u8bed\u8a00\u8bed\u97f3-\u6587\u672c\u6a21\u578b\u5185\u90e8\u5e76\u4e0d\u80fd\u5b8c\u5168\u6a21\u6001\u65e0\u5173\uff0c\u4e14\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u9ad8\u4f9d\u8d56\u6027\u53ef\u80fd\u5bfc\u81f4\u8de8\u6a21\u6001\u548c\u8bed\u8a00\u65f6\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u8fd9\u4e3a\u540e\u7eed\u6539\u8fdb\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18723", "abs": "https://arxiv.org/abs/2601.18723", "authors": ["Mengyuan Liu", "Juyi Sheng", "Peiming Li", "Ziyi Wang", "Tianming Xu", "Tiantian Xu", "Hong Liu"], "title": "Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods", "comment": null, "summary": "Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEval-Actions\u57fa\u51c6\u548cAutoEval\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u884c\u4e3a\u7684\u53ef\u4fe1\u8bc4\u6d4b\uff0c\u8d85\u8d8a\u4f20\u7edf\u4e8c\u5143\u6210\u529f\u7387\uff0c\u80fd\u533a\u5206\u6765\u6e90\u5e76\u91cf\u5316\u6267\u884c\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u89c6\u89c9-\u52a8\u4f5c\u53ca\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u80fd\u529b\u5feb\u901f\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u6210\u529f\u7387\uff0c\u96be\u4ee5\u8bc4\u4ef7\u673a\u5668\u884c\u4e3a\u7684\u53ef\u4fe1\u5ea6\uff0c\u5305\u62ec\u533a\u5206\u6765\u6e90\u771f\u5b9e\u6027\uff08\u5982\u4eba\u5de5\u9065\u64cd\u4f5c\u6216\u667a\u80fd\u4f53\u7b56\u7565\uff09\u548c\u6267\u884c\u8d28\u91cf\uff08\u5982\u5e73\u6ed1\u6027\u3001\u5b89\u5168\u6027\uff09\u3002\u56e0\u6b64\u4e9f\u9700\u66f4\u53ef\u9760\u3001\u591a\u5143\u7684\u8bc4\u6d4b\u4f53\u7cfb\u3002", "method": "1\uff09\u6784\u5efaEval-Actions\u6570\u636e\u96c6\uff0c\u5305\u542b\u4eba\u7c7b\u9065\u64cd\u4f5c\u3001\u653f\u7b56\u6267\u884c\u53ca\u5931\u8d25\u6848\u4f8b\uff0c\u5e76\u4ee5\u4e13\u5bb6\u8bc4\u5206\u3001\u504f\u597d\u6392\u5e8f\u548c\u94fe\u5f0f\u601d\u7ef4\u4e09\u7c7b\u76d1\u7763\u4fe1\u53f7\u6807\u6ce8\u30022\uff09\u63d0\u51faAutoEval\u67b6\u6784\uff0c\u57fa\u4e8e\u65f6\u7a7a\u805a\u5408\u5b9e\u73b0\u8bed\u4e49\u7ea7\u8bc4\u6d4b\uff0c\u5e76\u7528\u8f85\u52a9\u8fd0\u52a8\u5b66\u4fe1\u53f7\u6821\u6b63\u5e73\u6ed1\u6027\uff1b\u5347\u7ea7\u7248AutoEval-P\u5f15\u5165\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u4ee5\u52a0\u5f3a\u903b\u8f91\u63a8\u7406\u80fd\u529b\u30023\uff09\u901a\u8fc7\u591a\u534f\u8bae\u5bf9\u8bc4\u6d4b\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "AutoEval\u5728\u4e13\u5bb6\u8bc4\u5206\u548c\u504f\u597d\u6392\u5e8f\u534f\u8bae\u4e0b\uff0cSpearman\u79e9\u76f8\u5173\u7cfb\u6570\u5206\u522b\u8fbe\u52300.81\u548c0.84\u3002\u7cfb\u7edf\u53ef99.6%\u51c6\u786e\u533a\u5206\u667a\u80fd\u4f53\u548c\u4eba\u5de5\u793a\u8303\uff0c\u663e\u793a\u4e86\u4f18\u5f02\u7684\u6e90\u5224\u522b\u80fd\u529b\u3002", "conclusion": "Eval-Actions\u4e0eAutoEval\u4f53\u7cfb\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u4fe1\u8bc4\u6d4b\u6807\u51c6\uff1b\u901a\u8fc7\u5f15\u5165\u591a\u5143\u771f\u5b9e\u76d1\u7763\u4fe1\u53f7\u4e0e\u81ea\u52a8\u8bed\u4e49\u8bc4\u6d4b\u673a\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u66f4\u4e25\u683c\u3001\u53ef\u63a8\u5e7f\u7684\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u4fe1\u673a\u5668\u4eba\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2601.17185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17185", "abs": "https://arxiv.org/abs/2601.17185", "authors": ["Shima Salehi", "Atharva Agashe", "Andrew J. McFarland", "Joshua Peeples"], "title": "LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction", "comment": null, "summary": "We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u9891\u57df\u6b63\u5219\u5316\u7684\u5c11\u6837\u672c3D\u91cd\u5efa\u65b0\u65b9\u6cd5\uff0c\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u63d0\u5347\u7ec6\u8282\u4e0e\u51e0\u4f55\u7a33\u5b9a\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u5305\u542b\u56db\u4e2a\u5149\u8c31\u6ce2\u6bb5\u7684\u591a\u5149\u8c31\u6e29\u5ba4\u6570\u636e\u96c6\u53ca\u76f8\u5173\u8bc4\u6d4b\u5de5\u5177\u3002\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u81ea\u6709\u53ca\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6a21\u578b\u5728\u5c11\u89c6\u89d2\u3001\u7a00\u758f\u6570\u636e\u6761\u4ef6\u4e0b\u5bb9\u6613\u51fa\u73b0\u51e0\u4f55\u4e0d\u7a33\u5b9a\u548c\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\u3002\u9488\u5bf9\u771f\u5b9e\u573a\u666f\u4e0b\u91c7\u96c6\u6761\u4ef6\u53d7\u9650\u7684\u9700\u6c42\uff0c\u4e9f\u9700\u63d0\u5347\u7a00\u758f\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u878d\u5408\u5168\u5c40\u548c\u5c40\u90e8\u9891\u7387\u6b63\u5219\u7684\u65b9\u6cd5\u6765\u7ea6\u675f3D\u91cd\u5efa\u8fc7\u7a0b\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u5e76\u4fdd\u7559\u91cd\u8981\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u6784\u5efa\u5e76\u5f00\u653e\u4e86\u4e00\u4e2a\u6db5\u76d6\u56db\u4e2a\u5149\u8c31\u6ce2\u6bb5\u7684\u591a\u5149\u8c31\u6e29\u5ba4\u690d\u7269\u6570\u636e\u96c6\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u5c11\u6837\u672c\u91cd\u5efa\u8bc4\u6d4b\u5de5\u5177\u548c\u534f\u8bae\u3002", "result": "\u5728\u591a\u5149\u8c31\u6e29\u5ba4\u6570\u636e\u96c6\u53ca\u4e3b\u6d413D\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u91cd\u5efa\u6548\u679c\u66f4\u6e05\u6670\u3001\u7a33\u5b9a\uff0c\u5728\u5149\u8c31\u4e00\u81f4\u6027\u65b9\u9762\u4e5f\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5c11\u6837\u672c3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u9002\u5e94\u591a\u5149\u8c31\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u5e76\u63a8\u52a8\u4e86\u9886\u57df\u6807\u51c6\u5316\u8bc4\u6d4b\u73af\u5883\u7684\u6784\u5efa\uff0c\u5176\u6570\u636e\u4e0e\u4ee3\u7801\u7684\u5f00\u6e90\u6709\u5229\u4e8e\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.17397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17397", "abs": "https://arxiv.org/abs/2601.17397", "authors": ["Yucheng Hu", "Wei Zhou", "Juesi Xiao"], "title": "CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing", "comment": "EACL MME workshop paper", "summary": "Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLM-Bench\uff0c\u4e00\u4e2a\u4ee5\u4e2d\u6587\u6587\u5316\u4e3a\u57fa\u7840\u5e76\u517c\u5177\u82f1\u4e2d\u5bf9\u9f50\u7684\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\uff0c\u7528\u4ee5\u66f4\u516c\u5e73\u5e76\u6709\u6548\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u80fd\u529b\u3002\u4f5c\u8005\u63ed\u793a\u4e86\u73b0\u6709\u591a\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u8bc4\u4f30\u5b58\u5728\u7684\u504f\u5dee\uff0c\u5e76\u5b9e\u9a8c\u6027\u5206\u6790\u4e86\u8de8\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u76ee\u524d\u591a\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u5e38\u4ee5\u82f1\u6587\u6570\u636e\u96c6\u7ffb\u8bd1\u5f97\u6765\uff0c\u672a\u80fd\u4f53\u73b0\u76ee\u6807\u8bed\u8a00\u7684\u6587\u5316\u7279\u6027\uff0c\u5bfc\u81f4\u5bf9LLM\u4e2d\u6587\u80fd\u529b\u548c\u8de8\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u7684\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u539f\u751f\u4e2d\u6587\u6570\u636e\u96c6\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCLM-Bench\u57fa\u51c6\uff0c\u901a\u8fc7\u4e2d\u6587\u672c\u5730\u4f18\u5148\u6784\u5efa\u6db5\u76d6\u4e2d\u56fd\u6587\u5316\u80cc\u666f\u76841010\u7ec4\u9ad8\u8d28\u91cf\u7f16\u8f91\u5bf9\uff0c\u5e76\u5bf9\u5176\u4e0e\u82f1\u6587\u8fdb\u884c\u5bf9\u9f50\uff1b\u5728\u5178\u578bLLM\u5982Llama-3\u3001Qwen2\u7b49\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u7ed3\u5408\u5c42\u7ea7\u5411\u91cf\u51e0\u4f55\u5206\u6790\uff0c\u63a2\u8ba8\u4e86\u77e5\u8bc6\u7f16\u8f91\u5728\u4e2d\u82f1\u6587\u4e4b\u95f4\u7684\u8868\u73b0\u548c\u5e95\u5c42\u539f\u56e0\u3002", "result": "\u53d1\u73b0LLM\u7684\u77e5\u8bc6\u7f16\u8f91\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u96be\u4ee5\u4e92\u76f8\u8fc1\u79fb\uff0c\u5373\u5355\u8bed\u7684\u7f16\u8f91\u5f71\u54cd\u4e0d\u4f1a\u6709\u6548\u4f20\u64ad\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\u3002\u901a\u8fc7\u5411\u91cf\u51e0\u4f55\u5206\u6790\uff0c\u4e2d\u82f1\u6587\u7f16\u8f91\u5411\u91cf\u51e0\u4e4e\u6b63\u4ea4\uff0c\u5206\u522b\u5b58\u5728\u4e8e\u4e0d\u540c\u5b50\u7a7a\u95f4\uff1b\u6df7\u5408\u8bed\u8a00\u7f16\u8f91\u5219\u8868\u73b0\u4e3a\u7b80\u5355\u7ebf\u6027\u53e0\u52a0\u3002", "conclusion": "\u73b0\u6709\u8de8\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u8bed\u8a00\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5b9e\u8d28\u6027\u969c\u788d\uff0c\u5f3a\u8c03\u4e86\u4ee5\u6587\u5316\u4e3a\u57fa\u7840\u3001\u672c\u5730\u5316\u57fa\u51c6\u96c6\u7684\u91cd\u8981\u6027\u3002\u8be5\u5de5\u4f5c\u5bf9\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u548c\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u6539\u8fdb\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2601.18733", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18733", "abs": "https://arxiv.org/abs/2601.18733", "authors": ["Li Kang", "Heng Zhou", "Xiufeng Song", "Rui Li", "Bruno N. Y. Chen", "Ziye Wang", "Ximeng Meng", "Stone Tao", "Yiran Qin", "Xiaohong Liu", "Ruimao Zhang", "Lei Bai", "Yilun Du", "Hao Su", "Philip Torr", "Zhenfei Yin", "Ruihao Gong", "Yejun Zeng", "Fengjun Zhong", "Shenghao Jin", "Jinyang Guo", "Xianglong Liu", "Xiaojun Jia", "Tianqi Shan", "Wenqi Ren", "Simeng Qin", "Jialing Yang", "Xiaoyu Ma", "Tianxing Chen", "Zixuan Li", "Zijian Cai", "Yan Qin", "Yusen Qin", "Qiangyu Chen", "Kaixuan Wang", "Zhaoming Han", "Yao Mu", "Ping Luo", "Yuanqi Yao", "Haoming Song", "Jan-Nico Zaech", "Fabien Despinoy", "Danda Pani Paudel", "Luc Van Gool"], "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge", "comment": "MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/", "summary": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86MARS\u6311\u6218\u8d5b\uff0c\u8be5\u7ade\u8d5b\u65e8\u5728\u63a8\u52a8\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7814\u7a76\uff0c\u7279\u522b\u5173\u6ce8\u89c4\u5212\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u4fc3\u8fdb\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u5177\u8eab\u667a\u80fdAI\u9886\u57df\u65e5\u6e10\u590d\u6742\u5316\uff0c\u5355\u4e00\u667a\u80fd\u4f53\u5df2\u65e0\u6cd5\u6ee1\u8db3\u6269\u5c55\u6027\u548c\u534f\u4f5c\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u4e3a\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e0e\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u5e76\u4e3e\u529e\u4e86MARS\u6311\u6218\u8d5b\uff0c\u5206\u4e3a\u89c4\u5212\u4e0e\u63a7\u5236\u4e24\u4e2a\u6838\u5fc3\u8d5b\u9053\u3002\u53c2\u8d5b\u8005\u9700\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5f00\u5c55\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u6267\u884c\u653f\u7b56\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002\u901a\u8fc7\u6bd4\u62fc\u591a\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u5206\u914d\u53ca\u6267\u884c\u6548\u7387\uff0c\u8bc4\u4f30\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u7ade\u8d5b\uff0c\u6536\u96c6\u4e86\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u548c\u5b9e\u9a8c\u6570\u636e\uff0c\u83b7\u5f97\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u534f\u8c03\u8bbe\u8ba1\u3001\u4efb\u52a1\u5206\u914d\u548c\u6267\u884c\u7b49\u65b9\u9762\u7684\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "MARS\u6311\u6218\u8d5b\u62d3\u5c55\u4e86\u591a\u667a\u80fd\u4f53\u5177\u8eabAI\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u8fb9\u754c\uff0c\u4fc3\u8fdb\u4e86\u5408\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\uff1b\u4e3a\u672a\u6765\u667a\u80fd\u534f\u4f5cAI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17194", "abs": "https://arxiv.org/abs/2601.17194", "authors": ["Cheyu Lin", "Katherine A. Flanigan", "Sirajum Munir"], "title": "Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments", "comment": null, "summary": "Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize \"interaction\" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DUET\u6570\u636e\u96c6\u53ca\u5185\u5d4c\u80a2\u4f53\u8bed\u8a00\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u6d4b\u91cf\u548c\u5206\u6790\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u7b49\u7a7a\u95f4\u4e2d\u7684\u6709\u610f\u4e49\u793e\u4ea4\u4e92\u52a8\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5728\u571f\u6728\u5de5\u7a0b\u53ca\u5efa\u6210\u73af\u5883\u4e2d\uff0c\u7f3a\u4e4f\u4e00\u81f4\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u548c\u8868\u5f81\u7a7a\u95f4\u4e2d\u7684\u793e\u4f1a\u6027\u4e92\u52a8\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u5e72\u9884\u6548\u679c\u96be\u4ee5\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8be5\u9886\u57df\u5b58\u5728\u7684\u6d4b\u91cf\u53ca\u5206\u6790\u65b9\u6cd5\u5b66\u7f3a\u53e3\uff0c\u63d0\u5347\u793e\u4f1a\u8d44\u672c\u76f8\u5173\u4e92\u52a8\u7684\u91cf\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f15\u5165DUET\uff08Dyadic User Engagement DataseT\uff09\u6570\u636e\u96c6\uff0c\u6db5\u76d612\u79cd\u4e8c\u4eba\u4e92\u52a8\u573a\u666f\u548c\u4e94\u7c7b\u80a2\u4f53\u8bed\u8a00\u529f\u80fd\uff0c\u540c\u65f6\u7ed3\u5408\u56db\u79cd\u611f\u77e5\u65b9\u5f0f\u548c\u4e09\u79cd\u7a7a\u95f4\u7c7b\u578b\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u6846\u67b6\u76f4\u63a5\u4ece\u9aa8\u9abc\u8fd0\u52a8\u63a8\u65ad\u4ea4\u9645\u529f\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u4e49\u52a8\u4f5c\u529f\u80fd\u5b57\u5178\uff0c\u5e76\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5bf9\u516d\u79cd\u4e3b\u6d41\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cf\u5316\u4e86\u4ea4\u9645\u529f\u80fd\u8bc6\u522b\u7684\u96be\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u5355\u4eba\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u793e\u4f1a\u4e92\u52a8\u6d4b\u91cf\u4e0a\u7684\u5c40\u9650\u6027\u3002\u6846\u67b6\u53ef\u4ee5\u5f88\u597d\u5730\u533a\u5206\u80a2\u4f53\u8bed\u8a00\u529f\u80fd\uff0c\u5e76\u5c55\u793a\u6a21\u578b\u8868\u793a\u80fd\u529b\u4e0e\u5206\u7c7b\u6027\u80fd\u7684\u5f3a\u5173\u8054\uff0c\u5728\u4e0d\u540c\u88ab\u8bd5\u548c\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u4e3a\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u4e2d\u793e\u4ea4\u4e92\u52a8\u7684\u9690\u79c1\u4fdd\u62a4\u6d4b\u91cf\u4e0e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u7406\u89e3\u4e0e\u8bbe\u8ba1\u4fc3\u8fdb\u793e\u4f1a\u8d44\u672c\u5f62\u6210\u7684\u7269\u7406\u7a7a\u95f4\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u7406\u8bba\u9a8c\u8bc1\u4e0e\u5b9e\u8df5\u4f18\u5316\u3002"}}
{"id": "2601.17421", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17421", "abs": "https://arxiv.org/abs/2601.17421", "authors": ["Jaehui Hwang", "Dongyoon Han", "Sangdoo Yun", "Byeongho Heo"], "title": "Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning", "comment": null, "summary": "The emergence of discourse-like tokens such as \"wait\" and \"therefore\" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the \"wait\" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u7c7b\u8bdd\u8bed\u8bcd\uff08\u5982\u201cwait\u201d\u201ctherefore\u201d\uff09\u7684\u4fe1\u53f7\u53ca\u5176\u968f\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u89c4\u6a21\u7684\u53d8\u5316\u5173\u7cfb\u3002", "motivation": "\u5148\u524d\u6ce8\u610f\u5230LLM\u751f\u6210\u7c7b\u8bdd\u8bed\u8bcd\u53ef\u80fd\u4e0e\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e9b\u4fe1\u53f7\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u6a21\u578b\u89c4\u6a21\u4e4b\u95f4\u7cfb\u7edf\u6027\u7684\u91cf\u5316\u5206\u6790\u3002", "method": "\u4f5c\u8005\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u89c4\u6a21\u4e0b\uff0c\u9488\u5bf9\u7279\u5b9atoken\u7684\u6982\u7387\u8f93\u51fa\uff0c\u5c24\u5176\u5173\u6ce8\u5982\u201cwait\u201d\u8fd9\u6837\u7684token\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u7279\u5b9atoken\uff08\u5982\u201cwait\u201d\u201ctherefore\u201d\uff09\u7684\u51fa\u73b0\u6982\u7387\u4e0e\u63a8\u7406\u6b63\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\uff1b\u8fd9\u4e9b\u4fe1\u53f7\u968f\u8bad\u7ec3\u7b56\u7565\u4e0d\u540c\u800c\u53d8\u5316\uff0c\u4f46\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u95f4\u8868\u73b0\u7a33\u5b9a\u3002\u5fae\u8c03\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u7684\u6a21\u578b\u80fd\u901a\u8fc7\u8fd9\u4e9b\u4fe1\u53f7\u83b7\u5f97\u4e00\u5b9a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u4fe1\u53f7\u3002", "conclusion": "\u7c7b\u8bdd\u8bedtoken\u6982\u7387\u4e3a\u7406\u89e3LLM\u63a8\u7406\u52a8\u6001\u63d0\u4f9b\u4e86\u6709\u6548\u89c6\u89d2\uff0c\u7cfb\u7edf\u5316\u5206\u6790\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u63ed\u793a\u6a21\u578b\u5185\u90e8\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2601.18765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18765", "abs": "https://arxiv.org/abs/2601.18765", "authors": ["Shutong Chen", "Adnan Aijaz", "Yansha Deng"], "title": "Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery", "comment": "Submit to IEEE for potential publication", "summary": "Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u5411\u76ee\u6807\u7684\u901a\u4fe1\uff08GoC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u5de5\u5382\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5feb\u901f\u4e14\u9c81\u68d2\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff08FDR\uff09\uff0c\u6709\u6548\u964d\u4f4e\u4e86FDR\u65f6\u95f4\u5e76\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\u6846\u67b6\u5b58\u5728\u901a\u4fe1\u548c\u8ba1\u7b97\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u673a\u5668\u4eba\u8fd0\u52a8/\u8f68\u8ff9\u751f\u6210\u65b9\u9762\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\uff083C\uff09\u95ed\u73af\u672a\u56f4\u7ed5FDR\u76ee\u6807\u5171\u540c\u8bbe\u8ba1\uff0c\u4ece\u800c\u5f71\u54cd\u5b9e\u9645\u751f\u4ea7\u4e2d\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "method": "GoC\u6846\u67b6\u8054\u5408\u8bbe\u8ba13C\u95ed\u73af\uff0c\u4f18\u5316FDR\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002\u5176\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u8bbe\u8ba1\u8868\u793a\u63d0\u53d6\u5668\uff0c\u63d0\u53d63D\u573a\u666f\u56fe\uff083D-SG\uff09\u4f5c\u4e3a\u8bed\u4e49\u8868\u793a\uff0c\u901a\u8fc7\u76d1\u63a73D-SG\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u53d8\u5316\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\uff1b2\uff09\u91c7\u7528LoRA\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u63a8\u7406\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u81ea\u52a8\u751f\u6210\u5931\u8d25\u540e\u7684\u673a\u5668\u4eba\u6062\u590d\u52a8\u4f5c\uff1b3\uff09\u63d0\u51fa\u8f7b\u91cf\u5316\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u6a21\u5757\uff0c\u5728\u9700\u7cbe\u7ec6\u63a7\u5236\u65f6\uff0c\u4ec5\u7528\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u7269\u4f53\u8f6e\u5ed3\u5bf9SLM\u751f\u6210\u7684\u52a8\u4f5c\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5927\u91cf\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cGoC\u6846\u67b6\u76f8\u6bd4\u5f53\u524d\u4e3b\u6d41\u4f9d\u8d56\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u6545\u969c\u3001\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u6062\u590d\u7684\u4f53\u7cfb\uff0c\u5728FDR\u65f6\u95f4\u4e0a\u6700\u591a\u51cf\u5c1182.6%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6700\u591a\u63d0\u9ad876%\u3002", "conclusion": "\u8054\u5408\u8bbe\u8ba1\u76843C\u95ed\u73af\u548c\u521b\u65b0\u7684\u611f\u77e5-\u63a8\u7406-\u6062\u590d\u4f53\u7cfb\u6781\u5927\u63d0\u5347\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u667a\u80fd\u5de5\u5382\u73af\u5883\u4e2d\u7684FDR\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5177\u5907\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17211", "abs": "https://arxiv.org/abs/2601.17211", "authors": ["Anzhe Cheng", "Italo Ivo Lima Dias Pinto", "Paul Bogdan"], "title": "Structural Complexity of Brain MRI reveals age-associated patterns", "comment": "accepted by icassp2026", "summary": "We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e09\u7ef4\u4fe1\u53f7\uff08\u5c24\u5176\u662f\u8111MRI\uff09\u7684\u7ed3\u6784\u590d\u6742\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u7c97\u5316\u65b9\u6848\uff0c\u5e76\u7528\u4e8e\u5206\u6790MRI\u6570\u636e\uff0c\u8bc1\u660e\u5176\u5728\u751f\u7269\u5e74\u9f84\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8111MRI\u7b49\u4e09\u7ef4\u4fe1\u53f7\u6570\u636e\u5177\u6709\u591a\u5c3a\u5ea6\u548c\u9ad8\u590d\u6742\u6027\uff0c\u73b0\u6709\u7ed3\u6784\u590d\u6742\u6027\u5206\u6790\u96be\u4ee5\u7a33\u5b9a\u5730\u5904\u7406\u7c97\u7a7a\u95f4\u5c3a\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u91c7\u6837\u6709\u9650\u65f6\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u7a33\u5065\u3001\u7ec6\u81f4\u7684\u65b9\u6cd5\u6765\u523b\u753b\u591a\u5c3a\u5ea6\u7ed3\u6784\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u751f\u7269\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ed1\u52a8\u7a97\u53e3\u7c97\u5316(coarse-graining)\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u5206\u5757\u65b9\u5f0f\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4e0b\u7684\u7ed3\u6784\u590d\u6742\u6027\u6d4b\u91cf\u3002\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5bf9MRI\u5927\u6570\u636e\u96c6\u7684\u4f53\u7d20\u4fe1\u606f\u8fdb\u884c\u591a\u5c3a\u5ea6\u5904\u7406\uff0c\u91cf\u5316\u4fe1\u53f7\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u4fe1\u606f\u635f\u5931\u3002", "result": "\u7ecf\u8fc7\u65b0\u7684\u6ed1\u52a8\u7a97\u53e3\u7c97\u5316\u5206\u6790\uff0c\u5bf9\u5927\u91cf\u4e2d\u8001\u5e74MRI\u7ed3\u6784\u6570\u636e\u53d1\u73b0\uff1a\u7ed3\u6784\u590d\u6742\u6027\u6307\u6807\u968f\u5e74\u9f84\u7cfb\u7edf\u6027\u51cf\u4f4e\uff0c\u4e14\u5728\u8f83\u7c97\u5c3a\u5ea6\u4e0b\u6548\u679c\u6700\u663e\u8457\u3002", "conclusion": "\u7ed3\u6784\u590d\u6742\u6027\u65b9\u6cd5\u80fd\u53ef\u9760\u6355\u6349MRI\u7b49\u4e09\u7ef4\u4fe1\u53f7\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u662f\u6709\u6548\u7684\u4fe1\u53f7\u5904\u7406\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u751f\u7269\u5e74\u9f84\u7b49\u91cd\u8981\u6307\u6807\u7684\u9884\u6d4b\u3002"}}
{"id": "2601.17443", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17443", "abs": "https://arxiv.org/abs/2601.17443", "authors": ["Ondrej Bohdal", "Pramit Saha", "Umberto Michieli", "Mete Ozay", "Taha Ceritli"], "title": "Clustering-driven Memory Compression for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u8bb0\u5fc6\u538b\u7f29\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u751f\u6210\u3002\u76f8\u8f83\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u65b0\u65b9\u6cd5\u5728\u51cf\u5c11\u8bb0\u5fc6token\u6570\u91cf\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u505a\u6cd5\u901a\u8fc7\u7b80\u5355\u62fc\u63a5\u6216\u5e73\u5747\u7528\u6237\u8bb0\u5fc6\u6765\u5b9e\u73b0\u4e2a\u6027\u5316\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u5feb\u901f\u8017\u5c3d\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u8981\u4e48\u5bfc\u81f4\u8bed\u4e49\u51b2\u7a81\u548c\u6548\u679c\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u517c\u987e\u9ad8\u6548\u6027\u4e0e\u8d28\u91cf\u7684\u8bb0\u5fc6\u538b\u7f29\u624b\u6bb5\u3002", "method": "\u5c06\u7528\u6237\u8fc7\u5f80\u7684\u8bb0\u5fc6\u6309\u7167\u76f8\u4f3c\u6027\u805a\u7c7b\uff0c\u6bcf\u4e2a\u805a\u7c7b\u5185\u518d\u8fdb\u884c\u4fe1\u606f\u5408\u5e76\uff0c\u6700\u540e\u518d\u62fc\u63a5\u5230\u8f93\u5165\u63d0\u793a\u4e2d\u3002\u8fd9\u79cd\u805a\u7c7b\u5408\u5e76\u65b9\u5f0f\u51cf\u5c11\u4e86\u5197\u4f59\uff0c\u5e76\u4fdd\u7559\u4e86\u4fe1\u606f\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bb0\u5fc6token\u6570\u91cf\uff0c\u5e76\u5728\u5404\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u76f4\u63a5\u62fc\u63a5\u548c\u7b80\u5355\u5e73\u5747\u3002\u5bf9\u4e8e\u76f8\u540c\u7684\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\uff0c\u4e5f\u80fd\u63d0\u4f9b\u66f4\u7d27\u51d1\u4e14\u8868\u73b0\u66f4\u4f18\u7684\u4e2a\u6027\u5316\u8bb0\u5fc6\u3002", "conclusion": "\u805a\u7c7b\u9a71\u52a8\u7684\u8bb0\u5fc6\u5408\u5e76\u7b56\u7565\u517c\u987e\u4e86\u4e0a\u4e0b\u6587\u5229\u7528\u6548\u7387\u548c\u4e2a\u6027\u5316\u8d28\u91cf\uff0c\u5728\u4e2a\u6027\u5316\u751f\u6210\u573a\u666f\u4e2d\u4e3aLLM\u5e26\u6765\u65b0\u63d0\u5347\uff0c\u4f18\u4e8e\u5e38\u89c4\u538b\u7f29\u4e0e\u62fc\u63a5\u65b9\u6cd5\u3002"}}
{"id": "2601.17885", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17885", "abs": "https://arxiv.org/abs/2601.17885", "authors": ["Qingyu Fan", "Zhaoxiang Li", "Yi Lu", "Wang Chen", "Qiu Shen", "Xiao-xiao Long", "Yinghao Cai", "Tao Lu", "Shuo Wang", "Xun Cao"], "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation", "comment": null, "summary": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.\n  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.\n  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.\n  Project website: https://peafowlvla.github.io/.", "AI": {"tldr": "PEAfowl\u662f\u4e00\u79cd\u63d0\u5347\u53cc\u624b\u64cd\u4f5c\u5728\u590d\u6742\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\uff0c\u5229\u7528\u521b\u65b0\u7684\u591a\u89c6\u89d2\u7a7a\u95f4\u8868\u8fbe\u548c\u66f4\u7ec6\u81f4\u7684\u8bed\u8a00\u6307\u4ee4\u7ed3\u5408\u65b9\u5f0f\uff0c\u5b9e\u73b0\u7a33\u5065\u64cd\u4f5c\u548c\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u906e\u6321\u3001\u591a\u89c6\u89d2\u53d8\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u7a7a\u95f4\u7406\u89e3\u548c\u6307\u4ee4\u7ed3\u5408\u65b9\u5f0f\u6709\u9650\uff0c\u96be\u4ee5\u6cdb\u5316\u548c\u7cbe\u51c6\u7406\u89e3\u4efb\u52a1\u6307\u4ee4\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u63d0\u5347\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\u3002", "method": "PEAfowl\u901a\u8fc7\u9884\u6d4b\u6bcf\u4e2atoken\u7684\u6df1\u5ea6\u5206\u5e03\uff0c\u5b9e\u73b0\u53ef\u5fae\u5206\u76843D\u7279\u5f81\u53d8\u6362\uff0c\u5e76\u8de8\u89c6\u89d2\u805a\u5408\u90bb\u57df\u4fe1\u606f\uff0c\u83b7\u5f97\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u7a7a\u95f4\u8868\u8fbe\uff1b\u91c7\u7528Perceiver\u98ce\u683c\u7684\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u8bfb\u53d6\u65b9\u5f0f\uff0c\u5728\u51bb\u7ed3CLIP\u89c6\u89c9\u7279\u5f81\u7684\u57fa\u7840\u4e0a\u8fed\u4ee3\u7d2f\u79ef\u201c\u8bc1\u636e\u201d\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u8bed\u8a00\u6307\u4ee4\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8bad\u7ec3\u671f\u95f4\u5f15\u5165\u6df1\u5ea6\u84b8\u998f\uff0c\u5b9e\u73b0\u5bf9\u4f4e\u8d28\u91cf\u6df1\u5ea6\u6570\u636e\u611f\u77e5\u7684\u51e0\u4f55\u610f\u8bc6\u63d0\u5347\uff0c\u65e0\u9700\u589e\u52a0\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5728RoboTwin 2.0\u5e73\u53f0\u7684\u57df\u968f\u673a\u5316\u8bbe\u7f6e\u4e0b\uff0cPEAfowl\u5c06\u6700\u5f3a\u57fa\u7ebf\u7684\u6210\u529f\u7387\u63d0\u5347\u4e8623.0\u4e2a\u767e\u5206\u70b9\uff1b\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u5176\u826f\u597d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u548c\u6df1\u5ea6\u84b8\u998f\u5e26\u6765\u7684\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "PEAfowl\u663e\u8457\u63d0\u5347\u4e86\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u591a\u89c6\u89d2\u573a\u666f\u4e0b\u7684\u7a33\u5065\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u65b9\u6cd5\u5bf9\u7a7a\u95f4\u7406\u89e3\u53ca\u6307\u4ee4\u7ed3\u5408\u7684\u63d0\u5347\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5e26\u6765\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17216", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17216", "abs": "https://arxiv.org/abs/2601.17216", "authors": ["Murat Arda Onsu", "Poonam Lohan", "Burak Kantarci", "Aisha Syed", "Matthew Andrews", "Sean Kennedy"], "title": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction", "comment": "6 pages 5 figures, accepted to IEEE ICC 2026", "summary": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49V2X\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u78b0\u649e\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u8bed\u4e49\u5d4c\u5165\u66ff\u4ee3\u539f\u59cb\u89c6\u9891\u4f20\u8f93\uff0c\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u9700\u6c42\u4e14\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u8f66\u8f86\u901a\u4fe1\u4f20\u9012\u539f\u59cb\u89c6\u9891\u6216\u9ad8\u7ef4\u611f\u77e5\u6570\u636e\uff0c\u53d7\u9650\u4e8e\u5e26\u5bbd\u548c\u5ef6\u8fdf\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u4e0e\u5904\u7406\u65b9\u5f0f\u3002", "method": "RSU\u7aef\u91c7\u7528V-JEPA\u67b6\u6784\u5bf9\u672a\u6765\u5e27\u505a\u65f6\u7a7a\u8bed\u4e49\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7V2X\u53d1\u9001\u7ed9\u8f66\u8f86\u3002\u8f66\u8f86\u7aef\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u63a2\u6d4b\u5668\u548c\u5206\u7c7b\u5668\u89e3\u7801\u8bed\u4e49\u5d4c\u5165\uff0c\u4ece\u800c\u9884\u6d4b\u5373\u5c06\u53d1\u751f\u7684\u78b0\u649e\u3002\u5b9e\u9a8c\u5229\u7528\u57ce\u5e02\u4ea4\u901a\u6570\u5b57\u5b6a\u751f\u73af\u5883\u751f\u6210\u591a\u6837\u573a\u666f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u65b0\u6846\u67b6\u5728\u4e0d\u635f\u5931\u9884\u6d4b\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u901a\u4fe1\u9700\u6c42\u964d\u4f4e\u4e86\u56db\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u628a\u78b0\u649e\u9884\u6d4bF1\u503c\u63d0\u5347\u4e8610%\u3002", "conclusion": "\u8be5\u8bed\u4e49V2X\u901a\u4fe1\u67b6\u6784\u80fd\u9ad8\u6548\u652f\u6301\u5b9e\u65f6\u534f\u4f5c\u78b0\u649e\u9884\u6d4b\uff0c\u663e\u793a\u51fa\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u843d\u5730\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.17530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17530", "abs": "https://arxiv.org/abs/2601.17530", "authors": ["Gautam Siddharth Kashyap", "Harsh Joshi", "Niharika Jain", "Ebad Shabbir", "Jiechao Gao", "Nipun Joshi", "Usman Naseem"], "title": "Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes", "comment": "Accepted at EACL Findings 2026", "summary": "The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6ConLLM\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u97f3\u9891\u3001\u89c6\u9891\u53ca\u97f3\u89c6\u9891\u4f2a\u9020\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\u4e4b\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u514b\u670d\u6a21\u6001\u788e\u7247\u5316\u548c\u589e\u5f3a\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86ConLLM\u6846\u67b6\uff0c\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u5404\u6a21\u6001\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5d4c\u5165\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5f3a\u5316\u5bf9\u7ec6\u81f4\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "ConLLM\u5728\u97f3\u9891\u3001\u89c6\u9891\u3001\u97f3\u89c6\u9891\u591a\u6a21\u6001\u4e0a\u90fd\u5b9e\u73b0\u4e86\u660e\u663e\u6027\u80fd\u63d0\u5347\uff0c\u97f3\u9891\u4f2a\u9020EER\u964d\u4f4e50%\uff0c\u89c6\u9891\u63d0\u53478%\u51c6\u786e\u7387\uff0c\u97f3\u89c6\u9891\u7efc\u5408\u63d0\u9ad8\u7ea69%\uff0c\u6d88\u878d\u5b9e\u9a8c\u8868\u660ePTM\u5d4c\u5165\u5177\u67099%-10%\u7684\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "ConLLM\u5728\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u548c\u66f4\u9ad8\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4e3a\u63d0\u5347\u73b0\u5b9e\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u53ef\u7528\u6027\u548c\u9c81\u68d2\u6027\u5e26\u6765\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2601.17895", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17895", "abs": "https://arxiv.org/abs/2601.17895", "authors": ["Bin Tan", "Changjiang Sun", "Xiage Qin", "Hanat Adai", "Zelin Fu", "Tianxiang Zhou", "Han Zhang", "Yinghao Xu", "Xing Zhu", "Yujun Shen", "Nan Xue"], "title": "Masked Depth Modeling for Spatial Perception", "comment": "Tech report, 19 pages, 15 figures and 4 tables", "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLingBot-Depth\u7684\u6df1\u5ea6\u8865\u5168\u6a21\u578b\uff0c\u80fd\u591f\u5229\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u5bf9\u6709\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\u8fdb\u884c\u5b8c\u5584\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u5904\u7406\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u6a21\u578b\u5728\u6df1\u5ea6\u7cbe\u5ea6\u4e0e\u50cf\u7d20\u8986\u76d6\u7387\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41RGB-D\u76f8\u673a\u3002", "motivation": "\u73b0\u6709RGB-D\u76f8\u673a\u5728\u6355\u6349\u771f\u5b9e\u4e09\u7ef4\u573a\u666f\u65f6\u53d7\u9650\u4e8e\u786c\u4ef6\u548c\u6210\u50cf\u6761\u4ef6\uff08\u5982\u9ad8\u53cd\u5149\u6216\u65e0\u7eb9\u7406\u8868\u9762\uff09\uff0c\u5bfc\u81f4\u6df1\u5ea6\u56fe\u5b58\u5728\u7f3a\u5931\u6216\u8bef\u5dee\uff0c\u4e3a\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6709\u6548\u8865\u5168\u548c\u63d0\u5347\u6df1\u5ea6\u4fe1\u606f\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u201c\u6709\u906e\u7f69\u201d\u7684\u8bef\u5dee\u4fe1\u53f7\u53cd\u6620\u4e86\u5e95\u5c42\u7684\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\uff0c\u6545\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLingBot-Depth\u6df1\u5ea6\u8865\u5168\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u6df1\u5ea6\u56fe\u4e2d\u63a9\u853d\u90e8\u5206\u50cf\u7d20\uff08masked depth modeling\uff09\uff0c\u5229\u7528RGB\u56fe\u50cf\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u63a8\u7406\u5e76\u8865\u5168\u7f3a\u5931\u6df1\u5ea6\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u6d41\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u6700\u7ec8\u516c\u5f00\u4e86\u542b\u6709\u5927\u91cf\u771f\u5b9e\u548c\u6a21\u62df\u914d\u5bf9\u6570\u636e\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u53ca\u6a21\u578b\u3002", "result": "LingBot-Depth\u6a21\u578b\u5728\u6df1\u5ea6\u56fe\u8865\u5168\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u662f\u6df1\u5ea6\u7cbe\u5ea6\u8fd8\u662f\u50cf\u7d20\u8986\u76d6\u7387\uff0c\u5747\u4f18\u4e8e\u9ad8\u7ea7RGB-D\u76f8\u673a\u3002\u6a21\u578b\u5bf9\u4e0b\u6e38\u4efb\u52a1\u4e5f\u9002\u7528\uff0c\u5e76\u5728RGB\u4e0e\u6df1\u5ea6\u6a21\u6001\u95f4\u5f62\u6210\u4e86\u5bf9\u9f50\u7684\u6f5c\u5728\u8868\u5f81\u3002", "conclusion": "LingBot-Depth\u6709\u6548\u6539\u5584\u4e86\u53d7\u786c\u4ef6\u548c\u6210\u50cf\u6761\u4ef6\u5f71\u54cd\u7684\u6df1\u5ea6\u56fe\u8d28\u91cf\uff0c\u5bf9\u4e09\u7ef4\u7a7a\u95f4\u89c6\u89c9\u611f\u77e5\u548c\u76f8\u5173\u5e94\u7528\u5177\u6709\u79ef\u6781\u63a8\u52a8\u4f5c\u7528\u3002\u516c\u5f00\u7684\u5927\u89c4\u6a21\u6570\u636e\u548c\u6a21\u578b\u8d44\u6e90\u4fc3\u8fdb\u4e86\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.17228", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.17228", "abs": "https://arxiv.org/abs/2601.17228", "authors": ["Tengyue Zhang", "Ruiwen Ding", "Luoting Zhuang", "Yuxiao Wu", "Erika F. Rodriguez", "William Hsu"], "title": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification", "comment": null, "summary": "Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u534a\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08SSDA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u65e2\u4fdd\u7559\u7ec4\u7ec7\u5f62\u6001\u53c8\u5177\u6709\u76ee\u6807\u57df\u7279\u5f81\u7684\u5408\u6210\u75c5\u7406\u56fe\u50cf\uff0c\u63d0\u9ad8\u8de8\u961f\u5217\u3001\u8de8\u673a\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u56e0\u9886\u57df\u95f4\u5206\u5e03\u5dee\u5f02\uff08domain shift\uff09\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6cd5\u5f80\u5f80\u4e0d\u80fd\u6709\u6548\u5229\u7528\u76ee\u6807\u57df\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u6216\u4f9d\u8d56\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\uff0c\u53ef\u80fd\u5bfc\u81f4\u7ec4\u7ec7\u7ed3\u6784\u626d\u66f2\u3001\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u3002\u56e0\u6b64\u9700\u8981\u521b\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u9886\u57df\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u534a\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u65e2\u4fdd\u7559\u6e90\u57df\u7ec4\u7ec7\u7ed3\u6784\u53c8\u517c\u5177\u76ee\u6807\u57df\u5916\u89c2\u7279\u5f81\u7684\u5408\u6210\u56fe\u50cf\u3002\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u65f6\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7279\u5f81\u3001\u961f\u5217\u6807\u8bc6\u3001\u6837\u672c\u5236\u5907\u65b9\u5f0f\u7b49\u6761\u4ef6\uff0c\u751f\u6210\u76ee\u6807\u611f\u77e5\u5408\u6210\u56fe\u50cf\u3002\u7528\u8fd9\u4e9b\u5408\u6210\u56fe\u7247\u4e0e\u6e90\u57df\u6807\u6ce8\u56fe\u7247\u7ed3\u5408\uff0c\u8bad\u7ec3\u4e0b\u6e38\u5206\u7c7b\u5668\uff0c\u5e76\u5728\u76ee\u6807\u57df\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u80ba\u817a\u764c\u9884\u540e\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u76ee\u6807\u57df\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u76ee\u6807\u57df\u52a0\u6743F1\u8bc4\u5206\u4ece0.611\u63d0\u5347\u52300.706\uff0c\u5b8f\u5e73\u5747F1\u5206\u6570\u4ece0.641\u63d0\u5347\u52300.716\uff0c\u4e14\u672a\u5bfc\u81f4\u6e90\u57df\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u76ee\u6807\u57df\u7279\u5f81\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8ba1\u7b97\u75c5\u7406\u9886\u57df\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u6709\u524d\u666f\u4e14\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u53ca\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2601.17532", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17532", "abs": "https://arxiv.org/abs/2601.17532", "authors": ["Zhipeng Song", "Yizhi Zhou", "Xiangyu Kong", "Jiulong Jiao", "Xinrui Bao", "Xu You", "Xueqing Shi", "Yuhang Zhou", "Heng Qi"], "title": "Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection", "comment": "26 pages, 10 figures", "summary": "Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \\textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u68c0\u7d22\u6587\u6bb5\u7b5b\u9009\u65b9\u6cd5\u4fe1\u606f\u589e\u76ca\u526a\u679d(IGP)\uff0c\u80fd\u663e\u8457\u63d0\u5347\u95ee\u7b54\u8d28\u91cf\u5e76\u964d\u4f4e\u8f93\u5165\u8d1f\u62c5\u3002", "motivation": "\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\uff0c\u5982\u4f55\u9009\u62e9\u6700\u6709\u6548\u7684\u68c0\u7d22\u6587\u6bb5\u7528\u4e8e\u652f\u6301\u751f\u6210\u6a21\u578b\u4ecd\u662f\u6838\u5fc3\u96be\u9898\u3002\u4f20\u7edf\u68c0\u7d22\u76f8\u5173\u6027\u6307\u6807\uff08\u5982NDCG\uff09\u5728\u591a\u6587\u6bb5\u6ce8\u5165\u65f6\u4e0e\u95ee\u7b54\u8d28\u91cf\u5173\u8054\u5f31\u751a\u81f3\u8d1f\u76f8\u5173\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u51b2\u7a81\u5f71\u54cd\u751f\u6210\u3002", "method": "\u4f5c\u8005\u63d0\u51faIGP\uff08\u4fe1\u606f\u589e\u76ca\u526a\u679d\uff09\uff0c\u5176\u4e3b\u8981\u601d\u60f3\u662f\u4ee5\u751f\u6210\u5668\u6548\u7528\u4fe1\u53f7\u4e3a\u4f9d\u636e\uff0c\u91cd\u6392\u5e8f\u5e76\u8fc7\u6ee4\u68c0\u7d22\u6587\u6863\uff0c\u53bb\u9664\u65e0\u76ca\u6216\u6709\u5bb3\u6587\u6bb5\u540e\u518d\u6ce8\u5165\u751f\u6210\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6539\u53d8\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u9884\u7b97\u63a5\u53e3\uff0c\u6613\u4e8e\u90e8\u7f72\u3002", "result": "\u5728\u4e94\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u3001\u4e0d\u540c\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u8bbe\u7f6e\u4e0b\uff0cIGP\u65b9\u6cd5\u7a33\u5b9a\u63d0\u5347\u4e86\u8d28\u91cf-\u6210\u672c\u6743\u8861\u6548\u679c\u3002\u5728\u8981\u6c42\u6ce8\u5165\u591a\u6761\u8bc1\u636e\u7684\u5178\u578b\u573a\u666f\u4e0b\uff0c\u5e73\u5747F1\u76f8\u5bf9\u63d0\u534712-20%\uff0c\u8f93\u5165token\u6570\u91cf\u5374\u51cf\u5c1176-79%\u3002", "conclusion": "IGP\u53ef\u65e0\u7f1d\u96c6\u6210\u4e8e\u73b0\u6709RAG\u7cfb\u7edf\uff0c\u6709\u6548\u63d0\u5347\u95ee\u7b54\u751f\u6210\u8d28\u91cf\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u6587\u6bb5\u9009\u62e9\u63d0\u4f9b\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2601.18714", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18714", "abs": "https://arxiv.org/abs/2601.18714", "authors": ["Judith Vilella-Cantos", "Mauro Martini", "Marcello Chiaberge", "M\u00f3nica Ballesta", "David Valiente"], "title": "Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning", "comment": null, "summary": "Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8461\u8404\u56ed\u81ea\u52a8\u5316\u73af\u5883\u7684\u9ad8\u6548\u5b9a\u4f4d\u65b9\u6cd5MinkUNeXt-VINE\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u6280\u672f\u7684\u8d85\u8d8a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u3001\u7a00\u758fLiDAR\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u7ed3\u6784\u4e0d\u89c4\u5219\u3001\u7f3a\u4e4f\u660e\u663e\u5730\u6807\uff0c\u5bfc\u81f4\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u4efb\u52a1\u56f0\u96be\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u76ee\u6807\u5206\u7c7b\u548c\u5206\u5272\uff0c\u5b9a\u4f4d\u8bc6\u522b\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faMinkUNeXt-VINE\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u9884\u5904\u7406\u548cMatryoshka\u591a\u635f\u5931\u8868\u793a\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u7a00\u758fLiDAR\u8f93\u5165\u53ca\u4f4e\u7ef4\u8f93\u51fa\u5b9e\u73b0\u9ad8\u6548\u7387\uff0c\u5728\u8461\u8404\u56ed\u771f\u5b9e\u573a\u666f\u4e0b\u8fdb\u884c\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u65b9\u6cd5\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u957f\u671f\u8461\u8404\u56edLiDAR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u5bf9\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u3001\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u6570\u636e\u4e0b\u4f9d\u7136\u4fdd\u6301\u9ad8\u6548\u548c\u9c81\u68d2\u3002", "conclusion": "MinkUNeXt-VINE\u5728\u519c\u4e1a\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u7684\u5730\u70b9\u8bc6\u522b\uff0c\u5373\u4f7f\u5728\u6210\u672c\u53d7\u9650\u3001\u4f20\u611f\u5668\u6027\u80fd\u8f83\u4f4e\u65f6\uff0c\u4f9d\u7136\u80fd\u83b7\u5f97\u826f\u597d\u8868\u73b0\uff0c\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u5b9a\u4f4d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.17237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17237", "abs": "https://arxiv.org/abs/2601.17237", "authors": ["Mike Ranzinger", "Greg Heinrich", "Collin McCarthy", "Jan Kautz", "Andrew Tao", "Bryan Catanzaro", "Pavlo Molchanov"], "title": "C-RADIOv4 (Tech Report)", "comment": null, "summary": "By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86C-RADIOv4\u89c6\u89c9\u6a21\u578b\uff0c\u91c7\u7528\u591a\u6559\u5e08\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u7075\u6d3b\u7684\u5206\u8fa8\u7387\u652f\u6301\u548c\u9ad8\u6548\u7279\u6027\u3002", "motivation": "\u9488\u5bf9\u4e0d\u540c\u6559\u5e08\u6a21\u578b\u5404\u5177\u4f18\u52bf\u4e14\u96be\u4ee5\u7edf\u4e00\u8f6c\u79fb\u5230\u5355\u4e00\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u591a\u6559\u5e08\u84b8\u998f\uff0c\u5b9e\u73b0\u7edf\u4e00\u5b66\u751f\u6a21\u578b\u7ee7\u627f\u591a\u4f4d\u6559\u5e08\u80fd\u529b\u7684\u76ee\u6807\u3002", "method": "C-RADIOv4\u57fa\u4e8e\u524d\u4f5cAM-RADIO/RADIOv2.5\uff0c\u4f7f\u7528SigLIP2\u3001DINOv3\u3001SAM3\u4f5c\u4e3a\u6559\u5e08\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u6280\u672f\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u6db5\u76d6\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\uff0c\u589e\u5f3a\u4e86\u4efb\u610f\u5206\u8fa8\u7387\u652f\u6301\uff0c\u5e76\u91cd\u65b0\u52a0\u5165ViTDet\u4ee5\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u6548\u7387\u3002", "result": "C-RADIOv4\u5728\u6838\u5fc3\u8bc4\u6d4b\u6307\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b\u4e0a\u76f8\u8f83\u524d\u4ee3\u5927\u5e45\u63d0\u5347\uff0c\u80fd\u6a21\u4effSAM3\u83b7\u5f97\u65b0\u80fd\u529b\uff0c\u5e76\u63d0\u5347\u5bf9\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u652f\u6301\u548c\u63a8\u7406\u6548\u7387\uff0c\u4e14\u6a21\u578b\u4f7f\u7528\u4e86\u66f4\u4e3a\u5bbd\u677e\u7684\u5f00\u6e90\u8bb8\u53ef\u3002", "conclusion": "C-RADIOv4\u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u548c\u8bbe\u8ba1\u6539\u8fdb\uff0c\u6709\u6548\u5c06\u591a\u4e2a\u5148\u8fdb\u6559\u5e08\u7684\u80fd\u529b\u6574\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u3001\u7075\u6d3b\u6027\u548c\u5f00\u653e\u6027\uff0c\u662f\u901a\u7528\u89c6\u89c9\u9aa8\u5e72\u7f51\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2601.17569", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17569", "abs": "https://arxiv.org/abs/2601.17569", "authors": ["Alireza Salemi", "Hamed Zamani"], "title": "Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations", "comment": null, "summary": "Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86P^3\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u672c\u5730\u5ba2\u6237\u7aef\u6a21\u578b\u548c\u4e91\u7aef\u5927\u6a21\u578b\u95f4\u534f\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u5b9e\u9a8c\u8868\u660eP^3\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u4e2a\u6027\u5316\u5bf9\u4e8eLLM\u8f93\u51fa\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\u95f4\u5b58\u5728\u6743\u8861\uff1a\u8981\u4e48\u6cc4\u9732\u7528\u6237\u9690\u79c1\u7ed9\u4e91\u7aef\uff0c\u8981\u4e48\u4f9d\u8d56\u80fd\u529b\u8f83\u5f31\u7684\u672c\u5730\u6a21\u578b\u3002\u56e0\u6b64\u9700\u8981\u517c\u987e\u9690\u79c1\u548c\u6548\u679c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "P^3\u6846\u67b6\u4e2d\uff0c\u4e91\u7aef\u5927\u6a21\u578b\u4ec5\u6839\u636e\u7528\u6237\u67e5\u8be2\u751f\u6210\u521d\u6b65draft\uff0c\u5ba2\u6237\u7aef\u5c0f\u6a21\u578b\u7ed3\u5408\u7528\u6237\u9690\u79c1\u4fe1\u606f\u8bc4\u4f30\u5e76\u4fee\u6b63draft\uff0c\u6bcf\u8f6e\u751f\u6210\u540e\u91cd\u590d\u8be5\u8fc7\u7a0b\u76f4\u5230\u5b8c\u6210\u3002\u8fd9\u6837\u6570\u636e\u4e0d\u6cc4\u9732\u5230\u4e91\u7aef\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u4e2a\u6027\u5316\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cP^3\u6210\u7ee9\u663e\u8457\u9ad8\u4e8e\u975e\u4e2a\u6027\u5316\u4e91\u7aef\u548c\u4e2a\u6027\u5316\u672c\u5730\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u53477.4%-9%\uff0c\u4e14\u63a5\u8fd1\u4e8e\u5168\u90e8\u9690\u79c1\u6cc4\u9732\u4e0a\u9650\u768490.3%-95.7%\u6548\u7528\u3002\u9690\u79c1\u5206\u6790\u8868\u660e\u53ea\u67091.5%-3.5%\u7684\u989d\u5916\u6570\u636e\u6cc4\u9732\uff0c\u4e14\u5ba2\u6237\u7aef\u751f\u6210\u8d1f\u62c5\u4ec5\u4e3a9.2%\u3002", "conclusion": "P^3\u5728\u4e0d\u663e\u8457\u589e\u52a0\u9690\u79c1\u6cc4\u9732\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u751f\u6210\uff0c\u662f\u5b9e\u7528\u53ef\u9760\u7684\u4e2a\u6027\u5316\u9690\u79c1\u4fdd\u62a4\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17254", "abs": "https://arxiv.org/abs/2601.17254", "authors": ["Takato Yasuno"], "title": "Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization", "comment": "8 pages, 5 figures, 2 tables", "summary": "In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u5730\u57df\u4fe1\u606f\u9690\u79c1\u4fdd\u62a4\u7684\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6df7\u51dd\u571f\u88c2\u7f1d\u3001\u94a2\u7b4b\u66b4\u9732\u7b49\u75c5\u5bb3\u7684\u9ad8\u6548\u8bc6\u522b\uff0c\u5e76\u80fd\u81ea\u52a8\u4fdd\u62a4\u5305\u542b\u5730\u533a\u4fe1\u606f\u7684\u65bd\u5de5\u6807\u5fd7\u533a\u57df\u3002\u7cfb\u7edf\u5f00\u6e90\u4e14\u5904\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u7528\u4e8e\u65e5\u672c\u5f53\u524d\u7684\u57fa\u7840\u8bbe\u65bd\u5de1\u68c0\u9700\u6c42\u3002", "motivation": "\u65e5\u672c\u6cd5\u89c4\u8981\u6c42\u6bcf\u4e94\u5e74\u5bf9\u6c11\u7528\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u4e00\u6b21\u76ee\u89c6\u5de1\u68c0\uff0c\u91c7\u96c6\u7684\u56fe\u50cf\u5e38\u542b\u6709\u66b4\u9732\u5730\u57df\u4fe1\u606f\uff08\u5982\u65bd\u5de5\u6807\u5fd7\uff09\uff0c\u82e5\u76f4\u63a5\u516c\u5e03\u53ef\u80fd\u5f15\u8d77\u516c\u4f17\u7126\u8651\u3002\u56e0\u6b64\uff0c\u6025\u9700\u4e00\u79cd\u5728\u63d0\u53d6\u635f\u4f24\u7279\u5f81\u7684\u540c\u65f6\u4fdd\u62a4\u5730\u57df\u9690\u79c1\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8e\u201cSegment Anything Model 3\uff08SAM 3\uff09\u201d\u5bf9\u6df7\u51dd\u571f\u88c2\u7f1d\u548c\u94a2\u7b4b\u66b4\u9732\u8fdb\u884c\u68c0\u6d4b\uff1b\u5229\u7528DBSCAN\u7b97\u6cd5\u81ea\u52a8\u8865\u5168\u6f0f\u68c0\u533a\u57df\uff1b\u901a\u8fc7\u9ad8\u65af\u6a21\u7cca\u6280\u672f\u68c0\u6d4b\u5e76\u4fdd\u62a4\u5305\u542b\u5730\u57df\u4fe1\u606f\u7684\u65bd\u5de5\u6807\u5fd7\u533a\u57df\uff1b\u7ed3\u54084\u79cdOCR\u9884\u5904\u7406\u65b9\u6cd5\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\uff1b\u5e76\u901a\u8fc7GPU\u4f18\u5316\u5b9e\u73b01.7\u79d2/\u5f20\u7684\u5904\u7406\u901f\u5ea6\u3002\u6280\u672f\u6808\u5305\u62ecSAM3\u3001PyTorch\u3001OpenCV\u3001pytesseract\u548cscikit-learn\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u5b8c\u6210\u6865\u6881\u635f\u4f24\u81ea\u52a8\u68c0\u6d4b\u548c\u533a\u57df\u4fe1\u606f\u9690\u79c1\u4fdd\u62a4\uff0c\u6781\u5927\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u5de1\u68c0\u6548\u7387\uff0c\u540c\u65f6\u9632\u6b62\u654f\u611f\u5730\u57df\u4fe1\u606f\u6cc4\u9732\u3002\u5177\u4f53\u5728\u5904\u7406\u901f\u5ea6\u548c\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u8f83\u9ad8\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u517c\u987e\u6865\u6881\u7ed3\u6784\u635f\u4f24\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5730\u7406\u9690\u79c1\u4fdd\u62a4\uff0c\u9002\u5408\u65e5\u672c\u7b49\u5bf9\u5de1\u68c0\u65e2\u6709\u4fe1\u606f\u5b89\u5168\u9700\u6c42\u53c8\u8981\u6c42\u9ad8\u6548\u7684\u573a\u666f\u3002\u5f00\u6e90\u5b9e\u73b0\u5177\u5907\u5b9e\u9645\u5e94\u7528\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2601.17585", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17585", "abs": "https://arxiv.org/abs/2601.17585", "authors": ["Matija Luka Kuki\u0107", "Marko \u010culjak", "David Duki\u0107", "Martin Tutek", "Jan \u0160najder"], "title": "Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models", "comment": "Accepted at EACL 2026 Findings", "summary": "Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5e8f\u5217\u91cd\u590d\uff08SR\uff09\u65b9\u6cd5\uff0c\u4f7f\u89e3\u7801\u5668-only\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u50cf\u7f16\u7801\u5668\u90a3\u6837\u5229\u7528\u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u63d0\u5347\u5176\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u7684\u8868\u73b0\u3002SR\u65e0\u9700\u5927\u5e45\u4fee\u6539\u6a21\u578b\u7ed3\u6784\uff0c\u6bd4\u53bb\u56e0\u679cmask\u65b9\u6cd5\u66f4\u7b80\u5355\u6709\u6548\u3002", "motivation": "\u76ee\u524d\u8bed\u8a00\u6a21\u578b\u591a\u4e3a\u89e3\u7801\u5668-only\u7ed3\u6784\uff0c\u8bad\u7ec3\u65f6\u4ec5\u57fa\u4e8e\u5de6\u81f3\u53f3\u7684\u4e0a\u4e0b\u6587\uff0c\u800c\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u9700\u8981\u6bcf\u4e2atoken\u540c\u65f6\u5229\u7528\u5de6\u53f3\u6587\u4fe1\u606f\uff0c\u5bf9\u53cc\u5411\u6027\u6709\u8f83\u9ad8\u9700\u6c42\u3002\u4f20\u7edf\u65b9\u6cd5\u591a\u7528\u7f16\u7801\u5668-only\u6a21\u578b\uff08\u5982BERT\uff09\uff0c\u4f46\u968f\u7740\u89e3\u7801\u5668-only\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5176\u80fd\u9002\u914d\u5230\u66f4\u591a\u4efb\u52a1\u3002\u5982\u4f55\u8ba9\u89e3\u7801\u5668-only\u6a21\u578b\u4e5f\u80fd\u9ad8\u6548\u7528\u4e8e\u5e8f\u5217\u6807\u6ce8\uff0c\u6210\u4e3a\u5173\u6ce8\u70b9\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e8f\u5217\u91cd\u590d\uff08SR\uff09\u4f5c\u4e3a\u9002\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u5730\u5bf9\u8f93\u5165\u5e8f\u5217\u8fdb\u884c\u591a\u6b21\u91cd\u590d\uff0c\u4f7f\u5f97\u89e3\u7801\u5668-only\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5230\u53cc\u5411\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u4e0e\u53bb\u56e0\u679cmask\u548cencoder\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSR\u65b9\u6cd5\u80fd\u672c\u8d28\u4e0a\u63d0\u5347\u89e3\u7801\u5668\u7684\u53cc\u5411\u6027\uff0ctoken-level embedding\u8d28\u91cf\u660e\u663e\u63d0\u5347\uff0c\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e0a\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684encoder\u548c\u53bbmask\u7684\u89e3\u7801\u5668\u3002\u5e76\u4e14\uff0c\u591a\u6b21\u91cd\u590d\u5e76\u4e0d\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6a21\u578b\u4e2d\u95f4\u5c42\u7684embedding\u5728SR\u4e0b\u540c\u6837\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u66f4\u9ad8\u6548\u3002", "conclusion": "\u5e8f\u5217\u91cd\u590d\uff08SR\uff09\u65b9\u6cd5\u7f13\u89e3\u4e86\u89e3\u7801\u5668\u7ed3\u6784\u4e0a\u7684\u53cc\u5411\u6027\u9650\u5236\uff0c\u4f7f\u89e3\u7801\u5668-only\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u66f4\u5e7f\uff0c\u80fd\u66f4\u597d\u5e94\u7528\u4e8e\u5e8f\u5217\u6807\u6ce8\u7b49token-level\u4efb\u52a1\u3002"}}
{"id": "2601.17258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17258", "abs": "https://arxiv.org/abs/2601.17258", "authors": ["Jo\u00e3o Pereira", "Vasco Lopes", "Jo\u00e3o Neves", "David Semedo"], "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding", "comment": null, "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u89c6\u9891\u5f02\u5e38\u7406\u89e3\uff08VAU\uff09\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6FineVAU\uff0c\u5305\u62ec\u66f4\u8d34\u8fd1\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u6d4b\u6307\u6807FVScore\u548c\u5168\u65b0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6FineW3\u3002\u5b9e\u9a8c\u8868\u660e\u65b0\u6807\u51c6\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30\u5f02\u5e38\u4e8b\u4ef6\u7684\u63cf\u8ff0\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u5f53\u524dLVLM\u5728\u7a7a\u95f4\u548c\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u5f02\u5e38\u4e8b\u4ef6\u4e0a\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709VAU\u8bc4\u6d4b\u4f9d\u8d56n-gram\u6216LLM\u76f8\u5173\u6307\u6807\uff0c\u96be\u4ee5\u6355\u6349\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u7684\u3001\u7ec6\u7c92\u5ea6\u4e14\u4e0e\u89c6\u89c9\u7d27\u5bc6\u76f8\u5173\u7684\u5f02\u5e38\u63cf\u8ff0\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5408\u7406\u3001\u7cbe\u7ec6\u7684\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u6d4b\u65b9\u6cd5\u548c\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3002", "method": "\u5c06VAU\u7ec6\u5206\u4e3a\u4e09\u90e8\u5206\uff08\u4e8b\u4ef6\u3001\u53c2\u4e0e\u5b9e\u4f53\u3001\u5730\u70b9\uff09\uff0c\u63d0\u51fa\u65b0\u8bc4\u6d4b\u6307\u6807FVScore\u8bc4\u4ef7LVLM\u7b54\u6848\u4e2d\u5173\u952e\u89c6\u89c9\u5143\u7d20\u7684\u8986\u76d6\u5ea6\uff0c\u5e76\u81ea\u52a8\u6784\u5efa\u7efc\u5408\u6027\u6570\u636e\u96c6FineW3\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u6269\u5c55\u539f\u6709\u4eba\u7c7b\u6807\u6ce8\u5e76\u52a0\u5165\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u4eba\u7c7b\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\uff0cFVScore\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u8d34\u5408\u4eba\u7c7b\u5bf9\u5f02\u5e38\u4e8b\u4ef6\u7684\u7406\u89e3\u3002\u57fa\u4e8eFineVAU\u7684\u5b9e\u9a8c\u63ed\u793a\uff0c\u5c3d\u7ba1LVLM\u5728\u7c97\u7c92\u5ea6\u548c\u9759\u6001\u4fe1\u606f\u4e8b\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7a7a\u95f4\u3001\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u611f\u77e5\u7684\u5f02\u5e38\u4e8b\u4ef6\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "conclusion": "FineVAU\u53ca\u5176\u914d\u5957\u6307\u6807\u548c\u6570\u636e\u96c6\u53ef\u6709\u6548\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u7684\u8bc4\u4ef7\u8d28\u91cf\u548c\u7814\u7a76\u6c34\u5e73\uff0c\u4e3a\u672a\u6765\u63d0\u5347LVLM\u5bf9\u5f02\u5e38\u4e8b\u4ef6\u7684\u591a\u89d2\u5ea6\u3001\u591a\u7ef4\u5ea6\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17593", "abs": "https://arxiv.org/abs/2601.17593", "authors": ["Tianjun Zhong", "Linyang He", "Nima Mesgarani"], "title": "From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs", "comment": null, "summary": "Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.\n  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Reasoning DAG Probing\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001\u4e2d\u662f\u5426\u5b58\u5728\u53ef\u7ebf\u6027\u8bbf\u95ee\u7684\u63a8\u7406\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7ed3\u6784\uff0c\u5e76\u5206\u6790\u8be5\u7ed3\u6784\u5728\u4e0d\u540c\u5c42\u7ea7\u7684\u5206\u5e03\u60c5\u51b5\u3002\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u5728\u4e2d\u95f4\u5c42\u786e\u5b9e\u7f16\u7801\u4e86DAG\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u4e14\u53ef\u6062\u590d\u6027\u53d7\u5230\u8282\u70b9\u6df1\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5f71\u54cd\u3002", "motivation": "\u8bb8\u591a\u63a8\u7406\u4efb\u52a1\u5929\u7136\u5448\u73b0\u56fe\u7ed3\u6784\uff08DAG\uff09\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7ebf\u6027\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7ebf\u6027\u63a8\u7406\u94fe\uff0c\u5bf9LLM\u662f\u5426\u5185\u90e8\u53cd\u6620\u51fa\u590d\u6742\u7684\u56fe\u72b6\u63a8\u7406\u7ed3\u6784\u77e5\u4e4b\u751a\u5c11\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u660e\u786e\u63a2\u7a76\u8fd9\u4e00\u7ed3\u6784\u5728\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51faReasoning DAG Probing\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u63a8\u7406\u8282\u70b9\u8f6c\u5316\u4e3a\u6587\u672c\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u63a2\u9488\uff0c\u5229\u7528\u6a21\u578b\u9690\u85cf\u6001\u9884\u6d4b\u8282\u70b9\u6df1\u5ea6\u53ca\u8282\u70b9\u5bf9\u95f4\u8ddd\u79bb\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5c42\u7684\u63a2\u9488\u6548\u679c\uff0c\u5e76\u8bbe\u7f6e\u5bf9\u7167\u6270\u52a8\u5b9e\u9a8c\uff0c\u4ee5\u533a\u5206\u6a21\u578b\u662f\u5426\u771f\u6b63\u7f16\u7801\u4e86\u76f8\u5173\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLM\u7684\u4e2d\u95f4\u5c42\u80fd\u591f\u6709\u6548\u7f16\u7801\u63a8\u7406DAG\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd9\u4e00\u80fd\u529b\u968f\u7740\u8282\u70b9\u6df1\u5ea6\u589e\u52a0\u548c\u6a21\u578b\u89c4\u6a21\u53d8\u5316\u800c\u4e0d\u540c\u3002\u5373\u4f7f\u6270\u52a8\u6587\u672c\u8868\u9762\u5c5e\u6027\u540e\uff0c\u6a21\u578b\u4f9d\u7136\u80fd\u7ef4\u6301\u90e8\u5206\u63a8\u7406\u7ed3\u6784\u3002", "conclusion": "LLM\u5185\u90e8\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u5e8f\u5217\u5316\u63a8\u7406\uff0c\u5176\u9690\u85cf\u72b6\u6001\u8fd8\u5177\u5907\u53ef\u6d4b\u91cf\u7684\u56fe\u7ed3\u6784\u7f16\u7801\u80fd\u529b\u3002\u8fd9\u4e3a\u6df1\u5165\u7406\u89e3LLM\u591a\u6b65\u63a8\u7406\u673a\u5236\u548c\u8bbe\u8ba1\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.17259", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17259", "abs": "https://arxiv.org/abs/2601.17259", "authors": ["Angad Singh Ahuja", "Aarush Ram Anandh"], "title": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling", "comment": "25 Pages, 12 Figures, 3 Tables, 5 Appendices, 8 Algorithms", "summary": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u9636\u6bb5\u63a7\u5236\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8f93\u51fa\u989c\u8272\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u96c6\u6210\u5230\u6807\u51c6Stable Diffusion\u4fee\u590d\u6d41\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u5bf9\u7279\u5b9a\u533a\u57df\u989c\u8272\u7684\u7cbe\u51c6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\uff0c\u96be\u4ee5\u7cbe\u786e\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u989c\u8272\u8981\u6c42\uff0c\u5c24\u5176\u662f\u5728\u8bbe\u8ba1\u76f8\u5173\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8981\u4e25\u683c\u63a7\u5236\u8f93\u51fa\u989c\u8272\uff0c\u4f46\u5e38\u5e38\u5931\u8d25\u3002\u7eaf\u7cb9\u7528\u5747\u503c\u5bf9\u989c\u8272\u7ea6\u675f\u5bb9\u6613\u63a9\u76d6\u5c40\u90e8\u663e\u8457\u504f\u5dee\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6848\u3002", "method": "1\uff09\u57fa\u4e8eROI\uff08\u611f\u5174\u8da3\u533a\u57df\uff09\u7684\u4fee\u590d\u6cd5\uff0c\u5b9e\u73b0\u7a7a\u95f4\u9009\u62e9\u6027\uff1b2\uff09\u80cc\u666f\u9690\u7a7a\u95f4\u7684\u518d\u53e0\u52a0\uff0c\u9632\u6b62ROI\u5916\u989c\u8272\u6f02\u79fb\uff1b3\uff09\u5229\u7528\u5728CIE Lab\u4e0e\u7ebf\u6027RGB\u7a7a\u95f4\u4e0b\u5b9a\u4e49\u7684\u7ec4\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u68af\u5ea6\u5f15\u5bfc\u63a7\u5236\uff0c\u5305\u62ec\u5e73\u5747\u989c\u8272\u3001\u50cf\u7d20\u5206\u5e03\u5c3e\u90e8\uff0c\u91c7\u7528CVaR\u4e0e\u8f6f\u6700\u5927\u60e9\u7f5a\uff0c\u4ee5\u53ca\u95e8\u63a7\u4e0e\u6b65\u957f\u8c03\u5ea6\u4fdd\u8bc1\u6269\u6563\u8fc7\u7a0b\u7a33\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u53ea\u63a7\u5236\u5747\u503c\u7684\u65b9\u6848\u867d\u7136\u80fd\u6ee1\u8db3\u6574\u4f53\u989c\u8272\uff0c\u4f46\u5e38\u51fa\u73b0\u5c40\u90e8\u663e\u8457\u5f02\u5e38\uff1b\u800c\u672c\u6587\u65b9\u6cd5\u5219\u6709\u6548\u5b9e\u73b0\u5206\u5e03\u611f\u77e5\u7684\u989c\u8272\u7cbe\u51c6\u63a7\u5236\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u76ee\u6807\u989c\u8272\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u96c6\u6210\u5728Stable Diffusion\u4fee\u590d\u6d41\u7a0b\u4e2d\u7684\u5b9e\u7528\u6280\u672f\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u6269\u6563\u6a21\u578b\u5bf9\u7279\u5b9a\u533a\u57df\u76ee\u6807\u989c\u8272\u7684\u4e25\u683c\u9075\u5faa\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5bf9\u989c\u8272\u7cbe\u786e\u8981\u6c42\u8f83\u9ad8\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2601.17596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17596", "abs": "https://arxiv.org/abs/2601.17596", "authors": ["Yunxiang Zhang", "Kang Zhou", "Zhichao Xu", "Kiran Ramnath", "Yun Zhou", "Sangmin Woo", "Haibo Ding", "Lin Lee Cheong"], "title": "Learning to Ideate for Machine Learning Engineering Agents", "comment": "EACL 2026 main conference", "summary": "Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MLE-Ideator\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u201c\u60f3\u6cd5\u751f\u6210\u201d(Ideation)\u4e0e\u201c\u5b9e\u73b0\u201d(Implementation)\u8fc7\u7a0b\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u4f18\u5316\u3002\u5b9e\u9a8c\u663e\u793a\u65e0\u8bba\u662f\u5426\u8bad\u7ec3\uff0cMLE-Ideator\u5747\u4f18\u4e8e\u53ea\u5b9e\u73b0\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u201c\u60f3\u6cd5\u751f\u6210\u8005\u201d\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4f53\u5236\u96be\u4ee5\u5b9e\u73b0\u5bf9\u5df2\u5b9e\u73b0\u7b97\u6cd5\u7684\u6709\u6548\u3001\u8fed\u4ee3\u4f18\u5316\u3002\u4f5c\u8005\u53d1\u73b0\u5c06\u201c\u521b\u610f\u4ea7\u751f\u201d\u4e0e\u201c\u843d\u5b9e\u5b9e\u73b0\u201d\u5206\u5f00\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u4f18\u5316\u6548\u679c\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e13\u4e3a\u201c\u60f3\u6cd5\u751f\u6210\u201d\u670d\u52a1\u7684\u8f85\u52a9\u4ee3\u7406\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86MLE-Ideator\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u72ec\u7acb\u7684\u4ee3\u7406\u89d2\u8272\uff1a\u5b9e\u73b0\u4ee3\u7406\u548c\u60f3\u6cd5\u751f\u6210\u4ee3\u7406\uff08Ideator\uff09\u3002\u5b9e\u73b0\u4ee3\u7406\u53ef\u5728\u9700\u8981\u65f6\u5411Ideator\u8bf7\u6c42\u7b56\u7565\u5efa\u8bae\u3002\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\uff0c\u6bd4\u8f83\u4e86\u57fa\u7840\u672a\u8bad\u7ec3\u4e0eRL\uff08\u5f3a\u5316\u5b66\u4e60\uff09\u8bad\u7ec3\u540e\u7684Ideator\uff0c\u5e76\u5229\u7528MLE-Bench\u4efb\u52a1\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bc4\u3002", "result": "1\uff09\u5728\u65e0\u8bad\u7ec3\u7684\u72b6\u6001\u4e0b\uff0cMLE-Ideator\u6846\u67b6\u5728MLE-Bench\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u5b9e\u73b0\u4ee3\u7406\u7684\u57fa\u51c6\u30022\uff09\u91c7\u7528RL\u8bad\u7ec3\u540e\uff0cQwen3-8B\u6a21\u578b\u7684Ideator\u4ec5\u97001000\u6761\u6570\u636e\uff0c\u4ece10\u4e2a\u4efb\u52a1\u5b66\u5230\u66f4\u4f18\u7684\u7b56\u7565\uff0c\u76f8\u8f83\u672a\u8bad\u7ec3\u7684\u7248\u672c\u63d0\u534711.5%\uff0c\u5e76\u4f18\u4e8eClaude Sonnet 3.5\u3002", "conclusion": "\u5c06\u521b\u610f\u751f\u6210\u4e0e\u5b9e\u73b0\u5206\u79bb\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4f18\u5316\u6d41\u7a0b\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347Ideator\u4ee3\u7406\u7684\u6548\u679c\u3002\u8be5\u5de5\u4f5c\u4e3a\u6218\u7565\u578bAI\u7cfb\u7edf\u5728\u79d1\u5b66\u53d1\u73b0\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u8def\u5f84\u3002"}}
{"id": "2601.17271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17271", "abs": "https://arxiv.org/abs/2601.17271", "authors": ["Kun Huang", "Fang-Lue Zhang", "Neil Dodgson"], "title": "Cross360: 360\u00b0 Monocular Depth Estimation via Cross Projections Across Scales", "comment": "TIP, 12 pages", "summary": "360\u00b0 depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360\u00b0 field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360\u00b0 image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b0\u578b360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5Cross360\uff0c\u80fd\u591f\u6709\u6548\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u73b0\u6709360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u5168\u5c40\u8fde\u7eed\u6027\u4e0e\u53bb\u7578\u53d8\uff0c\u5c40\u90e8\u7279\u5f81\u5168\u5c40\u611f\u77e5\u4e0d\u8db3\uff0c\u4e14\u8de8\u6295\u5f71\u7279\u5f81\u878d\u5408\u5b58\u5728\u8fb9\u754c\u63d0\u53d6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51faCross360\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u5207\u5e73\u9762\uff08tangent\uff09\u5c40\u90e8\u6295\u5f71\u7279\u5f81\u4e0e\u7b49\u77e9\u5f62\uff08equirectangular\uff09\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u9010\u6b65\u7279\u5f81\u805a\u5408\u4e0e\u6ce8\u610f\u529b\u6a21\u5757\u7cbe\u7ec6\u5904\u7406\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "\u5728\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCross360\u5728\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u6027\u53ca\u5168\u5c40\u4e00\u81f4\u6027\u4e0a\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5bf9\u5b8c\u6574360\u00b0\u56fe\u50cf\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Cross360\u901a\u8fc7\u6709\u6548\u878d\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u8fde\u7eed\u6027\u548c\u4e00\u81f4\u6027\u96be\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u5f88\u9ad8\u7684\u6f5c\u529b\u548c\u6548\u679c\u3002"}}
{"id": "2601.17609", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17609", "abs": "https://arxiv.org/abs/2601.17609", "authors": ["Sara Rezaeimanesh", "Mohammad M. Ghassemi"], "title": "What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization", "comment": null, "summary": "In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \\textbf{59\\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5LoID\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u901a\u8fc7\u6982\u7387\u63d0\u53d6\u878d\u5165\u8d1d\u53f6\u65af\u903b\u8f91\u56de\u5f52\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u5728\u6570\u636e\u6709\u9650\u4e14\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u533b\u5b66\u548c\u91d1\u878d\u7b49\u9886\u57df\u4e2d\u83b7\u53d6\u5927\u89c4\u6a21\u6709\u6807\u7b7e\u6570\u636e\u975e\u5e38\u6602\u8d35\u3001\u56f0\u96be\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709LLM\u8574\u542b\u4e30\u5bcc\u9886\u57df\u77e5\u8bc6\uff0c\u5982\u4f55\u6709\u6548\u8403\u53d6\u5e76\u5f15\u5165\u5c0f\u6570\u636e\u5efa\u6a21\u8fc7\u7a0b\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u662f\u8be5\u5de5\u4f5c\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u4f5c\u8005\u63d0\u51faLoID\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u6709\u4ee3\u8868\u6027\u548c\u5bf9\u7acb\u8bed\u4e49\uff08\u6b63\u5411\u4e0e\u8d1f\u5411\uff09\u7684\u53e5\u5b50\uff0c\u76f4\u63a5\u63a2\u67e5LLM\u5bf9\u5404\u7279\u5f81\u5f71\u54cd\u65b9\u5411\u7684\u4fe1\u5fc3\u6c34\u5e73\u4e0e\u4e00\u81f4\u6027\uff0c\u636e\u6b64\u4e3a\u8d1d\u53f6\u65af\u903b\u8f91\u56de\u5f52\u76f4\u63a5\u63d0\u53d6\u7ed3\u6784\u5316\u7684\u5148\u9a8c\u5206\u5e03\u3002\u4e0d\u540c\u4e8e\u57fa\u4e8e\u6587\u672c\u751f\u6210\u7684\u63d0\u53d6\u65b9\u5f0f\uff0cLoID\u5229\u7528LLM\u5bf9Token\u7ea7\u522b\u9884\u6d4b\u7684\u6982\u7387\u4fe1\u606f\uff0c\u4ece\u591a\u6837\u5316\u8868\u8fbe\u4e0b\u5b9a\u91cf\u8bc4\u4f30\u5148\u9a8c\u5f3a\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u5341\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u8868\u683c\u6570\u636e\u96c6\u4e0a\uff08\u6a21\u62df\u8bad\u7ec3\u6d4b\u8bd5\u5206\u5e03\u504f\u79fb\uff09\uff0cLoID\u5728AUC\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u65e0\u4fe1\u606f\u5148\u9a8c\u3001AutoElicit\u53caLLMProcesses\uff0c\u5e76\u80fd\u6062\u590d\u9ad8\u8fbe59%\u7684oracle-\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u3002LoID\u57288/10\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3001\u7ed3\u679c\u53ef\u590d\u73b0\u3002", "conclusion": "LoID\u65b9\u6cd5\u4e3a\u6574\u5408LLM\u77e5\u8bc6\u5230\u8d1d\u53f6\u65af\u7edf\u8ba1\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u53d8\u5316\u65f6\u63d0\u5347\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u540c\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2601.17288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17288", "abs": "https://arxiv.org/abs/2601.17288", "authors": ["Jin Bai", "Huiyao Zhang", "Qi Wen", "Shengyang Li", "Xiaolin Tian", "Atta ur Rahman"], "title": "Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing", "comment": null, "summary": "The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fluxamba\uff0c\u4e00\u79cd\u7528\u4e8e\u5730\u8d28\u7ebf\u6027\u7279\u5f81\u7cbe\u786e\u5206\u5272\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\uff0c\u4e13\u95e8\u4e3a\u6355\u6349\u5730\u8d28\u7279\u5f81\u590d\u6742\u7684\u975e\u5747\u5300\u62d3\u6251\u5173\u7cfb\u800c\u8bbe\u8ba1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u548c\u6548\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dSSM\u7b49\u65b9\u6cd5\u5728\u5730\u8d28\u7ebf\u7279\u5f81\u5206\u5272\u4e2d\u56e0\u4f9d\u8d56\u521a\u6027\u8f74\u5411\u626b\u63cf\u8def\u5f84\uff0c\u4e0e\u66f2\u7ebf\u7279\u5f81\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\u5b58\u5728\u62d3\u6251\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e22\u5931\u548c\u5206\u5272\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u76ee\u6807\u62d3\u6251\u7ed3\u6784\u3001\u63d0\u5347\u4e0a\u4e0b\u6587\u6355\u83b7\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u6784\u6d41\u5757\uff08SFB\uff09\uff0c\u901a\u8fc7\u5404\u5411\u5f02\u6027\u7ed3\u6784\u95e8\uff08ASG\uff09\u4e0e\u5148\u9a8c\u8c03\u5236\u6d41\uff08PMF\uff09\u52a8\u6001\u5f15\u5bfc\u4fe1\u606f\u6d41\uff0c\u6709\u6548\u5206\u79bb\u7279\u5f81\u65b9\u5411\u4e0e\u7a7a\u95f4\u4f4d\u7f6e\uff1b\u540c\u65f6\u5f15\u5165\u5c42\u6b21\u5316\u7a7a\u95f4\u8c03\u8282\u5668\uff08HSR\uff09\u5b9e\u73b0\u591a\u5c3a\u5ea6\u8bed\u4e49\u5bf9\u9f50\u4ee5\u53ca\u9ad8\u4fdd\u771f\u805a\u7126\u5355\u5143\uff08HFFU\uff09\u63d0\u5347\u4f4e\u5bf9\u6bd4\u5ea6\u73af\u5883\u4e0b\u7684\u4fe1\u566a\u6bd4\u3002", "result": "\u5728LROC-Lineament\u3001LineaMapper\u548cGeoCrack\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cFluxamba\u83b7\u5f97\u6781\u9ad8\u5206\u5272\u7cbe\u5ea6\uff08\u5982LROC-Lineament\u6570\u636e\u96c6F1=89.22%\u3001mIoU=89.87%\uff09\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u8d8524FPS\uff0c\u53c2\u6570\u91cf\u4ec53.4M\u3001FLOPs\u4ec56.3G\uff0c\u8ba1\u7b97\u6d88\u8017\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c0f1-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "Fluxamba\u5728\u4fdd\u6301\u6781\u9ad8\u5206\u5272\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u7b97\u529b\u6d88\u8017\uff0c\u5b9e\u73b0\u4e86\u5206\u5272\u6548\u679c\u548c\u90e8\u7f72\u53ef\u884c\u6027\u7684Pareto\u6700\u4f18\uff0c\u4e3a\u5730\u8d28\u7ebf\u6027\u7279\u5f81\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2601.17658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17658", "abs": "https://arxiv.org/abs/2601.17658", "authors": ["Bich Ngoc", "Doan", "Giuseppe Russo", "Gianmarco De Francisci Morales", "Robert West"], "title": "Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization", "comment": null, "summary": "The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term \"radicalization personas.\" Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.", "AI": {"tldr": "\u672c\u6587\u4ee5QAnon\u9634\u8c0b\u8bba\u4e3a\u6848\u4f8b\uff0c\u5206\u6790\u6781\u7aef\u5316\u8fc7\u7a0b\u53ca\u5176\u5bf9\u4fe1\u4ef0\u8005\u4eb2\u53cb\u7684\u60c5\u611f\u5f71\u54cd\uff0c\u57fa\u4e8e12747\u6761\u53d7\u5bb3\u8005\u53d9\u8ff0\uff0c\u901a\u8fc7\u4e3b\u9898\u6a21\u578b\u548c\u60c5\u611f\u8bc6\u522b\uff0c\u53d1\u73b0\u4e0d\u540c\u6781\u7aef\u5316\u7c7b\u578b\u4e0e\u5177\u4f53\u60c5\u7eea\u4f24\u5bb3\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u9634\u8c0b\u8bba\u7684\u4f20\u64ad\u9664\u4e86\u9020\u6210\u793e\u4f1a\u4fe1\u4efb\u4e0b\u964d\u3001\u4e24\u6781\u5206\u5316\u5916\uff0c\u8fd8\u5bf9\u4fe1\u4ef0\u8005\u7684\u4eb2\u53cb\u4ea7\u751f\u88ab\u5ffd\u89c6\u7684\u4e2a\u4eba\u5c42\u9762\u4f24\u5bb3\u3002\u73b0\u6709\u8ba1\u7b97\u578b\u7814\u7a76\u4e3b\u8981\u805a\u7126\u5b8f\u89c2\u793e\u4f1a\u5f71\u54cd\uff0c\u5bf9\u4eb2\u53cb\u906d\u9047\u7684\u7ec6\u81f4\u60c5\u611f\u51b2\u51fb\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u6587\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u4ee5QAnon\u4e3a\u4f8b\uff0c\u62bd\u53d6r/QAnonCasualties\u793e\u533a12747\u6761\u53d9\u4e8b\uff0c\u9996\u5148\u7528BERTopic\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u68b3\u7406\u6781\u7aef\u5316\u8f68\u8ff9\uff0c\u5305\u62ec\u524d\u7f6e\u6761\u4ef6\u3001\u89e6\u53d1\u70b9\u548c\u540e\u671f\u7279\u5f81\uff1b\u518d\u4ee5LDA\u56fe\u6a21\u578b\u5206\u6790\uff0c\u5f52\u7eb3\u51fa\u516d\u7c7b\u201c\u6781\u7aef\u5316\u4eba\u683c\u539f\u578b\u201d\uff1b\u6700\u540e\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u60c5\u611f\u68c0\u6d4b\u4e0e\u56de\u5f52\u5206\u6790\uff0c\u5c06\u6781\u7aef\u5316\u7c7b\u578b\u4e0e\u4eb2\u53cb\u53d9\u4e8b\u4e2d\u7684\u60c5\u611f\u4f53\u9a8c\u8fdb\u884c\u91cf\u5316\u5173\u8054\u3002", "result": "\u516d\u7c7b\u6781\u7aef\u5316\u4eba\u683c\u539f\u578b\u4e0e\u4eb2\u53cb\u5177\u4f53\u60c5\u611f\u56f0\u6270\u76f4\u63a5\u76f8\u5173\u3002\u4f8b\u5982\uff0c\u5f53\u6781\u7aef\u5316\u88ab\u4eb2\u53cb\u89c6\u4e3a\u6709\u610f\u8bc6\u9009\u62e9\u65f6\uff0c\u6613\u5f15\u53d1\u6124\u6012\u548c\u538c\u6076\uff1b\u82e5\u8868\u73b0\u4e3a\u7cbe\u795e\u5d29\u6e83\u6216\u8ba4\u77e5\u8870\u9000\uff0c\u5219\u5173\u8054\u6050\u60e7\u548c\u60b2\u4f24\u3002\u4e0d\u540c\u6781\u7aef\u5316\u8def\u5f84\u5bf9\u4eb2\u53cb\u7684\u60c5\u611f\u4f24\u5bb3\u5404\u5f02\uff0c\u539f\u578b\u5177\u9884\u6d4b\u6027\u3002", "conclusion": "\u9996\u6b21\u4ee5\u5b9e\u8bc1\u65b9\u5f0f\u6784\u5efa\u6781\u7aef\u5316\u7684\u5173\u7cfb\u6027\u7814\u7a76\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u5176\u5bf9\u4eb2\u60c5\u3001\u53cb\u60c5\u5173\u7cfb\u7684\u7834\u574f\u6027\u540e\u679c\u63d0\u4f9b\u6307\u5f15\uff0c\u5bf9\u540e\u7eed\u76f8\u5173\u7814\u7a76\u53ca\u5b9e\u8df5\u5e72\u9884\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2601.17290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17290", "abs": "https://arxiv.org/abs/2601.17290", "authors": ["Weloday Fikadu Moges", "Jianmei Su", "Amin Waqas"], "title": "Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices", "comment": null, "summary": "Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u5143\u96c6\u6210\u6846\u67b6\uff08DMEF\uff09\uff0c\u7ed3\u5408\u4e09\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u9ad8\u6548\u7684\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\uff0c\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u5982\u7269\u8054\u7f51\u4f20\u611f\u5668\u548c\u667a\u80fd\u624b\u673a\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u53d7\u9650\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5728\u8fd9\u7c7b\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u517c\u987e\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DMEF\u6846\u67b6\u52a8\u6001\u5206\u914dMobileNetV2\u3001NASNetMobile\u548cInceptionV3\u4e09\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u96c6\u6210\u6743\u91cd\uff0c\u901a\u8fc7\u4f18\u5316\u7cbe\u5ea6\u63d0\u5347\u548c\u6a21\u578b\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6743\u91cd\u6839\u636e\u6a21\u578b\u8868\u73b0\u548c\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u5728\u9a6c\u94c3\u85af\u548c\u7389\u7c73\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\uff0cDMEF\u53d6\u5f97\u4e8699.53%\u548c96.61%\u7684\u7cbe\u5ea6\uff0c\u5206\u522b\u6bd4\u5355\u6a21\u578b\u548c\u9759\u6001\u96c6\u6210\u63d0\u5347\u4e862.1%\u548c6.3%\u3002\u63a8\u7406\u5ef6\u8fdf\u4f4e\u4e8e75ms\uff0c\u53c2\u6570\u91cf\u5c0f\u4e8e100\u4e07\uff0c\u5c55\u73b0\u4e86\u4f18\u79c0\u7684\u8fb9\u7f18\u8bbe\u5907\u9002\u5e94\u6027\u3002", "conclusion": "DMEF\u6709\u6548\u63d0\u5347\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u63a8\u52a8\u9ad8\u7cbe\u5ea6AI\u6a21\u578b\u5728\u519c\u4e1a\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u843d\u5730\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u524d\u666f\u3002"}}
{"id": "2601.17664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17664", "abs": "https://arxiv.org/abs/2601.17664", "authors": ["Syed Muhammad Ali", "Hammad Sajid", "Zainab Haider", "Ali Muhammad Asad", "Haya Fatima", "Abdul Samad"], "title": "UrduLM: A Resource-Efficient Monolingual Urdu Language Model", "comment": "12 pages", "summary": "Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e13\u4e3a\u4e4c\u5c14\u90fd\u8bed\u8bbe\u8ba1\u7684\u5355\u8bedTransformer\u9884\u8bad\u7ec3\u6a21\u578bUrduLM\uff0c\u5e76\u5236\u4f5c\u4e8633GB\u7684\u9ad8\u8d28\u91cf\u8bed\u6599\uff0c\u586b\u8865\u4e86\u76f8\u5173\u8d44\u6e90\u7684\u7a7a\u767d\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u67092.3\u4ebf\u4f7f\u7528\u8005\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684Transformer\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u6599\u8d44\u6e90\u3002\u591a\u8bed\u8a00\u6a21\u578b\u5bf9\u4e4c\u5c14\u90fd\u8bed\u652f\u6301\u6709\u9650\uff0c\u8868\u73b0\u8f83\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u6587\u5316\u504f\u5dee\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u9762\u5411\u4e4c\u5c14\u90fd\u8bed\u7684\u4e13\u5c5e\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u6599\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b33GB\u591a\u6837\u5316\u8bed\u6599\u7684\u4e4c\u5c14\u90fd\u8bed\u8bed\u6599\u5e93\uff0c\u5f00\u53d1\u4e86\u81ea\u5b9a\u4e49BPE\u5206\u8bcd\u5668\uff08\u6bd4\u591a\u8bed\u5206\u8bcd\u5668\u6548\u7387\u63d0\u534720%-30%\uff09\uff0c\u5e76\u4ee5\u6b64\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a1\u4ebf\u53c2\u6570\u3001\u4ec5\u89e3\u7801\u7ed3\u6784\u7684\u5355\u8bed\u6a21\u578bUrduLM\u3002", "result": "\u5728\u5c0f\u6837\u672c\u8bc4\u6d4b\u4e2d\uff0cUrduLM\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u523066.6%\u7684\u51c6\u786e\u7387\uff0c\u8bed\u6cd5\u7ea0\u9519\u4efb\u52a1 BLEU \u5206\u6570\u8d85\u8fc730\uff0c\u6027\u80fd\u4e0e\u89c4\u6a21\u592730\u500d\u7684\u591a\u8bed\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "UrduLM\u9996\u6b21\u4e3a\u4e4c\u5c14\u90fd\u8bedNLP\u63d0\u4f9b\u4e86\u4e13\u7528\u6a21\u578b\u548c\u5168\u5957\u65b9\u6cd5\u5b66\u8d44\u6e90\uff0c\u53ef\u4f5c\u4e3a\u57fa\u7ebf\u63a8\u52a8\u4e4c\u5c14\u90fd\u8bed\u53ca\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2601.17315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17315", "abs": "https://arxiv.org/abs/2601.17315", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading", "comment": "12 pages", "summary": "Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684KOA\uff08\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff09\u5206\u7ea7\u65b9\u6cd5\u2014\u2014ClinNet\uff0c\u7ed3\u5408\u7ed3\u6784\u5dee\u5f02\u5efa\u6a21\u3001\u7279\u5f81\u8bb0\u5fc6\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7ea7\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "KOA\u5206\u7ea7\u4f9d\u9760X\u5149\u5f71\u50cf\uff0c\u96be\u70b9\u5728\u4e8e\u5206\u7ea7\u5dee\u5f02\u7ec6\u5fae\u3001\u6807\u6ce8\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u4e14\u75be\u75c5\u5206\u7ea7\u6709\u5e8f\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f71\u54cd\u5206\u7ea7\u7684\u51c6\u786e\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51faClinNet\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a(1)\u53cc\u4fa7\u4e0d\u5bf9\u79f0\u7f16\u7801\u5668\uff08BAE\uff09\u5efa\u6a21\u819d\u5173\u8282\u5185\u5916\u4fa7\u7ed3\u6784\u5dee\u5f02\uff1b(2)\u8bca\u65ad\u8bb0\u5fc6\u5e93\u5b58\u50a8\u5404\u7c7b\u522b\u539f\u578b\u4ee5\u589e\u5f3a\u7279\u5f81\u7a33\u5b9a\u6027\uff1b(3)\u57fa\u4e8e\u6b63\u6001-\u9006\u4f3d\u9a6c\uff08NIG\uff09\u5206\u5e03\u7684\u8bc1\u636e\u6709\u5e8f\u5934\uff0c\u5b9e\u73b0\u8fde\u7eed\u5206\u7ea7\u4e0e\u4e0d\u786e\u5b9a\u6027\u8054\u5408\u4f30\u8ba1\u3002", "result": "ClinNet\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u4e860.892\u7684Quadratic Weighted Kappa\u5206\u6570\u4e0e0.768\u7684\u51c6\u786e\u7387\uff0c\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff08p < 0.001\uff09\u3002", "conclusion": "ClinNet\u4e0d\u4ec5\u63d0\u5347\u4e86KOA\u5206\u7ea7\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u6709\u6548\u6807\u8bb0\u5206\u5e03\u5916\u6837\u672c\u4e0e\u6f5c\u5728\u8bef\u8bca\uff0c\u4e3a\u4e34\u5e8a\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u4fdd\u969c\u3002"}}
{"id": "2601.17671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17671", "abs": "https://arxiv.org/abs/2601.17671", "authors": ["Chunxu Zhao", "Xin Huang", "Xue Han", "Shujian Huang", "Chao Deng", "Junlan Feng"], "title": "Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning", "comment": "This paper has been accepted by ICASSP 2026", "summary": "Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u8bed\u8a00\u6570\u5b66\u63a8\u7406\u65b9\u6cd5 PASMR\uff0c\u901a\u8fc7\u4e3b\u8bed\u8a00\u4f5c\u4e3a\u67a2\u7ebd\uff0c\u63d0\u5347\u5927\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff08\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u7684\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u5883\u4e0b\u63a8\u7406\u8868\u73b0\u4e0b\u964d\u3002\u4f5c\u8005\u8ba4\u4e3a\u539f\u56e0\u5728\u4e8e\u6a21\u578b\u591a\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u5bf9\u9f50\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa PASMR \u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u7684\u4e3b\u8981\u8bed\u8a00\u8bbe\u4e3a pivot language\u3002\u8bad\u7ec3\u65f6\uff0c\u5148\u5c06\u95ee\u9898\u7ffb\u8bd1\u4e3a pivot language\uff0c\u7528\u4e3b\u8bed\u8a00\u56de\u7b54\u5f15\u5bfc\u76ee\u6807\u8bed\u8a00\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u6784\u5efa\u8de8\u8bed\u8a00\u81ea\u53cd\u9988\u673a\u5236\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6b63\u786e\u7b54\u6848\u6216\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u591a\u8bed\u8a00\u95ee\u9898\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4efb\u52a1\u6548\u679c\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PASMR \u80fd\u6709\u6548\u6539\u5584 LLM \u5728\u591a\u8bed\u8a00\uff08\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u63d0\u5347\u5927\u6a21\u578b\u591a\u8bed\u8a00\u63a8\u7406\u8868\u73b0\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2601.17323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17323", "abs": "https://arxiv.org/abs/2601.17323", "authors": ["Debang Li", "Zhengcong Fei", "Tuanhui Li", "Yikun Dou", "Zheng Chen", "Jiangping Yang", "Mingyuan Fan", "Jingtao Xu", "Jiahua Wang", "Baoxuan Gu", "Mingshan Chang", "Yuqiang Xie", "Binjie Mao", "Youqiang Zhang", "Nuo Pang", "Hao Zhang", "Yuzhe Jin", "Zhiheng Xu", "Dixuan Lin", "Guibin Chen", "Yahui Zhou"], "title": "SkyReels-V3 Technique Report", "comment": null, "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SkyReels-V3\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u591a\u6a21\u6001\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u56fe\u50cf\u751f\u6210\u89c6\u9891\u3001\u89c6\u9891\u6269\u5c55\u3001\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u7b49\u591a\u79cd\u751f\u6210\u6a21\u5f0f\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u63a5\u8fd1\u6216\u8fbe\u5230\u5f53\u524d\u4e1a\u754c\u6700\u4f73\u6c34\u5e73\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u4e16\u754c\u6a21\u578b\u5efa\u8bbe\u81f3\u5173\u91cd\u8981\uff0c\u800c\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u63a8\u7406\u662f\u8bc4\u4f30\u5176\u80fd\u529b\u7684\u6838\u5fc3\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u4e0d\u540c\u573a\u666f\u4e0b\u591a\u6a21\u6001\u6761\u4ef6\u7684\u590d\u6742\u9700\u6c42\u4ee5\u53ca\u4e3b\u6d41\u7cfb\u7edf\u5173\u95ed\u6e90\u7684\u56f0\u5883\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u7edf\u4e00\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002", "method": "SkyReels-V3\u91c7\u7528\u6269\u6563Transformer\u4f5c\u4e3a\u6838\u5fc3\u9aa8\u5e72\uff0c\u7edf\u4e00\u652f\u6301\u4e09\u79cd\u751f\u6210\u6a21\u5f0f\uff1a1\uff09\u56fe\u50cf\u5230\u89c6\u9891\uff0c\u51ed\u501f\u591a\u6837\u6570\u636e\u5904\u7406\u6d41\u7a0b\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u8eab\u4efd\u4fdd\u6301\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\uff1b2\uff09\u89c6\u9891\u6269\u5c55\uff0c\u901a\u8fc7\u65f6\u7a7a\u4e00\u81f4\u5efa\u6a21\u548c\u5927\u89c4\u6a21\u89c6\u9891\u7406\u89e3\u652f\u6301\u5355\u955c\u5934\u4e0e\u591a\u955c\u5934\u65e0\u7f1d\u8854\u63a5\uff1b3\uff09\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u4eba\u89c6\u9891\uff0c\u5229\u7528\u9996\u672b\u5e27\u5d4c\u5165\u548c\u5173\u952e\u5e27\u63a8\u7406\u5b9e\u73b0\u957f\u65f6\u97f3\u89c6\u9891\u540c\u6b65\u751f\u6210\u3002", "result": "\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u5177\u4f53\u7ec6\u8282\u6307\u6807\u4e0a\uff0cSkyReels-V3\u8fbe\u5230\u6216\u63a5\u8fd1\u73b0\u6709\u6700\u4f18\u6c34\u5e73\uff0c\u90e8\u5206\u80fd\u529b\u4e0a\u903c\u8fd1\u4e3b\u6d41\u95ed\u6e90\u9886\u5148\u7cfb\u7edf\u3002", "conclusion": "SkyReels-V3\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u517c\u5177\u5353\u8d8a\u7684\u751f\u6210\u8d28\u91cf\u548c\u901a\u7528\u6027\uff0c\u4e3a\u5f00\u653e\u6e90\u4e16\u754c\u6a21\u578b\u53ca\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u5f3a\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2601.17702", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17702", "abs": "https://arxiv.org/abs/2601.17702", "authors": ["Qingsen Ma", "Dianyun Wang", "Yaoye Wang", "Lechen Ning", "Sujie Zhu", "Xiaohang Zhang", "Jiaming Lyu", "Linhao Ren", "Zhenbo Xu", "Zhaofeng He"], "title": "S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference", "comment": null, "summary": "Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.\n  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.\n  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.", "AI": {"tldr": "S3-Attention\u662f\u4e00\u79cd\u521b\u65b0\u7684\u957f\u6587\u672c\u63a8\u7406\u673a\u5236\uff0c\u80fd\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u5e76\u51cf\u5c11\u566a\u97f3\uff0c\u66ff\u4ee3\u4f20\u7edfKV\u7f13\u5b58\uff0c\u5177\u5907\u9ad8\u68c0\u7d22\u51c6\u786e\u5ea6\u548c\u7d27\u51d1\u8bc1\u636e\u68c0\u7d22\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u5927\u6a21\u578b\u5728\u591a\u6587\u6863\u548c\u957f\u6587\u672c\u63a8\u7406\u4e2d\u53d7\u5230\u5185\u5b58\u6d88\u8017\u5927\u548c\u65e0\u5173\u566a\u58f0\u5e72\u6270\u7684\u95ee\u9898\uff0c\u5e38\u7528\u7684KV\u7f13\u5b58\u5728\u957f\u4e0a\u4e0b\u6587\u4e0b\u6548\u7387\u4f4e\uff0c\u5916\u90e8\u68c0\u7d22\u4e5f\u5e38\u8fd4\u56de\u65e0\u5173\u5185\u5bb9\u3002\u56e0\u6b64\uff0c\u9700\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u75db\u70b9\u3002", "method": "S3-Attention\u5c06\u957f\u6587\u672c\u63a8\u7406\u89c6\u4e3a\u5185\u751f\u68c0\u7d22\u4efb\u52a1\uff0c\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u5c06Key/Query\u6295\u5f71\u4e3a\u7279\u5f81\u6807\u8bc6\uff0c\u518d\u901a\u8fc7CPU\u5012\u6392\u7d22\u5f15\u5efa\u7acb\u7279\u5f81\u4e0etoken\u4f4d\u7f6e\u6620\u5c04\uff0c\u5b9e\u73b0KV\u7f13\u5b58\u5b8c\u5168\u4e22\u5f03\uff0cGPU\u5185\u5b58\u4ec5\u7531\u626b\u63cf\u5757\u5927\u5c0f\u9650\u5236\uff0c\u5728\u751f\u6210\u65f6\u878d\u5408\u7279\u5f81\u5171\u73b0\u4e0eBM25\u65b9\u6cd5\u9ad8\u6548\u56de\u6eaf\u8bc1\u636e\u3002", "result": "\u5728LongBench\u7edf\u4e00\u8bc4\u6d4b\u4e0b\uff0cS3-Hybrid\u65b9\u6cd5\u51e0\u4e4e\u4e0e\u5168\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u6301\u5e73\uff0c\u5728\u591a\u79cd\u6a21\u578b\u548c\u4fe1\u606f\u5bc6\u96c6\u573a\u666f\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u539f\u578b\u5b9e\u73b0\u7684\u5ef6\u8fdf\u9ad8\u4e8e\u4f18\u5316\u7684\u5168KV\u65b9\u6cd5\u3002", "conclusion": "S3-Attention\u4e3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u9ad8\u6548\u53ef\u62d3\u5c55\u65b9\u6848\uff0c\u663e\u8457\u8282\u7701\u5185\u5b58\u5e76\u63d0\u9ad8\u76f8\u5173\u8bc1\u636e\u68c0\u7d22\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002\u672a\u6765\u4f18\u5316\u65b9\u5411\u662f\u5185\u6838\u7ea7\u52a0\u901f\u4ee5\u964d\u4f4e\u7cfb\u7edf\u5ef6\u8fdf\u3002"}}
{"id": "2601.17326", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17326", "abs": "https://arxiv.org/abs/2601.17326", "authors": ["Jasmine Lesner", "Michael Beyeler"], "title": "SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision", "comment": "Submitted to IEEE EMBC 2026. 7 pages, 6 figures, 2 tables", "summary": "Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u7b26\u53f7\u81ea\u8eab\uff08\u800c\u975e\u6539\u5584\u786c\u4ef6\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u89c6\u7f51\u819c\u5047\u4f53\u4e2d\u7b26\u53f7\u6b8b\u5f71\u6548\u5e94\u9020\u6210\u7684\u9605\u8bfb\u6df7\u6dc6\u3002\u4f7f\u7528\u7b26\u53f7-\u5b57\u6bcd\u91cd\u65b0\u6620\u5c04\u548c\u5927\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u901a\u8baf\u6df7\u6dc6\u7387\u5927\u5e45\u964d\u4f4e\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u7f51\u819c\u5047\u4f53\u56e0\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u548c\u89c6\u89c9\u6b8b\u5f71\u663e\u8457\uff0c\u5bfc\u81f4\u4f7f\u7528\u8005\u5728\u9605\u8bfb\u65f6\uff0c\u524d\u540e\u7b26\u53f7\u4fe1\u606f\u5e72\u6270\uff0c\u9605\u8bfb\u51c6\u786e\u7387\u4f4e\u3002\u786c\u4ef6\u5347\u7ea7\u6210\u672c\u9ad8\u4e14\u8fdb\u5c55\u7f13\u6162\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7d22\u8f6f\u4ef6\u5c42\u9762\u5373\u7b26\u53f7\u4f18\u5316\u7684\u89e3\u51b3\u601d\u8def\u3002", "method": "\u63d0\u51faSymbolSight\u8ba1\u7b97\u6846\u67b6\uff1a\u901a\u8fc7\u6a21\u62df\u5047\u4f53\u89c6\u89c9\u53ca\u795e\u7ecf\u4ee3\u7406\u89c2\u6d4b\uff0c\u8bc4\u4f30\u4e0d\u540c\u7b26\u53f7\u5bf9\u7684\u53ef\u6df7\u6dc6\u6027\uff0c\u7528\u9488\u5bf9\u5177\u4f53\u8bed\u8a00\u7684\u5927\u6570\u636e\u53cc\u5b57\u7edf\u8ba1\uff0c\u4f18\u5316\u7b26\u53f7-\u5b57\u6bcd\u7684\u6620\u5c04\uff0c\u4f7f\u5e38\u89c1\u4e34\u8fd1\u5b57\u6bcd\u95f4\u6df7\u6dc6\u6700\u5c0f\u5316\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u3001\u4fdd\u52a0\u5229\u4e9a\u8bed\u548c\u82f1\u8bed\u7684\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u4f18\u5316\u540e\u7684\u7b26\u53f7\u7cfb\u7edf\u6bd4\u539f\u751f\u5b57\u6bcd\u7cfb\u7edf\u5c06\u9884\u6d4b\u6df7\u6dc6\u7387\u4e2d\u4f4d\u6570\u964d\u4f4e22\u500d\u3002", "conclusion": "\u6807\u51c6\u5b57\u4f53\u5176\u5b9e\u5e76\u4e0d\u9002\u5408\u4f4e\u5e26\u5bbd\u3001\u4e32\u884c\u7279\u6027\u7684\u5047\u4f53\u89c6\u89c9\u3002\u901a\u8fc7\u8ba1\u7b97\u5efa\u6a21\u548c\u7b26\u53f7\u4f18\u5316\uff0c\u80fd\u591f\u4e3a\u672a\u6765\u5fc3\u7406\u7269\u7406\u6216\u4e34\u5e8a\u6d4b\u8bd5\u7b5b\u9009\u51fa\u66f4\u9ad8\u6f5c\u529b\u7684\u89c6\u89c9\u6807\u8bb0\u7cfb\u7edf\uff0c\u63a8\u52a8\u8f85\u52a9\u89c6\u89c9\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17705", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17705", "abs": "https://arxiv.org/abs/2601.17705", "authors": ["Abdullah Qureshi", "Kenneth Rice", "Alexander Wolpert"], "title": "Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings", "comment": "8 pages, 4 figures", "summary": "A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5d4c\u5165\u76f8\u4f3c\u5ea6\u8861\u91cf\u65b9\u6cd5DDR\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u4e0e\u4e0d\u76f8\u4f3c\u7684\u6587\u672c\u3002", "motivation": "\u76ee\u524d\u6587\u672c\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u5f80\u5f80\u4e0d\u80fd\u5f88\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u5bf9\u4e8e\u6587\u672c\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u611f\u77e5\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u5c0f\u8bed\u4e49\u53d8\u52a8\u9762\u524d\u533a\u5206\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u8d34\u5408\u4eba\u7c7b\u8bed\u4e49\u611f\u77e5\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u53d7Lipschitz\u8fde\u7eed\u6027\u542f\u53d1\uff0c\u63d0\u51fa\u8ddd\u79bb-\u8ddd\u79bb\u6bd4\uff08DDR\uff09\u8fd9\u4e00\u65b0\u6307\u6807\u3002\u901a\u8fc7\u5bf9\u53e5\u5b50\u505a\u8bcd\u8bed\u66ff\u6362\u6270\u52a8\uff08\u5305\u62ec\u7528\u540c\u4e49\u8bcd\u548c\u968f\u673a\u8bcd\u66ff\u6362\uff09\uff0c\u5206\u522b\u8003\u5bdf\u53d8\u66f4\u524d\u540e\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u53d8\u5316\uff0c\u4ece\u800c\u91cf\u5316\u4e0a\u4e0b\u6587\u5bf9\u8bed\u4e49\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u76f8\u4f3c\u5ea6\u6307\u6807\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8bed\u53e5\u4e2d\u5373\u4f7f\u662f\u6700\u5c0f\u5e45\u5ea6\u7684\u6270\u52a8\uff08\u5982\u63621\u4e2a\u540c\u4e49\u8bcd\u6216\u968f\u673a\u8bcd\uff09\uff0cDDR\u6307\u6807\u76f8\u6bd4\u5176\u4ed6\u4e3b\u6d41\u5d4c\u5165\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u80fd\u66f4\u7ec6\u81f4\u5730\u533a\u5206\u8bed\u4e49\u76f8\u8fd1\u4e0e\u76f8\u8fdc\u7684\u6587\u672c\u3002", "conclusion": "DDR\u4f5c\u4e3a\u65b0\u7684\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u5f0f\uff0c\u5728\u8bed\u4e49\u5fae\u6270\u4e0b\u80fd\u66f4\u8d34\u8fd1\u4eba\u7c7b\u8bed\u4e49\u611f\u77e5\uff0c\u4e3aLLM\u5d4c\u5165\u4e0b\u6587\u672c\u76f8\u4f3c\u6027\u6d4b\u91cf\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u3002"}}
{"id": "2601.17331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17331", "abs": "https://arxiv.org/abs/2601.17331", "authors": ["Fabian Vazquez", "Jose A. Nu\u00f1ez", "Diego Adame", "Alissen Moreno", "Augustin Zhan", "Huimin Li", "Jinghao Yang", "Haoteng Tang", "Bin Fu", "Pengfei Gu"], "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation", "comment": null, "summary": "Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u5148\u9a8c\u5f15\u5bfc\u6a21\u5757\uff08GPM\uff09\uff0c\u901a\u8fc7\u878d\u5408\u663e\u5f0f\u6df1\u5ea6\u51e0\u4f55\u4fe1\u606f\uff0c\u63d0\u5347\u4e86U-Net\u67b6\u6784\u5728\u7ed3\u80a0\u955c\u606f\u8089\u5206\u5272\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001Transformer\u6216Mamba\u7684U-Net\u7ed3\u6784\u867d\u7136\u5728\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u8f83\u597d\u8868\u73b0\uff0c\u4f46\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u6216\u6742\u4e71\u573a\u666f\u4e0b\u96be\u4ee5\u5145\u5206\u6355\u6349\u51e0\u4f55\u548c\u7ed3\u6784\u7ebf\u7d22\uff0c\u5f71\u54cd\u5206\u5272\u51c6\u786e\u5ea6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u5148\u9a8c\u6765\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u5148\u9a8c\u5f15\u5bfc\u6a21\u5757\uff08GPM\uff09\uff0c\u5176\u6838\u5fc3\u65b9\u6cd5\u4e3a\uff1a\u5148\u7528Visual Geometry Grounded Transformer\uff08VGGT\uff09\u5728\u6a21\u62df\u7684ColonDepth\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u751f\u6210\u4e0e\u5185\u955c\u606f\u8089\u573a\u666f\u76f8\u5339\u914d\u7684\u6df1\u5ea6\u56fe\uff1b\u8fd9\u4e9b\u6df1\u5ea6\u4fe1\u606f\u518d\u7ecfGPM\u5904\u7406\u88ab\u7f16\u7801\u5230U-Net\u7684\u7279\u5f81\u56fe\u4e2d\uff0c\u5e76\u7ed3\u5408\u7a7a\u95f4\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u7a81\u51fa\u672c\u5730\u7a7a\u95f4\u548c\u5168\u5c40\u901a\u9053\u4fe1\u606f\u3002GPM\u53ef\u63d2\u62d4\uff0c\u80fd\u591f\u8f7b\u677e\u6574\u5408\u8fdb\u5404\u79cdU-Net\u53d8\u4f53\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u606f\u8089\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u5f3a\u57fa\u7ebf\u4e0a\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4e0d\u540cU-Net\u7ed3\u6784\u7684\u666e\u9002\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u5f15\u5165\u51e0\u4f55\u6df1\u5ea6\u5148\u9a8c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e5f\u589e\u5f3a\u4e86\u5bf9\u590d\u6742\u573a\u666f\u7684\u9002\u5e94\u80fd\u529b\u3002GPM\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u53ef\u4e3a\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u68c0\u6d4b\u4e0e\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u5e26\u6765\u66f4\u9ad8\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17706", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17706", "abs": "https://arxiv.org/abs/2601.17706", "authors": ["Saptarshi Ghosh", "Linfeng Liu", "Tianyu Jiang"], "title": "A Computational Approach to Visual Metonymy", "comment": "EACL 2026", "summary": "Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u4e86\u89c6\u89c9\u8f6c\u55bb\uff08visual metonymy\uff09\uff0c\u5373\u901a\u8fc7\u56fe\u50cf\u7684\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u4f20\u8fbe\u6982\u5ff5\u3002\u4f5c\u8005\u6784\u5efa\u4e86ViMET\u89c6\u89c9\u8f6c\u55bb\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5176\u6d4b\u8bd5\u4e86\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u7406\u89e3\u95f4\u63a5\u89c6\u89c9\u8868\u8fbe\u65b9\u9762\u5b58\u5728\u8f83\u5927\u4e0d\u8db3\u3002", "motivation": "\u867d\u7136\u73b0\u5b9e\u751f\u6d3b\u4e2d\u4eba\u4eec\u5e38\u901a\u8fc7\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u7406\u89e3\u610f\u4e49\uff08\u5982\u5de5\u5177\u4ee3\u8868\u804c\u4e1a\uff09\uff0c\u4f46\u5f53\u524d\u5c1a\u65e0\u7cfb\u7edf\u63a2\u7d22\u89c6\u89c9\u8f6c\u55bb\u7684\u8ba1\u7b97\u65b9\u6cd5\u53ca\u76f8\u5173\u6570\u636e\u96c6\uff0c\u4e14\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u5177\u5907\u6b64\u7c7b\u8ba4\u77e5\u80fd\u529b\u4e5f\u4e0d\u660e\u786e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b26\u53f7\u5b66\u7406\u8bba\u3001\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6587\u751f\u56fe\u6a21\u578b\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u548c\u8bc6\u522b\u8f6c\u55bb\u6027\u89c6\u89c9\u8868\u8fbe\u3002\u57fa\u4e8e\u8be5\u65b9\u6cd5\uff0c\u4ed6\u4eec\u6784\u5efa\u4e86ViMET\u6570\u636e\u96c6\uff0c\u5305\u62ec2000\u4e2a\u591a\u9009\u9898\uff0c\u7528\u4ee5\u8bc4\u6d4b\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u89c6\u89c9\u8f6c\u55bb\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "result": "\u5728\u4eba\u7c7b\u4e0e\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0c\u4eba\u7c7b\u8868\u73b0\uff0886.9%\u6b63\u786e\u7387\uff09\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0865.9%\uff09\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5bf9\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u7684\u63a8\u7406\u8ba4\u77e5\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u89c6\u89c9\u8f6c\u55bb\u7684\u8ba1\u7b97\u6846\u67b6\u548c\u8bc4\u6d4b\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u95f4\u63a5\u89c6\u89c9\u8868\u8fbe\u7406\u89e3\u7684\u7f3a\u9677\uff0c\u4e3a\u672a\u6765\u63d0\u5347\u6a21\u578b\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u8303\u5f0f\u548c\u6d4b\u8bd5\u57fa\u51c6\u3002"}}
{"id": "2601.17336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17336", "abs": "https://arxiv.org/abs/2601.17336", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading", "comment": "22 pages", "summary": "Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bAGE-Net\uff0c\u7528\u4e8e\u819d\u5173\u8282X\u5149\u7247\u7684\u81ea\u52a8Kellgren-Lawrence\u5206\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7ea7\u51c6\u786e\u7387\u548c\u6a21\u578b\u89e3\u91ca\u6027\u3002", "motivation": "\u819d\u5173\u8282KL\u5206\u7ea7\u56e0\u7ed3\u6784\u53d8\u5316\u5fae\u5999\u3001\u89e3\u5256\u4f9d\u8d56\u957f\u8ddd\u79bb\u3001\u4ee5\u53ca\u7b49\u7ea7\u8fb9\u754c\u6a21\u7cca\u800c\u96be\u4ee5\u81ea\u52a8\u5316\u5b9e\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5f88\u597d\u5904\u7406\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6807\u7b7e\u987a\u5e8f\u6027\u3002", "method": "\u57fa\u4e8eConvNeXt\u67b6\u6784\uff0c\u878d\u5408\u9891\u8c31-\u7a7a\u95f4\u7279\u5f81\uff08SSF\uff09\u3001\u89e3\u5256\u56fe\u63a8\u7406\uff08AGR\uff09\u3001\u548c\u5dee\u5f02\u7ec6\u5316\uff08DFR\uff09\u6a21\u5757\u3002\u5f15\u5165Normal-Inverse-Gamma\uff08NIG\uff09\u8bc1\u636e\u56de\u5f52\u5934\u53ca\u5e8f\u5bf9\u6392\u5e8f\u7ea6\u675f\uff0c\u4ee5\u6355\u6349\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u7ef4\u6301\u6807\u7b7e\u987a\u5e8f\u3002", "result": "\u5728\u819d\u5173\u8282KL\u6570\u636e\u96c6\u4e0a\uff0cAGE-Net\u53d6\u5f97\u4e86QWK\u5206\u65700.9017\u00b10.0045\u3001\u5747\u65b9\u8bef\u5dee0.2349\u00b10.0028\uff0c\u4f18\u4e8e\u591a\u79cdCNN\u57fa\u7ebf\uff0c\u5e76\u5728\u6d88\u878d\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "AGE-Net\u6709\u6548\u89e3\u51b3\u4e86KL\u81ea\u52a8\u5206\u7ea7\u7684\u4e3b\u8981\u96be\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5206\u7ea7\u51c6\u786e\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\u53ca\u7ed3\u679c\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.17728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17728", "abs": "https://arxiv.org/abs/2601.17728", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei"], "title": "Unsupervised Elicitation of Moral Values from Language Models", "comment": null, "summary": "As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u514d\u76d1\u7763\u7b97\u6cd5\uff08ICM\uff09\u6316\u6398\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5185\u5728\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9053\u5fb7\u5224\u65ad\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u51cf\u5c11\u4e86\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u6781\u4e3a\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e14\u4eba\u5de5\u6784\u5efa\u9053\u5fb7\u8bc4\u4ef7\u6807\u6ce8\u56f0\u96be\u3001\u4e3b\u89c2\uff0c\u5e76\u5bb9\u6613\u5e26\u5165\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5bfb\u627e\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u6316\u6398\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u201c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\uff08ICM\uff09\u201d\u7b97\u6cd5\uff0c\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u6316\u6398\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u9053\u5fb7\u5224\u65ad\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002\u4e3b\u8981\u5bf9\u6bd4ICM\u65b9\u6cd5\u4e0e\u4f20\u7edf\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u53cachatbot\u6a21\u578b\u5728\u9053\u5fb7\u6807\u7b7e\u751f\u6210\u3001\u4e0d\u540c\u9053\u5fb7\u7406\u8bba\u6cdb\u5316\u548c\u793e\u4f1a\u504f\u89c1\u6291\u5236\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "ICM\u65b9\u6cd5\u5728Norm Bank\u3001ETHICS\u7b49\u6743\u5a01\u9053\u5fb7\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u5168\u90e8\u9884\u8bad\u7ec3\u53cachatbot\u6a21\u578b\uff1b\u7528ICM\u751f\u6210\u7684\u9053\u5fb7\u6807\u7b7e\u5fae\u8c03\u6a21\u578b\u6548\u679c\u53ef\u4e0e\u751a\u81f3\u8d85\u8fc7\u57fa\u4e8e\u4eba\u5de5\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u5728\u6b63\u4e49\u3001\u516c\u7406\u7b49\u9053\u5fb7\u7406\u8bba\u6846\u67b6\u4e0b\u8868\u73b0\u63d0\u5347\u5c24\u4e3a\u7a81\u51fa\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cICM\u80fd\u6709\u6548\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff08\u5982\u79cd\u65cf\u3001\u7ecf\u6d4e\u5730\u4f4d\u3001\u653f\u6cbb\u7b49\uff09\u9519\u8bef\u7387\u4e00\u534a\u4ee5\u4e0a\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6f5c\u85cf\u6709\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u901a\u8fc7\u514d\u76d1\u7763\u7b97\u6cd5\uff08\u5982ICM\uff09\u6709\u6548\u6fc0\u53d1\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u9053\u5fb7\u63a8\u7406\u7b97\u6cd5\u8868\u73b0\uff0c\u8fd8\u6709\u52a9\u4e8e\u63d0\u5347AI\u516c\u5e73\u6027\u548c\u51cf\u5c11\u4e3b\u89c2\u504f\u89c1\uff0c\u662f\u5b9e\u73b0AI\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2601.17340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17340", "abs": "https://arxiv.org/abs/2601.17340", "authors": ["Haodong He", "Xin Zhan", "Yancheng Bai", "Rui Lan", "Lei Sun", "Xiangxiang Chu"], "title": "TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution", "comment": "Accepted by ICASSP 2026", "summary": "Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u771f\u5b9e\u6570\u636e\u96c6Real-Texts\uff0c\u5e76\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u63d0\u51fa\u4e86TEXTS-Diff\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6587\u672c\u4e0e\u80cc\u666f\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u63d0\u5347\u4e86\u6587\u672c\u6062\u590d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7684\u6587\u672c\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\uff0c\u5bfc\u81f4\u6587\u672c\u533a\u57df\u7684\u8d85\u5206\u6548\u679c\u8f83\u5dee\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u6570\u636e\u96c6\u4ec5\u7531\u5b64\u7acb\u6587\u672c\u6837\u672c\u7ec4\u6210\uff0c\u80cc\u666f\u91cd\u5efa\u8d28\u91cf\u6709\u9650\u3002\u4e3a\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5927\u7684\u3001\u6765\u81ea\u771f\u5b9e\u573a\u666f\u7684\u542b\u6587\u672c\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u6709\u9488\u5bf9\u6027\u5730\u63d0\u5347\u6a21\u578b\u5bf9\u6587\u672c\u4e0e\u80cc\u666f\u7684\u6062\u590d\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u4e86Real-Texts\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e2d\u82f1\u6587\u3001\u573a\u666f\u591a\u6837\u7684\u771f\u5b9e\u56fe\u50cf\u6587\u672c\u5b9e\u4f8b\u3002\n2. \u63d0\u51fa\u4e86TEXTS-Aware Diffusion Model\uff08TEXTS-Diff\uff09\uff0c\u901a\u8fc7\u62bd\u8c61\u6982\u5ff5\u63d0\u5347\u5bf9\u573a\u666f\u4e2d\u6587\u5b57\u5143\u7d20\u7684\u7406\u89e3\uff0c\u901a\u8fc7\u5bf9\u5177\u4f53\u6587\u672c\u533a\u57df\u7684\u5efa\u6a21\u63d0\u9ad8\u7ec6\u8282\u6062\u590d\u80fd\u529b\uff0c\u5b9e\u73b0\u6587\u672c\u548c\u80cc\u666f\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTEXTS-Diff\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u6c34\u5e73\uff0c\u5177\u5907\u4f18\u5f02\u7684\u6cdb\u5316\u4e0e\u6587\u672c\u8fd8\u539f\u80fd\u529b\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u6587\u672c\u533a\u57df\u7684\u5931\u771f\u4e0e\u4f2a\u5f71\u3002", "conclusion": "\u7ed3\u5408\u5927\u89c4\u6a21\u771f\u5b9e\u6587\u672c\u56fe\u50cf\u6570\u636e\u96c6\u548c\u6587\u672c\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8d85\u5206\u65b9\u6cd5\u5bf9\u590d\u6742\u573a\u666f\u4e0b\u6587\u672c\u53ca\u80cc\u666f\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u63a8\u52a8\u4e86\u6587\u672c\u56fe\u50cf\u8d85\u5206\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17753", "abs": "https://arxiv.org/abs/2601.17753", "authors": ["Roberto Crotti", "Giovanni Denaro", "Zhiqiang Du", "Ricardo Mu\u00f1oz Mart\u00edn"], "title": "Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts", "comment": null, "summary": "Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Hylog\u7cfb\u7edf\uff0c\u4e00\u79cd\u7ed3\u5408\u6309\u952e\u8bb0\u5f55\u4e0e\u751f\u6001\u6587\u672c\u8bb0\u5f55\u7684\u6df7\u5408\u65e5\u5fd7\u5de5\u5177\uff0c\u4e13\u4e3a\u8f93\u5165\u6cd5\u5728\u975e\u62c9\u4e01\u6587\u5b57\u4e2d\u7684\u7814\u7a76\u8bbe\u8ba1\u3002\u901a\u8fc7\u63d2\u4ef6\u5f62\u5f0f\u652f\u6301\u5e38\u7528\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u6309\u952e\u4e0e\u5c4f\u5e55\u6e32\u67d3\u6587\u672c\u7684\u540c\u6b65\u8ffd\u8e2a\uff0c\u586b\u8865\u4e86\u4ee5\u5f80\u6309\u952e\u8bb0\u5f55\u9057\u6f0f\u8f93\u5165\u6cd5\u8f6c\u6362\u7ec6\u8282\u7684\u7a7a\u767d\u3002\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660e\u7cfb\u7edf\u80fd\u83b7\u53d6\u4f20\u7edf\u5de5\u5177\u4e0d\u53ef\u89c1\u7684\u6570\u636e\uff0c\u4e3a\u8f93\u5165\u6cd5\u4e0b\u7684\u6587\u672c\u8f93\u5165\u8ba4\u77e5\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u6309\u952e\u8bb0\u5f55\u5668\u65e0\u6cd5\u6355\u83b7\u8f93\u5165\u6cd5\uff08IME\uff09\u5bf9\u975e\u5b57\u6bcd\u6587\u5b57\u5728\u5c4f\u5e55\u4e0a\u7684\u8f6c\u6362\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u5bf9\u591a\u8bed\u8a00\u6587\u672c\u8f93\u5165\u8fc7\u7a0b\u7684\u8ba4\u77e5\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u5b66\u7f3a\u9677\u3002\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u63d0\u5347\u6587\u672c\u751f\u4ea7\u9886\u57df\u7684\u7814\u7a76\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86Hylog\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d2\u4ef6\u63a5\u5165Microsoft Word\u548cGoogle Chrome\u7b49\u4e3b\u6d41\u5e94\u7528\uff0c\u5206\u522b\u91c7\u96c6\u6309\u952e\u8f93\u51fa\u4e0e\u5c4f\u5e55\u6e32\u67d3\u6587\u672c\uff0c\u5e76\u5229\u7528\u6df7\u5408\u6a21\u5757\u540c\u6b65\u751f\u6210\u53cc\u91cd\u6d3b\u52a8\u8f68\u8ff9\u3002\u901a\u8fc7\u4e00\u9879\u7b80\u5316\u6c49\u5b57\u7ffb\u8bd1\u5b9e\u8bc1\u7814\u7a76\uff0c\u7cfb\u7edf\u8bb0\u5f55\u4e86\u62c9\u4e01\u5b57\u6bcd\u3001\u6c49\u5b57\u4e0e\u8f93\u5165\u6cd5\u786e\u8ba4\u7684\u6309\u952e\u884c\u4e3a\u53ca\u65f6\u5e8f\u7ec6\u8282\u3002", "result": "Hylog\u7cfb\u7edf\u80fd\u591f\u6210\u529f\u6355\u83b7\u4f20\u7edf\u6309\u952e\u8bb0\u5f55\u5668\u65e0\u6cd5\u83b7\u53d6\u7684IME\u76f8\u5173\u64cd\u4f5c\u548c\u65f6\u5e8f\u6570\u636e\uff0c\u5c55\u793a\u4e86\u6280\u672f\u53ef\u884c\u6027\u548c\u5206\u6790\u65b0\u80fd\u529b\uff0c\u4e3a\u5206\u6790\u8f93\u5165\u6cd5\u73af\u5883\u4e0b\u7684\u591a\u5c42\u6b21\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6570\u636e\u57fa\u7840\u3002", "conclusion": "Hylog\u7cfb\u7edf\u5f25\u8865\u4e86\u8f93\u5165\u6cd5\u7814\u7a76\u7684\u6280\u672f\u77ed\u677f\uff0c\u5176\u53ef\u6269\u5c55\u63d2\u4ef6\u67b6\u6784\u9002\u5408\u66f4\u591aIME\u4e0e\u5e94\u7528\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5177\u5305\u5bb9\u6027\u7684\u591a\u8bed\u8a00\u3001\u8de8\u5e73\u53f0\u6587\u672c\u8f93\u5165\u8ba4\u77e5\u7814\u7a76\u3002"}}
{"id": "2601.17342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17342", "abs": "https://arxiv.org/abs/2601.17342", "authors": ["Tong Wang", "Xiaodong Zhang", "Guanzhou Chen", "Jiaqi Wang", "Chenxi Liu", "Xiaoliang Tan", "Wenchao Guo", "Xuyang Li", "Xuanrui Wang", "Zifan Wang"], "title": "STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation", "comment": null, "summary": "Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \\textbf{STARS} (\\textbf{S}hared-specific \\textbf{T}ranslation and \\textbf{A}lignment for missing-modality \\textbf{R}emote \\textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9065\u611f\u591a\u6a21\u6001\u6570\u636e\u7f3a\u5931\u95ee\u9898\u7684\u8bed\u4e49\u5206\u5272\u65b0\u6846\u67b6STARS\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u7ffb\u8bd1\u4e0e\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u5206\u5272\u9c81\u68d2\u6027\u548c\u5c11\u6570\u7c7b\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5728\u9065\u611f\u5e94\u7528\u4e2d\uff0c\u878d\u5408\u5149\u5b66\u3001SAR\u548cDSM\u7b49\u591a\u6a21\u6001\u6570\u636e\u6709\u52a9\u4e8e\u63d0\u5347\u5730\u8868\u8bed\u4e49\u7406\u89e3\uff0c\u4f46\u5b9e\u9645\u4e2d\u7ecf\u5e38\u51fa\u73b0\u67d0\u4e00\u6a21\u6001\u6570\u636e\u7f3a\u5931\uff0c\u5bfc\u81f4\u878d\u5408\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u574d\u7f29\u548c\u6062\u590d\u7279\u5f81\u8fc7\u4e8e\u6cdb\u5316\u7b49\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u6570\u636e\u7f3a\u5931\u3002", "method": "\u672c\u6587\u63d0\u51faSTARS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u975e\u5bf9\u79f0\u5bf9\u9f50\u673a\u5236\uff0c\u91c7\u7528\u53cc\u5411\u7279\u5f81\u7ffb\u8bd1\u4e0estop-gradient\uff0c\u9632\u6b62\u7279\u5f81\u574d\u7f29\u5e76\u964d\u4f4e\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u6027\uff1b2\uff09\u50cf\u7d20\u7ea7\u8bed\u4e49\u91c7\u6837\u5bf9\u9f50\uff08PSA\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u7c7b\u522b\u5747\u8861\u50cf\u7d20\u91c7\u6837\u4e0e\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\uff0c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u5f15\u53d1\u7684\u5bf9\u9f50\u5931\u6548\uff0c\u63d0\u5347\u5c0f\u7c7b\u76ee\u6807\u7684\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTARS\u5728\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u8f93\u5165\u7684\u9065\u611f\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5c11\u6570\u7c7b\u522b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "STARS\u901a\u8fc7\u5171\u4eab-\u7279\u5f02\u6027\u7279\u5f81\u7ffb\u8bd1\u4e0e\u50cf\u7d20\u7ea7\u5bf9\u9f50\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7f3a\u5931\u5e26\u6765\u7684\u5206\u5272\u96be\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u6570\u636e\u4e0d\u5168\u60c5\u51b5\u4e0b\u7684\u5730\u8868\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2601.17755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17755", "abs": "https://arxiv.org/abs/2601.17755", "authors": ["Jinyoung Park", "Sanghyeok Lee", "Omar Zia Khan", "Hyunwoo J. Kim", "Joo-Kyung Kim"], "title": "ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation", "comment": "In progress", "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8fdb\u5c55\u611f\u77e5\u7684\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\u6846\u67b6ProGraph-R1\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u8d85\u56fe\u68c0\u7d22\u673a\u5236\u548c\u57fa\u4e8e\u63a8\u7406\u8fdb\u5c55\u7684\u5956\u52b1\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u591a\u8df3\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684RAG\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5ffd\u7565\u4e86\u56fe\u7ed3\u6784\uff0c\u5e76\u4e14\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u7ea7\u5956\u52b1\uff0c\u65e0\u6cd5\u6709\u6548\u523b\u753b\u4e2d\u95f4\u68c0\u7d22\u6b65\u9aa4\u7684\u8d28\u91cf\u548c\u4f9d\u8d56\u6027\uff0c\u5f71\u54cd\u591a\u8df3\u63a8\u7406\u7684\u8868\u73b0\u3002", "method": "ProGraph-R1\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\uff081\uff09\u7ed3\u6784\u611f\u77e5\u8d85\u56fe\u68c0\u7d22\u673a\u5236\uff0c\u7efc\u5408\u8003\u8651\u8bed\u4e49\u76f8\u5173\u6027\u548c\u56fe\u8282\u70b9\u95f4\u7684\u8fde\u901a\u6027\uff0c\u4fc3\u8fdb\u591a\u8df3\u63a8\u7406\u8def\u5f84\u7684\u8fde\u8d2f\u6027\u4e0e\u6df1\u5ea6\u63a8\u7406\uff1b\uff082\uff09\u57fa\u4e8e\u63a8\u7406\u8fdb\u5c55\u7684\u9010\u6b65\u5956\u52b1\u4f18\u5316\uff0c\u4e3a\u6bcf\u4e00\u6b65\u63a8\u7406\u5206\u914d\u5bc6\u96c6\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u6ce8\u91cd\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\u8fdb\u5c55\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6700\u7ec8\u8f93\u51fa\u7ed3\u679c\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cProGraph-R1\u4f18\u4e8e\u73b0\u6709\u7684GraphRAG\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u63a8\u7406\u51c6\u786e\u7387\u8fd8\u662f\u6587\u672c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ProGraph-R1\u80fd\u591f\u66f4\u597d\u5730\u7ed3\u5408\u56fe\u7ed3\u6784\u4fe1\u606f\u548c\u63a8\u7406\u8fc7\u7a0b\u8fdb\u5c55\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6c34\u5e73\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u57fa\u4e8e\u8bed\u4e49\u548c\u7a00\u758f\u5956\u52b1\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.17349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17349", "abs": "https://arxiv.org/abs/2601.17349", "authors": ["Hailong Yan", "Shice Liu", "Xiangtao Zhang", "Lujian Yao", "Fengxiang Yang", "Jinwei Chen", "Bo Li"], "title": "Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective", "comment": "Tech report", "summary": "In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eYUV\u8272\u5f69\u7a7a\u95f4\u7684\u8f7b\u91cf\u7ea7\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u79fb\u52a8\u4e92\u8054\u7f51\u65f6\u4ee3\uff0c\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u5bf9\u63d0\u5347\u79fb\u52a8\u8bbe\u5907\u62cd\u6444\u56fe\u7247\u7684\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u65b9\u6cd5\u5e38\u56e0\u5ffd\u7565\u901a\u9053\u7279\u5b9a\u7684\u9000\u5316\u6a21\u5f0f\u548c\u8de8\u901a\u9053\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u5728\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u548c\u6a21\u578b\u8f7b\u91cf\u5316\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u9891\u57df\u5206\u6790\u8bc1\u5b9eYUV\u8272\u5f69\u7a7a\u95f4\u5bf9\u4e8e\u4f4e\u5149\u7167\u589e\u5f3a\u66f4\u5177\u4f18\u52bf\u3002\u57fa\u4e8eYUV\u901a\u9053\u7684\u9000\u5316\u7279\u6027\uff0c\u63d0\u51fa\u4e86Y\u901a\u9053\u91c7\u7528\u53cc\u6d41\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u6a21\u5757\uff0cUV\u901a\u9053\u7528Y\u5f15\u5bfc\u7684\u5c40\u90e8\u611f\u77e5\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u4ea4\u4e92\u6a21\u5757\u878d\u5408\u7279\u5f81\u3002", "result": "\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u66f4\u9ad8\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\uff0c\u786e\u7acb\u4e86\u65b0\u4e00\u4ee3\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u6807\u6746\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u57fa\u4e8eYUV\u901a\u9053\u7279\u5b9a\u6062\u590d\u7b56\u7565\u7684\u8f7b\u91cf\u7ea7\u4f4e\u5149\u7167\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u517c\u987e\u89c6\u89c9\u8d28\u91cf\u4e0e\u6a21\u578b\u6548\u7387\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.17764", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17764", "abs": "https://arxiv.org/abs/2601.17764", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Jui Saha Pritha", "Abdullah Al Noman", "Abir Ahmed", "Golam Md Mohiuddin", "Tze Hui Liew"], "title": "Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali", "comment": "Accepted in 2025 4th International Conference on Smart Cities, Automation & Intelligent Computing Systems (ICON-SONICS)", "summary": "Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b5f\u52a0\u62c9\u8bed\u4e2d\u8868\u73b0\u51fa\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u4e8e\u82f1\u8bed\u7684\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u672c\u5730\u5316\u548c\u6587\u5316\u654f\u611f\u7684\u504f\u89c1\u68c0\u6d4b\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\uff0c\u7279\u522b\u662f\u5982\u5b5f\u52a0\u62c9\u8bed\u8fd9\u6837\u7684\u5168\u7403\u5357\u65b9\u8bed\u8a00\u4e2d\uff0c\u4ecd\u5b58\u5728\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u800c\u6b64\u524d\u76f8\u5173\u7814\u7a76\u4e3b\u8981\u805a\u7126\u4e8e\u82f1\u8bed\u3002\u8fd9\u6fc0\u53d1\u4e86\u672c\u6587\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u4e2d\u6027\u522b\u504f\u89c1\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cd\u65b9\u6cd5\u68c0\u6d4b\u6027\u522b\u504f\u89c1\uff0c\u5305\u62ec\u8bcd\u5178\u6316\u6398\u3001\u8ba1\u7b97\u5206\u7c7b\u6a21\u578b\u3001\u57fa\u4e8e\u7ffb\u8bd1\u7684\u5bf9\u6bd4\u5206\u6790\u53ca\u57fa\u4e8eGPT\u7684\u504f\u89c1\u751f\u6210\uff0c\u5e76\u4e14\u8fd8\u5728\u519c\u6751\u548c\u4f4e\u6536\u5165\u5730\u533a\u8fdb\u884c\u4e86\u5b9e\u5730\u8c03\u67e5\uff0c\u4ee5\u83b7\u53d6\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u522b\u504f\u89c1\u6570\u636e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76f4\u63a5\u5c06\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u65f6\uff0c\u53d7\u9650\u4e8e\u8bed\u8a00\u7ed3\u6784\u548c\u6587\u5316\u5dee\u5f02\uff0c\u6548\u679c\u6709\u9650\u3002\u6b64\u5916\uff0c\u5b9e\u5730\u8c03\u67e5\u63ed\u793a\u4e86\u5b5f\u52a0\u62c9\u8bed\u6027\u522b\u504f\u89c1\u8868\u73b0\u51fa\u4e0e\u82f1\u8bed\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u8bb8\u591a\u81ea\u52a8\u5316\u7cfb\u7edf\u96be\u4ee5\u8bc6\u522b\u7684\u504f\u89c1\u88ab\u793e\u533a\u8c03\u7814\u65b9\u6cd5\u6240\u53d1\u73b0\u3002", "conclusion": "\u5e94\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7b49\u5f31\u52bf\u8bed\u8a00\u5f00\u53d1\u672c\u5730\u5316\u4e14\u8d34\u5408\u6587\u5316\u7684\u504f\u89c1\u68c0\u6d4b\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u793e\u533a\u9a71\u52a8\u7684\u65b9\u6cd5\u8bc6\u522b\u88ab\u81ea\u52a8\u7cfb\u7edf\u5ffd\u89c6\u7684\u504f\u89c1\u3002\u8be5\u7814\u7a76\u4e3a\u5c06\u6765\u5728\u5b5f\u52a0\u62c9\u8bed\u548c\u5176\u4ed6\u5370\u5ea6\u6b21\u5927\u9646\u8bed\u8a00\u4e2d\u51cf\u5c11\u6027\u522b\u504f\u89c1\u3001\u63a8\u52a8\u66f4\u516c\u5e73\u5305\u5bb9\u7684NLP\u7cfb\u7edf\u6253\u4e0b\u57fa\u7840\u3002"}}
{"id": "2601.17350", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17350", "abs": "https://arxiv.org/abs/2601.17350", "authors": ["Xianliang Huang", "Zhizhou Zhong", "Shuhang Chen", "Yi Xu", "Juhong Guan", "Shuigeng Zhou"], "title": "NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields", "comment": "14 pages, 15 figures", "summary": "Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \\textbf{P}atch-based \\textbf{E}ntropy for \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \\textbf{P}rogressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5NeRF-MIR\uff0c\u4e13\u95e8\u7528\u4e8e\u6062\u590d\u88ab\u906e\u6321\uff08masked\uff09\u7684\u56fe\u50cf\uff0c\u5728\u6062\u590d\u635f\u574f\u56fe\u50cf\u751f\u6210\u4e09\u7ef4\u573a\u666f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136NeRF\u5728\u65b0\u89c6\u89d2\u5408\u6210\u6709\u663e\u8457\u8868\u73b0\uff0c\u4f46\u9762\u5bf9\u7531\u4e8e\u906e\u6321\u7b49\u539f\u56e0\u800c\u53d7\u635f\u7684\u56fe\u50cf\u65f6\u6062\u590d\u6548\u679c\u6709\u9650\u3002\u7136\u800c\u5728\u81ea\u7136\u573a\u666f\u6570\u636e\u91c7\u96c6\u65f6\uff0c\u56fe\u50cf\u635f\u574f\u5f88\u5e38\u89c1\uff0c\u56e0\u6b64\u63d0\u5347NeRF\u5bf9\u635f\u574f\uff08\u5c24\u5176\u662f\u906e\u6321\uff09\u56fe\u50cf\u7684\u5904\u7406\u80fd\u529b\u5341\u5206\u91cd\u8981\u3002", "method": "1. \u63d0\u51faNeRF-MIR\u65b9\u6cd5\uff0c\u4e13\u7528\u4e8e\u906e\u6321\u56fe\u50cf\u6062\u590d\uff1b2. \u63d0\u51fa\u57fa\u4e8ePatch\u7684\u71b5\u9a71\u52a8\u5c04\u7ebf\u5206\u5e03\u7b56\u7565\uff08PERE\uff09\uff0c\u66f4\u6709\u6548\u5730\u5b66\u4e60\u56fe\u50cf\u7eb9\u7406\u4fe1\u606f\u5e76\u591a\u89c6\u56fe\u878d\u5408\u4fe1\u606f\uff1b3. \u5f15\u5165\u9012\u8fdb\u5f0f\u81ea\u8bad\u7ec3\u6062\u590d\u673a\u5236\uff08PIRE\uff09\uff0c\u9010\u6b65\u6062\u590d\u88ab\u906e\u6321\u533a\u57df\uff1b4. \u8bbe\u8ba1\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u81ea\u52a8\u8c03\u6574\u906e\u6321\u533a\u57df\u635f\u5931\u6743\u91cd\uff1b5. \u6784\u5efa\u4e09\u4e2a\u4eba\u5de5\u906e\u6321\u6570\u636e\u96c6\u4ee5\u6a21\u62df\u771f\u5b9e\u635f\u574f\u60c5\u666f\u3002", "result": "\u5728\u771f\u5b9e\u4ee5\u53ca\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aNeRF-MIR\u5728\u906e\u6321\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5176\u4ed6\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "NeRF-MIR\u6709\u6548\u63d0\u5347\u4e86NeRF\u5bf9\u906e\u6321\u56fe\u50cf\u7684\u4e09\u7ef4\u6062\u590d\u80fd\u529b\uff0c\u5404\u9879\u521b\u65b0\u7b56\u7565\u5747\u5bf9\u63d0\u5347\u6027\u80fd\u6709\u663e\u8457\u4f5c\u7528\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u4e2d\u635f\u574f\u56fe\u50cf\u4e09\u7ef4\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.17777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17777", "abs": "https://arxiv.org/abs/2601.17777", "authors": ["Xiaoyu Liu", "Xiaoyu Guan", "Di Liang", "Xianjie Wu"], "title": "DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the \"seesaw effect\": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u4efb\u52a1\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u671f\u95f4\u964d\u4f4e\u4efb\u52a1\u5e72\u6270\u548c\u201c\u8df7\u8df7\u677f\u6548\u5e94\u201d\u7684\u53c2\u6570\u9694\u79bb\u65b9\u6cd5\u3002\u901a\u8fc7\u8bc6\u522b\u548c\u51bb\u7ed3\u4efb\u52a1\u76f8\u5173\u7684\u6838\u5fc3\u53c2\u6570\uff0c\u5b9e\u73b0\u5404\u4efb\u52a1\u95f4\u7684\u51b2\u7a81\u51cf\u5c11\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u7684\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u4ea7\u751f\u76ee\u6807\u51b2\u7a81\uff0c\u5373\u4f18\u5316\u4e00\u4e2a\u4efb\u52a1\u65f6\u4f1a\u635f\u5bb3\u5176\u4ed6\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5c24\u5176\u8868\u73b0\u5728\u53c2\u6570\u88ab\u65e0\u5dee\u522b\u66f4\u65b0\u65f6\u3002\u4f5c\u8005\u8bd5\u56fe\u4ece\u53c2\u6570\u7a7a\u95f4\u5f02\u8d28\u6027\u89d2\u5ea6\u89e3\u51b3\u6b64\u5e72\u6270\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5206\u522b\u5bf9\u4e0d\u540cSFT\u4efb\u52a1\u72ec\u7acb\u5fae\u8c03\u6a21\u578b\uff0c\u4f9d\u636e\u53c2\u6570\u66f4\u65b0\u5e45\u5ea6\u8bc6\u522b\u5404\u4efb\u52a1\u7684\u201c\u6838\u5fc3\u53c2\u6570\u533a\u201d\u3002\u9ad8\u5ea6\u91cd\u53e0\u7684\u4efb\u52a1\u88ab\u5408\u5e76\u8054\u5408\u8bad\u7ec3\uff0c\u57fa\u672c\u65e0\u91cd\u53e0\u7684\u4efb\u52a1\u88ab\u5206\u9636\u6bb5\u5904\u7406\u3002\u591a\u9636\u6bb5\u5fae\u8c03\u4e2d\uff0c\u5148\u524d\u4efb\u52a1\u7684\u6838\u5fc3\u53c2\u6570\u88ab\u51bb\u7ed3\uff0c\u9632\u6b62\u540e\u7eed\u4efb\u52a1\u8986\u76d6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u672c\u6587\u52a8\u6001\u53c2\u6570\u9694\u79bb\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u4efb\u52a1\u95f4\u51b2\u7a81\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u9636\u6bb5\u53ca\u591a\u4efb\u52a1\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u53c2\u6570\u9694\u79bb\u662f\u4e00\u79cd\u6709\u6548\u7f13\u89e3\u76d1\u7763\u5fae\u8c03\u65f6\u591a\u4e2a\u5f02\u8d28\u4efb\u52a1\u201c\u8df7\u8df7\u677f\u6548\u5e94\u201d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4fc3\u8fdbLLM\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u4e00\u81f4\u6027\u8868\u73b0\u63d0\u5347\u3002"}}
{"id": "2601.17352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17352", "abs": "https://arxiv.org/abs/2601.17352", "authors": ["M. L. Mamud", "Piyoosh Jaysaval", "Frederick D Day-Lewis", "M. K. Mudunuru"], "title": "HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data", "comment": null, "summary": "Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u5149\u8c31\u77ff\u7269\u5206\u7c7b\u6a21\u578bHyDeMiC\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u566a\u58f0\u6570\u636e\uff0c\u5728\u5b9e\u9645\u9065\u611f\u77ff\u7269\u63a2\u6d4b\u4e2d\u5177\u6709\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u5149\u8c31\u77ff\u7269\u5206\u7c7b\u65b9\u6cd5\u5728\u9762\u5bf9\u73af\u5883\u566a\u58f0\u3001\u4f20\u611f\u5668\u9650\u5236\u548c\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u80fd\u5728\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e0b\u9c81\u68d2\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578bHyDeMiC\uff0c\u5229\u7528\u7f8e\u56fd\u5730\u8d28\u8c03\u67e5\u5c40\uff08USGS\uff09\u77ff\u7269\u5e93\u7684115\u79cd\u77ff\u7269\u5b9e\u9a8c\u5ba4\u9ad8\u5149\u8c31\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6570\u636e\u901a\u8fc7\u4e0e\u4f20\u611f\u5668\u54cd\u5e94\u51fd\u6570\u5377\u79ef\u5408\u6210\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u4e09\u79cd\u542b\u94dc\u77ff\u7269\uff08\u94dc\u8f89\u77ff\u3001\u5b54\u96c0\u77f3\u3001\u9ec4\u94dc\u77ff\uff09\u4e3a\u6848\u4f8b\u8bc4\u4f30\u6027\u80fd\u3002", "result": "HyDeMiC\u6a21\u578b\u5728\u65e0\u566a\u58f0\u548c\u4f4e\u566a\u58f0\uff08MCC=1.00\uff09\u4e0b\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5728\u4e2d\u7b49\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f9d\u7136\u4f18\u79c0\uff0c\u5c55\u793a\u4e86\u8f83\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "HyDeMiC\u5728\u9762\u5bf9\u771f\u5b9e\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u7684\u566a\u58f0\u65f6\u4f9d\u7136\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u9ad8\u5149\u8c31\u9065\u611f\u9886\u57df\u77ff\u7269\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b0\u5de5\u5177\u3002"}}
{"id": "2601.17781", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17781", "abs": "https://arxiv.org/abs/2601.17781", "authors": ["Andreas S\u00e4uberli", "Darja Jepifanova", "Diego Frassinelli", "Barbara Plank"], "title": "Controlling Reading Ease with Gaze-Guided Text Generation", "comment": "Accepted for publication at EACL 2026", "summary": "The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5229\u7528\u9884\u6d4b\u4eba\u7c7b\u6ce8\u89c6\u6a21\u5f0f\u7684\u6a21\u578b\uff0c\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6613\u8bfb\u6216\u96be\u8bfb\u7684\u6587\u672c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u6709\u6548\u63a7\u5236\u6587\u672c\u53ef\u8bfb\u6027\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9\u6587\u672c\u53ef\u8bfb\u6027\u7684\u751f\u6210\u63a7\u5236\u65b9\u6cd5\u6709\u9650\uff0c\u800c\u773c\u52a8\u884c\u4e3a\u80fd\u53cd\u6620\u6587\u672c\u5904\u7406\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u56e0\u6b64\u4f5c\u8005\u5c1d\u8bd5\u7528\u773c\u52a8\u6570\u636e\u6307\u5bfc\u6587\u672c\u751f\u6210\uff0c\u4f7f\u6587\u672c\u53ef\u63a7\u5730\u66f4\u6613\u6216\u66f4\u96be\u8bfb\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u773c\u52a8\u9884\u6d4b\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u773c\u52a8\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u53cd\u9988\uff0c\u63a7\u5236\u751f\u6210\u6587\u672c\u7684\u53ef\u8bfb\u6027\u3002\u65b9\u6cd5\u901a\u8fc7\u5b9e\u9a8c\u8ba9\u82f1\u8bed\u6bcd\u8bed\u8005\u548c\u975e\u6bcd\u8bed\u8005\u5b9e\u9645\u9605\u8bfb\u751f\u6210\u6587\u672c\uff0c\u5e76\u7528\u773c\u52a8\u4eea\u91c7\u96c6\u9605\u8bfb\u65f6\u95f4\u7b49\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u6cd5\u80fd\u660e\u663e\u63d0\u9ad8\u6216\u964d\u4f4e\u751f\u6210\u6587\u672c\u7684\u6613\u8bfb\u6027\uff0c\u4e0d\u8bba\u662f\u4ece\u9605\u8bfb\u65f6\u95f4\u8fd8\u662f\u4e3b\u89c2\u96be\u5ea6\u8bc4\u4ef7\u3002\u7edf\u8ba1\u5206\u6790\u53d1\u73b0\u6b64\u53d8\u5316\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f71\u54cd\u8bcd\u6c47\u5904\u7406\u7684\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u6587\u672c\u7b80\u5316\u3001\u63d0\u5347\u4fe1\u606f\u83b7\u53d6\uff0c\u4ee5\u53ca\u8bed\u8a00\u5b66\u4e60\u4e2d\u4e2a\u6027\u5316\u6559\u5b66\u6750\u6599\u751f\u6210\u7b49\u573a\u666f\uff0c\u5bf9\u63d0\u9ad8\u4fe1\u606f\u53ef\u53ca\u6027\u548c\u5b66\u4e60\u6548\u679c\u5177\u6709\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.17354", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17354", "abs": "https://arxiv.org/abs/2601.17354", "authors": ["Wenzhi Guo", "Guangchi Fang", "Shu Yang", "Bing Wang"], "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling", "comment": null, "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.", "AI": {"tldr": "PocketGS\u662f\u4e00\u79cd\u4e13\u4e3a\u79fb\u52a8\u8bbe\u5907\u8bbe\u8ba1\u7684\u9ad8\u65483D\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6761\u4ef6\u4e0b\uff0c\u4f9d\u7136\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u5efa\uff0c\u4e14\u8d85\u8fc7\u4e3b\u6d41\u5de5\u4f5c\u7ad9\u65b9\u6848\u3002", "motivation": "\u73b0\u67093D Gaussian Splatting\uff083DGS\uff09\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u957f\u65f6\u95f4\u8bad\u7ec3\uff0c\u96be\u4ee5\u5728\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u5e94\u7528\u3002\u5982\u4f55\u5728\u4fdd\u8bc1\u9ad8\u5efa\u6a21\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u79fb\u52a8\u7aef\u9ad8\u6548\u3001\u5b9e\u65f6\u76843D\u573a\u666f\u91cd\u5efa\uff0c\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "PocketGS\u901a\u8fc7\u4e09\u4e2a\u534f\u540c\u8bbe\u8ba1\u7684\u64cd\u4f5c\u7b26\u6765\u6539\u826f\u4f20\u7edf3DGS\u6d41\u7a0b\uff1a\uff081\uff09G\u64cd\u4f5c\u7b26\u7528\u4e8e\u751f\u6210\u51e0\u4f55\u51c6\u786e\u7684\u70b9\u4e91\u5148\u9a8c\uff1b\uff082\uff09I\u64cd\u4f5c\u7b26\u6ce8\u5165\u5c40\u90e8\u8868\u9762\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f18\u5316\u65e9\u671f\u9ad8\u65af\u5206\u5e03\u7684\u8bbe\u5b9a\uff1b\uff083\uff09T\u64cd\u4f5c\u7b26\u91c7\u7528\u7f13\u5b58\u4e0e\u7d22\u5f15\u6620\u5c04\u7684\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u673a\u5236\uff0c\u63d0\u9ad8\u79fb\u52a8\u7aef\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002\u8fd9\u4e9b\u64cd\u4f5c\u7b26\u5171\u540c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3001\u5185\u5b58\u5360\u7528\u548c\u5efa\u6a21\u7cbe\u5ea6\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cPocketGS\u5728\u79fb\u52a8\u7aef\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e3b\u6d41\u5de5\u4f5c\u7ad93DGS\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u7ed3\u679c\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u8bad\u7ec3\u901f\u5ea6\u3001\u5185\u5b58\u7d27\u51d1\u6027\u4ee5\u53ca\u6a21\u578b\u8fd8\u539f\u7cbe\u5ea6\u3002", "conclusion": "PocketGS\u4e3a\u79fb\u52a8\u8bbe\u59073D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u3001\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f973D\u6355\u6349\u5230\u6e32\u67d3\u6d41\u7a0b\u53ef\u4ee5\u5b8c\u5168\u5728\u79fb\u52a8\u7aef\u8bbe\u5907\u4e0a\u5b9e\u65f6\u5b8c\u6210\u3002"}}
{"id": "2601.17786", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17786", "abs": "https://arxiv.org/abs/2601.17786", "authors": ["Yixin Liu", "Kehan Yan", "Shiyuan Li", "Qingfeng Chen", "Shirui Pan"], "title": "Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations", "comment": "17 pages, 7 tables, and 5 figures", "summary": "Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step \"embedding-detector\" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u591a\u89c6\u89d2\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6MCA^2\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u91cd\u5efa\u6a21\u578b\u548c\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u5bf9\u591a\u79cd\u6570\u636e\u96c6\u7684\u9002\u5e94\u6027\u548c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u5355\u4e00\u5d4c\u5165\u6a21\u578b\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5f02\u5e38\u7c7b\u578b\uff0c\u68c0\u6d4b\u6548\u679c\u53d7\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51faMCA^2\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u62bd\u53d6\u4e0d\u540c\u7684\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u91cd\u5efa\u6a21\u578b\u5b66\u4e60\u6b63\u5e38\u6587\u672c\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5bf9\u6bd4\u534f\u4f5c\u6a21\u5757\u6765\u4fc3\u8fdb\u4e0d\u540c\u89c6\u89d2\u95f4\u7684\u4fe1\u606f\u4e92\u8865\u3002\u540c\u65f6\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u5206\u914d\u6a21\u5757\uff0c\u6839\u636e\u5177\u4f53\u6570\u636e\u81ea\u52a8\u8c03\u6574\u5404\u89c6\u89d2\u7684\u6743\u91cd\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0e\u591a\u79cd\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\uff0cMCA^2\u8868\u73b0\u51fa\u66f4\u597d\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u89c6\u89d2\u3001\u591a\u5d4c\u5165\u3001\u591a\u6a21\u5757\u534f\u4f5c\u4ee5\u53ca\u81ea\u9002\u5e94\u673a\u5236\u80fd\u591f\u5168\u9762\u63d0\u5347\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u9002\u5e94\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660eMCA^2\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2601.17366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17366", "abs": "https://arxiv.org/abs/2601.17366", "authors": ["Chengbo Ding", "Fenghe Tang", "Shaohua Kevin Zhou"], "title": "UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation", "comment": "Accepted by ISBI 2026", "summary": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8f6e\u5ed3\u611f\u77e5\u533a\u57df\u7f6e\u6362\u6846\u67b6UCAD\uff0c\u7528\u4e8e\u63d0\u5347\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6548\u679c\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5206\u5272\u4e2d\u7684\u533a\u57df\u7f6e\u6362\u65b9\u6cd5\u901a\u5e38\u4ec5\u64cd\u4f5c\u77e9\u5f62\u533a\u57df\uff0c\u5bfc\u81f4\u89e3\u5256\u7ed3\u6784\u8fb9\u754c\u626d\u66f2\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u3002", "method": "UCAD\u6846\u67b6\u5229\u7528\u8d85\u50cf\u7d20\u6280\u672f\u751f\u6210\u7b26\u5408\u89e3\u5256\u7ed3\u6784\u7684\u533a\u57df\uff0c\u4ee5\u4fdd\u6301\u8f6e\u5ed3\u8bed\u4e49\uff1b\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9009\u62e9\u673a\u5236\uff0c\u4f18\u5148\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u533a\u57df\u8fdb\u884c\u7f6e\u6362\u4ee5\u589e\u5f3a\u4e00\u81f4\u6027\u5b66\u4e60\uff1b\u540c\u65f6\u63d0\u51fa\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u4e00\u81f4\u6027\u635f\u5931\uff0c\u81ea\u9002\u5e94\u5730\u7a33\u5b9a\u8bad\u7ec3\u5e76\u5bf9\u672a\u6807\u6ce8\u533a\u57df\u8fdb\u884c\u6709\u6548\u6b63\u5219\u5316\u3002", "result": "\u5927\u89c4\u6a21\u5b9e\u9a8c\u663e\u793a\uff0cUCAD\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u7684\u5206\u5272\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u7684\u534a\u76d1\u7763\u5206\u5272\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "UCAD\u5728\u4fdd\u6301\u89e3\u5256\u8f6e\u5ed3\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u6807\u6ce8\u6570\u636e\u5229\u7528\u7684\u6709\u6548\u6027\uff0c\u662f\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u534a\u76d1\u7763\u5206\u5272\u6027\u80fd\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.17823", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17823", "abs": "https://arxiv.org/abs/2601.17823", "authors": ["Pranav Kasela", "Marco Braga", "Alessandro Ghiotto", "Andrea Pilzer", "Marco Viviani", "Alessandro Raganato"], "title": "DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation", "comment": "Published in CLiC-IT '25: https://aclanthology.org/2025.clicit-1.52/", "summary": "In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DIETA\u2014\u2014\u4e00\u4e2a\u4e13\u4e3a\u610f\u82f1\u7ffb\u8bd1\u8bbe\u8ba1\u7684\u5c0f\u578b\u89e3\u7801\u5668Transformer\u6a21\u578b\uff0c\u62e5\u67095\u4ebf\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u4e13\u6ce8\u4e8e\u610f\u5927\u5229\u8bed-\u82f1\u8bed\u7684\u9ad8\u8d28\u91cf\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u6709\u9650\uff0c\u4e14\u9488\u5bf9\u8be5\u8bed\u8a00\u5bf9\u7684\u5c0f\u6a21\u578b\u8868\u73b0\u6709\u5f85\u63d0\u9ad8\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u4e13\u95e8\u4f18\u5316\u7684\u7cfb\u7edf\u53ca\u76f8\u5e94\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u3001\u6574\u7406\u4e86\u7ea62.07\u4ebf\u5bf9\u610f\u82f1\u5e73\u884c\u8bed\u6599\uff0c\u6db5\u76d6\u591a\u79cd\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u56de\u8bd1\u751f\u62103.52\u4ebf\u8bed\u6599\u3002\u6a21\u578b\u4e3a\u5e26\u67095\u4ebf\u53c2\u6570\u7684decoder-only Transformer\uff0c\u5e76\u65b0\u5efa\u4e86\u57fa\u4e8eWikiNews\u7684\u5f53\u4ee3\u6587\u672c\u8bc4\u6d4b\u96c6\u3002\u6a21\u578b\u53ca\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\u3002", "result": "\u5728\u591a\u4e2a\u610f\u82f1\u7ffb\u8bd1\u57fa\u51c6\u4e0a\uff0cDIETA\u7684\u8868\u73b0\u8fdb\u516532\u7cfb\u7edf\u6392\u884c\u699c\u7684\u7b2c\u4e8c\u68af\u961f\uff0c\u5e76\u5728\u4e94\u4e2a\u6d4b\u8bd5\u96c6\u4e2d\u6709\u56db\u4e2a\u8d85\u8d8a\u4e86\u5176\u5b833B\u53c2\u6570\u4ee5\u4e0b\u6a21\u578b\u3002", "conclusion": "DIETA\u8bc1\u660e\u4e86\u5c0f\u578b\u3001\u4e13\u95e8\u5316Transformer\u5728\u610f\u82f1\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u63a8\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2601.17383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17383", "abs": "https://arxiv.org/abs/2601.17383", "authors": ["Chen Ling", "Kai Hu", "Hangcheng Liu", "Xingshuo Han", "Tianwei Zhang", "Changhai Ou"], "title": "Physical Prompt Injection Attacks on Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5c06\u653b\u51fb\u6027\u63d0\u793a\u4fe1\u606f\u7269\u7406\u690d\u5165\u573a\u666f\u4e2d\u7684\u65b0\u578b\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u65e0\u9700\u8bbf\u95ee\u8f93\u5165\u901a\u9053\u6216\u5148\u77e5\u7528\u6237\u67e5\u8be2\uff0c\u5373\u53ef\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u5f71\u54cd\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u884c\u4e3a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6781\u9ad8\u653b\u51fb\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LVLMs\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4f46\u4f20\u7edf\u653b\u51fb\u5047\u8bbe\u96be\u4ee5\u6210\u7acb\uff08\u5982\u9700\u8bbf\u95ee\u8f93\u5165\u3001\u9884\u77e5\u67e5\u8be2\uff09\uff0c\u56e0\u6b64\u6025\u9700\u4e00\u79cd\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u573a\u666f\u7684\u65b0\u578b\u6709\u6548\u653b\u51fb\u65b9\u5f0f\uff0c\u4ee5\u66f4\u597d\u8bc4\u4f30\u4e0e\u63d0\u5347LVLMs\u5b89\u5168\u6027\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7269\u7406\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff08PPIA\uff09\uff0c\u901a\u8fc7\u5728\u7269\u7406\u73af\u5883\u7269\u4f53\u4e0a\u5d4c\u5165\u5177\u6709\u5f3a\u8bed\u4e49\u5f15\u5bfc\u4f5c\u7528\u7684\u89c6\u89c9\u63d0\u793a\uff0c\u5b9e\u73b0\u5bf9LVLMs\u7684\u9ed1\u76d2\u64cd\u63a7\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u79bb\u7ebf\u9ad8\u6548\u9009\u62e9\u53ef\u611f\u77e5\u6027\u4e0e\u8bed\u4e49\u6027\u517c\u4f18\u7684\u63d0\u793a\u3001\u5e76\u57fa\u4e8e\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u6218\u7565\u6027\u9009\u62e9\u690d\u5165\u4f4d\u7f6e\uff0c\u786e\u4fdd\u6a21\u578b\u5bf9\u5176\u8bc6\u522b\u548c\u53d7\u5f71\u54cd\u3002", "result": "\u572810\u4e2a\u4e3b\u6d41LVLMs\u4e0a\u4e8e\u89c6\u89c9\u95ee\u7b54\u3001\u89c4\u5212\u548c\u5bfc\u822a\u7b49\u591a\u4efb\u52a1\u4e2d\u8bc4\u4f30PPIA\uff0c\u8986\u76d6\u4eff\u771f\u53ca\u771f\u5b9e\u573a\u666f\uff0c\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u8fbe98%\uff0c\u4e14\u5728\u8ddd\u79bb\u3001\u89c6\u89d2\u3001\u5149\u7167\u7b49\u7269\u7406\u6761\u4ef6\u53d8\u6362\u4e0b\u4f9d\u7136\u8868\u73b0\u51fa\u6781\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "PPIA\u5c55\u793a\u4e86LVLMs\u5728\u73b0\u5b9e\u7269\u7406\u4e16\u754c\u4e2d\u9762\u5bf9\u9ed1\u76d2\u7269\u7406\u6ce8\u5165\u653b\u51fb\u7684\u9ad8\u5ea6\u8106\u5f31\u6027\uff0c\u63d0\u9192\u4e1a\u754c\u5173\u6ce8\u548c\u9632\u8303\u7269\u7406\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u5b89\u5168\u6027\u8bbe\u8ba1\u4e0e\u9632\u5fa1\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\u3002"}}
{"id": "2601.17829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17829", "abs": "https://arxiv.org/abs/2601.17829", "authors": ["Dan Greenstein", "Zohar Karnin", "Chen Amiraz", "Oren Somekh"], "title": "Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents", "comment": null, "summary": "The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \\texttt{city\\_name}, \\texttt{stock\\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u591a\u6837\u6027\u6307\u6807\u81ea\u52a8\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u51fd\u6570\u8c03\u7528\u667a\u80fd\u4f53\u7684\u6570\u636e\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51fd\u6570\u8c03\u7528\u667a\u80fd\u4f53\u7684\u80fd\u529b\u6269\u5c55\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u591a\u6837\u6027\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u8bf7\u6c42\u8bed\u8a00\u548c\u53c2\u6570\u7684\u591a\u6837\u6027\u8986\u76d6\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u4f18\u5316\u901a\u7528\u591a\u6837\u6027\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u517c\u5177\u8bf7\u6c42\u548c\u53c2\u6570\u591a\u6837\u6027\u7684\u8bed\u6599\uff0c\u800c\u975e\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u6216\u7279\u5b9a\u9886\u57df\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u751f\u6210\u6570\u636e\u96c6\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u5728\u672c\u4f53\u5185\u6d4b\u8bc4\u548c\u4e0b\u6e38\u5b9e\u9645\u4efb\u52a1\u4e2d\u90fd\u5bf9\u6bd4\u4e86\u4e3b\u6d41\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u6570\u636e\u6b63\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u591a\u6837\u6027\u3002\u6a21\u578b\u7528\u65b0\u6570\u636e\u96c6\u8bad\u7ec3\u540e\uff0c\u5728BFCL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53477.4%\u3002", "conclusion": "\u81ea\u52a8\u4f18\u5316\u591a\u6837\u6027\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51fd\u6570\u8c03\u7528\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u53c2\u6570\u53ca\u8bf7\u6c42\u8bed\u8a00\u591a\u6837\u6027\u8986\u76d6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u76f8\u5173\u6570\u636e\u96c6\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17388", "abs": "https://arxiv.org/abs/2601.17388", "authors": ["Xuan Ding", "Xiu Yan", "Chuanlong Xie", "Yao Zhu"], "title": "ONRW: Optimizing inversion noise for high-quality and robust watermark", "comment": "Preprint. Under review", "summary": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u3001\u5f3a\u9c81\u68d2\u6027\u6c34\u5370\u5d4c\u5165\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5728\u5404\u79cd\u56fe\u50cf\u635f\u4f24\u573a\u666f\u4e0b\u7684\u6c34\u5370\u63d0\u53d6\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u65b9\u6cd5\u5728\u4fdd\u8bc1\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u9762\u5bf9\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u7684\u56fe\u50cf\u635f\u4f24\uff08\u5982\u566a\u58f0\u3001\u538b\u7f29\u7b49\uff09\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u6709\u9650\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6c34\u5370\u5728\u53d7\u635f\u56fe\u50cf\u4e2d\u7684\u53ef\u6062\u590d\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u7a7a\u6587\u672c\u4f18\u5316\u8fc7\u7a0b\u5c06\u539f\u59cb\u56fe\u50cf\u7f16\u7801\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u53cd\u6f14\u566a\u58f0\uff0c\u7136\u540e\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u4f18\u5316\u53cd\u6f14\u566a\u58f0\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6c34\u5370\u56fe\u50cf\u3002\u4e3a\u9632\u6b62\u53cd\u6f14\u566a\u58f0\u4f18\u5316\u5bfc\u81f4\u539f\u6709\u8bed\u4e49\u5931\u771f\uff0c\u5f15\u5165\u4e86\u81ea\u6ce8\u610f\u529b\u7ea6\u675f\u548c\u4f2a\u63a9\u7801\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728COCO\u6570\u636e\u96c6\u4e0a\u5e94\u5bf912\u79cd\u4e0d\u540c\u56fe\u50cf\u53d8\u6362\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u7a33\u5b9a\u7b7e\u540d\u65b9\u6cd510%\uff0c\u5728\u591a\u79cd\u56fe\u50cf\u635f\u4f24\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4e86\u66f4\u597d\u7684\u7a33\u5065\u6027\u548c\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6c34\u5370\u65b9\u6cd5\u65e2\u4fdd\u8bc1\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u53c8\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u66f4\u4e3a\u53ef\u9760\u548c\u5b9e\u7528\u7684\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2601.17842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17842", "abs": "https://arxiv.org/abs/2601.17842", "authors": ["Lanqing Du", "Yunong Li", "YuJie Long", "Shihong Chen"], "title": "EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy", "comment": null, "summary": "Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a \"top-down\" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a \"bottom-up\" trajectory, it deconstructs the intervention into a three-stage reasoning flow: \"Embodied Perception - Cognitive Exploration - Narrative Intervention.\" Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed \"EFT-Instruct,\" a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u7eea\u805a\u7126\u7597\u6cd5\uff08EFT\uff09\u7684\u591a\u667a\u80fd\u4f53\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5927\u6a21\u578b\u6846\u67b6EFT-CoT\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u540c\u7406\u5fc3\u3001\u66f4\u7ed3\u6784\u5316\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u7b54\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u4f53\u9a8c\u611f\u77e5-\u8ba4\u77e5\u63a2\u7d22-\u53d9\u4e8b\u5e72\u9884\u201d\u4e09\u9636\u6bb5\u63a8\u7406\uff0c\u7ed3\u5408\u516b\u4e2a\u529f\u80fd\u667a\u80fd\u4f53\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5fc3\u7406\u5e72\u9884\u7684\u540c\u7406\u6027\u548c\u4e13\u4e1a\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b0\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCBT\u65b9\u6cd5\u53ca\u4eba\u7c7b\u7b54\u590d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\uff08CBT\uff09\u7684\u5fc3\u7406\u5065\u5eb7\u5927\u6a21\u578b\u591a\u91c7\u7528\u201c\u81ea\u4e0a\u800c\u4e0b\u201d\u7684\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u8eab\u4f53\u7ecf\u9a8c\u548c\u539f\u59cb\u60c5\u7eea\u5904\u7406\uff0c\u5bfc\u81f4\u5e72\u9884\u6548\u679c\u6709\u9650\u3002\u4e3a\u66f4\u597d\u6a21\u62df\u4eba\u4e0e\u4eba\u4e4b\u95f4\u6709\u6548\u5171\u60c5\u548c\u5fc3\u7406\u758f\u5bfc\uff0c\u9700\u8981\u5f15\u5165\u4ee5\u60c5\u7eea\u4e3a\u6838\u5fc3\u3001\u66f4\u5177\u4eba\u6027\u5316\u7684\u5e72\u9884\u673a\u5236\u3002", "method": "\u63d0\u51faEFT-CoT\u6846\u67b6\uff0c\u91c7\u7528\u201c\u81ea\u4e0b\u800c\u4e0a\u201d\u8def\u5f84\uff0c\u5c06\u5e72\u9884\u8fc7\u7a0b\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a\u4f53\u9a8c\u611f\u77e5\u3001\u8ba4\u77e5\u63a2\u7d22\u3001\u53d9\u4e8b\u5e72\u9884\u3002\u7cfb\u7edf\u5f15\u5165\u516b\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff0c\u5206\u522b\u6267\u884c\u4f53\u611f\u89c9\u5bdf\u3001\u9002\u5e94\u6027\u8bc4\u4f30\u3001\u6838\u5fc3\u4fe1\u5ff5\u63d0\u53d6\u548c\u53d9\u4e8b\u91cd\u6784\u7b49\u5173\u952e\u7597\u6cd5\u6b65\u9aa4\u3002\u6b64\u5916\uff0c\u6784\u5efa\u9ad8\u8d28\u91cfEFT-Instruct\u6570\u636e\u96c6\u5e76\u84b8\u998f\u5fae\u8c03\u4e13\u7528\u5927\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEFT-LLM\u5728\u540c\u7406\u5fc3\u6df1\u5ea6\u3001\u7ed3\u6784\u4e13\u4e1a\u6027\u7b49\u8bc4\u4f30\u6307\u6807\u4e0a\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u53ca\u4eba\u7c7b\u56de\u7b54\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u591a\u667a\u80fd\u4f53\u673a\u5236\u5bf9\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "EFT-CoT\u6a21\u578b\u4e3a\u53ef\u89e3\u91ca\u3001\u9ad8\u540c\u7406\u5ea6\u7684\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u5176\u591a\u667a\u80fd\u4f53\u5e95\u5c42\u63a8\u7406\u65b9\u5f0f\u63d0\u5347\u4e86\u5fc3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u5f25\u8865\u4e86\u4f20\u7edfCBT\u5927\u6a21\u578b\u7684\u5c40\u9650\u3002"}}
{"id": "2601.17391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17391", "abs": "https://arxiv.org/abs/2601.17391", "authors": ["Rui Fan", "Weidong Hao"], "title": "SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition", "comment": null, "summary": "Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\uff0c\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u5bf9\u8c61\u8bc6\u522b\u65b9\u6cd5\u5728\u65f6\u7a7a\u591a\u89c6\u89d2\u8868\u793a\u4e0a\u5b58\u5728\u7a7a\u95f4\u5e73\u79fb\u4e0d\u53d8\u6027\u5dee\u3001\u7279\u5f81\u878d\u5408\u65b9\u5f0f\u7b80\u5355\u7684\u5c40\u9650\uff0c\u4e0d\u5229\u4e8e\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u4ee5\u5e73\u79fb\u4e0d\u53d8\u7684\u7a20\u5bc6\u65b9\u5f0f\u5bf9\u7a00\u758f\u4e8b\u4ef6\u6d41\u8fdb\u884c\u7a7a\u95f4-\u65f6\u95f4\u591a\u89c6\u89d2\u8868\u793a\uff1b2\uff09\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u52a8\u6001\u878d\u5408\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e0d\u540c\u89c6\u89d2\u8fd0\u52a8\u7279\u5f81\u6309\u6837\u672c\u81ea\u9002\u5e94\u7684\u4e92\u8865\u878d\u5408\uff1b3\uff09\u63d0\u51fa\u53d7\u751f\u7269\u542f\u53d1\u7684\u65f6\u95f4\u626d\u66f2\u589e\u5f3a\u65b9\u6cd5\uff0c\u6a21\u62df\u73b0\u5b9e\u52a8\u4f5c\u901f\u5ea6\u53d8\u5316\uff0c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728HARDVS\u3001DailyDVS-200 \u548c THU-EACT-50-CHL \u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cTop-1\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e867.0%\u300110.7%\u300110.2%\uff1b\u53c2\u6570\u91cf\u51cf\u5c1130.1%\u3001\u8ba1\u7b97\u91cf\u51cf\u5c1135.7%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709SMVRL-EOR\u65b9\u6cd5\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u529b\u8303\u5f0f\u3002"}}
{"id": "2601.17865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17865", "abs": "https://arxiv.org/abs/2601.17865", "authors": ["Jia Gu", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models", "comment": "12 pages, 10 figures. Accepted by WWW'26", "summary": "The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\u7684\u6982\u7387\uff08P_token\uff09\u4e0e\u4efb\u52a1\u5c42\u9762\u76ee\u6807\u5206\u5e03\uff08P_task\uff09\u7684\u4e00\u81f4\u6027\uff0c\u533a\u5206\u4e86\u4e24\u7c7b\u6a21\u578b\uff08D-models\u548cE-models\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u5176\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1LLM\u80fd\u6a21\u62df\u771f\u5b9e\u5206\u5e03\uff0c\u4f46\u5176\u6bcf\u4e00\u6b65\u91c7\u6837\u6982\u7387\u662f\u5426\u4e0e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\u4e00\u81f4\uff0c\u4ecd\u672a\u660e\u6670\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8LLM\u8f93\u51fa\u6982\u7387\u4e0e\u5b9e\u9645\u4efb\u52a1\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u6027\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u53d7\u63a7\u5206\u5e03\u91c7\u6837\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4e0d\u540c\u7c7b\u578bLLM\u9884\u6d4btoken\u7684\u6982\u7387\u6ce2\u52a8\u4e0e\u4efb\u52a1\u6240\u9700\u5206\u5e03\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u8fdb\u4e00\u6b65\u5728\u4ee3\u7801\u751f\u6210\u3001\u63a8\u8350\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5bf9\u6bd4\u4e24\u7c7b\u6a21\u578b\u7684\u6548\u679c\uff1b\u8fd8\u5206\u6790\u4e86\u4e24\u79cd\u6a21\u578b\u5bb6\u65cf\u7684\u5185\u90e8\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cD-model\uff08\u5982Qwen-2.5\uff09\u7684\u91c7\u6837\u6982\u7387\u6ce2\u52a8\u5927\uff0c\u4e0e\u4efb\u52a1\u76ee\u6807\u5206\u5e03\u5951\u5408\u8f83\u5dee\uff1bE-model\uff08\u5982Mistral-Small\uff09\u7684\u91c7\u6837\u6982\u7387\u66f4\u7a33\u5b9a\uff0c\u4e0e\u4efb\u52a1\u76ee\u6807\u5206\u5e03\u66f4\u4e00\u81f4\u3002\u4e0b\u6e38\u4efb\u52a1\u9a8c\u8bc1\u4e86D/E\u6a21\u578b\u5728\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5f71\u54cd\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "D/E\u4e24\u7c7bLLM\u5728\u91c7\u6837\u884c\u4e3a\u6709\u672c\u8d28\u5dee\u5f02\uff0c\u9488\u5bf9\u63a8\u8350\u3001\u641c\u7d22\u3001\u5bf9\u8bdd\u7b49\u573a\u666f\uff0c\u5e94\u7ed3\u5408\u5b9e\u9645\u9700\u6c42\uff08\u591a\u6837\u6027vs.\u53ef\u9760\u6027\uff09\u9009\u7528\u5408\u9002\u6a21\u578b\u3002\u8fd9\u4e3a\u6a21\u578b\u9009\u62e9\u4e0e\u914d\u7f6e\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u4e5f\u6709\u52a9\u7406\u89e3LLM\u7684\u672c\u8d28\u91c7\u6837\u673a\u5236\u3002"}}
{"id": "2601.17399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17399", "abs": "https://arxiv.org/abs/2601.17399", "authors": ["Rui Fang", "Jian Li", "Wei Chen", "Bin Hu", "Ying-Cong Chen", "Xin Tang", "Liang Diao"], "title": "ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\\% compared to full-pass evaluations while maintaining a ranking correlation of $\u03c1=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReLE\u7684\u65b0\u578b\u8bc4\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u8bca\u65ad\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u548c\u80fd\u529b\u7ef4\u5ea6\u4e0a\u7684\u975e\u5747\u5300\u6027\u80fd\u3002\u5229\u7528ReLE\uff0c\u4f5c\u8005\u5bf9304\u4e2a\u4e3b\u6d41\u4e2d\u6587\u5927\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u6d4b\u8bc4\uff0c\u5e76\u5b9e\u73b0\u4e86\u8bc4\u6d4b\u7cbe\u51c6\u5ea6\u548c\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u5927\u6a21\u578b\u8bc4\u6d4b\u9762\u4e34\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u3001\u8ba1\u7b97\u6210\u672c\u6602\u8d35\u7b49\u96be\u9898\uff0c\u4e14\u9759\u6001\u6392\u884c\u699c\u5f80\u5f80\u63a9\u76d6\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df/\u80fd\u529b\u7684\u7ed3\u6784\u6027\u5dee\u5f02\u3002\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u66f4\u4e3a\u9ad8\u6548\u548c\u7ec6\u81f4\u5730\u63ed\u793a\u6a21\u578b\u8868\u73b0\u7684\u771f\u5b9e\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86ReLE\u8bc4\u6d4b\u7cfb\u7edf\uff0c\u57fa\u4e8e\u9886\u57df\u00d7\u80fd\u529b\u7684\u6b63\u4ea4\u6d4b\u8bc4\u77e9\u9635\uff0c\u914d\u5408\u4e24\u9879\u521b\u65b0\uff1a\uff081\uff09\u7b26\u53f7-\u8bed\u4e49\u6df7\u5408\u8bc4\u5206\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5d4c\u5165\u8bef\u5224\uff1b\uff082\uff09\u57fa\u4e8eNeyman\u5206\u914d\u548c\u566a\u58f0\u6821\u6b63\u7684\u52a8\u6001\u65b9\u5dee\u8c03\u5ea6\u5668\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6d88\u8017\u3002\u8be5\u7cfb\u7edf\u6d4b\u8bc4\u4e86304\u4e2a\u6a21\u578b\uff08\u5546\u7528\u548c\u5f00\u6e90\uff09\u3001\u5bf9207,843\u4e2a\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u7684\u5168\u91cf\u8bc4\u6d4b\uff0cReLE\u53ef\u8282\u7701\u7ea670%\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u53ef\u7ef4\u6301\u6781\u9ad8\u6392\u884c\u699c\u76f8\u5173\u6027\uff080.96\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\u5927\u6a21\u578b\u5728\u9886\u57df/\u80fd\u529b\u95f4\u7684\u6392\u540d\u6ce2\u52a8\u66f4\u5927\uff08RSA=11.4\uff09\uff0c\u8fdc\u9ad8\u4e8e\u4f20\u7edf\u57fa\u51c6\uff08RSA\u22485.0\uff09\uff0c\u663e\u793a\u4e3b\u6d41\u6a21\u578b\u5b9e\u9645\u4e0a\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4e13\u4e1a\u5316\u800c\u975e\u5168\u80fd\u6027\u3002", "conclusion": "ReLE\u53ef\u4f5c\u4e3a\u6a21\u578b\u751f\u6001\u52a8\u6001\u76d1\u63a7\u7684\u5de5\u5177\uff0c\u7cbe\u51c6\u66b4\u9732\u6a21\u578b\u5728\u80fd\u529b\u548c\u9886\u57df\u4e0a\u7684\u7ed3\u6784\u6027\u77ed\u677f\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u548c\u9009\u578b\u63d0\u4f9b\u4f9d\u636e\uff0c\u4f46\u5e76\u975e\u7528\u4e8e\u66ff\u4ee3\u5168\u9762\u9759\u6001\u57fa\u51c6\u3002"}}
{"id": "2601.17869", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17869", "abs": "https://arxiv.org/abs/2601.17869", "authors": ["Michelle Chao Chen", "Moritz Miller", "Bernhard Sch\u00f6lkopf", "Siyuan Guo"], "title": "On the Emergence and Test-Time Use of Structural Information in Large Language Models", "comment": null, "summary": "Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u62bd\u8c61\u7ed3\u6784\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u606f\u3002\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u9a8c\u6027\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u7ed3\u6784\u5b66\u4e60\u4e0e\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u6307\u51fa\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u751f\u6210\u65b0\u7ec4\u5408\u5185\u5bb9\u7684\u80fd\u529b\u4ecd\u6709\u9650\u3002", "motivation": "\u5b66\u4e60\u7ed3\u6784\u5316\u4fe1\u606f\u5bf9\u4e8e\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u673a\u5236\u7406\u89e3\u4ee5\u53ca\u5728\u6d4b\u8bd5\u65f6\u751f\u6210\u7075\u6d3b\u7ec4\u5408\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bed\u8a00\u6a21\u578b\u662f\u5982\u4f55\u5b66\u4e60\u548c\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u606f\u7684\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793a\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u80fd\u529b\u4e0e\u63a8\u7406\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u751f\u6210\u65b0\u5185\u5bb9\u65f6\u7684\u9650\u5236\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u57fa\u4e8e\u8bed\u8a00\u7ed3\u6784\u53d8\u5316\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u5bf9\u7ed3\u6784\u4fe1\u606f\u7684\u5b66\u4e60\u8fc7\u7a0b\u53ca\u5176\u5728\u6d4b\u8bd5\u65f6\u7684\u5e94\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u7ed3\u6784\u4fe1\u606f\u7684\u80fd\u529b\u4e0e\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5448\u73b0\u76f8\u5173\u6027\u3002\u4f46\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5229\u7528\u7ed3\u6784\u4fe1\u606f\u8fdb\u884c\u65b0\u7ec4\u5408\u751f\u6210\u7684\u80fd\u529b\u4f9d\u7136\u6709\u9650\u3002", "conclusion": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b66\u4e60\u62bd\u8c61\u7ed3\u6784\uff0c\u5e76\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u76f8\u5e94\u80fd\u529b\uff0c\u4f46\u5176\u5728\u7075\u6d3b\u65b0\u9896\u751f\u6210\u65b9\u9762\u5c1a\u6709\u660e\u663e\u4e0d\u8db3\u3002"}}
{"id": "2601.17405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17405", "abs": "https://arxiv.org/abs/2601.17405", "authors": ["Chunze Yang", "Wenjie Zhao", "Yue Tang", "Junbo Lu", "Jiusong Ge", "Qidong Liu", "Zeyu Gao", "Chen Li"], "title": "HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection", "comment": null, "summary": "Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6HAAF\uff0c\u89e3\u51b3\u4e86\u7cbe\u7ec6\u75c5\u7406\u8bca\u65ad\u4e2d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7c92\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u5c42\u5bf9\u9f50\u63d0\u5347ROI\u7cbe\u7ec6\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u75c5\u7406\u8bca\u65ad\u4f9d\u8d56\u4e8e\u5c40\u90e8\u3001\u7ec6\u7c92\u5ea6\u7684\u5f62\u6001\u5b66\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u7ec6\u5fae\u5f02\u5e38\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5f88\u597d\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4e0e\u5177\u4f53\u56fe\u50cfROI\u533a\u57df\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u9002\u914d\u4e0e\u5bf9\u9f50\u6846\u67b6\uff08HAAF\uff09\uff0c\u5305\u542b\u4e00\u79cd\u65b0\u578b\u8de8\u5c42\u7ea7\u7f29\u653e\u5bf9\u9f50\u673a\u5236\uff08CLSA\uff09\uff1a\u9996\u5148\u8ba9\u89c6\u89c9\u7279\u5f81\u4e3a\u6587\u672c\u63d0\u793a\u6ce8\u5165\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u518d\u901a\u8fc7\u81ea\u9002\u5e94\u6587\u672c\u53cd\u5411\u6307\u5bfc\u89c6\u89c9\u7f16\u7801\u5668\u5173\u6ce8\u5173\u952e\u5f02\u5e38\u533a\u57df\uff1b\u5e76\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u63a8\u7406\u7b56\u7565\uff0c\u7ed3\u5408\u8bed\u4e49\u5f97\u5206\u4e0e\u51e0\u4f55\u539f\u578b\uff0c\u589e\u5f3a\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u6d4b\u8bd5\uff0cHAAF\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u91c7\u7528\u9886\u57df\u7279\u5b9a\u9aa8\u5e72\u7f51\u7edc\uff08\u5982CONCH\uff09\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "HAAF\u80fd\u591f\u6709\u6548\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u75c5\u7406\u56fe\u50cf\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cbe\u7ec6\u533a\u57df\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u7cbe\u5bc6\u75c5\u7406\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.17879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17879", "abs": "https://arxiv.org/abs/2601.17879", "authors": ["Yilong Xu", "Zhi Zheng", "Xiang Long", "Yujun Cai", "Yiwei Wang"], "title": "Self-Manager: Parallel Agent Loop for Long-form Deep Research", "comment": null, "summary": "Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.", "AI": {"tldr": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5728\u5904\u7406\u590d\u6742\u3001\u957f\u5468\u671f\u4efb\u52a1\u65f6\uff0c\u53d7\u9650\u4e8e\u5355\u4e00\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0e\u987a\u5e8f\u6267\u884c\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51faSelf-Manager\u6846\u67b6\uff0c\u5b9e\u73b0\u5e76\u884c\u3001\u5f02\u6b65\u591a\u7ebf\u7a0b\u667a\u80fd\u4f53\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5468\u671f\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u5728\u6267\u884c\u957f\u5468\u671f\u3001\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u53d7\u9650\u4e8e\u7ebf\u6027\u4e0a\u4e0b\u6587\u79ef\u7d2f\u548c\u4fe1\u606f\u635f\u5931\uff0c\u4e14\u5355\u4e00\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u987a\u5e8f\u6267\u884c\u4f1a\u53d1\u751f\u4e92\u76f8\u5e72\u6270\u548c\u963b\u585e\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u7075\u6d3b\u9002\u5e94\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u63d0\u51faSelf-Manager\u67b6\u6784\uff0c\u5141\u8bb8\u4e3b\u7ebf\u7a0b\u521b\u5efa\u591a\u4e2a\u62e5\u6709\u72ec\u7acb\u4e0a\u4e0b\u6587\u7684\u5b50\u7ebf\u7a0b\uff0c\u901a\u8fc7\u7ebf\u7a0b\u63a7\u5236\u5757(TCB)\u8fdb\u884c\u8fed\u4ee3\u7ba1\u7406\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u7684\u5e76\u884c\u3001\u5f02\u6b65\u4e0e\u7075\u6d3b\u8c03\u5ea6\u3002", "result": "\u5728DeepResearch Bench\u57fa\u51c6\u4e0a\uff0cSelf-Manager\u5728\u6240\u6709\u8bc4\u4ef7\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5355\u667a\u80fd\u4f53\u987a\u5e8f\u7ba1\u7406\u65b9\u6cd5\uff1b\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u548c\u9488\u5bf9\u4e0a\u4e0b\u6587\u5bb9\u91cf\u3001\u6548\u7387\u53ca\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "Self-Manager\u65b9\u6cd5\u6709\u6548\u7834\u89e3\u4e86\u957f\u5468\u671f\u590d\u6742\u4efb\u52a1\u7ba1\u7406\u4e2d\u7684\u4f38\u7f29\u6027\u4e0e\u4e0a\u4e0b\u6587\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u5bf9\u591a\u667a\u80fd\u4f53\u7ba1\u7406\u9886\u57df\u5177\u6709\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2601.17408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17408", "abs": "https://arxiv.org/abs/2601.17408", "authors": ["Harsharaj Pathak", "Vineeth N Balasubramanian"], "title": "Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u90bb\u57df\u7b7e\u540d\u201d\u7684\u6e90\u6570\u636e\u4e0d\u53ef\u7528\u57df\u81ea\u9002\u5e94\uff08SFDA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u66f4\u5177\u4fe1\u606f\u6027\u7684\u805a\u7c7b\u548c\u51cf\u5c11\u566a\u58f0\u90bb\u57df\u5f71\u54cd\uff0c\u4ec5\u7528\u4e00\u4e2a\u5b9a\u5236\u635f\u5931\u51fd\u6570\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u9886\u57df\u9002\u5e94\uff0c\u7279\u522b\u662f\u5728VisDA\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dSFDA\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u90bb\u57df\u4e00\u81f4\u6027\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u9519\u8bef\u90bb\u57df\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u9002\u5e94\u6548\u679c\u4e0d\u4f73\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u51cf\u8f7b\u566a\u58f0\u90bb\u57df\u7684\u5e72\u6270\uff0c\u63d0\u9ad8\u805a\u7c7b\u4ee3\u8868\u6027\uff0c\u4ee5\u63d0\u5347\u65e0\u6e90\u9886\u57df\u81ea\u9002\u5e94\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u201c\u90bb\u57df\u7b7e\u540d\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u901a\u8fc7\u5206\u6790\u90bb\u57df\u4fe1\u606f\u5b66\u4e60\u5230\u66f4\u7cbe\u51c6\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u76ee\u6807\u57df\u805a\u7c7b\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4f18\u5316\u6837\u672c\u9884\u6d4b\u76f8\u4f3c\u5ea6\u548c\u5dee\u5f02\u6027\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u9886\u57df\u81ea\u9002\u5e94\u3002\u6574\u4e2a\u65b9\u6cd5\u5728\u65e0\u9700\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f9d\u8d56\u76ee\u6807\u57df\u6570\u636e\u81ea\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9886\u57df\u81ea\u9002\u5e94\u7ecf\u5178\u7684VisDA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5728\u5176\u4ed6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e5f\u8fbe\u5230\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8868\u660e\u65b9\u6cd5\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u5229\u7528\u90bb\u57df\u7b7e\u540d\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u3001\u7b80\u5316\u635f\u5931\u8bbe\u8ba1\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6e90\u4e0d\u53ef\u7528\u6761\u4ef6\u4e0b\u7684\u9886\u57df\u81ea\u9002\u5e94\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17898", "abs": "https://arxiv.org/abs/2601.17898", "authors": ["Qi Zhan", "Yile Wang", "Hui Huang"], "title": "Assessment of Generative Named Entity Recognition in the Era of Large Language Models", "comment": null, "summary": "Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\uff08NER\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u548c\u7ed3\u6784\u5316\u8f93\u51fa\uff0cLLMs\u5728NER\u4e0a\u5df2\u5177\u5907\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5ab2\u7f8e\u6216\u66f4\u4f18\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cNER\u4efb\u52a1\u9010\u6b65\u4ece\u5e8f\u5217\u6807\u6ce8\u8f6c\u5411\u751f\u6210\u8303\u5f0f\u3002\u6587\u7ae0\u610f\u5728\u63a2\u8ba8\u5e76\u7cfb\u7edf\u6027\u5206\u6790\u65b0\u8303\u5f0f\u4e0b\uff0c\u5f00\u6e90LLMs\u5728\u5404\u7c7bNER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3001\u5c40\u9650\u6027\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u516b\u79cd\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90LLM\uff0c\u5728\u56db\u4e2a\u4e3b\u6d41NER\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4e0e\u751f\u6210\u5f0fNER\u6a21\u578b\u8868\u73b0\uff0c\u8003\u5bdf\u8f93\u51fa\u683c\u5f0f\u3001\u6a21\u578b\u8bb0\u5fc6\u6027\u3001\u5fae\u8c03\u5bf9\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\uff081\uff09\u914d\u5408\u9ad8\u6548\u5fae\u8c03\u548c\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u5f00\u6e90LLMs\u5728NER\u4efb\u52a1\u4e0a\u53ef\u4e0e\u4f20\u7edf\u6a21\u578b\u7ade\u4e89\uff0c\u751a\u81f3\u4f18\u4e8e\u5982GPT-3\u7b49\u95ed\u6e90\u6a21\u578b\uff1b\uff082\uff09LLMs\u7684NER\u80fd\u529b\u6765\u81ea\u4e8e\u5176\u6307\u4ee4\u9075\u5faa\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u800c\u975e\u8bb0\u5fc6\uff1b\uff083\uff09\u8fdb\u884cNER\u6307\u4ee4\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u901a\u7528\u80fd\u529b\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\uff0c\u751a\u81f3\u5728\u5982DROP\u7b49\u6570\u636e\u96c6\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u751f\u6210\u5f0fNER\u65b9\u6cd5\u914d\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5df2\u6210\u4e3a\u6027\u80fd\u4f18\u5f02\u4e14\u7528\u6237\u53cb\u597d\u7684\u9009\u62e9\uff0c\u6709\u671b\u6210\u4e3aNER\u9886\u57df\u4e2d\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u8865\u5145\uff0c\u751a\u81f3\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.17414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17414", "abs": "https://arxiv.org/abs/2601.17414", "authors": ["Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase", "comment": null, "summary": "The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \\$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFirebase\u4e91\u5e73\u53f0\u7684\u7269\u8054\u7f51\u8fdc\u7a0b\u76d1\u63a7\u4e0e\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7ESP32\u5fae\u63a7\u5236\u5668\u548c\u591a\u79cd\u4f20\u611f\u5668\u5b9e\u73b0\u73af\u5883\u6570\u636e\u7684\u5b9e\u65f6\u91c7\u96c6\u4e0e\u8bbe\u5907\u64cd\u63a7\uff0c\u5177\u6709\u9ad8\u53ef\u9760\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u4f4e\u7b49\u4f18\u70b9\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u63a7\u7cfb\u7edf\u5728\u5b9e\u65f6\u6570\u636e\u83b7\u53d6\u3001\u8fdc\u7a0b\u63a7\u5236\u548c\u4e91\u5e73\u53f0\u96c6\u6210\u7b49\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u7269\u8054\u7f51\u5e94\u7528\u5bf9\u4e8e\u591a\u7ec8\u7aef\u3001\u8de8\u5730\u57df\u540c\u6b65\u548c\u6570\u636e\u6301\u4e45\u5316\u7684\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u57fa\u4e8eESP32\u5f00\u53d1\u677f\uff0c\u63a5\u5165\u4e86DHT22\u6e29\u6e7f\u5ea6\u4f20\u611f\u5668\u548cHC-SR04\u8d85\u58f0\u6ce2\u8ddd\u79bb\u4f20\u611f\u5668\uff0c\u4f7f\u7528Firebase Realtime Database\u5b9e\u73b0\u6570\u636e\u540c\u6b65\u548c\u591a\u7ec8\u7aef\u8bbf\u95ee\uff0c\u5e76\u901a\u8fc7\u4e91\u7aef\u754c\u9762\u8fdc\u7a0b\u63a7\u5236\u4e24\u7ec4LED\u6307\u793a\u706f\u3002\u5b9e\u9a8c\u5bf9\u6570\u636e\u4f20\u8f93\u53ef\u9760\u6027\u3001\u63a7\u5236\u5ef6\u8fdf\u548c\u6570\u636e\u6301\u4e45\u5316\u7b49\u6307\u6807\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6570\u636e\u4f20\u8f93\u6210\u529f\u7387\u9ad8\u8fbe99.2%\uff0c\u8fdc\u7a0b\u63a7\u5236\u65f6\u5ef6\u4e0d\u8d85\u8fc71.5\u79d2\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5386\u53f2\u6570\u636e\u7684\u6301\u4e45\u5b58\u50a8\u3002\u7cfb\u7edf\u603b\u6210\u672c\u4e3a32.5\u7f8e\u5143\uff0c\u8fd0\u884c\u7a33\u5b9a\u53ef\u9760\u3002", "conclusion": "\u57fa\u4e8eFirebase\u7684\u4e91\u7aef\u7269\u8054\u7f51\u7cfb\u7edf\u4e3a\u5f00\u53d1\u8005\u548c\u5b66\u8005\u63d0\u4f9b\u4e86\u65e0\u9700\u590d\u6742\u670d\u52a1\u5668\u914d\u7f6e\u3001\u5177\u5907\u4e91\u7aef\u80fd\u529b\u3001\u6210\u672c\u4f4e\u5ec9\u7684\u5e94\u7528\u6846\u67b6\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u5bb6\u5c45\u548c\u5de5\u4e1a\u76d1\u63a7\u7b49\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2601.17921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17921", "abs": "https://arxiv.org/abs/2601.17921", "authors": ["Yi Zhao", "Qinghua Yao", "Xinyuan song", "Wei Zhu"], "title": "ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation", "comment": "accepted by CPAL", "summary": "Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u53ef\u89e3\u91ca\u7684LoRA\u79e9\u5206\u914d\u65b9\u6cd5ShapLoRA\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfLoRA\u65b9\u6cd5\u4f7f\u7528\u7edf\u4e00\u79e9\uff0c\u540e\u7eed\u5de5\u4f5c\u5c1d\u8bd5\u6539\u8fdb\u79e9\u5206\u914d\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u79e9\u91cd\u8981\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u53ef\u89e3\u91ca\u4e14\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u79e9\u5206\u914d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86ShapLoRA\u6846\u67b6\uff0c\u5c06\u89e3\u91ca\u6027Shapley\u503c\u7684\u5f52\u56e0\u601d\u60f3\u4e0e\u654f\u611f\u5ea6\u5206\u6790\u7ed3\u5408\uff0c\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u91cd\u8981\u6027\u5ea6\u91cfShapley\u654f\u611f\u5ea6\uff0c\u5728\u5355\u72ec\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5e76\u91c7\u7528\u5206\u914d-\u518d\u8bad\u7ec3\u6d41\u7a0b\u786e\u4fdd\u516c\u5e73\u6bd4\u8f83\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aShapLoRA\u5728\u53c2\u6570\u89c4\u6a21\u53ef\u6bd4\u7684\u60c5\u51b5\u4e0b\uff0c\u6548\u679c\u4f18\u4e8e\u6700\u65b0\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ShapLoRA\u80fd\u66f4\u597d\u5730\u5206\u914dLoRA\u79e9\uff0c\u63d0\u5347PEFT\u6027\u80fd\u5e76\u5177\u5907\u826f\u597d\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.17420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17420", "abs": "https://arxiv.org/abs/2601.17420", "authors": ["Shiu-hong Kao", "Chak Ho Huang", "Huaiqian Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction", "comment": "Project page: https://danielshkao.github.io/cot-seg.html", "summary": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoT-Seg\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u63a8\u7406\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u94fe\u5f0f\u601d\u7ef4\u548c\u81ea\u6211\u4fee\u6b63\u673a\u5236\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e94\u5bf9\u590d\u6742\u548c\u6a21\u7cca\u5206\u5272\u95ee\u9898\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u672a\u77e5\u9886\u57df\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u548c\u81ea\u6211\u4fee\u6b63\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u4e00\u79cd\u66f4\u7c7b\u4eba\u4e14\u9ad8\u9c81\u68d2\u6027\u7684\u5206\u5272\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86CoT-Seg\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982GPT-4o\uff09\u8fdb\u884c\u94fe\u5f0f\u63a8\u7406\uff0c\u5c06\u590d\u6742\u67e5\u8be2\u62c6\u89e3\u4e3a\u5b50\u6307\u4ee4\uff0c\u5e76\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u5f15\u5165\u81ea\u6211\u4fee\u6b63\u6a21\u5757\uff0c\u6a21\u578b\u53ef\u6839\u636e\u63a8\u7406\u8f68\u8ff9\u548c\u67e5\u8be2\u81ea\u8bc4\u7ed3\u679c\u5e76\u53cd\u590d\u4f18\u5316\u5206\u5272\u63a9\u7801\uff0c\u540c\u65f6\u53ef\u96c6\u6210\u5916\u90e8\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6a21\u5757\u3002", "result": "\u5728\u65b0\u5f15\u5165\u7684\u9ad8\u96be\u5ea6\u5206\u5272\u6570\u636e\u96c6ReasonSeg-Hard\u4ee5\u53ca\u590d\u6742\u573a\u666f\u6d4b\u8bd5\u4e2d\uff0cCoT-Seg\u5c55\u73b0\u51fa\u5728\u5904\u7406\u6a21\u7cca\u548c\u5177\u6311\u6218\u6027\u4efb\u52a1\u65f6\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u94fe\u5f0f\u601d\u7ef4\u4e0e\u81ea\u6211\u4fee\u6b63\u7684\u7ed3\u5408\u4e3a\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8303\u5f0f\uff0cCoT-Seg\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5927\u5e45\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u5206\u5272\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.17952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17952", "abs": "https://arxiv.org/abs/2601.17952", "authors": ["Michail Mamalakis", "Tiago Azevedo", "Cristian Cosentino", "Chiara D'Ercoli", "Subati Abulikemu", "Zhongtian Sun", "Richard Bethlehem", "Pietro Lio"], "title": "A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models", "comment": null, "summary": "Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u5f81\u5f52\u56e0\u548c\u673a\u5236\u89e3\u91ca\u7684\u7edf\u4e00\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u5728LLM\u5c42\u7ea7\u6784\u5efa\u5355\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\uff0c\u51cf\u5c11\u5f52\u56e0\u65b9\u6cd5\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u7b49\u4e34\u5e8a\u8bbe\u7f6e\u4e2dLLM\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u7a33\u5b9a\u53ef\u9760\u7684\u8f93\u5165\u91cd\u8981\u6027\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u7684LLM\u53ef\u89e3\u91ca\u65b9\u6cd5\u5728\u4e34\u5e8a\u8bca\u65ad\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u8fdb\u5c55\u9884\u6d4b\uff09\u5e94\u7528\u4e0a\u9762\u4e34\u89e3\u91ca\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u56e0\u5f52\u56e0\u65b9\u6cd5\u5b58\u5728\u9ad8\u5ea6\u53ef\u53d8\u6027\uff0c\u673a\u5236\u89e3\u91ca\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u548c\u8f93\u5165/\u8f93\u51fa\u7684\u76f4\u63a5\u5bf9\u9f50\u4e14\u96be\u4ee5\u7ed9\u51fa\u663e\u5f0f\u91cd\u8981\u6027\u5206\u6570\u3002\u4e3a\u63d0\u5347\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u9700\u8981\u65b0\u7684\u7edf\u4e00\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u5f52\u56e0\uff08attributional\uff09\u548c\u673a\u5236\uff08mechanistic\uff09\u89e3\u91ca\u65b9\u6cd5\u901a\u8fc7\u5355\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\uff08monosemantic feature extraction\uff09\u7ed3\u5408\u8d77\u6765\u3002\u5728LLM\u67d0\u4e00\u5c42\u7ea7\u5efa\u7acb\u4e00\u4e2a\u5355\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u5bf9\u6846\u67b6\u8fdb\u884c\u4f18\u5316\uff0c\u660e\u786e\u51cf\u5c11\u4e0d\u540c\u5f52\u56e0\u65b9\u6cd5\u95f4\u7684\u89e3\u91ca\u5dee\u5f02\uff0c\u6700\u7ec8\u83b7\u5f97\u7a33\u5b9a\u7684\u8f93\u5165\u7ea7\u522b\u91cd\u8981\u6027\u5206\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8f93\u51fa\u7a33\u5b9a\u7684\u3001\u8f93\u5165\u5c42\u7ea7\u7684\u663e\u8457\u7279\u5f81\u5f52\u56e0\u5206\u6570\uff0c\u901a\u8fc7\u5206\u5c42\u89e3\u538b\u7684\u65b9\u5f0f\u6e05\u6670\u5730\u63ed\u793a\u6a21\u578b\u5728\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u7b49\u8ba4\u77e5\u969c\u788d\u65f6\u5173\u6ce8\u7684\u5173\u952e\u8f93\u5165\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u8be5\u7edf\u4e00\u89e3\u91ca\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u8ba4\u77e5\u5065\u5eb7\u548c\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bca\u65ad\u9886\u57df\u5e94\u7528\u7684\u53ef\u89e3\u91ca\u6027\u3001\u5b89\u5168\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2601.17429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17429", "abs": "https://arxiv.org/abs/2601.17429", "authors": ["Mehdi Yousefzadeh", "Siavash Shirzadeh Barough", "Ashkan Fakharifar", "Yashar Tayyarazad", "Narges Eghbali", "Mohaddeseh Mozaffari", "Hoda Taeb", "Negar Sadat Rafiee Tabatabaee", "Parsa Esfahanian", "Ghazaleh Sadeghi Gohar", "Amineh Safavirad", "Saeideh Mazloomzadeh", "Ehsan khalilipur", "Armin Elahifar", "Majid Maleki"], "title": "Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography", "comment": null, "summary": "X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9X\u5c04\u7ebf\u51a0\u72b6\u52a8\u8109\u9020\u5f71\uff08XCA\uff09\u4e2d\u8840\u7ba1\u5206\u5272\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8840\u7ba1\u5206\u5272\u4e0e\u7c7b\u578b\u6807\u6ce8\u6d41\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u53ca\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "XCA\u662f\u51a0\u5fc3\u75c5\u8bca\u65ad\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u7531\u4e8e\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u8fd0\u52a8\u4f2a\u5f71\u3001\u5668\u68b0\u5e72\u6270\u7b49\u5bfc\u81f4\u8840\u7ba1\u5206\u5272\u96be\u5ea6\u5927\uff0c\u8fdb\u800c\u5f71\u54cd\u540e\u7eed\u5b9a\u91cf\u5206\u6790\u548c\u591a\u4e2d\u5fc3\u5e94\u7528\u3002\u6b64\u9886\u57df\u4e9f\u9700\u80fd\u591f\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u9c81\u68d2\u8840\u7ba1\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4ece\u6700\u4f73\u9020\u5f71\u5e27\u9009\u62e9\u3001\u8d85\u5206\u8fa8\u4e0e\u56fe\u50cf\u589e\u5f3a\u3001\u7ecf\u5178\u8840\u7ba1\u6ee4\u6ce2\u5668\uff08Meijering\u3001Frangi\u3001Sato\uff09\u4e0e\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u53c2\u6570\u9884\u6d4b\u3001\u4ee5\u53caU-Net\u3001FPN\u3001Swin Transformer\u6df1\u5ea6\u6a21\u578b\u8054\u5408\u6807\u6ce8\u8bad\u7ec3\u3002\u540c\u65f6\u8fdb\u884c\u8840\u7ba1\u7c7b\u578b\uff08LAD\u3001LCX\u3001RCA\uff09\u5206\u914d\uff0c\u5185\u90e8\u548cDCA1\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\u3002", "result": "SVR\u6309\u56fe\u50cf\u53c2\u6570\u4f18\u5316\u63d0\u5347\u4e86\u6240\u6709\u7ecf\u5178\u6ee4\u6ce2\u5206\u5272\u7cbe\u5ea6\uff08\u5982Frangi Dice\u75310.741\u5347\u81f30.759\uff09\uff1bFPN\u5728\u51a0\u72b6\u52a8\u8109\u5206\u5272Dice\u8fbe0.914\uff0c\u8054\u5408\u7ba1\u9053\u6807\u6ce8\u8fdb\u4e00\u6b65\u63d0\u5347\u52300.931\uff1b\u5916\u90e8\u6d4b\u8bd5Dice\u7565\u964d\u4f46\u8f7b\u91cf\u5fae\u8c03\u53ef\u6062\u590d\u81f30.88+\uff1b\u8840\u7ba1\u7c7b\u578b\u6807\u6ce8RCA/LAD/LCX\u5206\u5272Dice\u4e0e\u51c6\u786e\u7387\u5747\u8d85\u8fc795%\u3002", "conclusion": "\u56fe\u50cf\u7ea7\u53c2\u6570\u9884\u6d4b\u589e\u5f3a\u4e86\u7ecf\u5178\u65b9\u6cd5\uff0cFPN\u7b49\u6df1\u5ea6\u6a21\u578b\u7ed3\u5408\u591a\u6807\u6ce8\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5206\u5272\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u65b9\u6848\u9002\u7528\u4e8e\u5e38\u89c4XCA\u56fe\u50cf\uff0c\u4e3a\u51a0\u8109\u5b9a\u91cf\u5206\u6790\u548c\u5f02\u6784\u4e2d\u5fc3\u63a8\u5e7f\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2601.17971", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17971", "abs": "https://arxiv.org/abs/2601.17971", "authors": ["Junior Cedric Tonga", "Chen Cecilia Liu", "Iryna Gurevych", "Fajri Koto"], "title": "LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction", "comment": "EACL 2026 MAIN", "summary": "Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u5316\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\uff08CCKG\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u6587\u5316\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5bf9\u82f1\u8bed\u6587\u5316\u7684\u77e5\u8bc6\u8868\u8fbe\u8f83\u597d\uff0c\u800c\u5bf9\u5176\u4ed6\u6587\u5316\u7684\u8868\u8fbe\u5b58\u5728\u5dee\u8ddd\u3002\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u53ef\u4ee5\u63d0\u5347\u76f8\u5173NLP\u4efb\u52a1\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u82f1\u8bed\u94fe\u6761\u4e0a\u8868\u73b0\u66f4\u660e\u663e\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8574\u542b\u4e86\u4e30\u5bcc\u7684\u6587\u5316\u5e38\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u662f\u9690\u542b\u4e14\u65e0\u7ed3\u6784\u5316\u7684\uff0c\u9650\u5236\u4e86\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u628aLLMs\u4e2d\u7684\u6587\u5316\u5e38\u8bc6\u663e\u5f0f\u5316\u3001\u7ed3\u6784\u5316\uff0c\u63d0\u9ad8\u5176\u53ef\u7528\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u5f0f\u63d0\u793a\uff08prompt\uff09\u7684\u6846\u67b6\uff0c\u5229\u7528LLMs\u4f5c\u4e3a\u6587\u5316\u77e5\u8bc6\u5e93\uff0c\u6709\u7cfb\u7edf\u5730\u62bd\u53d6\u4e0d\u540c\u6587\u5316\u7684\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u5b9e\u8df5\uff0c\u5e76\u6574\u7406\u6210\u591a\u6b65\u63a8\u7406\u94fe\uff08inferential chains\uff09\uff0c\u6700\u7ec8\u6784\u5efa\u6210\u6587\u5316\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\uff08CCKG\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u56fd\u5bb6\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u6587\u5316\u76f8\u5173\u6027\u3001\u6b63\u786e\u6027\u548c\u8def\u5f84\u8fde\u8d2f\u6027\u6765\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCCKG\u5728\u82f1\u8bed\u4e2d\u8868\u8fbe\u66f4\u597d\uff0c\u5373\u4f7f\u76ee\u6807\u6587\u5316\u662f\u975e\u82f1\u8bed\uff08\u5982\u4e2d\u6587\u3001\u5370\u5c3c\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\uff09\uff0c\u4e5f\u4f9d\u8d56\u82f1\u6587\u8868\u8fbe\uff0c\u66b4\u9732\u4e86\u73b0\u6709LLMs\u6587\u5316\u77e5\u8bc6\u7f16\u7801\u7684\u4e0d\u5747\u8861\u3002\u6b64\u5916\uff0c\u5c06CCKG\u5e94\u7528\u4e8e\u8f83\u5c0f\u7684LLMs\u4e0a\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728\u6587\u5316\u63a8\u7406\u548c\u6545\u4e8b\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u82f1\u8bed\u94fe\u6761\u65f6\u3002", "conclusion": "LLMs\u4f5c\u4e3a\u6587\u5316\u6280\u672f\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u77e5\u8bc6\u7f16\u7801\u65b9\u9762\u4ecd\u6709\u9650\u5236\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u6587\u5316\u77e5\u8bc6\u94fe\uff0c\u4e0d\u4ec5\u80fd\u66f4\u597d\u5730\u7528\u4e8eNLP\u4efb\u52a1\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u672a\u6765\u6587\u5316\u77e5\u8bc6\u5efa\u6a21\u7684\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2601.17468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17468", "abs": "https://arxiv.org/abs/2601.17468", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Jing-Hui Jung", "Yu-Jou Hsiao", "Chih-Chung Hsu", "Yu-Lun Liu"], "title": "ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation", "comment": "Project page: https://wuw2135.github.io/ReflexSplit-ProjectPage/", "summary": "Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.", "AI": {"tldr": "ReflexSplit\u662f\u4e00\u79cd\u65b0\u63d0\u51fa\u7684\u5355\u5f20\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u878d\u5408\u3001\u7ed3\u6784\u63d0\u53d6\u4e0e\u5206\u79bb\u6a21\u5757\uff0c\u4ee5\u53ca\u9010\u6b65\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u79bb\u8d28\u91cf\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5355\u5f20\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u6df7\u5408\u53ca\u6df1\u5ea6\u89e3\u7801\u9636\u6bb5\u5bb9\u6613\u51fa\u73b0\u900f\u5c04-\u53cd\u5c04\u6df7\u6dc6\uff0c\u4e3b\u8981\u539f\u56e0\u4e3a\u7279\u5f81\u878d\u5408\u673a\u5236\u9690\u5f0f\u4e14\u591a\u5c3a\u5ea6\u534f\u8c03\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1\uff09\u63d0\u51fa\u8de8\u5c3a\u5ea6\u95e8\u63a7\u878d\u5408\u6a21\u5757\uff08CrGF\uff09\uff0c\u5bf9\u591a\u5c42\u8bed\u4e49\u4e0e\u89e3\u7801\u4e0a\u4e0b\u6587\u8fdb\u884c\u81ea\u9002\u5e94\u805a\u5408\uff0c\u4fdd\u8bc1\u68af\u5ea6\u6d41\u7545\u4e0e\u7279\u5f81\u4e00\u81f4\u6027\uff1b2\uff09\u8bbe\u8ba1\u5c42\u878d\u5408-\u5206\u79bb\u6a21\u5757\uff08LFSB\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5171\u4eab\u63d0\u53d6\u548c\u5c42\u5dee\u5206\u79bb\u5b9e\u73b0\u6709\u6548\u5206\u79bb\uff0c\u5e76\u501f\u9274Differential Transformer\u601d\u60f3\uff0c\u5f15\u5165\u8de8\u6d41attention\u62b5\u6d88\uff1b3\uff09\u91c7\u7528\u5c42\u6b21\u9012\u8fdb\u8bad\u7ec3\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u548c\u8bad\u7ec3\u7b56\u7565\u9010\u5c42\u63d0\u5347\u5206\u79bb\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u53ca\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cReflexSplit\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5206\u79bb\u7cbe\u5ea6\u3001\u66f4\u597d\u611f\u77e5\u8d28\u91cf\u548c\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ReflexSplit\u901a\u8fc7\u521b\u65b0\u6027\u7684\u7f51\u7edc\u7ed3\u6784\u548c\u5206\u79bb\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u5f20\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u7684\u6548\u679c\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.17982", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17982", "abs": "https://arxiv.org/abs/2601.17982", "authors": ["Kshitij Mishra", "Nils Lukas", "Salem Lahlou"], "title": "SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets", "comment": "Accepted at EACL 2026", "summary": "Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u63a8\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u4e3a\u7b97\u529b\u53d7\u9650\uff0c\u96be\u4ee5\u8fdb\u884c\u590d\u6742\u63a8\u7406\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u63a2\u7d22\u6709\u6548\u63a8\u7406\u8def\u5f84\u7684\u8ba1\u7b97\u4ee3\u4ef7\u8fc7\u9ad8\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e13\u6ce8\u4e8e\u7ed3\u679c\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u591a\u6837\u6027\u63a2\u7d22\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86SD-E^2\uff08Semantic Diversity-Exploration-Exploitation\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\uff0c\u5bf9\u751f\u6210\u7684\u63a8\u7406\u8def\u5f84\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u591a\u6837\u6027\u8d4b\u4e88\u5956\u52b1\u3002\u5956\u52b1\u51fd\u6570\u8003\u8651\u4e86\u8bed\u4e49\u4e0a\u4e0d\u540c\u7b56\u7565\u7684\u8986\u76d6\u7387\u4ee5\u53ca\u5e73\u5747\u4e24\u4e24\u5d4c\u5165\u8ddd\u79bb\uff0c\u8054\u5408\u6b63\u786e\u7387\u4e0e\u6548\u7387\u505az-score\u5f52\u4e00\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u76f8\u8f83\u4e8e\u57fa\u7840\u6a21\u578bQwen2.5-3B-Instruct\u548c\u5f3a\u57fa\u7ebfGRPO\u63d0\u5347\u4e86+27.4\u3001+5.2\u3001+1.5\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u4e14\u5e73\u5747\u6bcf\u9898\u80fd\u53d1\u73b09.8\u79cd\u4e0d\u540c\u7684\u8bed\u4e49\u63a8\u7406\u7b56\u7565\uff1bMedMCQA\u548cAIME\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4e5f\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5bf9\u63a8\u7406\u8def\u5f84\u8bed\u4e49\u65b0\u9896\u6027\u8fdb\u884c\u5956\u52b1\uff0c\u4e3a\u53d7\u9650\u7b97\u529b\u7684SLM\u5e26\u6765\u4e86\u66f4\u9ad8\u6548\u7684\u63a2\u7d22-\u5229\u7528\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u6548\u7387\u63d0\u5347\u3002\u901a\u8fc7\u5173\u6ce8\u63a8\u7406\u7ed3\u6784\u800c\u975e\u9010Token\u8ba1\u7b97\uff0c\u4e3a\u5c0f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2601.17470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17470", "abs": "https://arxiv.org/abs/2601.17470", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Yu-Jou Hsiao", "Jing-Hui Jung", "Yu-Lun Liu", "Chih-Chung Hsu"], "title": "PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors", "comment": "Project Page: https://ming053l.github.io/PhaSR", "summary": "Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhaSR\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u5148\u9a8c\u5bf9\u9f50\uff0c\u4ece\u5355\u4e00\u5149\u6e90\u5230\u591a\u5149\u6e90\u73af\u5883\u4e0b\u90fd\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u9634\u5f71\u53bb\u9664\u3002\u65b9\u6cd5\u5728\u51c6\u786e\u3001\u6cdb\u5316\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u624b\u6bb5\u3002", "motivation": "\u5728\u590d\u6742\u591a\u6837\u7684\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9634\u5f71\u53bb\u9664\u975e\u5e38\u56f0\u96be\uff0c\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5c06\u5149\u7167\u5f71\u54cd\u548c\u7269\u4f53\u672c\u8eab\u7684\u53cd\u5c04\u7387\u51c6\u786e\u533a\u5206\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u5148\u9a8c\u5bf9\u9f50\u4e0d\u8db3\u65f6\u96be\u4ee5\u53d6\u5f97\u7406\u60f3\u6548\u679c\uff0c\u5c24\u5176\u5728\u591a\u5149\u6e90\u573a\u666f\u4e0b\u6613\u5931\u6548\u3002", "method": "PhaSR\u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1\uff09\u7269\u7406\u5bf9\u9f50\u5f52\u4e00\u5316\uff08PAN\uff09\u7ed3\u5408Gray-world\u5f52\u4e00\u5316\u3001log\u57dfRetinex\u5206\u89e3\u548c\u52a8\u6001\u8303\u56f4\u91cd\u7ec4\uff0c\u6291\u5236\u8272\u5f69\u504f\u7f6e\uff0c\u5b9e\u73b0\u5c01\u95ed\u5f62\u5f0f\u7684\u5149\u7167\u4fee\u6b63\uff1b2\uff09\u51e0\u4f55-\u8bed\u4e49\u77eb\u6b63\u6ce8\u610f\u529b\uff08GSRA\uff09\uff0c\u5c06\u6df1\u5ea6\u51e0\u4f55\u4fe1\u606f\u4e0eDINO-v2\u8bed\u4e49\u5d4c\u5165\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u7f13\u89e3\u5f3a/\u5f31\u5149\u7167\u4e0b\u7684\u6a21\u6001\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9634\u5f71\u53bb\u9664\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u590d\u6742\u5ea6\u4f4e\uff0c\u5e76\u5bf9\u591a\u6e90\u73af\u5883\u5177\u5907\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u4f20\u7edf\u65b9\u6cd5\u6613\u5931\u6548\u7684\u60c5\u5f62\u4e0b\u4f9d\u7136\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "PhaSR\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9634\u5f71\u53bb\u9664\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u5149\u6e90\u548c\u590d\u6742\u73af\u5883\u4e2d\u5177\u5907\u4f18\u52bf\uff0c\u4e14\u6a21\u578b\u8f83\u4e3a\u8f7b\u91cf\u3002"}}
{"id": "2601.17993", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17993", "abs": "https://arxiv.org/abs/2601.17993", "authors": ["Marina Zavertiaeva", "Petr Parshakov", "Mikhail Usanin", "Aleksei Smirnov", "Sofia Paklina", "Anastasiia Kibardina"], "title": "AI-based approach to burnout identification from textual data", "comment": "9 pages, 2 figures", "summary": "This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u548cNLP\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6539\u8fdb\u7684RuBERT\u6a21\u578b\uff0c\u901a\u8fc7\u68c0\u6d4b\u6587\u672c\u6570\u636e\u6765\u8bc6\u522b\u804c\u4e1a\u5026\u6020\uff0c\u5e76\u5728\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "motivation": "\u804c\u4e1a\u5026\u6020\u5728\u9ad8\u538b\u5de5\u4f5c\u73af\u5883\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u81ea\u52a8\u5316\u68c0\u6d4b\u8f83\u4e3a\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u8bc6\u522b\u548c\u76d1\u6d4b\u5026\u6020\u8ff9\u8c61\uff0c\u4ee5\u4fbf\u53ca\u65f6\u5e72\u9884\u3002", "method": "\u8be5\u65b9\u6cd5\u4ee5RuBERT\u6a21\u578b\u4e3a\u6838\u5fc3\uff0c\u5148\u7528\u4e8e\u60c5\u611f\u5206\u6790\u8bad\u7ec3\uff0c\u7136\u540e\u5229\u7528ChatGPT\u751f\u6210\u7684\u5408\u6210\u53e5\u5b50\u548c\u4fc4\u8bedYouTube\u5173\u4e8e\u5026\u6020\u7684\u89c6\u9891\u8bc4\u8bba\u8fdb\u4e00\u6b65\u5fae\u8c03\u6a21\u578b\uff0c\u4ee5\u8fdb\u884c\u5026\u6020\u68c0\u6d4b\u3002\u6a21\u578b\u53ef\u4e3a\u8f93\u5165\u6587\u672c\u5206\u914d\u5026\u6020\u6982\u7387\uff0c\u5e76\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u5904\u7406\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5206\u6790\u6587\u672c\u6570\u636e\uff0c\u8bc6\u522b\u4e0e\u804c\u4e1a\u5026\u6020\u76f8\u5173\u7684\u8bed\u8a00\u4fe1\u53f7\uff0c\u5bf9\u8f93\u5165\u6587\u672c\u7ed9\u51fa\u5026\u6020\u6982\u7387\u8bc4\u5206\uff0c\u53ef\u5728\u5b9e\u9645\u9ad8\u538b\u73af\u5883\u4e2d\u76d1\u6d4b\u5458\u5de5\u72b6\u6001\u3002", "conclusion": "AI\u4e0eNLP\u7ed3\u5408\u7684\u81ea\u52a8\u5026\u6020\u68c0\u6d4b\u65b9\u6cd5\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u53d1\u73b0\u548c\u7ba1\u7406\u804c\u4e1a\u5026\u6020\u3002"}}
{"id": "2601.17504", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.17504", "abs": "https://arxiv.org/abs/2601.17504", "authors": ["Yan Zhou", "Zhen Huang", "Yingqiu Li", "Yue Ouyang", "Suncheng Xiang", "Zehua Wang"], "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation", "comment": "16 pages, 5 figures. Manuscript prepared for submission to ACM TOMM", "summary": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBMDS-Net\u6a21\u578b\uff0c\u9488\u5bf9\u591a\u6a21\u6001MRI\u8111\u80bf\u7624\u5206\u5272\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5b9e\u9645\u4e34\u5e8a\u4e2d\u7f3a\u5931\u6a21\u6001\u65f6\u7684\u7a33\u5065\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5e76\u4e14\u4e3a\u6bcf\u4e2a\u4f53\u7d20\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ece\u800c\u66f4\u597d\u652f\u6301\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\uff0c\u867d\u7136\u5728\u7406\u60f3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u5e38\u89c1\u7684\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u6548\u679c\u5927\u5e45\u4e0b\u964d\uff0c\u4e14\u6a21\u578b\u5bf9\u81ea\u8eab\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u7f3a\u5c11\u91cf\u5316\uff0c\u8fd9\u5f71\u54cd\u4e86\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "BMDS-Net\u5305\u62ec\u4e09\u5927\u521b\u65b0\uff1a1\uff09\u91c7\u7528\u5177\u6709Zero-Init\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u878d\u5408\u6a21\u5757\uff08MMCF\uff09\u548c\u6b8b\u5dee\u95e8\u63a7\u6df1\u5ea6\u89e3\u7801\u76d1\u7763\uff08DDS\uff09\uff0c\u589e\u5f3a\u7279\u5f81\u7a33\u5b9a\u6027\u548c\u8fb9\u754c\u5206\u5272\u80fd\u529b\uff0c\u5373\u4f7f\u5b58\u5728\u6a21\u6001\u7f3a\u5931\u4e5f\u80fd\u964d\u4f4eHausdorff\u8ddd\u79bb\uff1b2\uff09\u5f15\u5165\u9ad8\u6548\u7684\u8d1d\u53f6\u65af\u5fae\u8c03\u7b56\u7565\uff0c\u5c06\u7f51\u7edc\u8f6c\u5316\u4e3a\u6982\u7387\u9884\u6d4b\u5668\uff0c\u80fd\u591f\u751f\u6210\u6bcf\u4f53\u7d20\u7684\u4e0d\u786e\u5b9a\u6027\u6982\u7387\u56fe\uff0c\u7528\u4ee5\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\uff1b3\uff09\u5728BraTS 2021\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\uff0cBMDS-Net\u5728\u5404\u79cd\u6a21\u6001\u7f3a\u5931\u7684\u60c5\u5883\u4e0b\uff0c\u4f9d\u7136\u7ef4\u6301\u4e0e\u5f53\u524d\u6700\u597d\u6a21\u578b\u76f8\u5f53\u7684\u5206\u5272\u51c6\u786e\u7387\uff0c\u5e76\u5728\u7a33\u5065\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\u3002", "conclusion": "BMDS-Net\u4e0d\u4ec5\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0c\u66f4\u5728\u9762\u5bf9\u5b9e\u9645\u4e34\u5e8a\u5e38\u89c1\u95ee\u9898\u65f6\u5c55\u73b0\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4e3aMRI\u8111\u80bf\u7624\u81ea\u52a8\u5206\u5272\u7684\u4e34\u5e8a\u91c7\u7eb3\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6491\u3002"}}
{"id": "2601.18006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18006", "abs": "https://arxiv.org/abs/2601.18006", "authors": ["Lorenzo Proietti", "Roman Grundkiewicz", "Matt Post"], "title": "PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation", "comment": "18 pages", "summary": "We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u53c2\u8003\u673a\u5668\u7ffb\u8bd1\u8bc4\u4ef7\u6307\u6807PEAR\uff0c\u4ee5\u6210\u5bf9\u6bd4\u8f83\u7684\u65b9\u5f0f\u9884\u6d4b\u4e24\u4e2a\u5019\u9009\u7ffb\u8bd1\u95f4\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u4f30\u8ba1\uff0c\u5c24\u5176\u662f\u65e0\u53c2\u8003\u7684\u81ea\u52a8\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u5b58\u5728\u8bc4\u4ef7\u4fe1\u53f7\u5197\u4f59\u3001\u6307\u6807\u6a21\u578b\u5e9e\u5927\u3001\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u5dee\u5f02\u7684\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u65b9\u6cd5\uff0c\u63d0\u5347\u8bc4\u4ef7\u51c6\u786e\u6027\uff0c\u5e76\u7b80\u5316\u6a21\u578b\u7ed3\u6784\u3002", "method": "PEAR\u4ee5\u6e90\u53e5\u548c\u4e24\u4e2a\u5019\u9009\u8bd1\u6587\u4e3a\u8f93\u5165\uff0c\u9884\u6d4b\u5b83\u4eec\u4e4b\u95f4\u8d28\u91cf\u5dee\u5f02\u7684\u65b9\u5411\u548c\u5e45\u5ea6\u3002\u6a21\u578b\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4ef7\u5dee\u5f02\u8fdb\u884c\u6210\u5bf9\u76d1\u7763\u8bad\u7ec3\uff0c\u52a0\u5165\u6b63\u5219\u9879\u4ee5\u4fdd\u8bc1\u6362\u4f4d\u65f6\u7b26\u53f7\u53cd\u8f6c\u3002\u4f5c\u8005\u57fa\u4e8e\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9aa8\u5e72\u7f51\u7edc\uff0c\u5c06PEAR\u4e0e\u5355\u5019\u9009\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u4e25\u683c\u5bf9\u6bd4\u3002", "result": "\u5728WMT24\u5143\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0cPEAR\u8d85\u8d8a\u4e86\u4f7f\u7528\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u548c\u53c2\u6570\u91cf\u7684\u5355\u5019\u9009\u65e0\u53c2\u8003\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4f18\u4e8e\u53c2\u6570\u91cf\u8fdc\u8d85\u81ea\u8eab\u7684\u5927\u578b\u6a21\u578b\u548c\u53c2\u8003\u578b\u6307\u6807\u3002\u5206\u6790\u8fd8\u663e\u793aPEAR\u63d0\u4f9b\u4e86\u66f4\u5177\u72ec\u7acb\u6027\u7684\u8bc4\u4ef7\u4fe1\u53f7\u3002", "conclusion": "PEAR\u4ee5\u66f4\u5c11\u53c2\u6570\u8fbe\u5230\u66f4\u4f18\u7684\u8bc4\u5206\u8868\u73b0\uff0c\u8bc4\u4ef7\u4fe1\u53f7\u66f4\u52a0\u6709\u6548\u4e14\u72ec\u7279\u3002\u5176\u4f5c\u4e3aBayes\u98ce\u9669\u6700\u5c0f\u5316\u89e3\u7801\u7684\u6548\u7528\u51fd\u6570\u65f6\uff0c\u80fd\u4ee5\u6781\u4f4e\u4ee3\u4ef7\u63d0\u4f9b\u6709\u6548\u652f\u6301\uff0c\u6709\u6f5c\u529b\u6210\u4e3a\u65e0\u53c2\u8003MT\u8bc4\u4f30\u5de5\u5177\u7684\u65b0\u6807\u51c6\u3002"}}
{"id": "2601.17529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17529", "abs": "https://arxiv.org/abs/2601.17529", "authors": ["Fengting Zhang", "Yue He", "Qinghao Liu", "Yaonan Wang", "Xiang Chen", "Hang Zhang"], "title": "FMIR, a foundation model-based Image Registration Framework for Robust Image Registration", "comment": null, "summary": "Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u533b\u7597\u56fe\u50cf\u914d\u51c6\u6846\u67b6FMIR\uff0c\u5b9e\u73b0\u4e86\u5728\u8bad\u7ec3\u57df\u548c\u975e\u8bad\u7ec3\u57df\u4e0a\u7684\u7a33\u5065\u6027\u80fd\uff0c\u7a81\u7834\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u63d0\u5347\u4e86\u533b\u7597\u56fe\u50cf\u914d\u51c6\u901f\u5ea6\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u9886\u57df\u6570\u636e\uff0c\u8fd9\u662f\u7531\u4e8e\u533b\u5b66\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6240\u81f4\u3002", "method": "FMIR\u7ed3\u5408\u4e86\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u7f16\u7801\u5668\u6765\u63d0\u53d6\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u901a\u9053\u6b63\u5219\u5316\u7b56\u7565\u8bad\u7ec3\u914d\u51c6\u5934\uff0c\u53ea\u9700\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5373\u53ef\u3002", "result": "FMIR\u5728\u8bad\u7ec3\u6570\u636e\u96c6\uff08in-domain\uff09\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u914d\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5916\u90e8\u6570\u636e\u96c6\uff08out-of-domain\uff09\u4e0a\u7684\u7ed3\u679c\u4e5f\u975e\u5e38\u7a33\u5065\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6784\u5efa\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u533b\u7597\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.18012", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18012", "abs": "https://arxiv.org/abs/2601.18012", "authors": ["Hendrika Maclean", "Mert Can Cakmak", "Muzakkiruddin Ahmed Mohammed", "Shames Al Mandalawi", "John Talburt"], "title": "Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems", "comment": null, "summary": "Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u4e1a\u52a1\u573a\u666f\uff08\u5982\u85aa\u8d44\u7cfb\u7edf\uff09\u4e2d\u7684\u7cbe\u786e\u8fd0\u7b97\u4e0e\u53ef\u5ba1\u8ba1\u80fd\u529b\uff0c\u901a\u8fc7\u4e0d\u540c\u6570\u636e\u3001\u63d0\u793a\u65b9\u5f0f\u53ca\u6a21\u578b\u5b9e\u9a8c\uff0c\u6307\u51fa\u4f55\u65f6\u4f9d\u8d56\u6a21\u578b\u63a8\u7406\u3001\u4f55\u65f6\u5fc5\u987b\u663e\u5f0f\u8ba1\u7b97\uff0c\u5e76\u63d0\u51fa\u53ef\u590d\u73b0\u7684\u6d4b\u8bd5\u6846\u67b6\u4e0e\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b57\u7406\u89e3\u4e0e\u751f\u6210\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7cbe\u786e\u6570\u503c\u8ba1\u7b97\u548c\u8f93\u51fa\u53ef\u8ffd\u6eaf\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u51c6\u786e\u6027\u548c\u5408\u89c4\u6027\u8981\u6c42\u9ad8\u7684\u4e1a\u52a1\u573a\u666f\uff08\u5982\u85aa\u8d44\u5904\u7406\uff09\u4e2d\u5b58\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u4e00\u5957\u6db5\u76d6\u7b80\u5355\u5230\u590d\u6742\u85aa\u8d44\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u7f6e\u4e86\u4ece\u6700\u57fa\u7840\u5230\u5f15\u5bfc\u5f0f\u3001\u63a8\u7406\u5f0f\u7b49\u4e0d\u540c\u7c7b\u578b\u7684\u63d0\u793a\u8bcd\uff0c\u5bf9GPT\u3001Claude\u3001Perplexity\u3001Grok\u3001Gemini\u7b49\u591a\u79cd\u4e3b\u6d41\u5927\u6a21\u578b\u9010\u9879\u6d4b\u8bd5\uff0c\u68c0\u9a8c\u5176\u5bf9\u85aa\u8d44\u89c4\u5219\u7684\u7406\u89e3\u3001\u89c4\u5219\u6392\u5e8f\u6267\u884c\u53ca\u5c0f\u6570\u70b9\u7ea7\u522b\u7684\u7cbe\u7b97\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u90e8\u5206\u573a\u666f\u4e0b\uff0c\u4ec5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u8bcd\u5373\u53ef\u83b7\u5f97\u51c6\u786e\u7ed3\u679c\uff1b\u4f46\u5728\u66f4\u590d\u6742\u8ba1\u7b97\u4efb\u52a1\u4e2d\uff0c\u5fc5\u987b\u7ed3\u5408\u663e\u5f0f\u7684\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5426\u5219\u6a21\u578b\u8f93\u51fa\u5bb9\u6613\u51fa\u9519\u3002\u6b64\u5916\uff0c\u8be5\u6587\u63d0\u51fa\u7684\u6d4b\u8bd5\u6846\u67b6\u80fd\u6709\u6548\u590d\u73b0\u7ed3\u679c\u5e76\u91cf\u5316\u5404\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7cbe\u5ea6\u8981\u6c42\u7684\u4e1a\u52a1\u573a\u666f\u4e2d\u4f7f\u7528\u65f6\uff0c\u9700\u8c28\u614e\u8bc4\u4f30\u5176\u63a8\u7406\u4e0e\u8ba1\u7b97\u80fd\u529b\u3002\u5bf9\u4e8e\u80fd\u88ab\u4ed4\u7ec6\u63d0\u793a\u63a7\u5236\u7684\u5185\u5bb9\u53ef\u76f4\u63a5\u7528\u6a21\u578b\uff0c\u4f46\u9047\u5230\u590d\u6742\u8ba1\u7b97\u4ecd\u5e94\u91c7\u7528\u660e\u786e\u7684\u7b97\u6cd5\u5b9e\u73b0\u3002\u8bba\u6587\u6846\u67b6\u4e3a\u76f8\u5173\u9886\u57df\u6269\u5c55\u5e94\u7528\u63d0\u4f9b\u4e86\u8303\u5f0f\u548c\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2601.17535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17535", "abs": "https://arxiv.org/abs/2601.17535", "authors": ["Kevin Robbins", "Xiaotong Liu", "Yu Wu", "Le Sun", "Grady McPeak", "Abby Stylianou", "Robert Pless"], "title": "Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries", "comment": null, "summary": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\uff0c\u901a\u8fc7\u751f\u6210\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u5408\u6210\u56fe\u50cf\uff0c\u540c\u65f6\u7ed3\u5408\u6587\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\u9884\u6d4b\u3002", "motivation": "\u867d\u7136VLM\u5982CLIP\u8ba9\u7528\u6237\u53ea\u9700\u547d\u540d\u7c7b\u522b\u5373\u53ef\u642d\u5efa\u89c6\u89c9\u5206\u7c7b\u5668\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u65b0\u9886\u57df\u4e2d\u6548\u679c\u96be\u4ee5\u5224\u5b9a\uff0c\u5c24\u5176\u666e\u901a\u7528\u6237\u7f3a\u4e4f\u8bc4\u4f30\u5de5\u5177\u3002\u73b0\u6709\u4ec5\u57fa\u4e8e\u6587\u672c\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f5c\u8005\u5728\u6587\u672c\u6bd4\u8f83\u57fa\u7840\u4e0a\uff0c\u63a2\u7d22\u4e86\u7ed3\u5408\u5408\u6210\u56fe\u50cf\u751f\u6210\uff0c\u6a21\u62df\u65b0\u4efb\u52a1\u7c7b\u522b\u6837\u672c\uff0c\u518d\u4ee5\u6587\u672c\u548c\u56fe\u50cf\u8054\u5408\u65b9\u5f0f\uff0c\u5bf9VLM\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u8bc4\u4f30\u548c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u5bf9VLM\u96f6\u6837\u672c\u7cbe\u5ea6\u7684\u9884\u6d4b\u8d28\u91cf\uff0c\u5e76\u8ba9\u7528\u6237\u4e86\u89e3\u8bc4\u4f30\u6240\u53c2\u8003\u7684\u56fe\u50cf\u7c7b\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u5728\u65e0\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5e2e\u52a9\u7528\u6237\u5224\u65adVLM\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u9002\u7528\u6027\u7684\u53ef\u89e3\u91ca\u8bc4\u4f30\u3002"}}
{"id": "2601.18014", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18014", "abs": "https://arxiv.org/abs/2601.18014", "authors": ["Adeeba Tarannum", "Muzakkiruddin Ahmed Mohammed", "Mert Can Cakmak", "Shames Al Mandalawi", "John Talburt"], "title": "A System for Name and Address Parsing with Large Language Models", "comment": null, "summary": "Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u8bcd\u9a71\u52a8\u548c\u9a8c\u8bc1\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5c06\u81ea\u7531\u6587\u672c\u683c\u5f0f\u7684\u4eba\u5458\u548c\u5730\u5740\u4fe1\u606f\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u5730\u8f6c\u5316\u4e3a17\u5b57\u6bb5\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5728\u591a\u8bed\u8a00\u6216\u566a\u58f0\u73af\u5883\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u4fe1\u606f\u7cfb\u7edf\u4e2d\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u7684\u4eba\u540d\u548c\u5730\u5740\u6587\u672c\u9ad8\u8d28\u91cf\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u4e00\u76f4\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u53ca\u6982\u7387\u7684\u65b9\u6cd5\u5728\u566a\u58f0\u6570\u636e\u548c\u591a\u8bed\u79cd\u73af\u5883\u4e0b\u8868\u73b0\u5dee\uff0c\u795e\u7ecf\u7f51\u7edc\u53ca\u5927\u6a21\u578b\u5219\u7f3a\u4e4f\u786e\u5b9a\u6027\u53ef\u63a7\u548c\u53ef\u590d\u73b0\u6027\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u517c\u5177\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u590d\u73b0\u6027\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u8f93\u5165\u5f52\u4e00\u5316\u3001\u7ed3\u6784\u5316\u63d0\u793a\u8bcd\u3001\u7ea6\u675f\u89e3\u7801\u53ca\u4e25\u683c\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\uff0c\u901a\u8fc7\u56fa\u5b9a\u5b9e\u9a8c\u8bbe\u7f6e\u4fdd\u969c\u53ef\u91cd\u590d\u6027\u3002\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u76f4\u63a5\u5c06\u81ea\u7531\u6587\u672c\u8f6c\u5316\u4e3a17\u5b57\u6bb5\u7684\u7edf\u4e00\u7ed3\u6784\u6a21\u5f0f\u3002", "result": "\u5728\u5b9e\u9645\u5f02\u6784\u5730\u5740\u6570\u636e\u96c6\u4e0a\u8bc4\u6d4b\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5b57\u6bb5\u7ea7\u51c6\u786e\u7387\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7b49\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e14\u4fdd\u6301\u9ad8\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u786e\u5b9a\u6027\u89c4\u5219\u9a8c\u8bc1\u4e0e\u751f\u6210\u5f0f\u63d0\u793a\u7b56\u7565\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u4e3a\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\u63d0\u4f9b\u4e86\u5f3a\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6848\uff0c\u662f\u73b0\u6709\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u7684\u6216\u5f3a\u4f9d\u8d56\u9886\u57df\u6a21\u578b\u7684\u6709\u6548\u66ff\u4ee3\u9009\u62e9\u3002"}}
{"id": "2601.17536", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17536", "abs": "https://arxiv.org/abs/2601.17536", "authors": ["Jiaming Liang", "Haowei Liu", "Chi-Man Pun"], "title": "OTI: A Model-free and Visually Interpretable Measure of Image Attackability", "comment": null, "summary": "Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u56fe\u50cf\u6613\u653b\u51fb\u6027\u5ea6\u91cf\u65b9\u6cd5\u2014\u2014\u5bf9\u8c61\u7eb9\u7406\u5f3a\u5ea6\uff08OTI\uff09\uff0c\u65e0\u9700\u4f9d\u8d56\u7279\u5b9a\u6a21\u578b\uff0c\u4e14\u5177\u5907\u53ef\u89c6\u5316\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6613\u653b\u51fb\u6027\u5ea6\u91cf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6a21\u578b\u548c\u4e0d\u53ef\u89c6\u5316\u7684\u7279\u5f81\uff0c\u4e14\u5f88\u591a\u5b9e\u9645\u4efb\u52a1\u6a21\u578b\u4e0d\u53ef\u7528\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u63a8\u5e7f\u5e94\u7528\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e0\u6a21\u578b\u3001\u5177\u5907\u76f4\u89c2\u89e3\u91ca\u6027\u7684\u56fe\u50cf\u6613\u653b\u51fb\u6027\u5ea6\u91cf\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u6613\u653b\u51fb\u6027\u8861\u91cf\u6307\u6807\u2014\u2014\u5bf9\u8c61\u7eb9\u7406\u5f3a\u5ea6\uff08OTI\uff09\uff0c\u5b83\u57fa\u4e8e\u56fe\u50cf\u4e2d\u8bed\u4e49\u5bf9\u8c61\u7684\u7eb9\u7406\u5f3a\u5ea6\u8fdb\u884c\u5ea6\u91cf\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u3002\u672c\u65b9\u6cd5\u7406\u8bba\u4e0a\u7ed3\u5408\u51b3\u7b56\u8fb9\u754c\u548c\u5bf9\u6297\u6270\u52a8\u7684\u9891\u7387\u7279\u6027\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOTI\u80fd\u591f\u9ad8\u6548\u3001\u6709\u6548\u5730\u8861\u91cf\u56fe\u50cf\u7684\u6613\u653b\u51fb\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4efb\u52a1\u4e2d\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "OTI\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u65e0\u9700\u6a21\u578b\u7684\u56fe\u50cf\u6613\u653b\u51fb\u6027\u5ea6\u91cf\uff0c\u8fd8\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u5bf9\u6297\u673a\u5668\u5b66\u4e60\u9886\u57df\u6df1\u5165\u7406\u89e3\u548c\u5229\u7528\u56fe\u50cf\u6613\u653b\u51fb\u6027\u8fd9\u4e00\u91cd\u8981\u5c5e\u6027\u3002"}}
{"id": "2601.18026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18026", "abs": "https://arxiv.org/abs/2601.18026", "authors": ["Pedro Ortiz Suarez", "Laurie Burchell", "Catherine Arnett", "Rafael Mosquera-G\u00f3mez", "Sara Hincapie-Monsalve", "Thom Vaughan", "Damian Stewart", "Malte Ostendorff", "Idris Abdulmumin", "Vukosi Marivate", "Shamsuddeen Hassan Muhammad", "Atnafu Lambebo Tonja", "Hend Al-Khalifa", "Nadia Ghezaiel Hammouda", "Verrah Otiende", "Tack Hwa Wong", "Jakhongir Saydaliev", "Melika Nobakhtian", "Muhammad Ravi Shulthan Habibi", "Chalamalasetti Kranti", "Carol Muchemi", "Khang Nguyen", "Faisal Muhammad Adam", "Luis Frentzen Salim", "Reem Alqifari", "Cynthia Amol", "Joseph Marvin Imperial", "Ilker Kesen", "Ahmad Mustafid", "Pavel Stepachev", "Leshem Choshen", "David Anugraha", "Hamada Nayel", "Seid Muhie Yimam", "Vallerie Alexandra Putra", "My Chiffon Nguyen", "Azmine Toushik Wasi", "Gouthami Vadithya", "Rob van der Goot", "Lanwenn ar C'horr", "Karan Dua", "Andrew Yates", "Mithil Bangera", "Yeshil Bangera", "Hitesh Laxmichand Patel", "Shu Okabe", "Fenal Ashokbhai Ilasariya", "Dmitry Gaynullin", "Genta Indra Winata", "Yiyuan Li", "Juan Pablo Mart\u00ednez", "Amit Agarwal", "Ikhlasul Akmal Hanif", "Raia Abu Ahmad", "Esther Adenuga", "Filbert Aurelian Tjiaranata", "Weerayut Buaphet", "Michael Anugraha", "Sowmya Vajjala", "Benjamin Rice", "Azril Hafizi Amirudin", "Jesujoba O. Alabi", "Srikant Panda", "Yassine Toughrai", "Bruhan Kyomuhendo", "Daniel Ruffinelli", "Akshata A", "Manuel Goul\u00e3o", "Ej Zhou", "Ingrid Gabriela Franco Ramirez", "Cristina Aggazzotti", "Konstantin Dobler", "Jun Kevin", "Quentin Pag\u00e8s", "Nicholas Andrews", "Nuhu Ibrahim", "Mattes Ruckdeschel", "Amr Keleg", "Mike Zhang", "Casper Muziri", "Saron Samuel", "Sotaro Takeshita", "Kun Kerdthaisong", "Luca Foppiano", "Rasul Dent", "Tommaso Green", "Ahmad Mustapha Wali", "Kamohelo Makaaka", "Vicky Feliren", "Inshirah Idris", "Hande Celikkanat", "Abdulhamid Abubakar", "Jean Maillard", "Beno\u00eet Sagot", "Thibault Cl\u00e9rice", "Kenton Murray", "Sarah Luger"], "title": "CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data", "comment": "17 pages, 7 tables, 5 figures", "summary": "Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6db5\u76d6109\u79cd\u8bed\u8a00\u7684Web\u9886\u57df\u8bed\u8a00\u8bc6\u522b\uff08LID\uff09\u57fa\u51c6\u6570\u636e\u96c6CommonLID\uff0c\u5bf9\u73b0\u6709LID\u6a21\u578b\u5728\u591a\u8bed\u8a00Web\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u8bc4\u6d4b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u9ad8\u4f30\u4e86\u6a21\u578b\u5b9e\u9645\u8868\u73b0\uff0c\u540c\u65f6\u5f00\u653e\u4e86\u6570\u636e\u96c6\u548c\u76f8\u5173\u4ee3\u7801\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u6784\u5efa\u7684\u9700\u6c42\u589e\u957f\uff0c\u81ea\u52a8\u5316\u8bed\u8a00\u8bc6\u522b\uff08LID\uff09\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u5728\u591a\u6837\u4e14\u566a\u58f0\u8f83\u5927\u7684Web\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5f31\u52bf\u8bed\u8a00\u4e0a\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5168\u9762\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u7ec4\u7ec7\u793e\u533a\u529b\u91cf\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u5927\u89c4\u6a21Web\u9886\u57dfLID\u6570\u636e\u96c6CommonLID\uff0c\u6db5\u76d6109\u79cd\u8bed\u8a00\uff0c\u5c24\u5176\u8986\u76d6\u4e86\u8bb8\u591a\u4e4b\u524d\u7814\u7a76\u8f83\u5c11\u7684\u8bed\u8a00\uff0c\u5e76\u5c06\u5176\u4e0e\u5176\u4ed6\u4e94\u4e2a\u5e38\u7528LID\u8bc4\u6d4b\u96c6\u4e00\u9053\uff0c\u6d4b\u8bd5\u4e86\u516b\u79cd\u6d41\u884c\u7684LID\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8bb8\u591a\u8bed\u8a00\u5728Web\u73af\u5883\u4e0b\u7684LID\u51c6\u786e\u7387\u88ab\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u9ad8\u4f30\uff0cCommonLID\u53ef\u4ee5\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u6a21\u578b\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u540c\u65f6\u5bf9\u5f53\u524d\u4e3b\u6d41LID\u6a21\u578b\u7684\u4f18\u52a3\u505a\u4e86\u5b9a\u91cf\u5206\u6790\u3002", "conclusion": "CommonLID\u4e3a\u591a\u8bed\u79cd\u8bed\u8a00\u8bc6\u522b\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u8bc4\u6d4b\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u5efa\u8bbe\u548cLID\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5e76\u4ee5\u5f00\u653e\u8bb8\u53ef\u53d1\u5e03\uff0c\u4fc3\u8fdb\u793e\u533a\u5408\u4f5c\u4e0e\u8d44\u6e90\u5171\u4eab\u3002"}}
{"id": "2601.17555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17555", "abs": "https://arxiv.org/abs/2601.17555", "authors": ["Justin Downes", "Sam Saltwick", "Anthony Chen"], "title": "Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper", "comment": "Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems (2023)", "summary": "The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u8457\u6027\u56fe\u548c\u4f20\u7edf\u6709\u635f\u538b\u7f29\u6807\u51c6\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u536b\u661f\u56fe\u50cf\u5185\u90e8\u4e0d\u540c\u533a\u57df\u7684\u81ea\u9002\u5e94\u538b\u7f29\uff0c\u6709\u6548\u517c\u987e\u4e86\u538b\u7f29\u6548\u7387\u548c\u5bf9\u611f\u5174\u8da3\u533a\u57df\u7684\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6bcf\u5929\u90fd\u4f1a\u4ea7\u751f\u6d77\u91cf\u536b\u661f\u56fe\u50cf\u6570\u636e\uff0c\u8fd9\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5b58\u50a8\u548c\u5e26\u5bbd\u6210\u672c\u3002\u867d\u7136\u56fe\u50cf\u5206\u8fa8\u7387\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u4e0b\u6e38\u5e94\u7528\u901a\u5e38\u53ea\u5173\u6ce8\u56fe\u50cf\u4e2d\u7684\u5c0f\u90e8\u5206\u533a\u57df\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u4f18\u5316\u8fd9\u4e9b\u611f\u5174\u8da3\u533a\u57df\u7684\u4fe1\u606f\u7f16\u7801\u65b9\u5f0f\u3002", "method": "\u4f5c\u8005\u5229\u7528\u663e\u8457\u6027\u56fe\u6807\u8bb0\u51fa\u611f\u5174\u8da3\u533a\u57df\uff0c\u5e76\u6839\u636e\u663e\u8457\u6027\u7ea7\u522b\u9009\u62e9\u4e0d\u540c\u5927\u5c0f\u7684\u5e73\u6ed1\u6838\u5bf9\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\u3002\u7136\u540e\uff0c\u5c06\u5904\u7406\u540e\u7684\u56fe\u50cf\u91c7\u7528\u4f20\u7edf\u7684\u6709\u635f\u538b\u7f29\u7f16\u7801\uff0c\u5b9e\u73b0\u540c\u4e00\u5e45\u5927\u56fe\u50cf\u5185\u7684\u53ef\u53d8\u538b\u7f29\u7387\uff0c\u5bf9\u91cd\u8981\u533a\u57df\u8fdb\u884c\u4f4e\u538b\u7f29\u3001\u975e\u91cd\u8981\u533a\u57df\u9ad8\u538b\u7f29\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u56fe\u7684\u9884\u5904\u7406\u65b9\u6cd5\u7ed3\u5408\u4f20\u7edf\u538b\u7f29\u6807\u51c6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5173\u952e\u533a\u57df\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u63d0\u5347\u6574\u4f53\u538b\u7f29\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u663e\u8457\u6027\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u9884\u5904\u7406\uff0c\u53ef\u4ee5\u4e3a\u536b\u661f\u56fe\u50cf\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u63d0\u4f9b\u66f4\u4f18\u7684\u538b\u7f29\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53ea\u9700\u5173\u6ce8\u90e8\u5206\u533a\u57df\u7684\u9065\u611f\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.18053", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18053", "abs": "https://arxiv.org/abs/2601.18053", "authors": ["Pulin Agrawal", "Prasoon Goyal"], "title": "Addressing LLM Diversity by Infusing Random Concepts", "comment": null, "summary": "Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form \"Name 10 Hollywood actors\", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5728\u63d0\u793a\u8bcd\u4e2d\u52a0\u5165\u968f\u673a\u6982\u5ff5\u662f\u5426\u80fd\u591f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u7684\u591a\u6837\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165\u968f\u673a\u65e0\u5173\u8bcd\u8bed/\u53e5\u5b50\u53ef\u4ee5\u6709\u6548\u589e\u52a0\u6a21\u578b\u8f93\u51fa\u5185\u5bb9\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5185\u5bb9\u5e38\u5e38\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u8fd9\u4e00\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u521b\u610f\u3001\u751f\u6210\u7b49\u573a\u666f\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002\u4e3a\u63d0\u5347\u8f93\u51fa\u591a\u6837\u6027\uff0c\u4f5c\u8005\u5c1d\u8bd5\u5f15\u5165\u968f\u673a\u6982\u5ff5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u7cfb\u7edf\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3b\u8981\u901a\u8fc7\u8ba9LLM\u56de\u7b54\u201c\u5217\u4e3e10\u4f4d\u597d\u83b1\u575e\u6f14\u5458\u201d\u8fd9\u79cd\u95ee\u9898\uff0c\u5e76\u5728\u95ee\u9898\u524d\u6dfb\u52a0\u4e0e\u5185\u5bb9\u65e0\u5173\u7684\u968f\u673a\u8bcd\u53e5\uff0c\u518d\u5206\u6790\u5176\u8f93\u51fa\u7684\u591a\u6837\u6027\u6307\u6807\u3002\u5e94\u7528\u591a\u79cdLLM\u8fdb\u884c\u4e86\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8f93\u5165\u63d0\u793a\u8bcd\u524d\u968f\u673a\u6dfb\u52a0\u65e0\u5173\u8bcd\u6216\u8bed\u53e5\uff0c\u80fd\u663e\u8457\u63d0\u5347LLM\u8f93\u51fa\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002\u8be5\u6548\u679c\u5728\u591a\u79cd\u4e3b\u6d41\u6a21\u578b\u4e0a\u5747\u6709\u4f53\u73b0\u3002", "conclusion": "\u5f15\u5165\u968f\u673a\u6027\u7684\u65b9\u6cd5\u5728\u63d0\u5347LLM\u8f93\u51fa\u591a\u6837\u6027\u4e0a\u6548\u679c\u663e\u8457\uff0c\u76f8\u5173\u8bc4\u6d4b\u534f\u8bae\u4e5f\u53ef\u63a8\u52a8\u672a\u6765LLM\u591a\u6837\u6027\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u76f8\u5173\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.17566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17566", "abs": "https://arxiv.org/abs/2601.17566", "authors": ["Qi Li", "Xinchao Wang"], "title": "Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning", "comment": null, "summary": "Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\u2014\u2014Sponge Tool Attack (STA)\uff0c\u80fd\u591f\u901a\u8fc7\u53ea\u4fee\u6539\u8f93\u5165\u63d0\u793a\u8bcd\uff0c\u4f7f\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u53d8\u5f97\u5197\u957f\u590d\u6742\uff0c\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3001\u540c\u65f6\u4e0d\u6539\u53d8\u4efb\u52a1\u8bed\u4e49\u7ed3\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u653b\u51fb\u6cdb\u5316\u6027\u5f3a\u3001\u5bf9\u73b0\u6709\u7cfb\u7edf\u9020\u6210\u660e\u663e\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u667a\u80fd\u63a8\u7406\uff0c\u4f46\u8fd9\u79cd\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u6f5c\u5728\u7684\u5b89\u5168\u8584\u5f31\u73af\u8282\u5c1a\u672a\u53d7\u5230\u5145\u5206\u5173\u6ce8\uff0c\u7279\u522b\u662f\u8f93\u5165\u4e0e\u5de5\u5177\u8c03\u7528\u6d41\u7a0b\u53ef\u80fd\u88ab\u6076\u610f\u64cd\u63a7\u3002", "method": "STA\u653b\u51fb\u65b9\u6cd5\u5728\u53ea\u62e5\u6709\u67e5\u8be2\u6743\u9650\u4e14\u4e0d\u6539\u53d8\u6a21\u578b\u6216\u5de5\u5177\u672c\u8eab\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u91cd\u5199\u8f93\u5165\uff0c\u5c06\u7cbe\u7b80\u9ad8\u6548\u7684\u63a8\u7406\u8f6c\u6362\u4e3a\u591a\u6b65\u9aa4\u3001\u5197\u4f59\u7684\u63a8\u7406\u6d41\u7a0b\uff0c\u5e76\u4e14\u4fdd\u6301\u8f93\u51fa\u7ed3\u679c\u4e0e\u539f\u610f\u4e00\u81f4\u3002\u6574\u4e2a\u6846\u67b6\u5bf9\u88ab\u653b\u51fb\u8005\u65e0\u4fb5\u5165\u3001\u5177\u5907\u9ad8\u5ea6\u9690\u853d\u6027\u3002", "result": "\u4f5c\u8005\u57286\u4e2a\u4e0d\u540c\u6a21\u578b\u300112\u4e2a\u5de5\u5177\u30014\u4e2aagentic\u6846\u67b6\u53ca13\u4e2a\u8de85\u5927\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aSTA\u80fd\u5927\u5e45\u589e\u52a0\u63a8\u7406\u6240\u9700\u8ba1\u7b97\u8d44\u6e90\u4e14\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u96be\u4ee5\u88ab\u5bdf\u89c9\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u539f\u59cb\u4efb\u52a1\u610f\u56fe\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u5927\u6a21\u578b\u5b58\u5728\u8f93\u5165\u64cd\u63a7\u7684\u65b0\u653b\u51fb\u9762\uff0cSTA\u4f5c\u4e3a\u4e00\u79cd\u53ef\u884c\u3001\u96be\u4ee5\u5bdf\u89c9\u7684\u9690\u853d\u653b\u51fb\u65b9\u6cd5\uff0c\u66b4\u9732\u4e86\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5728\u63a8\u7406\u6548\u7387\u548c\u5b89\u5168\u6027\u4e0a\u7684\u91cd\u8981\u9690\u60a3\u3002"}}
{"id": "2601.18056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18056", "abs": "https://arxiv.org/abs/2601.18056", "authors": ["Ahmet Yavuz Uluslu", "Elliot Murphy"], "title": "Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production", "comment": null, "summary": "We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5728\u53cc\u8bed\u4ea7\u51fa\u9519\u8bef\u7814\u7a76\u4e2d\uff0c\u7ed3\u5408\u8111\u7535\u632f\u8361\u7279\u5f81\uff08oscillatory signatures\uff09\u5bf9\u73b0\u6709\u7406\u8bba\u7684\u7ea6\u675f\uff0c\u5e76\u57fa\u4e8eROSE\u795e\u7ecf\u6a21\u578b\u5bf9\u53e5\u6cd5\u8fc1\u79fb\uff08syntactic transfer\uff09\u53ca\u8de8\u8bed\u8a00\u5f71\u54cd\u8fdb\u884c\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u53cc\u8bed\u4ea7\u51fa\u9519\u8bef\u7814\u7a76\u591a\u7528\u65f6\u5e8f\u7279\u5f81\uff08\u5982ERP\uff09\uff0c\u4f46\u5bf9\u795e\u7ecf\u52a8\u529b\u5b66\u7684\u5173\u6ce8\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u65b0\u7684\u795e\u7ecf\u5b9e\u73b0\u5c42\u9762\u8bc1\u636e\u6765\u4e30\u5bcc\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u5728\u5206\u6790\u53cc\u8bed\u8005\u4ea7\u51fa\u9519\u8bef\u548c\u53e5\u5b50\u89c4\u5212\u65f6\uff0c\u5173\u6ce8\u7279\u5b9a\u7684\u8111\u7535\u632f\u8361\u5931\u8c03\uff0c\u5e76\u5229\u7528ROSE\u795e\u7ecf\u6a21\u578b\u5bf9\u53e5\u6cd5\u8f6c\u79fb\u673a\u5236\u8fdb\u884c\u5efa\u6a21\uff0c\u901a\u8fc7\u8003\u5bdf\u8de8\u8bed\u8a00\u5f71\u54cd\uff08CLI\uff09\u4e2d\u7684\u529f\u80fd\u6027\u6291\u5236\u4e0e\u7ade\u4e89\u7406\u8bba\uff0c\u63ed\u793a\u5176\u4e0e\u8111\u7535\u632f\u8361\u5f02\u5e38\u7684\u5173\u8054\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528ROSE\u6a21\u578b\u80fd\u591f\u89e3\u91caCLI\u4e2d\u5e38\u89c1\u5f62\u6001\u53e5\u6cd5\u6392\u5e8f\u5931\u8d25\u5e76\u53ef\u5173\u8054\u5230\u5177\u4f53\u7684\u8111\u632f\u8361\u5931\u8c03\u6a21\u5f0f\uff0c\u8fd9\u4e3a\u8bed\u8a00\u529f\u80fd\u969c\u788d\u63d0\u4f9b\u66f4\u590d\u6742\u7684\u7a7a\u95f4-\u65f6\u95f4\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u5c06\u8111\u632f\u8361\u7279\u5f81\u5f15\u5165\u53cc\u8bed\u7814\u7a76\u548c\u7ed3\u5408ROSE\u795e\u7ecf\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7406\u8bba\u8054\u7cfb\u548c\u5bf9\u8bed\u8a00\u969c\u788d\u795e\u7ecf\u6807\u8bb0\u7684\u66f4\u5168\u9762\u63a2\u7d22\u3002"}}
{"id": "2601.17586", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17586", "abs": "https://arxiv.org/abs/2601.17586", "authors": ["Sebastian Doerrich", "Francesco Di Salvo", "Jonas Alle", "Christian Ledig"], "title": "Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2026)", "summary": "Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u7684\u65b0\u578bVision Transformer\u6a21\u578b\uff08Stylizing ViT\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u6743\u91cd\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u548c\u89e3\u5256\u4e00\u81f4\u6027\uff0c\u6781\u5927\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u7c7b\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53d7\u9650\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u548c\u7a00\u7f3a\u6027\uff0c\u96be\u4ee5\u8de8\u57df\u548c\u8de8\u4eba\u7fa4\u6cdb\u5316\u3002\u5c3d\u7ba1\u4f20\u7edf\u6269\u589e\u548c\u98ce\u683c\u5316\u589e\u5f3a\u6709\u6240\u5e2e\u52a9\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u4e0d\u80fd\u5f88\u597d\u5730\u5e94\u5bf9\u5927\u5e45\u5ea6\u57df\u8f6c\u79fb\uff0c\u6216\u4f1a\u5f15\u5165\u4f2a\u5f71\u3001\u98ce\u683c\u591a\u6837\u6027\u4e0d\u8db3\u3002\u6025\u9700\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u57df\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Vision Transformer\u7f16\u7801\u5668\uff08Stylizing ViT\uff09\uff0c\u91c7\u7528\u6743\u91cd\u5171\u4eab\u7684\u81ea\u6ce8\u610f\u529b\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u65e2\u4fdd\u969c\u56fe\u50cf\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\uff0c\u53c8\u80fd\u5b8c\u6210\u591a\u6837\u98ce\u683c\u8fc1\u79fb\uff0c\u5b9e\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u548c\u63a8\u7406\u8fc7\u7a0b\u7684\u52a8\u6001\u56fe\u50cf\u98ce\u683c\u589e\u5f3a\u3002\u5728\u4e09\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5bf9\u6bd4\u5b9e\u9a8c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002", "result": "\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u76ae\u80a4\u75c5\u5b66\u7684\u4e09\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u8be5\u6a21\u578b\u8fdb\u884c\u6269\u589e\u53ef\u4ee5\u4f7f\u51c6\u786e\u7387\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u81f3\u591a13%\uff1b\u6d4b\u8bd5\u65f6\u5229\u7528\u8be5\u65b9\u6cd5\u505a\u63a8\u7406\u671f\u6269\u589e\u8fd8\u80fd\u63d0\u534717%\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u56fe\u50cf\u611f\u5b98\u4e0a\u65e0\u4f2a\u5f71\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\uff0c\u4e0d\u4ec5\u6269\u5927\u4e86\u98ce\u683c\u591a\u6837\u6027\uff0c\u8fd8\u51cf\u5c11\u4e86\u4f2a\u5f71\u751f\u6210\uff0c\u5bf9\u8bad\u7ec3\u548c\u63a8\u7406\u5747\u6709\u663e\u8457\u76ca\u5904\u3002"}}
{"id": "2601.18065", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18065", "abs": "https://arxiv.org/abs/2601.18065", "authors": ["Aryan Roy", "Zekun Wang", "Christopher J. MacLellan"], "title": "Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models", "comment": null, "summary": "Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.", "AI": {"tldr": "\u672c\u8bba\u6587\u6bd4\u8f83\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0e\u4ec5\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4eba\u7c7b\u5bf9\u201c\u5177\u4f53\u6027\u201d\u654f\u611f\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0VLMs\u5728\u591a\u4e2a\u5c42\u9762\u4e0a\u66f4\u5177\u4eba\u7c7b\u4e00\u81f4\u6027\u3002", "motivation": "\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4ec5\u6587\u672c\u4efb\u52a1\u4e2d\u662f\u5426\u4f1a\u6bd4\u5355\u7eaf\u6587\u672c\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5bf9\u8bed\u8a00\u5177\u4f53\u6027\u7684\u5904\u7406\uff0c\u4ece\u800c\u63a2\u7d22\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u662f\u5426\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u201c\u611f\u77e5\u57fa\u7840\u201d\u3002", "method": "\u9009\u53d6\u7ed3\u6784\u5339\u914d\u7684Llama\u6587\u672c\u6a21\u578b\u53ca\u5176\u89c6\u89c9\u5bf9\u7167\u7ec4\uff0c\u5206\u522b\u5728\u8f93\u51fa\u884c\u4e3a\u3001\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u4e09\u4e2a\u5c42\u9762\u5206\u6790\u5177\u4f53\u6027\u8868\u73b0\uff0c\u5e76\u6bd4\u8f83\u6a21\u578b\u7ed9\u51fa\u7684\u5177\u4f53\u6027\u8bc4\u5206\u4e0e\u4eba\u7c7b\u6570\u636e\u7684\u543b\u5408\u7a0b\u5ea6\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u4e0d\u540c\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cVLMs\u5728\u9762\u5bf9\u5177\u4f53\u8f93\u5165\u65f6\u603b\u4f53\u8868\u73b0\u66f4\u4f73\uff0c\u5d4c\u5165\u7a7a\u95f4\u6cbf\u201c\u5177\u4f53\u6027\u201d\u7ef4\u5ea6\u7ed3\u6784\u66f4\u52a0\u6e05\u6670\uff0c\u5177\u4f53\u6027\u6253\u5206\u66f4\u63a5\u8fd1\u4eba\u7c7b\uff0c\u540c\u65f6\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u5e03\u4e5f\u66f4\u7b26\u5408\u611f\u77e5\u57fa\u7840\u7684\u9884\u671f\u3002", "conclusion": "\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4f7f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u91cc\u5c55\u73b0\u51fa\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u5177\u4f53\u6027\u654f\u611f\u6027\u4e0e\u8868\u5f81\uff0c\u8fd9\u79cd\u4f18\u52bf\u4f53\u73b0\u5728\u8f93\u51fa\u3001\u8868\u5f81\u7a7a\u95f4\u548c\u6ce8\u610f\u529b\u673a\u5236\u7b49\u591a\u4e2a\u5c42\u9762\u3002"}}
{"id": "2601.17657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17657", "abs": "https://arxiv.org/abs/2601.17657", "authors": ["Taewan Cho", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation", "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip", "AI": {"tldr": "SPACE-CLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u51bb\u7ed3\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u5e76\u878d\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684CLIP\u867d\u80fd\u7406\u89e3\u8bed\u4e49\uff0c\u4f46\u5bf9\u51e0\u4f55\u7ed3\u6784\u611f\u77e5\u4e0d\u8db3\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4f9d\u9760\u6587\u672c\u63d0\u793a\uff0c\u65b9\u5f0f\u95f4\u63a5\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSPACE-CLIP\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u5177\u6709\u8bed\u4e49\u901a\u8def\u548c\u7ed3\u6784\u901a\u8def\u7684\u53cc\u901a\u8def\u89e3\u7801\u7ed3\u6784\uff0c\u5206\u522b\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u4e0e\u7cbe\u7ec6\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u878d\u5408\u65b9\u5f0f\u5b9e\u73b0\u4fe1\u606f\u7efc\u5408\uff0c\u65e0\u9700\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u6216\u63d0\u793a\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSPACE-CLIP\u5927\u5e45\u8d85\u8d8a\u4ee5\u5f80\u57fa\u4e8eCLIP\u7684\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u53cc\u901a\u8def\u878d\u5408\u5bf9\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "SPACE-CLIP\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f18\u96c5\u7684\u67b6\u6784\u6a21\u677f\uff0c\u53ef\u96c6\u6210\u5230\u4e0b\u4e00\u4ee3\u5177\u8eabAI\u7cfb\u7edf\uff08\u5982\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff09\u4e2d\u3002"}}
{"id": "2601.18077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18077", "abs": "https://arxiv.org/abs/2601.18077", "authors": ["Mahesh Ramesh", "Kaousheik Jayakumar", "Aswinkumar Ramkumar", "Pavan Thodima", "Aniket Rege"], "title": "Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents", "comment": null, "summary": "Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e8617\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4eba\u5361\u724c\u6e38\u620f\u300aHanabi\u300b\u4e2d\u7684\u534f\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u96be\u5ea6\u7684\u63a8\u7406\u573a\u666f\u3002\u901a\u8fc7\u516c\u5f00\u65b0\u6570\u636e\u96c6\u548c\u9488\u5bf9LLM\u7684\u5fae\u8c03\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u5728\u300aHanabi\u300b\u53ca\u6cdb\u5316\u4efb\u52a1\u4e0a\u7684\u534f\u4f5c\u8868\u73b0\u3002", "motivation": "\u534f\u540c\u63a8\u7406\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u573a\u666f\u4e0b\u6781\u5177\u6311\u6218\uff0c\u800c\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u90fd\u96be\u4ee5\u6709\u6548\u534f\u4f5c\u3002\u300aHanabi\u300b\u662f\u590d\u6742\u534f\u4f5c\u63a8\u7406\u7684\u7ecf\u5178\u5b9e\u4f8b\uff0c\u56e0\u6b64\u672c\u6587\u4ee5\u5176\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u6df1\u5165\u7406\u89e3\u548c\u63d0\u5347LLMs\u5728\u590d\u6742\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u6c9f\u901a\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u8bbe\u7f6e\u4e86Watson\u3001Sherlock\u548cMycroft\u4e09\u79cd\u573a\u666f\uff0c\u5206\u522b\u4ee3\u8868\u6700\u5c11\u4e0a\u4e0b\u6587\u3001\u542f\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u3001\u4ee5\u53ca\u591a\u8f6e\u72b6\u6001\u8ffd\u8e2a\uff0c\u5e76\u5bf917\u79cd\u4e0d\u540c\u89c4\u6a21LLM\uff08\u4ece4B\u5230600B+\uff09\u57282-5\u4eba\u6e38\u620f\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u6d4b\u3002\u8fd8\u53d1\u5e03\u4e86\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4ee5\u7528\u4e8e\u6307\u4ee4\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u96c6\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u4e86\u6709\u76d1\u7763\u548cRL\u5fae\u8c03\u5b9e\u9a8c\u3002", "result": "\uff081\uff09LLM\u80fd\u901a\u8fc7\u201c\u5185\u90e8\u5de5\u4f5c\u8bb0\u5fc6\u201d\u8ffd\u8e2a\u6e38\u620f\u72b6\u6001\uff1b\uff082\uff09\u6a21\u578b\u95f4\u4ea4\u4e92\u8868\u73b0\u968f\u81ea\u8eab\u5f3a\u5ea6\u63d0\u5347\u800c\u5e73\u6ed1\u53d8\u5316\uff1b\uff083\uff09\u63a8\u7406\u80fd\u529b\u6700\u5f3a\u7684\u6a21\u578b\u5728\u300aHanabi\u300b\u5f97\u5206\u63a5\u8fd115\u5206\uff0c\u867d\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f46\u4ecd\u663e\u8457\u4f4e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4eba\u7c7b\u548c\u4e13\u7528AI\uff08\u5747\u572820\u5206\u4ee5\u4e0a\uff09\uff1b\uff084\uff09\u57fa\u4e8e\u65b0\u6570\u636e\u96c6\u5fae\u8c03\u540e\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u5408\u4f5c\u8868\u73b0\u4e0a\u83b7\u5f9721%\uff08\u76d1\u7763\uff09\u4e0e156%\uff08RL\uff09\u63d0\u5347\uff0c\u57fa\u672c\u903c\u8fd1\u5f3a\u95ed\u6e90\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u63a8\u7406\u578bLLM\u3002", "conclusion": "\u5f53\u524dLLM\u5df2\u5177\u5907\u4e00\u5b9a\u7a0b\u5ea6\u7684\u534f\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u7cbe\u7ec6\u63d0\u793a\u8bbe\u8ba1\u548c\u6570\u636e\u5fae\u8c03\u53ef\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u6709\u6311\u6218\u6027\u7684\u534f\u4f5c\u4efb\u52a1\u4e0a\u4ecd\u843d\u540e\u4e8e\u4e13\u4e1a\u7cfb\u7edf\u548c\u4eba\u7c7b\u3002\u516c\u5f00\u7684\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u540e\u7eed\u7814\u7a76\u57fa\u7840\u3002RL\u5fae\u8c03\u6a21\u578b\u5728Hanabi\u8303\u7574\u5916\u4efb\u52a1\u4e2d\u4e5f\u6709\u6cdb\u5316\u63d0\u5347\uff0c\u51f8\u663e\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u63a8\u7406\u80fd\u529b\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.17666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17666", "abs": "https://arxiv.org/abs/2601.17666", "authors": ["Xinyue Pan", "Yuhao Chen", "Fengqing Zhu"], "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting", "comment": "Accepted by CAI2026", "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08Prompt Grafting, PG\uff09\uff0c\u7528\u4ee5\u89e3\u51b3\u6587\u751f\u56fe\u6a21\u578b\u5728\u751f\u6210\u591a\u98df\u7269\u56fe\u7247\u65f6\u98df\u7269\u201c\u878d\u5408\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u7a7a\u95f4\u63d0\u793a\u548c\u5e03\u5c40\u5f15\u5bfc\uff0c\u5b9e\u73b0\u591a\u98df\u7269\u9879\u7684\u51c6\u786e\u5206\u79bb\u548c\u53ef\u63a7\u6df7\u5408\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u591a\u98df\u7269\u56fe\u7247\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u98df\u7269\u95f4\u76f8\u4e92\u878d\u5408\uff08\u5982\u7c73\u996d\u548c\u6c64\u6df7\u5408\uff09\uff0c\u5f71\u54cd\u56fe\u7247\u7684\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\u3002\u800c\u9ad8\u8d28\u91cf\u7684\u591a\u98df\u7269\u56fe\u7247\u5bf9\u996e\u98df\u8bc4\u4f30\u3001\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u83dc\u8c31\u53ef\u89c6\u5316\u7b49\u5e94\u7528\u5f88\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u63a7\u5236\u3001\u5206\u79bb\u6216\u878d\u5408\u591a\u98df\u7269\u9879\u3002", "method": "\u63d0\u51fa\u4e86Prompt Grafting\uff08PG\uff09\u6846\u67b6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u5e03\u5c40\u63d0\u793a\u751f\u6210\u4e0d\u540c\u98df\u7269\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u5e03\u5c40\u7a33\u5b9a\u540e\uff0c\u5c06\u76ee\u6807\u6587\u672c\u63d0\u793a\u878d\u5408\uff0c\u5b9e\u73b0\u6307\u5b9a\u98df\u7269\u5728\u6307\u5b9a\u533a\u57df\u7684\u5206\u5e03\u3002\u7528\u6237\u53ef\u901a\u8fc7\u7f16\u8f91\u5e03\u5c40\u5b9e\u73b0\u5206\u79bb\u6216\u6709\u610f\u878d\u5408\u98df\u7269\u3002", "result": "\u5728\u4e24\u4e2a\u98df\u7269\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cPG\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u98df\u7269\u9879\u7684\u51c6\u786e\u5206\u79bb\u7387\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u5b9a\u6027\u6548\u679c\uff0c\u5e76\u80fd\u5b9e\u73b0\u7528\u6237\u53ef\u63a7\u7684\u98df\u7269\u5206\u79bb\u6216\u6df7\u5408\u3002", "conclusion": "Prompt Grafting\u4e3a\u591a\u98df\u7269\u573a\u666f\u4e2d\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u63a7\u5206\u79bb\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u9700\u8981\u591a\u98df\u7269\u6570\u636e\u589e\u5f3a\u6216\u7cbe\u7ec6\u63a7\u5236\u56fe\u7247\u5185\u5bb9\u7684\u4efb\u52a1\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2601.18102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18102", "abs": "https://arxiv.org/abs/2601.18102", "authors": ["Stephanie Fong", "Zimu Wang", "Guilherme C. Oliveira", "Xiangyu Zhao", "Yiwen Jiang", "Jiahe Liu", "Beau-Luke Colton", "Scott Woods", "Martha E. Shenton", "Barnaby Nelson", "Zongyuan Ge", "Dominic Dwyer"], "title": "CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations", "comment": "This paper is accepted at EACL 2026", "summary": "The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CHiRPE\uff0c\u4e00\u4e2a\u4e13\u4e3a\u7cbe\u795e\u75c5\u98ce\u9669\u9884\u6d4b\u8bbe\u8ba1\u4e14\u5f3a\u5316\u4e86\u89e3\u91ca\u6027\u7684NLP\u7ba1\u9053\uff0c\u5728\u591a\u4e2d\u5fc3\u4e34\u5e8a\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u578b\u3001\u4e34\u5e8a\u4e13\u5bb6\u9ad8\u5ea6\u8ba4\u53ef\u7684\u53ef\u89e3\u91ca\u6027\u5c55\u793a\u5f62\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u4e0e\u4e34\u5e8a\u63a8\u7406\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u533b\u751f\u7684\u53c2\u4e0e\uff0c\u5bfc\u81f4NLP\u5de5\u5177\u5728\u533b\u5b66\u9886\u57df\u96be\u4ee5\u88ab\u91c7\u7eb3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u9884\u6d4b\u7cbe\u795e\u75c5\u98ce\u9669\uff0c\u53c8\u80fd\u4ee5\u4e34\u5e8a\u53cb\u597d\u7684\u65b9\u5f0f\u89e3\u91ca\u7ed3\u679c\u7684\u7cfb\u7edf\u3002", "method": "CHiRPE\u7ba1\u9053\u5904\u7406\u534a\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u8bbf\u8c08\u8bb0\u5f55\uff0c\u96c6\u6210\u4e86\u75c7\u72b6\u57df\u6620\u5c04\u3001LLM\u6458\u8981\u53caBERT\u5206\u7c7b\uff0c\u5b9e\u73b0\u7cbe\u795e\u75c5\u98ce\u9669\u9884\u6d4b\u3002\u89e3\u91ca\u6a21\u5757\u91c7\u7528\u4e0e\u4e34\u5e8a\u533b\u751f\u5171\u540c\u5f00\u53d1\u7684SHAP\u65b0\u683c\u5f0f\uff0c\u5e76\u901a\u8fc728\u4f4d\u4e13\u5bb6\u8bc4\u4f30\u5176\u53ef\u7528\u6027\u548c\u504f\u597d\u3002", "result": "\u5728AMP-SCZ\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\uff08944\u4efd\u8bbf\u8c08\u8bb0\u5f55\uff0c24\u5bb6\u56fd\u9645\u8bca\u6240\uff09\u4e0a\uff0cCHiRPE\u5728\u4e09\u79cdBERT\u6a21\u578b\u4e2d\u51c6\u786e\u7387\u5747\u8d8590%\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002\u5176\u521b\u65b0\u7684\u6982\u5ff5\u5f15\u5bfc\u578b\u89e3\u91ca\u65b9\u5f0f\uff08\u7279\u522b\u662f\u56fe\u6587\u7ed3\u5408\u7684\u6458\u8981\u683c\u5f0f\uff09\u88ab\u4e34\u5e8a\u4e13\u5bb6\u5e7f\u6cdb\u8ba4\u53ef\u3002", "conclusion": "\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u534f\u4f5c\u5f00\u53d1\u7684\u6a21\u578b\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u533b\u5b66NLP\u5de5\u5177\u7684\u51c6\u786e\u6027\uff0c\u4e5f\u589e\u5f3a\u4e86\u5176\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u6d4b\u8bd5\u3002"}}
{"id": "2601.17673", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17673", "abs": "https://arxiv.org/abs/2601.17673", "authors": ["Weiyu Zhang", "Yuan Hu", "Yong Li", "Yu Liu"], "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing", "comment": null, "summary": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9065\u611f\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u8868\u73b0\u4e0d\u4e00\u81f4\uff08\u7a7a\u95f4\u53cd\u8f6c\u8bc5\u5492\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Uni-RS\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u5e03\u5c40\u89c4\u5212\u3001\u7a7a\u95f4\u611f\u77e5\u76d1\u7763\u548c\u7a7a\u95f4\u53d8\u6362\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u751f\u6210\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u7684\u9065\u611f\u591a\u6a21\u6001\u6a21\u578b\u867d\u7136\u80fd\u51c6\u786e\u7406\u89e3\u56fe\u50cf\u7269\u4f53\u95f4\u7684\u4f4d\u7f6e\u5173\u7cfb\uff0c\u4f46\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u65f6\uff0c\u5e38\u5e38\u65e0\u6cd5\u5fe0\u5b9e\u8fd8\u539f\u8fd9\u4e9b\u7a7a\u95f4\u5173\u7cfb\uff0c\u8fd9\u5bf9\u9065\u611f\u8bed\u4e49\u6781\u4e3a\u5173\u952e\u3002\u4e3a\u89e3\u51b3\u5728\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u3002", "method": "1. \u7a7a\u95f4\u5e03\u5c40\u89c4\u5212\uff1a\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6210\u660e\u786e\u7684\u7a7a\u95f4\u5e03\u5c40\u8ba1\u5212\uff0c\u5b9e\u73b0\u51e0\u4f55\u89c4\u5212\u548c\u89c6\u89c9\u5408\u6210\u89e3\u8026\u30022. \u7a7a\u95f4\u611f\u77e5\u67e5\u8be2\u76d1\u7763\uff1a\u663e\u5f0f\u5f15\u5bfc\u53ef\u5b66\u4e60\u7684queries\u5173\u6ce8\u6307\u4ee4\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u30023. \u56fe\u6587\u7a7a\u95f4\u53d8\u6362\u589e\u5f3a\uff1a\u901a\u8fc7\u4e00\u81f4\u6027\u7684\u7a7a\u95f4\u53d8\u6362\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u7a7a\u95f4\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\uff0cUni-RS\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u63cf\u8ff0\u3001\u89c6\u89c9\u5b9a\u4f4d\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u4e5f\u4fdd\u6301\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "Uni-RS\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u591a\u6a21\u6001\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u5931\u771f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7406\u89e3\u548c\u751f\u6210\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4e3a\u9065\u611f\u591a\u6a21\u6001\u4efb\u52a1\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18106", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18106", "abs": "https://arxiv.org/abs/2601.18106", "authors": ["Jiatan Huang", "Zheyuan Zhang", "Tianyi Ma", "Mingchen Li", "Yaning Zheng", "Yanfang Ye", "Chuxu Zhang"], "title": "GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health", "comment": null, "summary": "Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GLEN-Bench\u2014\u2014\u9996\u4e2a\u57fa\u4e8e\u56fe\u548c\u8bed\u8a00\u7684\u7efc\u5408\u6027\u8425\u517b\u5065\u5eb7\u8bc4\u4f30\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u8425\u517b\u5e72\u9884\u9886\u57df\u4e2a\u6027\u5316\u996e\u98df\u5efa\u8bae\u548c\u6a21\u578b\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u4e2a\u6027\u5316\u8425\u517b\u5e72\u9884\u7684\u8ba1\u7b97\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a1\uff09\u996e\u98df\u6a21\u5f0f\u7814\u7a76\u5ffd\u7565\u4e86\u73b0\u5b9e\u56e0\u7d20\u5982\u7ecf\u6d4e\u6761\u4ef6\u3001\u5171\u75c5\u53ca\u98df\u7269\u53ef\u5f97\u6027\uff1b2\uff09\u63a8\u8350\u7cfb\u7edf\u7f3a\u4e4f\u9488\u5bf9\u63a8\u8350\u539f\u56e0\u7684\u89e3\u91ca\uff1b3\uff09\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u8bc4\u4ef7\u591a\u4efb\u52a1\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5f25\u8865\u8fd9\u4e9b\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6574\u5408NHANES\u5065\u5eb7\u6570\u636e\u3001FNDDS\u98df\u7269\u6210\u5206\u6570\u636e\u548cUSDA\u98df\u54c1\u53ef\u5f97\u6027\u6307\u6807\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4eba\u53e3\u3001\u5065\u5eb7\u3001\u996e\u98df\u3001\u8425\u517b\u3001\u793e\u4f1a\u7ecf\u6d4e\u7b49\u591a\u7ef4\u5ea6\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u63d0\u51fa\u4e09\u4e2a\u4efb\u52a1\uff1a\u98ce\u9669\u68c0\u6d4b\u3001\u4e2a\u6027\u5316\u591a\u7ea6\u675f\u996e\u98df\u63a8\u8350\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u3002\u540c\u65f6\uff0c\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6df7\u5408\u67b6\u6784\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u8fd9\u4e00\u5177\u4f53\u573a\u666f\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u548c\u8bc4\u4ef7\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u996e\u98df\u6a21\u5f0f\u4e0e\u5065\u5eb7\u98ce\u9669\u7684\u5173\u8054\uff0c\u5e76\u80fd\u5728\u591a\u7ea6\u675f\u4e0b\u7ed9\u51fa\u5408\u7406\u7684\u996e\u98df\u5efa\u8bae\uff0c\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u7b54\u590d\u3002\u5206\u6790\u8fd8\u660e\u786e\u4e86\u4e0d\u540c\u6a21\u578b\u8bbe\u8ba1\u5bf9\u4efb\u52a1\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "GLEN-Bench\u4e3a\u8425\u517b\u5065\u5eb7\u9886\u57df\u7684\u6548\u679c\u8bc4\u4f30\u6811\u7acb\u4e86\u7edf\u4e00\u7684\u9ad8\u6807\u51c6\uff0c\u4fc3\u8fdb\u4e86\u5b9e\u9645\u573a\u666f\u4e0b\u591a\u4efb\u52a1\u6a21\u578b\u7684\u8bbe\u8ba1\u4e0e\u843d\u5730\uff0c\u4e3a\u4eca\u540e\u7684\u4e2a\u6027\u5316\u8425\u517b\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.17697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17697", "abs": "https://arxiv.org/abs/2601.17697", "authors": ["Zexi Jia", "Jinchao Zhang", "Jie Zhou"], "title": "StyleDecoupler: Generalizable Artistic Style Disentanglement", "comment": null, "summary": "Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStyleDecoupler\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u827a\u672f\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u89e3\u8026\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u827a\u672f\u98ce\u683c\u6570\u636e\u96c6WeART\u3002\u65b9\u6cd5\u5728\u98ce\u683c\u68c0\u7d22\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u6548\u679c\u3002", "motivation": "\u827a\u672f\u98ce\u683c\u548c\u8bed\u4e49\u5185\u5bb9\u9ad8\u5ea6\u7ea0\u7f20\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5206\u79bb\u4e24\u8005\uff0c\u5bfc\u81f4\u98ce\u683c\u8bc6\u522b\u548c\u76f8\u5173\u4efb\u52a1\u53d7\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u51fa\u80fd\u591f\u72ec\u7acb\u63d0\u53d6\u827a\u672f\u98ce\u683c\u7279\u5f81\u7684\u6280\u672f\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u578b\uff08\u5305\u542b\u98ce\u683c\u548c\u5185\u5bb9\u4fe1\u606f\uff09\u4e0e\u5355\u6a21\u6001\u6a21\u578b\uff08\u503e\u5411\u4e8e\u4ec5\u7f16\u7801\u5185\u5bb9\uff09\u8868\u8fbe\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u901a\u8fc7\u6700\u5c0f\u5316\u4e92\u4fe1\u606f\u6765\u5206\u79bb\u7eaf\u7cb9\u7684\u98ce\u683c\u7279\u5f81\u3002StyleDecoupler\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u65e0\u9700\u5bf9\u51bb\u7ed3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728WeART\u548cWikiART\u4e24\u4e2a\u827a\u672f\u98ce\u683c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cStyleDecoupler\u5728\u98ce\u683c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u540c\u65f6\u652f\u6301\u98ce\u683c\u5173\u7cfb\u6620\u5c04\u548c\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u7b49\u5e94\u7528\u3002", "conclusion": "StyleDecoupler\u4e3a\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u89e3\u8026\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u827a\u672f\u98ce\u683c\u5206\u6790\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u6b64\u5916\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6WeART\u7684\u53d1\u5e03\u6709\u52a9\u4e8e\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2601.18116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18116", "abs": "https://arxiv.org/abs/2601.18116", "authors": ["Lin Sun", "Linglin Zhang", "Jingang Huang", "Change Jia", "Zhengwei Cheng", "Xiangzheng Zhang"], "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning", "comment": null, "summary": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u6846\u67b6FABLE\uff0c\u80fd\u591f\u6709\u6548\u878d\u5408\u957f\u4e0a\u4e0b\u6587LLM\u7684\u4f18\u52bf\u548c\u7ed3\u6784\u5316\u68c0\u7d22\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u8bf8\u591a\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587LLM\u867d\u7136\u80fd\u529b\u63d0\u5347\uff0c\u4f46\u5b58\u5728\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u591a\u6587\u6863\u63a8\u7406\u6269\u5c55\u6027\u5dee\u7b49\u5c40\u9650\uff1b\u800c\u4f20\u7edfRAG\u673a\u5236\u9ad8\u6548\u4f46\u68c0\u7d22\u7c92\u5ea6\u7c97\u7cd9\u3001\u8bed\u4e49\u566a\u58f0\u5927\uff0c\u96be\u4ee5\u5b9e\u73b0\u7ed3\u6784\u5316\u8de8\u6587\u6863\u8bc1\u636e\u6574\u5408\u3002\u4e9f\u9700\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6848\u3002", "method": "\u63d0\u51faFABLE\u6846\u67b6\uff1a\u57fa\u4e8e\u68ee\u6797\u7ed3\u6784\u7684\u81ea\u9002\u5e94\u53cc\u8def\u5f84LLM\u5f3a\u5316\u68c0\u7d22\u3002\u5177\u4f53\u505a\u6cd5\u5305\u62ec\uff1a1\uff09\u901a\u8fc7LLM\u6784\u5efa\u591a\u7c92\u5ea6\u8bed\u4e49\u5206\u5c42\u68ee\u6797\u7d22\u5f15\uff1b2\uff09\u5229\u7528LLM\u5f15\u5bfc\u7684\u5206\u5c42\u904d\u5386\u548c\u7ed3\u6784\u611f\u77e5\u7684\u8bc1\u636e\u4f20\u64ad\uff0c\u53cc\u8def\u5f84\u534f\u540c\u83b7\u53d6\u7cbe\u7ec6\u5316\u8bc1\u636e\uff1b3\uff09\u663e\u5f0f\u9884\u7b97\u63a7\u5236\u5b9e\u73b0\u6548\u7387\u548c\u6548\u679c\u95f4\u7684\u81ea\u9002\u5e94\u6743\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFABLE\u5728\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u4efb\u52a1\u4e0a\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u6700\u4f18RAG\u7cfb\u7edf\uff0c\u5e76\u5728\u663e\u8457\u51cf\u5c11\uff08\u6700\u9ad894%\uff09Token\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u5b8c\u6574\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587LLM\u867d\u6709\u63d0\u5347\uff0c\u4f46\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u7ed3\u6784\u5316\u68c0\u7d22\u3002FABLE\u4f5c\u4e3a\u4e00\u79cd\u878d\u5408\u5f0f\u6846\u67b6\uff0c\u517c\u5177\u51c6\u786e\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u5c06\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u8bc1\u636e\u6574\u5408\u6709\u673a\u7ed3\u5408\uff0c\u4e3aRAG\u7cfb\u7edf\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.17703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17703", "abs": "https://arxiv.org/abs/2601.17703", "authors": ["Nikhil Kadivar", "Guansheng Li", "Jianlu Zheng", "John M. Higgins", "Ming Dao", "George Em Karniadakis", "Mengjia Xu"], "title": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays", "comment": null, "summary": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9570\u72b6\u7ec6\u80de\u52a8\u6001\u53d8\u5316\u7684\u9ad8\u6548\u8bc6\u522b\u548c\u5b9a\u91cf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u5bc6\u5ea6\u548c\u91cd\u53e0\u7ec6\u80de\u7684\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u7ed3\u5408AI\u8f85\u52a9\u7684\u6807\u6ce8\u3001\u5206\u5272\u3001\u5206\u7c7b\u548c\u8ba1\u6570\u7b97\u6cd5\uff0c\u6781\u5927\u63d0\u5347\u4e86\u65f6\u5e8f\u663e\u5fae\u56fe\u50cf\u4e2d\u7ea2\u7ec6\u80de\u7684\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5206\u6790\u548c\u7406\u89e3\u9570\u72b6\u7ec6\u80de\u5728\u4e0d\u540c\u751f\u7269\u7269\u7406\u6761\u4ef6\u4e0b\u7684\u5f62\u6001\u53d8\u5316\u5bf9\u4e8e\u75be\u75c5\u673a\u7406\u7814\u7a76\u548c\u836f\u7269\u7597\u6548\u8bc4\u4f30\u5341\u5206\u5173\u952e\uff0c\u7136\u800c\u9ad8\u5bc6\u5ea6\u548c\u7ec6\u80de\u91cd\u53e0\u6781\u5927\u589e\u52a0\u4e86\u81ea\u52a8\u5316\u5b9a\u91cf\u7684\u96be\u5ea6\u3002", "method": "\u91c7\u7528Roboflow\u5e73\u53f0\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u7528nnU-Net\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7ec6\u80de\u5206\u5272\uff0c\u540c\u65f6\u5f15\u5165\u5206\u6c34\u5cad\u7b97\u6cd5\u89e3\u51b3\u7ec6\u80de\u91cd\u53e0\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u7684\u7ec6\u80de\u5206\u7c7b\u4e0e\u8ba1\u6570\u3002\u6574\u5957\u6d41\u7a0b\u9ad8\u5ea6\u81ea\u52a8\u5316\uff0c\u5bf9\u6807\u6ce8\u6570\u636e\u9700\u6c42\u4f4e\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u7ec6\u80de\u5206\u5272\u548c\u5b9a\u91cf\u3002\u80fd\u591f\u6709\u6548\u8ffd\u8e2a\u7ea2\u7ec6\u80de\u52a8\u6001\u53d8\u5316\uff0c\u8d85\u8fc72\u500d\u63d0\u5347\u5b9e\u9a8c\u901a\u91cf\uff0c\u4e14\u53ef\u5206\u8fa8\u836f\u7269\u4f5c\u7528\u4e0b\u7684\u7ec6\u80de\u5f62\u6001\u6f14\u5316\u7279\u5f81\u3002", "conclusion": "\u63d0\u51fa\u7684AI\u9a71\u52a8\u81ea\u52a8\u5316\u5206\u6790\u5e73\u53f0\u4e0d\u4ec5\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u7ea2\u7ec6\u80de\u52a8\u529b\u5b66\u7814\u7a76\u6548\u7387\uff0c\u8fd8\u4e3a\u7ec6\u80de\u529b\u5b66\u751f\u7269\u673a\u5236\u89e3\u6790\u4e0e\u836f\u7269\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2601.18129", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18129", "abs": "https://arxiv.org/abs/2601.18129", "authors": ["Kunat Pipatanakul", "Pittawat Taveekitworachai"], "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models", "comment": "19 pages. Code is publicly available at https://github.com/scb-10x/typhoon-s . Datasets and model weights are available at https://huggingface.co/collections/typhoon-ai/typhoon-s", "summary": "Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u3001\u5f00\u653e\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5728\u8d44\u6e90\u6709\u9650\u53ca\u9700\u6c42\u4e3b\u6743\u7684\u73af\u5883\u4e0b\uff0c\u4e5f\u80fd\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u533a\u57df\u6027\u6216\u4e3b\u6743\u5927\u6a21\u578b\u3002\u901a\u8fc7\u7528\u6cf0\u8bed\u6848\u4f8b\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u8d44\u6e90\u4e0b\u83b7\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5927\u6a21\u578b\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e14\u7531\u5c11\u6570\u673a\u6784\u638c\u63a7\uff0c\u4e0d\u5229\u4e8e\u5730\u533a\u673a\u6784\u5728\u6709\u9650\u8d44\u6e90\u3001\u9700\u8981\u900f\u660e\u548c\u81ea\u4e3b\u63a7\u5236\u7684\u60c5\u5883\u4e0b\u4f7f\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u5730\u533a\u4e3b\u6743\u548c\u53ef\u7528\u6027\u9700\u6c42\uff0c\u9700\u5f00\u53d1\u4f4e\u95e8\u69db\u3001\u6613\u9002\u5e94\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6848\u3002", "method": "\u63d0\u51faTyphoon S\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u3001on-policy\u84b8\u998f\u548c\u5c0f\u89c4\u6a21RFT\uff08\u57fa\u4e8eInK-GRPO\uff09\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e3b\u6743\u548c\u901a\u7528\u80fd\u529b\u517c\u5907\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6307\u4ee4\u8bed\u6599\u6216\u590d\u6742\u5de5\u5177\u94fe\u3002\u7528\u6cf0\u8bed\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTyphoon S\u65b9\u6848\u53ef\u5c06\u57fa\u7840\u6a21\u578b\u8f6c\u6362\u4e3a\u5177\u5907\u5f3a\u6cdb\u5316\u6027\u80fd\u7684\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u3002\u5728\u6cf0\u8bed\u6cd5\u5f8b\u63a8\u7406\u53ca\u672c\u5730\u77e5\u8bc6\u4e0a\u901a\u8fc7\u5c0f\u89c4\u6a21RFT\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u53ea\u9700\u5c0f\u89c4\u6a21\u3001\u9ad8\u6548\u540e\u8bad\u7ec3\u6d41\u7a0b\u5373\u53ef\u652f\u6491\u9ad8\u8d28\u91cf\u4e3b\u6743\u5927\u6a21\u578b\u8bad\u7ec3\uff0c\u6709\u6548\u964d\u4f4e\u5bf9\u5927\u6307\u4ee4\u6570\u636e\u548c\u7b97\u529b\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b66\u672f\u53ca\u5730\u533a\u5e94\u7528\u63d0\u4f9b\u4e86\u73b0\u5b9e\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.17720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17720", "abs": "https://arxiv.org/abs/2601.17720", "authors": ["Ting-Hsun Chi", "Chu-Rong Chen", "Chi-Tun Hsu", "Hsuan-Ting Lin", "Sheng-Yu Huang", "Cheng Sun", "Yu-Chiang Frank Wang"], "title": "Advancing Structured Priors for Sparse-Voxel Surface Reconstruction", "comment": null, "summary": "Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6e85\u5c04\u548c\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u4f18\u70b9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4f53\u7d20\u521d\u59cb\u5316\u6280\u672f\u548c\u6df1\u5ea6\u51e0\u4f55\u76d1\u7763\uff0c\u63d0\u5347\u4e86\u8f90\u5c04\u573a\u91cd\u5efa\u8868\u9762\u7cbe\u5ea6\u3001\u7ec6\u8282\u6062\u590d\u4e0e\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6e85\u5c04\u4e0e\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u5404\u6709\u4f18\u7f3a\u70b9\uff1a\u524d\u8005\u6536\u655b\u5feb\u4f46\u7ec6\u8282\u6709\u9650\uff0c\u540e\u8005\u51e0\u4f55\u6e05\u6670\u4f46\u521d\u59cb\u5316\u4ee3\u4ef7\u9ad8\u3001\u6536\u655b\u6162\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u4e24\u8005\u4f18\u70b9\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u8868\u9762\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u4e00\u662f\u63d0\u51fa\u4f53\u7d20\u521d\u59cb\u5316\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u4f53\u7d20\u667a\u80fd\u5206\u5e03\u5728\u5408\u7406\u4f4d\u7f6e\u5e76\u5339\u914d\u5408\u9002\u7ec6\u8282\u5c42\u7ea7\uff0c\u4f18\u5316\u7a00\u758f\u4f53\u7d20\u7684\u8d77\u59cb\u72b6\u6001\uff0c\u4fbf\u4e8e\u540e\u7eed\u4f18\u5316\uff1b\u4e8c\u662f\u63d0\u51fa\u7ec6\u5316\u7684\u6df1\u5ea6\u51e0\u4f55\u76d1\u7763\uff0c\u5c06\u591a\u89c6\u89d2\u4fe1\u606f\u8f6c\u6362\u4e3a\u9010\u5c04\u7ebf\u7684\u6df1\u5ea6\u6b63\u5219\uff0c\u63d0\u9ad8\u6df1\u5ea6\u4e00\u81f4\u6027\u4e14\u4e0d\u6a21\u7cca\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u51c6\u786e\u6027\u3001\u7ec6\u8282\u6062\u590d\u548c\u8868\u9762\u5b8c\u6574\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6536\u655b\u901f\u5ea6\u4e5f\u5f97\u4ee5\u4fdd\u6301\u5feb\u901f\u3002", "conclusion": "\u901a\u8fc7\u878d\u54083D\u9ad8\u65af\u6e85\u5c04\u4e0e\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u6709\u6548\u7684\u521d\u59cb\u5316\u4e0e\u76d1\u7763\u624b\u6bb5\uff0c\u80fd\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u8f90\u5c04\u573a\u8868\u9762\u91cd\u5efa\u3002"}}
{"id": "2601.18162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18162", "abs": "https://arxiv.org/abs/2601.18162", "authors": ["Ani Harutyunyan", "Sachin Kumar"], "title": "Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models", "comment": null, "summary": "Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.", "AI": {"tldr": "\u672c\u6587\u5728GoEmotions\u6570\u636e\u96c6\u4e0a\u5bf9\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u8fdb\u884c\u4e86\u4e09\u79cd\u4e3b\u6d41\u6a21\u578b\uff08TF-IDF\u903b\u8f91\u56de\u5f52\u3001BiLSTM\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u3001BERT\u591a\u6807\u7b7e\u5206\u7c7b\uff09\u7684\u7cfb\u7edf\u6027\u5bf9\u6bd4\u3002BERT\u7684\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u5b98\u65b9\u57fa\u7ebf\uff0c\u5bf9\u7a00\u6709/\u6a21\u7cca\u60c5\u611f\u66f4\u6709\u6548\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u9762\u4e34\u6807\u7b7e\u91cd\u53e0\u548c\u7c7b\u522b\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u4f18\u52bf\u9700\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u5728GoEmotions\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u4e09\u7c7b\u6a21\u578b\uff08\u57fa\u4e8eTF-IDF\u7684\u903b\u8f91\u56de\u5f52+\u4e8c\u5143\u76f8\u5173\u6027\u3001\u5e26\u6ce8\u610f\u529b\u7684BiLSTM\u3001\u5fae\u8c03\u7684BERT\uff09\u8fdb\u884c\u4e86\u591a\u6807\u7b7e\u5206\u7c7b\u5bf9\u6bd4\uff0c\u91c7\u7528\u9006\u9891\u6743\u91cd\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecMicro-F1\u3001Macro-F1\u3001Hamming Loss\u548cSubset Accuracy\u3002", "result": "\u903b\u8f91\u56de\u5f52\u6a21\u578bMicro-F1\u6700\u9ad8\uff080.51\uff09\uff0cBERT\u5728Macro-F1\uff080.49\uff09\u3001Hamming Loss\uff080.036\uff09\u548cSubset Accuracy\uff080.36\uff09\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4f18\u4e8e\u5b98\u65b9\u62a5\u544a\u3002", "conclusion": "\u9ad8\u9891\u60c5\u611f\u4e3b\u8981\u4f9d\u9760\u8868\u9762\u8bcd\u6c47\u7279\u5f81\uff0c\u800cBERT\u7b49\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u4f4e\u9891/\u6a21\u7cca\u60c5\u611f\u8bc6\u522b\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u6574\u4f53\u4e0a\u63d0\u5347\u4e86\u8868\u73b0\u3002"}}
{"id": "2601.17723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17723", "abs": "https://arxiv.org/abs/2601.17723", "authors": ["Tayyab Nasir", "Daochang Liu", "Ajmal Mian"], "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study", "comment": null, "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.", "AI": {"tldr": "\u672c\u6587\u5bf9\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u65b9\u6cd5\u5728\u4efb\u610f\u5c3a\u5ea6\u56fe\u50cf\u8d85\u5206\u8fa8\uff08ASSR\uff09\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u4e86\u8bad\u7ec3\u914d\u7f6e\u3001\u635f\u5931\u8bbe\u8ba1\u7b49\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u76ee\u524dINR\u5df2\u6210\u4e3aASSR\u4efb\u52a1\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u73b0\u6709\u65b9\u6cd5\u7cfb\u7edf\u6027\u7684\u5b9e\u8bc1\u5bf9\u6bd4\u548c\u5bf9\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u7f29\u653e\u6cd5\u5219\u3001\u76ee\u6807\u8bbe\u8ba1\u3001\u4f18\u5316\u7b56\u7565\u7b49\uff09\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u57fa\u51c6\u6807\u51c6\u6765\u660e\u786e\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u63ed\u793a\u74f6\u9888\u4e0e\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\u548c\u4ee3\u7801\u5e93\uff0c\u6c47\u603b\u5e76\u5bf9\u6bd4\u4e86\u591a\u79cdINR\u65b9\u6cd5 under \u5404\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u805a\u5408\u6027\u80fd\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u8bad\u7ec3\u914d\u7f6e\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\uff08\u60e9\u7f5a\u5f3a\u5ea6\u53d8\u5316\u3001\u4f46\u4fdd\u7559\u8fb9\u7f18\u548c\u7eb9\u7406\u7ec6\u8282\uff09\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a\uff081\uff09\u65b0\u578b\u590d\u6742\u7684INR\u65b9\u6cd5\u76f8\u8f83\u65e9\u671f\u65b9\u6cd5\u63d0\u5347\u6709\u9650\uff1b\uff082\uff09\u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u914d\u7f6e\u5f3a\u76f8\u5173\uff0c\u8fd9\u4e00\u70b9\u88ab\u4ee5\u5f80\u5ffd\u89c6\uff1b\uff083\uff09\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u7eb9\u7406\u91cd\u5efa\uff0c\u5bf9\u611f\u77e5\u8d28\u91cf\u6709\u79ef\u6781\u5f71\u54cd\uff1b\uff084\uff09INR\u65b9\u6cd5\u540c\u6837\u670d\u4ece\u7f29\u653e\u6cd5\u5219\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u548c\u6570\u636e\u591a\u6837\u6027\u63d0\u5347\u5e26\u6765\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u7cfb\u7edf\u5b9e\u8bc1\u8868\u660e\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u66f4\u5173\u6ce8\u8bad\u7ec3\u914d\u7f6e\u548c\u635f\u5931\u8bbe\u8ba1\uff0c\u800c\u4e0d\u662f\u4ec5\u8ffd\u6c42\u66f4\u590d\u6742\u6a21\u578b\u7ed3\u6784\u3002\u540c\u65f6\uff0c\u7edf\u4e00\u57fa\u51c6\u548c\u4ee3\u7801\u5e93\u6709\u52a9\u4e8e\u793e\u533a\u516c\u6b63\u8bc4\u6d4b\u3002INR\u6a21\u578b\u867d\u7136\u6709\u8fdb\u6b65\u7a7a\u95f4\uff0c\u4f46\u63d0\u5347\u5df2\u8d8b\u4e8e\u74f6\u9888\uff0c\u7a81\u7834\u9700\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18204", "abs": "https://arxiv.org/abs/2601.18204", "authors": ["Juexiang Ye", "Xue Li", "Xinyu Yang", "Chengkai Huang", "Lanshun Nie", "Lina Yao", "Dechen Zhan"], "title": "MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning", "comment": null, "summary": "Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\\% compared to long-context baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MemWeaver\uff0c\u4e00\u79cd\u9762\u5411\u957f\u65f6\u5e8f\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u539f\u59cb\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4\u548c\u65f6\u5e8f\u63a8\u7406\u51c6\u786e\u5ea6\uff0c\u5e76\u5927\u5e45\u51cf\u5c0f\u4e86\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u4ee3\u7406\u5728\u957f\u65f6\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u53ef\u591a\u8df3\u63a8\u7406\u548c\u8bc1\u636e\u6eaf\u6e90\u80fd\u529b\uff0c\u901a\u5e38\u4f9d\u8d56\u65e0\u7ed3\u6784\u68c0\u7d22\uff0c\u5bfc\u81f4\u63a8\u7406\u8106\u5f31\u3001\u65f6\u5e8f\u51b2\u7a81\u548c\u6eaf\u6e90\u6027\u5dee\u3002", "method": "MemWeaver\u5c06\u957f\u671f\u7ecf\u9a8c\u538b\u7f29\u6574\u5408\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a1\uff09\u65f6\u5e8f\u56fe\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u7684\u5173\u7cfb\u63a8\u7406\uff1b2\uff09\u7ecf\u9a8c\u8bb0\u5fc6\uff0c\u62bd\u8c61\u591a\u6b21\u89c2\u6d4b\u4e2d\u7684\u89c4\u5f8b\uff1b3\uff09\u539f\u6587\u7247\u6bb5\u8bb0\u5fc6\uff0c\u5b58\u50a8\u539f\u59cb\u6587\u672c\u8bc1\u636e\u3002\u540c\u65f6\u91c7\u7528\u53cc\u901a\u9053\u68c0\u7d22\uff0c\u540c\u65f6\u83b7\u5f97\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u8bc1\u636e\u4ee5\u6784\u5efa\u7cbe\u70bc\u4e14\u4fe1\u606f\u5bc6\u96c6\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u4e0a\uff0cMemWeaver\u5728\u591a\u6b65\u548c\u65f6\u5e8f\u63a8\u7406\u6b63\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u57fa\u7ebf\u6cd5\uff0c\u5e76\u5c06\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u7f29\u77ed\u4e8695%\u4ee5\u4e0a\u3002", "conclusion": "MemWeaver\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u4e14\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u5927\u6a21\u578b\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4e3a\u957f\u65f6\u5e8f\u4efb\u52a1\u7684\u8ffd\u6eaf\u6027\u548c\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.17733", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17733", "abs": "https://arxiv.org/abs/2601.17733", "authors": ["Junran Lu", "Yuanqi Li", "Hengji Li", "Jie Guo", "Yanwen Guo"], "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles", "comment": null, "summary": "Boundary Representation (B-Rep) is the widely adopted standard\n  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.\n  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684B-Rep\uff08\u8fb9\u754c\u8868\u793a\uff09\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06B-Rep\u8868\u8ff0\u4e3a\u7531\u4e0d\u540c\u7ef4\u5ea6\u51e0\u4f55\u5355\u5143\uff08k-cell\uff09\u7ec4\u6210\u7684\u7c92\u5b50\u96c6\u5408\uff0c\u901a\u8fc7\u7c92\u5b50\u5171\u4eab\u5b9e\u73b0\u66f4\u7d27\u5bc6\u7684\u51e0\u4f55\u8026\u5408\uff0c\u5927\u5e45\u63d0\u5347\u4e86CAD\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u7f16\u8f91\u6027\u3002", "motivation": "\u73b0\u6709B-Rep\u751f\u6210\u65b9\u6cd5\u56e0\u5176\u5c42\u7ea7\u7ed3\u6784\u590d\u6742\u3001\u51e0\u4f55\u5173\u7cfb\u8026\u5408\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4e0d\u540c\u7ef4\u5ea6\u5355\u5143\u95f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u7f16\u8f91\u6027\u53d7\u9650\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65b0\u8868\u8ff0\u65b9\u5f0f\uff0c\u80fd\u7edf\u4e00\u5904\u7406\u62d3\u6251\u4e0e\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u901a\u7528\u6027\u3002", "method": "\u4f5c\u8005\u5c06\u4f20\u7edfB-Rep\u5206\u89e3\u4e3a\u4e00\u7ec4\u53ef\u7ec4\u5408\u7684k-cell\u7c92\u5b50\uff0c\u6bcf\u4e2a\u62d3\u6251\u5355\u5143\u7531\u591a\u4e2a\u7c92\u5b50\u7ec4\u6210\uff0c\u76f8\u90bb\u5355\u5143\u5728\u754c\u9762\u5171\u4eab\u76f8\u540c\u6f5c\u53d8\u91cf\uff0c\u52a0\u5f3a\u5c40\u90e8\u51e0\u4f55\u7ea6\u675f\u3002\u901a\u8fc7\u591a\u6a21\u6001\u6d41\u5339\u914d\u6846\u67b6\u5bf9\u7c92\u5b50\u96c6\u5408\u8fdb\u884c\u5efa\u6a21\uff0c\u5b9e\u73b0\u65e0\u6761\u4ef6\u751f\u6210\u548c\u6761\u4ef6\u4efb\u52a1\uff08\u5982\u5355\u89c6\u56fe/\u70b9\u4e91\u91cd\u5efa\uff09\u3002\u8be5\u8868\u8ff0\u652f\u6301\u5c40\u90e8\u4fee\u8865\u3001\u975e\u6d41\u5f62\u7ed3\u6784\uff08\u5982\u7ebf\u6846\uff09\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684CAD\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6709\u6548\u6027\u548c\u53ef\u7f16\u8f91\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u9ad8\u4fdd\u771f\u5730\u8fd8\u539f\u548c\u751f\u6210\u590d\u6742\u7ed3\u6784\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u7a81\u7834\u4e86B-Rep\u751f\u6210\u9886\u57df\u7684\u5c42\u7ea7\u8026\u5408\u5f0a\u7aef\uff0c\u7edf\u4e00\u4e86\u62d3\u6251\u4e0e\u51e0\u4f55\u5efa\u6a21\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u5bf9CAD\u6570\u636e\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u975e\u6d41\u5f62\u548c\u5c40\u90e8\u7f16\u8f91\u7b49\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u76f4\u63a5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18238", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18238", "abs": "https://arxiv.org/abs/2601.18238", "authors": ["Tafazzul Nadeem", "Bhavik Shangari", "Manish Rai", "Gagan Raj Gupta", "Ashutosh Modi"], "title": "TechING: Towards Real World Technical Image Understanding via VLMs", "comment": "Accepted at Findings of EACL 2026, 30 Pages (9 Pages main paper + 4 pages references + 17 pages appendix)", "summary": "Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u6280\u672f\u56fe\u50cf\u5927\u89c4\u6a21\u8bad\u7ec3VLM\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u7f16\u8f91\u624b\u7ed8\u6280\u672f\u56fe\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u6280\u672f\u4eba\u5458\u5728\u4ea4\u6d41\u65f6\u5e38\u7528\u624b\u7ed8\u56fe\uff0c\u4f46\u8fd9\u4e9b\u56fe\u540e\u671f\u6570\u5b57\u5316\u4fee\u6539\u5f88\u9ebb\u70e6\u3002\u73b0\u6709VLM\u867d\u7136\u5728\u56fe\u7247\u7406\u89e3\u4e0a\u6709\u8fdb\u6b65\uff0c\u4f46\u5bf9\u6280\u672f\u56fe\u7406\u89e3\u8f83\u5dee\uff0c\u4e14\u5927\u91cf\u771f\u5b9e\u624b\u7ed8\u56fe\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u4f5c\u8005\u5408\u6210\u4e86\u4e00\u5927\u6279\u62df\u771f\u624b\u7ed8\u6280\u672f\u56fe\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u79cd\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u7ec8\u5728Llama 3.2 11B-instruct\u6a21\u578b\u57fa\u7840\u4e0a\u5fae\u8c03\uff0c\u5f97\u5230\u540d\u4e3aLLama-VL-TUG\u7684\u65b0\u6a21\u578b\u3002", "result": "LLama-VL-TUG\u6a21\u578b\u5728\u5408\u6210\u56fe\u4e0aROUGE-L\u6307\u6807\u6bd4\u539f\u6a21\u578b\u63d0\u53472.14\u500d\uff0c\u5728\u771f\u5b9e\u624b\u7ed8\u56fe\u4e0aF1\u5206\u6570\u63d0\u53476.97\u500d\uff0c\u5e76\u5728\u591a\u6570\u7c7b\u578b\u56fe\u4e2d\u8868\u73b0\u51fa\u6700\u5c11\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u5168\u9762\u8d85\u8d8a\u5404\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u7ed3\u5408\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u5347VLM\u5bf9\u6280\u672f\u624b\u7ed8\u56fe\u7684\u7406\u89e3\u548c\u6570\u5b57\u5316\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u4e0e\u9ad8\u6027\u80fd\u6a21\u578b\u3002"}}
{"id": "2601.17737", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17737", "abs": "https://arxiv.org/abs/2601.17737", "authors": ["Chenyu Mu", "Xin He", "Qu Yang", "Wanshun Chen", "Jiadi Yao", "Huang Liu", "Zihao Yi", "Bo Zhao", "Xingyu Chen", "Ruotian Ma", "Fanghua Ye", "Erkun Yang", "Cheng Deng", "Zhaopeng Tu", "Xiaolong Li", "Linus"], "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "comment": null, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u5bf9\u8bdd\u6587\u672c\u76f4\u63a5\u751f\u6210\u8fde\u8d2f\u7684\u7535\u5f71\u7ea7\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4e0e\u811a\u672c\u7684\u4e00\u81f4\u6027\u4e0e\u65f6\u5e8f\u8fde\u8d2f\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u751f\u6210\u89c6\u9891\u6a21\u578b\u867d\u7136\u80fd\u591f\u751f\u6210\u89c6\u89c9\u9707\u64bc\u7684\u77ed\u89c6\u9891\uff0c\u4f46\u9762\u5bf9\u957f\u7bc7\u5bf9\u8bdd\u548c\u590d\u6742\u53d9\u4e8b\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u628a\u521b\u610f\u60f3\u6cd5\u8f6c\u5316\u4e3a\u8fde\u8d2f\u7684\u7535\u5f71\u53d9\u4e8b\uff0c\u5b58\u5728\u201c\u8bed\u4e49\u9e3f\u6c9f\u201d\u3002\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u7528\u5bf9\u8bdd\u7b49\u9ad8\u5c42\u6b21\u8868\u8fbe\u751f\u6210\u957f\u7bc7\u3001\u8fde\u8d2f\u5e76\u5fe0\u4e8e\u5267\u672c\u7684\u89c6\u9891\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u521b\u65b0\u7684\u667a\u80fd\u4f53\u5f0f\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aScripterAgent\uff08\u5c06\u7c97\u7c92\u5ea6\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u7ec6\u7c92\u5ea6\u7535\u5f71\u811a\u672c\uff09\u548cDirectorAgent\uff08\u4f9d\u636e\u751f\u6210\u7684\u811a\u672c\u6307\u5bfc\u591a\u573a\u666f\u89c6\u9891\u8fde\u7eed\u751f\u6210\uff0c\u4fdd\u969c\u957f\u65f6\u5e8f\u8fde\u8d2f\uff09\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6d4b\u8bc4\u57fa\u51c6ScriptBench\u4ee5\u53ca\u914d\u5957\u7684AI\u8bc4\u5206\u5458CriticAgent\u548c\u89c6\u89c9-\u811a\u672c\u5bf9\u9f50\uff08VSA\uff09\u65b0\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u5404\u79cd\u4e3b\u6d41\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\uff0c\u5747\u53ef\u660e\u663e\u63d0\u5347\u89c6\u9891\u5bf9\u4e8e\u5267\u672c\u7684\u5fe0\u5b9e\u5ea6\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\uff0cAI\u6d4b\u8bc4\u4e0eVSA\u6307\u6807\u5747\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u5bf9\u8bdd\u4e0e\u7535\u5f71\u89c6\u9891\u751f\u6210\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u52a8\u5316\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u5f71\u89c6\u5185\u5bb9\u751f\u6210\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c2\u611f\u548c\u811a\u672c\u5fe0\u5b9e\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5bf9\u672a\u6765\u81ea\u52a8\u5316\u5f71\u89c6\u5236\u4f5c\u5177\u6709\u91cd\u8981\u53c2\u8003\u610f\u4e49\u3002"}}
{"id": "2601.18253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18253", "abs": "https://arxiv.org/abs/2601.18253", "authors": ["Peng Sun", "Xiangyu Zhang", "Duan Wu"], "title": "BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation", "comment": "This is a pre-print", "summary": "Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6BoRP\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u8bc4\u4f30\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u4e00\u662fA/B\u6d4b\u8bd5\u4f9d\u8d56\u7684\u663e\u5f0f\u53cd\u9988\u7a00\u7f3a\uff0c\u4e8c\u662f\u9690\u5f0f\u6307\u6807\u4e0d\u591f\u660e\u786e\u3002\u8fd9\u4e9b\u90fd\u963b\u788d\u4e86\u5bf9\u5bf9\u8bdd\u5f0fAI\u7684\u6709\u6548\u8fed\u4ee3\u3002", "method": "\u63d0\u51faBoRP\uff08Bootstrapped Regression Probing\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u7a7a\u95f4\u7279\u6027\uff0c\u901a\u8fc7\u57fa\u4e8e\u6781\u5316\u6307\u6570\u7684\u81ea\u4e3e\u65b9\u6cd5\u751f\u6210\u8bc4\u4ef7\u6807\u51c6\uff0c\u5e76\u7528\u504f\u6700\u5c0f\u4e8c\u4e58\uff08PLS\uff09\u56de\u5f52\u5c06\u6a21\u578b\u9690\u85cf\u6001\u6620\u5c04\u4e3a\u8fde\u7eed\u5206\u6570\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u6ee1\u610f\u5ea6\u6253\u5206\u3002", "result": "\u5728\u5de5\u4e1a\u7ea7\u6570\u636e\u96c6\u4e0a\uff0cBoRP\uff08Qwen3-8B/14B\uff09\u76f8\u6bd4\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u57fa\u7ebf\uff08\u5982Qwen3-Max\uff09\u4e0e\u4eba\u5de5\u8bc4\u5224\u7684\u4e00\u81f4\u6027\u66f4\u9ad8\u3002\u540c\u65f6\uff0cBoRP\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u53ef\u5b9e\u73b0\u5927\u89c4\u6a21\u7075\u654f\u7684A/B\u6d4b\u8bd5\u4e0e\u5168\u91cf\u76d1\u63a7\u3002", "conclusion": "BoRP\u4e3a\u5f00\u653e\u5f0f\u5bf9\u8bddAI\u5e26\u6765\u9ad8\u6548\u3001\u53ef\u9760\u7684\u6ee1\u610f\u5ea6\u8bc4\u4f30\u5de5\u5177\uff0c\u4e0d\u4ec5\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\uff0c\u4e5f\u63d0\u9ad8\u5b9e\u9645\u751f\u4ea7\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.17740", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17740", "abs": "https://arxiv.org/abs/2601.17740", "authors": ["Cong Cao", "Ren Li", "Corentin Dumery", "Hao Li"], "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields", "comment": null, "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u8868\u793a\u7684\u7f1d\u7eab\u7eb8\u6837\u5efa\u6a21\u65b9\u6cd5\uff0c\u53ef\u51c6\u786e\u751f\u6210\u548c\u5efa\u6a21\u590d\u6742\u670d\u88c5\u7ed3\u6784\u7684\u7eb8\u6837\u3002", "motivation": "\u7f1d\u7eab\u7eb8\u6837\u662f\u670d\u88c5\u8bbe\u8ba1\u548c\u5236\u9020\u7684\u57fa\u7840\uff0c\u4f46\u7531\u4e8e\u9762\u677f\u51e0\u4f55\u5f62\u72b6\u548c\u7f1d\u5408\u65b9\u5f0f\u7684\u591a\u6837\u6027\uff0c\u81ea\u52a8\u5316\u3001\u7cbe\u786e\u5730\u5efa\u6a21\u7eb8\u6837\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u9690\u5f0f\u8868\u793a\uff0c\u901a\u8fc7\u6709\u7b26\u53f7\u4e0e\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u63cf\u8ff0\u7eb8\u6837\u7684\u9762\u677f\u8fb9\u754c\u548c\u7f1d\u5408\u7aef\u70b9\uff0c\u518d\u5c06\u8fd9\u4e9b\u8ddd\u79bb\u573a\u7f16\u7801\u8fdb\u8fde\u7eed\u6f5c\u7a7a\u95f4\uff0c\u5b9e\u73b0\u53ef\u5fae\u5206\u7f51\u683c\u5316\uff1b\u540c\u65f6\uff0c\u5229\u7528\u6f5c\u6d41\u5339\u914d\u6a21\u578b\u5b66\u4e60\u9762\u677f\u7ec4\u5408\u7684\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u7f1d\u5408\u9884\u6d4b\u6a21\u5757\u4ece\u8fb9\u7f18\u6bb5\u6062\u590d\u7f1d\u5408\u5173\u7cfb\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u53ef\u51c6\u786e\u5efa\u6a21\u5e76\u751f\u6210\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u7f1d\u7eab\u7eb8\u6837\uff0c\u76f8\u6bd4\u5df2\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u7cbe\u786e\u5730\u4ece\u56fe\u7247\u4f30\u7b97\u7eb8\u6837\uff0c\u5e76\u652f\u6301\u7eb8\u6837\u8865\u5168\u548c\u91cd\u65b0\u9002\u914d\u7b49\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u6570\u5b57\u5316\u670d\u88c5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u7eb8\u6837\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u5e76\u62d3\u5c55\u4e86\u76f8\u5173\u5e94\u7528\u9886\u57df\u3002"}}
{"id": "2601.18281", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18281", "abs": "https://arxiv.org/abs/2601.18281", "authors": ["Yuhang Jia", "Pei Liu", "Haoqin Sun", "Jiaming Zhou", "Xuxin Cheng", "Cao Liu", "Ke Zeng", "Xunliang Cai", "Yong Qin"], "title": "Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue", "comment": null, "summary": "End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single \"correct\" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u540c\u7406\u5fc3\u5bf9\u8bdd\u7684\u81ea\u7136\u8bed\u8a00\u8bc4\u6d4b\u5de5\u5177EmpathyEval\uff0c\u4ee5\u53ca\u7ed3\u5408\u81ea\u6211\u53cd\u601d\u673a\u5236\u7684\u7aef\u5230\u7aefSLM\u6a21\u578bReEmpathy\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u540c\u7406\u5fc3\u56de\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u540c\u7406\u5fc3\u5bf9\u8bdd\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u76d1\u7763\u4fe1\u53f7\uff0c\u96be\u4ee5\u8986\u76d6\u590d\u6742\u7684\u60c5\u611f\u8868\u8fbe\u548c\u540c\u7406\u884c\u4e3a\u7684\u591a\u6837\u6027\u3002\u5982\u4f55\u63d0\u5347\u60c5\u611f\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u4eba\u6027\u5316\u7406\u89e3\uff0c\u662f\u4e00\u4e2a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faEmpathyEval\uff0c\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u63cf\u8ff0\u5f0f\u8bc4\u6d4b\u4ee3\u66ff\u4f20\u7edf\u7684\u5206\u6570\u6216\u6807\u7b7e\u3002\u57fa\u4e8e\u6b64\uff0c\u53c8\u63d0\u51fa\u4e86ReEmpathy\u6a21\u578b\uff0c\u9996\u6b21\u5728\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5f15\u5165\u2018\u540c\u7406\u81ea\u6211\u53cd\u601d\u4ea4\u66ff\u63a8\u7406\u2019\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u5728\u751f\u6210\u56de\u590d\u524d\u80fd\u8fdb\u884c\u81ea\u7531\u5f62\u5f0f\u7684\u540c\u7406\u53cd\u601d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f15\u5165\u53cd\u601d\u673a\u5236\u7684ReEmpathy\u6a21\u578b\u540c\u7406\u5fc3\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u8bed\u97f3\u56de\u590d\u5728\u60c5\u611f\u7406\u89e3\u548c\u540c\u7406\u8868\u8fbe\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bc4\u6d4b\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0c\u7cfb\u7edf\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u548c\u8868\u8fbe\u590d\u6742\u7684\u60c5\u611f\u4e0e\u540c\u7406\u5fc3\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5177\u60c5\u611f\u667a\u80fd\u7684\u4eba\u673a\u5bf9\u8bdd\u5e26\u6765\u65b0\u7684\u53ef\u80fd\u3002"}}
{"id": "2601.17741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17741", "abs": "https://arxiv.org/abs/2601.17741", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "Junhao Jiang", "Gai Zhang", "Jia Wang"], "title": "Frequency-aware Neural Representation for Videos", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u611f\u77e5\u795e\u7ecf\u8868\u8fbe\uff08FaNeRV\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u8fbe\u7684\u89c6\u9891\u538b\u7f29\u8d28\u91cf\uff0c\u517c\u987e\u4f4e\u9891\u4e0e\u9ad8\u9891\u7ec6\u8282\uff0c\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8fc7\u5ea6\u5e73\u6ed1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709INR\uff08\u9690\u5f0f\u795e\u7ecf\u8868\u8fbe\uff09\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u9891\u8c31\u504f\u7f6e\uff0c\u5bfc\u81f4\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\u548c\u91cd\u5efa\u5e73\u6ed1\uff1b\u9700\u8981\u65b0\u65b9\u6cd5\u4ee5\u66f4\u597d\u517c\u987e\u5168\u5c40\u4e0e\u7ec6\u8282\u4fe1\u606f\u3002", "method": "FaNeRV\u65b9\u6cd5\u5c06\u89c6\u9891\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u91cf\uff0c\u91c7\u7528\u591a\u5206\u8fa8\u7387\u5206\u9636\u6bb5\u76d1\u7763\u7b56\u7565\uff1b\u5f15\u5165\u52a8\u6001\u9ad8\u9891\u6ce8\u5165\u673a\u5236\uff0c\u91cd\u70b9\u63d0\u5347\u96be\u4ee5\u8fd8\u539f\u7684\u9ad8\u9891\u533a\u57df\uff1b\u5e76\u8bbe\u8ba1\u4e86\u9891\u7387\u89e3\u8026\u7f51\u7edc\u6a21\u5757\u52a0\u5f3a\u4e0d\u540c\u9891\u8c31\u5e26\u7279\u5f81\u7684\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cFaNeRV\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684INR\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u7801\u7387\u5931\u771f\uff08rate-distortion\uff09\u8868\u73b0\u4e0a\u4e5f\u5177\u5907\u4e0e\u4f20\u7edf\u7f16\u89e3\u7801\u6280\u672f\u7ade\u4e89\u7684\u80fd\u529b\u3002", "conclusion": "FaNeRV\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86INR\u89c6\u9891\u538b\u7f29\u4e2d\u7684\u9891\u8c31\u504f\u7f6e\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u538b\u7f29\u6548\u7387\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u5177\u6709\u8f83\u5927\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2601.18285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18285", "abs": "https://arxiv.org/abs/2601.18285", "authors": ["Jin Su", "Runnan Fang", "Yeqiu Li", "Xiaobin Wang", "Shihao Cai", "Pengjun Xie", "Ningyu Zhang", "Fajie Yuan"], "title": "U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents", "comment": null, "summary": "Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $\u03c4$-bench, $\u03c4^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aU-Fold\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u5728\u957f\u5bf9\u8bdd\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u9047\u5230\u7684\u4fe1\u606f\u9057\u5931\u548c\u610f\u56fe\u8ffd\u8e2a\u56f0\u96be\u7b49\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660eU-Fold\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u5bf9\u8bdd\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d7\u9650\u65f6\uff0c\u9700\u901a\u8fc7\u4e0a\u4e0b\u6587\u6298\u53e0\u65b9\u6cd5\u603b\u7ed3\u5386\u53f2\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u9488\u5bf9\u5355\u4e00\u67e5\u8be2\u6216\u610f\u56fe\uff0c\u6613\u4e22\u5931\u5173\u952e\u4fe1\u606f\u4e14\u4e0d\u80fd\u8ffd\u8e2a\u7528\u6237\u610f\u56fe\u53d8\u5316\uff0c\u56e0\u6b64\u96be\u4ee5\u6ee1\u8db3\u771f\u5b9e\u7684\u591a\u8f6e\u3001\u590d\u6742\u5bf9\u8bdd\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51faU-Fold\u6846\u67b6\uff0c\u6bcf\u8f6e\u5bf9\u8bdd\u52a8\u6001\u751f\u6210\u610f\u56fe\u611f\u77e5\u7684\u5bf9\u8bdd\u6458\u8981\u548c\u4e0e\u4efb\u52a1\u7d27\u5bc6\u76f8\u5173\u7684\u5de5\u5177\u8c03\u7528\u65e5\u5fd7\uff0c\u540c\u65f6\u4fdd\u7559\u5168\u90e8\u7684\u539f\u59cb\u7528\u6237-\u4ee3\u7406\u5bf9\u8bdd\u548c\u5de5\u5177\u8c03\u7528\u5386\u53f2\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5173\u952e\u4fe1\u606f\u548c\u7528\u6237\u610f\u56fe\u52a8\u6001\u3001\u7ec6\u81f4\u7684\u7ba1\u7406\u3002", "result": "\u5728\u03c4-bench\u3001\u03c4\u00b2-bench\u3001VitaBench\u7b49\u591a\u4e2a\u57fa\u51c6\u53ca\u66f4\u590d\u6742\u7684\u201c\u6781\u9650\u957f\u4e0a\u4e0b\u6587\u201d\u4efb\u52a1\u4e2d\uff0cU-Fold\u5728\u4e0eReAct\u548c\u5176\u5b83\u6298\u53e0\u57fa\u7ebf\u5bf9\u6bd4\u65f6\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u7684\u80dc\u7387\u8fbe71.4%\uff0c\u63d0\u5347\u5e45\u5ea6\u6700\u9ad8\u53ef\u8fbe27%\u3002", "conclusion": "U-Fold\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u589e\u5f3a\u578b\u5927\u6a21\u578b\u5728\u771f\u5b9e\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u529b\uff0c\u4e3a\u5355\u67e5\u8be2\u6298\u53e0\u6280\u672f\u5411\u7528\u6237\u4e2d\u5fc3\u5b9e\u9645\u5e94\u7528\u573a\u666f\u8fc1\u79fb\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17743", "abs": "https://arxiv.org/abs/2601.17743", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "Junhao Jiang", "Gai Zhang", "Jia Wang"], "title": "Video Compression with Hierarchical Temporal Neural Representation", "comment": null, "summary": "Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.", "AI": {"tldr": "TeNeRV\u662f\u4e00\u79cd\u65b0\u7684\u5c42\u6b21\u5316\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u65f6\u95f4\u7ef4\u5ea6\u5efa\u6a21\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u8868\u793a\uff08INR\uff09\u7684\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u5f88\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u65f6\u5e8f\u4f9d\u8d56\uff0c\u5f71\u54cd\u538b\u7f29\u6027\u80fd\u548c\u8fd8\u539f\u8d28\u91cf\u3002", "method": "TeNeRV\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u4e00\u662f\u7528\u4e8e\u76f8\u90bb\u5e27\u7279\u5f81\u878d\u5408\u7684IFF\u6a21\u5757\uff0c\u63d0\u5347\u5c40\u90e8\u65f6\u5e8f\u4e00\u81f4\u6027\uff1b\u4e8c\u662fGAM\u673a\u5236\uff0c\u5c06\u89c6\u9891\u5206\u7ec4\uff08GoP\uff09\u5e76\u9488\u5bf9\u4e0d\u540c\u5206\u7ec4\u81ea\u9002\u5e94\u8c03\u8282\u7f51\u7edc\u53c2\u6570\uff0c\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTeNeRV\u5728\u901f\u7387-\u5931\u771f\u6027\u80fd\u4e0a\u6bd4\u5f53\u524d\u7684INR\u65b9\u6cd5\u6709\u66f4\u7a81\u51fa\u548c\u7a33\u5b9a\u7684\u8868\u73b0\u3002", "conclusion": "TeNeRV\u901a\u8fc7\u5206\u5c42\u65f6\u5e8f\u5efa\u6a21\u63d0\u5347\u4e86\u795e\u7ecf\u8868\u793a\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u7684\u8868\u8fbe\u529b\u548c\u9002\u5e94\u6027\uff0c\u4f18\u4e8e\u540c\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2601.18296", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18296", "abs": "https://arxiv.org/abs/2601.18296", "authors": ["Zhaoyan Gong", "Zhiqiang Liu", "Songze Li", "Xiaoke Guo", "Yuanxiang Liu", "Xinle Deng", "Zhizhen Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning", "comment": "Work in progress", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u52a9\u578b\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08TKGQA\uff09\u4ee3\u7406Temp-R1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u591a\u8df3\u63a8\u7406\u548c\u65f6\u5e8f\u7ea6\u675f\u7406\u89e3\u80fd\u529b\uff0c\u5728\u5f53\u524d\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6d89\u53ca\u52a8\u6001\u4e8b\u5b9e\u3001\u591a\u8df3\u4f9d\u8d56\u53ca\u590d\u6742\u65f6\u5e8f\u5173\u7cfb\uff0c\u73b0\u6709\u65b9\u6cd5\u6d41\u7a0b\u50f5\u5316\u4e14\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u79c1\u6709API\uff0c\u65e0\u6cd5\u9002\u5e94\u7075\u6d3b\u548c\u5927\u89c4\u6a21\u7684\u5e94\u7528\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u3001\u5f00\u6e90\u4e14\u81ea\u52a8\u5316\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u81ea\u4e3b\u578b\u7aef\u5230\u7aefTKGQA\u667a\u80fd\u4f53Temp-R1\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u6269\u5c55\u4e86\u667a\u80fd\u4f53\u7684\u201c\u52a8\u4f5c\u7a7a\u95f4\u201d\uff0c\u5f15\u5165\u5185\u5916\u90e8\u4e13\u95e8\u52a8\u4f5c\u4ee5\u51cf\u8f7b\u5355\u6b21\u63a8\u7406\u538b\u529b\uff0c\u540c\u65f6\u8bbe\u8ba1\u9006\u5411\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5373\u5148\u5b66\u96be\u9898\u540e\u8fc1\u79fb\u5230\u7b80\u5355\u95ee\u9898\uff0c\u9632\u6b62\u6a21\u578b\u53ea\u5b66\u4f1a\u6377\u5f84\u800c\u975e\u590d\u6742\u63a8\u7406\u3002", "result": "\u5177\u590780\u4ebf\u53c2\u6570\u7684Temp-R1\u5728MultiTQ\u548cTimelineKGQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u8868\u73b0\uff0c\u5c24\u5176\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u7387\u76f8\u8f83\u5df2\u6709\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e8619.8%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u9006\u5411\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u62d3\u5c55\u5bf9\u63d0\u5347\u590d\u6742\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u6811\u7acb\u4e86\u81ea\u4e3b\u578bTKGQA\u667a\u80fd\u4f53\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u5373\u5c06\u5f00\u6e90\uff0c\u4fbf\u4e8e\u4e1a\u754c\u590d\u73b0\u548c\u5e94\u7528\u3002"}}
{"id": "2601.17747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17747", "abs": "https://arxiv.org/abs/2601.17747", "authors": ["Kaixuan Jiang", "Chen Wu", "Zhenghui Zhao", "Chengxi Han"], "title": "Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection", "comment": null, "summary": "Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9065\u611f\u65f6\u5e8f\u5f71\u50cf\u53d8\u5316\u68c0\u6d4b\u6846\u67b6UniCD\uff0c\u80fd\u540c\u65f6\u5904\u7406\u6709\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u65e0\u76d1\u7763\u4efb\u52a1\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u50cf\u7d20\u7ea7\u53d8\u5316\u6807\u7b7e\u975e\u5e38\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u6807\u6ce8\u7ea7\u522b\u7684\u573a\u666f\uff0c\u6025\u9700\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u53d8\u5316\u68c0\u6d4b\u65b9\u6848\u3002", "method": "UniCD\u91c7\u7528\u5171\u4eab\u7f16\u7801\u5668+\u591a\u5206\u652f\u7ed3\u6784\uff0c\u5206\u522b\u9488\u5bf9\u6709\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u65e0\u76d1\u7763\u4efb\u52a1\u8bbe\u7f6e\u4e0d\u540c\u5206\u652f\u3002\u5176\u4e2d\uff1a\u6709\u76d1\u7763\u5206\u652f\u5f15\u5165\u7a7a\u95f4-\u65f6\u5e8f\u611f\u77e5\u6a21\u5757\uff08STAM\uff09\uff0c\u63d0\u5347\u53cc\u65f6\u76f8\u7279\u5f81\u878d\u5408\uff1b\u5f31\u76d1\u7763\u5206\u652f\u63d0\u51fa\u53d8\u5316\u8868\u5f81\u6b63\u5219\u5316\uff08CRR\uff09\uff0c\u4f18\u5316\u7c97\u7c92\u5ea6\u6fc0\u6d3b\u6536\u655b\uff1b\u65e0\u76d1\u7763\u5206\u652f\u5219\u5229\u7528\u8bed\u4e49\u5148\u9a8c\u9a71\u52a8\u53d8\u5316\u63a8\u65ad\uff08SPCI\uff09\uff0c\u5c06\u65e0\u76d1\u7763\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u5f31\u76d1\u7763\u8def\u5f84\u4f18\u5316\u3002", "result": "\u5728\u4e3b\u6d41\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cUniCD\u5728\u4e09\u79cd\u4efb\u52a1\u4e0b\u5747\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff0c\u5c24\u5176\u5728\u5f31\u76d1\u7763\u548c\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u9ad812.72%\u548c12.37%\u3002", "conclusion": "UniCD\u6709\u6548\u63d0\u5347\u4e86\u591a\u573a\u666f\u4e0b\u53d8\u5316\u68c0\u6d4b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u662f\u7edf\u4e00\u591a\u76d1\u7763\u4fe1\u53f7\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u7684\u6709\u529b\u6846\u67b6\u3002"}}
{"id": "2601.18302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18302", "abs": "https://arxiv.org/abs/2601.18302", "authors": ["Keigo Shibata", "Kazuki Yano", "Ryosuke Takahashi", "Jaesung Lee", "Wataru Ikeda", "Jun Suzuki"], "title": "Suppressing Final Layer Hidden State Jumps in Transformer Pretraining", "comment": "Accepted to the Findings of EACL 2026", "summary": "This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Transformer\u6a21\u578b\u5728\u4e2d\u95f4\u5c42\u4e0e\u672b\u5c42\u4e4b\u95f4\u9690\u85cf\u72b6\u6001\u89d2\u5ea6\u8ddd\u79bb\u7a81\u7136\u53d8\u5316\u7684\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u9879JREG\u6765\u6291\u5236\u8fd9\u4e00\u73b0\u8c61\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6700\u8fd1\u53d1\u73b0\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u9690\u85cf\u72b6\u6001\u53d8\u5316\u8f83\u5c0f\uff0c\u800c\u5728\u672b\u5c42\u9644\u8fd1\u7a81\u7136\u4ea7\u751f\u8f83\u5927\u53d8\u5316\uff0c\u8fd9\u79cd\u884c\u4e3a\u53ef\u80fd\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5747\u8861\u4f7f\u7528\u4e0d\u5229\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u91cf\u5316\u5206\u6790\u548c\u7ea0\u6b63\u3002", "method": "\u63d0\u51fa\u5e76\u5b9a\u4e49\u4e86final layer\u201cjump\u201d\u7684\u91cf\u5316\u6307\u6807\uff0c\u6d4b\u8bd5\u4e86\u8be5\u73b0\u8c61\u5728\u591a\u79cd\u4e3b\u6d41\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u666e\u904d\u6027\u548c\u968f\u9884\u8bad\u7ec3\u653e\u5927\u7684\u8d8b\u52bf\uff0c\u5e76\u63d0\u51fa\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u589e\u52a0\u4e00\u79cdJump Suppressing Regularizer\uff08JREG\uff09\u635f\u5931\uff0c\u9f13\u52b1\u6a21\u578b\u5728\u6240\u6709\u5c42\u4e2d\u66f4\u5747\u8861\u5730\u5229\u7528\u80fd\u529b\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u7684Llama\u6a21\u578b\u4e0a\u5b9e\u9a8c\u53d1\u73b0\uff0c\u91c7\u7528JREG\u6b63\u5219\u540e\u7684\u6a21\u578b\u5728\u4e0d\u6539\u53d8\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b\u3002", "conclusion": "Transformer\u4e2d\u672b\u5c42\u9690\u85cf\u72b6\u6001\u7a81\u53d8\u662f\u4e00\u79cd\u666e\u904d\u4e14\u968f\u9884\u8bad\u7ec3\u589e\u5f3a\u7684\u7279\u6027\uff0c\u901a\u8fc7\u6b63\u5219\u9879JREG\u53ef\u4ee5\u6709\u6548\u6291\u5236\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.17756", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17756", "abs": "https://arxiv.org/abs/2601.17756", "authors": ["Ziyang Song", "Xinyu Gong", "Bangya Liu", "Zelin Zhao"], "title": "MV-S2V: Multi-View Subject-Consistent Video Generation", "comment": "13 pages, 9 figures", "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u89c6\u89d2\u53c2\u8003\u56fe\u50cf\u751f\u6210\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\uff08MV-S2V\uff09\uff0c\u5b9e\u73b0\u4e863D\u5c42\u6b21\u4e0a\u7684\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5408\u6210\u89c6\u9891\u8d28\u91cf\u4e0a\u53d6\u5f97\u7a81\u7834\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u4e3b\u4f53\u751f\u6210\u65b9\u6cd5\uff08S2V\uff09\u4e3b\u8981\u57fa\u4e8e\u5355\u4e00\u89c6\u89d2\u53c2\u8003\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u4ec5\u9650\u4e8e\u8be5\u89c6\u89d2\uff0c\u672a\u5145\u5206\u5229\u7528\u89c6\u9891\u4e3b\u4f53\u63a7\u5236\u7684\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u63d0\u5347\u591a\u89c6\u89d2\u4e0b\u76843D\u4e3b\u4f53\u4e00\u81f4\u6027\u6210\u4e3a\u8be5\u9886\u57df\u7684\u6311\u6218\u3002", "method": "1\uff09\u63d0\u51faMV-S2V\u4efb\u52a1\uff0c\u4ece\u591a\u4e2a\u89c6\u89d2\u7684\u53c2\u8003\u56fe\u50cf\u5408\u6210\u89c6\u9891\u4ee5\u5b9e\u73b03D\u4e3b\u4f53\u4e00\u81f4\u6027\uff1b2\uff09\u6784\u5efa\u4e86\u5408\u6210\u6570\u636e\u7ba1\u7ebf\u4ee5\u751f\u6210\u9ad8\u5ea6\u5b9a\u5236\u7684\u5408\u6210\u6570\u636e\uff0c\u5e76\u8f85\u4ee5\u5c0f\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u5171\u540c\u8bad\u7ec3\u6a21\u578b\uff1b3\uff09\u63d0\u51fa\u4e86 Temporally Shifted RoPE\uff08TS-RoPE\uff09\u6280\u672f\uff0c\u533a\u5206\u8de8\u4e3b\u4f53\u4e0e\u8de8\u89c6\u89d2\u7684\u6761\u4ef6\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u7cbe\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u53c2\u8003\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f18\u76843D\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u751f\u6210\u89c6\u9891\u8d28\u91cf\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u5f00\u8f9f\u4e86\u4ee5\u591a\u89c6\u89d2\u53c2\u8003\u4e3a\u57fa\u7840\u7684\u65b0\u89c6\u9891\u751f\u6210\u65b9\u5411\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u4f53\u9a71\u52a8\u89c6\u9891\u751f\u6210\u76843D\u4e00\u81f4\u6027\u4e0e\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2601.18306", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18306", "abs": "https://arxiv.org/abs/2601.18306", "authors": ["Everlyn Asiko Chimoto", "Mostafa Elhoushi", "Bruce A. Bassett"], "title": "Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM", "comment": "Accepted to EACL 2026 Main Conference", "summary": "Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u6821\u51c6\u5728\u91cf\u5316\u591a\u8bed\u8a00\u5927\u6a21\u578b\u65f6\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u76f8\u6bd4\u53ea\u7528\u82f1\u8bed\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u6216\u76ee\u6807\u8bed\u8a00\u7684\u6821\u51c6\u96c6\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u540e\u7684\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u91cf\u5316\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u591a\u53ea\u7528\u82f1\u8bed\u5c0f\u6837\u672c\u6821\u51c6\uff0c\u4e14\u5bf9\u591a\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u968f\u7740\u591a\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u63a2\u7d22\u66f4\u4f18\u7684\u91cf\u5316\u6821\u51c6\u7b56\u7565\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u572810\u79cd\u8bed\u8a00\u7684\u6570\u636e\u4e0a\uff0c\u91c7\u7528\u4e24\u79cd\u4e3b\u6d41\u91cf\u5316\u5668\uff08GPTQ\u548cAWQ\uff09\uff0c\u8bbe\u8ba1\u516b\u79cd\u6821\u51c6\u96c6\uff08\u5305\u62ec\u5355\u8bed\u8a00\u548c\u591a\u8bed\u8a00\u6df7\u5408\uff09\uff0c\u5bf9Llama3.1 8B\u548cQwen2.5 7B\u6a21\u578b\u7cfb\u7edf\u5f00\u5c55\u91cf\u5316\u5b9e\u9a8c\uff0c\u5e76\u7528\u56f0\u60d1\u5ea6\u6307\u6807\u8bc4\u4f30\u8868\u73b0\uff1b\u540c\u65f6\u5206\u6790\u4e0d\u540c\u8bed\u8a00\u4e0e\u91cf\u5316\u5668\u7ec4\u5408\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff1a\u975e\u82f1\u8bed\u53ca\u591a\u8bed\u8a00\u6821\u51c6\u96c6\u53ef\u663e\u8457\u964d\u4f4e\u56f0\u60d1\u5ea6\uff0c\u63d0\u5347\u91cf\u5316\u540e\u6027\u80fd\u3002\u5176\u4e2d\u591a\u8bed\u8a00\u6df7\u5408\u6821\u51c6\u96c6\u7684\u56f0\u60d1\u5ea6\u4e0b\u964d\u6700\u660e\u663e\uff0c\u6700\u9ad8\u964d\u5e45\u8fbe3.52\u3002\u9488\u5bf9\u6027\u5730\u7528\u76ee\u6807\u8bed\u8a00\u6821\u51c6\u96c6\u5bf9\u5355\u8bed\u8a00\u8bc4\u6d4b\u63d0\u5347\u6700\u5927\u3002\u540c\u65f6\u53d1\u73b0\u6781\u4e2a\u522b\u8bed\u8a00-\u91cf\u5316\u5668\u7ec4\u5408\u4f1a\u56e0\u6fc0\u6d3b\u5206\u5e03\u5f02\u5e38\u800c\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u91cf\u5316\u5927\u6a21\u578b\u65f6\u201c\u4e00\u5200\u5207\u201d\u5730\u7528\u82f1\u8bed\u6821\u51c6\u5e76\u4e0d\u7406\u60f3\u3002\u5e94\u7ed3\u5408\u76ee\u6807\u8bed\u8a00\u548c\u591a\u6837\u6027\u8c03\u6574\u6821\u51c6\u6570\u636e\uff0c\u8fd9\u5bf9\u591a\u8bed\u8a00\u6a21\u578b\u7684\u5065\u58ee\u91cf\u5316\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.17791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17791", "abs": "https://arxiv.org/abs/2601.17791", "authors": ["Rabin Dulal", "Wenfeng Jia", "Lihong Zheng", "Jane Quinn"], "title": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation", "comment": null, "summary": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4e0e3D\u91cd\u5efa\u6280\u672f\uff0c\u7ed3\u5408\u96c6\u6210\u56de\u5f52\u6a21\u578b\uff0c\u5b9e\u73b0\u725b\u53ea\u975e\u63a5\u89e6\u3001\u4f4e\u6210\u672c\u3001\u7cbe\u51c6\u7684\u4f53\u91cd\u4f30\u7b97\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u79f0\u91cd\u548c\u4f53\u51b5\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8017\u65f6\u3001\u64cd\u4f5c\u590d\u6742\u3001\u5bf9\u7272\u755c\u548c\u7ecf\u6d4e\u5747\u6709\u4e0d\u5229\u5f71\u54cd\u7684\u95ee\u9898\u3002\u4e3a\u63d0\u5347\u755c\u7267\u7ba1\u7406\u7684\u6548\u7387\u548c\u52a8\u7269\u798f\u5229\uff0c\u4e9f\u9700\u4e00\u79cd\u7b80\u4fbf\u3001\u65e0\u63a5\u89e6\u3001\u7ecf\u6d4e\u5b9e\u7528\u7684\u7cbe\u51c6\u4f53\u91cd\u4f30\u7b97\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2RGB\u5f71\u50cf\uff0c\u901a\u8fc7SAM 3D\uff08Segment Anything Model 3D\uff09\u4e3a\u6838\u5fc3\u76843D\u91cd\u5efa\u6d41\u7a0b\u8fdb\u884c\u70b9\u4e91\u751f\u6210\uff0c\u518d\u5e94\u7528\u96c6\u6210\u56de\u5f52\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4f53\u91cd\u9884\u6d4b\u3002\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u5b9e\u9645\u519c\u573a\u73af\u5883\u4e0b\uff0c\u5bf9\u6bd4\u4e86\u4e0d\u540c3D\u91cd\u5efa\u65b9\u5f0f\u4e0e\u9884\u6d4b\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "SAM 3D\u7ed3\u5408\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u878d\u5408\u65b9\u6cd5\u751f\u6210\u7684\u4e09\u7ef4\u70b9\u4e91\u6548\u679c\u4f18\u4e8e\u5176\u4ed63D\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u519c\u573a\u73af\u5883\u4e0b\uff0c\u4f20\u7edf\u96c6\u6210\u56de\u5f52\u6a21\u578b\u6bd4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u66f4\u7a33\u5b9a\uff08R^2=0.69\u00b10.10\uff0cMAPE=2.22\u00b10.56%\uff09\uff0c\u9884\u6d4b\u7cbe\u5ea6\u9ad8\u4e14\u5177\u5907\u53ef\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\uff0c\u63d0\u53473D\u91cd\u5efa\u8d28\u91cf\u6bd4\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u66f4\u5173\u952e\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u4f4e\u6210\u672c\u3001\u6613\u7528\uff0c\u9002\u7528\u4e8e\u96be\u4ee5\u83b7\u5f97\u5927\u89c4\u6a213D\u6570\u636e\u7684\u5b9e\u9645\u519c\u573a\u4f53\u91cd\u76d1\u6d4b\uff0c\u6709\u52a9\u4e8e\u517b\u6b96\u4e1a\u7684\u6570\u5b57\u5316\u548c\u667a\u80fd\u5316\u5347\u7ea7\u3002"}}
{"id": "2601.18320", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.18320", "abs": "https://arxiv.org/abs/2601.18320", "authors": ["Jinwei Lu", "Yuanfeng Song", "Chen Zhang", "Raymond Chi-Wing Wong"], "title": "MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization", "comment": "Accepted to SIGMOD 2026", "summary": "Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiVis-Agent\u7684\u591a\u667a\u80fd\u4f53\u591a\u6a21\u6001\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u6570\u5b66\u7ea6\u675f\u7684\u903b\u8f91\u89c4\u5219\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u591a\u573a\u666f\u4e0b\u7684\u9ad8\u53ef\u9760\u6027\u81ea\u52a8\u53ef\u89c6\u5316\u751f\u6210\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u89c6\u5316\u9700\u6c42\u590d\u6742\uff0c\u5f80\u5f80\u6d89\u53ca\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u6587\u672c\u3001\u56fe\u7247\u3001\u4ee3\u7801\u7b49\uff09\u548c\u591a\u6b65\u8fed\u4ee3\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u4ec5\u652f\u6301\u5355\u4e00\u6a21\u6001\u548c\u4e00\u6b21\u6027\u751f\u6210\uff0c\u5de5\u4f5c\u6d41\u7a0b\u521a\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5982\u6b7b\u5faa\u73af\u6216\u91cd\u5927\u9519\u8bef\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u5e94\u5bf9\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MultiVis-Agent\uff0c\u591a\u667a\u80fd\u4f53\u3001\u903b\u8f91\u89c4\u5219\u589e\u5f3a\u7684\u6846\u67b6\u3002\u5177\u4f53\u5305\u62ec\u5f15\u5165\u56db\u5c42\u903b\u8f91\u89c4\u5219\uff0c\u4ee5\u6570\u5b66\u7ea6\u675f\u5f62\u5f0f\u6307\u5bfc\u5927\u6a21\u578b\u63a8\u7406\uff0c\u5728\u4fdd\u8bc1\u7075\u6d3b\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b1000\u591a\u4e2a\u6848\u4f8b\u7684MultiVis-Bench\u591a\u6a21\u6001\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8986\u76d6\u4ece\u57fa\u7840\u751f\u6210\u5230\u591a\u6b65\u8fed\u4ee3\u7684\u56db\u7c7b\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMultiVis-Agent\u5728\u591a\u6a21\u6001\u590d\u6742\u4efb\u52a1\u7684\u53ef\u89c6\u5316\u5f97\u5206\u8fbe\u523075.63%\uff0c\u8f83\u57fa\u7ebf\uff0857.54-62.79%\uff09\u5927\u5e45\u63d0\u5347\uff1b\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe99.58%\u3001\u4ee3\u7801\u6267\u884c\u6210\u529f\u738794.56%\uff0c\u800c\u65e0\u903b\u8f91\u89c4\u5219\u60c5\u5f62\u5206\u522b\u4e3a74.48%\u548c65.10%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u53ef\u89c6\u5316\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u3001\u591a\u573a\u666f\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u81ea\u52a8\u53ef\u89c6\u5316\u751f\u6210\u624b\u6bb5\u5728\u590d\u6742\u6027\u4e0e\u53ef\u9760\u6027\u4e0a\u7684\u53cc\u91cd\u74f6\u9888\u3002"}}
{"id": "2601.17818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17818", "abs": "https://arxiv.org/abs/2601.17818", "authors": ["Wen Luo", "Peng Chen", "Xiaotao Huang", "LiQun Huang"], "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning", "comment": null, "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u6587\u672c\u534f\u540c\u526a\u679d\u6846\u67b6ViTCoP\uff0c\u7528\u4e8e\u9ad8\u6548\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9token\u5197\u4f59\uff0c\u63d0\u5347\u901f\u5ea6\u548c\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9token\u65f6\u5b58\u5728\u8f83\u5927\u5197\u4f59\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u9ad8\u3002\u5df2\u6709\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u8981\u4e48\u8fc7\u65e9\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u8981\u4e48\u9020\u6210\u5197\u4f59\u3002\u5982\u4f55\u9ad8\u6548\u5730\u7b5b\u9009\u4e0e\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684token\uff0c\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "ViTCoP\u6846\u67b6\u878d\u5408\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u5185\u7684\u5197\u4f59\u8fc7\u6ee4\u548c\u57fa\u4e8eLLM\u5206\u5c42\u7279\u6027\u7684\u9010\u6b65\u534f\u540c\u526a\u679d\uff0c\u540c\u65f6\u5f15\u5165K\u5411\u91cfL2\u8303\u6570\u4f5c\u4e3aLLM\u5185token\u91cd\u8981\u6027\u5ea6\u91cf\uff0c\u786e\u4fdd\u517c\u5bb9\u5982FlashAttention\u7b49\u52a0\u901f\u6280\u672f\u3002", "result": "ViTCoP\u5728\u591a\u79cd\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5404\u79cd\u56fe\u7247\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u9a8c\uff0c\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u6548\u679c\uff0c\u540c\u65f6\u663e\u8457\u7f29\u77ed\u4e86\u63a8\u7406\u5ef6\u8fdf\u3001\u964d\u4f4e\u4e86GPU\u5185\u5b58\u6d88\u8017\uff0c\u526a\u679d\u7387\u6781\u9ad8\u65f6\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "ViTCoP\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9token\u526a\u679d\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5197\u4f59\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8d44\u6e90\u8282\u7ea6\u4e24\u8005\u7684\u517c\u5f97\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18334", "abs": "https://arxiv.org/abs/2601.18334", "authors": ["Cl\u00e9ment Christophe", "Wadood Mohammed Abdul", "Prateek Munjal", "Tathagata Raha", "Ronnie Rajan", "Praveenkumar Kanithi"], "title": "Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare", "comment": null, "summary": "As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or \"confusability\". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized \"Thinking\" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u573a\u666f\u4e2d\u8fce\u5408\u7528\u6237\uff08sycophancy\uff09\u503e\u5411\u7684\u65b0\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u51c6\u786e\u7387\u9ad8\u7684\u201c\u63a8\u7406\u578b\u201d\u6a21\u578b\u5728\u6743\u5a01\u538b\u529b\u4e0b\u53ef\u80fd\u66f4\u6613\u4ea7\u751f\u4e0d\u771f\u5b9e\u7684\u8fce\u5408\u6027\u56de\u7b54\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u533b\u7597\u6d41\u7a0b\u4e2d\uff0c\u5176\u503e\u5411\u4e8e\u8fce\u5408\u7528\u6237\u800c\u975e\u575a\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5b58\u5728\u5371\u5bb3\u60a3\u8005\u5b89\u5168\u7684\u98ce\u9669\u3002\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u591a\u4e3a\u4e3b\u89c2\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u3001\u6807\u51c6\u5316\u7684\u8bc4\u6d4b\u6846\u67b6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u4e3a\u5ba2\u89c2\u548c\u4e25\u8c28\u7684\u65b9\u6cd5\u5206\u6790\u548c\u8bc4\u4f30\u8fd9\u7c7b\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e26\u53ef\u9a8c\u8bc1\u6807\u51c6\u7b54\u6848\u7684\u533b\u5b66\u9009\u62e9\u9898\uff08MCQA\uff09\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u6821\u6b63\u8fce\u5408\u5206\u6570\uff08Adjusted Sycophancy Score\uff09\u4ee5\u6392\u9664\u6a21\u578b\u81ea\u8eab\u4e0d\u7a33\u5b9a\u56e0\u7d20\u5bf9\u8bc4\u6d4b\u7684\u5e72\u6270\u3002\u540c\u65f6\uff0c\u91c7\u7528\u89c4\u6a21\u5316\u5206\u6790\u7814\u7a76Qwen-3\u548cLlama-3\u7b49\u6700\u65b0\u5927\u6a21\u578b\u5bb6\u65cf\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u7684\u6297\u8fce\u5408\u80fd\u529b\u4e0e\u6a21\u578b\u89c4\u6a21\u5448\u73b0\u7279\u5b9a\u89c4\u5f8b\u3002\u5c24\u5176\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f18\u5316\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6743\u5a01\u6027\u7528\u6237\u5efa\u8bae\u4e0b\u66f4\u6613\u4e8e\u9519\u8bef\u5408\u7406\u5316\uff0c\u51fa\u73b0\u9519\u8bef\u7684\u81ea\u6211\u63a8\u65ad\u3002", "conclusion": "\u7ed3\u679c\u63d0\u793a\uff0c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u6210\u7ee9\u4e0d\u80fd\u4ee3\u8868\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u9645\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7cbe\u7b80\u63a8\u7406\u7ed3\u6784\u53ef\u80fd\u53cd\u800c\u80fd\u63d0\u5347\u5bf9\u4e13\u5bb6\u8bf1\u5bfc\u4e0b\u8fce\u5408\u6027\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.17830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17830", "abs": "https://arxiv.org/abs/2601.17830", "authors": ["Mengmeng Wang", "Dengyang Jiang", "Liuzhuozheng Li", "Yucheng Lin", "Guojiang Shen", "Xiangjie Kong", "Yong Liu", "Guang Dai", "Jingdong Wang"], "title": "VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training", "comment": null, "summary": "Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \\textbf{\\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \\name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \\name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \\name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\\% extra GFLOPs with zero additional cost for external guidance models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u540d\u4e3a\\name\u7684\u8f7b\u91cf\u7ea7\u5185\u5728\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3VAE\u7279\u5f81\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u7684\u8bad\u7ec3\u6536\u655b\uff0c\u540c\u65f6\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u65e0\u987b\u989d\u5916\u7684\u5916\u90e8\u7f16\u7801\u5668\u6216\u53cc\u6a21\u578b\uff0c\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\u867d\u7136\u751f\u6210\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8bad\u7ec3\u6536\u655b\u6548\u7387\u4f4e\u3002\u5df2\u6709\u7684\u52a0\u901f\u65b9\u6848\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u6216\u53cc\u6a21\u578b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u3001\u8f7b\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\\name\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3VAE\u7684\u91cd\u5efa\u7279\u6027\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u9690\u7a7a\u95f4\u7279\u5f81\u4e0eVAE\u7279\u5f81\u901a\u8fc7\u8f7b\u91cf\u6295\u5f71\u5c42\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u5bf9\u9f50\u635f\u5931\u8fdb\u884c\u76d1\u7763\uff0c\u65e0\u9700\u5916\u90e8\u7279\u5f81\u7f16\u7801\u5668\u6216\u53cc\u6a21\u578b\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\\name\u5728\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u52a0\u5feb\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7840\u6269\u6563\u53d8\u6362\u5668\uff0c\u5e76\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u53ea\u589e\u52a04% GFLOPs\uff0c\u4e14\u65e0\u5916\u90e8\u989d\u5916\u6a21\u578b\u5f00\u9500\u3002", "conclusion": "\\name\u5b9e\u73b0\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7b80\u5355\u3001\u8ba1\u7b97\u5f00\u9500\u5c0f\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u52a0\u901f\u65b9\u6848\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u4f18\u9009\u62e9\u3002"}}
{"id": "2601.18350", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18350", "abs": "https://arxiv.org/abs/2601.18350", "authors": ["Junyi Zou"], "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs", "comment": null, "summary": "Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u533b\u7597\u9886\u57df\u5927\u6a21\u578b\u9002\u914d\u5668\u878d\u5408\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u9002\u914d\u5668\u878d\u5408\u65b9\u6cd5\u4ee5\u63d0\u5347\u533b\u5b66\u95ee\u7b54\u7684\u7cbe\u51c6\u6027\u548c\u6a21\u578b\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u533b\u5b66\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u5e38\u5e38\u9762\u4e34\u533b\u5b66\u672f\u8bed\u4e0d\u7cbe\u786e\u548c\u6307\u4ee4\u9075\u5faa\u6027\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u4e3a\u63d0\u5347\u5176\u9488\u5bf9\u533b\u7597\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u9700\u8981\u5e73\u8861\u6307\u4ee4\u6267\u884c\u80fd\u529b\u4e0e\u533b\u5b66\u77e5\u8bc6\u7684\u4fdd\u7559\u3002", "method": "\u8be5\u7814\u7a76\u4ee514B\u53c2\u6570\u57fa\u5ea7\u6a21\u578b\u4e3a\u4f8b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5LoRA\u9002\u914d\u7ba1\u9053\uff1a\uff081\uff09\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u5f15\u5165\u533b\u5b66\u77e5\u8bc6\uff1b\uff082\uff09\u4ee5\u6307\u4ee4\u98ce\u683c\u6570\u636e\uff0c\u901a\u8fc7\u6709\u76d1\u7763\u5fae\u8c03\u9002\u914d\u533b\u5b66\u95ee\u7b54\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u52a0\u6743\u9002\u914d\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u5c06SFT\u548cPT\u9002\u914d\u5668\u7ebf\u6027\u5408\u5e76\uff0c\u6700\u7ec8\u5bfc\u51fa\u878d\u5408\u540e\u7684\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u72ec\u7acb\u533b\u5b66\u9a8c\u8bc1\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u878d\u5408\u6a21\u578b\u5728BLEU-4\u3001ROUGE-1\u3001ROUGE-2\u3001ROUGE-L\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5206\u522b\u8fbe\u523016.38\u300120.42\u30014.60\u548c11.54\u3002\u540c\u65f6\u5bf9\u89e3\u7801\u654f\u611f\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u8fdb\u884c\u66f2\u7ebf\u548c\u5bf9\u6bd4\u5206\u6790\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u52a0\u6743\u9002\u914d\u5668\u878d\u5408\u65b9\u6cd5\u53ef\u4ee5\u5728\u63d0\u5347\u5927\u6a21\u578b\u533b\u7597\u9886\u57df\u80fd\u529b\u548c\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4e0d\u663e\u8457\u635f\u5931\u6307\u4ee4\u9075\u5faa\u6027\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5927\u6a21\u578b\u5fae\u8c03\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2601.17835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17835", "abs": "https://arxiv.org/abs/2601.17835", "authors": ["Baowen Zhang", "Chenxing Jiang", "Heng Li", "Shaojie Shen", "Ping Tan"], "title": "Geometry-Grounded Gaussian Splatting", "comment": "16 pages, 15 figures", "summary": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u548c\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u539f\u8bed\u89c6\u4e3a\u7279\u5b9a\u7684\u968f\u673a\u4f53\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u9ad8\u65af\u6295\u5f71\u7684\u9ad8\u8d28\u91cf\u4e09\u7ef4\u5f62\u72b6\u91cd\u5efa\u3002", "motivation": "\u5f53\u524d\u9ad8\u65af\u6295\u5f71\uff08GS\uff09\u6280\u672f\u867d\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ece\u9ad8\u65af\u539f\u8bed\u4e2d\u63d0\u53d6\u4e09\u7ef4\u5f62\u72b6\u4ecd\u7136\u5b58\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u5dee\u548c\u6d6e\u70b9\u566a\u58f0\u654f\u611f\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u63d0\u51fa\u66f4\u5065\u58ee\u7684\u51e0\u4f55\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u4ece\u7406\u8bba\u4e0a\u5c06\u9ad8\u65af\u539f\u8bed\u5efa\u6a21\u4e3a\u968f\u673a\u4f53\uff0c\u5e76\u636e\u6b64\u63d0\u51faGeometry-Grounded Gaussian Splatting\u65b9\u6cd5\uff0c\u76f4\u63a5\u5c06\u9ad8\u65af\u539f\u8bed\u4f5c\u4e3a\u663e\u5f0f\u51e0\u4f55\u8868\u793a\uff0c\u7ed3\u5408\u968f\u673a\u4f53\u7684\u4f53\u79ef\u5c5e\u6027\uff0c\u9ad8\u6548\u6e32\u67d3\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u4ee5\u63d0\u53d6\u7cbe\u7ec6\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8f83\u6240\u6709\u4ee5\u9ad8\u65af\u6295\u5f71\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u4e09\u7ef4\u5f62\u72b6\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u4e25\u8c28\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u521b\u65b0\uff0c\u672c\u6587\u4e3a\u9ad8\u65af\u6295\u5f71\u65b9\u6cd5\u4e0b\u7684\u9ad8\u8d28\u91cf\u5f62\u72b6\u91cd\u5efa\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u7a81\u7834\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0e\u6297\u566a\u80fd\u529b\u3002"}}
{"id": "2601.18352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18352", "abs": "https://arxiv.org/abs/2601.18352", "authors": ["Manjie Xu", "Isabella Yin", "Xinyi Tu", "Chi Zhang", "Yixin Zhu"], "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning", "comment": null, "summary": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9047\u5230\u4e0e\u5176\u9884\u8bad\u7ec3\u77e5\u8bc6\u76f8\u6096\u7684\u65b0\u89c4\u5219\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u6291\u5236\u5df2\u6709\u8ba4\u77e5\uff0c\u8fd9\u4e00\u73b0\u8c61\u88ab\u79f0\u4e3a\u8bed\u4e49\u60ef\u6027\u3002\u901a\u8fc7\u201cBaba Is You\u201d\u6e38\u620f\u4f5c\u4e3a\u5b9e\u9a8c\u5e73\u53f0\uff0c\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u89c4\u5219\u53d8\u5316\u4e0b\u6291\u5236\u5148\u9a8c\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5927\u6a21\u578b\u6709\u65f6\u8868\u73b0\u53cd\u800c\u4e0d\u5982\u5c0f\u6a21\u578b\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u5c06\u52a8\u6001\u89c4\u5219\u7528\u53ef\u6267\u884c\u4ee3\u7801\u8868\u793a\uff0c\u5e76\u91c7\u7528\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u6b62\u5148\u9a8c\u5e72\u6270\uff0c\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709LLM\u5bb9\u6613\u56e0\u9884\u8bad\u7ec3\u671f\u95f4\u56fa\u5316\u7684\u5e38\u8bc6\u5bfc\u81f4\u5728\u9762\u5bf9\u65b0\u89c4\u5219\u65f6\u65e0\u6cd5\u6b63\u786e\u63a8\u7406\uff0c\u4e14\u5927\u578b\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u5dee\u3002\u4f5c\u8005\u5e0c\u671b\u6df1\u5165\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\uff0c\u5177\u4f53\u8868\u73b0\u5728\u201cBaba Is You\u201d\u4e2d\u5982\u4f55\u51fa\u73b0\u53ca\u5982\u4f55\u89e3\u51b3\u3002", "method": "\u4f5c\u8005\u5728\u201cBaba Is You\u201d\u6e38\u620f\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u9047\u5230\u4e0e\u5e38\u8bc6\u76f8\u8fdd\u7684\u52a8\u6001\u89c4\u5219\uff08\u5982\u201cLava is Safe\u201d\uff09\u6765\u6d4b\u8bd5\u5176\u63a8\u7406\u80fd\u529b\u3002\u5206\u6790\u63d0\u51fa\u95ee\u9898\u7684\u6839\u6e90\u5728\u4e8e\u6a21\u578b\u5c06\u8bed\u4e49\u63cf\u8ff0\u548c\u903b\u8f91\u89c4\u5219\u6df7\u6dc6\u3002\u968f\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u7528\u4ee3\u7801\u5f62\u5f0f\u660e\u786e\u8868\u793a\u52a8\u6001\u89c4\u5219\uff0c\u5e76\u91c7\u7528Code-Grounded Vistas(LCV)\u8bad\u7ec3\u6cd5\uff0c\u8ba9\u6a21\u578b\u4e13\u6ce8\u903b\u8f91\u7ea6\u675f\u800c\u975e\u8868\u9762\u63cf\u8ff0\u3002\u8be5\u65b9\u6cd5\u6ce8\u91cd\u8bad\u7ec3\u65f6\u7684\u9488\u5bf9\u6027\u4f18\u5316\u800c\u975e\u9760\u63a8\u7406\u65f6\u7684\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5927\u578b\u6a21\u578b\u5728\u4ec5\u7528\u81ea\u7136\u8bed\u8a00\u8868\u793a\u89c4\u5219\u65f6\u8f83\u5c0f\u6a21\u578b\u66f4\u6613\u88ab\u539f\u6709\u5e38\u8bc6\u5e72\u6270\uff0c\u6291\u5236\u9519\u8bef\u5148\u9a8c\u80fd\u529b\u8f83\u5f31\u3002\u5f15\u5165\u4ee3\u7801\u5316\u89c4\u5219\u4e0eLCV\u65b9\u6cd5\u540e\uff0c\u6a21\u578b\u5bf9\u65b0\u89c4\u5219\u7684\u9002\u5e94\u80fd\u529b\u660e\u663e\u63d0\u5347\uff0c\u63a8\u7406\u66f4\u51c6\u786e\u4e14\u6548\u7387\u66f4\u9ad8\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u63a8\u7406\u65f6\u7684\u590d\u6742\u641c\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8868\u793a\u65b9\u5f0f\uff08\u81ea\u7136\u8bed\u8a00vs\u4ee3\u7801\uff09\u53ca\u8bad\u7ec3\u65b9\u5f0f\u51b3\u5b9a\u4e86\u5927\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u4ec5\u9760\u589e\u5927\u6a21\u578b\u89c4\u6a21\u65e0\u6cd5\u89e3\u51b3\u6240\u6709\u95ee\u9898\uff0c\u5b9e\u9645\u573a\u666f\u4e0b\u9700\u7ed3\u5408\u5408\u9002\u7684\u8868\u793a\u548c\u8bad\u7ec3\u673a\u5236\uff0c\u5c24\u5176\u5728\u52a8\u6001\u3001\u53cd\u5e38\u7684\u63a8\u7406\u9700\u6c42\u4e0b\u66f4\u662f\u5982\u6b64\u3002"}}
{"id": "2601.17857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17857", "abs": "https://arxiv.org/abs/2601.17857", "authors": ["Lan Yang", "Minghan Yang", "Ke Li", "Honggang Zhang", "Kaiyue Pang", "Yi-Zhe Song"], "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction", "comment": null, "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684fMRI\u56fe\u50cf\u91cd\u5efa\u6846\u67b6SynMind\uff0c\u901a\u8fc7\u5f15\u5165\u53e5\u5b50\u7ea7\u8bed\u4e49\u89e3\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u514b\u670d\u4ee5\u5f80\u65b9\u6cd5\u4ec5\u91cd\u89c6\u8868\u89c2\u4f4e\u5c42\u7279\u5f81\u4f46\u8bed\u4e49\u9519\u4f4d\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e3b\u6d41\u6700\u65b0\u65b9\u6cd5\uff0c\u4e14\u786c\u4ef6\u8d44\u6e90\u8981\u6c42\u4f4e\u3002", "motivation": "\u5f53\u524dfMRI\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u5728\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u5f80\u5f80\u5b58\u5728\u8bed\u4e49\u9519\u914d\u95ee\u9898\uff0c\u5373\u91cd\u5efa\u56fe\u50cf\u867d\u7136\u81ea\u7136\u4f46\u5185\u5bb9\u4e0e\u771f\u5b9e\u523a\u6fc0\u4e0d\u7b26\u3002\u9020\u6210\u8fd9\u79cd\u60c5\u51b5\u7684\u539f\u56e0\u5728\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u4f4e\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u7684\u5d4c\u5165\uff0c\u800c\u6ca1\u6709\u5bf9\u5927\u8111\u4fe1\u53f7\u4e2d\u7684\u663e\u5f0f\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u6709\u6548\u89e3\u91ca\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u601d\u8003fMRI\u89e3\u7801\u4e2d\u7684\u8bed\u4e49\u8868\u8fbe\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u6d41\uff0c\u5c06fMRI\u4fe1\u53f7\u89e3\u6790\u4e3a\u5bcc\u6709\u5c42\u6b21\u548c\u7ec4\u5408\u7ed3\u6784\u7684\u3001\u7c7b\u4eba\u53e5\u5b50\u7ea7\u8bed\u4e49\u63cf\u8ff0\u3002\u5177\u4f53\u5730\uff0c\u5229\u7528\u57fa\u7840\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u5305\u542b\u5bf9\u8c61\u8eab\u4efd\u548c\u7a7a\u95f4\u7ec4\u7ec7\u7684\u591a\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\uff0c\u518d\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u8f93\u5165\u5230\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u8fdb\u884c\u56fe\u50cf\u751f\u6210\u3002\u6700\u7ec8\u5f62\u6210SynMind\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u7f16\u7801\u9a71\u52a8\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSynMind\u5728\u591a\u4e2a\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u4f7f\u7528\u5c3a\u5bf8\u66f4\u5c0f\u3001\u7b97\u529b\u6d88\u8017\u66f4\u4f4e\u7684Stable Diffusion 1.4\u6a21\u578b\uff08\u76f8\u8f83\u4e8eSDXL\uff09\uff0c\u914d\u5408\u5355\u5361\u6d88\u8d39\u7ea7GPU\uff0cSynMind\u4ecd\u80fd\u83b7\u5f97\u66f4\u4f18\u8868\u73b0\u3002\u5927\u89c4\u6a21\u4eba\u5de5\u4e3b\u89c2\u8bc4\u4ef7\u9a8c\u8bc1\u4e86\u5176\u91cd\u5efa\u56fe\u50cf\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u66f4\u4e00\u81f4\u3002\u795e\u7ecf\u53ef\u89c6\u5316\u5206\u6790\u8868\u660e\uff0cSynMind\u6fc0\u6d3b\u4e86\u66f4\u5e7f\u6cdb\u4e14\u66f4\u5177\u8bed\u4e49\u76f8\u5173\u6027\u7684\u8111\u533a\uff0c\u51cf\u5f31\u4e86\u5bf9\u9ad8\u7ea7\u89c6\u89c9\u533a\u57df\u7684\u4f9d\u8d56\u3002", "conclusion": "SynMind\u901a\u8fc7\u5f15\u5165\u660e\u786e\u7684\u8bed\u4e49\u89e3\u6790\u548c\u6587\u672c\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86fMRI\u56fe\u50cf\u91cd\u5efa\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u89c6\u89c9\u5408\u7406\u6027\u3002\u6b64\u65b9\u6cd5\u4e0d\u4ec5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u8fd8\u5177\u5907\u8f83\u9ad8\u7684\u786c\u4ef6\u9002\u5e94\u6027\u548c\u63a8\u5e7f\u6f5c\u529b\uff0c\u6709\u671b\u63a8\u52a8\u57fa\u4e8e\u5927\u8111\u4fe1\u53f7\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u91cd\u5efa\u548c\u795e\u7ecf\u8868\u5f81\u7406\u89e3\u3002"}}
{"id": "2601.18374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18374", "abs": "https://arxiv.org/abs/2601.18374", "authors": ["Rodrigo Silva", "Jos\u00e9 Evans", "Jos\u00e9 Isidro", "Miguel Marques", "Afonso Fonseca", "Ricardo Morais", "Jo\u00e3o Canavilhas", "Arian Pasquali", "Purifica\u00e7\u00e3o Silvano", "Al\u00edpio Jorge", "Nuno Guimar\u00e3es", "S\u00e9rgio Nunes", "Ricardo Campos"], "title": "CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes", "comment": null, "summary": "City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86CitiLink\u5e73\u53f0\uff0c\u5229\u7528NLP\u548cIR\u6280\u672f\u5c06\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u68c0\u7d22\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u653f\u5e9c\u900f\u660e\u5ea6\u548c\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u867d\u516c\u5f00\uff0c\u4f46\u7ed3\u6784\u6742\u4e71\u4e14\u5197\u957f\uff0c\u666e\u901a\u5e02\u6c11\u548c\u8bb0\u8005\u96be\u4ee5\u9ad8\u6548\u67e5\u627e\u6240\u9700\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u6539\u5584\u5176\u53ef\u7528\u6027\u548c\u68c0\u7d22\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86CitiLink\u5e73\u53f0\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u62bd\u53d6\u4f1a\u8bae\u5143\u6570\u636e\u3001\u8bae\u9898\u3001\u6295\u7968\u7ed3\u679c\uff0c\u5b58\u5165\u6570\u636e\u5e93\uff0c\u5e76\u5229\u7528BM25\u5b9e\u73b0\u5168\u6587\u68c0\u7d22\u548c\u591a\u7ef4\u7b5b\u9009\u3002\u5e73\u53f0\u754c\u9762\u53cb\u597d\uff0c\u652f\u6301\u6613\u7528\u4ea4\u4e92\u3002\u4ee56\u4e2a\u8461\u8404\u7259\u57ce\u5e02\u3001\u5171120\u4efd\u7eaa\u8981\u4e3a\u5b9e\u9a8c\u6570\u636e\uff0c\u5bf9\u5e02\u653f\u5de5\u4f5c\u4eba\u5458\u5b9e\u9645\u64cd\u4f5c\u8fdb\u884c\u6307\u5bfc\u6d4b\u8bd5\uff0c\u5e76\u5229\u7528Gemini\u6a21\u578b\u8bc4\u4f30\u4fe1\u606f\u62bd\u53d6\u7684\u6709\u6548\u6027\u3002", "result": "\u7cfb\u7edf\u63d0\u5347\u4e86\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u7684\u4fe1\u606f\u68c0\u7d22\u4e0e\u53ef\u8bbf\u95ee\u6027\uff0c\u7528\u6237\u6d4b\u8bd5\u83b7\u5f97\u4e86\u5b9e\u9645\u4f7f\u7528\u53cd\u9988\uff0cGemini\u6a21\u578b\u5728\u76f8\u5173\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u6548\u3002", "conclusion": "CitiLink\u5e73\u53f0\u8bc1\u660e\u4e86NLP\u4e0eIR\u7ed3\u5408\u80fd\u591f\u63d0\u5347\u5e02\u653f\u6570\u636e\u7684\u7ed3\u6784\u5316\u548c\u900f\u660e\u5ea6\uff0c\u5bf9\u63d0\u5347\u5730\u65b9\u653f\u5e9c\u516c\u5f00\u6570\u636e\u7684\u5229\u7528\u548c\u900f\u660e\u5ea6\u5177\u6709\u501f\u9274\u610f\u4e49\u3002"}}
{"id": "2601.17862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17862", "abs": "https://arxiv.org/abs/2601.17862", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment", "comment": null, "summary": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u589e\u5f3a\u548c\u8f7b\u91cf\u7ea7\u67b6\u6784\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u53ef\u5728\u65e0\u771f\u5b9e\u591a\u4e2d\u5fc3\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u533b\u5b66\u5f71\u50cfAI\u6a21\u578b\u5728\u65b0\u57df\uff08\u4e2d\u5fc3\u3001\u8bbe\u5907\uff09\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5f71\u50cfAI\u6a21\u578b\u5728\u5355\u4e2d\u5fc3/\u5355\u8bbe\u5907\u4e0a\u6548\u679c\u5f88\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u591a\u4e2d\u5fc3\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u57df\u504f\u79fb\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4ee5MobileNetV2\u4e3a\u57fa\u7840\u7684\u57df\u4e0d\u53d8\u7f16\u7801\u5668\uff0c\u5e76\u4f18\u5316\u4e09\u5927\u7ec4\u4ef6\uff1a1\uff09\u7528\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9510\u5316\u548c\u566a\u58f0\u6270\u52a8\u6a21\u62df\u591a\u57df\u6210\u50cf\u6761\u4ef6\uff1b2\uff09\u901a\u8fc7\u57df\u5bf9\u6297\u8bad\u7ec3\uff08\u68af\u5ea6\u53cd\u8f6c\uff09\u6291\u5236\u57df\u5224\u522b\u7279\u5f81\uff1b3\uff09\u4f7f\u7528\u8f7b\u91cf\u5316\u91cf\u5b50\u7279\u5f81\u589e\u5f3a\u5c42\uff0c\u901a\u8fc7\u53ef\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u975e\u7ebf\u6027\u6620\u5c04\u548c\u7ea0\u7f20\u5efa\u6a21\u3002\u63a8\u7406\u9636\u6bb5\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\u8fdb\u4e00\u6b65\u7f13\u89e3\u5206\u5e03\u6f02\u79fb\u3002", "result": "\u5728\u6a21\u62df\u591a\u4e2d\u5fc3\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u65b0\u57df\u4e0a\u6bd4\u6ca1\u6709\u57df\u6cdb\u5316\u6216\u672a\u4f7f\u7528\u91cf\u5b50\u589e\u5f3a\u7684\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u5347AUC\u548c\u654f\u611f\u6027\uff0c\u964d\u4f4e\u4e86\u57df\u7279\u5f02\u6027\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u91cf\u5b50\u589e\u5f3a\u7684\u57df\u6cdb\u5316\u6280\u672f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u53ef\u4e3a\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u533b\u5b66\u5f71\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e00\u5957\u53ef\u884c\u7684\u8303\u5f0f\u65b9\u6848\u3002"}}
{"id": "2601.18375", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18375", "abs": "https://arxiv.org/abs/2601.18375", "authors": ["Jonas Golde", "Nicolaas Jedema", "Ravi Krishnan", "Phong Le"], "title": "Hierarchical Text Classification with LLM-Refined Taxonomies", "comment": null, "summary": "Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86TaxMorph\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4f18\u5316\u5c42\u7ea7\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u6807\u7b7e\u4f53\u7cfb\uff0c\u7ecf\u5b9e\u9a8c\u8bc1\u660e\u4f18\u5316\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c42\u7ea7\u6587\u672c\u5206\u7c7b\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u6807\u7b7e\u4f53\u7cfb\uff0c\u4f46\u8fd9\u4e9b\u4f53\u7cfb\u5e38\u5b58\u5728\u6b67\u4e49\uff0c\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u6548\u679c\u3002\u56e0\u6b64\uff0c\u671f\u671b\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u751f\u6210\u66f4\u9002\u5408\u6a21\u578b\u5b66\u4e60\u7684\u6807\u7b7e\u4f53\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86TaxMorph\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5b8c\u6574\u6807\u7b7e\u5c42\u7ea7\u8fdb\u884c\u91cd\u547d\u540d\u3001\u5408\u5e76\u3001\u62c6\u5206\u548c\u91cd\u6392\u5e8f\u7b49\u64cd\u4f5c\uff0c\u91cd\u65b0\u4fee\u6b63\u6807\u7b7e\u4f53\u7cfb\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u3002", "result": "\u5728\u4e09\u4e2a\u5c42\u7ea7\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cLLM\u4f18\u5316\u540e\u7684\u6807\u7b7e\u4f53\u7cfb\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0bF1\u503c\u6700\u9ad8\u63d0\u53472.9\u4e2a\u767e\u5206\u70b9\u3002\u5206\u6790\u8868\u660e\uff0c\u867d\u7136\u4eba\u5de5\u4f53\u7cfb\u4e0b\u7684\u7c7b\u522b\u5d4c\u5165\u66f4\u5206\u79bb\uff0c\u4f46LLM\u4f18\u5316\u4f53\u7cfb\u66f4\u5951\u5408\u6a21\u578b\u771f\u5b9e\u5206\u7c7b\u504f\u5dee\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684\u6807\u7b7e\u4f53\u7cfb\u4f18\u5316\u80fd\u63d0\u5347\u5c42\u7ea7\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u56e0\u4e3a\u4f18\u5316\u540e\u7684\u4f53\u7cfb\u66f4\u8d34\u5408\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u5f0f\u548c\u5f52\u7eb3\u504f\u597d\u3002"}}
{"id": "2601.17866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17866", "abs": "https://arxiv.org/abs/2601.17866", "authors": ["Yoonwoo Jeong", "Cheng Sun", "Yu-Chiang Frank Wang", "Minsu Cho", "Jaesung Choe"], "title": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance", "comment": "Project page, https://jaesung-choe.github.io/mv_sam/index.html", "summary": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6MV-SAM\uff0c\u5b9e\u73b0\u591a\u89c6\u56fe\u5206\u5272\u5e76\u4fdd\u8bc13D\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u989d\u59163D\u7f51\u7edc\u62163D\u6807\u6ce8\u6570\u636e\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u901a\u7528\u3002", "motivation": "\u73b0\u6709\u7684promptable segmentation\u65b9\u6cd5\uff08\u5982SAM\uff09\u5df2\u5c06\u5206\u5272\u6269\u5c55\u5230\u89c6\u9891\u548c\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f3D\u611f\u77e5\uff0c\u5bfc\u81f4\u5206\u5272\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e14\u901a\u5e38\u9700\u4ee3\u4ef7\u9ad8\u6602\u7684\u6bcf\u573a\u666f\u4f18\u5316\u6765\u4fdd\u8bc13D\u4e00\u81f4\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MV-SAM\u6846\u67b6\uff0c\u5229\u7528pointmaps\uff08\u4e00\u79cd\u65e0\u9700\u76f8\u673a\u4f4d\u59ff\u5373\u53ef\u91cd\u5efa\u76843D\u70b9\u4e91\uff09\u5c06\u56fe\u50cf\u548c\u63d0\u793a\u4fe1\u53f7\u63d0\u5347\u52303D\u7a7a\u95f4\uff0c\u6574\u5408SAM\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5e76\u7528transformer\u8fdb\u884c\u89e3\u7801\uff0c\u91c7\u75283D\u7a7a\u95f4\u7684\u63d0\u793a\u548c\u50cf\u7d20-\u70b9\u4e00\u5bf9\u4e00\u6620\u5c04\uff0c\u5b9e\u73b02D\u548c3D\u4ea4\u4e92\u7684\u8026\u5408\u3002", "result": "MV-SAM\u5728SA-1B\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\uff08\u5982NVOS\u3001SPIn-NeRF\u3001ScanNet++\u3001uCo3D\u548cDL3DV\uff09\u4e0a\u53d6\u5f97\u4e86\u6bd4SAM2-Video\u66f4\u4f18\u6216\u76f8\u5f53\u4e8e\u6bcf\u573a\u666f\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MV-SAM\u514b\u670d\u4e86\u73b0\u6709promptable segmentation\u65b9\u6cd5\u7f3a\u4e4f3D\u4e00\u81f4\u6027\u7684\u96be\u9898\uff0c\u65e0\u97003D\u663e\u5f0f\u7f51\u7edc\u62163D\u6807\u6ce8\u6570\u636e\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u5206\u5272\uff0c\u5177\u5907\u826f\u597d\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.18380", "categories": ["cs.CL", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18380", "abs": "https://arxiv.org/abs/2601.18380", "authors": ["Ignatius Ezeani"], "title": "Corpus-Based Approaches to Igbo Diacritic Restoration", "comment": "270 page. Ph.D. Thesis. The University of Sheffield", "summary": "With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.\n  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u4ee5\u4f0a\u535a\u8bed\u4e3a\u4f8b\uff09\u4e2d\u7684\u53d8\u97f3\u7b26\u53f7\u6b67\u4e49\u95ee\u9898\uff0c\u7efc\u8ff0\u4e86\u73b0\u6709\u6d88\u6b67\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u751f\u6210\u6d88\u6b67\u6570\u636e\u96c6\u548c\u6d88\u6b67\u65b9\u6cd5\u7684\u6846\u67b6\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u591a\u96c6\u4e2d\u4e8e\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\uff0c\u800c\u5168\u7403\u7edd\u5927\u591a\u6570\u8bed\u8a00\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u4e0e\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u53d8\u97f3\u7b26\u53f7\u8fd8\u539f\u4e0e\u6d88\u6b67\u9886\u57df\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u76f8\u5173\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u5176\u5728NLP\u4e2d\u7684\u53ef\u5904\u7406\u6027\u3002", "method": "\u9996\u5148\u7efc\u8ff0\u4e86\u53d8\u97f3\u7b26\u53f7\u6b67\u4e49\u4e0e\u6d88\u6b67\u65b9\u6cd5\u3002\u9488\u5bf9\u4f0a\u535a\u8bed\uff0c\u63d0\u51fa\u4e86\u7075\u6d3b\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u6d88\u6b67\u65b9\u6cd5\uff1a1\uff09\u6807\u51c6n-gram\u6a21\u578b\uff0c\u6839\u636e\u76ee\u6807\u8bcd\u524d\u5e8f\u8bcd\u9884\u6d4b\u6b63\u786e\u53d8\u4f53\uff1b2\uff09\u5206\u7c7b\u6a21\u578b\uff0c\u5229\u7528\u76ee\u6807\u8bcd\u524d\u540e\u7a97\u53e3\u7684\u4e0a\u4e0b\u6587\u8bcd\u6c47\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u5224\u65ad\uff1b3\uff09\u5d4c\u5165\u6a21\u578b\uff0c\u5c06\u4e0a\u4e0b\u6587\u5d4c\u5165\u4e0e\u5404\u5019\u9009\u53d8\u4f53\u5d4c\u5165\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u7528\u4ee5\u5224\u522b\u6b63\u786e\u53d8\u4f53\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u6d88\u6b67\u6570\u636e\u96c6\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e09\u7c7b\u6a21\u578b\uff0c\u80fd\u591f\u9488\u5bf9\u4f0a\u535a\u8bed\u5b9e\u73b0\u53d8\u97f3\u7b26\u53f7\u7684\u81ea\u52a8\u6d88\u6b67\u3002", "conclusion": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u79cd\u4f0a\u535a\u8bed\uff0c\u672c\u6587\u63d0\u51fa\u7684\u4e09\u79cd\u65b9\u6cd5\u5747\u53ef\u7528\u4e8e\u53d8\u97f3\u7b26\u53f7\u6d88\u6b67\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.17868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17868", "abs": "https://arxiv.org/abs/2601.17868", "authors": ["Zhihao He", "Tieyuan Chen", "Kangyu Wang", "Ziran Qin", "Yang Shao", "Chaofan Gan", "Shijie Li", "Zuxuan Wu", "Weiyao Lin"], "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding", "comment": null, "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578bVidLaDA\uff0c\u901a\u8fc7\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53cc\u5411\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u52a0\u901f\u673a\u5236\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video LLMs\uff09\u56e0\u56e0\u679c\u63a9\u7801\u5bfc\u81f4\u53ea\u80fd\u5355\u5411\u5efa\u6a21\uff0c\u96be\u4ee5\u6355\u6349\u5b8c\u6574\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fdb\u800c\u9650\u5236\u4e86\u6a21\u578b\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u5728\u5904\u7406\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u65f6\uff0c\u63a8\u7406\u6548\u7387\u4e5f\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u63d0\u51faVidLaDA\uff0c\u901a\u8fc7\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u89c6\u9891\u65f6\u7a7a\u7684\u5168\u5c40\u5efa\u6a21\u3002\u53e6\u5916\uff0c\u8bbe\u8ba1\u4e86MARS-Cache\u673a\u5236\uff0c\u901a\u8fc7\u5f02\u6b65\u89c6\u89c9\u7f13\u5b58\u66f4\u65b0\u4e0e\u6309\u5e27\u5206\u5757\u6ce8\u610f\u529b\uff0c\u65e2\u80fd\u6709\u6548\u53bb\u9664\u5197\u4f59\uff0c\u53c8\u4fdd\u6301\u5168\u5c40\u5173\u8054\u6027\uff0c\u5927\u5e45\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVidLaDA\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u57fa\u7ebf\uff0c\u5e76\u8fbe\u5230\u4e3b\u6d41\u6700\u65b0\u81ea\u56de\u5f52\u89c6\u9891LLM\uff08\u5982Qwen2.5-VL\u548cLLaVA-Video\uff09\u76f8\u5ab2\u7f8e\u7684\u8868\u73b0\uff1b\u540c\u65f6\uff0cMARS-Cache\u63a8\u7406\u63d0\u901f12\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u51c6\u786e\u7387\u65e0\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "VidLaDA\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u517c\u5177\u9ad8\u6548\u7684\u5168\u5c40\u5efa\u6a21\u4e0e\u63a8\u7406\u901f\u5ea6\uff0c\u8868\u660e\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u548c\u521b\u65b0\u7f13\u5b58\u67b6\u6784\u5728\u89c6\u9891LLM\u9886\u57df\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u53c2\u8003\u7684\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2601.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18395", "abs": "https://arxiv.org/abs/2601.18395", "authors": ["Mikel Zubillaga", "Oscar Sainz", "Oier Lopez de Lacalle", "Eneko Agirre"], "title": "Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction", "comment": "Submitted to IJCAI-ECAI 2026", "summary": "Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ThinkTwice\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9LLM\u751f\u6210\u7684\u591a\u4e2a\u6587\u6863\u4fe1\u606f\u62bd\u53d6\u5019\u9009\u6a21\u677f\u8fdb\u884c\u91c7\u6837\u4e0e\u7b5b\u9009\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62bd\u53d6\u6548\u679c\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u8d2a\u5a6a\u89e3\u7801\u548cSOTA\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u6587\u6863\u7ea7\u4fe1\u606f\u62bd\u53d6\u591a\u4f9d\u8d56\u4e8e\u8d2a\u5a6a\u89e3\u7801\uff0c\u8fd9\u9650\u5236\u4e86\u8f93\u51fa\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002\u4f5c\u8005\u6ce8\u610f\u5230\u91c7\u6837\u53ef\u4ee5\u83b7\u5f97\u66f4\u4f18\u7ed3\u679c\uff0c\u5e0c\u671b\u901a\u8fc7\u591a\u4e2a\u5019\u9009\u8f93\u51fa\u7684\u6311\u9009\uff0c\u63d0\u5347DocIE\u4efb\u52a1\u7684\u6574\u4f53\u8868\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a1\uff09LLM\u751f\u6210\u591a\u4e2a\u5019\u9009\u8f93\u51fa\uff08\u91c7\u7528\u91c7\u6837\uff0c\u975e\u8d2a\u5a6a\uff09\uff1b2\uff09\u5f15\u5165\u9009\u62e9\u6a21\u5757\u6311\u9009\u6700\u4f73\u6a21\u677f\u3002\u9009\u62e9\u5206\u4e3a\u65e0\u76d1\u7763\uff08\u901a\u8fc7\u5019\u9009\u95f4\u4e00\u81f4\u6027\uff09\u548c\u6709\u76d1\u7763\uff08\u57fa\u4e8e\u6709\u6807\u7b7e\u6570\u636e\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\uff09\u3002\u6b64\u5916\uff0c\u9488\u5bf9DocIE\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u7a00\u7f3a\uff0c\u63d0\u51fa\u62d2\u7edd\u91c7\u6837\u751f\u6210\u94f6\u6807\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cThinkTwice\u7684\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u65b9\u6cd5\u5747\u4f18\u4e8e\u8d2a\u5a6a\u89e3\u7801\u53ca\u73b0\u6709\u6700\u4f18DocIE\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u91c7\u6837\u4e0e\u7504\u9009\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u91c7\u6837\u7ed3\u5408\u6a21\u677f\u7b5b\u9009\u80fd\u5145\u5206\u6316\u6398LLM\u5728\u6587\u6863\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3aDocIE\u5e26\u6765\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff0c\u63d0\u4f9b\u4e86\u65b0\u9896\u6709\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2601.17880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17880", "abs": "https://arxiv.org/abs/2601.17880", "authors": ["Muhammad Umar Salman", "Mohammad Areeb Qazi", "Mohammed Talha Alam"], "title": "Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran", "comment": "6 pages, 2 tables and 2 figures", "summary": "We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset", "AI": {"tldr": "Quran MD \u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u53e4\u5170\u7ecf\u6570\u636e\u96c6\uff0c\u5305\u542b\u6587\u672c\u3001\u8bed\u8a00\u5b66\u3001\u97f3\u9891\u4fe1\u606f\uff0c\u652f\u6301\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u8bed\u97f3\u4efb\u52a1\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7ed3\u5408\u53e4\u5170\u7ecf\u6587\u672c\u4e0e\u591a\u6837\u97f3\u9891\u6717\u8bf5\uff08\u5305\u542b\u4e0d\u540c\u98ce\u683c\u3001\u65b9\u8a00\uff09\u7684\u516c\u5f00\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u76f8\u5173\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001TTS\u3001\u8bed\u97f3\u98ce\u683c\u5206\u6790\u7b49\u591a\u79cd\u7814\u7a76\u548c\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u4e3a\u6570\u5b57\u4f0a\u65af\u5170\u5b66\u548c\u591a\u79cd\u8ba1\u7b97\u65b9\u6cd5\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u6536\u96c6\u4e86\u6bcf\u8282\u7ecf\u6587\uff08ayah\uff09\u7684\u963f\u62c9\u4f2f\u6587\u3001\u82f1\u6587\u7ffb\u8bd1\u3001\u97f3\u8bd1\uff0c\u4ee5\u53ca\u753132\u4f4d\u4e0d\u540c\u98ce\u683c\u7684\u6717\u8bf5\u8005\u5f55\u5236\u7684\u97f3\u9891\u3002\u6bcf\u4e2a\u8bcd\uff08token\uff09\u5c42\u7ea7\u4e5f\u914d\u6709\u963f\u6587\u3001\u82f1\u6587\u3001\u97f3\u8bd1\u53ca\u5bf9\u5e94\u97f3\u9891\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u7684\u53d1\u97f3\u548c\u8bed\u4e49\u5206\u6790\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u6587\u5b57\u3001\u9010\u8bcd\u53d1\u97f3\u3001\u8de8\u6717\u8bf5\u8005\u591a\u6837\u97f3\u9891\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\uff0c\u80fd\u7528\u4e8eNLP\u3001ASR\u3001TTS\u3001\u8bed\u8a00\u5b66\u3001\u98ce\u683c\u8fc1\u79fb\u7b49\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "Quran MD\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u53e4\u5170\u7ecf\u7814\u7a76\u548c\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c06\u63a8\u52a8\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u98ce\u683c\u5206\u6790\u3001\u8bed\u97f3\u5408\u6210\u3001\u7ecf\u6587\u68c0\u7d22\u7b49\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18415", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18415", "abs": "https://arxiv.org/abs/2601.18415", "authors": ["Ivan Bondarenko", "Daniil Grebenkin", "Oleg Sedukhin", "Mikhail Klementev", "Roman Derunets", "Lyudmila Budneva"], "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews", "comment": null, "summary": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u6b3e\u540d\u4e3aPisets\u7684\u8bed\u97f3\u8f6c\u6587\u5b57\u7cfb\u7edf\uff0c\u91c7\u7528\u4e09\u91cd\u67b6\u6784\u5e76\u5f15\u5165\u591a\u79cd\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u548c\u964d\u4f4e\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u6548\u679c\u4f18\u4e8eWhisper\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff08\u5982Whisper\uff09\u5728\u957f\u97f3\u9891\u548c\u590d\u6742\u58f0\u5b66\u6761\u4ef6\u4e0b\u5b58\u5728\u9519\u8bef\u7387\u9ad8\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u5728\u4fc4\u8bed\u8bed\u6599\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e3a\u51c6\u786e\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u670d\u52a1\u79d1\u5b66\u5bb6\u548c\u8bb0\u8005\u7b49\u4e13\u4e1a\u7528\u6237\u3002", "method": "Pisets\u7cfb\u7edf\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u9996\u5148\u7528Wav2Vec2\u8fdb\u884c\u521d\u6b65\u8bc6\u522b\uff0c\u518d\u7528Audio Spectrogram Transformer (AST)\u8fc7\u6ee4\u5047\u9633\u6027\uff0c\u6700\u540e\u7531Whisper\u6a21\u578b\u5b8c\u6210\u8bed\u97f3\u8f6c\u6587\u5b57\u3002\u540c\u65f6\uff0c\u5f15\u5165\u8bfe\u7a0b\u5b66\u4e60\uff08curriculum learning\uff09\u548c\u591a\u6837\u5316\u7684\u4fc4\u8bed\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u5ea6\u5efa\u6a21\u6280\u672f\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u4fc4\u8bed\u8bed\u97f3\u8bed\u6599\u96c6\u6d4b\u8bd5\uff0cPisets\u7cfb\u7edf\u5728\u957f\u97f3\u9891\u3001\u591a\u79cd\u58f0\u5b66\u6761\u4ef6\u4e0b\u7684\u8bc6\u522b\u6548\u679c\u663e\u8457\u4f18\u4e8eWhisper\u53caWhisperX\uff0c\u964d\u4f4e\u4e86\u9519\u8bef\u548c\u5e7b\u89c9\u6982\u7387\u3002", "conclusion": "Pisets\u7cfb\u7edf\u80fd\u66f4\u9c81\u68d2\u5730\u5904\u7406\u590d\u6742\u5b9e\u9645\u73af\u5883\u4e0b\u7684\u4fc4\u8bed\u8bed\u97f3\u8f6c\u6587\u5b57\u4efb\u52a1\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\uff0c\u5b9e\u8df5\u610f\u4e49\u5f3a\uff0c\u4e3a\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u65b0\u9009\u62e9\u3002"}}
{"id": "2601.18468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18468", "abs": "https://arxiv.org/abs/2601.18468", "authors": ["Daniel B. Hier", "Tayo Obafemi-Ajayi"], "title": "Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models", "comment": null, "summary": "Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama 3.1 8B Instruct\uff09\u5728\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u5bf9\u751f\u7269\u533b\u5b66\u672c\u4f53\u672f\u8bed\u6620\u5c04\u77e5\u8bc6\u7684\u5b58\u50a8\u4e0e\u63d0\u53d6\u60c5\u51b5\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5bf9\u77e5\u8bc6\u7684\u638c\u63e1\u4e0d\u5747\uff1a\u90e8\u5206\u77e5\u8bc6\u4ee5\u6f5c\u5728\u5f62\u5f0f\u5b58\u5728\uff0c\u9700\u8981\u7279\u5b9a\u89e3\u7801\u6280\u5de7\u624d\u53ef\u63d0\u53d6\uff0c\u5bf9\u65b0\u77e5\u8bc6\u6cdb\u5316\u6709\u9650\u4e14\u6613\u9000\u5316\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u201c\u6f5c\u5728\u77e5\u8bc6\u201d\uff08\u901a\u8fc7\u968f\u673a\u89e3\u7801\u53ef\u83b7\u4f46\u5e38\u89c4\u89e3\u7801\u4e0d\u53ef\u5f97\u7684\u77e5\u8bc6\uff09\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5bf9\u65b0\u77e5\u8bc6\u5b66\u4e60\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7a76\u6a21\u578b\u8bb0\u5fc6\u5982\u4f55\u968f\u8bad\u7ec3\u53d8\u5316\u9000\u5316\u3002", "method": "\u5c06Llama 3.1 8B Instruct\u5728\u4eba\u4f53\u8868\u578b\u672c\u4f53\uff08HPO\uff09\u548c\u57fa\u56e0\u672c\u4f53\uff08GO\uff09\u672f\u8bed\u6620\u5c04\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u89e3\u7801\u68c0\u6d4b\u6f5c\u5728\u77e5\u8bc6\uff0c\u5e76\u91c7\u7528Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u5206\u6790\u5f71\u54cd\u5b66\u4e60\u3001\u6cdb\u5316\u548c\u9057\u5fd8\u7684\u9884\u6d4b\u56e0\u7d20\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5bf9HPO\u7684\u56de\u5fc6\u7387\u75312.8%\u63d0\u5347\u523071.9%\u3002\u62e5\u6709\u6f5c\u5728\u77e5\u8bc6\u7684\u4e8b\u5b9e\uff0c\u88ab\u6a21\u578b\u66f4\u5feb\u638c\u63e1\uff08\u98ce\u9669\u6bd42.6\uff09\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u5cf0\u5b66\u4e60\u7387\u548c\u66f4\u5feb\u6536\u655b\u901f\u5ea6\u3002\u65b0\u77e5\u8bc6\u6cdb\u5316\u8f83\u5dee\uff0c\u4f46\u6709\u6f5c\u5728\u77e5\u8bc6\u65f6\u51e0\u7387\u589e\u52a0\u3002\u89c1\u8fc7\u7684\u65b0\u77e5\u8bc6\u5bf9\u9000\u5316\u66f4\u6709\u4fdd\u62a4\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u77e5\u8bc6\u662f\u52a0\u901f\u4e8b\u5b9e\u5b66\u4e60\u7684\u5f3a\u529b\u9884\u6d4b\u56e0\u7d20\uff0c\u4f46\u6a21\u578b\u5bf9\u65b0\u672c\u4f53\u672f\u8bed\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5df2\u5b66\u77e5\u8bc6\u6613\u56e0\u672a\u88ab\u6301\u7eed\u5f3a\u5316\u800c\u9000\u5316\u3002"}}
{"id": "2601.18483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18483", "abs": "https://arxiv.org/abs/2601.18483", "authors": ["Arya Labroo", "Ivaxi Sheth", "Vyas Raina", "Amaani Ahmed", "Mario Fritz"], "title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs", "comment": "Accepted for publication at EACL main conference", "summary": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5fae\u8c03\u6587\u672c\u5c5e\u6027\u63a7\u5236\u80fd\u529b\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5355\u4e00\u548c\u7ec4\u5408\u6982\u5ff5\u7684\u63a7\u5236\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7ec4\u5408\u5c5e\u6027\u63a7\u5236\u65b9\u9762\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5f3a\u5927\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u5e38\u5e38\u9700\u8981\u5bf9\u5982\u5e7d\u9ed8\u6027\u3001\u8bf4\u670d\u529b\u6216\u6b63\u5f0f\u7a0b\u5ea6\u7b49\u5177\u4f53\u6587\u672c\u5c5e\u6027\u8fdb\u884c\u7ec6\u81f4\u64cd\u63a7\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4ec5\u80fd\u63a7\u5236\u5355\u4e00\u5c5e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6982\u5ff5\u7ec6\u7c92\u5ea6\u7ec4\u5408\u63a7\u5236\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4e0e\u6d4b\u8bc4\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u8861\u91cfLLM\u5728\u5355\u4e00\u548c\u7ec4\u5408\u5c5e\u6027\u63a7\u5236\u4e0b\u7684\u751f\u6210\u8868\u73b0\uff0c\u9009\u53d6\u4e86\u5982\u8bf4\u670d\u6027\u4e0e\u5e7d\u9ed8\u611f\u8fd9\u79cd\u8bed\u8a00\u5b66\u4e0a\u533a\u5206\u660e\u663e\u7684\u5c5e\u6027\u5bf9\uff0c\u7cfb\u7edf\u5730\u6d4b\u8bd5\u591a\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0b\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65e0\u8bba\u662f\u54ea\u79cd\u6a21\u578b\uff0c\u5728\u9700\u8981\u540c\u65f6\u63a7\u5236\u4e24\u4e2a\u5c5e\u6027\u65f6\uff0c\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u5c5e\u6027\u6309\u7406\u8bf4\u5e94\u8be5\u662f\u5f7c\u6b64\u72ec\u7acb\u4e14\u53ef\u7ec4\u5408\u7684\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8e\u63d0\u793a\u8bcd\u7684\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5b9e\u73b0\u591a\u5c5e\u6027\u7ec4\u5408\u63a7\u5236\uff0c\u6a21\u578b\u5728\u5c5e\u6027\u7ec4\u5408\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u672c\u8d28\u5c40\u9650\u3002\u63d0\u51fa\u7684\u8bc4\u6d4b\u6846\u67b6\u4e3a\u540e\u7eed\u6539\u8fdb\u65b9\u6cd5\u548c\u66f4\u6709\u6548\u7684\u591a\u6982\u5ff5\u63a7\u5236\u80fd\u529b\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17900", "abs": "https://arxiv.org/abs/2601.17900", "authors": ["Shengjun Zhang", "Min Chen", "Yibo Wei", "Mingyu Dong", "Yueqi Duan"], "title": "Revisiting 3D Reconstruction Kernels as Low-Pass Filters", "comment": "14 pages, 5 figures", "summary": "3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c63D\u91cd\u5efa\uff0c\u8bc6\u522b\u4e86\u79bb\u6563\u91c7\u6837\u5bfc\u81f4\u7684\u5468\u671f\u6027\u9891\u8c31\u6269\u5c55\u662f\u4e09\u7ef4\u91cd\u5efa\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u63d0\u51faJinc\u6838\u53ca\u5176\u8c03\u5236\u53d8\u4f53\uff0c\u6709\u6548\u63d0\u5347\u6e32\u67d3\u4e0e\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u6838\u5fc3\u901a\u5e38\u7528\u9ad8\u65af\u7b49\u4f4e\u901a\u6838\uff0c\u4f46\u8fd9\u4e9b\u6838\u4f4e\u901a\u7279\u6027\u4e0d\u7406\u60f3\uff0c\u5bfc\u81f4\u9891\u8c31\u4e2d\u9ad8\u4f4e\u9891\u4fe1\u53f7\u6df7\u53e0\uff0c\u5f71\u54cd\u91cd\u5efa\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u5e0c\u671b\u4ece\u4fe1\u53f7\u5904\u7406\u4e25\u683c\u610f\u4e49\u4e0a\u7406\u60f3\u7684\u4f4e\u901a\u6027\u8d28\u51fa\u53d1\u8bbe\u8ba1\u66f4\u4f18\u7684\u5185\u6838\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7528Jinc\u6838\u66ff\u4ee3\u5e38\u7528\u9ad8\u65af\u7b49\u6838\uff0c\u56e0\u5176\u5728\u622a\u6b62\u9891\u7387\u5904\u632f\u5e45\u77ac\u65f6\u5f52\u96f6\uff0c\u5177\u6709\u7406\u60f3\u4f4e\u901a\u7279\u6027\u3002\u9488\u5bf9Jinc\u7a7a\u95f4\u57df\u8870\u51cf\u6162\u7684\u95ee\u9898\uff0c\u53c8\u8bbe\u8ba1\u4e86\u8c03\u5236\u6838\uff0c\u5728\u7a7a\u95f4\u6548\u7387\u4e0e\u9891\u57df\u4fdd\u771f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u91c7\u7528Jinc\u548c\u8c03\u5236\u6838\u5728\u4e09\u7ef4\u91cd\u5efa\u548c\u6e32\u67d3\u4efb\u52a1\u4e2d\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u6838\uff0c\u9891\u7387\u4e0e\u7a7a\u95f4\u8868\u73b0\u5747\u6709\u6539\u5584\u3002", "conclusion": "Jinc\u53ca\u5176\u8c03\u5236\u6838\u56e0\u66f4\u7406\u60f3\u7684\u4f4e\u901a\u7279\u6027\uff0c\u6709\u6548\u6539\u5584\u4e86\u4e09\u7ef4\u91cd\u5efa\u7684\u9891\u8c31\u6df7\u53e0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u3002"}}
{"id": "2601.18486", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.18486", "abs": "https://arxiv.org/abs/2601.18486", "authors": ["Manuel Tonneau", "Neil K. R. Seghal", "Niyati Malhotra", "Victor Orozco-Olvera", "Ana Mar\u00eda Mu\u00f1oz Boudet", "Lakshmi Subramanian", "Sharath Chandra Guntuku", "Valentin Hofmann"], "title": "Demographic Probing of Large Language Models Lacks Construct Validity", "comment": null, "summary": "Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u524d\u7528\u4e8e\u63a2\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4e0d\u540c\u7fa4\u4f53\u5c5e\u6027\u53cd\u5e94\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u8fd9\u4e00\u65b9\u6cd5\u5b58\u5728\u6709\u6548\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5355\u4e00\u4eba\u53e3\u5b66\u7ebf\u7d22\u6765\u68c0\u6d4bLLM\u5bf9\u7fa4\u4f53\u5c5e\u6027\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\uff09\u53cd\u5e94\u7f3a\u4e4f\u5bf9\u4fe1\u53f7\u4e4b\u95f4\u66ff\u6362\u6027\u7684\u4e25\u8c28\u9a8c\u8bc1\u3002\u8fc7\u53bb\u7814\u7a76\u591a\u6570\u5047\u8bbe\u5982\u4eba\u540d\u3001\u65b9\u8a00\u7b49\u4fe1\u53f7\u53ef\u4ee3\u8868\u540c\u4e00\u7fa4\u4f53\uff0c\u5177\u6709\u5f3a\u6784\u5ff5\u6548\u5ea6\uff08construct validity\uff09\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u8bc1\u652f\u6301\u3002", "method": "\u4f5c\u8005\u5728\u7f8e\u56fd\u8bed\u5883\u4e0b\uff0c\u7ed3\u5408\u5bfb\u6c42\u5efa\u8bae\u7684\u771f\u5b9e\u4e92\u52a8\u573a\u666f\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e86\u59d3\u540d\u3001\u65b9\u8a00\u7b49\u7ebf\u7d22\u5728LLM\u4eba\u53e3\u5b66\u63a2\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u7ebf\u7d22\u4f5c\u4e3a\u540c\u4e00\u7fa4\u4f53\u4ee3\u8868\u65f6\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u7ec4\u95f4\u533a\u5206\u7684\u6709\u6548\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u5206\u6790\u4e86\u53ef\u80fd\u7684\u6df7\u6742\u53d8\u91cf\u3002", "result": "\u53d1\u73b0\u4ee3\u8868\u540c\u4e00\u7fa4\u4f53\u7684\u4e0d\u540c\u7ebf\u7d22\u4ec5\u80fd\u90e8\u5206\u91cd\u53e0\u5730\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u4f7f\u7528\u5355\u4e00\u7ebf\u7d22\u4f30\u8ba1\u7684\u4eba\u7fa4\u95f4\u5dee\u5f02\u5927\u5c0f\u53ca\u65b9\u5411\u6781\u4e0d\u7a33\u5b9a\u3002\u7ebf\u7d22\u5bf9\u5c5e\u6027\u7f16\u7801\u5f3a\u5ea6\u548c\u8bed\u8a00\u6df7\u6742\u53d8\u91cf\u662f\u5f71\u54cd\u4e00\u81f4\u6027\u7684\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "\u4eba\u53e3\u5b66\u63a2\u6d4b\u7f3a\u4e4f\u6784\u5ff5\u6548\u5ea6\uff0c\u65e0\u6cd5\u7a33\u5b9a\u51c6\u786e\u5730\u523b\u753bLLM\u4e2d\u4eba\u53e3\u5b66\u6761\u4ef6\u5316\u884c\u4e3a\u3002\u672a\u6765\u5e94\u7ec4\u5408\u591a\u79cd\u3001\u751f\u6001\u6709\u6548\u7684\u7ebf\u7d22\uff0c\u5e76\u660e\u786e\u63a7\u5236\u6df7\u6742\u56e0\u7d20\uff0c\u4ee5\u83b7\u5f97\u66f4\u53ef\u9760\u7684\u4eba\u53e3\u5b66\u6548\u5e94\u5206\u6790\u7ed3\u679c\u3002"}}
{"id": "2601.17905", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17905", "abs": "https://arxiv.org/abs/2601.17905", "authors": ["Jack Foster", "Kirill Paramonov", "Mete Ozay", "Umberto Michieli"], "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning", "comment": null, "summary": "Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6781\u5c11\u6837\u672c\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u7684\u65b0\u65b9\u6cd5Gen1S\uff0c\u5728\u4ec5\u6709\u6bcf\u7c7b1\u4e2a\u65b0\u6837\u672c\u4e14\u57fa\u7840\u8bad\u7ec3\u540e\u4e0d\u53ef\u8c03\u6574\u6a21\u578b\u53c2\u6570\u7684\u6781\u7aef\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65b0\u7c7b\u522b\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5728FSCIL\u4e2d\uff0c\u6a21\u578b\u9700\u8bc6\u522b\u4e0d\u65ad\u589e\u52a0\u7684\u65b0\u7c7b\uff0c\u7279\u522b\u662f\u5728\u6bcf\u4e2a\u65b0\u7c7b\u53ea\u67091\u4e2a\u6837\u672c\u3001\u4e14\u4e0d\u80fd\u5bf9\u7f51\u7edc\u7ee7\u7eed\u8bad\u7ec3\u7684\u6761\u4ef6\u4e0b\uff0c\u6cdb\u5316\u5230\u65b0\u7c7b\u53d8\u5f97\u6781\u4e3a\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u5e94\u5bf9\u5982\u6b64\u82db\u523b\u7684\u8bbe\u5b9a\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u7ed3\u6784\u6027\u5148\u9a8c\u548c\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u4f5c\u8005\u5047\u8bbe\u57fa\u7840\u7c7b\u4e0e\u65b0\u7c7b\u7684\u5d4c\u5165\u5448\u73b0\u7ed3\u6784\u6027\u76f8\u4f3c\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u901a\u8fc7\u51cf\u53bb\u6837\u672c\u6240\u5c5e\u7c7b\u522b\u539f\u578b\uff08\u5747\u503c\u5d4c\u5165\uff09\u5c06\u7279\u5f81\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\u7684\u64cd\u4f5c\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u6a21\u578b\uff08VAE\u6216\u6269\u6563\u6a21\u578b\uff09\u5b66\u4e60\u57fa\u7840\u7c7b\u6b8b\u5dee\u7684\u591a\u6a21\u6001\u5206\u5e03\uff0c\u8fdb\u4e00\u6b65\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u8f85\u52a9\u65b0\u7c7b\u8bc6\u522b\u3002", "result": "\u63d0\u51fa\u7684Gen1S\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4e0d\u540cbackbone\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5728\u65b0\u7c7b\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u6027\u7684\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u7c7b\u95f4\u5d4c\u5165\u7ed3\u6784\u76f8\u4f3c\u6027\u548c\u751f\u6210\u5efa\u6a21\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u80fd\u591f\u5728\u6781\u5c11\u6837\u672c\u548c\u4e0d\u53ef\u66f4\u65b0\u6a21\u578b\u7684\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u65b0\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2601.18512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18512", "abs": "https://arxiv.org/abs/2601.18512", "authors": ["Antonio Garzon-Vico", "Krithika Sharon Komalapati", "Arsalan Shahid", "Jan Rosier"], "title": "Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research", "comment": null, "summary": "This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6a21\u62df\u771f\u5b9e\u9ad8\u7ba1\u865a\u62df\u4eba\u683c\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u96be\u4ee5\u76f4\u63a5\u83b7\u53d6\u9ad8\u7ba1\u7684\u8bbf\u95ee\uff0c\u7ec4\u7ec7\u7814\u7a76\u6025\u9700\u66ff\u4ee3\u65b9\u5f0f\u4ee5\u7406\u89e3\u9ad8\u7ba1\u7684\u51b3\u7b56\u903b\u8f91\u548c\u9053\u5fb7\u5224\u65ad\u3002", "method": "\u57fa\u4e8eCEO\u771f\u5b9e\u6c9f\u901a\u5185\u5bb9\u548c\u9053\u5fb7\u57fa\u7840\u7406\u8bba\uff0c\u6784\u5efa\u865a\u62dfCEO\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff08\u7ed3\u6784\u6548\u5ea6\u3001\u53ef\u9760\u6027\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u4e0e\u4eba\u7c7b\u53c2\u7167\u5bf9\u6bd4\uff09\u7cfb\u7edf\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "result": "\u7406\u8bba\u6307\u5bfc\u4e0b\u7684\u865a\u62df\u4eba\u683c\u80fd\u591f\u5f88\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u6837\u672c\u4e2d\u7684\u9053\u5fb7\u5224\u65ad\uff0c\u4e0e\u771f\u4ebaCEO\u5177\u6709\u8f83\u9ad8\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u865a\u62df\u9ad8\u7ba1\u4eba\u683c\u53ef\u4f5c\u4e3a\u7ec4\u7ec7\u7814\u7a76\u4e2d\u7684\u53ef\u4fe1\u3001\u4e92\u8865\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u9ad8\u7ba1\u7684\u60c5\u5883\uff0c\u5e76\u63d0\u51fa\u4e86\u540e\u7eed\u7814\u7a76\u65b9\u6cd5\u7684\u5efa\u8bae\u3002"}}
{"id": "2601.17918", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17918", "abs": "https://arxiv.org/abs/2601.17918", "authors": ["Dain Kim", "Jiwoo Lee", "Jaehoon Yun", "Yong Hoe Koo", "Qingyu Chen", "Hyunjae Kim", "Jaewoo Kang"], "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models", "comment": "EACL 2026 (Findings)", "summary": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5728\u533b\u7597\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524dDPO\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u9762\u5411\u89c6\u89c9\u8bef\u5224\u7684\u504f\u597d\u6784\u5efa\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u7597LVLM\u5728\u533b\u7597\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u5bf9\u9f50\u6027\u548c\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u90e8\u7f72\u53d7\u9650\u3002DPO\u867d\u7136\u88ab\u8ba4\u4e3a\u80fd\u6539\u8fdb\u6a21\u578b\u54cd\u5e94\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u533b\u7597\u9886\u57df\u6548\u679c\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\uff0c\u7f3a\u5c11\u7cfb\u7edf\u7684\u5b9e\u8bc1\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u540e\u7eed\u65b9\u6cd5\u9769\u65b0\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u3002", "method": "\u4f5c\u8005\u8bc4\u4f30\u4e86\u4e5d\u79cd\u4e0d\u540cDPO\u53d8\u4f53\uff0c\u5206\u522b\u5728\u4e24\u79cd\u533b\u7597LVLM\uff08LLaVA-Med\u548cHuatuoGPT-Vision\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5168\u9762\u5206\u6790\u5404DPO\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u8868\u73b0\u3002\u5e76\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bef\u5224\u9519\u8bef\u7684\u504f\u597d\u6784\u5efa\u7b56\u7565\u4f5c\u4e3a\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff0c\u73b0\u6709DPO\u65b9\u6cd5\u76f8\u8f83\u4e8e\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6709\u9650\uff0c\u4e14\u6548\u679c\u5728\u4e0d\u540c\u4efb\u52a1\u53ca\u6a21\u578b\u4e0a\u8868\u73b0\u4e0d\u4e00\uff0c\u4e14\u96be\u4ee5\u89e3\u51b3\u89c6\u89c9\u8bef\u5224\u95ee\u9898\u3002\u63d0\u51fa\u7684\u65b0\u7b56\u7565\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u6bd4\u6700\u4f18DPO\u57fa\u7ebf\u63d0\u53473.6%\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5f53\u524dDPO\u65b9\u6cd5\u5728\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u504f\u597d\u6784\u5efa\u65b9\u6cd5\u8bc1\u660e\u4e86\u6539\u5584\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u540e\u7eed\u76f8\u5173\u7814\u7a76\uff0c\u5e76\u516c\u5f00\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u652f\u6301\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2601.18517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18517", "abs": "https://arxiv.org/abs/2601.18517", "authors": ["James Sungarda", "Hongkai Liu", "Zilong Zhou", "Tien-Hsuan Wu", "Johnson Chun-Sing Cheung", "Ben Kao"], "title": "GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback", "comment": "2025 IEEE International Conference on Big Data. ISBN: 979-8-3315-9447-3/25. Page numbers: 3544-3553", "summary": "Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSWITCH\u7684\u793e\u4f1a\u5de5\u4f5c\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7684\u5ba2\u6237\u60c5\u666f\u3001\u5b9e\u65f6\u8bc6\u522b\u54a8\u8be2\u6280\u5de7\u548c\u52a8\u673a\u5f0f\u8bbf\u8c08\u8fdb\u5c55\u7cfb\u7edf\uff0c\u63d0\u5347\u793e\u4f1a\u5de5\u4f5c\u5b9e\u4e60\u6559\u80b2\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "motivation": "\u793e\u4f1a\u5de5\u4f5c\u5b9e\u8df5\u6559\u5b66\u53d7\u9650\u4e8e\u5e08\u8d44\u548c\u771f\u5b9e\u5ba2\u6237\u8d44\u6e90\uff0c\u5b66\u5458\u96be\u4ee5\u53ca\u65f6\u83b7\u5f97\u5ba2\u89c2\u53cd\u9988\uff0c\u56e0\u6b64\u9700\u8981\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u57f9\u8bad\u8f85\u52a9\u5de5\u5177\uff0c\u63d0\u5347\u8bad\u7ec3\u7684\u53ca\u65f6\u6027\u548c\u53cd\u9988\u8d28\u91cf\u3002", "method": "SWITCH\u901a\u8fc7\u8bbe\u8ba1\u8ba4\u77e5\u5efa\u6a21\u7684\u865a\u62df\u5ba2\u6237\uff08\u5305\u542b\u9759\u6001\u5c5e\u6027\u548c\u52a8\u6001\u53d8\u5316\u7684\u5fc3\u7406\u7279\u5f81\uff09\uff0c\u7ed3\u5408\u7528\u6237\u8a00\u8bed\u81ea\u52a8\u8bc6\u522b\u54a8\u8be2\u6280\u5de7\uff0c\u5e76\u6839\u636e\u53cd\u9988\u8c03\u6574\u52a8\u673a\u5f0f\u8bbf\u8c08\u9636\u6bb5\u8fdb\u5c55\u3002\u6280\u80fd\u8bc6\u522b\u6a21\u5757\u57fa\u4e8ein-context learning\u7ed3\u5408\u68c0\u7d22\u5f0f\u6ce8\u91ca\u4f1a\u8bdd\u6587\u672c\u548c\u5fae\u8c03\u7684BERT\u6a21\u578b\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65e0\u8bba\u662fBERT\u5fae\u8c03\u6a21\u578b\u8fd8\u662f\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u5927\u5e45\u8d85\u8fc7\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002SWITCH\u80fd\u6709\u6548\u652f\u6301\u793e\u4f1a\u5de5\u4f5c\u9886\u57df\u6559\u80b2\uff0c\u63d0\u4f9b\u4e00\u81f4\u4e14\u7ecf\u6d4e\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "conclusion": "SWITCH\u80fd\u591f\u5728\u793e\u4f1a\u5de5\u4f5c\u9886\u57df\u6559\u80b2\u4e2d\u8865\u5145\u4f20\u7edf\u5b9e\u4e60\uff0c\u964d\u4f4e\u57f9\u8bad\u6210\u672c\u3001\u63d0\u5347\u8bad\u7ec3\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5e2e\u52a9\u5bfc\u5e08\u96c6\u4e2d\u7cbe\u529b\u8fdb\u884c\u66f4\u9ad8\u9636\u7684\u6307\u5bfc\u3002"}}
{"id": "2601.17927", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.17927", "abs": "https://arxiv.org/abs/2601.17927", "authors": ["Eashan Adhikarla", "Brian D. Davison"], "title": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry", "comment": null, "summary": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RemEdit\u751f\u6210\u5f0f\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5728\u56fe\u50cf\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0e\u63a8\u7406\u901f\u5ea6\u95f4\u53d6\u5f97\u65b0\u5e73\u8861\u3002\u901a\u8fc7\u521b\u65b0\u6027\u5730\u91c7\u7528Riemann\u6d41\u5f62\u4e0a\u7684\u6f5c\u7a7a\u95f4\u5bfc\u822a\u3001mamba\u6a21\u5757\u5b66\u4e60\u6d41\u5f62\u7ed3\u6784\u3001SLERP\u878d\u5408\u548cV-L\u6a21\u578b\u8f85\u52a9\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u6ce8\u610f\u529b\u526a\u679d\u5b9e\u73b0\u5b9e\u65f6\u4e0e\u9ad8\u6548\u3002\u6548\u679c\u8d85\u8fc7\u6b64\u524dSOTA\uff0c\u5e76\u5b9e\u73b050%\u526a\u679d\u4e0b\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u8bc1\u8bed\u4e49\u4fdd\u771f\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u77db\u76fe\u2014\u2014\u9ad8\u4fdd\u771f\u901a\u5e38\u610f\u5473\u7740\u8ba1\u7b97\u7e41\u91cd\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u65b9\u6cd5\uff0c\u7a81\u7834\u8fd9\u4e00\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u65e2\u9ad8\u6548\u53c8\u9ad8\u8d28\u91cf\u7684\u53ef\u63a7\u56fe\u50cf\u7f16\u8f91\u3002", "method": "RemEdit\u6846\u67b6\u7ed3\u5408\u4e24\u5927\u521b\u65b0\uff1a1\uff09\u7f16\u8f91\u4fdd\u771f\u65b9\u9762\uff0c\u5c06\u6f5c\u7a7a\u95f4\u89c6\u4e3a\u9ece\u66fc\u6d41\u5f62\uff0c\u5229\u7528mamba\u6a21\u5757\u5b66\u4e60\u6d41\u5f62\u7ed3\u6784\uff0c\u5e76\u5bfb\u6c42\u6700\u4f18\u6d4b\u5730\u8def\u5f84\u8fdb\u884c\u5e73\u6ed1\u7f16\u8f91\uff0c\u540c\u65f6\u5f15\u5165\u53ccSLERP\u878d\u5408\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76ee\u6807\u5bfc\u5411\u6587\u672c\u63d0\u793a\uff1b2\uff09\u52a0\u901f\u65b9\u9762\uff0c\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u526a\u679d\uff0c\u8f7b\u91cf\u5316\u5934\u90e8\u5b66\u4e60\u4fdd\u7559\u4e0e\u7f16\u8f91\u4efb\u52a1\u6700\u76f8\u5173Token\uff0c\u5b9e\u73b0\u9ad8\u6548\u800c\u65e0\u8bed\u4e49\u635f\u5931\u7684\u63a8\u7406\u3002", "result": "RemEdit\u5728\u7f16\u8f91\u8d28\u91cf\u4e0a\u8d85\u8fc7\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5728\u526a\u679d\u7387\u9ad8\u8fbe50%\u60c5\u51b5\u4e0b\u4f9d\u7136\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\uff0c\u517c\u987e\u8bed\u4e49\u4fdd\u771f\u548c\u5904\u7406\u901f\u5ea6\uff0c\u6811\u7acb\u4e86\u65b0\u4e00\u4ee3\u7f16\u8f91\u7cfb\u7edf\u6027\u80fd\u6807\u6746\u3002", "conclusion": "RemEdit\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u90e8\u7f72\u4e14\u9ad8\u6548\u3001\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9879\u521b\u65b0\u6709\u6548\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u4e0e\u901f\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u65b0\u9009\u62e9\u3002"}}
{"id": "2601.18527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18527", "abs": "https://arxiv.org/abs/2601.18527", "authors": ["Francesco Maria Molfese", "Momchil Hardalov", "Rexhina Blloshmi", "Bill Byrne", "Adri\u00e0 de Gispert"], "title": "Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models", "comment": "European Chapter of the Association for Computational Linguistics EACL 2026", "summary": "With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u9488\u5bf9\u8d85\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LCLMs\uff09\u7684\u5fae\u8c03\u7b56\u7565\u3001\u4fe1\u606f\u5229\u7528\u80fd\u529b\u53caKV-cache\u538b\u7f29\u4e0b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002", "motivation": "LCLMs\u80fd\u591f\u5904\u7406\u767e\u4e07\u7ea7Token\u7684\u4e0a\u4e0b\u6587\uff0c\u4f46\u5c1a\u4e0d\u660e\u786e\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u5176\u6027\u80fd\u53ca\u5728KV-cache\u538b\u7f29\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u5bf9\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0e\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5bf9LCLMs\u5e94\u7528\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u8bc4\u6d4b\u5176\u5728\u63d0\u53d6\u3001\u5229\u7528\u76f8\u5173\u4fe1\u606f\u548c\u5728KV-cache\u538b\u7f29\u4e0b\u8868\u73b0\u7684\u63d0\u5347\uff0c\u5e76\u6bd4\u8f83LCLMs\u4e0e\u5e38\u89c4RAG\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u9886\u57df\u5185\u4efb\u52a1LCLMs\u8f83\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u9ad8\u8fbe20\u5206\uff0c\u91d1\u878d\u7c7b\u95ee\u9898\u6cdb\u5316\u80fd\u529b\u63d0\u53479\u5206\uff0c\u4f46\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e0a\uff0cRAG\u65b9\u6cd5\u66f4\u4f18\u63d0\u53476\u5206\u3002\u5fae\u8c03\u540eKV-cache\u538b\u7f29\u4e0b\u7684\u9c81\u68d2\u6027\u6709\u4e2d\u7b49\u5ea6\u63d0\u5347\uff0c\u5404\u4efb\u52a1\u8868\u73b0\u6709\u6240\u5dee\u5f02\u3002", "conclusion": "\u9002\u5f53\u7684\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347LCLMs\u957f\u4e0a\u4e0b\u6587\u53ca\u538b\u7f29\u5065\u58ee\u6027\uff0c\u4f46\u6a21\u578b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4f9d\u4efb\u52a1\u7c7b\u578b\u6ce2\u52a8\u8f83\u5927\uff0c\u4e8c\u8005\u5404\u6709\u4f18\u52bf\uff0c\u5b9e\u7528\u65f6\u9700\u6743\u8861\u9009\u62e9\u3002"}}
{"id": "2601.17934", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17934", "abs": "https://arxiv.org/abs/2601.17934", "authors": ["Vi Vu", "Thanh-Huy Nguyen", "Tien-Thinh Nguyen", "Ba-Thinh Lam", "Hoang-Thien Nguyen", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images", "comment": "Accepted to ISBI 2026", "summary": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSC-SAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e86U-Net\u4e0eSAM\uff0c\u901a\u8fc7\u4e92\u76f8\u6307\u5bfc\u5b9e\u73b0\u534a\u76d1\u7763\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u6548\u679c\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3b\u6d41\u5927\u6a21\u578b\u5982SAM\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u9762\u4e34\u9886\u57df\u8f6c\u79fb\u3001\u6807\u6ce8\u7a00\u7f3a\u4ee5\u53ca\u96be\u4ee5\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e38\u89c4\u6a21\u578b\uff08\u5982U-Net\uff09\u867d\u64c5\u957f\u534a\u76d1\u7763\u5b66\u4e60\u4f46\u672a\u88ab\u7528\u4e8e\u8f85\u52a9SAM\u3002\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6548\u679c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u5bb6-\u5168\u80fd\u4f53\uff08specialist-generalist\uff09\u6846\u67b6SC-SAM\u3002\u5177\u4f53\u505a\u6cd5\u662f\uff1aU-Net\u4f5c\u4e3a\u4e13\u5bb6\uff0c\u63d0\u4f9b\u70b9\u5f0f\u63d0\u793a\u548c\u4f2a\u6807\u7b7e\u5e2e\u52a9SAM\u9002\u5e94\u533b\u5b66\u5f71\u50cf\uff0c\u540c\u65f6SAM\u4f5c\u4e3a\u5168\u80fd\u4f53\u5bf9U-Net\u65bd\u52a0\u76d1\u7763\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5f62\u6210\u53cc\u5411\u7684\u5171\u540c\u8bad\u7ec3\u5faa\u73af\uff0c\u4ece\u800c\u4f7f\u4e8c\u8005\u90fd\u80fd\u6709\u6548\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u524d\u5217\u817aMRI\u548c\u606f\u8089\u5206\u5272\u57fa\u51c6\u4e0a\uff0cSC-SAM\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f18\uff08state-of-the-art\uff09\u8868\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u6240\u6709\u534a\u76d1\u7763SAM\u53d8\u4f53\u53ca\u57fa\u91d1\u533b\u5b66\u6a21\u578b\u5982MedSAM\u3002", "conclusion": "\u4e13\u5bb6-\u5168\u80fd\u4f53\u534f\u4f5c\u6a21\u5f0f\u80fd\u6709\u6548\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6807\u7b7e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5927\u6a21\u578b\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18533", "abs": "https://arxiv.org/abs/2601.18533", "authors": ["Yuxin Jiang", "Yufei Wang", "Qiyuan Zhang", "Xingshan Zeng", "Liangyou Li", "Jierun Chen", "Chaofan Tao", "Haoli Bai", "Lifeng Shang"], "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation", "comment": "19 pages, 8 figures, 12 tables. Accepted at ICLR 2026", "summary": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RLVRR\uff09\uff0c\u901a\u8fc7\u5f15\u7528\u9ad8\u8d28\u91cf\u53c2\u8003\u6765\u63d0\u53d6\u5956\u52b1\u94fe\uff0c\u4ece\u800c\u5728\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u673a\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edf\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u53ea\u9002\u7528\u4e8e\u6709\u660e\u786e\u7b54\u6848\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\uff0c\u800c\u4e14\u5355\u70b9\u5956\u52b1\u5bb9\u6613\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u5956\u52b1\u4f5c\u5f0a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u5956\u52b1\u673a\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7684RLVRR\u65b9\u6cd5\uff0c\u4ece\u9ad8\u8d28\u91cf\u53c2\u8003\u6587\u672c\u4e2d\u63d0\u53d6\u6709\u5e8f\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5c06\u5956\u52b1\u5206\u89e3\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5185\u5bb9\u5173\u6ce8\u786e\u5b9a\u6027\u6838\u5fc3\u6982\u5ff5\uff0c\u98ce\u683c\u901a\u8fc7\u5927\u6a21\u578b\u9a8c\u8bc1\u6587\u672c\u7684\u98ce\u683c\u5c5e\u6027\u3002\u65b9\u6cd5\u517c\u5177\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u80fd\u529b\u4e0e\u6709\u76d1\u7763\u5fae\u8c03\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "result": "\u5728\u8d85\u8fc710\u4e2a\u57fa\u51c6\u4efb\u52a1\u3001\u4ee5\u53caQwen\u548cLlama\u7b49\u5927\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cRLVRR\u663e\u8457\u4f18\u4e8e\u7528\u5341\u500d\u6570\u636e\u548c\u9ad8\u7ea7\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u7684SFT\uff1b\u7edf\u4e00\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u4e0e\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\uff1b\u80fd\u5728\u4fdd\u6301\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\u66f4\u597d\u6cdb\u5316\u3002", "conclusion": "RLVRR\u4e3a\u901a\u7528\u5927\u6a21\u578b\u6821\u51c6\u9886\u57df\u7684\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u7406\u8bba\u652f\u6491\u7684\u9014\u5f84\uff0c\u6709\u671b\u6210\u4e3a\u9886\u57df\u5185\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u7ecf\u53d1\u5e03\u4e8eGithub\u3002"}}
{"id": "2601.17939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17939", "abs": "https://arxiv.org/abs/2601.17939", "authors": ["Chengkun Sun", "Jinqian Pan", "Renjie Liang", "Zhengkang Fan", "Xin Miao", "Jiang Bian", "Jie Xu"], "title": "DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation", "comment": null, "summary": "In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u5f62\u53cd\u5377\u79ef\uff08DTC\uff09\u65b9\u6cd5\uff0c\u53ef\u4ee5\u81ea\u9002\u5e94\u5b66\u4e60\u4e0a\u91c7\u6837\u7684\u91c7\u6837\u4f4d\u7f6e\uff0c\u6709\u6548\u63d0\u53472D/3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u7ec6\u8282\u8fd8\u539f\u548c\u7279\u5f81\u91cd\u5efa\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0a\u91c7\u6837\u65b9\u5f0f\u5982\u8f6c\u7f6e\u5377\u79ef\u548c\u7ebf\u6027\u63d2\u503c\u90fd\u57fa\u4e8e\u56fa\u5b9a\u91c7\u6837\u4f4d\u7f6e\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8d85\u51fa\u9884\u5b9a\u4e49\u4f4d\u7f6e\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u53ef\u80fd\u9020\u6210\u4f2a\u5f71\u6216\u7ec6\u8282\u4e22\u5931\u3002\u8be5\u95ee\u9898\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7814\u7a76\u66f4\u7075\u6d3b\u7684\u4e0a\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u53d7\u53ef\u53d8\u5f62\u5377\u79ef\u542f\u53d1\uff0c\u4f5c\u8005\u63d0\u51fa\u201c\u53d8\u5f62\u53cd\u5377\u79ef\uff08Deformable Transposed Convolution, DTC\uff09\u201d\uff0c\u5b83\u80fd\u591f\u5b66\u4e60\u52a8\u6001\uff08\u975e\u56fa\u5b9a\uff09\u5750\u6807\u6765\u8fdb\u884c\u4e0a\u91c7\u6837\uff0c\u4ece\u800c\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u7279\u5f81\u56fe\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e2D\u548c3D\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u53ef\u65b9\u4fbf\u5730\u5d4c\u5165\u73b0\u6709UNet\u7c7b\u7ed3\u6784\u4e2d\u3002", "result": "\u57283D\uff08\u5982BTCV15\uff09\u548c2D\uff08\u5982ISIC18, BUSI\uff09\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDTC\u80fd\u663e\u8457\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u91cd\u5efa\u548c\u7ec6\u8282\u8fd8\u539f\u80fd\u529b\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4e0a\u91c7\u6837\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "DTC\u80fd\u6709\u6548\u5f25\u8865\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u4e0a\u91c7\u6837\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u7279\u5f81\u91cd\u5efa\u624b\u6bb5\uff0c\u5728\u5b9e\u8df5\u4e2d\u5177\u6709\u5f88\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18536", "abs": "https://arxiv.org/abs/2601.18536", "authors": ["Abishek Stephen", "Jind\u0159ich Libovick\u00fd"], "title": "Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features", "comment": "Accepted to Findings of EACL 2026, 9 pages, 6 figures", "summary": "We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b50\u8bcd\u5207\u5206\u8bc4\u4ef7\u6307\u6807\uff0c\u4e0d\u4f9d\u8d56\u91d1\u6807\u51c6\u5206\u8bcd\u6570\u636e\uff0c\u800c\u662f\u5229\u7528\u5f62\u6001\u53e5\u6cd5\u7279\u5f81\uff0c\u9002\u7528\u66f4\u591a\u8bed\u8a00\u3002", "motivation": "\u73b0\u6709\u5b50\u8bcd\u5207\u5206\u8bc4\u4ef7\u5e38\u4f9d\u8d56\u4e8e\u91d1\u6807\u51c6\u5206\u8bcd\u6570\u636e\uff0c\u4f46\u8fd9\u79cd\u6570\u636e\u5728\u8bb8\u591a\u8bed\u8a00\u4e2d\u4e0d\u53ef\u7528\u6216\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u9650\u5236\u4e86\u5b50\u8bcd\u5207\u5206\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e0e\u6bd4\u5bf9\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u5982Universal Dependencies\u6216UniMorph\u7b49\u8d44\u6e90\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u5f62\u6001\u53e5\u6cd5\u7279\u5f81\uff0c\u901a\u8fc7IBM Model 1\u5bf9\u5b50\u8bcd\u4e0e\u5f62\u6001\u7279\u5f81\u8fdb\u884c\u6982\u7387\u5bf9\u9f50\uff0c\u8bbe\u8ba1\u5168\u65b0\u8bc4\u4ef7\u6307\u6807\uff0c\u65e0\u9700\u91d1\u6807\u51c6\u5206\u8bcd\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6307\u6807\u4e0e\u4f20\u7edf\u7684\u5f62\u6001\u8fb9\u754c\u53ec\u56de\u7387\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u4e14\u80fd\u9002\u7528\u4e8e\u4e0d\u540c\u5f62\u6001\u7cfb\u7edf\u7684\u591a\u79cd\u8bed\u8a00\u3002", "conclusion": "\u8be5\u6307\u6807\u4e3a\u5b50\u8bcd\u5207\u5206\u7684\u5f62\u6001\u5408\u7406\u6027\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u3001\u5b9e\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u8bed\u8a00\u5f62\u6001\u5206\u6790\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17947", "abs": "https://arxiv.org/abs/2601.17947", "authors": ["Bora Yimenicioglu", "Vishal Manikanden"], "title": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos", "comment": null, "summary": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.\n  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FlowMorph\uff0c\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u81ea\u6d3d\u3001\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u7ea2\u7ec6\u80de\u529b\u5b66\u7279\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u9ad8\u901a\u91cf\u5fae\u6d41\u63a7\u89c6\u9891\u6570\u636e\u4e2d\u7684\u7ec6\u80de\u5f62\u53d8\u6307\u6807\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u4e0e\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u7ea2\u7ec6\u80de\u673a\u68b0\u6027\u80fd\u662f\u591a\u79cd\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002\u9ad8\u901a\u91cf\u3001\u51c6\u786e\u4e14\u81ea\u52a8\u5316\u7684\u7ec6\u80de\u529b\u5b66\u6d4b\u91cf\u4e9f\u9700\u65e0\u9700\u6807\u6ce8\u3001\u80fd\u878d\u5408\u7269\u7406\u89c4\u5f8b\u7684\u65b9\u6cd5\uff0c\u514b\u670d\u76ee\u524d\u624b\u5de5\u7279\u5f81\u3001\u76d1\u7763\u5206\u5272\u7b49\u4e0d\u8db3\uff0c\u5e76\u63d0\u5347\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "FlowMorph\u91c7\u7528\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4f4e\u7ef4\u53c2\u6570\u8f6e\u5ed3\u5efa\u6a21\u7ea2\u7ec6\u80de\uff0c\u901a\u8fc7\u53ef\u5fae\u80f6\u56ca-\u6d41\u52a8\u6a21\u578b\u8054\u7acb\u5c42\u6d41\u5bf9\u6d41\u4e0e\u5f39\u6027\u7ea6\u675f\uff0c\u7ed3\u5408\u81ea\u52a8\u8f6e\u5ed3\u4e0e\u5149\u6d41\uff0c\u53ea\u4f9d\u8d56\u4eae\u573a\u89c6\u9891\uff0c\u8054\u5408\u4f18\u5316\u591a\u91cd\u635f\u5931\u4ee5\u5b9e\u73b0\u7269\u7406\u81ea\u6d3d\u8ddf\u8e2a\u4e0e\u529b\u5b66\u91cf\u63d0\u53d6\u3002", "result": "FlowMorph\u5728\u516c\u5f00\u5fae\u6d41\u63a7\u7ea2\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\uff0c\u8fbe\u5230\u5e73\u5747IoU 0.905\uff0c\u5206\u5272\u4e0e\u529b\u5b66\u7ea6\u675f\u8fdc\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u63d0\u51fa\u7684\u6807\u91cfk\u80fd\u6709\u6548\u533a\u5206\u7ea2\u7ec6\u80de\u8fd0\u52a8\u6a21\u5f0f\uff08AUC 0.863\uff09\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u5b9a\u5373\u53ef\u51c6\u786e\u9884\u6d4b\u8868\u89c2\u6768\u6c0f\u6a21\u91cf\uff08\u7edd\u5bf9\u8bef\u5dee0.118MPa\uff09\uff0c\u4e14\u5bf9\u4e8e\u5b9e\u9a8c\u6761\u4ef6\u53d8\u5316\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "FlowMorph\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7269\u7406\u81ea\u6d3d\u3001\u81ea\u76d1\u7763\u7684\u89c6\u9891\u7ec6\u80de\u529b\u5b66\u6d4b\u91cf\uff0c\u5728\u81ea\u52a8\u5316\u3001\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u6027\u65b9\u9762\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u5fae\u6d41\u63a7\u7684\u9ad8\u901a\u91cf\u7ea2\u7ec6\u80de\u751f\u7269\u529b\u5b66\u8868\u578b\u5206\u6790\u3002"}}
{"id": "2601.18552", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18552", "abs": "https://arxiv.org/abs/2601.18552", "authors": ["Devansh Srivastav", "David Pape", "Lea Sch\u00f6nherr"], "title": "Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection", "comment": null, "summary": "LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u201c\u9690\u85cf\u610f\u56fe\u201d\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u6613\u4e8e\u7406\u89e3\u4f46\u6db5\u76d6\u5168\u9762\u7684\u5206\u7c7b\u6cd5\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5728\u5b9e\u9645\u73af\u5883\u4e0b\u7684\u5931\u6548\u539f\u56e0\uff0c\u7a81\u51fa\u5f3a\u8c03\u4e86\u5efa\u7acb\u5065\u5168\u6cbb\u7406\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002", "motivation": "LLMs\u5728\u65e5\u5e38\u51b3\u7b56\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u8f93\u51fa\u53ef\u80fd\u6697\u542b\u5bf9\u7528\u6237\u884c\u4e3a\u548c\u4fe1\u5ff5\u4ea7\u751f\u5f71\u54cd\u7684\u201c\u9690\u85cf\u610f\u56fe\u201d\uff0c\u8fd9\u79cd\u610f\u56fe\u6765\u6e90\u53ef\u80fd\u662f\u8bad\u7ec3\u8fc7\u7a0b\u6216\u6076\u610f\u5f00\u53d1\u8005\uff0c\u5e76\u96be\u4ee5\u88ab\u53d1\u73b0\u3002\u4f5c\u8005\u5e0c\u671b\u6df1\u6316\u8fd9\u4e9b\u9690\u853d\u5f71\u54cd\uff0c\u4e3a\u540e\u7eed\u98ce\u9669\u6cbb\u7406\u548c\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "1\uff09\u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u5341\u7c7b\u9690\u85cf\u610f\u56fe\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ece\u610f\u56fe\u3001\u673a\u5236\u3001\u73af\u5883\u3001\u5f71\u54cd\u591a\u4e2a\u7ef4\u5ea6\u68b3\u7406\uff1b2\uff09\u5728\u53d7\u63a7\u6a21\u578b\u4e2d\u6ce8\u5165\u548c\u6f14\u793a\u9690\u85cf\u610f\u56fe\uff0c\u4e3a\u68c0\u6d4b\u548c\u8bc4\u4f30\u63d0\u4f9b\u6d4b\u8bd5\u7528\u4f8b\uff1b3\uff09\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8e\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u7c7bLLM\u7684\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u63d0\u793a\u5f3a\u5ea6\u3001\u7cbe\u5ea6-\u6f0f\u62a5\u6743\u8861\u7b49\u538b\u529b\u6d4b\u8bd5\uff1b4\uff09\u901a\u8fc7\u6848\u4f8b\u5206\u6790\uff0c\u5b9e\u8bc1\u5c55\u793a\u5b9e\u9645\u5e94\u7528\u4e2d\u5341\u7c7b\u610f\u56fe\u5747\u6709\u4f53\u73b0\u3002", "result": "\u5728\u771f\u5b9e\u5f00\u653e\u73af\u5883\u4e0b\uff0c\u9690\u85cf\u610f\u56fe\u6781\u96be\u68c0\u6d4b\uff0c\u5c24\u5176\u5728\u4f4e\u6d41\u884c\u5ea6\u6761\u4ef6\u4e0b\uff0c\u68c0\u6d4b\u65b9\u6cd5\u4f1a\u56e0\u5047\u9633\u6027\u8fc7\u591a\u800c\u7cbe\u5ea6\u5d29\u6e83\uff0c\u5047\u9634\u6027\u53c8\u63a9\u76d6\u4e86\u6f5c\u5728\u98ce\u9669\u3002\u538b\u529b\u6d4b\u8bd5\u63ed\u793a\uff0c\u9664\u975e\u5047\u9633\u6027\u63a5\u8fd10\u6216\u6709\u5f3a\u5148\u9a8c\uff0c\u5426\u5219\u5ba1\u8ba1\u51e0\u4e4e\u65e0\u6cd5\u594f\u6548\u3002\u6b64\u5916\uff0c\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e3b\u6d41\u90e8\u7f72\u6a21\u578b\u5747\u5df2\u51fa\u73b0\u5206\u7c7b\u4f53\u7cfb\u4e2d\u7684\u6240\u6709\u610f\u56fe\u7c7b\u578b\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u4f53\u7cfb\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u6216\u76d1\u7ba1LLMs\u4e2d\u7684\u9690\u85cf\u610f\u56fe\uff0c\u6cbb\u7406\u98ce\u9669\u88ab\u4e25\u91cd\u4f4e\u4f30\u3002\u8feb\u5207\u9700\u8981\u6784\u5efa\u9c81\u68d2\u7684\u76d1\u7ba1\u548c\u68c0\u6d4b\u6846\u67b6\uff0c\u672c\u6587\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u5b9e\u8bc1\u7814\u7a76\u4e3a\u672a\u6765\u98ce\u9669\u611f\u77e5\u3001\u884c\u4e3a\u5f52\u56e0\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.17950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17950", "abs": "https://arxiv.org/abs/2601.17950", "authors": ["Matthew Walmer", "Saksham Suri", "Anirud Aggarwal", "Abhinav Shrivastava"], "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders", "comment": null, "summary": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UPLiFT\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u7528\u50cf\u7d20\u5bc6\u96c6\u7279\u5f81\u4e0a\u91c7\u6837\u67b6\u6784\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u8fed\u4ee3\u65b9\u6cd5\u548c\u672c\u5730\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u5728\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u6700\u65b0\u4ea4\u53c9\u6ce8\u610f\u529b\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u7684\u65b9\u6cd5\u53d7\u5230\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u9650\u5236\u3002\u5f53\u524d\u4e3b\u6d41\u4ea4\u53c9\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u9ad8\uff0c\u4f46\u63a8\u7406\u6210\u672c\u4e0e\u4e3b\u5e72\u7f51\u7edc\u4e00\u6837\u9ad8\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u8868\u73b0\u4f18\u5f02\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faUPLiFT\uff0c\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u8fed\u4ee3\u4e0a\u91c7\u6837\u548c\u672c\u5730\u6ce8\u610f\u529b\u7b97\u5b50\u7684\u67b6\u6784\u3002\u8be5\u672c\u5730\u6ce8\u610f\u529b\u673a\u5236\u4ec5\u5728\u5c40\u90e8\u533a\u57df\u5185\u8fdb\u884c\u6ce8\u610f\u529b\u6c60\u5316\uff0c\u89c4\u907f\u4e86\u5168\u5c40\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u9ad8\u6602\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "UPLiFT\u5728\u591a\u9879\u57fa\u51c6\u548c\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\uff08\u5982VAE\u7279\u5f81\u4e0a\u91c7\u6837\uff09\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u6700\u65b0\u4ea4\u53c9\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u4f46\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002\u672c\u5730\u6ce8\u610f\u529b\u7b97\u5b50\u4e5f\u4fdd\u8bc1\u4e86\u7279\u5f81\u5728\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "UPLiFT\u4e3a\u751f\u6210\u9ad8\u5bc6\u5ea6\u7279\u5f81\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5f3a\u5927\u7684\u65b0\u65b9\u6cd5\uff0c\u517c\u5177\u901a\u7528\u6027\u548c\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u4e2d\u5177\u6709\u5e7f\u6cdb\u524d\u666f\u3002"}}
{"id": "2601.18572", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18572", "abs": "https://arxiv.org/abs/2601.18572", "authors": ["Franziska Weeber", "Vera Neplenbroek", "Jan Batzner", "Sebastian Pad\u00f3"], "title": "One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization", "comment": null, "summary": "Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u516d\u79cd\u5e38\u7528persona\u7ebf\u7d22\u5728\u4e03\u4e2a\u5f00\u653e\u6e90\u4ee3\u7801\u4e0e\u4e13\u6709\u5927\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u540c\u4e00persona\u4e0d\u540c\u7ebf\u7d22\u5f15\u5bfc\u8f93\u51fa\u6709\u660e\u663e\u5dee\u5f02\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u9700\u7528\u591a\u79cd\u7ebf\u7d22\u8bc4\u4f30\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u4e2a\u6027\u5316\u5927\u6a21\u578b\u5f80\u5f80\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u4e5f\u5e26\u6765\u53ef\u80fd\u7684\u7fa4\u4f53\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u3002\u524d\u4eba\u5e38\u7528\u201cpersona\u201d\u7ebf\u7d22\u5206\u6790\u504f\u89c1\uff0c\u5927\u591a\u53ea\u7528\u5355\u4e00\u7ebf\u7d22\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u7ebf\u7d22\u654f\u611f\u6027\u548c\u73b0\u5b9e\u6709\u6548\u6027\u7684\u8003\u91cf\u3002\u4f5c\u8005\u5e0c\u671b\u7cfb\u7edf\u5206\u6790\u4e0d\u540cpersona\u7ebf\u7d22\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u6311\u9009\u4e866\u79cd\u5e38\u7528\u7684persona\u7ebf\u7d22\u548c7\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u7684\uff09\uff0c\u57284\u4e2a\u5199\u4f5c\u548c\u5efa\u8bae\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u7ebf\u7d22\u5bf9\u6a21\u578b\u8f93\u51fa\u4e00\u81f4\u6027\u4e0e\u5dee\u5f02\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8861\u91cf\u5b83\u4eec\u5728\u4e0d\u540cpersona\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u4e0d\u540cpersona\u7ebf\u7d22\u867d\u7136\u6574\u4f53\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u5bf9\u4e8e\u540c\u4e00persona\uff0c\u7ebf\u7d22\u4e0d\u540c\u5bfc\u81f4\u8f93\u51fa\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u73b0\u51fa\u6a21\u578b\u5bf9\u63d0\u793a\u53d8\u4f53\u7684\u654f\u611f\u6027\u3002\u4e0d\u5efa\u8bae\u53ea\u7528\u5355\u4e00persona\u7ebf\u7d22\u5f97\u51fa\u7ed3\u8bba\u3002", "conclusion": "\u5355\u4e00persona\u7ebf\u7d22\u4f1a\u5927\u5927\u4f4e\u4f30\uff08\u6216\u9ad8\u4f30\uff09\u6a21\u578b\u504f\u89c1\u548c\u4e2a\u6027\u5316\u8f93\u51fa\u4e2d\u7684\u53d8\u5f02\u6027\u3002\u4e3a\u83b7\u5f97\u66f4\u6709\u5916\u90e8\u6548\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u7ed3\u8bba\uff0c\u672a\u6765\u6a21\u578b\u4e2a\u6027\u5316\u548c\u516c\u5e73\u6027\u7814\u7a76\u5e94\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u591a\u6837\u5316\u7684persona\u7ebf\u7d22\u3002"}}
{"id": "2601.17977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17977", "abs": "https://arxiv.org/abs/2601.17977", "authors": ["Jinchen Gu", "Nan Zhao", "Lei Qiu", "Lu Zhang"], "title": "Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors", "comment": "4 pages; 3 figures; accepted by International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u4e0e\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u2014\u2014DKGH-MoE\uff0c\u63d0\u5347\u533b\u5b66\u7b49\u5c0f\u6837\u672c\u9886\u57df\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u533b\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\uff0c\u4f20\u7edf\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u96be\u4ee5\u5b8c\u5168\u53d1\u6325\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u4e34\u5e8a\u533b\u751f\u7684\u7ecf\u9a8c\u548c\u773c\u52a8\u6570\u636e\u7b49\u4e13\u4e1a\u77e5\u8bc6\u672a\u88ab\u6709\u6548\u5229\u7528\u3002\u8be5\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0e\u533b\u5b66\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u548c\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86DKGH-MoE\u6a21\u5757\uff0c\u5c06\u6570\u636e\u9a71\u52a8\u7684MoE\u7528\u4e8e\u4ece\u539f\u59cb\u533b\u5b66\u5f71\u50cf\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u540c\u65f6\u5f15\u5165\u9886\u57df\u4e13\u5bb6\uff08\u5982\u533b\u751f\u773c\u52a8\u4fe1\u606f\uff09\u5f15\u5bfc\u7684MoE\uff0c\u5c06\u533b\u751f\u5173\u6ce8\u533a\u57df\u7b49\u8bca\u65ad\u5148\u9a8c\u6574\u5408\u5230\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u77e5\u8bc6\u4e0e\u6570\u636e\u878d\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7efc\u5408\u6570\u636e\u65b0\u6a21\u5f0f\u4e0e\u4e34\u5e8a\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u878d\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u7279\u5f81\u80fd\u8ba9\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u533b\u5b66\u7b49\u5c0f\u6837\u672c\u9886\u57df\u83b7\u5f97\u66f4\u4f18\u7ed3\u679c\uff0c\u63d0\u51fa\u7684DKGH-MoE\u662f\u4e00\u79cd\u6709\u6548\u3001\u53ef\u62d3\u5c55\u4e14\u5177\u5907\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18582", "abs": "https://arxiv.org/abs/2601.18582", "authors": ["Yuan Cao", "Feixiang Liu", "Xinyue Wang", "Yihan Zhu", "Hui Xu", "Zheng Wang", "Qiang Qiu"], "title": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection", "comment": "9 pages, 4 figures, AAAI 2026 Bridge", "summary": "Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u5c06\u4eba\u683c\u68c0\u6d4b\u4efb\u52a1\u89c6\u4e3a\u6392\u5e8f\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u683c\u7279\u8d28\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u53d6\u5f97\u4e86\u591a\u9879\u57fa\u51c6\u7684\u6700\u4f73\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4eba\u683c\u68c0\u6d4b\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5206\u7c7b\u578b\u7684\u5206\u6790\u548c\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u96be\u4ee5\u51c6\u786e\u5904\u7406\u4eba\u683c\u590d\u6742\u6027\u548c\u7279\u8d28\u4e4b\u95f4\u7684\u6a21\u7cca\u754c\u9650\uff0c\u540c\u65f6\u7f3a\u4e4f\u81ea\u4e3b\u5b66\u4e60\u6a21\u5f0f\u3002\u672c\u6587\u65e8\u5728\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\uff0c\u5bfb\u6c42\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u4eba\u683c\u68c0\u6d4b\u65b0\u8def\u5f84\u3002", "method": "\u4f5c\u8005\u5c06\u4eba\u683c\u68c0\u6d4b\u7531\u4f20\u7edf\u5206\u7c7b\u4efb\u52a1\u8f6c\u4e3a\u6392\u5e8f\u4efb\u52a1\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6709\u76d1\u7763\u5fae\u8c03(SFT)\u5efa\u7acb\u6392\u5e8f\u80fd\u529b\u5e76\u89c4\u8303\u8f93\u51fa\uff0c\u7b2c\u4e8c\u9636\u6bb5\u63d0\u51fa\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6392\u5e8f\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u5347\u6a21\u578b\u533a\u5206\u6a21\u7cca\u4eba\u683c\u7279\u8d28\u7684\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u4e3b\u6d41\u4eba\u683c\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u6700\u4f18\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7684\u5206\u7c7b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6392\u5e8f\u52a0\u5f3a\u5316\u8bad\u7ec3\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u4eba\u683c\u68c0\u6d4b\u4f5c\u4e3a\u6392\u5e8f\u4efb\u52a1\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u4e0d\u4ec5\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u8d28\u5212\u5206\u4e0a\u7684\u4e0d\u8db3\uff0c\u8fd8\u63d0\u5347\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18001", "abs": "https://arxiv.org/abs/2601.18001", "authors": ["Aqsa Yousaf", "Sint Sint Win", "Megan Coffee", "Habeeb Olufowobi"], "title": "MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images", "comment": "Accepted at WACV 2026", "summary": "Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6MorphXAI\uff0c\u7528\u4e8e\u66f4\u53ef\u89e3\u91ca\u7684\u5bc4\u751f\u866b\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5f62\u6001\u5b66\u5206\u6790\uff0c\u901a\u8fc7\u5f15\u5165\u8be6\u7ec6\u7684\u5f62\u6001\u5b66\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u68c0\u6d4b\u6027\u80fd\u4e0e\u89e3\u91ca\u6027\u7684\u517c\u987e\u3002", "motivation": "\u76ee\u524d\u5bc4\u751f\u866b\u611f\u67d3\u8bca\u65ad\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u8840\u6d82\u7247\u68c0\u67e5\u548c\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u81ea\u52a8\u5316\u6df1\u5ea6\u5b66\u4e60\u867d\u6709\u6548\u4f46\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u7c97\u7cd9\u7684\u89c6\u89c9\u70ed\u529b\u56fe\uff0c\u96be\u4ee5\u4f53\u73b0\u533b\u751f\u8bca\u65ad\u7528\u7684\u5f62\u6001\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u540c\u65f6\u517c\u5177\u68c0\u6d4b\u80fd\u529b\u4e0e\u7ec6\u7c92\u5ea6\u751f\u7269\u5b66\u89e3\u91ca\u7684\u65b0\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51faMorphXAI\u6846\u67b6\uff0c\u5728\u6a21\u578b\u9884\u6d4b\u6d41\u7a0b\u4e2d\u5f15\u5165\u5f62\u6001\u5b66\u5c5e\u6027\uff08\u5982\u5f62\u72b6\u3001\u66f2\u7387\u3001\u53ef\u89c1\u6591\u70b9\u6570\u3001\u97ad\u6bdb\u5b58\u5728\u4e0e\u53d1\u80b2\u9636\u6bb5\uff09\u76f4\u63a5\u76d1\u7763\uff0c\u5e76\u6784\u5efa\u5305\u542b\u4e09\u79cd\u5bc4\u751f\u866b\u53ca\u8be6\u7ec6\u5f62\u6001\u6807\u7b7e\u7684\u533b\u751f\u6807\u6ce8\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMorphXAI\u4e0d\u4ec5\u5728\u5bc4\u751f\u866b\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u8fd8\u80fd\u8f93\u51fa\u7ed3\u6784\u5316\u3001\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u89e3\u91ca\uff0c\u63d0\u5347\u6a21\u578b\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "MorphXAI\u5b9e\u73b0\u4e86\u5bc4\u751f\u866b\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5f62\u6001\u89e3\u91ca\u7684\u7edf\u4e00\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u7a81\u7834\u4e86\u4f20\u7edf\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u5bc4\u751f\u866b\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u3001\u89e3\u91ca\u5316\u7684\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2601.18722", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18722", "abs": "https://arxiv.org/abs/2601.18722", "authors": ["Lintang Sutawika", "Gokul Swamy", "Zhiwei Steven Wu", "Graham Neubig"], "title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning", "comment": "Code available at https://github.com/lintangsutawika/SP3F", "summary": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \\textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \\texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than\n  of the training data across the single-language, multilingual, and generalization to unseen language settings.", "AI": {"tldr": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u8f83\u5c11\u6d89\u53ca\u7684\u8bed\u8a00\u65f6\uff0c\u63a8\u7406\u6027\u80fd\u8fdc\u900a\u4e8e\u82f1\u6587\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SP3F\u65b9\u6cd5\uff0c\u5728\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u591a\u8bed\u79cd\u63a8\u7406\u80fd\u529b\u3002SP3F\u5206\u4e24\u9636\u6bb5\uff0c\u9996\u5148\u5229\u7528\u82f1\u6587\u95ee\u9898-\u7b54\u6848\u5bf9\u7684\u7ffb\u8bd1\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u501f\u52a9\u5305\u542b\u82f1\u6587\u53c2\u8003\u7b54\u6848\u7684\u5224\u522b\u5668\u8fdb\u884c\u81ea\u6211\u5bf9\u5f08\u578b\u5f3a\u5316\u5b66\u4e60\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSP3F\u663e\u8457\u63d0\u5347\u4e86\u5e95\u6a21\u7684\u591a\u8bed\u79cd\u63a8\u7406\u8868\u73b0\uff0c\u751a\u81f3\u4f18\u4e8e\u7528\u5927\u91cf\u76ee\u6807\u8bed\u6599\u540e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u79cd\u4e0b\u63a8\u7406\u80fd\u529b\u5927\u5e45\u4e0b\u964d\uff0c\u9650\u5236\u5176\u5728\u5168\u7403\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002\u73b0\u6709\u63d0\u5347\u65b9\u6cd5\u591a\u4f9d\u8d56\u5927\u91cf\u76ee\u6807\u8bed\u6599\uff0c\u5b9e\u9645\u96be\u4ee5\u83b7\u5f97\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u5b8c\u5168\u4e0d\u4f9d\u8d56\u76ee\u6807\u8bed\u8a00\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u591a\u8bed\u79cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u5229\u7528\u82f1\u6587QA\u5bf9\u7684\u673a\u5668\u7ffb\u8bd1\u7ed3\u679c\u5bf9\u6a21\u578b\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u63d0\u9ad8\u57fa\u7840\u51c6\u786e\u7387\uff1b2\uff09\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u8bbe\u7f6e\u4e00\u4e2a\u62e5\u6709\u82f1\u6587\u53c2\u8003\u7b54\u6848\u7684\u5224\u522b\u5668\u8ba9\u6a21\u578b\u8fdb\u884c\u81ea\u6211\u5bf9\u5f08\uff0c\u5224\u522b\u5668\u6bd4\u8f83\u591a\u8f6e\u56de\u7b54\u9009\u62e9\u66f4\u4f18\u8005\uff0c\u6307\u5bfc\u6a21\u578b\u63d0\u5347\uff0c\u751a\u81f3\u5728\u7b54\u6848\u90fd\u4e0d\u5b8c\u5168\u6b63\u786e\u65f6\u4e5f\u80fd\u5206\u8fa8\u4f18\u52a3\u3002", "result": "SP3F\u6781\u5927\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u591a\u8bed\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u5728\u591a\u9879\u6570\u5b66\u4e0e\u975e\u6570\u5b66\u4efb\u52a1\u4e2d\uff0cSP3F\u5728\u5355\u8bed\u3001\u591a\u8bed\u79cd\u548c\u5bf9\u672a\u89c1\u8bed\u79cd\u7684\u6cdb\u5316\u8bbe\u5b9a\u4e0b\uff0c\u5747\u4f18\u4e8e\u4f7f\u7528\u591a\u500d\u8bad\u7ec3\u6570\u636e\u7684\u5168\u91cf\u540e\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "SP3F\u65b9\u6cd5\u8bc1\u660e\u4e86\u65e0\u9700\u4f7f\u7528\u76ee\u6807\u8bed\u8a00\u6570\u636e\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u79cd\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u79cd\u53ca\u591a\u8bed\u79cd\u73af\u5883\u6709\u5b9e\u9645\u5e94\u7528\u610f\u4e49\u3002"}}
{"id": "2601.18008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18008", "abs": "https://arxiv.org/abs/2601.18008", "authors": ["Asiegbu Miracle Kanu-Asiegbu", "Nitin Jotwani", "Xiaoxiao Du"], "title": "Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection", "comment": "This work has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Code available at: https://github.com/akanuasiegbu/stripfusion", "summary": "Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Strip-Fusion\u7a7a\u95f4-\u65f6\u5e8f\u878d\u5408\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u884c\u4eba\u68c0\u6d4b\u5728\u5f3a\u906e\u6321\u548c\u56fe\u50cf\u9519\u4f4d\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u5173\u6ce8\u7a7a\u95f4\u878d\u5408\uff0c\u5ffd\u89c6\u4e86\u65f6\u5e8f\u7279\u5f81\u3002\u540c\u65f6\uff0c\u53ef\u89c1\u5149\u4e0e\u70ed\u7ea2\u5916\u56fe\u50cf\u5e38\u6709\u9519\u4f4d\uff0c\u884c\u4eba\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u906e\u6321\uff09\u4e0b\u96be\u4ee5\u68c0\u6d4b\u3002\u56e0\u800c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u65f6\u5e8f\u4fe1\u606f\u5e76\u6297\u9519\u4f4d\u3001\u9002\u5e94\u590d\u6742\u73af\u5883\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Strip-Fusion\u7f51\u7edc\uff0c\u5b83\u901a\u8fc7\u65f6\u5e8f\u81ea\u9002\u5e94\u5377\u79ef\u52a8\u6001\u878d\u5408\u7a7a\u95f4\u548c\u65f6\u5e8f\u7279\u5f81\uff0c\u589e\u5f3a\u5bf9\u884c\u4eba\u8fd0\u52a8\u548c\u4e0a\u4e0b\u6587\u7684\u6355\u6349\u3002\u540c\u65f6\u521b\u65b0\u6027\u5730\u5f15\u5165KL\u6563\u5ea6\u635f\u5931\uff0c\u7f13\u89e3\u53ef\u89c1\u5149\u4e0e\u70ed\u7ea2\u5916\u6a21\u6001\u4fe1\u606f\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5bfc\u7279\u5f81\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u540e\u5904\u7406\u7b97\u6cd5\u4ee5\u51cf\u5c11\u8bef\u68c0\u3002", "result": "\u5728KAIST\u4e0eCVC-14\u7b49\u591a\u6a21\u6001\u884c\u4eba\u68c0\u6d4b\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\uff0cStrip-Fusion\u5c55\u73b0\u51fa\u4e0e\u6700\u65b0\u6280\u672f\u6c34\u5e73\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u906e\u6321\u4e25\u91cd\u548c\u56fe\u50cf\u9519\u4f4d\u7b49\u6311\u6218\u573a\u666f\u4e0b\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "Strip-Fusion\u7f51\u7edc\u80fd\u6709\u6548\u878d\u5408\u7a7a\u95f4\u548c\u65f6\u5e8f\u4fe1\u606f\uff0c\u4e14\u5bf9\u591a\u6a21\u6001\u56fe\u50cf\u9519\u4f4d\u548c\u884c\u4eba\u906e\u6321\u5177\u6709\u8f83\u5f3a\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u7b49\u9886\u57df\u7684\u884c\u4eba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4e3a\u5b9e\u7528\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2601.18724", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.18724", "abs": "https://arxiv.org/abs/2601.18724", "authors": ["Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences", "comment": "Work In Progress", "summary": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b02024\u548c2025\u5e74ACL\u3001NAACL\u3001EMNLP\u7b49\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9876\u4f1a\u4e2d\uff0c\u6709\u8fd1300\u7bc7\u8bba\u6587\u542b\u6709\u865a\u5047\u5f15\u7528\uff08HalluCitation\uff09\uff0c\u4e14\u6570\u91cf\u6b63\u5728\u5feb\u901f\u4e0a\u5347\uff0c\u5df2\u5f71\u54cd\u5b66\u672f\u4f1a\u8bae\u53ef\u9760\u6027\u3002", "motivation": "\u8fd1\u5e74\u5728\u8bba\u6587\u3001\u9884\u5370\u672c\u548c\u5df2\u53d1\u8868\u8bba\u6587\u4e2d\u9891\u7e41\u51fa\u73b0\u865a\u5047\u5f15\u7528\uff0c\u4e0d\u4ec5\u5f71\u54cd\u79d1\u5b66\u53ef\u9760\u6027\uff0c\u8fd8\u4f1a\u635f\u5bb3\u4f1a\u8bae\u516c\u4fe1\u529b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u7814\u7a76\u865a\u5047\u5f15\u7528\u7684\u73b0\u8c61\u548c\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6536\u96c6\u5e76\u5206\u6790\u4e862024\u5e74\u548c2025\u5e74ACL\u3001NAACL\u3001EMNLP\u4e3b\u4f1a\u3001Findings\u53caworkshop\u5168\u90e8\u8bba\u6587\uff0c\u5bf9\u6587\u732e\u4e2d\u51fa\u73b0\u7684\u865a\u5047\u5f15\u7528\uff08HalluCitation\uff09\u8fdb\u884c\u7edf\u8ba1\u548c\u7cfb\u7edf\u6027\u5206\u6790\u3002", "result": "\u7edf\u8ba1\u6570\u636e\u663e\u793a\uff0c\u8fd1300\u7bc7\u8bba\u6587\u5305\u542b\u81f3\u5c11\u4e00\u6761\u865a\u5047\u5f15\u7528\uff0c\u5927\u90e8\u5206\u6765\u81ea2025\u5e74\uff0c\u4e14\u4e00\u534a\u96c6\u4e2d\u51fa\u73b0\u5728\u6700\u65b0\u7684EMNLP 2025\uff1b\u5176\u4e2d\u6709100\u4f59\u7bc7\u4e3a\u4e3b\u4f1a\u548cFindings\u8bba\u6587\u3002", "conclusion": "\u968f\u7740\u865a\u5047\u5f15\u7528\u6570\u91cf\u5feb\u901f\u589e\u52a0\uff0c\u5c24\u5176\u662f\u6700\u65b0\u4f1a\u8bae\u4e2d\u7684\u5927\u91cf\u51fa\u73b0\uff0c\u5df2\u660e\u663e\u5f71\u54cd\u76f8\u5173\u4f1a\u8bae\u7684\u5b66\u672f\u516c\u4fe1\u529b\u548c\u79d1\u5b66\u53ef\u4fe1\u5ea6\uff0c\u9700\u5f15\u8d77\u5b66\u754c\u9ad8\u5ea6\u91cd\u89c6\u3002"}}
{"id": "2601.18045", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18045", "abs": "https://arxiv.org/abs/2601.18045", "authors": ["Zhuangzhi Gao", "Feixiang Zhou", "He Zhao", "Xiuju Chen", "Xiaoxin Li", "Qinkai Yu", "Yitian Zhao", "Alena Shantsila", "Gregory Y. H. Lip", "Eduard Shantsila", "Yalin Zheng"], "title": "Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation", "comment": "Accepted by IEEE International Symposium on Biomedical Imaging (ISBI) 2026. 5 pages, 3 figures", "summary": "Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7f51\u7edc\u7ed3\u6784\u76f4\u63a5\u878d\u5408\u62d3\u6251\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u4e2d\u66f2\u72b6\u7ed3\u6784\u7684\u5206\u5272\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u66f2\u72b6\u7ed3\u6784\u5206\u5272\u5bf9\u4e34\u5e8a\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u878d\u5408\u62d3\u6251\u5c5e\u6027\uff08\u5982\u8fde\u901a\u6027\uff09\u53ef\u4ee5\u63d0\u5347\u5206\u5272\u8868\u73b0\uff0c\u4f46\u4ecePD\uff08Persistence Diagrams\uff09\u4e2d\u63d0\u53d6\u5e76\u5d4c\u5165\u8fd9\u4e9b\u5c5e\u6027\u5177\u6709\u9ad8\u96be\u5ea6\u4e0e\u9ad8\u8ba1\u7b97\u4ee3\u4ef7\uff0c\u4e14\u4ee5\u5f80\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faPIs-Regressor\u6a21\u5757\uff0c\u80fd\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60Persistence Image\uff08PI\uff09\u7684\u62d3\u6251\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u5230Topology SegNet\u7f51\u7edc\u4e2d\uff0c\u5b9e\u73b0\u62d3\u6251\u7279\u5f81\u5728\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u9636\u6bb5\u7684\u878d\u5408\uff0c\u4ece\u800c\u5c06\u62d3\u6251\u4fe1\u606f\u5d4c\u5165\u7f51\u7edc\u7ed3\u6784\u672c\u8eab\u3002\u8be5\u8bbe\u8ba1\u7075\u6d3b\uff0c\u53ef\u4e0e\u5176\u4ed6\u57fa\u4e8e\u62d3\u6251\u7684\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u8fc7\u66dd\u548c\u6a21\u7cca\u7b49\u95ee\u9898\u8868\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e09\u4e2a\u66f2\u72b6\u7ed3\u6784\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u50cf\u7d20\u7ea7\u51c6\u786e\u7387\u548c\u62d3\u6251\u4e00\u81f4\u6027\u7684\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "\u76f4\u63a5\u5c06\u62d3\u6251\u4fe1\u606f\u6df1\u5ea6\u878d\u5408\u8fdb\u7f51\u7edc\u67b6\u6784\uff0c\u800c\u975e\u901a\u8fc7\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u53ef\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u591f\u7075\u6d3b\u6269\u5c55\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u3002"}}
{"id": "2601.18730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18730", "abs": "https://arxiv.org/abs/2601.18730", "authors": ["Henry Bell", "Caroline Zhang", "Mohammed Mobasserul Haque", "Dhaval Potdar", "Samia Zaman", "Brandon Fain"], "title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale", "comment": null, "summary": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \\textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \\textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \\textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \\textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \\textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \\textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aReflect\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u5baa\u6cd5\u6027\u539f\u5219\u5bf9\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u7ef4\u6301\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9075\u5faa\u591a\u6837\u590d\u6742\u539f\u5219\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u5982RLHF\u7b49\u53c2\u6570\u5fae\u8c03\uff0c\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u96be\u4ee5\u83b7\u5f97\u7684\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff0c\u5de5\u7a0b\u6210\u672c\u9ad8\uff0c\u5e76\u4e14\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u65b0\u7684\u539f\u5219\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7075\u6d3b\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "Reflect\u6846\u67b6\u5728\u63a8\u7406\u65f6\uff08inference-time\uff09\u64cd\u4f5c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u901a\u8fc7\u5982\u4e0b\u6b65\u9aa4\u5b9e\u73b0\u539f\u5219\u5bf9\u9f50\uff1a1\uff09\u57fa\u4e8e\u539f\u5219\u751f\u6210\u521d\u59cb\u54cd\u5e94\uff0c2\uff09\u6a21\u578b\u81ea\u8bc4\u751f\u6210\u5185\u5bb9\u662f\u5426\u7b26\u5408\u539f\u5219\uff0c3\uff09\u6a21\u578b\u81ea\u6211\u6279\u8bc4\u6307\u51fa\u4e0d\u8db3\uff0c4\uff09\u6839\u636e\u539f\u5219\u548c\u6279\u8bc4\u7ed3\u679c\u4fee\u6b63\u6700\u7ec8\u7b54\u6848\u3002\u6574\u4e2a\u8fc7\u7a0b\u90fd\u5728\u4e0a\u4e0b\u6587\u5185\u5b8c\u6210\u3002", "result": "Reflect\u5728\u591a\u79cd\u590d\u6742\u3001\u591a\u6837\u7684\u539f\u5219\u96c6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u539f\u5219\u9075\u5faa\u7387\uff0c\u4e14\u80fd\u5904\u7406\u4e0e\u539f\u6709\u5fae\u8c03\u8fc7\u7a0b\u5f3a\u8c03\u5185\u5bb9\u5b8c\u5168\u4e0d\u540c\u7684\u65b0\u539f\u5219\u3002\u4e0e\u4f20\u7edffew-shot\u65b9\u6cd5\u76f8\u6bd4\uff0cReflect\u6709\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u548c\u7f55\u89c1\u8fdd\u89c4\u53d1\u751f\u7387\u3002", "conclusion": "Reflect\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6570\u636e\u9a71\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u5b89\u5168\u6027\u3001\u5065\u58ee\u6027\u4ee5\u53ca\u9002\u5e94\u590d\u6742\u539f\u5219\u7684\u80fd\u529b\uff0c\u540c\u65f6\u8fd8\u80fd\u81ea\u7136\u751f\u6210\u6709\u5229\u4e8e\u540e\u7eed\u4f20\u7edf\u5fae\u8c03\u7684\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u4f4e\u6210\u672c\u5730\u90e8\u7f72\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u3002"}}
{"id": "2601.18049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18049", "abs": "https://arxiv.org/abs/2601.18049", "authors": ["Yunfei Qiu", "Qiqiong Ma", "Tianhua Lv", "Li Fang", "Shudong Zhou", "Wei Yao"], "title": "Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling", "comment": null, "summary": "Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u5148\u9a8c\u4fe1\u606f\u4e0e\u52a8\u6001\u5b66\u4e60\u673a\u5236\uff0c\u76ee\u7684\u5728\u4e8e\u63d0\u5347\u6807\u7b7e\u7a33\u5b9a\u6027\u548c\u5206\u7c7b\u6027\u80fd\u3002\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u8d85\u50cf\u7d20\u6807\u7b7e\u4f20\u64ad\uff08EASLP\uff09\u53ca\u52a8\u6001\u5386\u53f2\u878d\u5408\u9884\u6d4b\uff08DHP\uff09\u7b49\u65b9\u6cd5\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u73b0\u6709\u4f2a\u6807\u7b7e\u6269\u6563\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5206\u7c7b\u8868\u73b0\u3002", "motivation": "\u534a\u76d1\u7763\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u56e0\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u4e14\u6807\u6ce8\u4ee3\u4ef7\u9ad8\uff0c\u4f9d\u8d56\u5c11\u91cf\u6837\u672c\u8fdb\u884c\u6709\u6548\u5b66\u4e60\u4ecd\u5b58\u5728\u6807\u7b7e\u6269\u6563\u548c\u4f2a\u6807\u7b7e\u4e0d\u7a33\u5b9a\u7b49\u74f6\u9888\uff0c\u4e9f\u9700\u63d0\u5347\u8fb9\u754c\u533a\u5206\u80fd\u529b\u4e0e\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u5927\u6a21\u5757\uff1a1\uff09\u8fb9\u7f18\u611f\u77e5\u8d85\u50cf\u7d20\u6807\u7b7e\u4f20\u64ad\uff08EASLP\uff09\uff0c\u901a\u8fc7\u8fb9\u7f18\u60e9\u7f5a\u4e0e\u90bb\u57df\u6821\u6b63\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u8fb9\u754c\u7684\u6807\u7b7e\u4f20\u64ad\u9c81\u68d2\u6027\uff1b2\uff09\u52a8\u6001\u5386\u53f2\u878d\u5408\u9884\u6d4b\uff08DHP\uff09\uff0c\u878d\u5408\u5386\u53f2\u548c\u5f53\u524d\u9884\u6d4b\u7ed3\u679c\uff0c\u51cf\u5c0f\u4f2a\u6807\u7b7e\u6ce2\u52a8\uff1b3\uff09\u81ea\u9002\u5e94\u4e09\u5143\u6837\u672c\u5206\u7c7b\uff08ATSC\uff09\uff0c\u57fa\u4e8e\u4fe1\u5fc3\u548c\u4e00\u81f4\u6027\u6307\u6807\uff0c\u5bf9\u6837\u672c\u8fdb\u884c\u5206\u5c42\u5229\u7528\u63d0\u9ad8\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002\u8fd9\u4e9b\u6a21\u5757\u5171\u540c\u7ec4\u6210\u52a8\u6001\u53ef\u9760\u589e\u5f3a\u4f2a\u6807\u7b7e\u6846\u67b6\uff08DREPL\uff09\uff0c\u5e76\u4e0eEASLP\u534f\u540c\uff0c\u5b9e\u73b0\u65f6\u7a7a\u4e00\u81f4\u6027\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u4f2a\u6807\u7b7e\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u5206\u7c7b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u7a7a\u95f4\u5148\u9a8c\u53ca\u52a8\u6001\u673a\u5236\u7684\u534a\u76d1\u7763\u5206\u7c7b\u6846\u67b6\u6709\u6548\u5e94\u5bf9\u4e86\u6807\u7b7e\u6269\u6563\u548c\u4f2a\u6807\u7b7e\u4e0d\u7a33\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u5728\u5b9e\u9645\u6709\u9650\u6807\u6ce8\u573a\u666f\u4e0b\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18731", "abs": "https://arxiv.org/abs/2601.18731", "authors": ["Hongru Cai", "Yongqi Li", "Tiezheng Yu", "Fengbin Zhu", "Wenjie Wang", "Fuli Feng", "Wenjie Li"], "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment", "comment": null, "summary": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5956\u52b1\u5efa\u6a21\uff08MRM\uff09\u65b9\u6cd5\uff0c\u5c06\u4e2a\u6027\u5316\u5956\u52b1\u5efa\u6a21\u95ee\u9898\u91cd\u5851\u4e3a\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u5927\u6a21\u578b\u5bf9\u7528\u6237\u504f\u597d\u7684\u5feb\u901f\u9002\u5e94\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e2a\u6027\u5316\u5bf9\u9f50\u8868\u73b0\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\uff0c\u9762\u4e34\u7684\u5355\u4e2a\u7528\u6237\u53cd\u9988\u7a00\u7f3a\u53ca\u5bf9\u65b0\u7528\u6237\u9002\u5e94\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5e38\u89c4\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5b66\u4e60\u4e2a\u4f53\u5316\u7528\u6237\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8303\u5f0f\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u901f\u5ea6\u3002", "method": "\u4f5c\u8005\u63d0\u51faMeta Reward Modeling\uff08MRM\uff09\uff0c\u5c06\u6bcf\u4e2a\u7528\u6237\u7684\u5956\u52b1\u6a21\u578b\u8868\u793a\u4e3a\u57fa\u7840\u5956\u52b1\u51fd\u6570\u7684\u52a0\u6743\u7ec4\u5408\uff0c\u5e76\u501f\u52a9MAML\u98ce\u683c\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u5316\u7528\u6237\u4e2a\u6027\u5316\u6743\u91cd\u7684\u521d\u59cb\u5316\uff0c\u652f\u6301\u5728\u5c11\u91cf\u53cd\u9988\u4e0b\u5feb\u901f\u9002\u5e94\u65b0\u7528\u6237\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86Robust Personalization Objective\uff08RPO\uff09\uff0c\u901a\u8fc7\u5728\u5143\u4f18\u5316\u8fc7\u7a0b\u4e2d\u52a0\u5f3a\u5bf9\u96be\u4ee5\u5b66\u4e60\u7528\u6237\u7684\u5173\u6ce8\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e2a\u6027\u5316\u504f\u597d\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMRM\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u5c11\u6837\u672c\u4e2a\u6027\u5316\u3001\u63d0\u5347\u9762\u5411\u96be\u5b66\u7528\u6237\u7684\u9c81\u68d2\u6027\uff0c\u5404\u9879\u6307\u6807\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5143\u5956\u52b1\u5efa\u6a21\u4e0e\u9c81\u68d2\u4e2a\u6027\u5316\u76ee\u6807\uff0c\u672c\u6587\u6709\u6548\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u6a21\u578b\u5728\u7528\u6237\u53cd\u9988\u7a00\u7f3a\u548c\u9002\u5e94\u65b0\u7528\u6237\u65b9\u9762\u7684\u96be\u9898\uff0c\u4e3a\u5927\u6a21\u578b\u4e2a\u6027\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2601.18088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18088", "abs": "https://arxiv.org/abs/2601.18088", "authors": ["Jianshu Chao", "Tianhua Lv", "Qiqiong Ma", "Yunfei Qiu", "Li Fang", "Huifang Shen", "Wei Yao"], "title": "Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification", "comment": null, "summary": "Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6e90\u57df\u6807\u7b7e\u7684\u81ea\u76d1\u7763\u8de8\u57df\u8f6c\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u5149\u8c31\u6570\u636e\u7684\u9ad8\u6548\u9002\u5e94\u6027\u5206\u7c7b\uff0c\u5728\u6837\u672c\u7a00\u7f3a\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u9ad8\u5149\u8c31\u81ea\u76d1\u7763\u5b66\u4e60\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u8de8\u57df\u8fc1\u79fb\u65f6\u4f9d\u8d56\u6e90\u57df\u6807\u6ce8\uff0c\u4e14\u6613\u53d7\u6570\u636e\u5206\u5e03\u6f02\u79fb\u5f71\u54cd\uff0c\u5bfc\u81f4\u5728\u65b0\u57df\u6cdb\u5316\u80fd\u529b\u964d\u4f4e\u3002\u4e9f\u9700\u4e0d\u4f9d\u8d56\u6807\u7b7e\u4e14\u8fc1\u79fb\u9c81\u68d2\u7684\u65b9\u6cd5\u63d0\u5347\u6cdb\u5316\u6027\u3002", "method": "1. \u5f15\u5165\u7a7a\u95f4-\u5149\u8c31\u53cc\u5206\u652fTransformer\uff08S2Former\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7a7a\u95f4\u7ed3\u6784\u4e0e\u5149\u8c31\u7ec6\u8282\u534f\u540c\u8868\u5f81\uff0c\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u3002\n2. \u8bbe\u8ba1\u9891\u57df\u7ea6\u675f\uff08FDC\uff09\uff0c\u901a\u8fc7rFFT\u4e0e\u9ad8\u9891\u5e45\u503c\u635f\u5931\u4fdd\u6301\u7279\u5f81\u9891\u8c31\u4e00\u81f4\uff0c\u589e\u5f3a\u6a21\u578b\u7ec6\u7c92\u5ea6\u4e0e\u8fb9\u754c\u8fa8\u522b\u80fd\u529b\u3002\n3. \u5fae\u8c03\u9636\u6bb5\u91c7\u7528\u6269\u6563\u5bf9\u9f50\u84b8\u998f\uff08DAFT\uff09\uff0c\u5e08\u751f\u6a21\u578b\u5bf9\u9f50\u8bed\u4e49\u8fc1\u79fb\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4f4e\u6807\u6ce8\u60c5\u5f62\u4e0b\u9c81\u68d2\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5c55\u73b0\u51fa\u7a33\u5b9a\u7684\u5206\u7c7b\u7cbe\u5ea6\u4ee5\u53ca\u4f18\u79c0\u7684\u8de8\u57df\u9002\u5e94\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u7b7e\u6216\u65e0\u6807\u7b7e\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u6e90\u57df\u6807\u7b7e\uff0c\u5728\u6837\u672c\u6709\u9650\u65f6\u4ecd\u5177\u5907\u5f3a\u6cdb\u5316\u548c\u8fc1\u79fb\u6027\uff0c\u4e3a\u9ad8\u5149\u8c31\u8de8\u57df\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18771", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18771", "abs": "https://arxiv.org/abs/2601.18771", "authors": ["Yanming Liu", "Xinyue Peng", "Zixuan Yan", "Yanxin Shen", "Wenjie Xu", "Yuefeng Huang", "Xinyi Wang", "Jiannan Cao", "Jianwei Yin", "Xuhong Zhang"], "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory", "comment": "Dep-Search 1st version", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDep-Search\uff0c\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\u611f\u77e5\u641c\u7d22\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u591a\u6b65\u63a8\u7406\u641c\u7d22\u6846\u67b6\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u9690\u5f0f\u63a8\u7406\uff0c\u5bfc\u81f4\u5b50\u95ee\u9898\u4f9d\u8d56\u5173\u7cfb\u7ba1\u7406\u3001\u77e5\u8bc6\u590d\u7528\u548c\u5f3a\u5316\u5b66\u4e60\u641c\u7d22\u7b56\u7565\u5b58\u5728\u74f6\u9888\u3002", "method": "Dep-Search\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u3001\u68c0\u7d22\u548c\u6301\u4e45\u5316\u8bb0\u5fc6\uff08\u6574\u5408GRPO\u6a21\u5757\uff09\uff0c\u5f15\u5165\u663e\u5f0f\u63a7\u5236\u673a\u5236\uff1a\u53ef\u5bf9\u95ee\u9898\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u5206\u89e3\u3001\u6309\u9700\u68c0\u7d22\u4fe1\u606f\u3001\u8bbf\u95ee/\u590d\u7528\u5df2\u5b58\u77e5\u8bc6\uff0c\u5e76\u5c06\u957f\u63a8\u7406\u8fc7\u7a0b\u6458\u8981\u4e3a\u53ef\u590d\u7528\u8bb0\u5fc6\u5355\u5143\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u6837\u5316\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cDep-Search\u5728\u591a\u8df3\u63a8\u7406\u65b9\u9762\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u5747\u53d6\u5f97\u660e\u663e\u63d0\u5347\u3002", "conclusion": "Dep-Search \u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u641c\u7d22\u6846\u67b6\u4f9d\u8d56\u9690\u5f0f\u63a8\u7406\u7684\u5c40\u9650\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u53ca\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u4e86 LLM \u591a\u6b65\u63a8\u7406\u4efb\u52a1\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2601.18098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18098", "abs": "https://arxiv.org/abs/2601.18098", "authors": ["Chuang Yang", "Haozhao Ma", "Xu Han", "Yuan Yuan", "Qi Wang"], "title": "Text-Pass Filter: An Efficient Scene Text Detector", "comment": null, "summary": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Text-Pass Filter\uff08TPF\uff09\u65b9\u6cd5\u7528\u4e8e\u4efb\u610f\u5f62\u72b6\u6587\u672c\u68c0\u6d4b\uff0c\u901a\u8fc7\u4eff\u9020\u5e26\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u6587\u672c\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u4e14\u5b9e\u65f6\u7684\u5206\u5272\u548c\u68c0\u6d4b\u3002\u5f15\u5165\u7684REU\u548cFPU\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6536\u7f29\u63a9\u7801\u6269\u5c55\u7b56\u7565\u5728\u6587\u672c\u68c0\u6d4b\u4e2d\u4f1a\u4e22\u5931\u6587\u672c\u8fb9\u7f18\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5bfc\u81f4\u524d\u666f\u548c\u80cc\u666f\u6df7\u6dc6\uff0c\u5bf9\u6587\u672c\u7279\u5f81\u8bc6\u522b\u9020\u6210\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u4efb\u610f\u5f62\u72b6\u7684\u6587\u672c\u5e76\u907f\u514d\u8fd9\u4e9b\u5185\u5728\u7f3a\u9677\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86Text-Pass Filter\uff08TPF\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u5206\u5272\u6574\u5757\u6587\u672c\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u3002TPF\u901a\u8fc7\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u552f\u4e00\u7684\u7279\u5f81\uff0d\u6ee4\u6ce2\u5668\u5bf9\uff0c\u901a\u8fc7\u6ee4\u6ce2\u5668\u7b5b\u51fa\u5339\u914d\u6587\u672c\u3002\u540c\u65f6\uff0c\u63d0\u51faReinforcement Ensemble Unit\uff08REU\uff09\u89e3\u51b3\u5e26\u72b6\u957f\u6587\u672c\u7684\u7279\u5f81\u4e00\u81f4\u6027\u95ee\u9898\u5e76\u6269\u5927\u8bc6\u522b\u8303\u56f4\uff0c\u589e\u52a0Foreground Prior Unit\uff08FPU\uff09\u4ee5\u63d0\u5347\u524d\u666f\u548c\u80cc\u666f\u7684\u8fa8\u6790\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTPF\u65b9\u6cd5\u5728\u6587\u672c\u5206\u5272\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002REU\u548cFPU\u4e24\u4e2a\u6a21\u5757\u4e5f\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u63d0\u5347\u3002", "conclusion": "TPF\u80fd\u591f\u81ea\u7136\u5730\u5206\u79bb\u7c98\u8fde\u6587\u672c\u3001\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u7279\u5f81\u4e22\u5931\u4e0e\u8fb9\u754c\u6df7\u6dc6\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u4efb\u610f\u5f62\u72b6\u6587\u672c\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2601.18788", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18788", "abs": "https://arxiv.org/abs/2601.18788", "authors": ["Mumin Jia", "Jairo Diaz-Rodriguez"], "title": "Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings", "comment": "arXiv admin note: substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437", "summary": "Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u6587\u672c\u5206\u5272\u65b0\u65b9\u6cd5Embed-KCPD\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u901a\u8fc7\u4f18\u5316\u5e26\u60e9\u7f5a\u9879\u7684KCPD\u76ee\u6807\u51fd\u6570\uff0c\u57fa\u4e8e\u5d4c\u5165\u5411\u91cf\u68c0\u6d4b\u8fb9\u754c\u3002\u7406\u8bba\u5206\u6790\u4e0e\u6a21\u62df\u5b9e\u9a8c\u5e76\u7ed3\u5408\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u5747\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u6587\u672c\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u6807\u6ce8\u5f88\u8d35\u3001\u4e3b\u89c2\uff0c\u5e76\u4e14\u96be\u4ee5\u8de8\u9886\u57df\u3001\u8de8\u7c92\u5ea6\u8fc1\u79fb\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u3001\u65e0\u9700\u76d1\u7763\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u53e5\u5b50\u8868\u793a\u4e3a\u5411\u91cf\uff0c\u5229\u7528\u5e26\u60e9\u7f5a\u9879\u7684KCPD\u76ee\u6807\u51fd\u6570\u5728\u65e0\u9700\u8bad\u7ec3\u60c5\u51b5\u4e0b\u4f30\u8ba1\u5206\u5272\u8fb9\u754c\uff0c\u7406\u8bba\u4e0a\u5206\u6790$m$-\u76f8\u5173\u5e8f\u5217\u4e0b\u7684\u65b9\u6cd5\u6027\u80fd\uff0c\u5e76\u7528\u5927\u6a21\u578b\u8bbe\u8ba1\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684oracle\u4e0d\u7b49\u5f0f\u53ca\u7cbe\u786e\u5b9a\u4f4d\u80fd\u529b\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u4e0e\u771f\u5b9e\u63a8\u6587\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "Embed-KCPD\u5728\u7406\u8bba\u3001\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u5c55\u73b0\u4e86\u4f18\u79c0\u7684\u6587\u672c\u5206\u5272\u6027\u80fd\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2601.18099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18099", "abs": "https://arxiv.org/abs/2601.18099", "authors": ["Akbar Saadat"], "title": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs", "comment": "9 pages, 14 input images, 3 TikZ images. arXiv admin note: substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779", "summary": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u524d\u5411\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u9ad8\u65af\u6a21\u578b\u5e94\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\uff0c\u901a\u8fc7\u79bb\u6563\u8ba1\u7b97\u6765\u6062\u590d\u9510\u5316\u56fe\u50cf\u7684\u6563\u7126\uff08\u6a21\u7cca\uff09\u7248\u672c\uff0c\u5e76\u5728\u5b9e\u9645\u56fe\u50cf\u4e0a\u83b7\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6a21\u7cca\u4f30\u8ba1\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u9a8c\u8bc1\u4e86\u9ad8\u65af\u6a21\u7cca\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4f46\u5982\u4f55\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u3001\u5feb\u901f\u4e14\u51c6\u786e\u5730\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u8be5\u6a21\u578b\u4ecd\u5b58\u5728\u6311\u6218\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5b9e\u73b0\u6563\u7126\u6a21\u7cca\u7684\u5feb\u901f\u63a8\u65ad\uff0c\u4ece\u800c\u6ee1\u8db3\u5b9e\u65f6\u6216\u5728\u7ebf\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u5316\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7b5b\u9009\u591a\u89e3\uff0c\u5e76\u9002\u7528\u4e8e\u4e24\u5e45\u90e8\u5206\u6a21\u7cca\u7684\u56fe\u50cf\u4e92\u76f8\u8f6c\u6362\u7684\u573a\u666f\u3002\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u76f4\u63a5\u8ba1\u7b97\u51fa\u6a21\u7cca\u6838\u7684\u6807\u51c6\u5dee\uff0c\u5e76\u9009\u62e9\u6700\u4f18\u89e3\u3002", "result": "\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u5408\u6210\u6a21\u7cca\u503c\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u63a7\u5236\u57281.7%\u4ee5\u4e0b\uff0c\u6a21\u7cca\u56fe\u50cf\u4e0e\u4f30\u8ba1\u56fe\u50cf\u4e4b\u95f4\u7684\u5f3a\u5ea6\u5dee\u5f02\u63a7\u5236\u57282%\u4ee5\u5185\uff0c\u663e\u793a\u4e86\u5f88\u9ad8\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u8ba1\u7b97\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\uff0c\u53ef\u6709\u6548\u5e76\u51c6\u786e\u5b9e\u73b0\u9ad8\u65af\u6563\u7126\u4f30\u8ba1\uff0c\u5177\u5907\u5b9e\u65f6\u4e0e\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.18790", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18790", "abs": "https://arxiv.org/abs/2601.18790", "authors": ["Etienne Lanzeray", "Stephane Meilliez", "Malo Ruelle", "Damien Sileo"], "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts", "comment": null, "summary": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MortalMATH\u57fa\u51c6\uff0c\u901a\u8fc7150\u4e2a\u6781\u7aef\u5371\u6025\u60c5\u51b5\u4e0b\u7684\u4ee3\u6570\u6c42\u52a9\u573a\u666f\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u63a8\u7406\u4f18\u5316\u4e0b\u662f\u5426\u5ffd\u89c6\u5b89\u5168\u98ce\u9669\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u4e13\u6ce8\u63a8\u7406\u7684\u6a21\u578b\u66f4\u5bb9\u6613\u5ffd\u89c6\u7d27\u6025\u72b6\u51b5\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u6ce8\u91cd\u590d\u6742\u63a8\u7406\u4e0e\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u672c\u6587\u8bd5\u56fe\u63a2\u8ba8\u8fd9\u79cd\u4f18\u5316\u662f\u5426\u5bfc\u81f4\u6a21\u578b\u5728\u9762\u5bf9\u7528\u6237\u63cf\u8ff0\u7684\u751f\u547d\u5371\u9669\u65f6\u4ea7\u751f\u201c\u96a7\u9053\u89c6\u91ce\u201d\uff0c\u5373\u4e00\u5473\u5173\u6ce8\u4efb\u52a1\u800c\u5ffd\u89c6\u7528\u6237\u5b89\u5168\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86MortalMATH\u57fa\u51c6\u96c6\uff0c\u5305\u542b150\u4e2a\u6848\u4f8b\uff0c\u6bcf\u4e2a\u6848\u4f8b\u4e2d\u7528\u6237\u5728\u63cf\u8ff0\u751f\u547d\u5a01\u80c1(\u5982\u4e2d\u98ce\u3001\u81ea\u7531\u843d\u4f53)\u7684\u540c\u65f6\u8bf7\u6c42\u4ee3\u6570\u5e2e\u52a9\u3002\u5bf9\u6bd4\u8bc4\u4f30\u4e86\u6cdb\u5316\u578b\u6a21\u578b\uff08\u5982Llama-3.1\uff09\u548c\u63a8\u7406\u4e13\u957f\u578b\u6a21\u578b\uff08\u5982Qwen-3-32b\u3001GPT-5-nano\uff09\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6d4b\u8bd5\u53d1\u73b0\u6cdb\u5316\u578b\u6a21\u578b\u5f80\u5f80\u62d2\u7edd\u7ee7\u7eed\u6570\u5b66\u5e2e\u52a9\uff0c\u8f6c\u800c\u5173\u6ce8\u7528\u6237\u5371\u9669\uff1b\u800c\u4e13\u6ce8\u63a8\u7406\u578b\u6a21\u578b\u57fa\u672c\u5ffd\u89c6\u7d27\u6025\u60c5\u5883\uff0c\u4ecd\u4ee5\u8d85\u8fc795%\u7684\u6982\u7387\u5b8c\u6210\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u63a8\u7406\u8fc7\u7a0b\u7684\u8ba1\u7b97\u5ef6\u8fdf\u53ef\u8fbe15\u79d2\uff0c\u8fdb\u4e00\u6b65\u589e\u52a0\u98ce\u9669\u3002", "conclusion": "\u5355\u7eaf\u4f18\u5316\u6a21\u578b\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u5bfc\u81f4\u5176\u5ffd\u89c6\u5b9e\u9645\u5b89\u5168\u9700\u6c42\uff0c\u964d\u4f4e\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u65f6\u7684\u5b89\u5168\u6027\u3002\u5f3a\u8c03\u6df1\u5ea6\u63a8\u7406\u8bad\u7ec3\u53ef\u80fd\u53cd\u800c\u8ba9\u6a21\u578b\u201c\u9057\u5fd8\u201d\u5e94\u6709\u7684\u5b89\u5168\u672c\u80fd\u3002"}}
{"id": "2601.18100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18100", "abs": "https://arxiv.org/abs/2601.18100", "authors": ["James Tribble", "Hao Wang", "Si-En Hong", "Chaoyi Zhou", "Ashish Bastola", "Siyu Huang", "Abolfazl Razi"], "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos", "comment": null, "summary": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u957f\u65f6 egocentric\uff08\u81ea\u6211\u89c6\u89d2\uff09\u89c6\u9891\u4e2d\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u7a7a\u95f4\u4fe1\u53f7\u548c\u6df1\u5ea6\u4fe1\u606f\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u65b0\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u505a\u4e86\u7cfb\u7edf\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u867d\u5728\u56fe\u50cf\u548c\u77ed\u89c6\u9891\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u5bf9\u89c6\u89d2\u6f02\u79fb\u548c\u7f3a\u4e4f\u6301\u7eed\u51e0\u4f55\u4e0a\u4e0b\u6587\u7684\u957f\u65f6 egocentric \u89c6\u9891\u65f6\uff0c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002\u5982\u4f55\u63d0\u5347VLM\u5728\u7c7b\u4f3c\u590d\u6742\u573a\u666f\u4e0b\u7684\u7a7a\u95f4\u7406\u89e3\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5bf9Google Sanpo\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u91cd\u65b0\u6807\u6ce8\uff08Sanpo-D\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9762\u5411\u5bfc\u822a\u7684\u7a7a\u95f4\u67e5\u8be2\u4efb\u52a1\uff0c\u5bf9\u591a\u4e2a\u4e3b\u6d41VLM\u8fdb\u884c\u6027\u80fd\u57fa\u7ebf\u6d4b\u8bd5\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5728RGB\u5e27\u4e2d\u878d\u5408\u6df1\u5ea6\u56fe\uff0c\u5206\u6790\u8f93\u5165\u5c42\u9762\u7684\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\u5bf9\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6df1\u5ea6\u611f\u77e5\u548c\u7a7a\u95f4\u7ea6\u675f\u7684\u8f93\u5165\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u5982\u884c\u4eba\u68c0\u6d4b\u3001\u969c\u788d\u7269\u8bc6\u522b\u7b49\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u7a7a\u95f4\u63a8\u7406\u8868\u73b0\uff0c\u4f46\u4f1a\u5728\u901a\u7528\u4efb\u52a1\u51c6\u786e\u7387\u4e0e\u7a7a\u95f4\u4e13\u7528\u6027\u4e4b\u95f4\u4ea7\u751f\u6743\u8861\u3002", "conclusion": "\u5c06\u6df1\u5ea6\u4fe1\u606f\u548c\u7a7a\u95f4\u4fe1\u53f7\u5f15\u5165VLM\u8f93\u5165\uff0c\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6egocentric\u89c6\u9891\u7684\u5bfc\u822a\u548c\u5b89\u5168\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u8f83\u5927\u4f18\u52bf\u3002"}}
{"id": "2601.18791", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18791", "abs": "https://arxiv.org/abs/2601.18791", "authors": ["Iaroslav Chelombitko", "Mika H\u00e4m\u00e4l\u00e4inen", "Aleksey Komissarov"], "title": "Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets", "comment": "15 pages, 4 figues, 4 tables", "summary": "We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Wikipedia\u8bcd\u5e93\uff0c\u5229\u7528BPE\uff08Byte-Pair Encoding\uff09\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86242\u79cd\u62c9\u4e01\u548c\u897f\u91cc\u5c14\u5b57\u6bcd\u8bed\u8a00\u7684\u8bcd\u6c47\u548c\u5f62\u6001\u5b66\u5f02\u540c\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u8de8\u8bed\u8a00\u5206\u6790\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u7cfb\u7edf\u6027\u624b\u6bb5\u5bf9\u591a\u79cd\u8bed\u8a00\u95f4\u7684\u8bcd\u6c47\u548c\u5f62\u6001\u5173\u7cfb\u8fdb\u884c\u7edf\u4e00\u6bd4\u8f83\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u5de5\u5177\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8bcd\u6c47\u5206\u5272\u4e0e\u7f16\u7801\u5c42\u9762\u7814\u7a76\u8bed\u8a00\u4eb2\u7f18\u4e0e\u5f02\u540c\u3002", "method": "\u6784\u5efa\u8de8\u8bed\u79cd\u8bcd\u6c47\u96c6\u5408\uff08glottosets\uff09\uff0c\u5e94\u7528BPE\u5f97\u5230\u5206\u8bcd\u5411\u91cf\uff0c\u901a\u8fc7\u8bcd\u6c47\u8986\u76d6\u5ea6\u3001\u8bcd\u6c47\u5206\u5316\u548c\u8bed\u8a00\u76f8\u4f3c\u6027\u6307\u6807\u8fdb\u884c\u91cf\u5316\u6bd4\u5bf9\uff0c\u5e76\u4e0e\u9057\u4f20\u8bed\u8a00\u4eb2\u7f18\u5173\u7cfb\u8fdb\u884c\u7edf\u8ba1\u76f8\u5173\u6027\u5206\u6790\u3002\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u8a00\u95f4\u540c\u5f62\u5f02\u4e49\u8bcd\uff08homographs\uff09\u7684\u5206\u5272\u5dee\u5f02\u3002", "result": "BPE\u5206\u8bcd\u65b9\u6cd5\u5728\u5206\u754c\u5904\u4e0e\u771f\u5b9e\u8bcd\u7d20\u8fb9\u754c\u7684F1\u663e\u8457\u9ad8\u4e8e\u968f\u673a\uff080.34\u5bf90.15\uff09\uff1bBPE\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u4e0e\u8bed\u8a00\u4eb2\u7f18\u663e\u8457\u76f8\u5173\uff08Mantel r=0.329,p<0.001\uff09\uff0c\u5982\u7f57\u66fc\u8bed\u65cf\u6700\u96c6\u4e2d\uff0c\u8de8\u8bed\u65cf\u8bcd\u6c47\u660e\u663e\u533a\u5206\u3002\u90e8\u5206\u540c\u5f62\u8bcd\u5206\u5272\u663e\u8457\u4e0d\u540c\uff0c\u4e14\u4e0e\u8bed\u8a00\u5206\u5316\u8ddd\u79bb\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u8bed\u8a00\u8bcd\u6c47\u6bd4\u8f83\u548c\u8bed\u8a00\u5171\u6027\u3001\u5dee\u5f02\u7684\u5b9a\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5bf9\u5b8f\u89c2\u8bed\u4e49\u5b66\u4e0e\u8bed\u8a00\u9057\u4f20\u5173\u7cfb\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.18118", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18118", "abs": "https://arxiv.org/abs/2601.18118", "authors": ["Daeyoung Kim"], "title": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment", "comment": null, "summary": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faLungCRCT\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u8868\u793a\u5b66\u4e60\u63d0\u5347\u80ba\u764c\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u5206\u6790\u7684\u6548\u679c\uff0c\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u4e14\u6a21\u578b\u8f7b\u91cf\u5316\u3002", "motivation": "\u7531\u4e8e\u80ba\u764c\u65e9\u671f\u75c7\u72b6\u4e0d\u660e\u663e\u4e14\u4e0e\u5176\u4ed6\u547c\u5438\u75be\u75c5\u75c7\u72b6\u76f8\u4f3c\uff0c\u5bfc\u81f4\u65e9\u671f\u53d1\u73b0\u7387\u4f4e\uff0c\u4ece\u800c\u63d0\u9ad8\u80ba\u764c\u60a3\u8005\u7684\u6b7b\u4ea1\u7387\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u6709\u6548\u7684\u76d1\u6d4b\u548c\u5206\u6790\u65b9\u6cd5\u4ee5\u63d0\u5347\u65e9\u671f\u68c0\u6d4b\u548c\u751f\u5b58\u7387\u3002", "method": "\u63d0\u51fa\u4e86LungCRCT\u6846\u67b6\uff0c\u878d\u5408\u4e86\u57fa\u4e8e\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u7684\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u3001\u8ddd\u79bb\u76f8\u5173\u6027\u89e3\u7ea0\u7f20\u548c\u57fa\u4e8e\u71b5\u7684\u56fe\u50cf\u91cd\u5efa\u4f18\u5316\uff0c\u4ece\u80ba\u764c\u8fdb\u5c55\u7684\u7269\u7406\u56e0\u679c\u673a\u5236\u4e2d\u5b66\u4e60\u6f5c\u5728\u56e0\u679c\u8868\u793a\uff0c\u5e76\u7528\u4e8e\u56e0\u679c\u5e72\u9884\u5206\u6790\u548c\u4e0b\u6e38\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u3002", "result": "LungCRCT\u6846\u67b6\u4e0d\u4ec5\u652f\u6301\u80ba\u764c\u6cbb\u7597\u7684\u56e0\u679c\u5e72\u9884\u5206\u6790\uff0c\u5176\u4e0b\u6e38\u6076\u6027\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e2d\u6a21\u578b\u8868\u73b0\u5f3a\u52b2\u4e14\u6781\u4e3a\u8f7b\u91cf\uff0cAUC\u8fbe\u523093.91%\u3002", "conclusion": "LungCRCT\u901a\u8fc7\u56e0\u679c\u8868\u793a\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u80ba\u764c\u65e9\u671f\u68c0\u6d4b\u4e0e\u6cbb\u7597\u5206\u6790\u7684\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u56e0\u679c\u63a8\u65ad\u548c\u8f7b\u91cf\u5316AI\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18796", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18796", "abs": "https://arxiv.org/abs/2601.18796", "authors": ["Brian Ondov", "Chia-Hsuan Chang", "Yujia Zhou", "Mauro Giuffr\u00e8", "Hua Xu"], "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models", "comment": null, "summary": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5f00\u653e\u6e90\u4ee3\u7801\u3001\u9886\u57df\u65e0\u5173\u7684Embedding Language Model\uff08ELM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4e34\u5e8a\u8bd5\u9a8c\u5d4c\u5165\u5bf9\u9f50\uff0c\u5e76\u5b9e\u73b0\u4e86\u5d4c\u5165\u5230\u6587\u672c\u7684\u53ef\u89e3\u91ca\u4e0e\u751f\u6210\u3002\u6700\u7ec8\u6a21\u578bctELM\u53ef\u4ee5\u4ec5\u901a\u8fc7\u5d4c\u5165\u51c6\u786e\u63cf\u8ff0\u548c\u6bd4\u8f83\u672a\u77e5\u7684\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u8fd8\u80fd\u6839\u636e\u65b0\u7684\u5411\u91cf\u751f\u6210\u5408\u7406\u7684\u4e34\u5e8a\u8bd5\u9a8c\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5d4c\u5165\u5728\u5f88\u591a\u8bed\u8a00\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u89e3\u91ca\u3001\u63a2\u7d22\u53ca\u53cd\u5411\u751f\u6210\u5d4c\u5165\u7a7a\u95f4\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u5e94\u7528\u900f\u660e\u6027\u4e0e\u751f\u6210\u6027\u7528\u4f8b\u5f00\u53d1\u3002\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408LLM\u4e0e\u5d4c\u5165\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e0e\u751f\u6210\u6027\u7684\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u901a\u7528ELM\u67b6\u6784\u53ca\u8bad\u7ec3\u6846\u67b6\uff0c\u8bbe\u8ba1\u9488\u5bf9\u4e34\u5e8a\u8bd5\u9a8c\u7684\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u5206\u522b\u8bad\u7ec3\u4e0d\u540c\u8bbe\u7f6e\u7684ELM\uff0c\u63a2\u7d22\u4efb\u52a1\u7c7b\u578b\u548c\u8bad\u7ec3\u65b9\u5f0f\u5bf9\u6a21\u578b\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6700\u7ec8\u7684ctELM\u6a21\u578b\u80fd\u591f\u4ec5\u4f9d\u636e\u5d4c\u5165\uff0c\u6709\u6548\u5730\u63cf\u8ff0\u548c\u6bd4\u8f83\u672a\u89c1\u7684\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u8fd8\u80fd\u591f\u4ece\u65b0\u5d4c\u5165\u5411\u91cf\u751f\u6210\u5408\u7406\u7684\u4e34\u5e8a\u8bd5\u9a8c\u6587\u672c\u3002\u5b9e\u9a8c\u8fd8\u8bc1\u660e\uff0c\u6a21\u578b\u751f\u6210\u7684\u8bd5\u9a8c\u6458\u8981\u5bf9\u5d4c\u5165\u7684\u8bed\u4e49\u64cd\u4f5c\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u6982\u5ff5\u5411\u91cf\uff09\u6709\u660e\u663e\u54cd\u5e94\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u516c\u5f00\u4e86ELM\u5b9e\u73b0\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u751f\u7269\u533b\u5b66\u53ca\u5176\u5b83\u9886\u57df\u4e2dLLM\u548c\u5d4c\u5165\u7a7a\u95f4\u7684\u5bf9\u9f50\u4e0e\u89e3\u91ca\u6027\u5e94\u7528\u5f00\u53d1\u3002"}}
{"id": "2601.18135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18135", "abs": "https://arxiv.org/abs/2601.18135", "authors": ["Jiahao Lyu", "Minghua Zhao", "Xuewen Huang", "Yifei Chen", "Shuangli Du", "Jing Hu", "Cheng Shi", "Zhiyong Lv"], "title": "Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection", "comment": "It has been submitted to the KBS journal", "summary": "As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u8f7b\u91cf\u7ea7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578bFoGA\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u51c6\u5ea6\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u590d\u6742\u5e9e\u5927\u7684\u6a21\u578b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u5e76\u4e14\u4e00\u822c\u53ea\u5229\u7528\u5355\u5e27\u9884\u6d4b\u8bef\u5dee\uff0c\u5ffd\u89c6\u4e86\u66f4\u957f\u65f6\u95f4\u6bb5\u7684\u4fe1\u606f\u3002", "method": "FoGA\u57fa\u4e8eUnet\u7ed3\u6784\uff0c\u5bf9\u8fde\u7eed\u5e27\u7279\u5f81\u8fdb\u884c\u63d0\u53d6\uff0c\u4ea7\u751f\u5373\u65f6\u4e0e\u524d\u5411\u9884\u6d4b\u3002\u5f15\u5165\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\u6a21\u5757\u4ee5\u5728\u76f8\u540c\u7a7a\u95f4\u5c3a\u5ea6\u4e0b\u52a8\u6001\u878d\u5408\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u524d\u5411\u4e00\u81f4\u6027\u635f\u5931\u548c\u6df7\u5408\u5f02\u5e38\u6d4b\u91cf\u7b56\u7565\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "result": "FoGA\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u7ea62M\uff0c\u80fd\u591f\u4ee5155\u5e27\u6bcf\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u4e0a\u5747\u8d85\u8fc7\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "FoGA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u4f18\u5f02\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528\u4e2d\u3002"}}
{"id": "2601.18157", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18157", "abs": "https://arxiv.org/abs/2601.18157", "authors": ["Aniket Rege", "Arka Sadhu", "Yuliang Li", "Kejie Li", "Ramya Korlakai Vinayak", "Yuning Chai", "Yong Jae Lee", "Hyo Jin Kim"], "title": "Agentic Very Long Video Understanding", "comment": "26 pages, 7 figures, 8 tables", "summary": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f53\u573a\u666f\u56fe\uff08entity scene graphs\uff09\u7684agentic\u67b6\u6784EGAgent\uff0c\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\uff08\u8de8\u5929\u751a\u81f3\u6570\u5468\uff09\u81ea\u4e2d\u5fc3\u89c6\u9891\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002", "motivation": "\u968f\u7740\u5168\u5929\u5019\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u4e2a\u4ebaAI\u52a9\u624b\u7684\u666e\u53ca\uff0cAI\u9700\u7406\u89e3\u6301\u7eed\u800c\u975e\u7247\u6bb5\u7684\u89c6\u9891\u6d41\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u53ea\u652f\u6301\u6709\u9650\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u5904\u7406\u957f\u65f6\u95f4\u3001\u591a\u6b65\u590d\u5408\u63a8\u7406\u3002", "method": "\u63d0\u51faEGAgent\u6846\u67b6\uff1a\u4ee5\u5b9e\u4f53\u573a\u666f\u56fe\u6301\u7eed\u8bb0\u5f55\u89c6\u9891\u4e2d\u7684\u4eba\u7269\u3001\u5730\u70b9\u3001\u7269\u4f53\u53ca\u5173\u7cfb\u53d8\u5316\uff0c\u5f15\u5165\u8ba1\u5212agent\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u68c0\u7d22\u4e0e\u8de8\u6a21\u6001\uff08\u89c6\u89c9\u4e0e\u97f3\u9891\uff09\u63a8\u7406\uff0c\u652f\u6301\u8be6\u7ec6\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u63a8\u7406\u3002", "result": "\u5728EgoLifeQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523057.5%\u7684\u5148\u8fdb\u6c34\u5e73\uff0c\u5728Video-MME(Long)\u6570\u636e\u96c6\u4e0a\u53d6\u5f9774.1%\u7684\u6709\u7ade\u4e89\u529b\u6210\u7ee9\uff0c\u5747\u8d85\u8d8a\u6216\u63a5\u8fd1\u5f53\u524d\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u4f53\u573a\u666f\u56fe+agentic\u89c4\u5212\u4f53\u7cfb\u80fd\u6709\u6548\u652f\u6301\u957f\u65f6\u7a0b\u3001\u591a\u6a21\u6001\u3001\u81ea\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\uff0c\u4e3a\u4e2a\u4ebaAI\u52a9\u624b\u5e26\u6765\u66f4\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u56de\u5fc6\u4e0e\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.18168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18168", "abs": "https://arxiv.org/abs/2601.18168", "authors": ["Zehua Liu", "Shihao Zou", "Jincai Huang", "Yanfang Zhang", "Chao Tong", "Weixin Si"], "title": "TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration", "comment": "Accepted by IEEE BIBM 2025", "summary": "Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\\% lower MSE and 17.7\\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \\textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76842D-3D\u8840\u7ba1\u914d\u51c6\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u809d\u764c\u7b49\u809d\u810f\u6076\u6027\u80bf\u7624TACE\uff08\u7ecf\u52a8\u8109\u5316\u7597\u6813\u585e\u672f\uff09\u7684\u5bfc\u822a\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "TACE\u624b\u672f\u5bf9\u809d\u810f\u80bf\u7624\u7684\u6cbb\u7597\u6548\u679c\u663e\u8457\uff0c\u4f46\u7531\u4e8e\u8840\u7ba1\u7ed3\u6784\u590d\u6742\u3001\u4e2a\u4f53\u89e3\u5256\u53d8\u5f02\u5927\uff0c\u624b\u672f\u4e2d\u7684\u8840\u7ba1\u5bfc\u822a\u9762\u4e34\u6781\u5927\u6311\u6218\u3002\u73b0\u6709\u76842D-3D\u8840\u7ba1\u914d\u51c6\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4ecd\u6709\u8f83\u5927\u53d1\u5c55\u7a7a\u95f4\uff0c\u56e0\u6b64\u4e9f\u5f85\u6539\u5584\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7531\u4e24\u90e8\u5206\u7ec4\u6210\u7684coarse-to-fine\u914d\u51c6\u7b56\u7565\u3002\u7b2c\u4e00\uff0c\u63d0\u51fa\u5168\u5c40\u5bf9\u9f50\u6a21\u5757SA-PnP\uff0c\u5b9e\u73b02D-3D\u8840\u7ba1\u7ed3\u6784\u4e4b\u95f4\u7684\u7ed3\u6784\u611f\u77e5\u5339\u914d\uff1b\u7b2c\u4e8c\uff0c\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86TempDiffReg\uff0c\u57fa\u4e8e\u65f6\u5e8f\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u5e8f\u4e0a\u4e0b\u6587\u5b9e\u73b0\u8840\u7ba1\u53d8\u5f62\u5efa\u6a21\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u89e3\u5256\u53d8\u5316\u4e0e\u5c40\u90e8\u7ed3\u6784\u5dee\u5f02\u3002\u65b9\u6cd5\u572823\u4f8b\u60a3\u8005\u3001\u5171626\u7ec4\u591a\u5e27\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u65b9\u6cd5\u5728\u914d\u51c6\u51c6\u786e\u5ea6\u548c\u89e3\u5256\u5408\u7406\u6027\u65b9\u9762\u5747\u663e\u8457\u63d0\u5347\u3002\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u4e3a0.63mm\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a0.51mm\uff0c\u5206\u522b\u6bd4\u6700\u4f18\u79c0\u7684\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e\u4e8666.7%\u548c17.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aTACE\u7b49\u590d\u6742\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u4e3a\u51c6\u786e\u3001\u7a33\u5b9a\u7684\u8840\u7ba1\u914d\u51c6\u5de5\u5177\uff0c\u6709\u671b\u5e2e\u52a9\u7ecf\u9a8c\u4e0d\u8db3\u7684\u4e34\u5e8a\u533b\u751f\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u5730\u5b8c\u6210\u624b\u672f\uff0c\u6539\u5584\u4e34\u5e8a\u7597\u6548\u4e0e\u60a3\u8005\u9884\u540e\uff0c\u5176\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.18172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18172", "abs": "https://arxiv.org/abs/2601.18172", "authors": ["Lin Huang", "Yujuan Tan", "Weisheng Li", "Shitai Shan", "Liu Liu", "Bo Liu", "Linlin Shen", "Jing Yu", "Yue Niu"], "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection", "comment": null, "summary": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86YOLO-DS\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u7edf\u8ba1\u534f\u540c\u7b97\u5b50\uff08DSO\uff09\u53ca\u76f8\u5173\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u5668\u5bf9\u5f02\u8d28\u76ee\u6807\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709YOLO\u76ee\u6807\u68c0\u6d4b\u5668\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u5171\u4eab\u901a\u9053\u4e2d\u5f02\u8d28\u76ee\u6807\u7684\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u7279\u5f81\u533a\u5206\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Dual-Statistic Synergy Operator\uff08DSO\uff09\uff0c\u5bf9\u7279\u5f81\u7684\u5747\u503c\u548c\u5cf0\u503c-\u5747\u503c\u5dee\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86Dual-Statistic Synergy Gating\uff08DSG\uff09\u6a21\u5757\u548cMulti-Path Segmented Gating\uff08MSG\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u901a\u9053\u9009\u62e9\u548c\u6df1\u5ea6\u52a0\u6743\u3002", "result": "\u5728MS-COCO\u6570\u636e\u96c6\u4e0a\uff0cYOLO-DS\u5728\u4e94\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\uff08N, S, M, L, X\uff09AP\u63d0\u53471.1%-1.7%\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ec5\u7a0d\u5fae\u589e\u52a0\u3002", "conclusion": "YOLO-DS\u80fd\u9ad8\u6548\u533a\u5206\u5f02\u8d28\u76ee\u6807\uff0c\u517c\u987e\u68c0\u6d4b\u7cbe\u5ea6\u548c\u901f\u5ea6\uff0c\u6548\u679c\u4f18\u4e8eYOLOv8\u3002"}}
{"id": "2601.18188", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18188", "abs": "https://arxiv.org/abs/2601.18188", "authors": ["Weiye Zhu", "Zekai Zhang", "Xiangchen Wang", "Hewei Pan", "Teng Wang", "Tiantian Geng", "Rongtao Xu", "Feng Zheng"], "title": "\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation", "comment": "18 pages, 14 figures", "summary": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NaVIDA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u52a8\u4f5c\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u7b56\u7565\u6267\u884c\uff0c\u6709\u6548\u63d0\u5347\u4e86Vision-and-Language Navigation (VLN)\u4efb\u52a1\u4e2d\u667a\u80fd\u4f53\u7684\u5bfc\u822a\u8868\u73b0\u3002", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u5927\u591a\u76f4\u63a5\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u6620\u5c04\u5230\u52a8\u4f5c\uff0c\u672a\u663e\u5f0f\u5efa\u6a21\u52a8\u4f5c\u5bf9\u89c6\u89c9\u53d8\u5316\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u5bfc\u81f4\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u9884\u6d4b\u81ea\u5df1\u52a8\u4f5c\u5e26\u6765\u7684\u89c6\u89c9\u540e\u679c\uff0c\u5bb9\u6613\u79ef\u7d2f\u8bef\u5dee\u5e76\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faNaVIDA\u6846\u67b6\uff0c\u5c06\u7b56\u7565\u5b66\u4e60\u4e0e\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u89c9\u52a8\u6001\u5efa\u6a21\u7ed3\u5408\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u7528\u5757\u72b6\u9006\u52a8\u529b\u5b66\u76d1\u7763\u5b66\u4e60\u89c6\u89c9\u53d8\u5316\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u6982\u7387\u52a8\u4f5c\u5206\u5757\uff08HPAC\uff09\u7ed3\u6784\u5316\u76d1\u7763\u548c\u6269\u5c55\u89c4\u5212\u8303\u56f4\u3002\u540c\u65f6\uff0c\u5f15\u5165\u71b5\u5f15\u5bfc\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u81ea\u9002\u5e94\u5730\u8bbe\u7f6e\u52a8\u4f5c\u5757\u7684\u6267\u884c\u8303\u56f4\uff0c\u4ee5\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\uff0cNaVIDA\u5728\u53c2\u6570\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\uff083B vs 8B\uff09\u5bfc\u822a\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u52a8\u4f5c\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u3001\u5206\u5757\u5206\u5c42\u7b56\u7565\u548c\u81ea\u9002\u5e94\u52a8\u4f5c\u6267\u884c\u673a\u5236\uff0cNaVIDA\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728VLN\u4efb\u52a1\u4e0a\u7684\u5bfc\u822a\u6548\u679c\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u548c\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2601.18190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18190", "abs": "https://arxiv.org/abs/2601.18190", "authors": ["Yifan Li", "Shiying Wang", "Jianqiang Huang"], "title": "Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval", "comment": "7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP", "summary": "Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MPS-CLIP\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u9065\u611f\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u7684\u9ad8\u6548\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\uff0c\u83b7\u5f97\u4e86\u663e\u8457\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u9065\u611f\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u5168\u5c40\u5bf9\u9f50\uff0c\u96be\u4ee5\u6355\u6349\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u591a\u5c3a\u5ea6\u3001\u5bc6\u96c6\u8bed\u4e49\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5fae\u8c03\u73b0\u6709\u5927\u6a21\u578b\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u4e14\u6613\u9057\u5fd8\u539f\u6709\u80fd\u529b\u3002\u56e0\u6b64\u8feb\u5207\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7ec6\u81f4\u7684\u56fe\u6587\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "1\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u62bd\u53d6\u6587\u672c\u6838\u5fc3\u8bed\u4e49\u5173\u952e\u8bcd\uff0c\u5f15\u5bfcSegment Anything Model\uff08SamGeo\uff09\u5206\u5272\u51fa\u56fe\u50cf\u7684\u76f8\u5173\u8bed\u4e49\u533a\u57df\uff1b2\uff09\u63d0\u51faG^2A\u9002\u914d\u5668\uff0c\u4f4e\u6210\u672c\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0c\u5e76\u4ee5\u51bb\u7ed3\u9aa8\u5e72\u65b9\u5f0f\u9ad8\u6548\u9002\u914d\uff1b3\uff09\u901a\u8fc7\u591a\u89c6\u89d2\u8868\u793a\u6a21\u5757\uff08MPR\uff09\u6574\u5408\u5c40\u90e8\u7ebf\u7d22\u5f97\u5230\u9c81\u68d2\u591a\u89c6\u89d2\u7279\u5f81\uff1b4\uff09\u7ed3\u5408\u591a\u89c6\u89d2\u5bf9\u6bd4\u635f\u5931\u548c\u52a0\u6743\u4e09\u5143\u7ec4\u635f\u5931\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728RSICD\u548cRSITMD\u4e24\u4e2a\u9065\u611f\u68c0\u7d22\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMPS-CLIP\u5206\u522b\u53d6\u5f97\u4e8635.18%\u548c48.40%\u7684\u5e73\u5747\u53ec\u56de\u7387\uff08mR\uff09\uff0c\u5747\u5927\u5e45\u8d85\u8d8a\u5168\u91cf\u5fae\u8c03\u6a21\u578b\u53ca\u73b0\u6709\u6700\u65b0\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "MPS-CLIP\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u3001\u5173\u952e\u8bcd\u5f15\u5bfc\u7684\u7ec6\u7c92\u5ea6\u591a\u89c6\u89d2\u56fe\u6587\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u9065\u611f\u56fe\u6587\u68c0\u7d22\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2601.18192", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18192", "abs": "https://arxiv.org/abs/2601.18192", "authors": ["Tian-Yi Zhou", "Xuan-Hao Liu", "Bao-Liang Lu", "Wei-Long Zheng"], "title": "MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models", "comment": null, "summary": "Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MindCine\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8111\u7535\uff08EEG\uff09\u4fe1\u53f7\u5230\u89c6\u9891\u91cd\u5efa\u7684\u6548\u679c\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "EEG\u4fe1\u53f7\u5bf9\u4eba\u7c7b\u52a8\u6001\u89c6\u89c9\u611f\u77e5\u7684\u91cd\u5efa\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u4e0e\u6587\u672c\u5bf9\u9f50\u3001\u6613\u8fc7\u62df\u5408\uff0c\u4e14\u53d7\u9650\u4e8eEEG-\u89c6\u9891\u6570\u636e\u532e\u4e4f\uff0c\u8bad\u7ec3\u96be\u4ee5\u6536\u655b\u3002", "method": "\u63d0\u51faMindCine\u6846\u67b6\uff0c\u91c7\u7528\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u6587\u672c\u4ee5\u5916\u7684\u6a21\u6001\u5f15\u5165\u8bad\u7ec3\u9636\u6bb5\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u89c4\u6a21EEG\u6a21\u578b\u7f13\u89e3\u6570\u636e\u532e\u4e4f\uff0c\u4f7f\u7528\u5e26\u56e0\u679c\u6ce8\u610f\u529b\u7684Seq2Seq\u6a21\u578b\u5206\u522b\u89e3\u7801\u8bed\u4e49\u548c\u611f\u77e5\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u591a\u6a21\u6001\u4fe1\u606f\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3EEG\u6a21\u578b\u76f8\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3EEG\u6a21\u578b\u80fd\u591f\u7a81\u7834\u5355\u4e00\u6a21\u6001\u548c\u6570\u636e\u6709\u9650\u7684\u74f6\u9888\uff0c\u4e3aEEG\u5230\u89c6\u9891\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18195", "abs": "https://arxiv.org/abs/2601.18195", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Kaiwei Zhang", "Jun Jia", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding", "comment": null, "summary": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6846\u67b6QualiRAG\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5229\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u4e3a\u7ec6\u81f4\u548c\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u4e14\u6613\u53d7\u6570\u636e\u96c6\u504f\u89c1\u5f71\u54cd\uff0c\u96be\u4ee5\u6ee1\u8db3\u5bf9\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9700\u6c42\u3002", "method": "QualiRAG\u6846\u67b6\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u52a8\u6001\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u8bf7\u6c42\uff0c\u5e76\u6784\u5efa\u56db\u79cd\u77e5\u8bc6\u6e90\uff08\u89c6\u89c9\u5143\u6570\u636e\u3001\u4e3b\u4f53\u5b9a\u4f4d\u3001\u5168\u5c40\u8d28\u91cf\u6458\u8981\u3001\u672c\u5730\u8d28\u91cf\u63cf\u8ff0\uff09\uff0c\u7136\u540e\u6839\u636e\u76f8\u5173\u6027\u68c0\u7d22\u8f85\u52a9\u4fe1\u606f\uff0c\u4e3a\u63a8\u7406\u51b3\u7b56\u63d0\u4f9b\u8bc1\u636e\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQualiRAG\u5728\u89c6\u89c9\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u672a\u4e13\u95e8\u5fae\u8c03\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u7ecf\u8fc7VQA\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u6bd4\u8f83\u4efb\u52a1\u4e0a\u4e5f\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "conclusion": "QualiRAG\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e3a\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u65e0\u76d1\u7763\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18222", "abs": "https://arxiv.org/abs/2601.18222", "authors": ["Mengfan He", "Liangzheng Sun", "Chunyu Li", "Ziyang Meng"], "title": "HomoFM: Deep Homography Estimation with Flow Matching", "comment": null, "summary": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540c\u4f26\u53d8\u6362\u4f30\u8ba1\u7b97\u6cd5HomoFM\uff0c\u521b\u65b0\u6027\u5730\u5c06\u6d41\u5339\u914d\u6280\u672f\u5f15\u5165\u540c\u4f26\u4f30\u8ba1\u4efb\u52a1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u5bf9\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u540c\u4f26\u4f30\u8ba1\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u76f4\u63a5\u56de\u5f52\u6216\u8fed\u4ee3\u4f18\u5316\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u51e0\u4f55\u53d8\u6362\uff0c\u4e14\u5bf9\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6HomoFM\uff0c\u5c06\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u6d41\u5339\u914d(flow matching)\u65b9\u6cd5\u9996\u6b21\u5f15\u5165\u540c\u4f26\u53d8\u6362\u4f30\u8ba1\u95ee\u9898\u3002\u65b9\u6cd5\u5c06\u540c\u4f26\u4f30\u8ba1\u5efa\u6a21\u4e3a\u901f\u5ea6\u573a\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u5b66\u4e60\u8fde\u7eed\u7684\u70b9\u5bf9\u70b9\u901f\u5ea6\u573a\uff0c\u5b9e\u73b0\u4ece\u566a\u58f0\u5206\u5e03\u5230\u6807\u51c6\u5750\u6807\u7684\u9ad8\u7cbe\u5ea6\u53d8\u6362\u3002\u540c\u65f6\uff0c\u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42(GRL)\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff0c\u4f7f\u7f16\u7801\u5668\u83b7\u5f97\u9886\u57df\u4e0d\u53d8\u7684\u7279\u5f81\uff0c\u63d0\u9ad8\u5bf9\u591a\u6a21\u6001\u5339\u914d\u6216\u5149\u7167\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aHomoFM\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "HomoFM\u901a\u8fc7\u6d41\u5339\u914d\u4e0e\u9886\u57df\u81ea\u9002\u5e94\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u540c\u4f26\u4f30\u8ba1\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u5176\u5728\u591a\u9886\u57df\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u80fd\u529b\u3002"}}
{"id": "2601.18228", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18228", "abs": "https://arxiv.org/abs/2601.18228", "authors": ["Sahil Naik", "Soham Bagayatkar", "Pavankumar Singh"], "title": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach", "comment": "6 pages, 4 figures", "summary": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728\u4f4e\u8d28\u91cf\u3001\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u91cf\u663e\u8457\u4f4e\u4e8e\u4e3b\u6d41\u5927\u6a21\u578b\uff0c\u4e14\u51c6\u786e\u7387\u53ef\u89c2\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\u53d7\u56fe\u50cf\u8d28\u91cf\u3001\u566a\u58f0\u6807\u6ce8\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u4f20\u7edfVGG/ResNet\u7b49\u5927\u6a21\u578b\u867d\u51c6\u786e\u4f46\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\uff0c\u4e9f\u9700\u9ad8\u6548\u8f7b\u91cf\u65b9\u6848\u3002", "method": "\u57fa\u4e8eEfficientNetB2\uff0c\u91c7\u7528\u4e24\u9636\u6bb5warm-up\u548c\u5fae\u8c03\u8bad\u7ec3\u7b56\u7565\uff0c\u5f15\u5165AdamW\u4f18\u5316\u5668\u3001\u6807\u7b7e\u5e73\u6ed1\u3001\u7c7b\u6743\u91cd\u88c1\u526a\u3001dropout\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u7528\u5206\u5c42\u6570\u636e\u5212\u5206\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728FER-2013\u6570\u636e\u96c6\u4e0a\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523068.78%\uff0c\u53c2\u6570\u91cf\u7ea6\u4e3aVGG16\u7684\u5341\u5206\u4e4b\u4e00\uff0c\u5404\u7c7b\u6307\u6807\u548c\u5b66\u4e60\u66f2\u7ebf\u8868\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u548c\u7a33\u5b9a\u6027\uff0c\u8bad\u7ec3\u9ad8\u6548\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u517c\u5177\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u975e\u5e38\u9002\u5408\u5bf9\u8d44\u6e90\u654f\u611f\u7684\u5b9e\u65f6\u6216\u8fb9\u7f18\u8bbe\u5907\u573a\u666f\u4e0b\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2601.18240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18240", "abs": "https://arxiv.org/abs/2601.18240", "authors": ["Mengyuan Jin", "Zehui Liao", "Yong Xia"], "title": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86V-Loop\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u903b\u8f91\u95ed\u73af\u68c0\u6d4b\u533b\u5b66\u591a\u6a21\u6001\u5927\u6a21\u578b\u56de\u7b54\u4e2d\u7684\u5e7b\u89c9\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u9886\u57df\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u662f\u5176\u56de\u7b54\u4ecd\u5bb9\u6613\u51fa\u73b0\u4e0e\u4e8b\u5b9e\u4e0d\u7b26\u7684\u5e7b\u89c9\u7b54\u6848\uff0c\u8fd9\u5728\u533b\u7597\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u6781\u5177\u5371\u5bb3\u3002\u73b0\u6709\u4e3b\u8981\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u65e0\u6cd5\u76f4\u63a5\u9a8c\u8bc1\u56de\u7b54\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u7684\u68c0\u6d4b\u601d\u8def\u3002", "method": "V-Loop\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u63d2\u5165\u73b0\u6709\u6d41\u7a0b\u3002\u5176\u6838\u5fc3\u662f\u63d0\u51fa\u4e00\u79cd\u89c6\u89c9\u903b\u8f91\u95ed\u73af\uff1a\u9996\u5148\u7531\u591a\u6a21\u6001\u5927\u6a21\u578b\u751f\u6210\u7b54\u6848\uff0c\u968f\u540e\u5206\u6790\u95ee\u7b54\u5bf9\u7684\u8bed\u4e49\u5355\u5143\uff0c\u5bf9\u7b54\u6848\u76f8\u5173\u5355\u5143\u53cd\u5411\u63d0\u95ee\u5e76\u5229\u7528\u89c6\u89c9\u6ce8\u610f\u529b\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u9a8c\u8bc1\u73af\u662f\u5426\u95ed\u5408\uff08\u5373\u9a8c\u8bc1\u7b54\u6848\u8bed\u4e49\u5185\u5bb9\u7684\u4e00\u81f4\u6027\uff09\uff0c\u5224\u65ad\u8be5\u56de\u7b54\u662f\u5426\u4e8b\u5b9e\u624e\u5b9e\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u548c\u4e0d\u540c\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\uff0cV-Loop\u68c0\u6d4b\u5e7b\u89c9\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002\u5f53\u4e0e\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u7ed3\u5408\u65f6\uff0c\u68c0\u6d4b\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "V-Loop\u4e3a\u533b\u5b66\u591a\u6a21\u6001\u5927\u6a21\u578b\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8bad\u7ec3\u65e0\u5173\u4e14\u8868\u73b0\u5353\u8d8a\u7684\u65b0\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u533b\u5b66AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.18242", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18242", "abs": "https://arxiv.org/abs/2601.18242", "authors": ["Zerui Kang", "Yishen Lim", "Zhouyou Gu", "Seung-Woo Ko", "Tony Q. S. Quek", "Jihong Park"], "title": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation", "comment": null, "summary": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8f85\u52a9\u7684\u5c04\u7ebf\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u52a0\u901f\u5e76\u7a33\u5b9a\u591a\u6750\u6599\u53c2\u6570\u4f30\u8ba1\uff0c\u5927\u5e45\u63d0\u5347\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u6536\u655b\u65f6\u95f4\u3002", "motivation": "\u57286G\u7cfb\u7edf\u7535\u78c1\u4eff\u771f\u4e2d\uff0c\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u5c04\u9891\u6750\u6599\u53c2\u6570\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u53cd\u6f14\u5c04\u7ebf\u8ffd\u8e2a\u65e2\u5bf9\u521d\u503c\u654f\u611f\u53c8\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u6d4b\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4ee5VLM\u4e3a\u6838\u5fc3\u7684\u53c2\u6570\u4f30\u8ba1\u7b97\u6cd5\u3002\u9996\u5148\uff0cVLM\u5bf9\u573a\u666f\u56fe\u50cf\u8fdb\u884c\u89e3\u6790\uff0c\u8bc6\u522b\u6750\u6599\u7c7b\u522b\u5e76\u636eITU-R\u8868\u7ed9\u51fa\u5408\u7406\u7684\u7535\u5bfc\u7387\u521d\u59cb\u503c\u3002VLM\u8fd8\u8f85\u52a9\u9009\u62e9\u6709\u4fe1\u606f\u91cf\u7684\u6536\u53d1\u673a\u4f4d\u7f6e\uff0c\u4f18\u5316\u8def\u5f84\u7684\u6750\u8d28\u533a\u5206\u80fd\u529b\u3002\u6b64\u540e\u7528\u53ef\u5fae\u5c04\u7ebf\u8ffd\u8e2a\uff08DRT\uff09\u7ed3\u5408\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u6d4b\u91cf\uff0c\u57fa\u4e8e\u68af\u5ea6\u8fdb\u4e00\u6b65\u4f18\u5316\u6750\u6599\u53c2\u6570\u3002", "result": "\u5728NVIDIA Sionna\u5e73\u53f0\u5ba4\u5185\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u53472-4\u500d\uff0c\u6700\u7ec8\u53c2\u6570\u8bef\u5dee\u6bd4\u968f\u673a\u6216\u5747\u5300\u521d\u59cb\u5316\u4f4e10-100\u500d\uff0c\u4ec5\u9700\u5c11\u6570\u63a5\u6536\u5668\u5373\u53ef\u8fbe\u5230\u4f4e\u4e8e0.1%\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u3002\u7b97\u6cd5\u590d\u6742\u5ea6\u968f\u6750\u6599\u3001\u6d4b\u91cf\u6570\u91cf\u8fd1\u7ebf\u6027\u589e\u52a0\uff0cVLM\u4f18\u5316\u964d\u4f4e\u4e86\u6240\u9700\u6d4b\u91cf\u6b21\u6570\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u63d0\u5347\u5c04\u7ebf\u6df1\u5ea6\u548c\u6570\u91cf\u80fd\u5e26\u6765\u989d\u5916\u7cbe\u5ea6\u63d0\u5347\uff0c\u4e14\u4e0d\u4f1a\u589e\u52a0\u5355\u6b21\u8fed\u4ee3\u7684\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8eVLM\u7684\u8bed\u4e49\u5148\u9a8c\u80fd\u6709\u6548\u6307\u5bfc\u7269\u7406\u4f18\u5316\uff0c\u5927\u5e45\u63d0\u5347\u5c04\u9891\u6750\u6599\u53c2\u6570\u4f30\u8ba1\u7684\u901f\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4e3a6G\u7535\u78c1\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2601.18250", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18250", "abs": "https://arxiv.org/abs/2601.18250", "authors": ["Kang Yu", "Dingyu Wang", "Zimu Yuan", "Nan Zhou", "Jiajun Liu", "Jiaxin Liu", "Shanggui Liu", "Yaoyan Zheng", "Huishu Yuan", "Di Huang", "Dong Jiang"], "title": "A multimodal vision foundation model for generalizable knee pathology", "comment": null, "summary": "Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86OrthoFoundation\uff0c\u4e00\u79cd\u9762\u5411\u808c\u8089\u9aa8\u9abc\u5f71\u50cf\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u5177\u5907\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u9aa8\u79d1\u533b\u5b66\u5f71\u50cf\u7684AI\u4e3b\u8981\u4f9d\u8d56\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5b58\u5728\u4efb\u52a1\u5272\u88c2\uff0c\u6570\u636e\u6807\u6ce8\u91cf\u5927\u4e14\u6cdb\u5316\u6027\u5dee\u7b49\u95ee\u9898\u3002\u540c\u65f6\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u5f00\u653e\u808c\u8089\u9aa8\u9abc\u6570\u636e\u96c6\u9650\u5236\u4e86\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u8bca\u65ad\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b120\u4e07\u5f20\u819d\u5173\u8282X\u5149\u548cMRI\u5f71\u50cf\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5229\u7528Dinov3\u4e3b\u5e72\u7f51\u7edc\u548c\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u63d0\u53d6\u5f71\u50cf\u8868\u5f81\u3002\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "OrthoFoundation\u572814\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97SOTA\u8868\u73b0\uff0c\u5728X\u5149\u9aa8\u5173\u8282\u708e\u8bca\u65ad\u548cMRI\u7ed3\u6784\u635f\u4f24\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7cbe\u5ea6\u6700\u9ad8\uff0c\u4ec5\u752850%\u6807\u6ce8\u6837\u672c\u5373\u53ef\u8fbe\u5230\u6709\u76d1\u7763\u57fa\u7ebf\u3002\u540c\u65f6\uff0c\u867d\u4ec5\u5728\u819d\u90e8\u9884\u8bad\u7ec3\uff0c\u4f46\u5bf9\u9acb\u3001\u80a9\u3001\u8e1d\u7b49\u90e8\u4f4d\u4e5f\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OrthoFoundation\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u901a\u7528\u808c\u8089\u9aa8\u9abc\u5f71\u50cfAI\u6a21\u578b\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u6807\u6ce8\u8d1f\u62c5\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4e3a\u8be5\u9886\u57dfAI\u6280\u672f\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.18252", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18252", "abs": "https://arxiv.org/abs/2601.18252", "authors": ["Chao Wang", "Xuanying Li", "Cheng Dai", "Jinglei Feng", "Yuxiang Luo", "Yuqi Ouyang", "Hao Qin"], "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing", "comment": null, "summary": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Co-PLNet\uff0c\u4e00\u79cd\u70b9-\u7ebf\u534f\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u4fc3\u8fdb\u7ebf\u6bb5\u4e0e\u4ea4\u70b9\u7684\u534f\u540c\u89e3\u6790\uff0c\u63d0\u5347\u7ebf\u6846\u89e3\u6790\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7ebf\u6846\u89e3\u6790\u65b9\u6cd5\u5927\u591a\u5c06\u7ebf\u6bb5\u4e0e\u4ea4\u70b9\u5206\u5f00\u9884\u6d4b\uff0c\u6700\u540e\u518d\u8fdb\u884c\u5339\u914d\u548c\u878d\u5408\uff0c\u8fd9\u6613\u5bfc\u81f4\u4e0d\u4e00\u81f4\u53ca\u8bc6\u522b\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u540e\u7eed\u5982SLAM\u7b49\u4efb\u52a1\u7684\u8868\u73b0\u3002\u4e3a\u63d0\u5347\u7ed3\u6784\u5316\u51e0\u4f55\u89e3\u6790\u7684\u51c6\u786e\u6027\u4e0e\u7a33\u5065\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u70b9-\u7ebf\u534f\u540c\u6548\u5e94\u5f3a\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Co-PLNet\u6846\u67b6\u3002\u9996\u5148\u5229\u7528PLP-Encoder\u5c06\u65e9\u671f\u68c0\u6d4b\u51fa\u7684\u70b9\u548c\u7ebf\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u3001\u7a7a\u95f4\u5bf9\u9f50\u7684\u63d0\u793a\uff08prompts\uff09\uff0c\u5bf9\u5176\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002\u968f\u540e\u7528CGL-Decoder\u5229\u7528\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u4e92\u8865\u63d0\u793a\u8fdb\u884c\u7ebf\u6bb5\u548c\u4ea4\u70b9\u7684\u8054\u5408\u63a8\u7406\uff0c\u5b9e\u73b0\u70b9-\u7ebf\u4fe1\u606f\u7684\u53cc\u5411\u7ea6\u675f\u4e0e\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728Wireframe\u548cYorkUrban\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u6d4b\uff0cCo-PLNet\u5728\u51c6\u786e\u7387\u3001\u9c81\u68d2\u6027\u7b49\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u5177\u5907\u8f83\u597d\u7684\u5b9e\u65f6\u6027\u3002", "conclusion": "Co-PLNet\u6709\u6548\u6574\u5408\u5e76\u63d0\u5347\u4e86\u7ebf\u6bb5\u4e0e\u4ea4\u70b9\u7684\u534f\u540c\u89e3\u6790\u80fd\u529b\uff0c\u4e3a\u7ed3\u6784\u5316\u51e0\u4f55\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u5907\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18260", "abs": "https://arxiv.org/abs/2601.18260", "authors": ["Eytan Kats", "Kai Geissler", "Daniel Mensing", "Jochen G. Hirsch", "Stefan Heldman", "Mattias P. Heinrich"], "title": "Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images", "comment": "preprint", "summary": "Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-D\u76f8\u673a\u83b7\u53d6\u6df1\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6846\u67b6\u76f4\u63a5\u4ece\u5355\u5f20\u4e8c\u7ef4\u4f53\u8868\u6df1\u5ea6\u56fe\u50cf\u9884\u6d4b\u4f53\u5185\u591a\u4e2a\u5668\u5b98\u7684\u4e09\u7ef4\u4f4d\u7f6e\u548c\u5f62\u72b6\uff0c\u5b9e\u73b0\u81ea\u52a8\u60a3\u8005\u6446\u4f4d\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u60a3\u8005\u6446\u4f4d\u5bf9\u4e8e\u4f18\u5316\u626b\u63cf\u6d41\u7a0b\u4e0e\u63d0\u5347\u75c5\u4eba\u901a\u91cf\u975e\u5e38\u5173\u952e\uff0c\u800c\u5982\u4f55\u51c6\u786e\u3001\u9ad8\u6548\u5730\u4f30\u7b97\u4f53\u5185\u5668\u5b98\u4f4d\u7f6e\u4ecd\u662f\u96be\u9898\u3002\u4f20\u7edf\u65b9\u5f0f\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u6216\u8868\u9762\u91cd\u5efa\u6548\u7387\u4f4e\uff0c\u4f5c\u8005\u5e0c\u671b\u501f\u52a9\u6df1\u5ea6\u56fe\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u51c6\u5ea6\u53ca\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4ee5\u5168\u8eabMRI\u914d\u5bf9\u6df1\u5ea6\u56fe\u548c\u89e3\u5256\u5206\u5272\u4e3a\u57fa\u7840\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4ece\u5355\u5f20\u4f53\u8868\u6df1\u5ea6\u56fe\u50cf\u76f4\u63a5\u56de\u5f52\u9884\u6d4b\u591a\u4e2a\u5185\u810f\u5668\u5b98\u7684\u4e09\u7ef4\u4f4d\u7f6e\u4e0e\u5f62\u72b6\uff0c\u65e0\u9700\u8868\u9762\u91cd\u5efa\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u7cbe\u51c6\u5b9a\u4f4d\u5305\u62ec\u9aa8\u9abc\u548c\u8f6f\u7ec4\u7ec7\u5728\u5185\u7684\u591a\u79cd\u4f53\u5185\u89e3\u5256\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u51c6\u786e\u6709\u6548\uff0c\u5177\u5907\u5728\u4e34\u5e8a\u653e\u5c04\u79d1\u81ea\u52a8\u5316\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u4e0e\u5b66\u4e60\u6846\u67b6\u53ef\u63d0\u5347\u81ea\u52a8\u60a3\u8005\u6446\u4f4d\u7cbe\u51c6\u5ea6\u548c\u6548\u7387\uff0c\u6709\u671b\u7b80\u5316\u653e\u5c04\u79d1\u626b\u63cf\u6d41\u7a0b\uff0c\u4f18\u5316\u60a3\u8005\u4f53\u9a8c\u3002"}}
{"id": "2601.18263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18263", "abs": "https://arxiv.org/abs/2601.18263", "authors": ["Subhajeet Das", "Susmita Ghosh", "Abhiroop Chatterjee"], "title": "Revisiting Aerial Scene Classification on the AID Benchmark", "comment": "Presented at the IEEE India Geoscience and Remote Sensing Symposium 2025 and accepted for publication in IEEE Xplore", "summary": "Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u822a\u62cd\u5f71\u50cf\u5206\u7c7b\u7684\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7a7a\u95f4\u6ce8\u610f\u529b\u591a\u5c3a\u5ea6\u878d\u5408\u7684CNN\u6a21\u578b\uff0c\u5728AID\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u822a\u62cd\u56fe\u50cf\u5177\u6709\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u5305\u542b\u591a\u79cd\u7ed3\u6784\u4e0e\u573a\u666f\uff0c\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u4e0e\u73af\u5883\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u573a\u666f\u590d\u6742\u4e14\u7c7b\u522b\u591a\u6837\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5206\u7c7b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4ecd\u6709\u6311\u6218\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u68b3\u7406\u76f8\u5173\u65b9\u6cd5\u5e76\u63d0\u51fa\u66f4\u6709\u6548\u7684\u6a21\u578b\u3002", "method": "\u9996\u5148\u7cfb\u7edf\u8bc4\u8ff0\u4e86\u4ece\u624b\u5de5\u7279\u5f81\uff08\u5982SIFT\u3001LBP\uff09\u3001\u4f20\u7edfCNN\uff08\u5982VGG\u3001GoogLeNet\uff09\u5230\u6df1\u5ea6\u6df7\u5408\u7f51\u7edc\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u968f\u540e\u8bbe\u8ba1\u4e86Aerial-Y-Net\u6a21\u578b\uff0c\u7ed3\u5408\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u52a0\u5f3a\u6a21\u578b\u5bf9\u590d\u6742\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728AID\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684Aerial-Y-Net\u6a21\u578b\u8fbe\u5230\u4e8691.72%\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u591a\u79cd\u4e3b\u6d41\u57fa\u51c6\u67b6\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684Aerial-Y-Net\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u822a\u62cd\u56fe\u50cf\u591a\u6837\u6027\u4e0e\u590d\u6742\u6027\uff0c\u5728\u573a\u666f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u8868\u660e\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u4e8e\u63d0\u5347\u822a\u62cd\u56fe\u50cf\u7406\u89e3\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2601.18301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18301", "abs": "https://arxiv.org/abs/2601.18301", "authors": ["Seyedali Mousavi", "Seyedhamidreza Mousavi", "Masoud Daneshtalab"], "title": "Contextual Range-View Projection for 3D LiDAR Point Clouds", "comment": null, "summary": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb3D\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6295\u5f71\u52302D\u89c6\u56fe\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u591a\u70b9\u6295\u5f71\u51b2\u7a81\uff0c\u63d0\u9ad8\u5206\u5272\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5c063D\u70b9\u4e91\u6295\u5f71\u52302D\u56fe\u7247\u7684\u65b9\u6cd5\u591a\u91c7\u7528\u4fdd\u7559\u6700\u5c0f\u6df1\u5ea6\u70b9\u7684\u7b56\u7565, \u5ffd\u89c6\u4e86\u70b9\u7684\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7269\u4f53\u7ed3\u6784\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u673a\u5236\uff1a1) Centerness-Aware Projection (CAP)\uff0c\u6309\u70b9\u5230\u5b9e\u4f8b\u4e2d\u5fc3\u7684\u8ddd\u79bb\u8c03\u6574\u6df1\u5ea6\uff0c\u4f18\u5148\u4fdd\u7559\u9760\u8fd1\u4e2d\u5fc3\u7684\u70b9\uff0c\u51cf\u5c11\u8fb9\u754c\u566a\u58f0\u70b9\uff1b2) Class-Weighted-Aware Projection (CWAP)\uff0c\u5bf9\u4e0d\u540c\u7c7b\u522b\u7ed9\u4e88\u81ea\u5b9a\u4e49\u6743\u91cd\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u70b9\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\uff0cCAP\u65b9\u6cd5\u80fd\u5728\u6295\u5f71\u65f6\u4fdd\u7559\u66f4\u591a\u5b9e\u4f8b\u4e2d\u5fc3\u70b9\uff0cIoU\u6700\u5927\u63d0\u53473.1%\uff1bCWAP\u5bf9\u76ee\u6807\u7c7b\u522b\u7684\u8868\u73b0\u6709\u63d0\u5347\uff0c\u5bf9\u5176\u4ed6\u7c7b\u522b\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u5f15\u5165\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u7684\u6295\u5f71\u9009\u62e9\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u591a\u70b9\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u3002"}}
{"id": "2601.18305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18305", "abs": "https://arxiv.org/abs/2601.18305", "authors": ["Xuan Wang", "Siyuan Su", "Quantong Fu", "Yongxiang Hu", "Yangfan Zhou"], "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis", "comment": "15 pages, 3 figures. Under review. Code and dataset will be released upon acceptance", "summary": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u7ba1\u9053SwipeGen\uff0c\u7528\u4e8e\u5408\u6210\u66f4\u8d34\u8fd1\u4eba\u7c7b\u7684\u6ed1\u52a8\u4ea4\u4e92\uff0c\u5e76\u636e\u6b64\u9996\u6b21\u6784\u5efa\u4e86GUI\u4ee3\u7406\u6ed1\u52a8\u80fd\u529b\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86GUISwiper\u4ee3\u7406\u7cfb\u7edf\uff0c\u5176\u6ed1\u52a8\u6267\u884c\u51c6\u786e\u7387\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GUI\u81ea\u52a8\u5316\u4ee3\u7406\u5927\u591a\u4e13\u6ce8\u4e8e\u63d0\u5347\u754c\u9762\u611f\u77e5\u80fd\u529b\uff0c\u628a\u4efb\u52a1\u6307\u4ee4\u8f6c\u5316\u4e3a\u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\uff0c\u4f46\u6267\u884c\u6ed1\u52a8\u7b49\u5177\u4f53\u64cd\u4f5c\u7684\u80fd\u529b\u4ecd\u662f\u74f6\u9888\uff0c\u5c24\u5176\u73b0\u6709\u65b9\u6cd5\u5bf9\u6ed1\u52a8\u4ea4\u4e92\u7684\u5904\u7406\u8fc7\u4e8e\u7b80\u5316\uff0c\u4e0e\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u5dee\u8ddd\u8f83\u5927\u3002", "method": "\u4f5c\u8005\u5c06\u4eba\u7c7b\u6ed1\u52a8\u624b\u52bf\u5206\u89e3\u6210\u591a\u4e2a\u53ef\u91cf\u5316\u7ef4\u5ea6\uff0c\u901a\u8fc7GUI\u81ea\u52a8\u63a2\u7d22\u63d0\u51fa\u4e86SwipeGen\u7ba1\u9053\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u4eff\u771f\u7684\u6ed1\u52a8\u4ea4\u4e92\u6570\u636e\u3002\u5e76\u636e\u6b64\u6784\u5efa\u4e86\u6ed1\u52a8\u80fd\u529b\u8bc4\u6d4b\u57fa\u51c6\uff0c\u540c\u65f6\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e86GUISwiper\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u4ee3\u7406\u7684\u6267\u884c\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGUISwiper\u6ed1\u52a8\u6267\u884c\u51c6\u786e\u7387\u8fbe\u523069.07%\uff0c\u76f8\u6bd4\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u57fa\u7ebf\u63d0\u5347\u4e86214%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51faSwipeGen\u548c\u76f8\u5e94\u8bc4\u6d4b\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u6ed1\u52a8\u6267\u884c\u667a\u80fd\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u6ed1\u52a8\u52a8\u4f5c\u6267\u884c\u8fd9\u4e00\u74f6\u9888\uff0c\u4e3a\u81ea\u52a8\u5316\u4ea4\u4e92\u4ee3\u7406\u8fc8\u5411\u4eba\u7c7b\u6c34\u5e73\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2601.18330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18330", "abs": "https://arxiv.org/abs/2601.18330", "authors": ["Muhammad Ali Shah", "Muhammad Mansoor Alam", "Saddam Hussain Khan"], "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification", "comment": "33 Pages, 8 Tables, Figures 16", "summary": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8111\u80bf\u7624MRI\u5206\u6790\u6846\u67b6EDSH\uff0c\u878d\u5408DenseNet\u548cSwin Transformer\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u7eb9\u7406\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u7684CNN\u6216\u5355\u7eafTransformer\u6a21\u578b\u5728\u8111\u80bf\u7624\u5f62\u6001\u548c\u7eb9\u7406\u590d\u6742\u6027\u5206\u6790\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u96be\u4ee5\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6df7\u5408\u67b6\u6784\u5e94\u5bf9\u4e0d\u540c\u7c7b\u578b\u8111\u80bf\u7624\u7684\u8bca\u65ad\u6311\u6218\u3002", "method": "\u63d0\u51faEDSH\u6df7\u5408\u67b6\u6784\uff0c\u5305\u62ec\u4e24\u4e2a\u65b0\u5b9e\u9a8c\u8bbe\u7f6e\uff1a1\uff09Boosted Feature Space\uff08BFS\uff09\u5c06\u5b9a\u5236DenseNet\u548cSwin Transformer\u5206\u652f\u5b66\u4e60\u4e92\u8865\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u518d\u878d\u5408\u63d0\u5347\uff0c\u589e\u5f3a\u5f25\u6f2b\u6027\u80f6\u8d28\u7624\u68c0\u51fa\u80fd\u529b\uff1b2\uff09\u5c42\u7ea7DenseNet Swint\u7ed3\u6784\u7ed3\u5408\u6df1\u5c42\u7279\u5f81\u63d0\u53d6\u4e0e\u53cc\u91cd\u6b8b\u5dee\u8fde\u63a5\uff0cDenseNet\u8d1f\u8d23\u5c40\u90e8\u7ed3\u6784\uff0cSwin_t\u5efa\u6a21\u5168\u5c40\u5f62\u6001\uff0c\u6709\u6548\u51cf\u5c11\u8111\u819c\u7624\u548c\u5782\u4f53\u7624\u8bef\u68c0\u3002\u6240\u6709\u5206\u652f\u5747\u7ed3\u5408MRI\u5f71\u50cf\u7279\u70b9\u8bbe\u8ba1\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3002", "result": "\u5728\u5305\u542b\u56db\u79cd\u80bf\u7624\u7684\u5927\u89c4\u6a21MRI\u6570\u636e\u96c6\uff0840,260\u5f20\u56fe\u50cf\uff09\u4e0a\uff0cEDSH\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u7684CNN\u3001\u7eafTransformer\u53ca\u6df7\u5408\u7ed3\u6784\uff0c\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u8fbe98.50%\u3002", "conclusion": "EDSH\u6846\u67b6\u53ef\u9ad8\u6548\u89e3\u6790MRI\u8111\u80bf\u7624\u590d\u6742\u7279\u5f81\uff0c\u7279\u522b\u80fd\u5e94\u5bf9\u4e0d\u540c\u7c7b\u578b\u80bf\u7624\u7684\u5f02\u8d28\u6027\u4e0e\u5f62\u6001\u5dee\u5f02\uff0c\u5bf9\u63d0\u5347\u8111\u80bf\u7624\u81ea\u52a8\u8bca\u65ad\u7684\u6027\u80fd\u6709\u663e\u8457\u4ef7\u503c\u3002"}}
{"id": "2601.18336", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18336", "abs": "https://arxiv.org/abs/2601.18336", "authors": ["Isaac Deutsch", "Nicolas Mo\u00ebnne-Loccoz", "Gavriel State", "Zan Gojcic"], "title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction", "comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/ppisp/", "summary": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u6821\u6b63\u6a21\u5757PPISP\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\u5728\u9762\u5bf9\u6444\u50cf\u5934\u5149\u5b66\u548cISP\u5dee\u5f02\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u65e0\u771f\u5b9e\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u65b0\u89c6\u89d2\u5b9e\u73b0\u66f4\u771f\u5b9e\u4e0e\u516c\u5e73\u7684\u91cd\u5efa\uff0c\u540c\u65f6\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "motivation": "\u591a\u89c6\u56fe3D\u91cd\u5efa\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f1a\u53d7\u5230\u4e0d\u540c\u6444\u50cf\u5934\u53ca\u5176\u5904\u7406\u7b97\u6cd5\u5e26\u6765\u7684\u8272\u5f69\u3001\u4eae\u5ea6\u7b49\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\u5f71\u54cd\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u4e0d\u7406\u60f3\u3002\u73b0\u6709\u7684\u5bf9\u7b56\uff08\u5982\u9010\u5e27\u6f5c\u53d8\u91cf\u6216\u4eff\u5c04\u989c\u8272\u6821\u6b63\uff09\u7f3a\u4e4f\u7269\u7406\u4f9d\u636e\uff0c\u6cdb\u5316\u5230\u65b0\u89c6\u89d2\u80fd\u529b\u8f83\u5dee\u3002\u8bba\u6587\u52a8\u673a\u662f\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u6446\u8131\u5bf9\u771f\u5b9e\u56fe\u50cf\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u4e0b\u7684\u91cd\u5efa\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86PPISP\u6821\u6b63\u6a21\u5757\uff0c\u901a\u8fc7\u7269\u7406\u53ef\u89e3\u91ca\u7684\u53d8\u6362\uff0c\u5206\u79bb\u6444\u50cf\u5934\u5185\u5728\u5c5e\u6027\u548c\u91c7\u96c6\u4f9d\u8d56\u6548\u5e94\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u4e2aPPISP\u63a7\u5236\u5668\uff0c\u53ef\u6839\u636e\u8f93\u5165\u89c6\u56fe\u5b66\u4e60\u53c2\u6570\u5e76\u9884\u6d4b\u65b0\u89c6\u89d2\u7684ISP\u53c2\u6570\uff0c\u7c7b\u4f3c\u771f\u5b9e\u76f8\u673a\u7684\u81ea\u52a8\u66dd\u5149\u548c\u81ea\u52a8\u767d\u5e73\u8861\uff0c\u4f7f\u65b9\u6cd5\u5bf9\u5143\u6570\u636e\u5177\u6709\u517c\u5bb9\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u6807\u51c6\u591a\u89c6\u56fe3D\u91cd\u5efa\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u76f4\u89c2\u63a7\u5236\u548c\u53ef\u96c6\u6210\u5143\u6570\u636e\u7279\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u8865\u6551\u7b56\u7565\u3002", "conclusion": "PPISP\u6a21\u5757\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u7269\u7406\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u6548\u679c\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u65b0\u89c6\u89d2\u4e0b\u65e0\u987b\u771f\u5b9e\u56fe\u50cf\u7684\u771f\u5b9e\u4e14\u516c\u5e73\u91cd\u5efa\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u591a\u89c6\u56fe3D\u91cd\u5efa\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2601.18340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18340", "abs": "https://arxiv.org/abs/2601.18340", "authors": ["Bingzheng Qu", "Kehai Chen", "Xuefeng Bai", "Jun Yu", "Min Zhang"], "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing", "comment": null, "summary": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NRVBench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u7528\u4e8e\u8bc4\u4f30\u975e\u521a\u6027\u89c6\u9891\u7f16\u8f91\u7684\u57fa\u51c6\uff0c\u5305\u62ec\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6570\u636e\u96c6\u3001\u7ec6\u81f4\u7684\u4efb\u52a1\u6307\u4ee4\u3001\u4ee5\u53ca\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u8bc4\u6d4b\u6307\u6807\u548c\u65e0\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8bc1\u660e\u4ed6\u4eec\u65b9\u6cd5\u5728\u7269\u7406\u5408\u7406\u6027\u548c\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u9a71\u52a8\u7684\u89c6\u9891\u7f16\u8f91\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u975e\u521a\u6027\u5f62\u53d8\u65f6\u5e38\u5e38\u51fa\u73b0\u7269\u7406\u5931\u771f\u548c\u65f6\u5e8f\u95ea\u70c1\uff0c\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u548c\u590d\u6742\u52a8\u6001\u7684\u6709\u6548\u8bc4\u4f30\u65b9\u5f0f\u3002\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u6784\u5efaNRVBench\u6570\u636e\u96c6\uff0c\u5305\u62ec180\u6bb5\u6765\u81ea\u516d\u5927\u7c7b\u7269\u7406\u7279\u6027\u7684\u975e\u521a\u6027\u8fd0\u52a8\u89c6\u9891\uff0c\u914d\u5907\u8be6\u7ec6\u4efb\u52a1\u548c\u6d4b\u8bd5\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u8bc4\u6d4b\u6307\u6807NRVE-Acc\uff0c\u80fd\u8bc4\u4f30\u7269\u7406\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u8fde\u7eed\u6027\u3001\u548c\u6307\u4ee4\u5bf9\u9f50\u5ea6\uff1b3. \u63a8\u51fa\u65e0\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5VM-Edit\uff0c\u901a\u8fc7\u53cc\u533a\u57df\u53bb\u566a\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u7684\u7f16\u8f91\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u5408\u7406\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u517c\u987e\u7ed3\u6784\u4fdd\u6301\u4e0e\u52a8\u6001\u5f62\u53d8\uff0c\u5728\u6807\u51c6\u548c\u65b0\u63d0\u51fa\u7684\u6307\u6807\u4e0a\u5747\u6709\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "NRVBench\u53ef\u6210\u4e3a\u63a8\u52a8\u7269\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u7684\u91cd\u8981\u6807\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bba\u6587\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u975e\u521a\u6027\u7f16\u8f91\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u65f6\u5e8f\u8868\u73b0\u3002"}}
{"id": "2601.18346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18346", "abs": "https://arxiv.org/abs/2601.18346", "authors": ["Sijing Wu", "Yunhao Li", "Zicheng Zhang", "Qi Jia", "Xinyue Li", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Q-Bench-Portrait\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4eba\u50cf\u56fe\u50cf\u8d28\u91cf\u611f\u77e5\u7684\u5168\u9762\u6027\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u65b9\u9762\u5f53\u524d\u6a21\u578b\u8fd8\u6709\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u901a\u7528\u56fe\u50cf\u7684\u4f4e\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u9488\u5bf9\u7ed3\u6784\u548c\u611f\u77e5\u7279\u6027\u72ec\u7279\u7684\u4eba\u50cf\u56fe\u50cf\u7684\u611f\u77e5\u80fd\u529b\u5c1a\u672a\u88ab\u7cfb\u7edf\u8bc4\u4f30\u548c\u6316\u6398\u3002\u7f3a\u4e4f\u4e13\u95e8\u7684\u4eba\u50cf\u56fe\u50cf\u8d28\u91cf\u8bc4\u4ef7\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u548c\u6a21\u578b\u6539\u8fdb\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Q-Bench-Portrait\u57fa\u51c6\uff0c\u5305\u542b2765\u7ec4\u56fe\u50cf-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u9898\u6750\u6db5\u76d6\u81ea\u7136\u3001\u4eba\u5de5\u5931\u771f\u3001AI\u751f\u6210\u3001\u827a\u672f\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u7b49\u591a\u6837\u7684\u4eba\u50cf\u56fe\u7247\uff0c\u8bc4\u4ef7\u7ef4\u5ea6\u6db5\u76d6\u6280\u672f\u5931\u771f\u3001AIGC\u4e13\u5c5e\u5931\u771f\u4e0e\u7f8e\u5b66\uff0c\u95ee\u9898\u7c7b\u578b\u591a\u6837\uff08\u5355\u9009\u3001\u591a\u9009\u3001\u5224\u65ad\u3001\u5f00\u653e\u95ee\u7b54\uff09\uff0c\u517c\u987e\u6574\u4f53\u4e0e\u5c40\u90e8\u3002\u5229\u7528\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e8625\u79cd\u4e3b\u6d41\u5f00\u6e90/\u95ed\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002", "result": "\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u90e8\u5206\u6a21\u578b\u5728\u67d0\u4e9b\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u4efb\u52a1\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u4ecd\u6709\u9650\uff0c\u51c6\u786e\u6027\u548c\u7cbe\u7ec6\u5ea6\u4e0e\u4eba\u7c7b\u8bc4\u5224\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "Q-Bench-Portrait\u4e3a\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u8bc4\u6d4b\u5de5\u5177\uff0c\u63ed\u793a\u51fa\u5f53\u524dMLLMs\u5728\u6b64\u65b9\u5411\u7684\u4e0d\u8db3\uff0c\u671f\u5f85\u63a8\u52a8\u901a\u7528\u6216\u9886\u57df\u4e13\u7528\u6a21\u578b\u5728\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u80fd\u529b\u4e0a\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2601.18368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18368", "abs": "https://arxiv.org/abs/2601.18368", "authors": ["Caterina Fuster-Barcel\u00f3", "Claudia Castrill\u00f3n", "Laura Rodrigo-Mu\u00f1oz", "Victor Manuel Vega-Su\u00e1rez", "Nicol\u00e1s P\u00e9rez-Fern\u00e1ndez", "Gorka Bastarrika", "Arrate Mu\u00f1oz-Barrutia"], "title": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI", "comment": null, "summary": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.\n  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.\n  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOREHAS\u7684\u81ea\u52a8\u5316\u6280\u672f\uff0c\u53ef\u4ece\u5e38\u89c4MRI\u6570\u636e\u4e2d\u5b9e\u73b0\u524d\u5ead\u6c34\u80bf\u4f53\u79ef\u7684\u5168\u81ea\u52a8\u5b9a\u91cf\u3002", "motivation": "\u76ee\u524d\uff0c\u5185\u6dcb\u5df4\u6c34\u80bf\u7684\u4f53\u79ef\u5b9a\u91cf\u4e3b\u8981\u4f9d\u8d56\u534a\u81ea\u52a8\u6216\u624b\u5de5\u65b9\u6cd5\uff0c\u64cd\u4f5c\u7e41\u7410\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u4e34\u5e8a\u6216\u7814\u7a76\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u5b9e\u73b0\u65e0\u9700\u624b\u52a8\u5e72\u9884\u7684\u5168\u81ea\u52a8\u5b9a\u91cf\u6d41\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u7cbe\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "OREHAS\u5305\u542b\u5207\u7247\u5206\u7c7b\u3001\u5185\u8033\u5b9a\u4f4d\u548c\u5e8f\u5217\u7279\u5f02\u6027\u5206\u5272\u4e09\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u80fd\u4ece\u5168\u5934MRI\u76f4\u63a5\u8ba1\u7b97\u6bcf\u53ea\u8033\u6735\u7684\u4f53\u79ef\u6bd4\u503c\u3002\u8bad\u7ec3\u4ec5\u9700\u6bcf\u4f8b3-6\u5f20\u6709\u6807\u6ce8\u5207\u7247\uff0c\u5374\u80fd\u6cdb\u5316\u5230\u5b8c\u65743D\u4f53\u79ef\u3002\u901a\u8fc7\u4e0e\u4e13\u5bb6\u624b\u5de5\u6807\u6ce8\u53ca\u4e3b\u6d41\u8f6f\u4ef6\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\uff0cOREHAS Dice\u5206\u6570\u9ad8\u8fbe0.90\uff08SPACE-MRC\uff09\u548c0.75\uff08REAL-IR\uff09\u3002\u4e0e\u4e13\u5bb6\u4eba\u5de5\u6807\u6ce8\u91cf\u5316\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff08VSI=74.3%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4e34\u5e8a\u5e38\u7528\u7684syngo.via\u8f6f\u4ef6\uff08VSI=42.5%\uff09\u3002\u6d4b\u91cf\u66f4\u52a0\u751f\u7406\u53ef\u4fe1\uff0c\u4e14\u5bf9\u64cd\u4f5c\u8005\u4f9d\u8d56\u5c11\u3001\u91cd\u73b0\u6027\u597d\u3002", "conclusion": "OREHAS\u65e0\u9700\u590d\u6742\u624b\u5de5\u64cd\u4f5c\uff0c\u53ef\u517c\u5bb9\u4e3b\u6d41\u5f71\u50cf\u534f\u8bae\u3001\u9ad8\u6548\u5b8c\u6210\u5185\u8033\u6c34\u80bf\u7684\u7cbe\u51c6\u4f53\u79ef\u5b9a\u91cf\uff0c\u9002\u7528\u4e8e\u5927\u6837\u672c\u7814\u7a76\uff0c\u4e5f\u4e3a\u4e34\u5e8a\u8bca\u65ad\u9608\u503c\u7684\u91cd\u65b0\u6821\u51c6\u63d0\u4f9b\u575a\u5b9e\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2601.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18372", "abs": "https://arxiv.org/abs/2601.18372", "authors": ["Christos Petrou", "Harris Partaourides", "Athanasios Balomenos", "Yannis Kopsinis", "Sotirios Chatzis"], "title": "Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues", "comment": null, "summary": "Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5934\u6234\u663e\u793a\u5668\uff08HMD\uff09\u8fd0\u52a8\u4fe1\u53f7\u548c\u56fe\u50cf\u663e\u8457\u6027\u4fe1\u606f\u7684\u6ce8\u89c6\u70b9\u9884\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5728\u65e0\u773c\u52a8\u4eea\u786c\u4ef6\u65f6\u7684VR\u4f53\u9a8c\u3002", "motivation": "\u5728VR\u5e94\u7528\u4e2d\uff0c\u6ce8\u89c6\u70b9\u9884\u6d4b\u53ef\u4ee5\u964d\u4f4e\u611f\u77e5\u5ef6\u8fdf\u5e76\u652f\u6301\u8d44\u6e90\u9700\u6c42\u5927\u7684\u6280\u672f\uff08\u5982\u805a\u7126\u6e32\u67d3\uff09\u3002\u4f46\u53d7\u5236\u4e8e\u786c\u4ef6\u6210\u672c\u548c\u9690\u79c1\uff0c\u76f4\u63a5\u773c\u52a8\u8ffd\u8e2a\u5e76\u4e0d\u603b\u662f\u53ef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u5176\u4ed6\u65b9\u5f0f\u9884\u6d4b\u7528\u6237\u6ce8\u89c6\u70b9\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u89c6\u70b9\u9884\u6d4b\u6846\u67b6\uff0c\u5c06HMD\u8fd0\u52a8\u4fe1\u53f7\u4e0e\u7531\u89c6\u9891\u5e27\u63d0\u53d6\u7684\u89c6\u89c9\u663e\u8457\u6027\u7ebf\u7d22\u7ed3\u5408\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u663e\u8457\u6027\u7f16\u7801\u5668\uff08UniSal\uff09\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4e0eHMD\u8fd0\u52a8\u6570\u636e\u878d\u5408\uff0c\u4e4b\u540e\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u5757\uff08\u5305\u62ecTSMixer\u548cLSTM\u4e24\u79cd\u67b6\u6784\uff09\u6765\u9884\u6d4b\u672a\u6765\u7684\u6ce8\u89c6\u65b9\u5411\u3002", "result": "\u5728EHTask\u6570\u636e\u96c6\u548c\u5546\u7528VR\u786c\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6ce8\u89c6\u70b9\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\uff08\u5982Center-of-HMD\u548cMean Gaze\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5f25\u8865\u4e86\u7f3a\u4e4f\u76f4\u63a5\u773c\u52a8\u4eea\u786c\u4ef6\u65f6\u7684\u5c40\u9650\uff0c\u80fd\u591f\u7f29\u77ed\u611f\u77e5\u6ede\u540e\u5e76\u63d0\u5347VR\u4e2d\u7684\u81ea\u7136\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2601.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18385", "abs": "https://arxiv.org/abs/2601.18385", "authors": ["Rinka Kawano", "Masaki Kawamura"], "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals", "comment": null, "summary": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6570\u5b57\u6c34\u5370\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u56fe\u50cf\u7ecf\u8fc7\u51e0\u4f55\u53d8\u6362\uff08\u5982\u7f29\u653e\u3001\u526a\u88c1\u7b49\uff09\u540e\u51c6\u786e\u540c\u6b65\u5e76\u63d0\u53d6\u6c34\u5370\u3002\u901a\u8fc7\u5d4c\u5165\u7279\u6b8a\u7684\u201c\u7f51\u683c\u201d\u5f15\u5bfc\u4fe1\u53f7\uff0c\u5e76\u5229\u7528Radon\u53d8\u6362\u5206\u6790\u5176\u7578\u53d8\uff0c\u4ece\u800c\u4f30\u7b97\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u51e0\u4f55\u653b\u51fb\u5747\u80fd\u51c6\u786e\u4f30\u7b97\u53d8\u6362\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u6570\u5b57\u6c34\u5370\u65b9\u6848\u5728\u9047\u5230\u56fe\u50cf\u526a\u88c1\u7b49\u51e0\u4f55\u653b\u51fb\u65f6\uff0c\u96be\u4ee5\u4fdd\u8bc1\u6c34\u5370\u7684\u540c\u6b65\u548c\u6b63\u786e\u63d0\u53d6\u3002\u7279\u522b\u662f\u526a\u88c1\u4f1a\u6539\u53d8\u56fe\u50cf\u539f\u70b9\uff0c\u4f7f\u5f97\u4f20\u7edf\u7684\u6c34\u5370\u5d4c\u5165\u4e0e\u68c0\u6d4b\u65b9\u5f0f\u96be\u4ee5\u5e94\u5bf9\u3002\u9488\u5bf9\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u540c\u6b65\u3001\u6709\u6548\u62b5\u5fa1\u6b64\u7c7b\u653b\u51fb\u7684\u6c34\u5370\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u5728\u56fe\u50cf\u4e2d\u5d4c\u5165\u5177\u6709\u660e\u663e\u6c34\u5e73\u548c\u5782\u76f4\u7279\u5f81\u7684\u7f51\u683c\u72b6\u5f15\u5bfc\u4fe1\u53f7\u3002\u56fe\u50cf\u906d\u53d7\u51e0\u4f55\u53d8\u6362\u540e\uff0c\u8be5\u7f51\u683c\u540c\u6837\u88ab\u626d\u66f2\u3002\u5229\u7528Radon\u53d8\u6362\u5206\u6790\u626d\u66f2\u540e\u7684\u7f51\u683c\u7279\u5f81\uff0c\u51c6\u786e\u4f30\u7b97\u51fa\u6240\u627f\u53d7\u7684\u53d8\u6362\u77e9\u9635\u3002\u7f51\u683c\u7684\u7f16\u7801\u65b9\u5f0f\u533a\u5206\u4e86\u6c34\u5e73\u65b9\u5411\u4e0e\u5782\u76f4\u65b9\u5411\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u540c\u6b65\u65f6\u7684\u6a21\u7cca\u6027\u3002", "result": "\u5728\u6a21\u62df\u5404\u7c7b\u51e0\u4f55\u653b\u51fb\uff0c\u5305\u62ec\u975e\u7b49\u6bd4\u4f8b\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u5207\u53d8\u548c\u526a\u88c1\u7b49\u6761\u4ef6\u4e0b\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5747\u80fd\u4ee5\u8f83\u4f4e\u7684\u8bef\u5dee\u51c6\u786e\u4f30\u7b97\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u6c34\u5370\u540c\u6b65\u3002\u5bf9\u590d\u5408\u653b\u51fb\u4e5f\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u901a\u8fc7\u7f51\u683c\u5f15\u5bfc\u4fe1\u53f7\u548cRadon\u53d8\u6362\uff0c\u5b9e\u73b0\u4e86\u5bf9\u51e0\u4f55\u653b\u51fb\u4e0b\u56fe\u50cf\u6c34\u5370\u7684\u9ad8\u7cbe\u5ea6\u540c\u6b65\u548c\u9c81\u68d2\u63d0\u53d6\u3002\u7279\u522b\u5728\u4ee5\u5f80\u96be\u4ee5\u5904\u7406\u7684\u526a\u88c1\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u4e86\u660e\u663e\u4f18\u52bf\uff0c\u5bf9\u5b9e\u9645\u6570\u5b57\u7248\u6743\u4fdd\u62a4\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.18386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18386", "abs": "https://arxiv.org/abs/2601.18386", "authors": ["Gabriel Lee Jun Rong", "Christos Korgialas", "Dion Jia Xu Ho", "Pai Chet Ng", "Xiaoxiao Miao", "Konstantinos N. Plataniotis"], "title": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks", "comment": null, "summary": "Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARMOR\u7684\u65b0\u578b\u81ea\u52a8\u5316\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8bed\u4e49\u5f15\u5bfc\u5b9e\u73b0\u66f4\u5f3a\u5927\u3001\u66f4\u7075\u6d3b\u7684\u5bf9\u6297\u6027\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u653b\u51fb\u5de5\u5177\u65b9\u6cd5\u5355\u4e00\u3001\u6d41\u7a0b\u56fa\u5b9a\uff0c\u65e0\u6cd5\u81ea\u9002\u5e94\u8c03\u6574\u5e76\u7f3a\u4e4f\u56fe\u50cf\u8bed\u4e49\u5c42\u6b21\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u653b\u51fb\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u548c\u66f4\u590d\u6742\u7684\u76ee\u6807\u3002", "method": "ARMOR\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6765\u534f\u540c\u8c03\u5ea6\u4e09\u79cd\u5178\u578b\u7684\u5bf9\u6297\u653b\u51fb\uff08CW\u3001JSMA\u3001STA\uff09\uff0c\u5e76\u901a\u8fc7\u4e00\u5957\u201cMixing Desk\u201d\u6df7\u5408\u7cfb\u7edf\uff0c\u5c06\u591a\u4e2a\u6270\u52a8\u7b56\u7565\u8fdb\u884c\u7ec4\u5408\u548c\u4f18\u5316\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f1a\u5b9e\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u5404\u653b\u51fb\u667a\u80fd\u4f53\u53c2\u6570\uff0c\u5b9e\u73b0\u56fe\u50cf\u7279\u5b9a\u8bed\u4e49\u6f0f\u6d1e\u7684\u7cbe\u51c6\u6316\u6398\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cARMOR\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u8de8\u67b6\u6784\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u80fd\u6709\u6548\u6b3a\u9a97\u591a\u79cd\u9632\u5fa1\u8bbe\u7f6e\uff0c\u5c24\u5176\u662f\u5728\u76f2\u76ee\u76ee\u6807\u548c\u767d\u76d2\u76ee\u6807\u653b\u51fb\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ARMOR\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u5927\u6a21\u578b\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u8bed\u4e49\u611f\u77e5\u548c\u7b56\u7565\u5f39\u6027\u7684\u81ea\u52a8\u5316\u653b\u51fb\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u6709\u6548\u6027\u4e0e\u901a\u7528\u6027\u3002"}}
{"id": "2601.18392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18392", "abs": "https://arxiv.org/abs/2601.18392", "authors": ["Moritz Rempe", "Lukas T. Rotkopf", "Marco Schlimbach", "Helmut Becker", "Fabian H\u00f6rst", "Johannes Haubold", "Philipp Dammann", "Kevin Kr\u00f6ninger", "Jens Kleesiek"], "title": "Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space", "comment": null, "summary": "Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u5728MRI\u539f\u59cb\u9891\u57df(k-Space)\u6570\u636e\u4e0a\u8fdb\u884c\u5206\u7c7b\u7684\u590d\u6742\u503cVision Transformer\uff08kViT\uff09\u6a21\u578b\uff0c\u7528\u5f84\u5411k-Space\u5206\u5757\u65b9\u5f0f\u63d0\u5347\u6548\u7387\u548c\u5951\u5408\u7269\u7406\u7279\u6027\uff0c\u5e76\u5728\u51c6\u786e\u7387\u548c\u8d44\u6e90\u6d88\u8017\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u5728MRI\u591a\u4ee5\u91cd\u5efa\u540e\u7684\u5e45\u503c\u56fe\u50cf\u5f00\u5c55\uff0c\u5ffd\u89c6\u4e86\u539f\u59cb\u7684\u76f8\u4f4d\u4fe1\u606f\uff0c\u5e76\u9700\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5c40\u90e8\u64cd\u4f5c\u4e5f\u4e0d\u9002\u5408\u5904\u7406k-Space\u8fd9\u79cd\u5168\u5c40\u9891\u57df\u6570\u636e\u3002\u4e3a\u4f18\u5316\u6548\u7387\u5e76\u4fdd\u7559\u66f4\u591a\u7269\u7406\u4fe1\u606f\uff0c\u9700\u5f00\u53d1\u9002\u7528\u4e8ek-Space\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u590d\u6742\u503cVision Transformer\uff08kViT\uff09\uff0c\u53ef\u76f4\u63a5\u5bf9k-Space\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff1b\u521b\u65b0\u6027\u5730\u91c7\u7528\u5f84\u5411k-Space\u5206\u5757\uff0c\u5951\u5408\u9891\u57df\u7279\u6027\u7684\u80fd\u91cf\u5206\u5e03\uff0c\u63d0\u5347\u4e0eMRI\u7269\u7406\u7684\u5339\u914d\u5ea6\u3002", "result": "\u5728fastMRI\u548c\u81ea\u6709\u6570\u636e\u96c6\u4e0a\uff0ckViT\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0eResNet\u3001EfficientNet\u3001ViT\u7b49\u56fe\u50cf\u57df\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5bf9\u9ad8\u52a0\u901f\u56e0\u5b50\u7684\u6297\u5e72\u6270\u6027\u66f4\u5f3a\uff0c\u8bad\u7ec3\u65f6\u6700\u9ad8\u53ef\u51cf\u5c1168\u500d\u663e\u5b58\u6d88\u8017\u3002", "conclusion": "kViT\u5b9e\u73b0\u4e86\u65e0\u9700\u56fe\u50cf\u91cd\u5efa\u7684MRI\u9891\u57df\u76f4\u63a5\u5206\u7c7b\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u8d44\u6e90\u8282\u7ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u76f4\u63a5\u57fa\u4e8e\u626b\u63cf\u4eea\u6570\u636e\u7684AI\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.18407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18407", "abs": "https://arxiv.org/abs/2601.18407", "authors": ["Jon Sporring", "David Stansby"], "title": "Larger than memory image processing", "comment": "10 pages", "summary": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9PB\u7ea7\u8d85\u5927\u56fe\u50cf\u6570\u636e\u7684\u5206\u6790\u63d0\u51faI/O\u53cb\u597d\u7684\u4e32\u6d41\u5904\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6d41\u4e0e\u78c1\u76d8\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5b9e\u73b0\u6781\u5927\u63d0\u5347\u5728\u6709\u9650\u5185\u5b58\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u5904\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u663e\u5fae\u7535\u5b50\u3001\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u4ea7\u751f\u7684\u56fe\u50cf\u6570\u636e\u6025\u5267\u589e\u5927\uff08\u59821.4 PB\u7ea7\u4f53\u79ef\u6570\u636e\uff09\uff0c\u4f20\u7edf\u7684\u201c\u5168\u91cf\u8f7d\u5165\u5185\u5b58\u201d\u65b9\u6cd5\u5df2\u65e0\u6cd5\u5e94\u5bf9\uff0c\u4e0d\u4ec5\u56e0\u5185\u5b58\u9650\u5236\uff0c\u8fd8\u53d7\u9650\u4e8e\u78c1\u76d8I/O\u74f6\u9888\u3002\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u5145\u5206\u5229\u7528\u4e32\u6d41\u673a\u5236\uff0c\u4f18\u5316\u78c1\u76d8\u8bfb\u5199\u4e0e\u7b97\u6cd5\u6267\u884c\u6a21\u5f0f\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u4e24\u79cd\u4e3b\u6d413D\u56fe\u50cf\u5b58\u50a8\uff082D\u5207\u7247\u5806\u53e0\u4e0e3D\u5206\u5757\u5b58\u50a8\uff09\uff0c\u63d0\u51fa\u5728\u8fd9\u4e24\u7c7b\u7ed3\u6784\u4e0a\u90fd\u53ef\u91c7\u7528\u4ee5\u5207\u7247\u4e3a\u4e2d\u5fc3\u7684\u4e32\u6d41\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5b9a\u4e49\u201csweep-based\u201d\u6267\u884c\u3001\u7a97\u53e3\u64cd\u4f5c\u4e0e\u91cd\u53e0\u611f\u77e5\u7684\u5206\u7247\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u78c1\u76d8\u5197\u4f59\u8bbf\u95ee\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u81ea\u52a8\u5206\u6790\u7b97\u6cd5\u6240\u9700\u5185\u5b58\u53caI/O\u6a21\u5f0f\uff0c\u5728\u7f16\u8bd1/\u8fd0\u884c\u65f6\u4f18\u5316\u7a97\u53e3/\u6d41\u8c03\u5ea6\uff0c\u5b9e\u73b0\u7b97\u6cd5\u6d41\u6c34\u7ebf\u81ea\u52a8\u878d\u5408\u4e0e\u8c03\u5ea6\u3002", "result": "\u8be5\u67b6\u6784\u663e\u8457\u51cf\u5c11\u4e86\u78c1\u76d8I/O\u6b21\u6570\uff0c\u4fdd\u6301\u7ebf\u6027I/O\u626b\u63cf\uff0c\u9884\u6d4b\u6027\u5185\u5b58\u5360\u7528\u3002\u540c\u65f6\uff0c\u517c\u5bb9\u73b0\u6709\u5206\u5272\u548c\u5f62\u6001\u5b66\u5de5\u5177\uff0c\u5e76\u5c06\u5927\u6570\u636e\u7684\u524d\u5904\u7406/\u540e\u5904\u7406\u8f6c\u5316\u4e3a\u987a\u5e8f\u6d41\u6c34\u7ebf\u6a21\u5f0f\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u8d85\u5927\u6570\u636e\u56fe\u50cf\u5206\u6790\u7684\u541e\u5410\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e32\u6d41\u673a\u5236\u4e0eDSL\u5de5\u5177\u80fd\u5728\u6709\u9650\u5185\u5b58\u4e0b\u7a33\u5b9a\u9ad8\u6548\u5904\u7406PB\u7ea7\u56fe\u50cf\u6570\u636e\uff0c\u662f\u7279\u5927\u89c4\u6a21\u56fe\u50cf\u5206\u6790\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5bf9\u9ad8\u6027\u80fd\u786c\u4ef6\u7684\u4f9d\u8d56\uff0c\u6709\u671b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2601.18414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18414", "abs": "https://arxiv.org/abs/2601.18414", "authors": ["Aura Loredana Dan"], "title": "Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings", "comment": "9 pages, 8 figures", "summary": "Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.", "AI": {"tldr": "\u672c\u8bba\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u57fa\u4e8e\u513f\u7ae5\u753b\u4f5c\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u95ed\u75c7\u513f\u7ae5\u60c5\u611f\u72b6\u6001\u7684\u8bc6\u522b\u6548\u7387\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9\u81ea\u95ed\u75c7\u513f\u7ae5\u60c5\u611f\u72b6\u6001\u7684\u8bc6\u522b\u65b9\u6cd5\u591a\u4e3a\u4fb5\u5165\u6027\u3001\u4e3b\u89c2\u6216\u4e0d\u6613\u6807\u51c6\u5316\uff0c\u4e9f\u9700\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u60c5\u611f\u8bc4\u4f30\u624b\u6bb5\u3002", "method": "\u4f5c\u8005\u4ee5\u513f\u7ae5\u753b\u4f5c\u4e3a\u60c5\u611f\u8868\u8fbe\u8f7d\u4f53\uff0c\u5229\u7528\u5fc3\u7406\u4e13\u5bb6\u6807\u6ce8\u7684\u60c5\u611f\u6807\u7b7e\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3MobileNet\u3001EfficientNet\u548cVGG16\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u7edf\u4e00\u5b9e\u9a8c\u6846\u67b6\u8bc4\u4f30\u5b83\u4eec\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u8868\u73b0\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u8f7b\u91cf\u5316\u4e0e\u6df1\u5ea6\u7ed3\u6784\u95f4\u5b58\u5728\u6743\u8861\u3002\u5176\u4e2d\u8f7b\u91cf\u7ea7\u6a21\u578b\u66f4\u9002\u5408\u79fb\u52a8\u7aef\u53ca\u5b9e\u65f6\u5e94\u7528\u573a\u666f\uff0c\u800c\u6df1\u5c42\u6a21\u578b\u5219\u5728\u51c6\u786e\u7387\u4e0a\u66f4\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u81ea\u95ed\u75c7\u513f\u7ae5\u60c5\u611f\u72b6\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u753b\u4f5c\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u60c5\u611f\u5206\u7c7b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u5e76\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2601.18448", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18448", "abs": "https://arxiv.org/abs/2601.18448", "authors": ["Lloyd Austin Courtenay"], "title": "On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics", "comment": "17 pages, 5 figures, Preprint pending review", "summary": "Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust \"diagonal\" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f62\u6001\u8ba1\u91cf\u5b66\u4e2d\u7684\u6807\u51c6\u9884\u5904\u7406\u65b9\u6cd5\uff08GPA\uff09\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u5e26\u6765\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6821\u51c6\u6d41\u7a0b\u4ee5\u907f\u514d\u8fd9\u79cd\u6c61\u67d3\u3002\u901a\u8fc72D\u53ca3D\u6a21\u62df\uff0c\u5bf9\u4e0d\u540c\u6837\u672c\u548c\u7279\u5f81\u5bc6\u5ea6\u4e0b\u7684\u5f71\u54cd\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u5206\u6790\u4e2d\uff0c\u5e7f\u6cdb\u91c7\u7528GPA\u5bf9\u6240\u6709\u6807\u672c\u8fdb\u884c\u5168\u5c40\u5bf9\u9f50\uff0c\u4f46\u8fd9\u79cd\u505a\u6cd5\u53ef\u80fd\u9020\u6210\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\u7edf\u8ba1\u76f8\u5173\uff0c\u5f71\u54cd\u9884\u6d4b\u7ed3\u679c\u7684\u516c\u6b63\u6027\u548c\u51c6\u786e\u6027\u3002\u4f5c\u8005\u65e8\u5728\u6b63\u5f0f\u91cf\u5316\u8fd9\u79cd\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u53d7\u63a7\u7684\u4e8c\u7ef4\u548c\u4e09\u7ef4\u6a21\u62df\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8003\u5bdf\u6837\u672c\u91cf\u3001\u6807\u8bb0\u70b9\u5bc6\u5ea6\u53ca\u5f02\u901f\u751f\u957f\u5bf9GPA\u5f15\u5165\u4f9d\u8d56\u7684\u5f71\u54cd\u3002\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u6d41\u7a0b\uff1a\u5148\u5c06\u8bad\u7ec3\u96c6\u5bf9\u9f50\uff0c\u518d\u5c06\u6d4b\u8bd5\u6807\u672c\u5bf9\u9f50\u5230\u8bad\u7ec3\u96c6\uff0c\u4ece\u800c\u6d88\u9664\u4ea4\u53c9\u4f9d\u8d56\u3002\u5f15\u5165RMSE\u4e0e\u6807\u672c\u7a7a\u95f4\u3001\u5750\u6807\u81ea\u7531\u5ea6\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u53ca\u5377\u79ef\u56de\u5f52\u6a21\u578b\u68c0\u9a8c\u6807\u8bb0\u70b9\u7a7a\u95f4\u81ea\u76f8\u5173\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u62df\u663e\u793a\uff1a\u6837\u672c\u91cf\u4e0e\u6807\u8bb0\u7a7a\u95f4\u4e4b\u95f4\u7684\u201c\u5bf9\u89d2\u7ebf\u201d\u5173\u7cfb\u660e\u663e\uff0cRMSE\u8868\u73b0\u53ef\u7528\u81ea\u7531\u5ea6\u89e3\u6790\u63a8\u5bfc\u3002\u540c\u65f6\u53d1\u73b0\u82e5\u5ffd\u89c6\u6807\u8bb0\u70b9\u7684\u7a7a\u95f4\u81ea\u76f8\u5173\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u65b0\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u6d88\u9664\u7edf\u8ba1\u4f9d\u8d56\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u5f62\u6001\u8ba1\u91cf\u5b66\u65f6\uff0c\u9700\u4e25\u683c\u5904\u7406\u6570\u636e\u9884\u5904\u7406\u6b65\u9aa4\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u5b9e\u9645\u7684\u5bf9\u9f50\u64cd\u4f5c\u6307\u5357\uff0c\u6f84\u6e05\u4e86\u5f62\u6001\u7a7a\u95f4\u5904\u7406\u4e2d\u7684\u57fa\u672c\u7edf\u8ba1\u7ea6\u675f\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u5b66\u57fa\u7840\u3002"}}
{"id": "2601.18451", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18451", "abs": "https://arxiv.org/abs/2601.18451", "authors": ["Xuanmeng Sha", "Liyun Zhang", "Tomohiro Mashita", "Naoya Chiba", "Yuki Uranishi"], "title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control", "comment": "13 pages, 5 figures", "summary": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd53DGesPolicy\uff0c\u7528\u4e8e\u751f\u6210\u80fd\u591f\u878d\u5408\u5168\u8eab\u52a8\u4f5c\u548c\u9762\u90e8\u8868\u60c5\u7684\u6574\u4f53\u624b\u52bf\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u8868\u8fbe\u4e30\u5bcc\u4e14\u4e0e\u8bed\u97f3\u9ad8\u5ea6\u5bf9\u9f50\u7684\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u8eab\u4f53\u52a8\u4f5c\u534f\u8c03\u6027\u5dee\u3001\u52a8\u4f5c\u65e0\u610f\u4e49\u4e14\u7a7a\u95f4\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u96be\u4ee5\u5b9e\u73b0\u6574\u4f53\u7684\u52a8\u4f5c\u4e0e\u8868\u60c5\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u7edf\u4e00\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u591a\u6a21\u6001\u6574\u4f53\u624b\u52bf\u751f\u6210\u4e2d\u7684\u534f\u8c03\u548c\u8868\u8fbe\u7cbe\u5ea6\u96be\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u6574\u4f53\u624b\u52bf\u751f\u6210\u4efb\u52a1\u91cd\u65b0\u5efa\u6a21\u4e3a\u8fde\u7eed\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u501f\u9274\u4e86\u673a\u5668\u4eba\u9886\u57df\u7684\u6269\u6563\u7b56\u7565\uff08diffusion policy\uff09\u3002\u5f15\u5165\u4e86GAP\uff08Gesture-Audio-Phoneme\uff09\u591a\u6a21\u6001\u4fe1\u53f7\u878d\u5408\u6a21\u5757\uff0c\u7528\u4e8e\u7ec6\u81f4\u5bf9\u9f50\u8bed\u97f3\u8bed\u4e49\u3001\u8eab\u4f53\u52a8\u4f5c\u548c\u9762\u90e8\u8868\u60c5\u3002\u540c\u65f6\uff0c\u5c06\u5e27\u95f4\u53d8\u5316\u4f5c\u4e3a\u7edf\u4e00\u7684\u6574\u4f53\u884c\u4e3a\u6765\u5efa\u6a21\uff0c\u4f7f\u5f97\u52a8\u4f5c\u751f\u6210\u5177\u6709\u66f4\u597d\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "\u5728BEAT2\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c3DGesPolicy\u65b9\u6cd5\u5728\u751f\u6210\u81ea\u7136\u3001\u5bcc\u6709\u8868\u73b0\u529b\u3001\u4e14\u4e0e\u8bed\u97f3\u9ad8\u5ea6\u5bf9\u9f50\u7684\u6574\u4f53\u624b\u52bf\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u5176\u5b83\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "3DGesPolicy\u901a\u8fc7\u65b0\u9896\u7684\u7b56\u7565\u548c\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u9a71\u52a8\u7684\u6574\u4f53\u52a8\u4f5c\u4e0e\u8868\u60c5\u751f\u6210\u7684\u81ea\u7136\u5ea6\u548c\u534f\u8c03\u6027\uff0c\u4e3a\u8bed\u97f3\u4e0e\u52a8\u4f5c\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2601.18464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18464", "abs": "https://arxiv.org/abs/2601.18464", "authors": ["Wenbin Wei", "Suyuan Yao", "Cheng Huang", "Xiangyu Gao"], "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System", "comment": null, "summary": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Fair-Eye Net\uff0c\u4e00\u79cd\u591a\u6a21\u6001AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u7c7b\u578b\u6570\u636e\u5e76\u6ce8\u91cd\u516c\u5e73\u6027\uff0c\u5927\u5e45\u63d0\u5347\u9752\u5149\u773c\u7b5b\u67e5\u548c\u968f\u8bbf\u7684\u5ba2\u89c2\u6027\u3001\u4e00\u81f4\u6027\u4e0e\u53ef\u53ca\u6027\u3002", "motivation": "\u9752\u5149\u773c\u662f\u4e0d\u53ef\u9006\u81f4\u76f2\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u7b5b\u67e5\u548c\u968f\u8bbf\u81f3\u5173\u91cd\u8981\u3002\u4f46\u76ee\u524d\u7b5b\u67e5\u548c\u8bc4\u4f30\u65b9\u5f0f\u4e3b\u89c2\u6027\u5f3a\u3001\u6d41\u7a0b\u788e\u7247\u5316\uff0c\u4e14\u4f18\u8d28\u5f71\u50cf\u548c\u4e13\u5bb6\u8d44\u6e90\u6709\u9650\uff0c\u5bfc\u81f4\u8bca\u7597\u4e0d\u4e00\u81f4\u548c\u5065\u5eb7\u516c\u5e73\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFair-Eye Net\u7cfb\u7edf\uff0c\u878d\u5408\u773c\u5e95\u7167\u7247\u3001OCT\u7ed3\u6784\u6570\u636e\u3001\u89c6\u91ce\u529f\u80fd\u6307\u6807\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\uff0c\u91c7\u7528\u53cc\u6d41\u5f02\u6784\u878d\u5408\u67b6\u6784\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5206\u5c42\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u6709\u9009\u62e9\u6027\u9884\u6d4b\u548c\u5b89\u5168\u8f6c\u8bca\uff0c\u5e76\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u51cf\u5c11\u5f31\u52bf\u7fa4\u4f53\u6f0f\u8bca\u3002", "result": "\u7cfb\u7edf\u5b9e\u9a8c\u4e2dAUC\u8fbe0.912\uff08\u7279\u5f02\u602796.7%\uff09\uff0c\u79cd\u65cf\u5047\u9634\u6027\u5dee\u8ddd\u964d\u4f4e73.4%\uff0812.31%\u964d\u81f33.28%\uff09\uff0c\u8de8\u57df\u6027\u80fd\u7a33\u5b9a\uff0c\u80fd\u63d0\u524d3-12\u4e2a\u6708\u53d1\u51fa\u9ad8\u654f\u611f\u6027\uff0892%\uff09\u548c\u9ad8\u7279\u5f02\u6027\uff0888%\uff09\u98ce\u9669\u9884\u8b66\u3002", "conclusion": "Fair-Eye Net\u4e0d\u4ec5\u4ee5\u591a\u4efb\u52a1\u5b66\u4e60\u5c06\u516c\u5e73\u6027\u4f5c\u4e3a\u6838\u5fc3\u76ee\u6807\uff0c\u540c\u65f6\u517c\u987e\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u4e3a\u9752\u5149\u773cAI\u8bca\u7597\u7684\u5927\u89c4\u6a21\u5e94\u7528\u548c\u5168\u7403\u773c\u5065\u5eb7\u516c\u5e73\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18493", "abs": "https://arxiv.org/abs/2601.18493", "authors": ["Sara Tehrani", "Yonghao Xu", "Leif Haglund", "Amanda Berg", "Michael Felsberg"], "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment", "comment": "Under review at ICPR 2026", "summary": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.\n  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DisasterInsight\uff0c\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u707e\u5bb3\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u7684\u91cd\u6784\u53ca\u591a\u6837\u5316\u4efb\u52a1\u652f\u6301\uff0c\u586b\u8865\u4e86\u707e\u5bb3\u54cd\u5e94\u4e2d\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u76ee\u524d\u9065\u611f\u9886\u57df\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u591a\u5173\u6ce8\u4e8e\u7c97\u7c92\u5ea6\u6807\u7b7e\u548c\u56fe\u50cf\u7ea7\u8bc6\u522b\uff0c\u7f3a\u4e4f\u9762\u5411\u771f\u5b9e\u4eba\u9053\u4e3b\u4e49\u6d41\u7a0b\u7684\u529f\u80fd\u7406\u89e3\u4e0e\u6307\u4ee4\u9c81\u68d2\u6027\u8bc4\u4ef7\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u8861\u91cf\u6a21\u578b\u5728\u66f4\u590d\u6742\u3001\u66f4\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002", "method": "1\uff09\u5c06xBD\u6570\u636e\u96c6\u91cd\u6784\u4e3a11.2\u4e07\u4f59\u4e2a\u4ee5\u5efa\u7b51\u4e3a\u4e2d\u5fc3\u7684\u5b9e\u4f8b\uff0c\u652f\u6301\u5efa\u7b51\u529f\u80fd\u5206\u7c7b\u3001\u707e\u5bb3\u7ea7\u522b\u4e0e\u7c7b\u578b\u5206\u7c7b\u3001\u6570\u91cf\u7edf\u8ba1\u3001\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u7b49\u591a\u4efb\u52a1\u8bc4\u6d4b\u30022\uff09\u63d0\u51faDI-Chat\uff0c\u901a\u8fc7\u5728\u707e\u5bb3\u6307\u4ee4\u6570\u636e\u4e0a\u5bf9\u73b0\u6709VLM\u9aa8\u5e72\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7cbe\u8c03\uff0c\u5f62\u6210\u9886\u57df\u9002\u5e94\u57fa\u7ebf\u30023\uff09\u7528DI-Chat\u53ca\u4e3b\u6d41VLM\u5728\u57fa\u51c6\u4e0a\u505a\u7cfb\u7edf\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u4e3b\u6d41VLM\uff08\u5305\u62ec\u9065\u611f\u9886\u57df\u4e13\u7528\u6a21\u578b\uff09\u5728\u591a\u4efb\u52a1\u4e0a\u666e\u904d\u5b58\u5728\u8f83\u5927\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u635f\u6bc1\u5224\u5b9a\u548c\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002DI-Chat\u5728\u707e\u5bb3\u7ea7\u522b\u3001\u7c7b\u578b\u5206\u7c7b\u53ca\u62a5\u544a\u751f\u6210\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5efa\u7b51\u529f\u80fd\u5206\u7c7b\u4efb\u52a1\u5bf9\u6240\u6709\u6a21\u578b\u4ecd\u5177\u8f83\u5927\u6311\u6218\u3002", "conclusion": "DisasterInsight\u4e3a\u707e\u5bb3\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u8bc4\u6d4b\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8VLM\u5728\u5b9e\u9645\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\u4e0e\u7814\u7a76\u3002"}}
{"id": "2601.18532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18532", "abs": "https://arxiv.org/abs/2601.18532", "authors": ["Devon Levy", "Bar Assayag", "Laura Gaspar", "Ilan Shimshoni", "Bella Specktor-Fadida"], "title": "From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation", "comment": "19 pages without references", "summary": "Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u4e3b\u52a8\u5b66\u4e60\u51b7\u542f\u52a8\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u4e0e\u805a\u7c7b\u65b9\u6cd5\uff0c\u81ea\u52a8\u786e\u5b9a\u805a\u7c7b\u6570\u5e76\u5b9e\u73b0\u6bd4\u4f8b\u91c7\u6837\u4ee5\u6784\u5efa\u591a\u6837\u5316\u8bad\u7ec3\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6570\u636e\u91cf\u60c5\u5f62\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8017\u65f6\uff0c\u5f71\u54cd\u75be\u75c5\u76d1\u6d4b\u7684\u6548\u7387\u3002\u4e3b\u52a8\u5b66\u4e60\u80fd\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u51b7\u542f\u52a8\u9636\u6bb5\u5982\u4f55\u9ad8\u6548\u91c7\u6837\u4ee3\u8868\u6027\u6837\u672c\u4ecd\u662f\u6311\u6218\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4f18\u5316\u51b7\u542f\u52a8\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u9ad8\u5206\u5272\u6a21\u578b\u6027\u80fd\u3002", "method": "\u65b0\u65b9\u6cd5\u5c06\u57fa\u7840\u6a21\u578b\uff08\u5982\u5927\u6a21\u578b\uff09\u7684\u7279\u5f81\u5d4c\u5165\u4e0e\u805a\u7c7b\u7b97\u6cd5\u7ed3\u5408\uff0c\u81ea\u52a8\u9009\u5b9a\u805a\u7c7b\u6570\u5e76\u6309\u6bd4\u4f8b\u91c7\u6837\uff0c\u786e\u4fdd\u521d\u59cb\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u3002\u4e4b\u540e\u91c7\u7528\u6574\u5408\u7a7a\u95f4\u591a\u6837\u6027\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6837\u672c\u9009\u62e9\uff0c\u5e76\u53ef\u53ef\u89c6\u5316\u5019\u9009\u6837\u672c\u5728\u7279\u5f81\u7a7a\u95f4\u7684\u5206\u5e03\u3002", "result": "\u5728X-ray\u548cMRI\u7684\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u65b9\u6848\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u4e0e\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u57fa\u7ebf\u3002\u4ee5CheXmask\u4e3a\u4f8b\uff0cDice\u7cfb\u6570\u4ece0.918\u63d0\u9ad8\u52300.929\uff0cHausdorff\u8ddd\u79bb\u4ece32.41mm\u964d\u81f327.66mm\uff1b\u4e3b\u52a8\u5b66\u4e60\u9636\u6bb5Dice\u4ece0.919\u5347\u81f30.939\uff0cHausdorff\u8ddd\u79bb\u964d\u523019.16mm\u3002\u5176\u4ed6\u6570\u636e\u96c6\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u63d0\u5347\u3002", "conclusion": "\u8be5\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u5728\u4f4e\u6807\u6ce8\u6570\u636e\u60c5\u666f\u4e0b\u80fd\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5bf9\u5b9e\u9645\u8f85\u52a9\u8bca\u65ad\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.18543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18543", "abs": "https://arxiv.org/abs/2601.18543", "authors": ["Kaixun Jiang", "Yuzheng Wang", "Junjie Zhou", "Pandeng Li", "Zhihang Liu", "Chen-Wei Xie", "Zhaoyu Chen", "Yun Zheng", "Wenqiang Zhang"], "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning", "comment": null, "summary": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.", "AI": {"tldr": "GenAgent\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ee3\u7406\u6846\u67b6\u89e3\u8026\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u7684\u65b0\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8de8\u5de5\u5177\u6cdb\u5316\u3001\u591a\u8f6e\u81ea\u6cbb\u63a8\u7406\u4e0e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7edf\u4e00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0e\u751f\u6210\u4e0a\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u4e24\u8005\u6743\u8861\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u7a81\u7834\u8fd9\u4e9b\u74f6\u9888\u3002", "method": "GenAgent\u91c7\u7528\u4ee3\u7406\u5f0f\u67b6\u6784\uff0c\u5c06\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u5206\u79bb\uff1a\u7406\u89e3\u7531\u591a\u6a21\u6001\u6a21\u578b\u5b8c\u6210\uff0c\u751f\u6210\u4ea4\u7531\u53ef\u8c03\u7528\u7684\u751f\u6210\u6a21\u578b\u5de5\u5177\u5b8c\u6210\u3002\u6a21\u578b\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u6709\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5b66\u4e60\u5982\u4f55\u81ea\u4e3b\u8c03\u7528\u5de5\u5177\u3001\u591a\u8f6e\u63a8\u7406\u3001\u53cd\u601d\u548c\u9012\u8fdb\u4f18\u5316\u8f93\u51fa\u3002\u5f3a\u5316\u5b66\u4e60\u4e2d\u7ed3\u5408\u6700\u7ec8\u56fe\u7247\u8d28\u91cf\u3001\u53cd\u601d\u51c6\u786e\u5ea6\u7b49\u5956\u52b1\uff0c\u901a\u8fc7\u8def\u5f84\u91cd\u91c7\u6837\u63d0\u5347\u5e8f\u5217\u63a2\u7d22\u80fd\u529b\u3002", "result": "GenAgent\u5728GenEval++\u548cWISE\u7b49\u751f\u6210\u8bc4\u6d4b\u4e2d\uff0c\u5206\u522b\u63d0\u5347\u4e8623.6%\u548c14%\uff0c\u5728\u8de8\u5de5\u5177\u6cdb\u5316\u3001\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u80fd\u6839\u636e\u4efb\u52a1\u81ea\u52a8\u8c03\u6574\u63a8\u7406\u6d41\u7a0b\u3002", "conclusion": "GenAgent\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u8fd8\u5b9e\u73b0\u4e86\u67b6\u6784\u901a\u7528\u6027\u3001\u4ea4\u4e92\u81ea\u9002\u5e94\u548c\u5de5\u5177\u6cdb\u5316\uff0c\u4e3a\u540e\u7eed\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.18547", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18547", "abs": "https://arxiv.org/abs/2601.18547", "authors": ["Qing Ding", "Mai Xu", "Shengxi Li", "Xin Deng", "Xin Zou"], "title": "REMAC: Reference-Based Martian Asymmetrical Image Compression", "comment": "Accepted for publication in IEEE Transactions on Geoscience and Remote Sensing (TGRS). 2025 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. 18 pages, 20 figures", "summary": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u706b\u661f\u56fe\u50cf\u7684\u53c2\u8003\u56fe\u50cf\u5f15\u5bfc\u975e\u5bf9\u79f0\u538b\u7f29\u65b9\u6cd5REMAC\uff0c\u5728\u4fdd\u8bc1\u9ad8\u538b\u7f29\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u706b\u661f\u7aef\u7684\u7f16\u7801\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u53d7\u9650\u4f20\u8f93\u548c\u8fd0\u7b97\u573a\u666f\u3002", "motivation": "\u706b\u661f\u5230\u5730\u7403\u7684\u901a\u4fe1\u5e26\u5bbd\u53d7\u9650\uff0c\u9700\u8981\u9ad8\u6548\u7684\u56fe\u50cf\u538b\u7f29\u6280\u672f\u3002\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u4e00\u662f\u672a\u8003\u8651\u706b\u661f\u7aef\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u4e8c\u662f\u672a\u5229\u7528\u706b\u661f\u56fe\u50cf\u95f4\u5f3a\u76f8\u5173\u6027\u7684\u7279\u70b9\u3002", "method": "\u63d0\u51faREMAC\u65b9\u6cd5\uff0c\u5c06\u5927\u90e8\u5206\u8ba1\u7b97\u4ece\u7f16\u7801\u7aef\uff08\u706b\u661f\uff09\u8f6c\u79fb\u5230\u89e3\u7801\u7aef\uff08\u5730\u7403\uff09\uff0c\u5229\u7528\u53c2\u8003\u56fe\u50cf\u6a21\u5757\u548cref-decoder\uff0c\u5206\u522b\u901a\u8fc7\u8de8\u56fe\u50cf\u548c\u56fe\u50cf\u5185\u90e8\u7684\u76f8\u4f3c\u6027\u63d0\u5347\u538b\u7f29\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u3001\u6df1\u5927\u611f\u53d7\u91ce\u67b6\u6784\u548c\u6f5c\u7279\u5f81\u5faa\u73af\u673a\u5236\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u4e0e\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0cREMAC\u80fd\u51cf\u5c1143.51%\u7684\u7f16\u7801\u7aef\u590d\u6742\u5ea6\uff0c\u540c\u65f6BD-PSNR\u63d0\u53470.2664 dB\u3002", "conclusion": "REMAC\u65b9\u6cd5\u517c\u987e\u4e86\u8d44\u6e90\u53d7\u9650\u548c\u9ad8\u538b\u7f29\u6bd4\u7684\u9700\u6c42\uff0c\u662f\u706b\u661f\u56fe\u50cf\u8fdc\u7a0b\u4f20\u8f93\u7684\u66f4\u4f18\u9009\u62e9\uff0c\u5bf9\u661f\u9645\u901a\u4fe1\u548c\u65e0\u4eba\u63a2\u6d4b\u4efb\u52a1\u5177\u6709\u63a8\u52a8\u610f\u4e49\u3002"}}
{"id": "2601.18555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18555", "abs": "https://arxiv.org/abs/2601.18555", "authors": ["Roberto Di Via", "Vito Paolo Pastore", "Francesca Odone", "Si\u00f4n Glyn-Jones", "Irina Voiculescu"], "title": "Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray", "comment": "Accepted at International Symposium on Biomedical Imaging (ISBI 2026)", "summary": "Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86X\u5149\u548cMRI\u5728\u80a1\u9aa8\u9acb\u81fc\u649e\u51fb\u75c7\uff08FAI\uff09\u7b5b\u67e5\u4e2d\u57fa\u4e8e\u89d2\u5ea6\u6d4b\u91cf\u7684\u7b49\u6548\u6027\uff0c\u53d1\u73b0MRI\u53ef\u8fbe\u5230\u4e0eX\u5149\u7c7b\u4f3c\u7684\u4e34\u5e8a\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfFAI\u7b5b\u67e5\u4f9d\u8d56X\u5149\u89d2\u5ea6\u6d4b\u91cf\uff0c\u4f46X\u5149\u65e0\u6cd5\u63d0\u4f9b\u649e\u51fb\u533a\u57df\u9ad8\u5ea6\u4e0e\u8de8\u5ea6\u7684\u4e09\u7ef4\u4fe1\u606f\uff0c\u800cMRI\u53ef\u4ee5\u8865\u5145\u8fd9\u4e00\u7ef4\u5ea6\u3002\u786e\u4fdd\u4e0d\u540c\u6210\u50cf\u65b9\u5f0f\u6d4b\u91cf\u6307\u6807\u7684\u7b49\u6548\u6027\u5bf9\u533b\u751f\u51b3\u7b56\u548c\u81ea\u52a8\u5316\u8bca\u65ad\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u62db\u52df\u4e8689\u540d\u60a3\u8005\uff0c\u5bf9\u5176\u8fdb\u884c\u5339\u914d\u7ec4\u8bbe\u8ba1\uff0c\u540c\u65f6\u6536\u96c6MRI\u548cX\u5149\uff0c\u91c7\u7528\u6807\u51c6\u70ed\u529b\u56fe\u56de\u5f52\u65b9\u6cd5\u5bf9\u4e24\u79cd\u6210\u50cf\u7684\u89e3\u5256\u6807\u5fd7\u70b9\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u65b9\u5f0f\u7684\u5b9a\u4f4d\u51c6\u786e\u7387\u548c\u8bca\u65ad\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMRI\u5728\u68c0\u6d4bcam\u578b\u649e\u51fb\u75c7\u5173\u952e\u6807\u5fd7\u70b9\u548c\u4e34\u5e8a\u8bca\u65ad\u4e0a\u4e0eX\u5149\u7b49\u6548\uff0c\u652f\u6301MRI\u5728FAI\u7b5b\u67e5\u9886\u57df\u7684\u4e34\u5e8a\u53ef\u884c\u6027\u3002", "conclusion": "MRI\u4e0d\u4ec5\u80fd\u7b49\u6548\u66ff\u4ee3X\u5149\u63d0\u4f9bFAI\u8bca\u65ad\u7684\u5fc5\u8981\u6d4b\u91cf\uff0c\u8fd8\u80fd\u652f\u6301\u4e09\u7ef4\u4f53\u79ef\u5206\u6790\uff0c\u6709\u6f5c\u529b\u4f5c\u4e3a\u5e38\u89c4\u81ea\u52a8\u5316FAI\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8MRI\u5728\u4e34\u5e8a\u8bca\u65ad\u6d41\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2601.18556", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18556", "abs": "https://arxiv.org/abs/2601.18556", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis", "comment": null, "summary": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SDA-QEC\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u4e0e\u91cf\u5b50\u7279\u5f81\u5f3a\u5316\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6548\u679c\uff0c\u7279\u522b\u9488\u5bf9\u6837\u672c\u6781\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff08\u5982\u80f8\u7247\u4e2d\u7684\u80ba\u708e\u68c0\u6d4b\u3001\u4e73\u817a\u764c\u7b5b\u67e5\uff09\u5e38\u89c1\u7c7b\u522b\u6781\u5ea6\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u5bf9\u5c11\u6570\u7c7b\u8bc6\u522b\u6548\u679c\u5dee\uff0c\u4e0d\u5229\u4e8e\u4e34\u5e8a\u5b89\u5168\u3002\u5982\u4f55\u6539\u5584\u6781\u5c11\u6570\u6837\u672c\u60c5\u51b5\u4e0b\u7684\u6a21\u578b\u8868\u73b0\uff0c\u662f\u5b9e\u9645\u5e94\u7528\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSDA-QEC\u6846\u67b6\uff1a1\uff09\u7528\u8f7b\u91cf\u5316\u6269\u6563\u5f0f\u6570\u636e\u589e\u5f3a\u5668\u4e3a\u5c11\u6570\u7c7b\u522b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\uff0c\u91cd\u5e73\u8861\u6837\u672c\u5206\u5e03\uff1b2\uff09\u5728MobileNetV2\u7f51\u7edc\u4e2d\u878d\u5408\u91cf\u5b50\u7279\u5f81\u5c42\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u9ad8\u7ef4\u6620\u5c04\u589e\u5f3a\u6a21\u578b\u533a\u5206\u529b\u3002", "result": "\u5728\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u56fe\u50cf\u5206\u7c7b\u5b9e\u9a8c\u4e2d\uff0cSDA-QEC\u83b7\u5f97\u4e8698.33%\u51c6\u786e\u7387\u300198.78%AUC\u548c98.33%F1\u5206\u6570\uff0c\u663e\u8457\u8d85\u8d8aResNet18\u3001MobileNetV2\u3001DenseNet121\u3001VGG16\u7b49\u57fa\u7ebf\u3002\u5176\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u5747\u4e3a98.33%\uff0c\u5c55\u73b0\u51fa\u51fa\u8272\u7684\u5747\u8861\u6027\u80fd\u3002", "conclusion": "SDA-QEC\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u4e0e\u91cf\u5b50\u7279\u5f81\u5efa\u6a21\uff0c\u4e3a\u6781\u5c11\u6837\u672c\u3001\u9ad8\u98ce\u9669\u7684\u533b\u5b66AI\u7cfb\u7edf\u7814\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5728\u771f\u5b9e\u533b\u5b66\u5f71\u50cf\u4e2d\u5c55\u73b0\u4e86\u6781\u9ad8\u6f5c\u529b\u3002"}}
{"id": "2601.18560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18560", "abs": "https://arxiv.org/abs/2601.18560", "authors": ["Li Fang", "Tianyu Li", "Yanghong Lin", "Shudong Zhou", "Wei Yao"], "title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging", "comment": null, "summary": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684AI\u8d4b\u80fd\u536b\u661f\u8fb9\u7f18\u8ba1\u7b97\u8303\u5f0f\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u5b9e\u73b0\u536b\u661f\u81ea\u4e3b\u51b3\u7b56\uff0c\u5e76\u89e3\u51b3\u4e86\u661f\u8f7d\u8d44\u6e90\u53d7\u9650\u4e0e\u6570\u636e\u4e0b\u884c\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u9ad8\u5149\u8c31\u536b\u661f\u6210\u50cf\u63d0\u4f9b\u4e30\u5bcc\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u4e0b\u884c\u4f20\u8f93\u901f\u7387\u53d7\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u5982\u707e\u5bb3\u76d1\u6d4b\u3001\u5e94\u6025\u5236\u56fe\u7b49\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u5e94\u7528\u3002\u661f\u8f7d\u5904\u7406\u662f\u89e3\u51b3\u74f6\u9888\u7684\u6709\u6548\u9014\u5f84\uff0c\u4f46\u9700\u517c\u987e\u7b97\u529b\u53d7\u9650\u3001\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u975e\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u6b21\u5b66\u4e60\uff08few-shot learning\uff09\u9ad8\u5149\u8c31\u5206\u7c7b\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u50cf\u7d20\u7ea7\u6807\u7b7e\u4f20\u64ad\u673a\u5236\u3002\u9996\u5148\uff0c\u901a\u8fc7\u951a\u70b9\u50cf\u7d20\u4e0e\u76ee\u6807\u50cf\u7d20\u6784\u5efa\u4eb2\u548c\u77e9\u9635\uff0c\u4f20\u64ad\u83b7\u53d6\u521d\u59cb\u6807\u7b7e\uff1b\u7136\u540e\uff0c\u8ba1\u7b97\u50cf\u7d20\u95f4\u76f8\u4f3c\u6027\u751f\u6210\u7a00\u758f\u56fe\uff0c\u5229\u7528\u95ed\u5f0f\u89e3\u4ee3\u66ff\u8fed\u4ee3\u3002\u951a\u70b9\u6807\u7b7e\u7531\u79e9\u7ea6\u675f\u56fe\u805a\u7c7b\u7b97\u6cd5\u81ea\u52a8\u786e\u5b9a\u3002", "result": "\u65b9\u6cd5\u65e0\u9700\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u4ec5\u7528\u50cf\u7d20\u5185\u5149\u8c31\u7279\u5f81\uff0c\u80fd\u9002\u5e94\u4f20\u611f\u5668\u5931\u6548\u3001\u626b\u63cf\u8bef\u5dee\u7b49\u5bfc\u81f4\u7684\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u661f\u8f7d\u5206\u7c7b\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u661f\u8f7d\u8ba1\u7b97\u8303\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u4e3a\u536b\u661f\u81ea\u4e3b\u51b3\u7b56\u548c\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u529b\u6280\u672f\u652f\u6491\uff0c\u5177\u6709\u73b0\u5b9e\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.18577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18577", "abs": "https://arxiv.org/abs/2601.18577", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Saining Xie", "Jaehong Yoon", "Sung Ju Hwang"], "title": "Self-Refining Video Sampling", "comment": "Project page: https://agwmon.github.io/self-refine-video/", "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u73b0\u6709\u89c6\u9891\u751f\u6210\u5668\u57fa\u7840\u4e0a\u7684\u81ea\u6211\u7ec6\u5316\u91c7\u6837\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u5c31\u80fd\u6709\u6548\u63d0\u5347\u7269\u7406\u52a8\u6001\u771f\u5b9e\u6027\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u89c6\u9891\u751f\u6210\u5668\u5728\u590d\u6742\u7269\u7406\u52a8\u6001\u4e0b\u751f\u6210\u7684\u89c6\u9891\u5f80\u5f80\u7f3a\u4e4f\u771f\u5b9e\u611f\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u4f7f\u7528\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u989d\u5916\u8bad\u7ec3\u589e\u5f3a\u6570\u636e\uff0c\u65e2\u8ba1\u7b97\u91cf\u5927\u53c8\u96be\u4ee5\u7ec6\u81f4\u6355\u6349\u52a8\u4f5c\u7ec6\u8282\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u5668\u672c\u8eab\u89c6\u4e3a\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u5728\u63a8\u7406\u65f6\u5185\u5faa\u73af\u8fed\u4ee3\u7ec6\u5316\u751f\u6210\u7ed3\u679c\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u989d\u5916\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\uff0c\u9488\u5bf9\u81ea\u4e00\u81f4\u6027\u4f4e\u7684\u533a\u57df\u6709\u9009\u62e9\u5730\u7ec6\u5316\uff0c\u907f\u514d\u8fc7\u5ea6\u7ec6\u5316\u4ea7\u751f\u4f2a\u5f71\u3002", "result": "\u5728\u591a\u79cd\u4e3b\u6d41\u89c6\u9891\u751f\u6210\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u89c6\u9891\u7684\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u7528\u6237\u504f\u597d\u5ea6\u8d85\u8fc770%\uff0c\u8d85\u8d8a\u9ed8\u8ba4\u91c7\u6837\u5668\u548c\u57fa\u4e8e\u5f15\u5bfc\u7684\u91c7\u6837\u5668\u3002", "conclusion": "\u6240\u63d0\u81ea\u6211\u7ec6\u5316\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u6539\u5584\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u548c\u611f\u77e5\u8d28\u91cf\u3002"}}
{"id": "2601.18585", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18585", "abs": "https://arxiv.org/abs/2601.18585", "authors": ["Chenxi Liu", "Selena Ling", "Alec Jacobson"], "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization", "comment": null, "summary": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGimmBO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u6269\u6563\u5f0f\u56fe\u50cf\u751f\u6210\u4e2d\u9002\u914d\u5668\u5408\u5e76\u7684\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u76ee\u524d\uff0c\u57fa\u4e8e\u5fae\u8c03\u7684\u6269\u6563\u5f0f\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u9002\u914d\u5668\u5b9e\u73b0\u5bf9\u4e0d\u540c\u4e3b\u9898\u548c\u98ce\u683c\u7684\u81ea\u5b9a\u4e49\u3002\u591a\u4e2a\u9002\u914d\u5668\u5408\u5e76\u540e\uff0c\u53ef\u751f\u6210\u4e30\u5bcc\u7684\u89c6\u89c9\u7ed3\u679c\u3002\u7136\u800c\uff0c\u901a\u8fc7\u624b\u52a8\u6ed1\u5757\u8c03\u53c2\u63a2\u7d22\u9002\u914d\u5668\u6743\u91cd\u7a7a\u95f4\uff0c\u4e0d\u4f46\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u9002\u914d\u5668\u6570\u91cf\u589e\u52a0\u540e\u96be\u4ee5\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u4f18\u5316\u624b\u6bb5\u3002", "method": "\u4f5c\u8005\u63d0\u51faGimmBO\u6846\u67b6\uff0c\u57fa\u4e8e\u504f\u597d\u578b\u8d1d\u53f6\u65af\u4f18\u5316\uff08PBO\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8d1d\u53f6\u65af\u4f18\u5316\u673a\u5236\uff0c\u9488\u5bf9\u5408\u5e76\u9002\u914d\u5668\u6743\u91cd\u7a00\u758f\u548c\u6743\u91cd\u8303\u56f4\u53d7\u9650\u7b49\u5b9e\u9645\u95ee\u9898\uff0c\u63d0\u5347\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u91c7\u6837\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u7528\u6237\u548c\u771f\u5b9e\u7528\u6237\u7814\u7a76\uff0cGimmBO\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u7ebf\u6027\u641c\u7d22\u65b9\u6cd5\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u7a33\u5b9a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u9ad8\u5ea6\u7075\u6d3b\u6027\u3002", "conclusion": "GimmBO\u4e3a\u6269\u6563\u5f0f\u56fe\u50cf\u751f\u6210\u9002\u914d\u5668\u6743\u91cd\u5408\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u63a2\u7d22\u548c\u4f18\u5316\u65b9\u6848\uff0c\u5e76\u53ef\u9002\u914d\u591a\u79cd\u6269\u5c55\u9700\u6c42\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.18589", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18589", "abs": "https://arxiv.org/abs/2601.18589", "authors": ["KV Karthikeya", "Ashok Kumar Das", "Shantanu Pal", "Vivekananda Bhat K", "Arun Sekar Rajasekaran"], "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment", "comment": null, "summary": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AGSP-DSA\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u6587\u672c\u3001\u97f3\u9891\u548c\u56fe\u50cf\u7b49\u5f02\u6784\u6570\u636e\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u6001\u95f4\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u878d\u5408\u4e0e\u5bf9\u9f50\u65b9\u5f0f\u3002", "method": "1. \u6784\u5efa\u53cc\u56fe\uff08dual-graph\uff09\uff0c\u5206\u522b\u5b66\u4e60\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u5173\u7cfb\uff1b2. \u91c7\u7528\u8c31\u56fe\u6ee4\u6ce2\u5f3a\u5316\u6709\u7528\u4fe1\u53f7\uff1b3. \u5229\u7528\u591a\u5c3a\u5ea6\u56fe\u5377\u79ef\u7f51\u7edc\u83b7\u5f97\u8282\u70b9\u5d4c\u5165\uff1b4. \u5f15\u5165\u8bed\u4e49\u611f\u77e5\u6ce8\u610f\u673a\u5236\uff0c\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u5206\u914d\u5404\u6a21\u6001\u8d21\u732e\u3002", "result": "AGSP-DSA\u5728CMU-MOSEI\u3001AVE\u548cMM-IMDB\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u66f4\u597d\u7684\u8868\u73b0\uff0c\u5982CMU-MOSEI\u4e0a\u8fbe\u5230\u4e8695.3%\u51c6\u786e\u7387\u30010.936\u7684F1\u548c0.924\u7684mAP\uff0c\u8f83MM-GNN\u63d0\u53472.6%\uff1b\u540c\u65f6\u5728\u7f3a\u5931\u6a21\u6001\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AGSP-DSA\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u878d\u5408\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u60c5\u611f\u5206\u6790\u3001\u4e8b\u4ef6\u8bc6\u522b\u4ee5\u53ca\u591a\u5a92\u4f53\u5206\u7c7b\u7b49\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18597", "abs": "https://arxiv.org/abs/2601.18597", "authors": ["Yu Xia", "Chang Liu", "Tianqi Xiang", "Zhigang Tu"], "title": "EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery", "comment": null, "summary": "Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \\textbf{1.6}\\% and \\textbf{5.8}\\% in AP and AP$_{s}$ on VisDrone, while obtaining \\textbf{188} FPS inference speed on a single RTX 4090 GPU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEFSI-DETR\u7684\u65b0\u578b\u65e0\u4eba\u673a\u5c0f\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u7684\u7279\u5f81\u589e\u5f3a\u548c\u52a8\u6001\u9891\u7387-\u7a7a\u95f4\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u65e0\u4eba\u673a\u5f71\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u56e0\u7279\u5f81\u8868\u5f81\u6709\u9650\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u4ecd\u5177\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u9891\u7387\u4fe1\u606f\u3001\u4f9d\u8d56\u9759\u6001\u5377\u79ef\uff0c\u5bfc\u81f4\u6df1\u5c42\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u52a8\u6001\u7684\u4fe1\u606f\u878d\u5408\u548c\u7279\u5f81\u63d0\u53d6\u673a\u5236\u3002", "method": "\u63d0\u51faEFSI-DETR\u68c0\u6d4b\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) DyFusNet\uff0c\u8054\u5408\u5229\u7528\u9891\u7387\u4e0e\u7a7a\u95f4\u4fe1\u606f\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff1b2) ESFC\uff0c\u6709\u6548\u63d0\u53d6\u6df1\u5c42\u8bed\u4e49\u7279\u5f81\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002\u6b64\u5916\u5f15\u5165\u7ec6\u7c92\u5ea6\u7279\u5f81\u4fdd\u7559\uff08FFR\uff09\u7b56\u7565\uff0c\u5c06\u7a7a\u95f4\u4e30\u5bcc\u7684\u6d45\u5c42\u4fe1\u606f\u5728\u878d\u5408\u8fc7\u7a0b\u4e2d\u4fdd\u7559\uff0c\u6539\u5584\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002", "result": "\u5728VisDrone\u548cCODrone\u6570\u636e\u96c6\u4e0a\uff0cEFSI-DETR\u6846\u67b6\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86AP\u63d0\u53471.6%\u3001AP_s\u63d0\u53475.8%\uff0c\u63a8\u7406\u901f\u7387\u8fbe\u5230188 FPS\uff08RTX 4090\uff09\u3002", "conclusion": "EFSI-DETR\u901a\u8fc7\u9891\u7387-\u7a7a\u95f4\u8054\u5408\u5f15\u5bfc\u548c\u9ad8\u6548\u8bed\u4e49\u96c6\u4e2d\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\uff0c\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u6700\u4f18\u6c34\u5e73\u3002"}}
{"id": "2601.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18619", "abs": "https://arxiv.org/abs/2601.18619", "authors": ["Jorge Quesada", "Ghassan AlRegib"], "title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.", "AI": {"tldr": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u6807\u7b7e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u662f\u4e00\u79cd\u6709\u6548\u8868\u793a\u5b66\u4e60\u7b56\u7565\uff0c\u4f46\u5bf9\u5c0f\u76ee\u6807\u5206\u5272\u6548\u679c\u6b20\u4f73\u3002\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5c0f\u7a97\u53e3\u88c1\u526a\u589e\u5f3a\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u63d0\u5347\u5c0f\u3001\u7a00\u758f\u3001\u4e0d\u89c4\u5219\u76ee\u6807\u7684\u5206\u5272\u8868\u73b0\uff0c\u5e76\u5728\u5730\u9707\u548c\u795e\u7ecf\u6210\u50cf\u9886\u57df\u7ecf\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5272\u5c0f\u578b\u3001\u7a00\u758f\u6216\u5c40\u90e8\u4e0d\u89c4\u5219\u76ee\u6807\u65f6\u6548\u679c\u4e0d\u7406\u60f3\u3002\u79d1\u5b66\u5f71\u50cf\u4efb\u52a1\u5982\u5730\u9707\u65ad\u5c42\u548c\u795e\u7ecf\u7ec6\u80de\u5206\u5272\uff0c\u5e38\u89c1\u4e0a\u8ff0\u7279\u5f81\uff0c\u9700\u5bf9SSL\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\u4ee5\u63d0\u5347\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u5c3a\u5ea6\u611f\u77e5\u7684SSL\u9002\u914d\u65b9\u6cd5\uff0c\u5728\u9884\u8bad\u7ec3\u7684\u6570\u636e\u589e\u5f3a\u9636\u6bb5\u5f15\u5165\u5c0f\u7a97\u53e3\u88c1\u526a\u6280\u672f\uff0c\u4f7f\u6a21\u578b\u66f4\u4e13\u6ce8\u4e8e\u7ec6\u7c92\u5ea6\u7ed3\u6784\uff0c\u5e76\u5728\u5730\u9707\u6210\u50cf\u548c\u795e\u7ecf\u6210\u50cf\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u65ad\u5c42\u5206\u5272\u548c\u795e\u7ecf\u7ec6\u80de\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u6bd4\u6807\u51c6\u548c\u6700\u65b0SSL\u57fa\u7ebf\u63d0\u5347\u65ad\u5c42\u5206\u5272\u51c6\u786e\u7387\u6700\u9ad8\u53ef\u8fbe13%\uff0c\u7ec6\u80de\u8fb9\u754c\u5206\u5272\u63d0\u5347\u8fbe5%\u3002\u5bf9\u5927\u5c3a\u5ea6\u76ee\u6807\u5206\u5272\u63d0\u5347\u6709\u9650\u3002", "conclusion": "SSL\u5728\u76ee\u6807\u5206\u5272\u4e2d\u7684\u6548\u679c\u4f9d\u8d56\u76ee\u6807\u7684\u5c3a\u5ea6\u548c\u7a00\u758f\u6027\uff1b\u9488\u5bf9\u5c0f\u5c3a\u5ea6\u76ee\u6807\u5f15\u5165\u7ec6\u7c92\u5ea6\u589e\u5f3a\u80fd\u663e\u8457\u63d0\u5347\u8868\u73b0\u3002\u5efa\u8baeSSL\u65b9\u6848\u8bbe\u8ba1\u9700\u6839\u636e\u76ee\u6807\u7279\u6027\u8c03\u6574\uff0c\u4e3a\u79d1\u5b66\u5f71\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.18623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18623", "abs": "https://arxiv.org/abs/2601.18623", "authors": ["Zihao Wang", "Yuzhou Chen", "Shaogang Ren"], "title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "comment": "Paper accepted as a conference paper at ICLR 2026", "summary": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7a7a\u95f4\u53d8\u5316\u7684\u6df7\u5408\u573a\u548c\u663e\u5f0f\u7684\u4fee\u590d\u9879\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff08\u5982\u6807\u51c6\u6269\u6563\u65b9\u6cd5\uff09\u901a\u5e38\u4f7f\u7528\u5168\u5c40\u7ebf\u6027\u8f6c\u6362\uff0c\u8fd9\u4f1a\u8ba9\u91c7\u6837\u8fc7\u7a0b\u7ecf\u8fc7\u4e0d\u5408\u7406\u7684\u9ad8\u4ee3\u4ef7\u533a\u57df\uff0c\u9020\u6210\u8bed\u4e49\u504f\u79fb\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u4f5c\u8005\u5c06\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u56fa\u5b9a\u8fdb\u5ea6\u57df\u8f6c\u79fb\uff0c\u4e9f\u9700\u66f4\u7075\u6d3b\u3001\u6709\u6548\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u521b\u65b0\u6027\u5730\u5c06\u57df\u5207\u6362\u52a8\u6001\u5d4c\u5165\u5230\u6269\u6563\u751f\u6210\u8fc7\u7a0b\uff1a\u5728\u6bcf\u4e00\u6b65\u53cd\u5411\u6269\u6563\u4e2d\u9884\u6d4b\u7a7a\u95f4\u53d8\u5316\u7684\u6df7\u5408\u573a\uff0c\u5e76\u4e3a\u6f02\u79fb\u9879\u6ce8\u5165\u4e0e\u76ee\u6807\u4e00\u81f4\u7684\u4fee\u590d\u9879\uff0c\u5b9e\u73b0\u9010\u6b65\u3001\u663e\u5f0f\u7684\u5f15\u5bfc\u3002\u8fd9\u79cd\u65b9\u6cd5\u5f3a\u8c03\u5c40\u90e8\u6b8b\u5dee\u4fee\u6b63\u800c\u975e\u5168\u5c40\u5bf9\u9f50\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u8fde\u7eed\u65f6\u95f4\u7684\u7cbe\u786e\u89e3\u548c\u5b9e\u7528\u7684\u4e00\u9636\u62bd\u6837\u5668\uff0c\u786e\u4fdd\u4e86\u8fb9\u7f18\u4e00\u81f4\u6027\u3002", "result": "\u5728\u533b\u5b66\u6210\u50cf\u3001\u9065\u611f\u548c\u7535\u81f4\u53d1\u5149\u8bed\u4e49\u6620\u5c04\u7b49\u591a\u4e2a\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u6846\u67b6\u80fd\u63d0\u5347\u7ed3\u6784\u4fdd\u771f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e14\u53ea\u9700\u66f4\u5c11\u7684\u53bb\u566a\u6b65\u9aa4\u5373\u53ef\u6536\u655b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u907f\u514d\u4e86\u6807\u51c6\u6269\u6563\u65b9\u6cd5\u4e2d\u7684\u9ad8\u4ee3\u4ef7\u3001\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u80fd\u9ad8\u6548\u4e14\u66f4\u51c6\u786e\u5730\u5b8c\u6210\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\uff0c\u5177\u6709\u5b9e\u9645\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2601.18625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18625", "abs": "https://arxiv.org/abs/2601.18625", "authors": ["Zequn Xie"], "title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search", "comment": "Accepted by ICASSP 2026", "summary": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.", "AI": {"tldr": "CONQUER\u662f\u4e00\u79cd\u4e3a\u6587\u5b57\u63cf\u8ff0-\u884c\u4eba\u68c0\u7d22\uff08TBPS\uff09\u4efb\u52a1\u8bbe\u8ba1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u597d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dTBPS\u4efb\u52a1\u56e0\u8de8\u6a21\u6001\u5dee\u5f02\u5927\u548c\u7528\u6237\u67e5\u8be2\u542b\u7cca\u800c\u96be\u4ee5\u5e94\u5bf9\u5b9e\u9645\u9700\u6c42\uff0c\u5bfc\u81f4\u68c0\u7d22\u51c6\u786e\u7387\u6709\u9650\u3002\u7814\u7a76\u8005\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u5bf9\u4e0d\u540c\u6a21\u6001\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f18\u5316\u7528\u6237\u67e5\u8be2\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u8bad\u7ec3\u548c\u63a8\u7406\u4e24\u4e2a\u9636\u6bb5\u3002\u8bad\u7ec3\u65f6\uff0cCONQUER\u7ed3\u5408\u591a\u7c92\u5ea6\u7f16\u7801\u3001\u4e92\u8865\u914d\u5bf9\u6316\u6398\u548c\u4ee5\u6700\u4f18\u4f20\u8f93\u4e3a\u57fa\u7840\u7684\u4e0a\u4e0b\u6587\u5f15\u5bfc\u5339\u914d\uff0c\u63d0\u5347\u5d4c\u5165\u8868\u5f81\u80fd\u529b\u3002\u63a8\u7406\u65f6\uff0c\u91c7\u7528\u63d2\u62d4\u5f0f\u7684\u67e5\u8be2\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u951a\u70b9\u9009\u62e9\u548c\u5c5e\u6027\u8865\u5168\u673a\u5236\u4f18\u5316\u539f\u59cb\u63cf\u8ff0\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e3b\u5e72\u7f51\u7edc\u3002", "result": "\u8be5\u65b9\u6cd5\u5728CUHK-PEDES\u3001ICFG-PEDES\u548cRSTPReid\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86Rank-1\u51c6\u786e\u7387\u548cmAP\uff0c\u5728\u8de8\u57df\u548c\u4e0d\u5b8c\u6574\u67e5\u8be2\u573a\u666f\u4e0b\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "CONQUER\u80fd\u591f\u6709\u6548\u89e3\u51b3TBPS\u4e2d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u67e5\u8be2\u6b67\u4e49\u95ee\u9898\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\uff0c\u4e3a\u5b9e\u9645\u516c\u5171\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2601.18633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18633", "abs": "https://arxiv.org/abs/2601.18633", "authors": ["Tong Shi", "Melonie de Almeida", "Daniela Ivanova", "Nicolas Pugeault", "Paul Henderson"], "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting", "comment": null, "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u55b7\u6d12\uff08Gaussian Splatting\uff09\u65b9\u6cd5\u7684\u4e09\u7ef4\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u7b97\u6cd5Splat-Portrait\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5934\u50cf\u52a8\u753b\u7684\u771f\u5b9e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u4e09\u7ef4\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u542f\u53d1\uff08\u5982\u57fa\u4e8e\u56fe\u50cf\u5f62\u53d8\u7684\u4eba\u8138\u8fd0\u52a8\u5148\u9a8c\uff09\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u4e0d\u51c6\u786e\uff0c\u751f\u6210\u7684\u5934\u50cf\u52a8\u753b\u7f3a\u4e4f\u81ea\u7136\u611f\u548c\u771f\u5b9e\u6027\u3002\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u514b\u670d\u4e09\u7ef4\u91cd\u5efa\u548c\u5507\u90e8\u52a8\u4f5c\u5408\u6210\u4e2d\u7684\u96be\u9898\u3002", "method": "Splat-Portrait\u65b9\u6cd5\u57fa\u4e8e\u9ad8\u65af\u55b7\u6d12\u6280\u672f\uff0c\u5c06\u5355\u5f20\u5934\u50cf\u56fe\u50cf\u89e3\u8026\u4e3a\u9759\u6001\u4e09\u7ef4\u91cd\u5efa\u90e8\u5206\uff08\u9759\u6001\u9ad8\u65af\u55b7\u6d12\u8868\u793a\uff09\u548c\u4e8c\u7ef4\u80cc\u666f\uff0c\u540c\u65f6\u6839\u636e\u8f93\u5165\u97f3\u9891\u81ea\u7136\u751f\u6210\u5507\u90e8\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u8fd0\u52a8\u9a71\u52a8\u5148\u9a8c\uff0c\u65e0\u9700\u4e09\u7ef4\u76d1\u7763\u6216\u4eba\u8138\u5173\u952e\u70b9\u6807\u6ce8\uff0c\u8bad\u7ec3\u4ec5\u4f9d\u9760\u4e8c\u7ef4\u91cd\u5efa\u635f\u5931\u548c\u5f97\u5206\u84b8\u998f\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSplat-Portrait\u5728\u751f\u6210\u8bf4\u8bdd\u5934\u50cf\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u7684\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u52a8\u753b\u771f\u5b9e\u611f\u3002", "conclusion": "Splat-Portrait\u4e3a\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65e0\u9700\u4e09\u7ef4\u76d1\u7763\u7684\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e09\u7ef4\u91cd\u5efa\u548c\u8bf4\u8bdd\u52a8\u753b\u7684\u6548\u679c\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u9886\u5148\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u3002\u9879\u76ee\u4ee3\u7801\u4e0e\u8d44\u6599\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.18698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18698", "abs": "https://arxiv.org/abs/2601.18698", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge", "comment": "Work in progress", "summary": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5GAP\u53ca\u5305\u542b500\u4e2a\u4e16\u754c\u5404\u5730\u666f\u70b9\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u9a8c\u6587\u672c\u751f\u6210\u89c6\u9891\u6a21\u578b\u5728\u5730\u7406\u516c\u5e73\u6027\u548c\u89c6\u89c9\u77e5\u8bc6\u8986\u76d6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u5168\u7403\u8303\u56f4\u5185\u8868\u73b0\u76f8\u5bf9\u5747\u8861\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u751f\u6210\u89c6\u9891\u6a21\u578b\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5916\u754c\u4ecd\u62c5\u5fc3\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u504f\u5411\u7279\u5b9a\u5730\u533a\uff0c\u7f3a\u4e4f\u5730\u7406\u516c\u5e73\u6027\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u65b9\u6cd5\u68c0\u9a8c\u5176\u5728\u5168\u7403\u4e0d\u540c\u5730\u533a\u7684\u77e5\u8bc6\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51faGAP\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b500\u4e2a\u5168\u7403\u666f\u70b9\u7684GEOATTRACTION-500\u57fa\u51c6\uff0c\u91c7\u7528\u7ed3\u6784\u5bf9\u9f50\u3001\u5173\u952e\u70b9\u5bf9\u9f50\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc4\u5224\u7b49\u591a\u79cd\u6307\u6807\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e3b\u6d41\u6a21\u578b\u751f\u6210\u4e0d\u540c\u5730\u533a\u666f\u70b9\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u7b7e\u7ed3\u679c\u6bd4\u5bf9\u3002", "result": "\u5bf9Sora 2\u7b49\u4e3b\u6d41\u6a21\u578b\u5b9e\u6d4b\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u5730\u533a\u3001\u53d1\u5c55\u6c34\u5e73\u548c\u6587\u5316\u5206\u7ec4\u4e2d\u7684\u5730\u7406\u89c6\u89c9\u77e5\u8bc6\u5206\u5e03\u8f83\u4e3a\u5747\u5300\uff0c\u53ea\u5bf9\u666f\u70b9\u77e5\u540d\u5ea6\u7565\u6709\u4f9d\u8d56\uff0c\u6253\u7834\u4e86\u6a21\u578b\u5f3a\u70c8\u5730\u7406\u504f\u89c1\u7684\u666e\u904d\u8ba4\u77e5\u3002", "conclusion": "\u5f53\u524d\u6587\u672c\u751f\u6210\u89c6\u9891\u6a21\u578b\u5728\u5168\u7403\u89c6\u89c9\u77e5\u8bc6\u8986\u76d6\u4e0a\u6bd4\u9884\u671f\u66f4\u4e3a\u5747\u8861\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u5168\u7403\u5316\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u4f46\u4ecd\u9700\u6301\u7eed\u76d1\u6d4b\u4e0e\u8bc4\u4f30\uff0c\u786e\u4fdd\u65e5\u540e\u6f14\u8fdb\u65f6\u4e0d\u4ea7\u751f\u65b0\u7684\u504f\u89c1\u3002"}}
{"id": "2601.18739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18739", "abs": "https://arxiv.org/abs/2601.18739", "authors": ["Ignacio Antequera-S\u00e1nchez", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification", "comment": "28 pages", "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u65b9\u6cd5SeNeDiF-OOD\uff0c\u901a\u8fc7\u5d4c\u5957\u4e8c\u5143\u878d\u5408\u8bbe\u8ba1\uff0c\u6709\u6548\u68c0\u6d4b\u591a\u79cd\u7c7b\u578b\u7684OOD\u6570\u636e\uff0c\u5b9e\u9a8c\u663e\u793a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u9636\u6bb5OOD\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u5f02\u6784\u7684OOD\u6570\u636e\uff08\u4ece\u4f4e\u5c42\u6b21\u6270\u52a8\u5230\u8bed\u4e49\u53d8\u5316\uff09\uff0c\u8fd9\u4e00\u96be\u9898\u5236\u7ea6\u4e86AI\u7cfb\u7edf\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SeNeDiF-OOD\u65b9\u6cd5\uff0c\u5c06OOD\u68c0\u6d4b\u5206\u89e3\u6210\u591a\u4e2a\u6309\u8bed\u4e49\u5c42\u7ea7\u5206\u5e03\u7684\u4e8c\u5143\u878d\u5408\u8282\u70b9\uff0c\u6bcf\u4e00\u5c42\u7ed3\u5408\u4e0e\u7279\u5b9a\u8bed\u4e49\u62bd\u8c61\u6c34\u5e73\u5bf9\u9f50\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5b9e\u73b0\u5206\u5c42\u68c0\u6d4b\u3002", "result": "\u5728MonuMAI\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u975e\u5206\u5e03\u5185\u6837\u672c\u3001\u672a\u77e5\u7c7b\u522b\u548c\u5bf9\u6297\u653b\u51fb\uff0c\u5e76\u4e14\u68c0\u6d4b\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0d\u635f\u5931\u5206\u5e03\u5185\u7684\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "SeNeDiF-OOD\u4f5c\u4e3a\u5206\u5c42\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u5bf9\u591a\u6837OOD\u6570\u636e\u65f6\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u73b0\u5b9eAI\u5e94\u7528\u4e2d\u7684OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u52a0\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
