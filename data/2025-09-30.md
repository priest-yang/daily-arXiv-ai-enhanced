<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 353]
- [cs.CL](#cs.CL) [Total: 195]
- [cs.RO](#cs.RO) [Total: 108]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Pathological Truth Bias in Vision-Language Models](https://arxiv.org/abs/2509.22674)
*Yash Thube*

Main category: cs.CV

TL;DR: 本文提出了MATS（多模态真实空间化审计）对视觉语言模型(VLMs)的真实空间理解能力进行评估，并发现主流生成式VLM表现较差，编码式模型更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 目前主流VLM虽然在标准测试中成绩优秀，但这些测试可能掩盖了其在现实场景中的系统性失误，影响模型可信度。因此作者希望引入新的评测方法，揭示VLM在空间与真伪判断上的不足。

Method: 作者提出MATS评测工具，结合两个新指标（空间一致性得分SCS与错误同意率IAR），对模型在面对视觉信息时能否拒绝明显错误描述进行行为审计，并用activation patching方法定位模型失效的偶发区域。对比分析了多种主流VLM，包括生成式与对比式模型。

Result: 生成式VLMs（如LLaVA 1.5、QwenVLchat）在空间一致性和错误同意度上的表现显著逊色于对比式模型（如CLIP、SigLIP）。通过activation patching发现生成式模型在中后期的交叉注意层存在问题，而对比式模型则在池化投影部分表现不足。

Conclusion: 当前许多VLM在空间与真实性方面存在显著缺陷，尤其是生成式模型。所提出的MATS工具和分析框架能更有效找出并定位这些问题，并为模型未来的改进提供了方向。

Abstract: Vision Language Models (VLMs) are improving quickly, but standard benchmarks
can hide systematic failures that reduce real world trust. We introduce MATS
(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that
measures whether models reject visually contradicted statements, and two
metrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).
Instruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS
and high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.
Activation patching causally localizes failure loci (mid to late cross
attention for generative models, pooled projection components for contrastive
models) and suggests concrete repair paths.

</details>


### [2] [Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method](https://arxiv.org/abs/2509.22686)
*Shinji Yamashita,Yuma Kinoshita,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 提出了一种高效算法，可对两幅图像进行尺度和旋转的亚像素级联合估计，精度高于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的相位相关方法只能有效计算平移，无法精确估算因相机缩放或旋转导致的尺度和旋转变化，这在医学成像与计算机视觉中影响图像配准效果。需要更准确高效的方法。

Method: 提出利用傅里叶变换下的对数极坐标空间来联合估计尺度和旋转，并结合交叉相关最大化策略和辅助函数法实现亚像素级精度。

Result: 实验显示新方法在尺度和旋转估计上的平均误差低于依赖离散交叉相关的传统傅里叶变换方法。

Conclusion: 所提算法能够更精确地估计图像间的尺度和旋转变换，有助于提升图像配准在相关领域中的应用效果。

Abstract: This paper introduces a highly efficient algorithm capable of jointly
estimating scale and rotation between two images with sub-pixel precision.
Image alignment serves as a critical process for spatially registering images
captured from different viewpoints, and finds extensive use in domains such as
medical imaging and computer vision. Traditional phase-correlation techniques
are effective in determining translational shifts; however, they are inadequate
when addressing scale and rotation changes, which often arise due to camera
zooming or rotational movements. In this paper, we propose a novel algorithm
that integrates scale and rotation estimation based on the Fourier transform in
log-polar coordinates with a cross-correlation maximization strategy,
leveraging the auxiliary function method. By incorporating sub-pixel-level
cross-correlation our method enables precise estimation of both scale and
rotation. Experimental results demonstrate that the proposed method achieves
lower mean estimation errors for scale and rotation than conventional Fourier
transform-based techniques that rely on discrete cross-correlation.

</details>


### [3] [Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization](https://arxiv.org/abs/2509.22688)
*Xu Jia*

Main category: cs.CV

TL;DR: 该论文提出了一种基于强化学习的多模态大模型优化框架，用于提升模型在结构化感知任务中的定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视觉-语言推理方面表现优异，但在需要精确定位与高鲁棒性的结构化感知任务上表现较差。为解决此问题，亟需新的优化方法提升其感知能力。

Method: 作者提出基于Group Relative Policy Optimization（GRPO）的强化学习框架，引入了课程式数据调度与难度感知筛选，有效缓解了稀疏、噪声奖励下的优化不稳定问题，并使模型逐步适应复杂数据样本。此外设计了奖励结构和KL正则项，提高了模型的收敛性及泛化能力。

Result: 在自动驾驶相关基准测试上，该方法在检测精度与鲁棒性方面均获得显著提升。消融实验进一步证实了奖励设计、KL正则化与课程节奏对模型稳定收敛和泛化能力具有关键作用。

Conclusion: 强化学习与结构化数据课程联合优化为多模态检测提供了可扩展且具备可解释性的技术路线，有助于提升未来多模态大模型的感知能力和实际应用表现。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language reasoning
but often struggle with structured perception tasks requiring precise
localization and robustness. We propose a reinforcement learning framework that
augments Group Relative Policy Optimization (GRPO) with curriculum-based data
scheduling and difficulty-aware filtering. This approach stabilizes
optimization under sparse, noisy rewards and enables progressive adaptation to
complex samples. Evaluations on autonomous driving benchmarks demonstrate
substantial improvements in detection accuracy and robustness. Ablation studies
confirm the importance of reward design, KL regularization, and curriculum
pacing for convergence stability and generalization. Our findings highlight
reinforcement-driven optimization with structured data curricula as a scalable
path toward robust and interpretable multimodal detection.

</details>


### [4] [Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.22689)
*Ha-Hieu Pham,Minh Le,Han Huynh,Nguyen Quoc Khanh Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 本文提出了一种基于图论约束的半监督语义分割方法（TGC），显著提升了在有限标注条件下的分割表现，弥补了传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 在计算病理领域，像素级语义分割对稠密标注的依赖导致高昂的人力代价，而现有半监督方法易传播伪标签噪声，并产生断裂或不合理的分割遮罩。作者为了解决这些方法中伪标签带来的拓扑结构不一致问题，提出了新的方法。

Method: 作者设计了拓扑图一致性（Topology Graph Consistency, TGC）框架，核心思想是通过在预测结果与基准图之间对齐拉普拉斯谱、连通分量数量与邻接统计，显式施加图论约束，以强化全局拓扑结构，抑制伪标签噪声。

Result: 在GlaS和CRAG病理数据集（监督比例仅为5-10%）上的实验表明，TGC取得了领先于现有方法的结果，并明显缩小了与全监督方案之间的性能差距。

Conclusion: TGC有效融合了图论拓扑信息到半监督分割框架中，兼顾结构与准确性，在稀缺标注场景下具备广泛应用前景。

Abstract: Semi-supervised semantic segmentation (SSSS) is vital in computational
pathology, where dense annotations are costly and limited. Existing methods
often rely on pixel-level consistency, which propagates noisy pseudo-labels and
produces fragmented or topologically invalid masks. We propose Topology Graph
Consistency (TGC), a framework that integrates graph-theoretic constraints by
aligning Laplacian spectra, component counts, and adjacency statistics between
prediction graphs and references. This enforces global topology and improves
segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC
achieves state-of-the-art performance under 5-10% supervision and significantly
narrows the gap to full supervision. Code is available at
https://github.com/hieuphamha19/TGC.

</details>


### [5] [A review of Recent Techniques for Person Re-Identification](https://arxiv.org/abs/2509.22690)
*Andrea Asperti,Salvatore Fiorilla,Simone Nardi,Lorenzo Orsini*

Main category: cs.CV

TL;DR: 本文综述了行人重识别领域的最新进展，重点分析了监督学习和无监督学习两大方法的优势与发展趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习极大推动了监督式行人重识别的发展，但对大量标注数据的依赖限制了其可扩展性。因此，探索解决标注数据瓶颈的无监督方法成为当前研究的热点。

Method: 文章分为两部分：一是回顾并分类现有的监督式行人重识别研究，总结其方法和发展瓶颈；二是系统梳理近三年无监督行人重识别的新进展，分析其技术演变和发展潜力。

Result: 监督方法在性能上接近顶峰，提升空间有限。无监督方法虽传统上落后但近年取得显著进展，和监督方法的性能差距正在逐渐缩小。

Conclusion: 行人重识别未来的发展趋势是无监督方法通过技术创新有望达到甚至逼近监督方法的表现，推动领域向无需大规模标注数据的方向演化。

Abstract: Person re-identification (ReId), a crucial task in surveillance, involves
matching individuals across different camera views. The advent of Deep
Learning, especially supervised techniques like Convolutional Neural Networks
and Attention Mechanisms, has significantly enhanced person Re-ID. However, the
success of supervised approaches hinges on vast amounts of annotated data,
posing scalability challenges in data labeling and computational costs. To
address these limitations, recent research has shifted towards unsupervised
person re-identification. Leveraging abundant unlabeled data, unsupervised
methods aim to overcome the need for pairwise labelled data. Although
traditionally trailing behind supervised approaches, unsupervised techniques
have shown promising developments in recent years, signalling a narrowing
performance gap. Motivated by this evolving landscape, our survey pursues two
primary objectives. First, we review and categorize significant publications in
supervised person re-identification, providing an in-depth overview of the
current state-of-the-art and emphasizing little room for further improvement in
this domain. Second, we explore the latest advancements in unsupervised person
re-identification over the past three years, offering insights into emerging
trends and shedding light on the potential convergence of performance between
supervised and unsupervised paradigms. This dual-focus survey aims to
contribute to the evolving narrative of person re-identification, capturing
both the mature landscape of supervised techniques and the promising outcomes
in the realm of unsupervised learning.

</details>


### [6] [Sequential Token Merging: Revisiting Hidden States](https://arxiv.org/abs/2509.22691)
*Yan Wen,Peng Ye,Lin Zhang,Baopu Li,Jiakang Yuan,Yaoxin Yang,Tao Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的令牌合并方法Sequential Token Merging（STM），旨在提升Vision Mambas系列模型处理高分辨率图像时的效率。STM有效减少了计算复杂度，同时保证准确率几乎不下降。实验验证了该方法比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上Vision Mambas模型具有亚二次复杂度优势，但当图像分辨率提升时，令牌数量的二次增长带来效率瓶颈。已有方法多关注如何减少冗余令牌，却忽视了模型固有的有限方向性顺序依赖机制，影响信息流动。因此有必要设计更智能的令牌合并策略。

Method: 作者分析了Vision Mambas中的有限方向顺序依赖（LDSD）特性，洞察到模型通过选择性扫描实现信息在隐藏状态中的逐步汇聚。基于此，提出STM方法，核心包括：1）双向最近邻合并，空间对称聚合以保序列依赖；2）隐藏态保护，稳固和类令牌相关的隐藏态。STM配合层级式损失收敛特性，实现序列“遗忘性”到稳定性的转化。

Result: 在ViM-Ti模型上，删减20%令牌仅损失1.0%的准确率，ViM-S删减40%令牌仅损失1.4%。该方法在各项指标上均优于现有令牌合并方案，且复杂度最低。

Conclusion: STM方法提升了Vision Mambas模型的效率，极大减少了计算开销且损失可控，促进了大分辨率图像下高效视觉建模，并为状态空间模型的动态行为研究提供了新视角。

Abstract: Vision Mambas (ViMs) achieve remarkable success with sub-quadratic
complexity, but their efficiency remains constrained by quadratic token scaling
with image resolution. While existing methods address token redundancy, they
overlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a
critical information flow mechanism revealed in our analysis. We further
identify Mamba's selective scan enables gradual information aggregation in
hidden states. Based on these insights, we propose Sequential Token Merging
(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve
sequential dependencies through symmetric spatial aggregation, and 2) Hidden
states protection to stabilize the hidden states around the class token. STM
strategically leverages Mamba's layer-wise loss convergence to convert temporal
forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%
accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for
ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with
minimal complexity, while providing new insights into state-space model
dynamics. Codes will be released soon.

</details>


### [7] [Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2509.22692)
*Le Zhang,Ao Li,Qibin Hou,Ce Zhu,Yonina C. Eldar*

Main category: cs.CV

TL;DR: 该论文系统综述了超分辨率（SR）技术的发展，涵盖了单帧、视频、立体和光场等多种领域，并整理了150余种SISR方法、近70种VSR方法及约30种SSR、LFSR技术。作者对方法论、数据集、评价体系、实验结果和复杂度进行全面梳理，并建立了基于骨干结构的分类体系，指出了当前尚未充分研究的问题，并提供了相关资源仓库。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术的进步和高质量视觉应用需求的增长，超分辨率成为计算机视觉领域的研究热点。然而，现有综述多聚焦于具体子领域，缺乏对整个超分辨率领域的全面梳理和总结，因此亟需一篇系统性综述以整合现有研究进展并提出未来研究方向。

Method: 本综述系统回顾和分析了单张图像、视频、立体和光场等多个超分领域的最新方法和进展，总结了150余种SISR、近70种VSR和约30种SSR、LFSR方法，并对它们的方法框架、数据集、实验评价指标、结果与计算复杂度等进行了详细分析。此外，还针对骨干结构和研究目的设计了分类体系，并讨论了存在的未解决问题。

Result: 该综述归纳了目前主要SR技术，包括详细的数量、类别、方法对比，并对比了各类方法的优缺点、数据集适用性、实验结果和计算开销。通过对比和分析，揭示了不同研究方法的趋势和不足，并为后续研究指出了方向。

Conclusion: 作者认为本文将成为SR领域的重要参考资源，不仅能够方便研究人员查找方法综述与资源，同时通过对未充分研究问题的讨论，为未来研究提供新的思路和指导。

Abstract: Super-resolution (SR) has garnered significant attention within the computer
vision community, driven by advances in deep learning (DL) techniques and the
growing demand for high-quality visual applications. With the expansion of this
field, numerous surveys have emerged. Most existing surveys focus on specific
domains, lacking a comprehensive overview of this field. Here, we present an
in-depth review of diverse SR methods, encompassing single image
super-resolution (SISR), video super-resolution (VSR), stereo super-resolution
(SSR), and light field super-resolution (LFSR). We extensively cover over 150
SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR
and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical
results, and complexity. In addition, we conducted a taxonomy based on each
backbone structure according to the diverse purposes. We also explore valuable
yet under-studied open issues in the field. We believe that this work will
serve as a valuable resource and offer guidance to researchers in this domain.
To facilitate access to related work, we created a dedicated repository
available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.

</details>


### [8] [Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment](https://arxiv.org/abs/2509.22697)
*Abhiroop Chatterjee,Susmita Ghosh*

Main category: cs.CV

TL;DR: 本文提出了一种针对高光谱图像（HSI）场景理解的高效视觉-语言模型，利用类CLIP结构对多模态信息进行对齐，仅修改极少数参数即实现了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 面对庞大的数据和模型规模，研究者希望通过对高价值数据的高效提取与精炼，提升学习效率。高光谱图像具有高维复杂结构，跨模态对齐问题依然未被充分探索。本文致力于优化视觉-语言模型于该领域。

Method: 该方法采用一种CLIP风格的对比训练思路，将视觉主干抽取的体素级特征嵌入到冻结的大型嵌入模型的语义空间中，并通过可训练探针对视觉与文本特征进行对齐。对比损失集中于精心挑选的hard和semi-hard负样本及正样本。引入描述性prompt，增强语义锚定。参数更新量仅为总参数的0.07%。

Result: 在Indian Pines和Pavia University数据集上，模型分别在总体准确率（OA）和Kappa系数（κ）相较于单模态和多模态基线有显著提升；同时使用的可训练参数比主流方法缩小了50到90倍。

Conclusion: 提出的方法能够在仅极少参数更新条件下，有效实现高光谱图像多模态对齐和理解，取得领先性能，为高效处理高维场景理解任务提供了有价值的思路。

Abstract: As data requirements continue to grow, efficient learning increasingly
depends on the curation and distillation of high-value data rather than
brute-force scaling of model sizes. In the case of a hyperspectral image (HSI),
the challenge is amplified by the high-dimensional 3D voxel structure, where
each spatial location is associated with hundreds of contiguous spectral
channels. While vision and language models have been optimized effectively for
natural image or text tasks, their cross-modal alignment in the hyperspectral
domain remains an open and underexplored problem. In this article, we make an
attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene
understanding by exploiting a CLIP-style contrastive training framework. Our
framework maps voxel-level embeddings from a vision backbone onto the latent
space of a frozen large embedding model (LEM), where a trainable probe aligns
vision features with the model's textual token representations. The two
modalities are aligned via a contrastive loss restricted to a curated set of
hard (closest wrong classes) and semi-hard (random distractors) negatives,
along with positive pairs. To further enhance alignment, descriptive prompts
that encode class semantics are introduced and act as structured anchors for
the HSI embeddings. It is seen that the proposed method updates only 0.07
percent of the total parameters, yet yields state-of-the-art performance. For
example, on Indian Pines (IP) the model produces better results over unimodal
and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa
($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA
and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters,
nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

</details>


### [9] [Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning](https://arxiv.org/abs/2509.22700)
*Zhuang Qi,Pan Yu,Lei Meng,Sijin Zhou,Han Yu,Xiaoxiao Li,Xiangxu Meng*

Main category: cs.CV

TL;DR: 本文提出了一种用于一次性联邦提示学习（FPL）的新方法——Global Prompt Refinement with Non-Interfering Attention Masking（GPR-NIAM），在10个数据集和两种任务下，相比8种先进方法，在类别和领域泛化能力上均取得了更好表现。


<details>
  <summary>Details</summary>
Motivation: 当前联邦提示学习方法依赖多轮通信，且现有一次性联邦学习方法在跨任务泛化能力较弱。为解决高效泛化和减少通信轮数的问题，作者提出了新的改进方法。

Method: 文章提出GPR-NIAM方法，包含两个关键模块：一是注意力隔离模块，通过掩码机制抑制可学习提示对原文本的注意力，并对反向注意力加权，增强跨任务泛化；二是跨中心协同优化模块，通过多源跨模态知识对齐将去中心化的知识统一整合并校准全局提示，从而缓解数据异质性带来的问题。

Result: 在10个基准数据集和两项任务实验中，GPR-NIAM在类别级和领域级泛化能力方面优于8个主流对比方法。

Conclusion: GPR-NIAM在提升联邦提示学习中的效率、一次性泛化能力、应对数据异质性等方面效果显著，是应对现有FPL方法局限的有效方案。

Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by
tuning lightweight prompts on top of frozen pre-trained models. Existing FPL
methods typically rely on global information, which is only available after the
second training round, to facilitate collaboration among client models.
Therefore, they are inherently dependent on multi-round communication to fully
exhibit their strengths. Moreover, existing one-shot federated learning methods
typically focus on fitting seen tasks, but lack cross-task generalization. To
bridge this gap, we propose the Global Prompt Refinement with Non-Interfering
Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to
design a masking mechanism that restricts excessive interaction between the
original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves
this through the collaboration of two key modules. Firstly, the attention
isolation module suppresses attention from the learnable prompt tokens to the
original text tokens, and reweights the reverse attention which preserves
generalization across tasks. Secondly, the cross-silo collaborative refinement
module integrates decentralized visual knowledge into a unified base and
calibrates the global prompt through multi-source cross-modal knowledge
alignment, further mitigating the inconsistency caused by data heterogeneity.
Extensive experiments conducted on ten benchmark datasets under two tasks show
that GPR-NIAM outperforms eight state-of-the-art methods in both class-level
and domain-level generalization.

</details>


### [10] [GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot](https://arxiv.org/abs/2509.22708)
*Ahed Alboody*

Main category: cs.CV

TL;DR: 本文提出了一种结合专家混合机制（Mixture-of-Experts, MoE）的广义零样本学习（GZSL-MoE）模型，用于提升3D点云语义分割任务中特征生成的真实性，特别适用于类标签稀缺的复杂环境。


<details>
  <summary>Details</summary>
Motivation: 在3D点云语义分割中，部分类因样本稀缺导致难以有效训练。现有GZSL方法虽用生成模型合成新类特征，但生成特征与真实特征差距较大，限制了辨识能力。因此，作者希望提升生成特征的真实性和泛化能力，尤其是在涉及人机协作等复杂环境时。

Method: 作者将专家混合机制（MoE）引入到生成器和判别器中，通过多个专家网络合作生成器生成更接近真实的伪特征。特征提取采用预训练KPConv网络。模型在COVERED人机协作数据集上进行训练和测试，评估其对已见类和未见类的分割能力。

Result: 实验表明，提出的GZSL-MoE模型在COVERED数据集上显著提升了3D点云语义分割的准确率，无论对于已见还是未见类别表现都优于传统GZSL方法。

Conclusion: GZSL-MoE模型通过整合混合专家机制，有效提升了生成特征的质量和最终分割性能，为在样本有限情况下理解复杂3D环境提供了有前景的解决方案。

Abstract: Generative Zero-Shot Learning approach (GZSL) has demonstrated significant
potential in 3D point cloud semantic segmentation tasks. GZSL leverages
generative models like GANs or VAEs to synthesize realistic features (real
features) of unseen classes. This allows the model to label unseen classes
during testing, despite being trained only on seen classes. In this context, we
introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts
(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to
generate fake features that closely resemble real features extracted using a
pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main
contribution of this paper is the integration of Mixture-of-Experts into the
Generator and Discriminator components of the Generative Zero-Shot Learning
model for 3D point cloud semantic segmentation, applied to the COVERED dataset
(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)
environments. By combining the Generative Zero-Shot Learning model with
Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides
a promising solution for understanding complex 3D environments, especially when
comprehensive training data for all object classes is unavailable. The
performance evaluation of the GZSL-MoE model highlights its ability to enhance
performance on both seen and unseen classes. Keywords Generalized Zero-Shot
Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot
Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,
Mixture-of Experts

</details>


### [11] [IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism](https://arxiv.org/abs/2509.22719)
*Adithya Giri*

Main category: cs.CV

TL;DR: 本文提出了一种在小数据集上表现更好的视觉Transformer方法——IBiT（具备归纳偏置的图像Transformer），通过引入可学习的掩码提升模型效果。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然已成视觉领域主流，但缺乏CNN那样的归纳偏置，通常需大数据集才能学到良好特征。作者希望能在小数据集上增强Transformer表现，无需依赖知识蒸馏。

Method: 在Vision Transformer中引入基于学习的掩码，把CNN的归纳偏置导入Transformer结构，从而帮助模型在数据较少时也能有效学习。

Result: 引入掩码的IBiT模型在小数据集上的准确率显著超越原始Transformer，并且仍保有Transformer的可解释性。

Conclusion: 通过引入归纳偏置，IBiT在数据有限时弥补了Transformer的不足，可以无需大规模数据集或辅助蒸馏手段也能获得优异表现。

Abstract: In recent years, Transformer-based architectures have become the dominant
method for Computer Vision applications. While Transformers are explainable and
scale well with dataset size, they lack the inductive biases of Convolutional
Neural Networks. While these biases may be learned on large datasets, we show
that introducing these inductive biases through learned masks allow Vision
Transformers to learn on much smaller datasets without Knowledge Distillation.
These Transformers, which we call Inductively Biased Image Transformers (IBiT),
are significantly more accurate on small datasets, while retaining the
explainability Transformers.

</details>


### [12] [LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning](https://arxiv.org/abs/2509.22720)
*Zezhong Fan,Xiaohan Li,Luyi Ma,Kai Zhao,Liang Peng,Topojoy Biswas,Evren Korpeoglu,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: 提出了一种新的多对象场景生成方法LayoutAgent，融合了视觉-语言推理与组合扩散模型，实现了更合理、真实的布局生成，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 单纯的扩散模型虽然能够生成高质量图像，但缺乏空间推理，导致布局不真实；而传统机器人领域的空间布局方法则难以体现视觉场景的语义丰富性。为弥合这一差距，研究提出联合视觉、语言与空间推理的方法。

Method: 方法包括三步：首先利用视觉-语言模型对输入图片进行切分、对象尺寸估计、构建场景图，以及重写提示词；其次借助传统机器人领域的组合扩散技术，根据场景图生成符合对象关系的物体边界框布局；最后通过前景条件的图像生成器，用设计的提示渲染符合布局的完整场景图像。

Result: 实验表明，LayoutAgent在布局连贯性、空间真实性和美学一致性等方面均优于其他最新布局生成模型。

Conclusion: LayoutAgent有效统一了视觉-语言理解与空间组合推理，为复杂多对象场景的高质量生成提供了新方法，对图像生成和场景理解领域具有积极推动作用。

Abstract: Designing realistic multi-object scenes requires not only generating images,
but also planning spatial layouts that respect semantic relations and physical
plausibility. On one hand, while recent advances in diffusion models have
enabled high-quality image generation, they lack explicit spatial reasoning,
leading to unrealistic object layouts. On the other hand, traditional spatial
planning methods in robotics emphasize geometric and relational consistency,
but they struggle to capture semantic richness in visual scenes. To bridge this
gap, in this paper, we propose LayoutAgent, an agentic framework that unifies
vision-language reasoning with compositional diffusion for layout generation.
Given multiple input images with target objects in them, our method first
employs visual-language model to preprocess the inputs through segmentation,
object size estimation, scene graph construction, and prompt rewriting. Then we
leverage compositional diffusion-a method traditionally used in robotics-to
synthesize bounding boxes that respect object relations encoded in the scene
graph for spatial layouts. In the end, a foreground-conditioned image generator
composes the complete scene by rendering the objects into the planned layout
guided by designed prompts. Experiments demonstrate that LayoutAgent
outperforms other state-of-the-art layout generation models in layout
coherence, spatial realism and aesthetic alignment.

</details>


### [13] [CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.22737)
*Jie Cai,Kangning Yang,Lan Fu,Jiaming Ding,Jinlong Li,Huiming Sun,Daitao Xing,Jinglin Shen,Zibo Meng*

Main category: cs.CV

TL;DR: 本文提出了CompareBench基准，用于系统性评估视觉-语言模型（VLMs）在视觉对比推理任务中的表现，并揭示了当前主流模型在该领域的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 视觉对比推理能力是视觉-语言模型实现更复杂、多样化理解的关键，但这一能力目前研究和评估较少。缺乏系统测评手段限制了模型进步。因此亟需建立专门的基准，推动领域发展。

Method: 作者构建了CompareBench基准，包含1000个问答对，覆盖数量、时间、几何和空间四类对比任务，数据来自TallyBench和HistCaps两个辅助数据集。此外，作者评测了多个主流闭源和开源视觉-语言模型在该基准上的表现，对模型能力进行全面诊断分析。

Result: 结果显示尽管模型规模提升带来一定性能提升，但无论是最先进的闭源API还是最新开源模型，在时间排序、空间关系、基础计数和几何对比任务上均存在较大短板，远不能与人类直觉表现匹配。

Conclusion: 视觉对比推理是当前视觉-语言模型的系统性短板，亟需专门关注和改进。CompareBench为今后多模态模型的可靠推理能力研究提供了诊断性平台和推动力。

Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison
reasoning in vision-language models (VLMs), a fundamental yet understudied
skill. CompareBench consists of 1000 QA pairs across four tasks: quantity
(600), temporal (100), geometric (200), and spatial (100). It is derived from
two auxiliary datasets that we constructed: TallyBench (2000 counting images
with QA) and HistCaps (515 historical images with bilingual captions). We
evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source
models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but
also reveal critical limitations: even the strongest models consistently fail
at temporal ordering and spatial relations, and they often make mistakes in
basic counting and geometric comparisons that are trivial for humans. These
findings demonstrate that visual comparison remains a systematic blind spot for
current VLMs. By providing controlled, diverse, and diagnostic evaluation,
CompareBench establishes a foundation for advancing more reliable multimodal
reasoning.

</details>


### [14] [MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning](https://arxiv.org/abs/2509.22761)
*Yapeng Mi,Hengli Li,Yanpeng Zhao,Chenxi Li,Huimin Wu,Xiaojian Ma,Song-Chun Zhu,Ying Nian Wu,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为MILR的新方法，在图像生成任务中实现了联合图像和文本的推理，并在多个基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于推理的图像生成方法受限于仅在单一模态（图像或文本）上进行推理，或依赖高质量推理数据进行微调，因此无法充分发挥多模态推理潜力。

Method: MILR是一种测试时推理方法，在统一的潜在向量空间中对图像和文本联合推理。具体做法是在离散图像与文本token的向量表示中进行搜索，利用policy gradient算法并结合图像质量评价器进行优化。MILR在支持推理和生成的MUG框架内实现，允许在生成图像前进行跨模态推理，整个流程均可于测试阶段完成。

Result: MILR在GenEval、T2I-CompBench和WISE等基准上取得最优秀表现，尤其在知识密集型的WISE测试上得分0.63，比基线提升80%。

Conclusion: 联合图像与文本的潜在空间推理是性能提升的关键。MILR不仅能处理知识密集任务，还展现了强大的时序和文化推理能力，证实了其推理方法的有效性。

Abstract: Reasoning-augmented machine learning systems have shown improved performance
in various domains, including image generation. However, existing
reasoning-based methods for image generation either restrict reasoning to a
single modality (image or text) or rely on high-quality reasoning data for
fine-tuning. To tackle these limitations, we propose MILR, a test-time method
that jointly reasons over image and text in a unified latent vector space.
Reasoning in MILR is performed by searching through vector representations of
discrete image and text tokens. Practically, this is implemented via the policy
gradient method, guided by an image quality critic. We instantiate MILR within
the unified multimodal understanding and generation (MUG) framework that
natively supports language reasoning before image synthesis and thus
facilitates cross-modal reasoning. The intermediate model outputs, which are to
be optimized, serve as the unified latent space, enabling MILR to operate
entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,
achieving state-of-the-art results on all benchmarks. Notably, on
knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over
the baseline by 80%. Our further analysis indicates that joint reasoning in the
unified latent space is the key to its strong performance. Moreover, our
qualitative studies reveal MILR's non-trivial ability in temporal and cultural
reasoning, highlighting the efficacy of our reasoning method.

</details>


### [15] [UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation](https://arxiv.org/abs/2509.22763)
*Tangqi Shi,Pietro Lio*

Main category: cs.CV

TL;DR: 本文提出了一种新的U型网络UESA-Net，用于乳腺和甲状腺超声图像分割，在两个公开数据集上实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺和甲状腺癌发病率上升，超声是常用筛查手段，但图像噪声大，结构重叠明显，导致分割任务难以同时捕获全局语义和局部细节。现有方法无法很好地结合这两类特征。

Method: 作者提出了UESA-Net，一种具备多向收缩注意力机制的U型分割网络。其编码器和解码器均运用空间多维注意力模块（水平、垂直、深度），并集成收缩（阈值）策略，将先验知识和局部特征结合。解码器进一步运用成对收缩机制以增强上下文建模。

Result: 在TN3K和BUSI两个公开乳腺与甲状腺超声图像分割数据集上，UESA-Net分别取得了0.8487和0.6495的IoU，优于当前主流方法。

Conclusion: UESA-Net能有效融合多维空间信息与先验知识，提升乳腺和甲状腺超声图像分割的鲁棒性和精确性，在多个基准数据集上表现突出，具有应用前景。

Abstract: Background: Breast and thyroid cancers pose an increasing public-health
burden. Ultrasound imaging is a cost-effective, real-time modality for lesion
detection and segmentation, yet suffers from speckle noise, overlapping
structures, and weak global-local feature interactions. Existing networks
struggle to reconcile high-level semantics with low-level spatial details. We
aim to develop a segmentation framework that bridges the semantic gap between
global context and local detail in noisy ultrasound images.
  Methods: We propose UESA-Net, a U-shaped network with multidirectional
shrinkage attention. The encoder-decoder architecture captures long-range
dependencies and fine-grained structures of lesions. Within each encoding
block, attention modules operate along horizontal, vertical, and depth
directions to exploit spatial details, while a shrinkage (threshold) strategy
integrates prior knowledge and local features. The decoder mirrors the encoder
but applies a pairwise shrinkage mechanism, combining prior low-level physical
cues with corresponding encoder features to enhance context modeling.
  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -
UESA-Net achieved state-of-the-art performance with intersection-over-union
(IoU) scores of 0.8487 and 0.6495, respectively.
  Conclusions: UESA-Net effectively aggregates multidirectional spatial
information and prior knowledge to improve robustness and accuracy in breast
and thyroid ultrasound segmentation, demonstrating superior performance to
existing methods on multiple benchmarks.

</details>


### [16] [PartCo: Part-Level Correspondence Priors Enhance Category Discovery](https://arxiv.org/abs/2509.22769)
*Fernando Julio Cendra,Kai Han*

Main category: cs.CV

TL;DR: 提出了PartCo方法，通过引入部件级的视觉特征对应关系，提升了用于识别已知与新类别的通用类别发现（GCD）任务的表现，在多个数据集上实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用类别发现方法主要依赖全局语义标签和图像表示，忽视了区分相近类别所需的部件级信息。本文旨在弥补这一不足，提升GCD算法在细粒度类别区分上的能力。

Method: 提出PartCo框架，通过建模部件级的视觉特征对应关系，捕捉更细粒度的语义结构。该方法能够与现有GCD方法无缝集成，无需大幅修改原有流程。

Result: 在多项公共基准数据集上进行大量实验，结果表明，PartCo大幅提升了当前GCD方法的性能，并且达到了新的最优结果。

Conclusion: PartCo有效弥合了语义标签和部件级视觉信息之间的鸿沟，为通用类别发现树立了新基准，并为后续研究提供了新的思路。

Abstract: Generalized Category Discovery (GCD) aims to identify both known and novel
categories within unlabeled data by leveraging a set of labeled examples from
known categories. Existing GCD methods primarily depend on semantic labels and
global image representations, often overlooking the detailed part-level cues
that are crucial for distinguishing closely related categories. In this paper,
we introduce PartCo, short for Part-Level Correspondence Prior, a novel
framework that enhances category discovery by incorporating part-level visual
feature correspondences. By leveraging part-level relationships, PartCo
captures finer-grained semantic structures, enabling a more nuanced
understanding of category relationships. Importantly, PartCo seamlessly
integrates with existing GCD methods without requiring significant
modifications. Our extensive experiments on multiple benchmark datasets
demonstrate that PartCo significantly improves the performance of current GCD
approaches, achieving state-of-the-art results by bridging the gap between
semantic labels and part-level visual compositions, thereby setting new
benchmarks for GCD. Project page: https://visual-ai.github.io/partco

</details>


### [17] [DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models](https://arxiv.org/abs/2509.22793)
*Komal Kumar,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Ivan Laptev,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出了一种高效的微调方法DEFT（Decompositional Efficient Fine-Tuning），通过分解权重更新，实现了在个性化、泛化和可编辑性之间的权衡，并取得了最先进的实验效果。


<details>
  <summary>Details</summary>
Motivation: 微调文本到图像(T2I)预训练模型时，面临如何兼顾对目标分布的对齐、模型泛化多任务能力以及编辑性三个目标，并且要在参数与计算资源有限的条件下实现高效调整。

Method: 作者提出DEFT框架，将预训练模型的权重更新分解为两个部分：（1）正交于低秩子空间的投影更新，（2）低秩更新。通过两个可训练的低秩矩阵，一个限定子空间，一个在该子空间进行灵活调整，从而高效适配新任务或数据。

Result: 在Dreambooth和Dreambench Plus数据集（个性化）、InsDet数据集（物体与场景适应）、VisualCloze数据集（上下文视觉生成）上，基于Stable Diffusion等模型，实验结果均达到了最新最优水平。

Conclusion: DEFT框架通过合理的参数分解，不仅实现了高效的模型微调，还兼顾了个性化、泛化能力和编辑性，为后续T2I模型微调提供了新思路。

Abstract: Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves
adjusting the model to suit a particular task or dataset while minimizing
computational resources and limiting the number of trainable parameters.
However, it often faces challenges in striking a trade-off between aligning
with the target distribution: learning a novel concept from a limited image for
personalization and retaining the instruction ability needed for unifying
multiple tasks, all while maintaining editability (aligning with a variety of
prompts or in-context generation). In this work, we introduce DEFT,
Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that
adapts a pre-trained weight matrix by decomposing its update into two
components with two trainable matrices: (1) a projection onto the complement of
a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.
The single trainable low-rank matrix defines the subspace, while the other
trainable low-rank matrix enables flexible parameter adaptation within that
subspace. We conducted extensive experiments on the Dreambooth and Dreambench
Plus datasets for personalization, the InsDet dataset for object and scene
adaptation, and the VisualCloze dataset for a universal image generation
framework through visual in-context learning with both Stable Diffusion and a
unified model. Our results demonstrated state-of-the-art performance,
highlighting the emergent properties of efficient fine-tuning. Our code is
available on \href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.

</details>


### [18] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出了VideoScore2，一个能够多维度、可解释且与人类评价保持一致的视频生成质量评估框架，专为文本到视频生成模型的综合性评价而设计，突破了以往评估器的局限。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成技术虽然进步极大，但相关视频的评估仍旧困难，依赖于多方面因素（比如视觉质量、语义对齐和物理一致性），以往方法仅给出单一分数且难以解释，不能全面反映视频质量。因此，有必要开发一个能多维度、清晰解释并与人类认知一致的评估体系。

Method: 提出了VideoScore2框架，分别评估视觉质量、文本-视频对齐，以及物理/常识一致性，还可输出详细的推理链支撑评分。模型基于新构建的VideoFeedback2大规模数据集（包含27168个人工标注的视频及三方面得分与推理），采用两阶段：先监督微调，再用改进的群体相对策略优化（GRPO）强化学习提升分析能力。

Result: VideoScore2在自有基准VideoScore-Bench-v2上准确率达到44.35（提升5.94）以及四个外部基准上平均表现50.37（提升4.32），并能输出具体、可解释的评估分析，在Best-of-N采样奖励建模中能有效连接评估与生成质量可控性。

Conclusion: VideoScore2不仅显著提升了文本到视频生成的评价准确性，还具备可解释性和与人类对齐的评价维度，为实现更可控、更高质量的视频生成提供了有力手段。

Abstract: Recent advances in text-to-video generation have produced increasingly
realistic and diverse content, yet evaluating such videos remains a fundamental
challenge due to their multi-faceted nature encompassing visual quality,
semantic alignment, and physical consistency. Existing evaluators and reward
models are limited to single opaque scores, lack interpretability, or provide
only coarse analysis, making them insufficient for capturing the comprehensive
nature of video quality assessment. We present VideoScore2, a
multi-dimensional, interpretable, and human-aligned framework that explicitly
evaluates visual quality, text-to-video alignment, and physical/common-sense
consistency while producing detailed chain-of-thought rationales. Our model is
trained on a large-scale dataset VideoFeedback2 containing 27,168
human-annotated videos with both scores and reasoning traces across three
dimensions, using a two-stage pipeline of supervised fine-tuning followed by
reinforcement learning with Group Relative Policy Optimization (GRPO) to
enhance analytical robustness. Extensive experiments demonstrate that
VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our
in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance
across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),
while providing interpretable assessments that bridge the gap between
evaluation and controllable generation through effective reward modeling for
Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [19] [TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses](https://arxiv.org/abs/2509.22813)
*Sahar Dastani,Ali Bahri,Gustavo Adolfo Vargas Hakim,Moslem Yazdanpanah,Mehrdad Noori,David Osowiechi,Samuel Barbeau,Ismail Ben Ayed,Herve Lombaert,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本论文提出了TRUST方法，提升了State Space Models在分布偏移下的泛化能力，并优于现有的测试时自适应方法。


<details>
  <summary>Details</summary>
Motivation: 虽然SSM（如VMamba）在视觉任务上高效，但其在分布偏移情况下表现下降，亟需提升其泛化和鲁棒性。

Method: 提出了一种新的测试时自适应方法TRUST，通过多种遍历方式对输入图像产生不同的因果视角。模型预测作为伪标签，引导Mamba特定参数的更新，最后对自适应后的权重进行平均整合。

Result: 在七个基准测试中，TRUST方法表现出更好的鲁棒性，并相比现有的测试时自适应（TTA）方法有更优结果。

Conclusion: TRUST首次显式利用了SSM的独特结构特性进行自适应，有效提升了模型的泛化性能和鲁棒性，在实际应用中更具优势。

Abstract: State Space Models (SSMs) have emerged as efficient alternatives to Vision
Transformers (ViTs), with VMamba standing out as a pioneering architecture
designed for vision tasks. However, their generalization performance degrades
significantly under distribution shifts. To address this limitation, we propose
TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel
test-time adaptation (TTA) method that leverages diverse traversal permutations
to generate multiple causal perspectives of the input image. Model predictions
serve as pseudo-labels to guide updates of the Mamba-specific parameters, and
the adapted weights are averaged to integrate the learned information across
traversal scans. Altogether, TRUST is the first approach that explicitly
leverages the unique architectural properties of SSMs for adaptation.
Experiments on seven benchmarks show that TRUST consistently improves
robustness and outperforms existing TTA methods.

</details>


### [20] [MMPB: It's Time for Multi-Modal Personalization](https://arxiv.org/abs/2509.22820)
*Jaeik Kim,Woojin Kim,Woohyeon Park,Jaeyoung Do*

Main category: cs.CV

TL;DR: 本文提出了首个评估多模态大模型（VLM）个性化能力的大型基准测试MMPB，发现现有VLM在个性化方面表现仍不理想，明确指出未来提升方向。


<details>
  <summary>Details</summary>
Motivation: 视觉个性化对用户导向的AI系统至关重要，实现模型与用户需求和偏好的一致性还是未被充分研究，尤其是在大规模视觉语言模型（VLM）上。

Method: 建立了包含1万组图像-查询对和111个可个性化概念的MMPB基准，涵盖人类、动物、物体和角色四类，并设计三大任务类型。采用23个广泛使用的VLM，通过三阶段协议（概念注入、多轮对话、个性化查询）进行评测。

Result: 大多数VLM在维护对话一致性、偏好处理和视觉线索适应方面表现不佳，经常出现拒答和“长上下文遗忘”，个性化能力有限。

Conclusion: 本文工作为VLM个性化研究奠定了基础，MMPB基准有助于发现模型不足和未来改进方向，为实现真正个性化的多模态AI系统提供支撑。

Abstract: Visual personalization is essential in user-facing AI systems such as smart
homes and healthcare, where aligning model behavior with user-centric concepts
is critical. However, recent large Vision-Language Models (VLMs), despite their
broad applicability, remain underexplored in their ability to adapt to
individual users. In this paper, we introduce MMPB, the first extensive
benchmark for evaluating VLMs on personalization. MMPB comprises 10k
image-query pairs and includes 111 personalizable concepts across four
categories: humans, animals, objects, and characters, with the human category
enriched with preference-grounded queries. We structure personalization into
three main task types, each highlighting a different key property of VLMs.
Using 23 widely used VLMs including both open- and closed-source models, we
evaluate personalization performance via a three-stage protocol: concept
injection, multi-turn dialogue, and personalized querying. Our findings
indicate that most VLMs (including some closed-source models) struggle with
personalization, particularly in maintaining consistency over dialogue,
handling user preferences, and adapting to visual cues. Our analysis reveals
that the challenges in VLM personalization (such as refusal behaviors and
long-context forgetting) highlight substantial room for improvement. By
identifying these limitations and offering a scalable benchmark, MMPB offers
valuable insights and a solid foundation for future research toward truly
personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB

</details>


### [21] [Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN](https://arxiv.org/abs/2509.22836)
*Roie Kazoom,Alon Goldberg,Hodaya Cohen,Ofer Hadar*

Main category: cs.CV

TL;DR: 本文提出了一种全新框架，用于生成高度可控且逼真的对抗性补丁攻击，可在保持视觉真实的情况下，随意指定输入图片和目标类别，实现精确目标误分类。该方法在多种主流神经网络上均取得了超过99%的攻击成功率，兼具现实性、目标性和黑盒适用性，在补丁攻击领域设立了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性补丁方法通常依赖不切实际的白盒假设、非定向目标，或生成容易被识别的明显人工痕迹补丁，降低了其在现实场景下的威胁。研究动机在于开发一种攻击方法，既可指定攻击目标、实现可控输出，又能保持补丁视觉真实，同时适用于更严苛的黑盒设置，以提升实际威胁性。

Method: 该方法采用生成式U-Net结构结合Grad-CAM引导补丁定位，实现语义感知的局部修改。这样既保证补丁的攻击有效性，也提升了伪装性。攻击者可自行选择输入图片和目标类别，实现精确可控的攻击目标，突破了传统方法的局限。

Result: 实验覆盖多种主流卷积网络（DenseNet-121、ResNet-50）和视觉Transformer（ViT-B/16、Swin-B/16等）。该方法在所有设置下均取得超过99%的攻击成功率（ASR及目标类成功率TCS）。且在白盒与黑盒环境下均超越历史方法，并优于强化伪装但有可检测瑕疵的现有补丁方法。

Conclusion: 该方法在补丁攻击最具挑战性的三个维度——真实性、目标可控、黑盒适用性上都实现了突破，设立了新基准。有效弥合了理论攻击强度与实际隐蔽性之间的鸿沟，为未来对抗鲁棒性研究提供了重要支撑。

Abstract: Adversarial patch attacks pose a severe threat to deep neural networks, yet
most existing approaches rely on unrealistic white-box assumptions, untargeted
objectives, or produce visually conspicuous patches that limit real-world
applicability. In this work, we introduce a novel framework for fully
controllable adversarial patch generation, where the attacker can freely choose
both the input image x and the target class y target, thereby dictating the
exact misclassification outcome. Our method combines a generative U-Net design
with Grad-CAM-guided patch placement, enabling semantic-aware localization that
maximizes attack effectiveness while preserving visual realism. Extensive
experiments across convolutional networks (DenseNet-121, ResNet-50) and vision
transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach
achieves state-of-the-art performance across all settings, with attack success
rates (ASR) and target-class success (TCS) consistently exceeding 99%.
  Importantly, we show that our method not only outperforms prior white-box
attacks and untargeted baselines, but also surpasses existing non-realistic
approaches that produce detectable artifacts. By simultaneously ensuring
realism, targeted control, and black-box applicability-the three most
challenging dimensions of patch-based attacks-our framework establishes a new
benchmark for adversarial robustness research, bridging the gap between
theoretical attack strength and practical stealthiness.

</details>


### [22] [Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention](https://arxiv.org/abs/2509.22839)
*Ibrahim Delibasoglu,Fredrik Heintz*

Main category: cs.CV

TL;DR: 本文提出了一种新的时间序列预测模型CrossScaleNet，将多尺度处理和patch-based cross-attention机制结合，实现了更强的预测能力和更高的可解释性，尤其是在识别时间重要性（temporal saliency）方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型在实际应用中对解释性要求极高，但现有的方法在实现高性能的同时往往牺牲了模型的透明度和可解释性。现有的时间重要性检测方法计算代价高，尤其是用于时间维度的显著性分析时更加复杂，因此亟需一种高效且可解释性强的方法。

Method: 提出CrossScaleNet模型，将patch-based cross-attention机制嵌入多尺度结构，通过训练中的嵌入的注意力层实现对时间显著性的内生解释，在减少计算消耗的同时提供决策过程的透明度。在多种合成数据集（有已知显著性）和真实数据集上进行了实证验证。

Result: CrossScaleNet能够准确检测时间重要性，同时在公开基准测试和真实任务中预测效果优于大多数基于Transformer的模型。此外，对比现有声称具有可解释性的模型，CrossScaleNet在标准评测上兼具更强的预测能力和可解释性。

Conclusion: CrossScaleNet实现了时间序列预测领域中预测精度和模型可解释性的平衡，在不同复杂度的数据集上均达到业界领先水平，为未来可解释时间序列模型的研究提供了新的方向。

Abstract: Explainability in time series forecasting is essential for improving model
transparency and supporting informed decision-making. In this work, we present
CrossScaleNet, an innovative architecture that combines a patch-based
cross-attention mechanism with multi-scale processing to achieve both high
performance and enhanced temporal explainability. By embedding attention
mechanisms into the training process, our model provides intrinsic
explainability for temporal saliency, making its decision-making process more
transparent. Traditional post-hoc methods for temporal saliency detection are
computationally expensive, particularly when compared to feature importance
detection. While ablation techniques may suffice for datasets with fewer
features, identifying temporal saliency poses greater challenges due to its
complexity. We validate CrossScaleNet on synthetic datasets with known saliency
ground truth and on established public benchmarks, demonstrating the robustness
of our method in identifying temporal saliency. Experiments on real-world
datasets for forecasting task show that our approach consistently outperforms
most transformer-based models, offering better explainability without
sacrificing predictive accuracy. Our evaluations demonstrate superior
performance in both temporal saliency detection and forecasting accuracy.
Moreover, we highlight that existing models claiming explainability often fail
to maintain strong performance on standard benchmarks. CrossScaleNet addresses
this gap, offering a balanced approach that captures temporal saliency
effectively while delivering state-of-the-art forecasting performance across
datasets of varying complexity.

</details>


### [23] [Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging](https://arxiv.org/abs/2509.22841)
*Yi Luo,Yike Guo,Hamed Hooshangnejad,Rui Zhang,Xue Feng,Quan Chen,Wil Ngwa,Kai Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种基于迁移学习和多模态交互感知网络的方法，有效提升了肺癌PET/CT中的内部肿瘤体积（IGTV）分割精度。


<details>
  <summary>Details</summary>
Motivation: IGTV精确勾画对肺癌放疗至关重要，但受限于训练数据稀缺和PET图像边界信号弱，现有方法效果有限。

Method: 提出一种迁移学习方法，将MAMBA多模态交互感知网络在大规模GTV数据集上预训练，然后在私人IGTV数据集上微调。同时引入切片交互模块（SIM）于2.5D分割框架中，用通道和空间注意力结合深度卷积建模切片间依赖。

Result: 在私人IGTV数据集上，方法Dice系数达到0.609，显著超过传统基线0.385。

Conclusion: 迁移学习结合多模态及SIM模块，显著提升了肺癌IGTV分割性能，为放疗计划提供了更可靠的技术支持，具有良好的临床应用前景。

Abstract: Lung cancer remains the leading cause of cancerrelated deaths globally.
Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is
pivotal for optimal radiation therapy in mobile tumors such as lung cancer to
account for tumor motion, yet is hindered by the limited availability of
annotated IGTV datasets and attenuated PET signal intensity at tumor
boundaries. In this study, we present a transfer learningbased methodology
utilizing a multimodal interactive perception network with MAMBA, pre-trained
on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a
private IGTV cohort. This cohort constitutes the PET/CT subset of the
Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the
challenge of weak PET intensities in IGTV peripheral slices, we introduce a
slice interaction module (SIM) within a 2.5D segmentation framework to
effectively model inter-slice relationships. Our proposed module integrates
channel and spatial attention branches with depthwise convolutions, enabling
more robust learning of slice-to-slice dependencies and thereby improving
overall segmentation performance. A comprehensive experimental evaluation
demonstrates that our approach achieves a Dice of 0.609 on the private IGTV
dataset, substantially surpassing the conventional baseline score of 0.385.
This work highlights the potential of transfer learning, coupled with advanced
multimodal techniques and a SIM to enhance the reliability and clinical
relevance of IGTV segmentation for lung cancer radiation therapy planning.

</details>


### [24] [ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models](https://arxiv.org/abs/2509.22864)
*Yixuan Hu,Yuxuan Xue,Simon Klenk,Daniel Cremers,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 本文提出了ControlEvents，一种利用扩散模型合成高质量事件相机数据的新方法，可通过文本标签、2D骨架、3D姿态等多种方式进行控制，极大简化了事件数据集标注与生成。


<details>
  <summary>Details</summary>
Motivation: 由于事件相机具备高时间分辨率与高动态范围，但受限于大规模准确标注数据集难以获取，影响了事件视觉相关任务的发展。

Method: 本文提出ControlEvents，借鉴了大模型（如Stable Diffusion）中的扩散先验，结合最少的微调和有限的标注数据，通过扩散生成模型，基于不同控制信号（如文本、2D骨架、3D姿态）生成高质量事件相机数据。

Result: 实验结果表明，合成的标注事件数据可提升视觉识别、2D骨架估计和3D姿态估计算法的性能，并能实现基于训练时未出现文本标签的事件生成，展现出强大的泛化和文本生成能力。

Conclusion: ControlEvents简化并降低了事件数据集标注的成本，为事件相机相关深度学习任务的数据获取和性能提升提供了新的实用途径。

Abstract: In recent years, event cameras have gained significant attention due to their
bio-inspired properties, such as high temporal resolution and high dynamic
range. However, obtaining large-scale labeled ground-truth data for event-based
vision tasks remains challenging and costly. In this paper, we present
ControlEvents, a diffusion-based generative model designed to synthesize
high-quality event data guided by diverse control signals such as class text
labels, 2D skeletons, and 3D body poses. Our key insight is to leverage the
diffusion prior from foundation models, such as Stable Diffusion, enabling
high-quality event data generation with minimal fine-tuning and limited labeled
data. Our method streamlines the data generation process and significantly
reduces the cost of producing labeled event datasets. We demonstrate the
effectiveness of our approach by synthesizing event data for visual
recognition, 2D skeleton estimation, and 3D body pose estimation. Our
experiments show that the synthesized labeled event data enhances model
performance in all tasks. Additionally, our approach can generate events based
on unseen text labels during training, illustrating the powerful text-based
generation capabilities inherited from foundation models.

</details>


### [25] [Learning KAN-based Implicit Neural Representations for Deformable Image Registration](https://arxiv.org/abs/2509.22874)
*Nikita Drozdov,Marat Zinovev,Dmitry Sorokin*

Main category: cs.CV

TL;DR: 本文提出了一种结合Kolmogorov-Arnold Networks（KANs）和隐式神经表达（INRs）的方法，用于医学图像的可变形配准，并通过随机基函数采样，既保证了精度又降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有学习型方法如CNN、Transformer等虽然推理速度快，但对大数据量依赖强，并且在某些器官或模态下精度不及传统方法。INRs提供了新的方向，但计算效率和学习稳定性面临挑战，因此急需更高效、精确且稳定的DIR方法。

Method: 文章首次将KAN引入INR框架的DIR中，并提出了两种方法：KAN-IDIR与RandKAN-IDIR。通过随机基函数采样，减少所需基函数数量，降低计算量。此外，RandKAN-IDIR使用随机采样，避免了可学习基索引带来的额外训练复杂性。方法在肺CT、脑MRI、心脏MRI三种数据集上进行了评估，并与现有多种方法进行全面对比。

Result: KAN-IDIR和RandKAN-IDIR在所有评估的模态和解剖结构上，都实现了INR方法中的最高配准准确性，且计算消耗和多随机种子下的学习稳定性优于其他方法。特别是RandKAN-IDIR略优于可学习基数的模型，并大幅简化了训练。

Conclusion: 该文方法在保证高配准精度的同时，实现了显著的计算效率提升和学习过程的稳定性，推动了INR在医学图像配准中的应用，为相关下游应用提供了更优的技术路径。

Abstract: Deformable image registration (DIR) is a cornerstone of medical image
analysis, enabling spatial alignment for tasks like comparative studies and
multi-modal fusion. While learning-based methods (e.g., CNNs, transformers)
offer fast inference, they often require large training datasets and struggle
to match the precision of classical iterative approaches on some organ types
and imaging modalities. Implicit neural representations (INRs) have emerged as
a promising alternative, parameterizing deformations as continuous mappings
from coordinates to displacement vectors. However, this comes at the cost of
requiring instance-specific optimization, making computational efficiency and
seed-dependent learning stability critical factors for these methods. In this
work, we propose KAN-IDIR and RandKAN-IDIR, the first integration of
Kolmogorov-Arnold Networks (KANs) into deformable image registration with
implicit neural representations (INRs). Our proposed randomized basis sampling
strategy reduces the required number of basis functions in KAN while
maintaining registration quality, thereby significantly lowering computational
costs. We evaluated our approach on three diverse datasets (lung CT, brain MRI,
cardiac MRI) and compared it with competing instance-specific learning-based
approaches, dataset-trained deep learning models, and classical registration
approaches. KAN-IDIR and RandKAN-IDIR achieved the highest accuracy among
INR-based methods across all evaluated modalities and anatomies, with minimal
computational overhead and superior learning stability across multiple random
seeds. Additionally, we discovered that our RandKAN-IDIR model with randomized
basis sampling slightly outperforms the model with learnable basis function
indices, while eliminating its additional training-time complexity.

</details>


### [26] [Convolutional Set Transformer](https://arxiv.org/abs/2509.22889)
*Federico Chinello,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 本文提出了一种可直接对3D图像张量操作的全新神经网络架构——卷积集变换器（Convolutional Set Transformer, CST），专为处理类别、场景等高层语义一致但视觉异质的图像集任务设计，且具备优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的集合输入网络（如Deep Sets和Set Transformer）仅支持向量输入，不能直接处理3D图像张量，需要先用CNN提取特征后再建立图像间关系，这样割裂了特征提取和上下文建模，不能深入挖掘两者的协同效应。

Method: CST将多张图像的3D张量作为输入，在同一网络中兼顾特征提取与上下文关系建模，避免传统方法串联CNN和集合网络的弊端，并且天然支持如Grad-CAM等CNN可解释性方法。

Result: CST在集合分类、集合异常检测等任务上表现出色，并且可以在大规模数据集上预训练，之后通过迁移学习适配新领域和新任务。此外，作者公开了基于ImageNet预训练的CST模型以支持后续研究。

Conclusion: CST不仅提升了处理视觉异质图像集的能力，还增强了模型的可解释性和迁移能力，为集合型图像理解任务提供了更优解决方案。

Abstract: We introduce the Convolutional Set Transformer (CST), a novel neural
architecture designed to process image sets of arbitrary cardinality that are
visually heterogeneous yet share high-level semantics - such as a common
category, scene, or concept. Existing set-input networks, e.g., Deep Sets and
Set Transformer, are limited to vector inputs and cannot directly handle 3D
image tensors. As a result, they must be cascaded with a feature extractor,
typically a CNN, which encodes images into embeddings before the set-input
network can model inter-image relationships. In contrast, CST operates directly
on 3D image tensors, performing feature extraction and contextual modeling
simultaneously, thereby enabling synergies between the two processes. This
design yields superior performance in tasks such as Set Classification and Set
Anomaly Detection and further provides native compatibility with CNN
explainability methods such as Grad-CAM, unlike competing approaches that
remain opaque. Finally, we show that CSTs can be pre-trained on large-scale
datasets and subsequently adapted to new domains and tasks through standard
Transfer Learning schemes. To support further research, we release CST-15, a
CST backbone pre-trained on ImageNet
(https://github.com/chinefed/convolutional-set-transformer).

</details>


### [27] [TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection](https://arxiv.org/abs/2509.22909)
*Abdulkarim Atrash,Omar Moured,Yufan Chen,Jiaming Zhang,Seyda Ertekin,Omur Ugur*

Main category: cs.CV

TL;DR: 本文提出了一种用于红外小目标检测的新方法TY-RIST，并在多个数据集上达到了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在国防和监控中十分重要，但面临特征弱、易产生误检漏检，以及计算成本高的挑战。

Method: 提出了基于YOLOv12n优化结构的TY-RIST方法：包含步幅感知骨干网络、高分辨率检测头、级联坐标注意力模块以及分支剪枝策略（降低25.5%计算量），并引入Normalized Gaussian Wasserstein Distance (NWD)提升回归稳定性。

Result: 在四个基准数据集和20个模型上，TY-RIST的mAP@0.5提升7.9%，Precision提升3%，Recall提升10.2%，单GPU下最高达123 FPS。跨数据集测试验证了极强的泛化能力。

Conclusion: TY-RIST显著提升了红外小目标检测的精度与实时性，降低了计算成本，具备广阔的实际应用前景。

Abstract: Infrared small target detection (IRSTD) is critical for defense and
surveillance but remains challenging due to (1) target loss from minimal
features, (2) false alarms in cluttered environments, (3) missed detections
from low saliency, and (4) high computational costs. To address these issues,
we propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a
stride-aware backbone with fine-grained receptive fields, (2) a high-resolution
detection head, (3) cascaded coordinate attention blocks, and (4) a branch
pruning strategy that reduces computational cost by about 25.5% while
marginally improving accuracy and enabling real-time inference. We also
incorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance
regression stability. Extensive experiments on four benchmarks and across 20
different models demonstrate state-of-the-art performance, improving mAP at 0.5
IoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123
FPS on a single GPU. Cross-dataset validation on a fifth dataset further
confirms strong generalization capability. Additional results and resources are
available at https://www.github.com/moured/TY-RIST

</details>


### [28] [Learning Unified Representation of 3D Gaussian Splatting](https://arxiv.org/abs/2509.22917)
*Yuelin Xin,Yuheng Liu,Xiaohui Xie,Xinke Li*

Main category: cs.CV

TL;DR: 本文针对3D高斯撒点技术（3DGS）中的矢量化表达问题，提出了一种基于连续子流形场的嵌入式新表达方式，提高其在神经网络学习系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS支持高效、显式的三维重建，但其基于参数的表达方式在用作特征时难以兼容神经网络，因参数的不唯一性和异质性造成模型表现高度依赖数据。

Method: 提出利用连续子流形场作为嵌入空间来表达3DGS，将高斯原语的本质信息 encapsulate 成统一、意义明确的特征嵌入，保证表达的唯一性和通道一致性。

Result: 该新的嵌入表达方式能更好保留三维结构与颜色信息，使神经网络在学习3DGS时更为高效且泛化能力更强。

Conclusion: 基于连续子流形场的3DGS表达为三维重建和神经网络特征学习应用提供了更稳定、有效的输入表征途径。

Abstract: A well-designed vectorized representation is crucial for the learning systems
natively based on 3D Gaussian Splatting. While 3DGS enables efficient and
explicit 3D reconstruction, its parameter-based representation remains hard to
learn as features, especially for neural-network-based models. Directly feeding
raw Gaussian parameters into learning frameworks fails to address the
non-unique and heterogeneous nature of the Gaussian parameterization, yielding
highly data-dependent models. This challenge motivates us to explore a more
principled approach to represent 3D Gaussian Splatting in neural networks that
preserves the underlying color and geometric structure while enforcing unique
mapping and channel homogeneity. In this paper, we propose an embedding
representation of 3DGS based on continuous submanifold fields that encapsulate
the intrinsic information of Gaussian primitives, thereby benefiting the
learning of 3DGS.

</details>


### [29] [Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings](https://arxiv.org/abs/2509.22925)
*Yuanzhi Zhu,Xi Wang,Stéphane Lathuilière,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出了soft embeddings，将离散token输出替换为生成器分布下的期望嵌入，使得单步生成器可端到端优化，解决了以往蒸馏生成器无法细化的问题，在多项任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有从掩码扩散模型(MDMs)蒸馏得到的单步生成器虽然高效，但受限于：1) 继承教师模型偏差；2) 离散输出阻断梯度，无法进行对抗训练、奖励微调和测试时优化等后处理。为此，亟需一种方法兼容离散生成和端到端可微优化。

Method: 作者提出soft embeddings，用生成器输出分布下的期望嵌入替代原有离散token，既保留了表达充分性，又实现了端到端可微。将其集成到Di[M]O蒸馏框架，命名为Soft-Di[M]O，使单步生成器支持GAN细化、奖励微调和测试时嵌入优化。

Result: 在不同的MDM教师（如MaskBit、MaskGen）上，Soft-Di[M]O在多任务上表现出SOTA。一步生成时，ImageNet-256上经过GAN细化的FID达1.56，文本生成图像任务上GenEval和HPS分均更高，TTEO进一步提升性能。

Conclusion: Soft embeddings和Soft-Di[M]O不仅解决了先前单步生成器不可微无法进一步优化的难题，也显著提升了离散生成模型的表现与可微性，为文本及图像生成任务提供了新的高效路径。

Abstract: One-step generators distilled from Masked Diffusion Models (MDMs) compress
multiple sampling steps into a single forward pass, enabling efficient text and
image synthesis. However, they suffer two key limitations: they inherit
modeling bias from the teacher, and their discrete token outputs block gradient
flow, preventing post-distillation refinements such as adversarial training,
reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this
work, we introduce soft embeddings, a simple relaxation that replaces discrete
tokens with the expected embeddings under the generator's output distribution.
Soft embeddings preserve representation fidelity for one-step discrete
generator while providing a fully differentiable continuous surrogate that is
compatible with teacher backbones and tokenizer decoders. Integrating soft
embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes
one-step generators end-to-end trainable and enables straightforward
application of GAN-based refinement, differentiable reward fine-tuning, and
TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),
Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image
performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,
along with higher GenEval and HPS scores on text-to-image with reward
fine-tuning, and further gains from TTEO.

</details>


### [30] [FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning](https://arxiv.org/abs/2509.22930)
*Chenghan Yang,Peng Zhou,Dong-Sheng Zhang,Yueyun Wang,Hong-Bin Shen,Xiaoyong Pan*

Main category: cs.CV

TL;DR: FishAI 2.0是一种针对海洋稀有鱼类少样本识别困难的问题，结合多模态训练和数据增强的智能识别框架，有效提升了识别准确率，对海洋生态监测和保护具有重要应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统海洋生物图像识别面临数据集不完整、模型识别准确率低，尤其在稀有鱼类的少样本条件下，数据稀缺严重影响了模型性能。需要创新方法解决少样本类别识别效率和准确率低的问题。

Method: 本研究提出FishAI 2.0框架，融合多模态少样本深度学习与图像生成增强。首先建立分层海洋鱼类基准数据集，为模型训练提供全面数据。针对稀有类别数据稀缺问题，利用大语言模型DeepSeek生成高质量文本描述，再输入Stable Diffusion 2，通过层级扩散策略生成增强图像，拓展多模态特征空间。随后，联合增强后的视觉-文本数据集，使用对比语言-图像CLIP模型进行训练，实现鲁棒的少样本识别。

Result: FishAI 2.0在鱼类家族层面Top-1准确率91.67%、Top-5准确率97.97%，明显优于基线CLIP和ViT模型，尤其对训练样本少于10的稀有类别提升更大。在属和种层面分别达到87.58%和85.42%的Top-1准确率，表现出良好的实际应用前景。

Conclusion: FishAI 2.0有效提升了海洋鱼类识别的效率和准确性，提供了一种可拓展的技术方案用于海洋生态监测与保护，具备较高的科学意义和实践价值。

Abstract: Traditional marine biological image recognition faces challenges of
incomplete datasets and unsatisfactory model accuracy, particularly for
few-shot conditions of rare species where data scarcity significantly hampers
the performance. To address these issues, this study proposes an intelligent
marine fish recognition framework, FishAI 2.0, integrating multimodal few-shot
deep learning techniques with image generation for data augmentation. First, a
hierarchical marine fish benchmark dataset, which provides a comprehensive data
foundation for subsequent model training, is utilized to train the FishAI 2.0
model. To address the data scarcity of rare classes, the large language model
DeepSeek was employed to generate high-quality textual descriptions, which are
input into Stable Diffusion 2 for image augmentation through a hierarchical
diffusion strategy that extracts latent encoding to construct a multimodal
feature space. The enhanced visual-textual datasets were then fed into a
Contrastive Language-Image Pre-Training (CLIP) based model, enabling robust
few-shot image recognition. Experimental results demonstrate that FishAI 2.0
achieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent
at the family level, outperforming baseline CLIP and ViT models with a
substantial margin for the minority classes with fewer than 10 training
samples. To better apply FishAI 2.0 to real-world scenarios, at the genus and
species level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58
percent and 85.42 percent, demonstrating practical utility. In summary, FishAI
2.0 improves the efficiency and accuracy of marine fish identification and
provides a scalable technical solution for marine ecological monitoring and
conservation, highlighting its scientific value and practical applicability.

</details>


### [31] [Brain Tumor Classification from MRI Scans via Transfer Learning and Enhanced Feature Representation](https://arxiv.org/abs/2509.22956)
*Ahta-Shamul Hoque Emran,Hafija Akter,Abdullah Al Shiam,Abu Saleh Musa Miah,Anichur Rahman,Fahmid Al Farid,Hezerul Abdul Karim*

Main category: cs.CV

TL;DR: 本研究提出一种自动高效的脑肿瘤检测深度学习框架，并发布了新的脑肿瘤MRI数据集，有效提升了检测性能与数据可用性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的及时检测对改善患者预后至关重要，然而现有脑部MRI数据集资源有限，且高效自动检测方法有待发展。

Method: 采用预训练的ResNet50进行特征提取，结合Global Average Pooling和线性投影获得高层次表征。提出Dense-Dropout序列模块，增强特征非线性表达，减少过拟合，提高模型鲁棒性。同时自建MMCBT数据集，通过数据增强解决类别不平衡问题，适用于深度学习训练。

Result: 提出的框架能够自动高效地从MRI图像中检测出脑肿瘤。自制的MMCBT数据集为相关研究提供了可靠资源，且经过确认的数据增强获得了均衡的数据分布。Dense-Dropout结构提升模型性能和泛化能力。

Conclusion: 本工作在方法和数据资源两方面对脑肿瘤MRI智能检测具有创新贡献，为相关研究和实际应用提供了基础和推动。

Abstract: Brain tumors are abnormal cell growths in the central nervous system (CNS),
and their timely detection is critical for improving patient outcomes. This
paper proposes an automatic and efficient deep-learning framework for brain
tumor detection from magnetic resonance imaging (MRI) scans. The framework
employs a pre-trained ResNet50 model for feature extraction, followed by Global
Average Pooling (GAP) and linear projection to obtain compact, high-level image
representations. These features are then processed by a novel Dense-Dropout
sequence, a core contribution of this work, which enhances non-linear feature
learning, reduces overfitting, and improves robustness through diverse feature
transformations. Another major contribution is the creation of the Mymensingh
Medical College Brain Tumor (MMCBT) dataset, designed to address the lack of
reliable brain tumor MRI resources. The dataset comprises MRI scans from 209
subjects (ages 9 to 65), including 3671 tumor and 13273 non-tumor images, all
clinically verified under expert supervision. To overcome class imbalance, the
tumor class was augmented, resulting in a balanced dataset well-suited for deep
learning research.

</details>


### [32] [Hemorica: A Comprehensive CT Scan Dataset for Automated Brain Hemorrhage Classification, Segmentation, and Detection](https://arxiv.org/abs/2509.22993)
*Kasra Davoodi,Mohammad Hoseyni,Javad Khoramdel,Reza Barati,Reihaneh Mortazavi,Amirhossein Nikoofard,Mahdi Aliyari-Shoorehdeli,Jaber Hatam Parikhan*

Main category: cs.CV

TL;DR: 本文介绍了Hemorica，这是一个涵盖372例头部CT的公开数据集，专门用于颅内出血（ICH）及其五种亚型的检测和分割。通过细致标注和严格质控，数据集增强了AI模型在ICH诊断中的发展和评测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的ICH AI研究受限于数据碎片化，难以形成鲁棒的智能诊断工具。临床需求推动高质量、统一的数据集公开，以助力相关AI模型的开发和验证。

Method: 作者收集了2012-2024年间372例头部CT数据，并为5种ICH亚型标注了病人级和切片级分类标签、亚型特异性边界框、二维像素掩码与三维体素掩码。采用专家双重阅片与神经外科医生裁决，保证标注一致性。统计分析用于确认数据集的临床真实性。利用主流CNN与Transformer架构进行二分类和分割任务基线测试。

Result: MobileViT-XS在二分类任务中F1分数为87.8%；DenseNet161编码的U-Net在二分类分割任务中Dice分数为85.5%。

Conclusion: Hemorica数据集为ICH检测及分割提供了细粒度、统一的基准，促进了多任务与迁移学习，对未来AI辅助ICH诊断系统的研究与应用具有重要价值。

Abstract: Timely diagnosis of Intracranial hemorrhage (ICH) on Computed Tomography (CT)
scans remains a clinical priority, yet the development of robust Artificial
Intelligence (AI) solutions is still hindered by fragmented public data. To
close this gap, we introduce Hemorica, a publicly available collection of 372
head CT examinations acquired between 2012 and 2024. Each scan has been
exhaustively annotated for five ICH subtypes-epidural (EPH), subdural (SDH),
subarachnoid (SAH), intraparenchymal (IPH), and intraventricular (IVH)-yielding
patient-wise and slice-wise classification labels, subtype-specific bounding
boxes, two-dimensional pixel masks and three-dimensional voxel masks. A
double-reading workflow, preceded by a pilot consensus phase and supported by
neurosurgeon adjudication, maintained low inter-rater variability.
Comprehensive statistical analysis confirms the clinical realism of the
dataset. To establish reference baselines, standard convolutional and
transformer architectures were fine-tuned for binary slice classification and
hemorrhage segmentation. With only minimal fine-tuning, lightweight models such
as MobileViT-XS achieved an F1 score of 87.8% in binary classification, whereas
a U-Net with a DenseNet161 encoder reached a Dice score of 85.5% for binary
lesion segmentation that validate both the quality of the annotations and the
sufficiency of the sample size. Hemorica therefore offers a unified,
fine-grained benchmark that supports multi-task and curriculum learning,
facilitates transfer to larger but weakly labelled cohorts, and facilitates the
process of designing an AI-based assistant for ICH detection and quantification
systems.

</details>


### [33] [ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View](https://arxiv.org/abs/2509.23008)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于自回归（AR）模型的新颖框架ARSS，用于从单张图片和预定义相机轨迹生成新视角图像，克服了扩散模型在世界建模中的一致性和增量学习瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型虽生成质量高，但在基于世界建模的多视角新视图生成任务表现有限，主要因为其非因果的生成方式导致不同视角间的不一致与难以利用已知信息增量适应新查询。相比之下，自回归模型因其因果性有望提升生成序列间的连贯性和一致性。

Method: 作者提出ARSS框架，利用GPT风格的解码器自回归模型，从视频tokenizer提取离散token序列，并引入相机编码器实现相机轨迹到三维位置编码的转换。同时设计了自回归Transformer模块，随机打乱空间token顺序，增强生成质量同时保持时间因果性。

Result: 该方法在多个公开数据集上的定量和定性实验显示，ARSS的效果与最先进的基于扩散模型的方法相当或更优。

Conclusion: ARSS方法通过自回归结构实现了更高质量和一致性的多视角新视图生成，为世界建模领域提供了有效的新思路。

Abstract: Despite their exceptional generative quality, diffusion models have limited
applicability to world modeling tasks, such as novel view generation from
sparse inputs. This limitation arises because diffusion models generate outputs
in a non-causal manner, often leading to distortions or inconsistencies across
views, and making it difficult to incrementally adapt accumulated knowledge to
new queries. In contrast, autoregressive (AR) models operate in a causal
fashion, generating each token based on all previously generated tokens. In
this work, we introduce \textbf{ARSS}, a novel framework that leverages a
GPT-style decoder-only AR model to generate novel views from a single image,
conditioned on a predefined camera trajectory. We employ a video tokenizer to
map continuous image sequences into discrete tokens and propose a camera
encoder that converts camera trajectories into 3D positional guidance. Then to
enhance generation quality while preserving the autoregressive structure, we
propose a autoregressive transformer module that randomly permutes the spatial
order of tokens while maintaining their temporal order. Extensive qualitative
and quantitative experiments on public datasets demonstrate that our method
performs comparably to, or better than, state-of-the-art view synthesis
approaches based on diffusion models. Our code will be released upon paper
acceptance.

</details>


### [34] [Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition](https://arxiv.org/abs/2509.23009)
*Masato Kobayashi,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: 针对动作识别中的静态偏置问题，提出将时序动态信息与场景静态信息分离的方法，通过独立性损失和场景预测损失联合训练，有效缓解静态偏置，并在实验中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别模型过于依赖静态线索（静态偏置），导致模型对真实场景和零样本动作识别效果差。作者的动机是提升模型对动态人类动作的识别能力，减小静态偏置带来的负面影响。

Method: 提出分离时间动态信息和静态场景信息的网络架构，利用统计独立性损失（让偏置与非偏置流独立），并引入场景预测损失作为辅助任务，联合优化。

Result: 实验证明该方法能有效降低动作识别模型的静态偏置，并通过消融实验验证了场景预测损失对于减缓静态偏置的重要作用。

Conclusion: 论文方法成功减少了动作识别中的静态偏置，提高了模型泛化能力，对实际应用和零样本识别场景有积极意义。

Abstract: Action recognition models rely excessively on static cues rather than dynamic
human motion, which is known as static bias. This bias leads to poor
performance in real-world applications and zero-shot action recognition. In
this paper, we propose a method to reduce static bias by separating temporal
dynamic information from static scene information. Our approach uses a
statistical independence loss between biased and unbiased streams, combined
with a scene prediction loss. Our experiments demonstrate that this method
effectively reduces static bias and confirm the importance of scene prediction
loss.

</details>


### [35] [Desensitizing for Improving Corruption Robustness in Point Cloud Classification through Adversarial Training](https://arxiv.org/abs/2509.23010)
*Zhiqiang Tian,Weigang Li,Chunhua Deng,Junwei Hu,Yongqiang Wang,Wenping Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗性训练方法（DesenAT），通过减少深度神经网络对点云特征的依赖，提高模型在点云数据腐蚀情况下的鲁棒性，实验证明无需牺牲干净数据集表现即可增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前点云任务中，场景复杂性及传感器与处理的误差导致点云数据不可避免地被破坏，且深度神经网络对部分输入特征过度依赖，从而引发鲁棒性问题。现有工作尚不清楚降低这种依赖是否能提升模型抗腐蚀能力，因此作者试图通过定量分析解决该疑问。

Method:  (1) 利用Shapley值定量分析DNN对点云特征的敏感性；(2) 设计一种‘去敏感化对抗训练’（DesenAT）：首先去除对模型贡献大的特征，并通过空间变换模拟点云腐蚀场景，生成对抗样本，进行对抗训练；(3) 采用自蒸馏，将干净样本的知识迁移到对抗样本，缓解丢失信息问题并提升训练效果。

Result: 实验显示，传统方法训练的模型对特定特征依赖过高，对这些高敏感特征的剪枝会急剧恶化性能。DesenAT在ModelNet-C和PointCloud-C数据集上能够显著提升模型对腐蚀点云的鲁棒性，且对干净数据集性能无损。

Conclusion: 本文证实了深度模型对点云特征过度依赖确实影响鲁棒性，并提出的DesenAT能有效缓解这一问题。该方法无损精度下，提升了模型的实际应用可靠性，对鲁棒点云学习具有理论和实践意义。

Abstract: Due to scene complexity, sensor inaccuracies, and processing imprecision,
point cloud corruption is inevitable. Over-reliance on input features is the
root cause of DNN vulnerabilities. It remains unclear whether this issue exists
in 3D tasks involving point clouds and whether reducing dependence on these
features can enhance the model's robustness to corrupted point clouds. This
study attempts to answer these questions. Specifically, we quantified the
sensitivity of the DNN to point cloud features using Shapley values and found
that models trained using traditional methods exhibited high sensitivity values
for certain features. Furthermore, under an equal pruning ratio, prioritizing
the pruning of highly sensitive features causes more severe damage to model
performance than random pruning. We propose `Desensitized Adversarial Training'
(DesenAT), generating adversarial samples using feature desensitization and
conducting training within a self-distillation framework, which aims to
alleviate DNN's over-reliance on point clouds features by smoothing
sensitivity. First, data points with high contribution components are
eliminated, and spatial transformation is used to simulate corruption scenes,
generate adversarial samples, and conduct adversarial training on the model.
Next, to compensate for information loss in adversarial samples, we use the
self-distillation method to transfer knowledge from clean samples to
adversarial samples, and perform adversarial training in a distillation
manner.Extensive experiments on ModelNet-C and PointCloud-C demonstrate show
that the propose method can effectively improve the robustness of the model
without reducing the performance of clean data sets. This code is publicly
available at
\href{https://github.com/JerkyT/DesenAT/tree/master}{https://github.com/JerkyT/DesenAT}.

</details>


### [36] [Geometry-Aware Losses for Structure-Preserving Text-to-Sign Language Generation](https://arxiv.org/abs/2509.23011)
*Zetian Wu,Tianshuo Zhou,Stefan Lee,Liang Huang*

Main category: cs.CV

TL;DR: 本文提出了一种面向文本到手语视频翻译的新方法，通过引入骨骼几何约束和动态机制，使生成的手语动作更符合人体结构和自然运动规律，大幅提升了运动自然度和解剖一致性。


<details>
  <summary>Details</summary>
Motivation: 手语视频生成助力聋人及听障人士交流，但现有方法常忽视骨骼结构及协作模式，导致动作僵硬不自然，缺乏解剖合理性。

Method: 该方法显式建模肩膀、手臂、手等骨骼关节的关系，通过几何约束关节位置、骨长和动态，并在训练中引入父节点相对权重机制以增强手指灵活性，减少僵硬。此外，通过骨骼姿态损失与骨长约束来保证解剖结构一致性。

Result: 该方法使得系统性能与真实手语动作之间的差距缩小了56.51%，并将骨长和运动变化的偏差分别减少了18.76%和5.48%。

Conclusion: 实验显示，该方法显著提升了手语视频合成的解剖真实性和运动自然性，对聋人辅助交流具有实际应用价值。

Abstract: Sign language translation from text to video plays a crucial role in enabling
effective communication for Deaf and hard--of--hearing individuals. A major
challenge lies in generating accurate and natural body poses and movements that
faithfully convey intended meanings. Prior methods often neglect the anatomical
constraints and coordination patterns of human skeletal motion, resulting in
rigid or biomechanically implausible outputs. To address this, we propose a
novel approach that explicitly models the relationships among skeletal
joints--including shoulders, arms, and hands--by incorporating geometric
constraints on joint positions, bone lengths, and movement dynamics. During
training, we introduce a parent-relative reweighting mechanism to enhance
finger flexibility and reduce motion stiffness. Additionally, bone-pose losses
and bone-length constraints enforce anatomically consistent structures. Our
method narrows the performance gap between the previous best and the
ground-truth oracle by 56.51%, and further reduces discrepancies in bone length
and movement variance by 18.76% and 5.48%, respectively, demonstrating
significant gains in anatomical realism and motion naturalness.

</details>


### [37] [Planning with Unified Multimodal Models](https://arxiv.org/abs/2509.23014)
*Yihao Sun,Zhilong Zhang,Yang Yu,Pierre-Luc Bacon*

Main category: cs.CV

TL;DR: 本文提出了一种基于统一多模态模型（UMMs）的决策规划框架Uni-Plan，通过融合图像和文本推理，提升了长周期规划任务中的决策性能，并显著超越现有视觉-语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型和视觉-语言模型在决策推理上主要依赖文本，限制了推理和决策的有效性。新近的UMMs具备多模态输入输出能力，推动了生成视觉内容的推理新方向，因此亟需探索如何利用其在决策制定中的全部潜力。

Method: 作者提出了Uni-Plan框架，基于UMMs实现policy、dynamics model和value function三合一。同时，创新性地设计了自判别过滤方法，通过生成模型自我判别以过滤掉动态预测中的虚假内容，提升模型的可靠性。

Result: 在长周期规划任务上，Uni-Plan成功率显著高于基于VLM的方法。在无专家示范、同等训练数据规模下，也展现出更优表现和数据可扩展性。

Conclusion: Uni-Plan为在UMMs框架下实现更强推理和决策能力奠定了基础，对未来多模态智能体的研究具有重要启示和推动作用。

Abstract: With the powerful reasoning capabilities of large language models (LLMs) and
vision-language models (VLMs), many recent works have explored using them for
decision-making. However, most of these approaches rely solely on
language-based reasoning, which limits their ability to reason and make
informed decisions. Recently, a promising new direction has emerged with
unified multimodal models (UMMs), which support both multimodal inputs and
outputs. We believe such models have greater potential for decision-making by
enabling reasoning through generated visual content. To this end, we propose
Uni-Plan, a planning framework built on UMMs. Within this framework, a single
model simultaneously serves as the policy, dynamics model, and value function.
In addition, to avoid hallucinations in dynamics predictions, we present a
novel approach self-discriminated filtering, where the generative model serves
as a self-discriminator to filter out invalid dynamics predictions. Experiments
on long-horizon planning tasks show that Uni-Plan substantially improves
success rates compared to VLM-based methods, while also showing strong data
scalability, requiring no expert demonstrations and achieving better
performance under the same training-data size. This work lays a foundation for
future research in reasoning and decision-making with UMMs.

</details>


### [38] [Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy](https://arxiv.org/abs/2509.23022)
*Xiafeng Man,Zhipeng Wei,Jingjing Chen*

Main category: cs.CV

TL;DR: 本论文提出D-Plus-Minus（DPM）框架，用于在文本到图像扩散模型中检测版权侵权，无需访问原始训练集或文本提示，能够高效、可解释地保护知识产权。


<details>
  <summary>Details</summary>
Motivation: 随着诸如Stable Diffusion等大型视觉模型的广泛应用，这些模型可能会在未经授权情况下记忆并再现受版权保护的内容，引发法律和伦理担忧。现有检测方法通常不够健壮且理论基础薄弱，因此亟需更完善的检测方法。

Method: 作者借鉴差分隐私（DP）理论，提出了conditional sensitivity指标来衡量训练集中加入或移除特定样本对模型输出的影响，并据此设计了DPM检测框架。DPM通过对模型进行“学习”与“遗忘”方向的微调模拟样本的包含与排除，并结合统计方法分离概念影响和全局参数偏移。此外，作者构建了专门的数据集CIDD，用于标准化检测评测。

Result: 实验结果显示，DPM能够在无需原始训练数据或文本提示的前提下，有效检测出文本到图像生成模型中的版权侵权内容。

Conclusion: DPM为生成式AI时代的知识产权保护提供了高效、可解释且实用的侵权检测方案，并推动了该领域检测基准的标准化。

Abstract: The widespread deployment of large vision models such as Stable Diffusion
raises significant legal and ethical concerns, as these models can memorize and
reproduce copyrighted content without authorization. Existing detection
approaches often lack robustness and fail to provide rigorous theoretical
underpinnings. To address these gaps, we formalize the concept of copyright
infringement and its detection from the perspective of Differential Privacy
(DP), and introduce the conditional sensitivity metric, a concept analogous to
sensitivity in DP, that quantifies the deviation in a diffusion model's output
caused by the inclusion or exclusion of a specific training data point. To
operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc
detection framework that identifies copyright infringement in text-to-image
diffusion models. Specifically, DPM simulates inclusion and exclusion processes
by fine-tuning models in two opposing directions: learning or unlearning.
Besides, to disentangle concept-specific influence from the global parameter
shifts induced by fine-tuning, DPM computes confidence scores over orthogonal
prompt distributions using statistical metrics. Moreover, to facilitate
standardized benchmarking, we also construct the Copyright Infringement
Detection Dataset (CIDD), a comprehensive resource for evaluating detection
across diverse categories. Our results demonstrate that DPM reliably detects
infringement content without requiring access to the original training dataset
or text prompts, offering an interpretable and practical solution for
safeguarding intellectual property in the era of generative AI.

</details>


### [39] [Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement](https://arxiv.org/abs/2509.23025)
*Gabriel A. Viana,Luis F. Alves Pereira,Tsang Ing Ren,George D. C. Cavalcanti,Jan Sijbers*

Main category: cs.CV

TL;DR: 本文研究了感知损失（perceptual loss）在低剂量CT（LDCT）图像增强中的应用，提出了感知损失设计的新评价指标，并给出了优化感知损失设计以提升去噪和结构保真度的具体方法。


<details>
  <summary>Details</summary>
Motivation: 传统像素级损失（如MSE）在训练网络增强低剂量CT时，常导致图像过度平滑和丢失临床重要细节。感知损失虽然缓解了这一问题，但其设计（如特征层次选择、预训练数据集、损失权重等）尚缺系统研究和优化指导。

Method: 作者提出了“感知影响力”（perceptual influence）这一新指标，系统评估感知损失在不同设计选择下对网络训练效果的影响，并进行了系统实验比较常见配置与更优设计方案。

Result: 实验显示，当前文献常用的感知损失配置效果不佳，相比更优设计方案，噪声去除和结构还原能力均落后。优化后的感知损失设计能显著提升重建CT图像的质量，无需更改网络结构。

Conclusion: 本文提出的感知损失评价和优化方法，为LDCT去噪中的感知损失应用提供了坚实理论和实践依据，并给出了可操作的配置建议。

Abstract: Perceptual losses have emerged as powerful tools for training networks to
enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to
traditional pixel-wise losses such as Mean Squared Error, which often lead to
over-smoothed reconstructions and loss of clinically relevant details in LDCT
images. The perceptual losses operate in a latent feature space defined by a
pretrained encoder and aim to preserve semantic content by comparing high-level
features rather than raw pixel values. However, the design of perceptual losses
involves critical yet underexplored decisions, including the feature
representation level, the dataset used to pretrain the encoder, and the
relative importance assigned to the perceptual component during optimization.
In this work, we introduce the concept of perceptual influence (a metric that
quantifies the relative contribution of the perceptual loss term to the total
loss) and propose a principled framework to assess the impact of the loss
design choices on the model training performance. Through systematic
experimentation, we show that the widely used configurations in the literature
to set up a perceptual loss underperform compared to better-designed
alternatives. Our findings show that better perceptual loss designs lead to
significant improvements in noise reduction and structural fidelity of
reconstructed CT images, without requiring any changes to the network
architecture. We also provide objective guidelines, supported by statistical
analysis, to inform the effective use of perceptual losses in LDCT denoising.
Our source code is available at
https://github.com/vngabriel/perceptual-influence.

</details>


### [40] [Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities](https://arxiv.org/abs/2509.23035)
*Tomohiro Tanaka,Narumasa Tsutsumida*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖且高效的洪水探测方法，通过微调轻量级多模态预训练变换器（Presto），能灵活处理SAR、MS或两者结合的数据，实现快速、准确的洪水淹没范围映射，并且性能优于现有体量庞大的主流方法。


<details>
  <summary>Details</summary>
Motivation: 洪水灾害频发，造成严重损失。现有的洪水遥感监测方法存在单一传感器受限、数据时效性差、以及多传感器方法对计算资源和标注数据需求高等问题，因此需要一种高效、灵活且对传感器数据依赖低的新方法。

Method: 作者通过微调名为Presto的轻量级（约0.4M参数）多模态预训练Transformer，能够在同一模型架构下处理SAR、MS或两者融合数据。该方法在Sen1Floods11数据集上进行评估，并与参数量大的Prithvi-100M模型进行了对比。

Result: 在最优的多传感器融合场景下，所提方法的F1分数达0.896、mIoU为0.886，超过了大型基线模型。同时，在MS-only（F1: 0.893）和SAR-only（F1: 0.718）的传感器不可用场景下也表现出色，显示出良好的鲁棒性。

Conclusion: 该方法以极少的参数量，实现了优于大模型的多场景泛用性和性能，为实际灾害应急中的洪水快速测绘提供了高效、灵活、实用的解决方案。

Abstract: Floods are increasingly frequent natural disasters causing extensive human
and economic damage, highlighting the critical need for rapid and accurate
flood inundation mapping. While remote sensing technologies have advanced flood
monitoring capabilities, operational challenges persist: single-sensor
approaches face weather-dependent data availability and limited revisit
periods, while multi-sensor fusion methods require substantial computational
resources and large-scale labeled datasets. To address these limitations, this
study introduces a novel sensor-flexible flood detection methodology by
fine-tuning Presto, a lightweight ($\sim$0.4M parameters) multi-modal
pre-trained transformer that processes both Synthetic Aperture Radar (SAR) and
multispectral (MS) data at the pixel level. Our approach uniquely enables flood
mapping using SAR-only, MS-only, or combined SAR+MS inputs through a single
model architecture, addressing the critical operational need for rapid response
with whatever sensor data becomes available first during disasters. We
evaluated our method on the Sen1Floods11 dataset against the large-scale
Prithvi-100M baseline ($\sim$100M parameters) across three realistic data
availability scenarios. The proposed model achieved superior performance with
an F1 score of 0.896 and mIoU of 0.886 in the optimal sensor-fusion scenario,
outperforming the established baseline. Crucially, the model demonstrated
robustness by maintaining effective performance in MS-only scenarios (F1:
0.893) and functional capabilities in challenging SAR-only conditions (F1:
0.718), confirming the advantage of multi-modal pre-training for operational
flood mapping. Our parameter-efficient, sensor-flexible approach offers an
accessible and robust solution for real-world disaster scenarios requiring
immediate flood extent assessment regardless of sensor availability
constraints.

</details>


### [41] [GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization](https://arxiv.org/abs/2509.23038)
*Jingxing Li,Yongjae Lee,Deliang Fan*

Main category: cs.CV

TL;DR: GeLoc3r 提出了一种新的相机姿态估计方法，在保持高速推理的同时，大幅提升了几何一致性和精度，优于现有的 ReLoc3R 和接近 MASt3R。


<details>
  <summary>Details</summary>
Motivation: 当前前沿的相机姿态回归方法（如 ReLoc3R）推理速度极快，但受限于几何表示不一致，精度难以达到基于特征对应点的方法（如 MASt3R）。本文旨在弥合高速回归与精确几何之间的性能差距。

Method: 提出 GeLoc3r，通过在训练阶段引入 Geometric Consistency Regularization（GCR），利用真实深度生成大量 3D-2D 对应关系，通过 FusionTransformer 学习对应点的重要性，并用加权 RANSAC 求得几何一致性姿态，形成监督损失传递几何知识至回归网络。推理时只需回归头，无需额外几何计算，兼顾速度和精度。

Result: GeLoc3r 在多个主流 benchmark 上超越 ReLoc3R：CO3Dv2 数据集 AUC@5° 为 40.45%（对比 34.85%），RealEstate10K 为 68.66%（对比 66.70%），MegaDepth1500 为 50.45%（对比 49.60%），均有显著提升。

Conclusion: GeLoc3r 通过训练期几何一致性正则化，将几何知识高效迁移至回归模型，实现了类似对应点方法的高精度与回归方法的高速度，代表了相机几何神经网络学习的新范式。

Abstract: Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and
state-of-the-art regression accuracy, yet our analysis reveals subtle geometric
inconsistencies in its internal representations that prevent reaching the
precision ceiling of correspondence-based methods like MASt3R (which require
300ms per pair). In this work, we present GeLoc3r, a novel approach to relative
camera pose estimation that enhances pose regression methods through Geometric
Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma
by training regression networks to produce geometrically consistent poses
without inference-time geometric computation. During training, GeLoc3r
leverages ground-truth depth to generate dense 3D-2D correspondences, weights
them using a FusionTransformer that learns correspondence importance, and
computes geometrically-consistent poses via weighted RANSAC. This creates a
consistency loss that transfers geometric knowledge into the regression
network. Unlike FAR method which requires both regression and geometric solving
at inference, GeLoc3r only uses the enhanced regression head at test time,
maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On
challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving
significant improvements including 40.45% vs. 34.85% AUC@5{\deg} on the CO3Dv2
dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\deg} on
RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric
consistency during training rather than enforcing it at inference, GeLoc3r
represents a paradigm shift in how neural networks learn camera geometry,
achieving both the speed of regression and the geometric understanding of
correspondence methods.

</details>


### [42] [MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition](https://arxiv.org/abs/2509.23044)
*Ye-eun Kim,Suhyeon Lim,Andrew J. Choi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模态传感器的远程家庭中风康复动作监测系统，并开发了适用于中风患者的动作识别深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前中风患者康复治疗需求上升，但医护资源短缺，需要远程监测系统减轻医护负担。现有的人体动作识别多针对健常人，缺乏为中风患者量身打造的方法。

Method: 系统结合IMU传感器与RGB-D相机采集家庭环境下的上肢日常活动数据，建立中风患者专属的数据集。通过合理的数据预处理，设计了一种适于多模态数据的深度学习动作识别模型，并对数据的聚类性质进行了分析。

Result: 分析发现中风患者的动作数据聚类性较差，但新模型能在这些特征中仍学得各标签间的相似性，取得了有效的动作识别效果。

Conclusion: 该系统和模型为中风患者家庭康复监测提供了可行方案，并为未来基于深度学习的自动化动作评估与反馈打下基础。代码已开源，可推动进一步研究。

Abstract: Rehabilitation therapy for stroke patients faces a supply shortage despite
the increasing demand. To address this issue, remote monitoring systems that
reduce the burden on medical staff are emerging as a viable alternative. A key
component of these remote monitoring systems is Human Action Recognition (HAR)
technology, which classifies actions. However, existing HAR studies have
primarily focused on non-disable individuals, making them unsuitable for
recognizing the actions of stroke patients. HAR research for stroke has largely
concentrated on classifying relatively simple actions using machine learning
rather than deep learning. In this study, we designed a system to monitor the
actions of stroke patients, focusing on domiciliary upper limb Activities of
Daily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors
and an RGB-D camera, which are the most common modalities in HAR. We directly
collected a dataset through this system, investigated an appropriate preprocess
and proposed a deep learning model suitable for processing multimodal data. We
analyzed the collected dataset and found that the action data of stroke
patients is less clustering than that of non-disabled individuals.
Simultaneously, we found that the proposed model learns similar tendencies for
each label in data with features that are difficult to clustering. This study
suggests the possibility of expanding the deep learning model, which has
learned the action features of stroke patients, to not only simple action
recognition but also feedback such as assessment contributing to domiciliary
rehabilitation in future research. The code presented in this study is
available at https://github.com/ye-Kim/MMeViT.

</details>


### [43] [Activation Matching for Explanation Generation](https://arxiv.org/abs/2509.23051)
*Pirzada Suhail,Aditya Anand,Amit Sethi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于激活匹配的最小化、忠实的模型决策可解释性方法，通过学习二值掩码，生成能保留分类器决策的小区域解释。


<details>
  <summary>Details</summary>
Motivation: 深度模型的可解释性日益重要，但通常很难生成既简洁又能忠实还原模型决策依据的解释。该工作旨在让用户理解模型决策的关键区域，同时保证解释的简洁性和对原始模型行为的忠实还原。

Method: 针对给定图片和预训练分类模型，训练一个轻量级自编码器输出二值掩码，使得经过掩码的输入能尽可能保留模型的决策及中间激活。优化目标包括多层激活匹配（采用KL散度）、交叉熵确保top-1预测不变、掩码稀疏性和二值性正则化、总变分正则化以及归纳约束，以实现最小且忠实的解释区域。

Result: 实验表明，该方法生成的掩码区域较小且易于人类理解，同时能够保留分类器的关键决策行为，成功去除无关输入区域。

Conclusion: 该方法为分类器决策提供了实用且忠实的极简解释，既能保证掩码的可解释性，又能忠实反映模型的真实决策机制，对模型的可解释性分析具有实用价值。

Abstract: In this paper we introduce an activation-matching--based approach to generate
minimal, faithful explanations for the decision-making of a pretrained
classifier on any given image. Given an input image \(x\) and a frozen model
\(f\), we train a lightweight autoencoder to output a binary mask \(m\) such
that the explanation \(e = m \odot x\) preserves both the model's prediction
and the intermediate activations of \(x\). Our objective combines: (i)
multi-layer activation matching with KL divergence to align distributions and
cross-entropy to retain the top-1 label for both the image and the explanation;
(ii) mask priors -- L1 area for minimality, a binarization penalty for crisp
0/1 masks, and total variation for compactness; and (iii) abductive constraints
for faithfulness and necessity. Together, these objectives yield small,
human-interpretable masks that retain classifier behavior while discarding
irrelevant input regions, providing practical and faithful minimalist
explanations for the decision making of the underlying model.

</details>


### [44] [Mask What Matters: Controllable Text-Guided Masking for Self-Supervised Medical Image Analysis](https://arxiv.org/abs/2509.23054)
*Ruilang Wang,Shuotong Xu,Bowen Liu,Runlin Huang,Donglong Chen,Weifeng Su*

Main category: cs.CV

TL;DR: 本论文提出了一种文本引导的可控掩码方法（Mask What Matters），用于提升医学影像自监督学习的效率和表现，对比现有方法在多个医学影像任务上实现了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 医学影像等专业领域标注数据稀缺，难以训练高性能视觉模型。现有自监督掩码建模方法通常使用随机高比例掩码，效率低且语义对齐差，区域感知变体又依赖重建启发式或监督信号，难以适应不同任务与模态。

Method: 提出Mask What Matters方法，利用视觉-语言模型通过文本提示定位诊断相关区域，对医学影像数据进行有区分度的掩码处理，与背景区域区分对待，提高语义对齐与自监督表示学习能力。该方法通过可控掩码方式实现，对多种影像模态均适用。

Result: 在脑MRI、胸部CT、肺部X光等多个医学影像任务的广泛评测中，Mask What Matters在分类准确率、BoxAP和MaskAP等指标上均优于主流MIM方法（如SparK），最高提升分别达3.1%、1.3和1.1个百分点。同时掩码比例显著降低（40%对比70%）。

Conclusion: 可控、文本驱动的掩码方法能实现更好的语义对齐与高效自监督学习，为医学影像分析领域稳健视觉模型的发展提供了新的方向。

Abstract: The scarcity of annotated data in specialized domains such as medical imaging
presents significant challenges to training robust vision models. While
self-supervised masked image modeling (MIM) offers a promising solution,
existing approaches largely rely on random high-ratio masking, leading to
inefficiency and poor semantic alignment. Moreover, region-aware variants
typically depend on reconstruction heuristics or supervised signals, limiting
their adaptability across tasks and modalities. We propose Mask What Matters, a
controllable text-guided masking framework for self-supervised medical image
analysis. By leveraging vision-language models for prompt-based region
localization, our method flexibly applies differentiated masking to emphasize
diagnostically relevant regions while reducing redundancy in background areas.
This controllable design enables better semantic alignment, improved
representation learning, and stronger cross-task generalizability.
Comprehensive evaluation across multiple medical imaging modalities, including
brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently
outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1
percentage points in classification accuracy, +1.3 in box average precision
(BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it
achieves these improvements with substantially lower overall masking ratios
(e.g., 40\% vs. 70\%). This work demonstrates that controllable, text-driven
masking can enable semantically aligned self-supervised learning, advancing the
development of robust vision models for medical image analysis.

</details>


### [45] [FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection](https://arxiv.org/abs/2509.23056)
*Ben Liang,Yuan Liu,Bingwen Qiu,Yihong Wang,Xiubao Sui,Qian Chen*

Main category: cs.CV

TL;DR: 提出了一种新的空中视角小目标检测框架FMC-DETR，通过频率解耦增强上下文感知，并有效提升了检测性能，尤其适用于高分辨率航拍图像中的微小目标检测。


<details>
  <summary>Details</summary>
Motivation: 空中图片的小目标检测因视觉线索有限和场景复杂，难以建模全局上下文，当前方法在全局信息利用和非线性建模上存在瓶颈，导致检测精度受限。

Method: 1) 提出WeKat主干网络，用小波变换增强浅层全局低频感知，并结合Kolmogorov-Arnold网络自适应建模多尺度依赖；2) 引入轻量级跨阶段部分融合（CPF）模块，减少特征冗余，提高多尺度信息交互；3) 设计多域特征协调（MDFC）模块，统一空间、频率和结构先验，平衡细节保持与全局增强。

Result: 在多个空中视角基准数据集上达到新SOTA表现，参数量更少；在VisDrone数据集上，AP提升6.5%，AP50提升8.2%。

Conclusion: FMC-DETR显著提升了小目标检测性能，高效、通用，极具实际应用价值。相关代码已开源。

Abstract: Aerial-view object detection is a critical technology for real-world
applications such as natural resource monitoring, traffic management, and
UAV-based search and rescue. Detecting tiny objects in high-resolution aerial
imagery presents a long-standing challenge due to their limited visual cues and
the difficulty of modeling global context in complex scenes. Existing methods
are often hampered by delayed contextual fusion and inadequate non-linear
modeling, failing to effectively use global information to refine shallow
features and thus encountering a performance bottleneck. To address these
challenges, we propose FMC-DETR, a novel framework with frequency-decoupled
fusion for aerial-view object detection. First, we introduce the Wavelet
Kolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet
transforms to enhance global low-frequency context perception in shallow
features while preserving fine-grained details, and employs Kolmogorov-Arnold
networks to achieve adaptive non-linear modeling of multi-scale dependencies.
Next, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy
and improves multi-scale feature interaction. Finally, we introduce the
Multi-Domain Feature Coordination (MDFC) module, which unifies spatial,
frequency, and structural priors to to balance detail preservation and global
enhancement. Extensive experiments on benchmark aerial-view datasets
demonstrate that FMC-DETR achieves state-of-the-art performance with fewer
parameters. On the challenging VisDrone dataset, our model achieves
improvements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its
effectiveness in tiny object detection. The code can be accessed at
https://github.com/bloomingvision/FMC-DETR.

</details>


### [46] [Follow-Your-Preference: Towards Preference-Aligned Image Inpainting](https://arxiv.org/abs/2509.23082)
*Yutao Shen,Junkun Yuan,Toru Aonishi,Hideki Nakayama,Yue Ma*

Main category: cs.CV

TL;DR: 本文系统性分析了图像修复任务中的偏好对齐问题，通过回归基础方法与对齐训练流程，总结现有奖励模型及数据的系统偏差，提出简单但有效的集成策略大幅提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 图像修复要求模型输出能满足人类偏好，但现有研究往往关注新方法，忽略了基础对齐流程中偏好数据和奖励模型的根本问题。作者想要系统性分析这些问题，促进该领域建立更可靠的基线。

Method: 采用直接偏好优化（Direct Preference Optimization, DPO）进行对齐训练，利用公开奖励模型构建偏好数据集，在九种奖励模型、两个数据基准与两种不同结构和生成机制的基线模型上开展实验。进一步分析不同奖励模型的有效性和系统偏差，并设计奖励模型集成用于缓解偏差。

Result: 1. 大多数奖励模型虽不完全可靠，但足以产生有效的偏好数据；2. 偏好数据在不同模型和数据集下表现出稳定趋势；3. 奖励模型在亮度、构图和色彩方面存在明显偏差，可能导致奖励作弊；4. 简单的奖励模型集成法可有效抑制偏差并提升结果鲁棒性。利用这些观察，作者训练的对齐模型在标准指标、GPT-4 评价和人工评价上均优于此前方法，无需更改模型结构和额外新数据。

Conclusion: 理清了偏好对齐中的基础流程和主要偏差来源，提出集成简化方法大幅提升性能，为图像修复偏好对齐问题设立了强健且易用的基线，推动该领域发展。代码已开源。

Abstract: This paper investigates image inpainting with preference alignment. Instead
of introducing a novel method, we go back to basics and revisit fundamental
problems in achieving such alignment. We leverage the prominent direct
preference optimization approach for alignment training and employ public
reward models to construct preference training datasets. Experiments are
conducted across nine reward models, two benchmarks, and two baseline models
with varying structures and generative algorithms. Our key findings are as
follows: (1) Most reward models deliver valid reward scores for constructing
preference data, even if some of them are not reliable evaluators. (2)
Preference data demonstrates robust trends in both candidate scaling and sample
scaling across models and benchmarks. (3) Observable biases in reward models,
particularly in brightness, composition, and color scheme, render them
susceptible to cause reward hacking. (4) A simple ensemble of these models
yields robust and generalizable results by mitigating such biases. Built upon
these observations, our alignment models significantly outperform prior models
across standard metrics, GPT-4 assessments, and human evaluations, without any
changes to model structures or the use of new datasets. We hope our work can
set a simple yet solid baseline, pushing this promising frontier. Our code is
open-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.

</details>


### [47] [Streamline pathology foundation model by cross-magnification distillation](https://arxiv.org/abs/2509.23097)
*Ziyu Su,Abdul Rehman Akbar,Usama Sajjad,Anil V. Parwani,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 该论文提出了一种名为XMAG的轻量级基础模型，通过跨倍镜知识蒸馏大幅提升计算效率，能够在临床病理领域实现更快、更低资源消耗的AI分析。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在计算病理学中表现优异，但其巨大的参数量和高倍镜需求使得在临床部署中成本高昂且不现实。因此，需要一种既高效又性能不损失的模型部署方法。

Method: 作者提出XMAG，通过跨倍镜知识蒸馏，将在20x倍镜下训练的大型模型的知识迁移到5x倍镜下的高效模型。同时，采用双层次知识迁移策略，对齐全局图像表示和局部空间token映射。全部在5x倍镜下运行，极大减少了图像切片数。

Result: XMAG在六项与癌症相关的临床任务中达到了与大型基础模型相差不足1%的诊断精度，同时处理速度提升30倍，达到了每分钟8.8个切片。跨机构验证显示其良好的泛化能力。进一步的端到端训练策略使其性能接近大型模型。

Conclusion: 跨倍镜蒸馏是一种在资源受限的临床环境中部署基础模型的有效方法，有望支持实时的病理AI集成。

Abstract: Foundation models (FM) have transformed computational pathology but remain
computationally prohibitive for clinical deployment due to their massive
parameter counts and high-magnification processing requirements. Here, we
introduce XMAG, a lightweight FM developed through corss-magnification
distillation that transfers knowledge from state-of-the-art 20x magnification
teacher to an efficient 5x magnification student architecture. XMAG employs a
compact backbone and operates entirely at 5x, requiring 11.3 times fewer
patches per whole slide image (WSI) compared to existing approaches. Our Novel
distillation framework incorporates dual-level knowledge transfer, aligning
both global image representations and local spatial token mapping. We trained
XMAG on 3.49 million images curated from publicly available datasets and
evaluated performance across six clinically relevant histopathology analysis
tasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within
1% of substantially larger foundation models while delivering 30-fold
processing acceleration, reaching 8.8 WSIs per minute processing speed. Our
cross-institutional validation confirmed robust generalization. Further, we
developed an end-to-end training strategy to further boost our model's
performance to approach the larger FMs' performance. These results establish
cross-magnification distillation as a viable approach for deploying FM
capabilities in resource-constrained clinical environments, potentially
enabling real-time pathology AI integration.

</details>


### [48] [CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP](https://arxiv.org/abs/2509.23098)
*Na Min An,Inha Kang,Minhyun Lee,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本论文提出了一个名为CoPatch的零样本指代图像分割（RIS）框架，通过提升视觉-语言模型（如CLIP）对空间关系的理解能力，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有主流视觉-语言模型虽然善于图文对齐，但对空间关系的感知较弱，尤其在RIS任务中对空间信息处理不足，限制了模型性能。主要包括：语言流主要关注主体名词短语，忽略空间线索；视觉流对不同空间布局的图片特征敏感性较低。

Method: 提出了CoPatch方法，在语言方面，通过融合包含空间提示的上下文词元，构建混合文本特征；在视觉方面，利用从模型中间层提取的patch级特征，更好保留图像的空间结构。将以上增强后的特征融合成聚类式图文相似度图（CoMap），实现更精确的分割掩膜选择。

Result: 在RefCOCO、RefCOCO+、RefCOCOg和PhraseCut等数据集上，CoPatch在零样本情况下实现mIoU提升2-7个百分点，无需额外训练。

Conclusion: CoPatch证明了充分利用视觉-语言模型中未被挖掘的空间信息能够大幅提升零样本RIS性能，为相关领域带来了新机遇。

Abstract: Spatial grounding is crucial for referring image segmentation (RIS), where
the goal of the task is to localize an object described by language. Current
foundational vision-language models (VLMs), such as CLIP, excel at aligning
images and text but struggle with understanding spatial relationships. Within
the language stream, most existing methods often focus on the primary noun
phrase when extracting local text features, undermining contextual tokens.
Within the vision stream, CLIP generates similar features for images with
different spatial layouts, resulting in limited sensitivity to spatial
structure. To address these limitations, we propose \textsc{CoPatch}, a
zero-shot RIS framework that leverages internal model components to enhance
spatial representations in both text and image modalities. For language,
\textsc{CoPatch} constructs hybrid text features by incorporating context
tokens carrying spatial cues. For vision, it extracts patch-level image
features using our novel path discovered from intermediate layers, where
spatial structure is better preserved. These enhanced features are fused into a
clustered image-text similarity map, \texttt{CoMap}, enabling precise mask
selection. As a result, \textsc{CoPatch} significantly improves spatial
grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+
2--7 mIoU) without requiring any additional training. Our findings underscore
the importance of recovering and leveraging the untapped spatial knowledge
inherently embedded in VLMs, thereby paving the way for opportunities in
zero-shot RIS.

</details>


### [49] [Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt, and Swin Transformer](https://arxiv.org/abs/2509.23100)
*Ajo Babu George,Sadhvik Bathini,Niranjana S R*

Main category: cs.CV

TL;DR: 本文系统比较了五种主流Transformer结构（ViT、DeiT、ConvNeXt、Swin Transformer、BEiT）在口腔疾病多分类中的表现，并特别关注现实中常见的数据不平衡问题。结果显示ConvNeXt表现最佳。


<details>
  <summary>Details</summary>
Motivation: 目前Transformer模型在医学影像诊断中的应用不断扩展，但针对多类别口腔疾病分类，尤其是如何应对数据不平衡问题的系统性比较研究尚缺失。本文旨在补全这一空白。

Method: 作者选用Oral Diseases数据集，对五个Transformer架构进行训练与验证，通过准确率、精确率、召回率和F1分数等指标全面评估其性能，并考查其在类别不平衡情况下的表现。

Result: ConvNeXt模型在验证集上取得了81.06%的最高准确率，BEiT和Swin Transformer也表现优异（分别为80.00%和79.73%），且三者F1分数也高。ViT和DeiT准确率略低，并且在牙齿龋齿相关类别上表现不足。

Conclusion: ConvNeXt、Swin Transformer和BEiT具备可靠的诊断性能，有望应用于实际口腔疾病影像筛查。研究结果为后续AI口腔诊断工具模型选择提供参考，也凸显了处理数据不平衡问题的重要性。

Abstract: Objective: The aim of this study was to systematically evaluate and compare
the performance of five state-of-the-art transformer-based architectures -
Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), ConvNeXt,
Swin Transformer, and Bidirectional Encoder Representation from Image
Transformers (BEiT) - for multi-class dental disease classification. The study
specifically focused on addressing real-world challenges such as data
imbalance, which is often overlooked in existing literature.
  Study Design: The Oral Diseases dataset was used to train and validate the
selected models. Performance metrics, including validation accuracy, precision,
recall, and F1-score, were measured, with special emphasis on how well each
architecture managed imbalanced classes.
  Results: ConvNeXt achieved the highest validation accuracy at 81.06, followed
by BEiT at 80.00 and Swin Transformer at 79.73, all demonstrating strong
F1-scores. ViT and DeiT achieved accuracies of 79.37 and 78.79, respectively,
but both struggled particularly with Caries-related classes.
  Conclusions: ConvNeXt, Swin Transformer, and BEiT showed reliable diagnostic
performance, making them promising candidates for clinical application in
dental imaging. These findings provide guidance for model selection in future
AI-driven oral disease diagnostic tools and highlight the importance of
addressing data imbalance in real-world scenarios

</details>


### [50] [HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing](https://arxiv.org/abs/2509.23103)
*Emadeldeen Hamdan,Ahmet Enis Cetin*

Main category: cs.CV

TL;DR: HTMA-Net是一种结合Hadamard变换和基于SRAM存内计算免乘操作的高效深度神经网络架构，可以大幅减少乘法运算，维持准确率，特别适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 在计算资源有限和能耗受限的设备（特别是边缘设备）上，深度神经网络部署面临乘法运算过多导致的能耗和速度问题。现有解决方案要么只优化卷积层的乘法，要么仅关注存内加速，难以高效兼顾复杂度和精度。

Method: 提出HTMA-Net框架，将Hadamard变换与乘法规避型存内计算相结合。它用混合Hadamard变换层代替部分卷积，卷积内部用SRAM存内计算避免乘法（MA操作）。该方法在多个数据集和ResNet结构上与传统和单独改良的变体进行对比。

Result: HTMA-Net相比标准ResNet在ResNet-18、20、50上减少了高达52%的乘法运算，模型参数量和计算复杂度显著降低，同时在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上准确率基本不下降。

Conclusion: 将结构化Hadamard变换层与SRAM存内乘法规避计算相结合，为高效深度学习模型设计提供了有效思路，尤其适用于能耗敏感的设备。

Abstract: Reducing the cost of multiplications is critical for efficient deep neural
network deployment, especially in energy-constrained edge devices. In this
work, we introduce HTMA-Net, a novel framework that integrates the Hadamard
Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing
to reduce arithmetic complexity while maintaining accuracy. Unlike prior
methods that only target multiplications in convolutional layers or focus
solely on in-memory acceleration, HTMA-Net selectively replaces intermediate
convolutions with Hybrid Hadamard-based transform layers whose internal
convolutions are implemented via multiplication-avoiding in-memory operations.
We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet,
and provide a detailed comparison against regular, MF-only, and HT-only
variants. Results show that HTMA-Net eliminates up to 52\% of multiplications
compared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while
achieving comparable accuracy in evaluation and significantly reducing
computational complexity and the number of parameters. Our results demonstrate
that combining structured Hadamard transform layers with SRAM-based in-memory
computing multiplication-avoiding operators is a promising path towards
efficient deep learning architectures.

</details>


### [51] [Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM](https://arxiv.org/abs/2509.23105)
*Junxiao Xue,Quan Deng,Xuecheng Wu,Kelu Yao,Xinyi Yin,Fei Yu,Wei Zhou,Yanfei Zhong,Yang Liu,Dingkang Yang*

Main category: cs.CV

TL;DR: 本文提出了一个新的交互式多任务遥感变化理解数据集ChangeIMTI，并基于该数据集设计了新型视觉引导的视觉-语言模型ChangeVG，实现了变化描述、二元变化分类、变化计数和定位等任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的遥感变化理解（RSCU）数据集无法覆盖更深层次的变化描述、多样任务（如描述、计数、定位）的交互性和理解需求，因此需要构建一个更全面的数据集和方法来提升多任务下的表现和理解能力。

Method: 1）构建ChangeIMTI多任务数据集，涵盖变化描述、分类、计数与定位；2）提出双分支视觉引导模块的ChangeVG模型，结合细粒度空间特征与高层语义摘要，通过辅助提示优化大规模视觉-语言模型（例如Qwen2.5-VL-7B）的指令微调，实现分层交叉模态学习；3）通过多项实验证明有效性。

Result: ChangeVG方法在四项任务上表现优越；在变化描述任务中，性能（S*m指标）比当前最优方法Semantic-CC高出1.39分。同时，消融实验验证了模型各关键模块的有效性。

Conclusion: 提出的ChangeIMTI数据集和ChangeVG方法均显著提升了遥感图像多任务变化理解的表现，展示了双粒度视觉引导和跨模态学习在遥感变化任务上的有效性和优越性。

Abstract: Remote sensing change understanding (RSCU) is essential for analyzing remote
sensing images and understanding how human activities affect the environment.
However, existing datasets lack deep understanding and interactions in the
diverse change captioning, counting, and localization tasks. To tackle these
gaps, we construct ChangeIMTI, a new large-scale interactive multi-task
instruction dataset that encompasses four complementary tasks including change
captioning, binary change classification, change counting, and change
localization. Building upon this new dataset, we further design a novel
vision-guided vision-language model (ChangeVG) with dual-granularity awareness
for bi-temporal remote sensing images (i.e., two remote sensing images of the
same area at different times). The introduced vision-guided module is a
dual-branch architecture that synergistically combines fine-grained spatial
feature extraction with high-level semantic summarization. These enriched
representations further serve as the auxiliary prompts to guide large
vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning,
thereby facilitating the hierarchical cross-modal learning. We extensively
conduct experiments across four tasks to demonstrate the superiority of our
approach. Remarkably, on the change captioning task, our method outperforms the
strongest method Semantic-CC by 1.39 points on the comprehensive S*m metric,
which integrates the semantic similarity and descriptive accuracy to provide an
overall evaluation of change caption. Moreover, we also perform a series of
ablation studies to examine the critical components of our method.

</details>


### [52] [Stochastic Interpolants via Conditional Dependent Coupling](https://arxiv.org/abs/2509.23122)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.CV

TL;DR: 该论文针对现有图像生成模型在计算量与生成质量之间的权衡问题，提出了一种统一的多阶段生成框架，并以条件相关耦合策略实现高保真与高效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE的图像生成方法存在信息损失和无法端到端训练的问题，而直接在像素空间操作则计算量巨大，层叠模型虽然能降低计算开销，但多阶段分离阻碍了有效的端到端优化和知识共享，且导致各阶段学习的分布不准确。作者希望提出一种既高效又高质量的新方法。

Method: 作者提出了一种统一的多阶段生成框架，并引入了'条件相关耦合'策略，将整个生成过程分解为多阶段的插值轨迹。整个模型使用一个统一的扩散Transformer，无需分离的子模块，从而实现知识共享和端到端优化。

Result: 大量实验表明，该方法在多个分辨率下都能兼顾高效率与高保真图像生成，效果优于现有方法。

Conclusion: 所提框架可有效解决现有模型在效率和质量上的权衡问题，并通过统一的端到端流程实现更好的分布学习和知识共享，在图像生成任务中表现突出。

Abstract: Existing image generation models face critical challenges regarding the
trade-off between computation and fidelity. Specifically, models relying on a
pretrained Variational Autoencoder (VAE) suffer from information loss, limited
detail, and the inability to support end-to-end training. In contrast, models
operating directly in the pixel space incur prohibitive computational cost.
Although cascade models can mitigate computational cost, stage-wise separation
prevents effective end-to-end optimization, hampers knowledge sharing, and
often results in inaccurate distribution learning within each stage. To address
these challenges, we introduce a unified multistage generative framework based
on our proposed Conditional Dependent Coupling strategy. It decomposes the
generative process into interpolant trajectories at multiple stages, ensuring
accurate distribution learning while enabling end-to-end optimization.
Importantly, the entire process is modeled as a single unified Diffusion
Transformer, eliminating the need for disjoint modules and also enabling
knowledge sharing. Extensive experiments demonstrate that our method achieves
both high fidelity and efficiency across multiple resolutions.

</details>


### [53] [Benchmarking DINOv3 for Multi-Task Stroke Analysis on Non-Contrast CT](https://arxiv.org/abs/2509.23132)
*Donghao Zhang,Yimin Chen,Kauê TN Duarte,Taha Aslan,Mohamed AlShamrani,Brij Karmur,Yan Wan,Shengcai Chen,Bo Hu,Bijoy K Menon,Wu Qiu*

Main category: cs.CV

TL;DR: 本研究利用先进的自监督视觉Transformer（DINOv3）提升非对比CT（NCCT）在卒中分析中的特征表征能力，实现了多项卒中相关任务的高效自动化分析。


<details>
  <summary>Details</summary>
Motivation: 非对比CT是卒中快速诊断的关键环节，但受限于图像对比度低和信噪比较差。论文旨在提升NCCT在卒中自动化诊断中的表现，解决传统方法效果有限的问题。

Method: 研究使用DINOv3自监督视觉Transformer，在多个公开和私有数据集上进行训练和评估。涉及的任务包括梗死和出血分割、异常分类、出血亚型分类以及ASPECTS二分类。

Result: 模型在卒中区域分割、各类异常/亚型分类等任务上建立了强基线，展示了DINOv3在NCCT分析中的优势。

Conclusion: 高级自监督模型有望显著提升NCCT卒中自动诊断性能，但仍存在一定局限。研究为相关算法的发展提供了清晰基准和分析。

Abstract: Non-contrast computed tomography (NCCT) is essential for rapid stroke
diagnosis but is limited by low image contrast and signal to noise ratio. We
address this challenge by leveraging DINOv3, a state-of-the-art self-supervised
vision transformer, to generate powerful feature representations for a
comprehensive set of stroke analysis tasks. Our evaluation encompasses infarct
and hemorrhage segmentation, anomaly classification (normal vs. stroke and
normal vs. infarct vs. hemorrhage), hemorrhage subtype classification (EDH,
SDH, SAH, IPH, IVH), and dichotomized ASPECTS classification (<=6 vs. >6) on
multiple public and private datasets. This study establishes strong benchmarks
for these tasks and demonstrates the potential of advanced self-supervised
models to improve automated stroke diagnosis from NCCT, providing a clear
analysis of both the advantages and current constraints of the approach. The
code is available at https://github.com/Zzz0251/DINOv3-stroke.

</details>


### [54] [Earth-Agent: Unlocking the Full Landscape of Earth Observation with Agents](https://arxiv.org/abs/2509.23141)
*Peilin Feng,Zhutao Lv,Junyan Ye,Xiaolei Wang,Xinjie Huo,Jinhua Yu,Wanghan Xu,Wenlong Zhang,Lei Bai,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出了Earth-Agent，这是首个在地球观测（EO）领域融合RGB与光谱数据、并基于多工具调用的智能代理框架，能跨模态执行多步定量推理，同时提供了大规模高质量的评测基准Earth-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）虽推动了EO领域进步，但对需要多步推理与领域专用工具操作的复杂任务力有不逮，现有的智能体方法也多局限于RGB浅层推理，缺乏系统性的评估。

Method: 提出Earth-Agent框架，将多模态数据（包括RGB与光谱）统一在一个基于多工具调用（MCP）的系统中，智能体可根据任务动态调用专家工具和模型，支持跨模态、多步、定量时空推理。同时提出Earth-Bench评测集，含248个专家任务、13729张图片，采用双层次评测方法。

Result: 通过对比不同大模型（LLM）后端、与通用代理框架和MLLMs在遥感基准上的表现，Earth-Agent在复杂科学任务上表现更优，验证了其实用性和领先性。

Conclusion: Earth-Agent为EO分析提供了新范式，推进大模型在地球观测的科学应用前沿。相关代码与数据将开源，为后续研究和应用提供基础。

Abstract: Earth observation (EO) is essential for understanding the evolving states of
the Earth system. Although recent MLLMs have advanced EO research, they still
lack the capability to tackle complex tasks that require multi-step reasoning
and the use of domain-specific tools. Agent-based methods offer a promising
direction, but current attempts remain in their infancy, confined to RGB
perception, shallow reasoning, and lacking systematic evaluation protocols. To
overcome these limitations, we introduce Earth-Agent, the first agentic
framework that unifies RGB and spectral EO data within an MCP-based tool
ecosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal
reasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific
tasks such as geophysical parameter retrieval and quantitative spatiotemporal
analysis by dynamically invoking expert tools and models across modalities. To
support comprehensive evaluation, we further propose Earth-Bench, a benchmark
of 248 expert-curated tasks with 13,729 images, spanning spectrum, products and
RGB modalities, and equipped with a dual-level evaluation protocol that
assesses both reasoning trajectories and final outcomes. We conduct
comprehensive experiments varying different LLM backbones, comparisons with
general agent frameworks, and comparisons with MLLMs on remote sensing
benchmarks, demonstrating both the effectiveness and potential of Earth-Agent.
Earth-Agent establishes a new paradigm for EO analysis, moving the field toward
scientifically grounded, next-generation applications of LLMs in Earth
observation. Our code and dataset will be publicly released.

</details>


### [55] [WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning](https://arxiv.org/abs/2509.23150)
*Wenxuan Fang,Jiangwei Weng,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为WeatherCycle的新颖无监督图像修复框架，针对多种天气条件下的图像退化问题实现了统一且强泛化能力的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 当前无监督多天气图像修复方法依赖于任务特定的物理先验，泛化性和可扩展性差，难以应对真实环境中多变复杂的天气影响。因此，迫切需要一种能够适用于多种天气条件的统一本领恢复方案。

Method: 提出WeatherCycle框架，将天气修复任务转化为双向的退化-内容翻译循环，并利用退化感知课程序列正则化引导训练。创新性地引入lumina-chroma分解方法，将退化与内容解耦，省去复杂天气建模；同时设计Lumina退化引导模块（LDGM），通过频域调制方式将亮度退化先验注入到清洁图像，控制和模拟真实多样的退化效应。此外，增加基于CLIP的Difficulty-Aware对比正则化（DACR），识别难样本并增强其语义一致性和鲁棒性。

Result: 在多个多天气数据集上进行实验，WeatherCycle在无监督方法中取得了最优的性能，并在复杂天气退化条件下展现了极强的泛化能力。

Conclusion: WeatherCycle作为一种统一、高效的无监督图像修复方法，为多变复杂的天气条件下图像恢复提供了有效、泛化性强的新范式，在无监督领域具有重要应用和推广价值。

Abstract: Unsupervised image restoration under multi-weather conditions remains a
fundamental yet underexplored challenge. While existing methods often rely on
task-specific physical priors, their narrow focus limits scalability and
generalization to diverse real-world weather scenarios. In this work, we
propose \textbf{WeatherCycle}, a unified unpaired framework that reformulates
weather restoration as a bidirectional degradation-content translation cycle,
guided by degradation-aware curriculum regularization. At its core,
WeatherCycle employs a \textit{lumina-chroma decomposition} strategy to
decouple degradation from content without modeling complex weather, enabling
domain conversion between degraded and clean images. To model diverse and
complex degradations, we propose a \textit{Lumina Degradation Guidance Module}
(LDGM), which learns luminance degradation priors from a degraded image pool
and injects them into clean images via frequency-domain amplitude modulation,
enabling controllable and realistic degradation modeling. Additionally, we
incorporate a \textit{Difficulty-Aware Contrastive Regularization (DACR)}
module that identifies hard samples via a CLIP-based classifier and enforces
contrastive alignment between hard samples and restored features to enhance
semantic consistency and robustness. Extensive experiments across serve
multi-weather datasets, demonstrate that our method achieves state-of-the-art
performance among unsupervised approaches, with strong generalization to
complex weather degradations.

</details>


### [56] [Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction](https://arxiv.org/abs/2509.23169)
*Bolin Chen,Ru-Ling Liao,Yan Ye,Jie Chen,Shanzhi Yin,Xinrui Ju,Shiqi Wang,Yibo Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Sparse2Dense的生成式视频压缩框架，仅依赖极少量的三维关键点，实现了极低码率的人体视频压缩及精确顶点预测，优于传统和生成式视频编解码方案。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的多媒体应用中，既要极低码率地压缩人体视频，又需保证几何顶点的精准预测，是一个难以统筹的挑战。现有技术难以兼顾动态动作、外观细节与几何一致性。

Method: 提出Sparse2Dense框架，通过多任务学习和关键点感知的深度生成模型，以极稀疏的三维关键点作为信息传递单元，联合训练视频生成和顶点预测任务，有效复原动态视频并保证几何结构与视觉内容一致。

Result: 实验表明，Sparse2Dense在人体视频压缩方面性能优于现有的传统及生成式视频编码方法，同时实现了高精度的人体顶点预测，适用于后续几何分析应用。

Conclusion: Sparse2Dense可极大地提升人体相关媒体传输的带宽效率，有望应用于实时动作分析、虚拟人动画和沉浸式娱乐等场景。

Abstract: For bandwidth-constrained multimedia applications, simultaneously achieving
ultra-low bitrate human video compression and accurate vertex prediction
remains a critical challenge, as it demands the harmonization of dynamic motion
modeling, detailed appearance synthesis, and geometric consistency. To address
this challenge, we propose Sparse2Dense, a keypoint-driven generative framework
that leverages extremely sparse 3D keypoints as compact transmitted symbols to
enable ultra-low bitrate human video compression and precise human vertex
prediction. The key innovation is the multi-task learning-based and
keypoint-aware deep generative model, which could encode complex human motion
via compact 3D keypoints and leverage these sparse keypoints to estimate dense
motion for video synthesis with temporal coherence and realistic textures.
Additionally, a vertex predictor is integrated to learn human vertex geometry
through joint optimization with video generation, ensuring alignment between
visual content and geometric structure. Extensive experiments demonstrate that
the proposed Sparse2Dense framework achieves competitive compression
performance for human video over traditional/generative video codecs, whilst
enabling precise human vertex prediction for downstream geometry applications.
As such, Sparse2Dense is expected to facilitate bandwidth-efficient
human-centric media transmission, such as real-time motion analysis, virtual
human animation, and immersive entertainment.

</details>


### [57] [TRAX: TRacking Axles for Accurate Axle Count Estimation](https://arxiv.org/abs/2509.23171)
*Avinash Rai,Sandeep Jana,Vishal Vijay*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的视频车辆轴数统计系统，结合YOLO-OBB和TRAX算法，在复杂交通环境下实现了高精度的车辆轴数统计。


<details>
  <summary>Details</summary>
Motivation: 车辆轴数的精确统计对交通管控、收费和基础设施建设至关重要，传统方法在密集交通环境下效果有限。

Method: 系统通过YOLO-OBB检测和分类车辆，YOLO检测轮胎，再智能地关联轮胎与车辆，实现车辆轴数预测。针对车辆被遮挡和超长车辆检测难题，提出了TRAX算法，在帧间追踪轴相关特征，提高检测鲁棒性。

Result: 方法在长车辆和实际交通视频中有效地减少了误报，提高了轴数统计的准确率，具有良好的鲁棒性。

Conclusion: 该方法为可扩展的AI驱动车辆轴数统计系统迈出了重要一步，为机器视觉替代传统路边设备提供了有力方案。

Abstract: Accurate counting of vehicle axles is essential for traffic control, toll
collection, and infrastructure development. We present an end-to-end,
video-based pipeline for axle counting that tackles limitations of previous
works in dense environments. Our system leverages a combination of YOLO-OBB to
detect and categorize vehicles, and YOLO to detect tires. Detected tires are
intelligently associated to their respective parent vehicles, enabling accurate
axle prediction even in complex scenarios. However, there are a few challenges
in detection when it comes to scenarios with longer and occluded vehicles. We
mitigate vehicular occlusions and partial detections for longer vehicles by
proposing a novel TRAX (Tire and Axle Tracking) Algorithm to successfully track
axle-related features between frames. Our method stands out by significantly
reducing false positives and improving the accuracy of axle-counting for long
vehicles, demonstrating strong robustness in real-world traffic videos. This
work represents a significant step toward scalable, AI-driven axle counting
systems, paving the way for machine vision to replace legacy roadside
infrastructure.

</details>


### [58] [Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift](https://arxiv.org/abs/2509.23176)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级适配框架CalSAM，用于提升SAM模型在医学影像分割中的准确性及不确定性校准，尤其应对域迁移带来的性能下降和过度自信问题。


<details>
  <summary>Details</summary>
Motivation: SAM在自然图像上有优秀的零样本分割能力，但在医学影像（特别是不同中心或不同扫描仪的数据）中，由于域迁移，会表现出性能下降和结果不可靠的问题。作者希望解决SAM在医学影像领域中的泛化性和结果不确定性不足的问题。

Method: CalSAM的核心是加入特征Fisher信息惩罚（FIP）降低编码器对于域迁移的敏感性，以及置信失配惩罚（CMP）惩罚过度自信的体素预测。二者组成联合损失，仅微调用于输出mask的解码器，主编码器部分保持冻结，实现高效适配。

Result: 在多中心、不同扫描仪等场景的MRI分割任务下，CalSAM显著提升了分割准确率与校准指标（如DSC提高7.4%，HD95降低26.9%，ECE降低39.5%），且各模块互补有效，训练时开销也较低。

Conclusion: CalSAM能有效提升SAM在医学MRI分割的泛化能力和不确定性校准，具备良好的应用前景，并通过只微调解码器保证计算效率，适合实际临床部署。

Abstract: The Segment Anything Model (SAM) exhibits strong zero-shot performance on
natural images but suffers from domain shift and overconfidence when applied to
medical volumes. We propose \textbf{CalSAM}, a lightweight adaptation framework
that (i) reduces encoder sensitivity to domain shift via a \emph{Feature Fisher
Information Penalty} (FIP) computed on 3D feature maps and (ii) penalizes
overconfident voxel-wise errors through a \emph{Confidence Misalignment
Penalty} (CMP). The combined loss, \(\mathcal{L}_{\mathrm{CalSAM}}\) fine-tunes
only the mask decoder while keeping SAM's encoders frozen. On cross-center and
scanner-shift evaluations, CalSAM substantially improves accuracy and
calibration: e.g., on the BraTS scanner split (Siemens$\to$GE) CalSAM shows a
$+7.4\%$ relative improvement in $\mathrm{DSC}$ (80.1\% vs.\ 74.6\%), a
$-26.9\%$ reduction in $\mathrm{HD95}$ (4.6 mm vs.\ 6.3 mm), and a $-39.5\%$
reduction in $\mathrm{ECE}$ (5.2\% vs.\ 8.6\%). On ATLAS-C (motion
corruptions), CalSAM achieves a $+5.3\%$ relative improvement in $\mathrm{DSC}$
(75.9\%) and a $-32.6\%$ reduction in $\mathrm{ECE}$ (5.8\%). Ablations show
FIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty
incurs a modest $\sim$15\% training-time overhead. CalSAM therefore delivers
improved domain generalization and better-calibrated uncertainty estimates for
brain MRI segmentation, while retaining the computational benefits of freezing
SAM's encoder.

</details>


### [59] [Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss](https://arxiv.org/abs/2509.23194)
*Yifan Zhang,Wei Zhang,Chuangxin He,Zhonghua Miao,Junhui Hou*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无监督在线3D实例分割方法，通过生成合成点云序列丰富训练数据、灵活采样时间帧并采用动态加权损失，极大提升了分割精度和时间上一致性。


<details>
  <summary>Details</summary>
Motivation: 无监督的3D实例分割难以获取连续帧中物体的稳定身份，同时现有无监督方法依赖有限的训练多样性、固定的时间采样和受噪声伪标签影响严重，导致性能受限。因此，提升在线3D点云序列的无监督分割效果意义重大。

Method: 作者提出通过合成点云序列丰富训练分布，无需人工标签或仿真引擎。同时引入灵活的时间采样策略结合相邻和非相邻帧，能学习长短时依赖关系。此外，动态加权损失聚焦置信度高且有信息的数据样本，引导模型学习更健壮的表示。

Result: 在SemanticKITTI、nuScenes和PandaSet三大公开数据集上的实验显示，该方法在分割精度和跨时刻关联性等指标上均显著优于UNIT和其他无监督基线方法。

Conclusion: 提出的方法在无监督3D实例分割任务中表现出更强的鲁棒性和精度，能够为后续无人驾驶和自动感知等应用提供更高质量的时空一致对象分割能力。

Abstract: Unsupervised online 3D instance segmentation is a fundamental yet challenging
task, as it requires maintaining consistent object identities across LiDAR
scans without relying on annotated training data. Existing methods, such as
UNIT, have made progress in this direction but remain constrained by limited
training diversity, rigid temporal sampling, and heavy dependence on noisy
pseudo-labels. We propose a new framework that enriches the training
distribution through synthetic point cloud sequence generation, enabling
greater diversity without relying on manual labels or simulation engines. To
better capture temporal dynamics, our method incorporates a flexible sampling
strategy that leverages both adjacent and non-adjacent frames, allowing the
model to learn from long-range dependencies as well as short-term variations.
In addition, a dynamic-weighting loss emphasizes confident and informative
samples, guiding the network toward more robust representations. Through
extensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method
consistently outperforms UNIT and other unsupervised baselines, achieving
higher segmentation accuracy and more robust temporal associations. The code
will be publicly available at github.com/Eaphan/SFT3D.

</details>


### [60] [Real-World Transferable Adversarial Attack on Face-Recognition Systems](https://arxiv.org/abs/2509.23198)
*Andrey Kaznacheev,Matvey Mikhalchuk,Andrey Kuznetsov,Aleksandr Petiushko,Anton Razzhigaev*

Main category: cs.CV

TL;DR: 本文提出了一种针对人脸识别系统的强大对抗攻击方法GaP（高斯贴片），能够在严格的黑盒条件下实现高效、可迁移的物理攻击。


<details>
  <summary>Details</summary>
Motivation: 当前人脸识别系统常遭受对抗攻击，但大多数方法仅限于数字领域或需白盒访问，缺乏在黑盒和物理环境下通用、可迁移攻击的有效方案。

Method: 提出GaP方法：使用高斯斑点依次叠加的对称灰度贴片，通过对替代人脸识别模型的余弦相似度分数进行优化，利用零阶贪心算法，以约10,000次查询逐步生成前额区域的物理贴片。

Result: GaP贴片能实现高攻击成功率，能够迷惑ArcFace黑盒模型，并在数字和真实物理实验中表现优秀。同时，对未见过的FaceNet模型同样有效，表现强大的迁移性。

Conclusion: 即便对目标系统知之甚少，也能构建出鲁棒、可迁移的物理对抗攻击，暴露了人脸识别系统的重大安全隐患。

Abstract: Adversarial attacks on face recognition (FR) systems pose a significant
security threat, yet most are confined to the digital domain or require
white-box access. We introduce GaP (Gaussian Patch), a novel method to generate
a universal, physically transferable adversarial patch under a strict black-box
setting. Our approach uses a query-efficient, zero-order greedy algorithm to
iteratively construct a symmetric, grayscale pattern for the forehead. The
patch is optimized by successively adding Gaussian blobs, guided only by the
cosine similarity scores from a surrogate FR model to maximally degrade
identity recognition. We demonstrate that with approximately 10,000 queries to
a black-box ArcFace model, the resulting GaP achieves a high attack success
rate in both digital and real-world physical tests. Critically, the attack
shows strong transferability, successfully deceiving an entirely unseen FaceNet
model. Our work highlights a practical and severe vulnerability, proving that
robust, transferable attacks can be crafted with limited knowledge of the
target system.

</details>


### [61] [UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions](https://arxiv.org/abs/2509.23225)
*Alisher Myrgyyassov,Zhen Song,Yu Sun,Bruce Xiao Wang,Min Ney Wong,Yongping Zheng*

Main category: cs.CV

TL;DR: 提出UltraUNet模型，有效提升超声舌像实时分割精度与速度，可广泛应用于言语研究与临床。


<details>
  <summary>Details</summary>
Motivation: 当前超声舌像分割因信噪比低、成像变异大和计算需求高，导致难以实现高效且实时的舌轮廓追踪，限制了其在言语研究和临床分析中的应用。

Method: 文中提出轻量级的UltraUNet编码-解码网络，特有包括Squeeze-and-Excitation模块、Group Normalization（提升小批量稳定性）、和求和型跳跃连接（减小内存与计算开销）；引入针对超声的增强手段（如降噪、模糊模拟）提升适应性。

Result: UltraUNet在8个公开数据集上表现出色，单数据集Dice系数0.855、MSD 0.993像素，跨数据集Dice平均0.734和0.761，且分割速度达250帧/秒。

Conclusion: UltraUNet可作为超声舌像实时分割的高效方案，满足科学研究和临床对快速、精准舌轮廓分析的需求。

Abstract: Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for
studying speech articulation, motor control, and related disorders. However,
real-time tongue contour segmentation remains challenging due to low
signal-to-noise ratios, imaging variability, and computational demands. We
propose UltraUNet, a lightweight encoder-decoder architecture optimized for
real-time segmentation of tongue contours in ultrasound images. UltraUNet
incorporates domain-specific innovations such as lightweight
Squeeze-and-Excitation blocks, Group Normalization for small-batch stability,
and summation-based skip connections to reduce memory and computational
overhead. It achieves 250 frames per second and integrates ultrasound-specific
augmentations like denoising and blur simulation. Evaluations on 8 datasets
demonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and
MSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet
provides a fast, accurate solution for speech research, clinical diagnostics,
and analysis of speech motor disorders.

</details>


### [62] [Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers](https://arxiv.org/abs/2509.23235)
*Seongsoo Heo,Dong-Wan Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新的模型反演方法Patch Rebirth Inversion（PRI），提升了数据无关学习中视觉Transformer模型反演的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的模型反演方法在面对ViTs时因自注意力机制计算成本高，效率低下。虽然有方法通过剪枝降低计算量，但过早丢弃补丁会影响知识迁移与提取。

Method: PRI方法在反演过程中逐步分离最重要的patch，形成稀疏合成图像，但保留其余patch继续演化，挖掘更多有用知识，平衡类别相关与非相关特征，称为'Re-Birth '效应。

Result: PRI实现了比标准Dense Model Inversion快10倍、比Sparse Model Inversion快2倍的反演速度，并且在准确率上显著超过SMI，与DMI匹配。

Conclusion: PRI不仅提高了ViT模型反演效率，还有效提升了合成数据的代表性和准确率，为数据无关知识迁移和模型压缩提供了更优方案。

Abstract: Model inversion is a widely adopted technique in data-free learning that
reconstructs synthetic inputs from a pretrained model through iterative
optimization, without access to original training data. Unfortunately, its
application to state-of-the-art Vision Transformers (ViTs) poses a major
computational challenge, due to their expensive self-attention mechanisms. To
address this, Sparse Model Inversion (SMI) was proposed to improve efficiency
by pruning and discarding seemingly unimportant patches, which were even
claimed to be obstacles to knowledge transfer. However, our empirical findings
suggest the opposite: even randomly selected patches can eventually acquire
transferable knowledge through continued inversion. This reveals that
discarding any prematurely inverted patches is inefficient, as it suppresses
the extraction of class-agnostic features essential for knowledge transfer,
along with class-specific features. In this paper, we propose Patch Rebirth
Inversion (PRI), a novel approach that incrementally detaches the most
important patches during the inversion process to construct sparse synthetic
images, while allowing the remaining patches to continue evolving for future
selection. This progressive strategy not only improves efficiency, but also
encourages initially less informative patches to gradually accumulate more
class-relevant knowledge, a phenomenon we refer to as the Re-Birth effect,
thereby effectively balancing class-agnostic and class-specific knowledge.
Experimental results show that PRI achieves up to 10x faster inversion than
standard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently
outperforming SMI in accuracy and matching the performance of DMI.

</details>


### [63] [Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection](https://arxiv.org/abs/2509.23236)
*Mingfei Han,Haihong Hao,Jinxing Zhou,Zhihui Li,Yuhui Zheng,Xueqing Deng,Linjie Yang,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出了一种基于自洽性的新方法，通过对模型长答案和短答案之间的一致性进行分析，生成偏好对用于训练，显著减少视觉-语言模型的幻觉现象，无需人工标注或外部监督。实验表明该方法在多个基准测试上提升了事实准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型常常生成虚假细节或错误属性（即“幻觉”），影响输出的真实性。大多数已有方法依赖人工标注或更强大模型的外部监督，难以规模化。作者希望找到无需人类介入、可扩展且高效的自动化方式来提升模型的可靠性。

Method: 设计了自反思管道，通过让模型先给出细致长回答，再对简明二元问题回答，是/否类短答案。将长答案与短答案做自洽性比对，若不一致则视为幻觉，利用该信号自动收集高质量训练数据，无需人工或外部模型参与。模型基于仅依赖自身一致性信号来训练，以降低幻觉。

Result: 在AMBER、MultiObject-Hal（ROPE）、Object HalBench和MMHal-Bench等基准测试上，模型的事实准确性和可靠性显著提升。同时，在LLaVA-Bench和MMBench的指令遵循能力测试上表现也有所提高，表明方法泛化性与鲁棒性良好。

Conclusion: 本文提出的自洽性驱动训练方法，无需人工标注或外部信息，通过利用模型自身的短、长答一致性，有效提升了视觉-语言模型事实性，降低了幻觉，并具备良好的扩展性和实际意义。

Abstract: Vision-language models often hallucinate details, generating non-existent
objects or inaccurate attributes that compromise output reliability. Existing
methods typically address these issues via extensive human annotations or
external supervision from more powerful models. In this work, we present a
novel framework that leverages the model's self-consistency between long
responses and short answers to generate preference pairs for training. We
observe that short binary questions tend to yield highly reliable responses,
which can be used to query the target model to evaluate and rank its generated
responses. Specifically, we design a self-reflection pipeline where detailed
model responses are compared against concise binary answers, and inconsistency
signals are utilized to automatically curate high-quality training data without
human annotations or external model-based supervision. By relying solely on
self-consistency rather than external supervision, our method offers a scalable
and efficient solution that effectively reduces hallucinations using unlabeled
data. Extensive experiments on multiple benchmarks, i.e., AMBER,
MultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate
significant improvements in factual grounding and reliability. Moreover, our
approach maintains robust instruction-following ability, as evidenced by
enhanced performance on LLaVA-Bench and MMBench.

</details>


### [64] [TATTOO: Training-free AesTheTic-aware Outfit recOmmendation](https://arxiv.org/abs/2509.23242)
*Yuntian Wu,Xiaonan Hu,Ziqi Zhou,Hao Lu*

Main category: cs.CV

TL;DR: 提出了一种无训练、具审美感知能力的服装搭配推荐方法TATTOO，在效果上超越传统有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前时尚电商依赖智能、能理解美学的搭配推荐系统促进销售。但现有方法普遍依赖大规模标注数据和专门的训练，且很少将人类审美显式纳入推荐流程。

Method: 利用多模态大语言模型（MLLMs），无须训练，首先自动生成目标物品描述，然后以链式思维总结图片审美特征（如颜色、风格、场合、季节、材质、平衡度），综合视觉摘要、文本描述以及美学向量，通过熵门控机制将候选物品向量结合并排序，实现搭配推荐。

Result: 在真实世界的Aesthetic-100评测集上，TATTOO表现优于现有基于训练的方法；在Polyvore数据集上展现出出色的零样本检索能力。

Conclusion: TATTOO不依赖专门训练即可实现高质量、具审美感知的服装搭配推荐，推动了时尚推荐向训练无关与审美融合发展。

Abstract: The global fashion e-commerce market relies significantly on intelligent and
aesthetic-aware outfit-completion tools to promote sales. While previous
studies have approached the problem of fashion outfit-completion and
compatible-item retrieval, most of them require expensive, task-specific
training on large-scale labeled data, and no effort is made to guide outfit
recommendation with explicit human aesthetics. In the era of Multimodal Large
Language Models (MLLMs), we show that the conventional training-based pipeline
could be streamlined to a training-free paradigm, with better recommendation
scores and enhanced aesthetic awareness. We achieve this with TATTOO, a
Training-free AesTheTic-aware Outfit recommendation approach. It first
generates a target-item description using MLLMs, followed by an aesthetic
chain-of-thought used to distill the images into a structured aesthetic profile
including color, style, occasion, season, material, and balance. By fusing the
visual summary of the outfit with the textual description and aesthetics
vectors using a dynamic entropy-gated mechanism, candidate items can be
represented in a shared embedding space and be ranked accordingly. Experiments
on a real-world evaluation set Aesthetic-100 show that TATTOO achieves
state-of-the-art performance compared with existing training-based methods.
Another standard Polyvore dataset is also used to measure the advanced
zero-shot retrieval capability of our training-free method.

</details>


### [65] [Increasing the Diversity in RGB-to-Thermal Image Translation for Automotive Applications](https://arxiv.org/abs/2509.23243)
*Kaili Wang,Leonardo Ravaglia,Roberto Longo,Lore Goetschalckx,David Van Hamme,Julie Moeyersoms,Ben Stoffelen,Tom De Schepper*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的RGB到热成像转换方法，可以更加真实、多样地生成热成像图像。这将提升ADAS夜间或恶劣天气下的感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统ADAS系统在弱光和恶劣天气下性能有限，热成像技术虽优越，但受限于数据集稀缺和驾驶模拟器支持不足。现有的RGB到热成像方法只支持一对一映射，难以生成多样图像，限制了研究发展。

Method: 作者提出了一种一对多的多模态图像转换框架，并提出了Component-aware Adaptive Instance Normalization（CoAdaIN）。与传统AdaIN的全局风格应用不同，CoAdaIN可对不同图像部分分别适配风格，从而实现更高质量和多样性的图像生成。

Result: 该方法相比于现有的一对一映射方法，能够生成更加真实且多样性的热成像图像。实验结果验证了所提方法的有效性，有望提升热成像合成质量。

Conclusion: 通过引入CoAdaIN并实现一对多RGB到热成像转化，本文显著提升了合成热成像图像的真实性和多样性。这为ADAS等系统在缺乏真实热像数据时提供了有力的数据增强工具，对提升自动驾驶感知系统的安全性具有实际应用价值。

Abstract: Thermal imaging in Advanced Driver Assistance Systems (ADAS) improves road
safety with superior perception in low-light and harsh weather conditions
compared to traditional RGB cameras. However, research in this area faces
challenges due to limited dataset availability and poor representation in
driving simulators. RGB-to-thermal image translation offers a potential
solution, but existing methods focus on one-to-one mappings. We propose a
one-to-many mapping using a multi-modal translation framework enhanced with our
Component-aware Adaptive Instance Normalization (CoAdaIN). Unlike the original
AdaIN, which applies styles globally, CoAdaIN adapts styles to different image
components individually. The result, as we show, is more realistic and diverse
thermal image translations. This is the accepted author manuscript of the paper
published in IEEE Sensors Conference 2024. The final published version is
available at 10.1109/SENSORS60989.2024.10785056.

</details>


### [66] [LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis](https://arxiv.org/abs/2509.23255)
*Sasan Sharifipour,Constantino Álvarez Casado,Le Nguyen,Tharindu Ekanayake,Manuel Lage Cañellas,Nhi Nguyen,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出一种基于图谱分析的LiDAR点云人体活动识别方法，在隐私保护和照明鲁棒性方面优于摄像头，且在大规模多类数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统摄像头用于人体活动识别存在隐私泄露风险且受光照影响，LiDAR点云因其隐私性和环境适应性成为更优选择，但如何高效从点云中提取鲁棒特征仍是挑战。

Method: 每帧LiDAR点云映射为邻近图，通过计算拉普拉斯谱得到特征——包括特征值及特征向量统计；利用滑动窗口获取时序统计特征，最终用支持向量机和随机森林进行分类。

Result: 在MM-Fi数据集（40个人、27类活动）上，主方法在严格的被试独立协议下，对13类康复活动识别准确率达94.4%，对全部27类活动达90.3%，优于点云骨架方法。

Conclusion: 提出的点云图谱谱特征集判别性强、易解释，无需端到端深度学习即可高效识别人体活动，具有实际应用价值。

Abstract: Human Activity Recognition supports applications in healthcare,
manufacturing, and human-machine interaction. LiDAR point clouds offer a
privacy-preserving alternative to cameras and are robust to illumination. We
propose a HAR method based on graph spectral analysis. Each LiDAR frame is
mapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is
computed. Eigenvalues and statistics of eigenvectors form pose descriptors, and
temporal statistics over sliding windows yield fixed vectors for classification
with support vector machines and random forests. On the MM-Fi dataset with 40
subjects and 27 activities, under a strict subject-independent protocol, the
method reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all
27 activities. It also surpasses the skeleton-based baselines reported for
MM-Fi. The contribution is a compact and interpretable feature set derived
directly from point cloud geometry that provides an accurate and efficient
alternative to end-to-end deep learning.

</details>


### [67] [OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2509.23258)
*Atakan Topaloglu,Kunyi Li,Michael Niemeyer,Nassir Navab,A. Murat Tekalp,Federico Tombari*

Main category: cs.CV

TL;DR: OracleGS是一种针对稀疏视角新视图合成的新框架，通过结合生成模型和回归模型的优点，能够实现视角生成的完整性与几何一致性，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角的新视图合成对于重建真实场景存在严重几何不确定性。传统回归模型虽几何精度高但细节缺失，生成模型可补全信息但常引入结构错误，需要权衡两者优缺点。

Method: 提出OracleGS，先用预训练的3D扩散生成模型合成完整场景，再用多视角立体(MVS)网络判别生成视角的不确定性，最后以不确定性加权损失优化3D Gaussian Splatting模型，将生成先验与多视角几何证据耦合。

Result: 在Mip-NeRF 360和NeRF Synthetic等主流数据集上，OracleGS相比最新方法取得了更优表现，实现了完整且一致的新视图合成。

Conclusion: OracleGS方法能有效结合生成和回归模型优点，过滤不可信的虚假细节，保留合理补全，推动稀疏视角三维重建和新视图合成性能提升。

Abstract: Sparse-view novel view synthesis is fundamentally ill-posed due to severe
geometric ambiguity. Current methods are caught in a trade-off: regressive
models are geometrically faithful but incomplete, whereas generative models can
complete scenes but often introduce structural inconsistencies. We propose
OracleGS, a novel framework that reconciles generative completeness with
regressive fidelity for sparse view Gaussian Splatting. Instead of using
generative models to patch incomplete reconstructions, our
"propose-and-validate" framework first leverages a pre-trained 3D-aware
diffusion model to synthesize novel views to propose a complete scene. We then
repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the
3D uncertainties of generated views, using its attention maps to reveal regions
where the generated views are well-supported by multi-view evidence versus
where they fall into regions of high uncertainty due to occlusion, lack of
texture, or direct inconsistency. This uncertainty signal directly guides the
optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss.
Our approach conditions the powerful generative prior on multi-view geometric
evidence, filtering hallucinatory artifacts while preserving plausible
completions in under-constrained regions, outperforming state-of-the-art
methods on datasets including Mip-NeRF 360 and NeRF Synthetic.

</details>


### [68] [Learning Regional Monsoon Patterns with a Multimodal Attention U-Net](https://arxiv.org/abs/2509.23267)
*Swaib Ilias Mazumder,Manish Kumar,Aparajita Khan*

Main category: cs.CV

TL;DR: 作者提出了一种利用多模态深度学习进行高分辨率季风降雨分类的方法，结合了卫星与地球观测数据，并构建了1公里分辨率的新数据集，实现了对印度重要区域的降雨精确预测。实验显示，该方法在极端降雨类别下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 印度农业和水资源强烈依赖季风降雨。传统降雨预测因观测稀疏和区域复杂性难以获得高精度，尤其在极端天气下更具挑战性。因此，亟需能够融合多源信息、提升分辨率和性能的降雨预测方法。

Method: 作者自建了1公里分辨率、包括七种地理空间模态（地表温度、NDVI、土壤湿度、相对湿度、风速、海拔、土地利用）的大型数据集，覆盖印度五个邦的2024年季风季，采用融合注意力机制的U-Net神经网络架构，结合focal和dice损失函数以应对降雨样本不平衡问题。

Result: 该多模态神经网络框架在分类准确率上显著优于单模态和现有深度学习基线方法，尤其在极端降雨类下提升明显。

Conclusion: 工作为区域季风预测、气候韧性和地理空间AI在印度的应用，贡献了可扩展的框架、基准数据集和最先进的预测结果。

Abstract: Accurate monsoon rainfall prediction is vital for India's agriculture, water
management, and climate risk planning, yet remains challenging due to sparse
ground observations and complex regional variability. We present a multimodal
deep learning framework for high-resolution precipitation classification that
leverages satellite and Earth observation data. Unlike previous rainfall
prediction models based on coarse 5-50 km grids, we curate a new 1 km
resolution dataset for five Indian states, integrating seven key geospatial
modalities: land surface temperature, vegetation (NDVI), soil moisture,
relative humidity, wind speed, elevation, and land use, covering the
June-September 2024 monsoon season. Our approach uses an attention-guided U-Net
architecture to capture spatial patterns and temporal dependencies across
modalities, combined with focal and dice loss functions to handle rainfall
class imbalance defined by the India Meteorological Department (IMD).
Experiments demonstrate that our multimodal framework consistently outperforms
unimodal baselines and existing deep learning methods, especially in extreme
rainfall categories. This work contributes a scalable framework, benchmark
dataset, and state-of-the-art results for regional monsoon forecasting, climate
resilience, and geospatial AI applications in India.

</details>


### [69] [SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction](https://arxiv.org/abs/2509.23273)
*Yihao Ding,Soyeon Caren Han,Yanbei Jiang,Yan Li,Zechuan Li,Yifan Peng*

Main category: cs.CV

TL;DR: 本文提出了SynDoc框架，结合生成模型和判别模型，通过合成数据增强和递归推理机制，提升专业领域文档的理解能力。


<details>
  <summary>Details</summary>
Motivation: 面对医学、金融等领域文档的专业性和复杂性，现有多模态大模型存在幻觉、领域适应不足和对大规模标注数据的依赖等问题，迫切需要新的方法改进关键信息抽取的准确性和泛化能力。

Method: 提出SynDoc框架：首先利用结构化信息和领域自适应查询生成高质量合成注释数据，通过adaptive instruction tuning增强判别模型对领域知识的抽取能力。同时采用递归推理机制，反复优化生成模型和判别模型的输出，实现输出结果的稳定精确。

Result: 该方法实现了可扩展、高效且精确的文档理解，提升了领域适应能力，在关键性信息抽取任务中效果更优。

Conclusion: SynDoc有效缩小了领域适应和常识之间的差距，有助于专业领域文档的高质量理解与信息抽取。

Abstract: Domain-specific Visually Rich Document Understanding (VRDU) presents
significant challenges due to the complexity and sensitivity of documents in
fields such as medicine, finance, and material science. Existing Large
(Multimodal) Language Models (LLMs/MLLMs) achieve promising results but face
limitations such as hallucinations, inadequate domain adaptation, and reliance
on extensive fine-tuning datasets. This paper introduces SynDoc, a novel
framework that combines discriminative and generative models to address these
challenges. SynDoc employs a robust synthetic data generation workflow, using
structural information extraction and domain-specific query generation to
produce high-quality annotations. Through adaptive instruction tuning, SynDoc
improves the discriminative model's ability to extract domain-specific
knowledge. At the same time, a recursive inferencing mechanism iteratively
refines the output of both models for stable and accurate predictions. This
framework demonstrates scalable, efficient, and precise document understanding
and bridges the gap between domain-specific adaptation and general world
knowledge for document key information extraction tasks.

</details>


### [70] [Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing](https://arxiv.org/abs/2509.23279)
*Rohit Chowdhury,Aniruddha Bala,Rohan Jaiswal,Siddharth Roheda*

Main category: cs.CV

TL;DR: 本文提出了一种针对图像到视频生成模型的防护方法，通过对输入图片加入对抗性扰动，有效抑制生成视频中的运动，使得合成的视频类似静止画面。


<details>
  <summary>Details</summary>
Motivation: 随着图像到视频生成(I2V)模型的发展，静态图片自动生成动态视频的能力带来了内容伪造和滥用的风险。现有的方法难以系统性阻止合成的视频出现运动，对如何从根源上防护I2V模型产生恶意内容仍存在不足。

Method: 提出Vid-Freeze方法，对输入图片添加专门设计的对抗性扰动。这些扰动针对I2V模型内部的注意力机制，显著干扰模型的运动合成能力，同时最大限度保留原图像的语义信息。

Result: 实验结果表明，经过Vid-Freeze处理的图片，生成的视频基本为静止或几乎不动，极大抑制了恶意合成内容的可能性，防护效果优异。

Conclusion: Vid-Freeze展示了针对I2V模型注意力机制的对抗性攻击在防止模型被滥用方面的潜力，为未来主动防护生成式AI模型提供了新方向。

Abstract: The rapid progress of image-to-video (I2V) generation models has introduced
significant risks, enabling video synthesis from static images and facilitating
deceptive or malicious content creation. While prior defenses such as I2VGuard
attempt to immunize images, effective and principled protection to block motion
remains underexplored. In this work, we introduce Vid-Freeze - a novel
attention-suppressing adversarial attack that adds carefully crafted
adversarial perturbations to images. Our method explicitly targets the
attention mechanism of I2V models, completely disrupting motion synthesis while
preserving semantic fidelity of the input image. The resulting immunized images
generate stand-still or near-static videos, effectively blocking malicious
content creation. Our experiments demonstrate the impressive protection
provided by the proposed approach, highlighting the importance of attention
attacks as a promising direction for robust and proactive defenses against
misuse of I2V generation models.

</details>


### [71] [Seeing Through the Blur: Unlocking Defocus Maps for Deepfake Detection](https://arxiv.org/abs/2509.23289)
*Minsun Jeon,Simon S. Woo*

Main category: cs.CV

TL;DR: 本论文提出利用物理可解释的模糊信号（Defocus Blur）作为检测深度伪造（Deepfake）和AI生成内容（AIGC）的有效方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，真实和伪造视觉内容的边界逐渐模糊，尤其是在深度伪造（如人脸操控）及全场景合成图像中，这对视觉媒体的可信度构成威胁。亟需鲁棒、可解释的新型取证手段以应对AI合成内容的泛滥。

Method: 作者提出了一种基于物理成像原理的检测框架，利用焦外模糊（Defocus Blur）现象作为主要鉴别特征。通过构建模糊度图（defocus blur map），提取景深与物体结构间的不自然性，用于区分真实相机拍摄与AI生成图像。该方法既不同于传统的RGB纹理、也不同于频域特征分析。

Result: 通过三组深入的特征分析与实验，验证了基于模糊度特征的深度伪造检测方法在可靠性和泛化性方面的优越性，能够有效区分真实照片与合成图片。

Conclusion: 本文证明了基于物理原理的模糊信号是一种稳健且具有可解释性的深度伪造检测线索，并将实现与解释工具向媒体取证相关领域公开，有望推动该方向的进一步研究。

Abstract: The rapid advancement of generative AI has enabled the mass production of
photorealistic synthetic images, blurring the boundary between authentic and
fabricated visual content. This challenge is particularly evident in deepfake
scenarios involving facial manipulation, but also extends to broader
AI-generated content (AIGC) cases involving fully synthesized scenes. As such
content becomes increasingly difficult to distinguish from reality, the
integrity of visual media is under threat. To address this issue, we propose a
physically interpretable deepfake detection framework and demonstrate that
defocus blur can serve as an effective forensic signal. Defocus blur is a
depth-dependent optical phenomenon that naturally occurs in camera-captured
images due to lens focus and scene geometry. In contrast, synthetic images
often lack realistic depth-of-field (DoF) characteristics. To capture these
discrepancies, we construct a defocus blur map and use it as a discriminative
feature for detecting manipulated content. Unlike RGB textures or
frequency-domain signals, defocus blur arises universally from optical imaging
principles and encodes physical scene structure. This makes it a robust and
generalizable forensic cue. Our approach is supported by three in-depth feature
analyses, and experimental results confirm that defocus blur provides a
reliable and interpretable cue for identifying synthetic images. We aim for our
defocus-based detection pipeline and interpretability tools to contribute
meaningfully to ongoing research in media forensics. The implementation is
publicly available at:
https://github.com/irissun9602/Defocus-Deepfake-Detection

</details>


### [72] [Seeing the Unseen in Low-light Spike Streams](https://arxiv.org/abs/2509.23304)
*Liwen Hu,Yang Li,Mianzhi Liu,Yijia Guo,Shenghao Xie,Ziluo Ding,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型（Diff-SPK）的尖峰相机重建方法，有效提升了低光高速场景下的重建质量，并建立了首个低光尖峰流重建基准。


<details>
  <summary>Details</summary>
Motivation: 现有尖峰相机（Spike Camera）重建方法在低光高速场景中面临噪声大、信息稀疏的难题，传统方法难以获得高质量图像。

Method: 1. 提出Diff-SPK：首个用于尖峰流重建的扩散模型方法。
2. 首先通过增强的尖峰间隔纹理（ETFI）模块聚合低光环境下的稀疏信息。
3. 将ETFI输出作为ControlNet的条件输入，进一步生成高质量高速场景内容。
4. 在生成过程中引入基于ETFI的特征融合模块提升重建效果。

Result: 建立了首个大规模、包含定量光照信息的低光场景重建基准。实验证明Diff-SPK在真实低光尖峰流数据上显著优于现有方法。

Conclusion: Diff-SPK为低光高速尖峰流的高质量重建提供了新思路，在理论和实际应用中均展现出优越表现，并推动了相关领域基准数据集建设。

Abstract: Spike camera, a type of neuromorphic sensor with high-temporal resolution,
shows great promise for high-speed visual tasks. Unlike traditional cameras,
spike camera continuously accumulates photons and fires asynchronous spike
streams. Due to unique data modality, spike streams require reconstruction
methods to become perceptible to the human eye.
  However, lots of methods struggle to handle spike streams in low-light
high-speed scenarios due to severe noise and sparse information. In this work,
we propose Diff-SPK, the first diffusion-based reconstruction method for spike
camera. Diff-SPK effectively leverages generative priors to supplement texture
information in low-light conditions. Specifically, it first employs an
\textbf{E}nhanced \textbf{T}exture \textbf{f}rom Inter-spike \textbf{I}nterval
(ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI
serves as a conditioning input for ControlNet to generate the high-speed
scenes. To improve the quality of results, we introduce an ETFI-based feature
fusion module during the generation process.
  Moreover, we establish the first bona fide benchmark for the low-light spike
stream reconstruction task. It significantly surpasses existing reconstruction
datasets in scale and provides quantitative illumination information. The
performance on real low-light spike streams demonstrates the superiority of
Diff-SPK.

</details>


### [73] [Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2509.23310)
*Hao Liu,Yongjie Zheng,Yuhan Kang,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种基于平衡扩散引导融合（BDGF）的多模态深度学习方法，用于高效集成遥感数据以提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感数据分析可集成来自不同传感器的互补信息，但现有扩散模型在多模态预训练时易出现模态失衡，难以有效指导特征提取，限制了方法的性能。

Method: 提出了一种BDGF框架，利用自适应模态遮蔽策略，使DDPM模型获得模态均衡的数据分布；通过融合扩散特征，设计多分支神经网络（结合CNN、Mamba和Transformer），集成多种注意力机制进行层次引导；最后，通过互学习策略提升分支协同。

Result: 在四个多模态遥感数据集上的大量实验表明，该方法分类精度优于当前主流方法。

Conclusion: BDGF方法能有效克服多模态扩散模型的模态失衡问题，并提升了遥感地物分类的整体性能，对多模态场景具有良好的适应性和推广价值。

Abstract: Deep learning-based techniques for the analysis of multimodal remote sensing
data have become popular due to their ability to effectively integrate
complementary spatial, spectral, and structural information from different
sensors. Recently, denoising diffusion probabilistic models (DDPMs) have
attracted attention in the remote sensing community due to their powerful
ability to capture robust and complex spatial-spectral distributions. However,
pre-training multimodal DDPMs may result in modality imbalance, and effectively
leveraging diffusion features to guide complementary diversity feature
extraction remains an open question. To address these issues, this paper
proposes a balanced diffusion-guided fusion (BDGF) framework that leverages
multimodal diffusion features to guide a multi-branch network for land-cover
classification. Specifically, we propose an adaptive modality masking strategy
to encourage the DDPMs to obtain a modality-balanced rather than spectral
image-dominated data distribution. Subsequently, these diffusion features
hierarchically guide feature extraction among CNN, Mamba, and transformer
networks by integrating feature fusion, group channel attention, and
cross-attention mechanisms. Finally, a mutual learning strategy is developed to
enhance inter-branch collaboration by aligning the probability entropy and
feature similarity of individual subnetworks. Extensive experiments on four
multimodal remote sensing datasets demonstrate that the proposed method
achieves superior classification performance. The code is available at
https://github.com/HaoLiu-XDU/BDGF.

</details>


### [74] [Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning](https://arxiv.org/abs/2509.23311)
*Haorui Yu,Qiufeng Yi,Yijia Chu,Yang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个诊断框架，用于检测视觉-语言模型（VLM）对带有“火”元素的文化图像理解能力，发现这些模型对西方文化事件识别较好，但对非西方文化及紧急场景存在严重误判和偏见。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在文化语境下虽然表现出色，但主要依赖表面模式识别，缺乏真正的文化理解。这可能导致模型在解释和分类涉及不同文化背景的图像时产生偏见和危险误判。

Method: 作者设计了一个诊断性框架，通过分类和解释分析的方法，测试多种视觉-语言模型对于多样文化中包含“火”元素的图像（如西方节日、非西方传统及紧急事件）的识别和解释能力。

Result: 研究结果显示，模型对西方盛大节日的图像能较准确地识别，但对非西方文化和紧急事件的图像容易给出模糊标签，甚至把危险场面误判为庆典，暴露出模型的系统性偏见与风险。

Conclusion: 该研究表明，仅靠准确率指标无法全面评估模型的文化理解力，需要从解释性和公平性层面制定新的多模态系统评估和优化方案，防止模型因符号捷径带来的安全和公平问题。

Abstract: Vision-Language Models (VLMs) often appear culturally competent but rely on
superficial pattern matching rather than genuine cultural understanding. We
introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural
imagery through both classification and explanation analysis. Testing multiple
models on Western festivals, non-Western traditions, and emergency scenes
reveals systematic biases: models correctly identify prominent Western
festivals but struggle with underrepresented cultural events, frequently
offering vague labels or dangerously misclassifying emergencies as
celebrations. These failures expose the risks of symbolic shortcuts and
highlight the need for cultural evaluation beyond accuracy metrics to ensure
interpretable and fair multimodal systems.

</details>


### [75] [C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection](https://arxiv.org/abs/2509.23316)
*Siheng Wang,Zhengdao Li,Yanshu Li,Canran Xiao,Haibo Zhan,Zhengtao Yao,Xuzhi Zhang,Jiale Kang,Linshan Li,Weiming Liu,Zhikang Dong,Jifeng Shen,Junhao Dong,Qiang Sun,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出了C3-OWD框架，结合了可见-红外增强的鲁棒性与视觉-语言联合的泛化能力，有效提升了目标检测在开放世界和恶劣环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法在泛化到未见类别以及应对复杂环境条件时存在明显不足，且鲁棒性与类别多样性提升往往不可兼得。

Method: 提出两阶段课程式跨模态对比学习框架C3-OWD：第一阶段通过RGBT（可见光-红外）数据预训练增强鲁棒性，第二阶段利用视觉-语言对齐提升类别泛化能力。为防止两阶段间遗忘，引入EMA机制以保证参数更新和性能一致性。

Result: 在FLIR、OV-COCO和OV-LVIS三大数据集上，C3-OWD分别取得了80.1 AP50，48.6 AP50_Novel和35.7 mAP_r的优异成绩，表现出兼具鲁棒性与多样性的强大性能。

Conclusion: C3-OWD在目标检测中实现了对鲁棒性和多样性的统一提升，为实际复杂环境下的部署提供了更具潜力的解决方案。

Abstract: Object detection has advanced significantly in the closed-set setting, but
real-world deployment remains limited by two challenges: poor generalization to
unseen categories and insufficient robustness under adverse conditions. Prior
research has explored these issues separately: visible-infrared detection
improves robustness but lacks generalization, while open-world detection
leverages vision-language alignment strategy for category diversity but
struggles under extreme environments. This trade-off leaves robustness and
diversity difficult to achieve simultaneously. To mitigate these issues, we
propose \textbf{C3-OWD}, a curriculum cross-modal contrastive learning
framework that unifies both strengths. Stage~1 enhances robustness by
pretraining with RGBT data, while Stage~2 improves generalization via
vision-language alignment. To prevent catastrophic forgetting between two
stages, we introduce an Exponential Moving Average (EMA) mechanism that
theoretically guarantees preservation of pre-stage performance with bounded
parameter lag and function consistency. Experiments on FLIR, OV-COCO, and
OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$
AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\text{Novel}}$ on OV-COCO, and $35.7$
mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness
and diversity evaluations. Code available at:
https://github.com/justin-herry/C3-OWD.git.

</details>


### [76] [Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion](https://arxiv.org/abs/2509.23321)
*Yizhen Jiang,Mengting Ma,Anqi Zhu,Xiaowen Ma,Jiaxin Li,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型二值神经网络（S2BNet），用于高效执行遥感图像的全色-多光谱融合，有效减少运算复杂度的同时，保持了良好的空间和谱特性重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习全色锐化模型虽性能优异，但运算复杂度高、难以部署在计算资源有限的设备上，急需更高效的解决方案。

Method: 核心创新为定制化的空间-谱二值卷积（S2B-Conv），由频谱再分配机制（SRM）和Gabor空间特征放大器（GSFA）组成，前者可动态调整谱分布，后者则强化多尺度及方向特征提取，显著缓解二值化带来的谱失真与空间轮廓丢失问题。

Result: 通过大量定量及定性实验，所提方法在保证高效率前提下，获得了与传统高复杂度模型相当或更优的全色锐化效果。

Conclusion: S2BNet首次验证了二值神经网络在遥感全色锐化领域的可行性，为后续高效模型的开发与实际应用提供了有力支持。

Abstract: Remote sensing pansharpening aims to reconstruct spatial-spectral properties
during the fusion of panchromatic (PAN) images and low-resolution
multi-spectral (LR-MS) images, finally generating the high-resolution
multi-spectral (HR-MS) images. Although deep learning-based models have
achieved excellent performance, they often come with high computational
complexity, which hinder their applications on resource-limited devices. In
this paper, we explore the feasibility of applying the binary neural network
(BNN) to pan-sharpening. Nevertheless, there are two main issues with
binarizing pan-sharpening models: (i) the binarization will cause serious
spectral distortion due to the inconsistent spectral distribution of the
PAN/LR-MS images; (ii) the common binary convolution kernel is difficult to
adapt to the multi-scale and anisotropic spatial features of remote sensing
objects, resulting in serious degradation of contours. To address the above
issues, we design the customized spatial-spectral binarized convolution
(S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM)
and Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine
transformation, generating its scaling and bias parameters through a dynamic
learning process. GSFA, which randomly selects different frequencies and angles
within a preset range, enables to better handle multi-scale and-directional
spatial features. A series of S2B-Conv form a brand-new binary network for
pan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative
experiments have shown our high-efficiency binarized pan-sharpening method can
attain a promising performance.

</details>


### [77] [Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning](https://arxiv.org/abs/2509.23322)
*Hongrui Jia,Chaoya Jiang,Shikun Zhang,Wei Ye*

Main category: cs.CV

TL;DR: 本文提出了一种简单但有效的无训练视觉推理流程，通过将高层推理与视觉感知解耦，大幅提升了大规模多模态模型（LMMs）在视觉推理任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在长链推理过程中逐渐偏向文本逻辑，忽视视觉信息，导致推理与图像内容脱节并产生错误结论，因此需要方法增强LMMs推理的视觉依据。

Method: 该方法将推理与感知流程解耦：利用强大的LLM进行高层推理，并在需要视觉信息时，指示LMM以视觉问答引擎的方式提供所需的感知细节；该流程无需额外训练或模型结构修改，可直接集成使用。

Result: 实验显示，该方法有效减少了推理中与视觉脱节的步骤，大幅提升了视觉推理的准确性和可靠性。

Conclusion: 本文提出的无训练、分离式推理—感知框架能显著提升LMMs视觉链式推理的质量，无需修改模型结构或重新训练，可作为轻量级的插件方案广泛应用。

Abstract: Significant advancements in the reasoning capabilities of Large Language
Models (LLMs) are now driven by test-time scaling laws, particularly those
leveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these
breakthroughs, researchers have extended these paradigms to Large Multimodal
Models (LMMs). However, a critical limitation emerges: as their reasoning
chains extend, LMMs increasingly rely on textual logic, progressively losing
grounding in the underlying visual information. This leads to reasoning paths
that diverge from the image content, culminating in erroneous conclusions. To
address this, we introduce a strikingly simple yet effective training-free
visual-reasoning pipeline. The core concept is to decouple the reasoning and
perception processes. A powerful LLM orchestrates the high-level reasoning,
strategically interrogating a LMM to extract specific visual information
required for its logical chain. The LMM, in turn, functions exclusively as a
visual question-answering engine, supplying the necessary perceptual details on
demand. This lightweight, plug-and-play approach requires no additional
training or architectural changes. Comprehensive evaluations validate that our
framework effectively governs the visual reasoning process, leading to a
significant reduction in visually-unfounded reasoning steps and a substantial
improvement in reasoning fidelity.

</details>


### [78] [DDP: Dual-Decoupled Prompting for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2509.23335)
*Kaile Du,Zihan Ye,Junzhou Xie,Fan Lyu,Yixi Shen,Yuyang Li,Miaoxuan Zhu,Fuyuan Hu,Ling Shao,Guangcan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dual-Decoupled Prompting (DDP)的新方法，有效解决了多标签增量学习（MLCIL）中的混淆和误判问题，不依赖样本重放，且参数高效，实验优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 多标签增量学习面临类别共现导致的语义混淆和部分标注引发的真假负混淆问题，现有的提示（prompt）方法在MLCIL场景下效果不佳，需提出专门方法提升性能。

Method: 提出DDP框架：为不同类别分配正负类特异性提示以解耦语义，并引入渐进式信心解耦（PCD）策略来抑制假阳性，历史提示被冻结作为知识锚点，同时采用跨层提示提升效率。

Result: 在MS-COCO和PASCAL VOC数据集上，DDP方法明显优于以往工作，是首个无需样本重放、在标准MS-COCO B40-C10指标下mAP超过80%、F1超过70%的MLCIL方法。

Conclusion: DDP实现了无需重放下多标签增量学习的有效性和新纪录，显著缓解了语义混淆和误判，并为相关任务提供了新的高效方案。

Abstract: Prompt-based methods have shown strong effectiveness in single-label
class-incremental learning, but their direct extension to multi-label
class-incremental learning (MLCIL) performs poorly due to two intrinsic
challenges: semantic confusion from co-occurring categories and
true-negative-false-positive confusion caused by partial labeling. We propose
Dual-Decoupled Prompting (DDP), a replay-free and parameter-efficient framework
that explicitly addresses both issues. DDP assigns class-specific
positive-negative prompts to disentangle semantics and introduces Progressive
Confidence Decoupling (PCD), a curriculum-inspired decoupling strategy that
suppresses false positives. Past prompts are frozen as knowledge anchors, and
interlayer prompting enhances efficiency. On MS-COCO and PASCAL VOC, DDP
consistently outperforms prior methods and is the first replay-free MLCIL
approach to exceed 80% mAP and 70% F1 under the standard MS-COCO B40-C10
benchmark.

</details>


### [79] [LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning](https://arxiv.org/abs/2509.23339)
*Bin Wu,Yahui Liu,Chi Zhang,Yao Zhao,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于策略优化的新方法（LRPO），利用在线强化学习用于盲人脸修复，有效提升修复质量并获得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 盲人脸修复在处理噪声和低质量图像时容易出现细节缺失和身份模糊等伪影，传统方法难以探索巨大解空间，亟需创新性的提升手段提高修复效果。

Method: 提出Likelihood-Regularized Policy Optimization（LRPO）框架，将在线强化学习首次引入盲人脸修复。方法利用采样结果反馈调整策略网络，同时设计三项关键策略：1）为人脸修复量身定制的复合奖励函数；2）引入与真值一致的似然正则化；3）利用噪声水平分配优势。

Result: 通过大量实验，LRPO显著提升了人脸修复图像的质量，在低质量输入下表现突出，并且超越了现有主流方法，获得了最新最好（SOTA）的结果。

Conclusion: 创新性的将强化学习引入盲人脸修复任务，并结合多项针对性技术，LRPO既保证了感知质量又提高了与真值的一致性，显著推动了BFR任务的技术进步。

Abstract: Blind Face Restoration (BFR) encounters inherent challenges in exploring its
large solution space, leading to common artifacts like missing details and
identity ambiguity in the restored images. To tackle these challenges, we
propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the
first to apply online reinforcement learning (RL) to the BFR task. LRPO
leverages rewards from sampled candidates to refine the policy network,
increasing the likelihood of high-quality outputs while improving restoration
performance on low-quality inputs. However, directly applying RL to BFR creates
incompatibility issues, producing restoration results that deviate
significantly from the ground truth. To balance perceptual quality and
fidelity, we propose three key strategies: 1) a composite reward function
tailored for face restoration assessment, 2) ground-truth guided likelihood
regularization, and 3) noise-level advantage assignment. Extensive experiments
demonstrate that our proposed LRPO significantly improves the face restoration
quality over baseline methods and achieves state-of-the-art performance.

</details>


### [80] [DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice](https://arxiv.org/abs/2509.23344)
*Zijie Meng,Jin Hao,Xiwei Dai,Yang Feng,Jiaxiang Liu,Bin Feng,Huikai Wu,Xiaotang Gai,Hengchuan Zhu,Tianxiang Hu,Yangyang Wu,Hongxia Xu,Jin Li,Jun Xiao,Xiaoqiang Liu,Joey Tianyi Zhou,Fudong Zhu,Zhihe Zhao,Lunguo Xia,Bing Fang,Jimeng Sun,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: DentVLM是一款多模态视觉-语言模型，在口腔疾病诊断中展现出专家级表现，大幅优于现有AI模型及部分牙医，并能提升临床效率和护理水平。


<details>
  <summary>Details</summary>
Motivation: 目前AI模型多在单一任务上表现良好，难以应对实际口腔临床中涉及多模态、多任务的信息整合需求。亟需开发能支持复杂诊疗流程的多模态AI系统，提高口腔医疗的智能化与普惠性。

Method: 作者开发了DentVLM模型，利用包含110,447张口腔影像及246万组视觉问答数据的双语大规模数据集进行训练，支持七种2D影像模态、36项诊断任务。通过与主流AI模型和实际牙医对比，全面测试了DentVLM的性能。

Result: DentVLM在口腔疾病与错牙合诊断准确率上分别超越主流模型19.6%、27.9%；在25位牙医临床测试中，模型在36项任务中比13位初级牙医有21项表现更优，比12位资深牙医有12项表现更好，并能提升初级牙医诊断水平、缩短诊断时间15-22%。在居家、医院和多主体协作等场景下也验证了其实用价值。

Conclusion: DentVLM作为强有力的临床辅助决策工具，有望提升基础口腔护理、缓解医患资源不平衡，并推动专业口腔医疗的普惠化。

Abstract: Diagnosing and managing oral diseases necessitate advanced visual
interpretation across diverse imaging modalities and integrated information
synthesis. While current AI models excel at isolated tasks, they often fall
short in addressing the complex, multimodal requirements of comprehensive
clinical dental practice. Here we introduce DentVLM, a multimodal
vision-language model engineered for expert-level oral disease diagnosis.
DentVLM was developed using a comprehensive, large-scale, bilingual dataset of
110,447 images and 2.46 million visual question-answering (VQA) pairs. The
model is capable of interpreting seven 2D oral imaging modalities across 36
diagnostic tasks, significantly outperforming leading proprietary and
open-source models by 19.6% higher accuracy for oral diseases and 27.9% for
malocclusions. In a clinical study involving 25 dentists, evaluating 1,946
patients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic
performance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12
senior dentists on 12 of 36 tasks. When integrated into a collaborative
workflow, DentVLM elevated junior dentists' performance to senior levels and
reduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM
exhibited promising performance across three practical utility scenarios,
including home-based dental health management, hospital-based intelligent
diagnosis and multi-agent collaborative interaction. These findings establish
DentVLM as a robust clinical decision support tool, poised to enhance primary
dental care, mitigate provider-patient imbalances, and democratize access to
specialized medical expertise within the field of dentistry.

</details>


### [81] [Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling](https://arxiv.org/abs/2509.23352)
*Xiaolong Fu,Lichen Ma,Zipeng Guo,Gaojing Zhou,Chongxiao Wang,ShiPing Dong,Shizhe Zhou,Shizhe Zhou,Ximan Liu,Jingling Fu,Tan Lit Sin,Yu Shi,Zhen Chen,Junshi Huang,Jason Li*

Main category: cs.CV

TL;DR: 本文提出一种新的动态树结构采样与强化学习结合的方法，用于提升文本生成图像（T2I）模型的表现，显著提升了语义一致性、图像质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有将强化学习（RL）引入文本到图像生成的流匹配模型，虽然提升了生成质量，但探索过程消耗大且采样效率低。为解决这一问题，作者希望设计更高效的探索与采样机制，降低计算成本同时提升生成效果。

Method: 提出Dynamic-TreeRPO方法，将滑动窗口采样扩展为带有动态噪声强度的树结构搜索，通过共享前缀路径减少计算开销，并在每层动态调整噪声增强探索多样性。同时，整合了有监督微调（SFT）和强化学习，将SFT损失重构为动态加权进展奖励模型（PRM），辅以自适应裁剪，提升模型的探索效果和稳定性。

Result: 在多个评测基准（如HPS-v2.1、PickScore、ImageReward）上，所提模型在语义一致性、视觉保真度和人类偏好对齐等方面分别超越了SoTA基线4.9%、5.91%、8.66%，训练效率提升近50%。

Conclusion: Dynamic-TreeRPO及融合SFT和RL的LayerTuning-RL新范式，能在保证训练效率的前提下，有效扩大探索空间，显著提升文本生成图像的综合表现。

Abstract: The integration of Reinforcement Learning (RL) into flow matching models for
text-to-image (T2I) generation has driven substantial advances in generation
quality. However, these gains often come at the cost of exhaustive exploration
and inefficient sampling strategies due to slight variation in the sampling
group. Building on this insight, we propose Dynamic-TreeRPO, which implements
the sliding-window sampling strategy as a tree-structured search with dynamic
noise intensities along depth. We perform GRPO-guided optimization and
constrained Stochastic Differential Equation (SDE) sampling within this tree
structure. By sharing prefix paths of the tree, our design effectively
amortizes the computational overhead of trajectory search. With well-designed
noise intensities for each tree layer, Dynamic-TreeRPO can enhance the
variation of exploration without any extra computational cost. Furthermore, we
seamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within
Dynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the
loss function of SFT as a dynamically weighted Progress Reward Model (PRM)
rather than a separate pretraining method. By associating this weighted PRM
with dynamic-adaptive clipping bounds, the disruption of exploration process in
Dynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and
the LayerTuning-RL paradigm, our model dynamically explores a diverse search
space along effective directions. Compared to existing baselines, our approach
demonstrates significant superiority in terms of semantic consistency, visual
fidelity, and human preference alignment on established benchmarks, including
HPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA
by $4.9\%$, $5.91\%$, and $8.66\%$ on those benchmarks, respectively, while
improving the training efficiency by nearly $50\%$.

</details>


### [82] [Test-time Uncertainty Estimation for Medical Image Registration via Transformation Equivariance](https://arxiv.org/abs/2509.23355)
*Lin Tian,Xiaoling Hu,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了一种适用于任何预训练图像配准网络的测试时不确定性估计方法，利用输入空间扰动下预测结果的变化推断不确定性，无需修改原有网络结构或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像配准网络在实际应用中很难判断预测结果的可靠性，目前常用的不确定性估计方法如贝叶斯方法、集成或MC dropout，往往需要改变网络结构或重新训练，不适用于已经训练好的网络。

Method: 作者提出了一种基于变换等变性的测试时不确定性估计框架，对输入图像施加空间扰动后多次通过网络，统计预测变换的方差，并理论上将不确定性分解为两个部分：固有噪声（epistemic）与系统性偏差（bias jitter），操作上无需改变原始网络结构。

Result: 在脑部、心脏、腹部和肺部四种结构的多种配准网络上，所得不确定性分布与真实配准误差高度相关，能够有效提示高风险区域。

Conclusion: 该方法无需重新训练或结构改动，就能使任意预训练配准网络具备感知风险的能力，为医学影像配准在临床及大规模研究中的安全部署提供了重要工具。

Abstract: Accurate image registration is essential for downstream applications, yet
current deep registration networks provide limited indications of whether and
when their predictions are reliable. Existing uncertainty estimation
strategies, such as Bayesian methods, ensembles, or MC dropout, require
architectural changes or retraining, limiting their applicability to pretrained
registration networks. Instead, we propose a test-time uncertainty estimation
framework that is compatible with any pretrained networks. Our framework is
grounded in the transformation equivariance property of registration, which
states that the true mapping between two images should remain consistent under
spatial perturbations of the input. By analyzing the variance of network
predictions under such perturbations, we derive a theoretical decomposition of
perturbation-based uncertainty in registration. This decomposition separates
into two terms: (i) an intrinsic spread, reflecting epistemic noise, and (ii) a
bias jitter, capturing how systematic error drifts under perturbations. Across
four anatomical structures (brain, cardiac, abdominal, and lung) and multiple
registration models (uniGradICON, SynthMorph), the uncertainty maps correlate
consistently with registration errors and highlight regions requiring caution.
Our framework turns any pretrained registration network into a risk-aware tool
at test time, placing medical image registration one step closer to safe
deployment in clinical and large-scale research settings.

</details>


### [83] [GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval](https://arxiv.org/abs/2509.23370)
*Zhaohua Zhang,Jianhuan Zhuo,Muxi Chen,Chenchen Zhao,Wenyu Jiang,Tianwen Jiang,Mingyang Chen,Yu Tang,Qiuyong Xiao,Jihong Zhang,Zhixun Su*

Main category: cs.CV

TL;DR: 本文提出了GRAPE方法，通过结合排序信号优化LLM生成的检索查询，有效提升了CLIP模型在多语言、长文本和多模态输入等分布转移条件下的检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型虽在大规模检索领域表现突出，但当遇到输入数据分布与训练集分布差异较大（如多语言、长文本、多模态查询）时性能明显下降。现有方法依赖LLM进行查询改写，但因缺乏有效监督信号，改写结果往往不能很好对齐CLIP训练分布，检索效果有限。

Method: 作者提出GRAPE，一种即插即用的增强方案，通过将检索排序信号引导到LLM的查询改写环节，并引入分组排名优化（GRPO）来更好地消除分布差异。如直接用相似度分数微调LLM会导致分数膨胀，作者则引入基于语料的相对排名奖励，有效控制膨胀并准确对齐排序目标。

Result: 大量实验证明，GRAPE在多语言（如Flickr30k-CN等）、文本长度差异（Wikipedia）、多模态（CIRR）等不同分布迁移下，检索Recall@10平均提升4.9%。

Conclusion: GRAPE方法可以作为通用插件显著增强CLIP等检索系统在各类分布转移场景下的表现，为大规模多模态检索带来更强的适应能力。

Abstract: The CLIP model has become a cornerstone of large-scale retrieval systems by
aligning text and image data in a unified embedding space. Despite its
simplicity and efficiency, CLIP struggles when applied to tasks whose input
distributions diverge from its training corpus, such as queries with
multilingual, long-form, or multimodal differences. To avoid costly retraining,
existing methods mainly adopt query-rewriting strategies with large language
models (LLMs), aiming to mitigate distribution gaps at the query level.
However, due to the lack of supervision signals, LLMs fail to generate the
optimal one that fits the training distribution. We address this challenge with
GRAPE (Grouped Ranking-Aware Policy Optimization Enhancement), a plug-and-play
enhancement approach that incorporates ranking signals into retrieval-guided
query rewriting with LLMs. Intuitively, GRAPE proposes to leverage GRPO to
bridge distributional differences -- including length, multilingual, and
modality shifts -- by transforming queries into forms better aligned with the
retriever's training distribution. However, our preliminary experiment finds
that naively finetuning LLM with similarity scores can lead to score inflation,
where nearly all candidates are assigned unexpectedly high scores regardless of
their true relevance. To address score inflation, we propose a corpus-relative
ranking-based reward, which explicitly aligns optimization with ranking metrics
while suppressing spurious score inflation. Extensive experiments demonstrate
that GRAPE consistently improves retrieval performance under distributional
shifts -- including multilingual differences (Flickr30k-CN, CVLUE, XM3600),
length differences (Wikipedia), and multimodal differences (CIRR) -- achieving
an average improvement of 4.9\% in Recall\@10. The code is available at
https://github.com/Chinese0123456/GRAPE.git

</details>


### [84] [CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation](https://arxiv.org/abs/2509.23375)
*Yifan Yang,Yuxiang Yan,Boda Liu,Jian Pu*

Main category: cs.CV

TL;DR: 该论文提出CasPoinTr，一个结合级联网络和知识蒸馏的点云补全框架，在ShapeNet-55数据集上优于现有方法，提升了形状恢复和细节还原能力。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中采集到的点云因传感器分辨率、视角、遮挡和噪声等因素常常不完整，难以直接用于三维应用，补全点云变得十分关键。主要挑战是如何从高度不完整的点云中预测出整体形状并重建缺失部分。

Method: 方法上，CasPoinTr分为两级：第一阶段进行形状重建，生成辅助信息；第二阶段则融合这些辅助信息，通过知识蒸馏，利用在稠密点云上训练的教师模型将完整性知识传给在稀疏点云上工作的学生模型，两网络协同提升全局形状理解及细节还原能力。

Result: 在公认的ShapeNet-55数据集上，在不同难度下，CasPoinTr均超过了现有主流方法，无论是整体形状恢复还是局部细节保留方面表现优异。

Conclusion: 级联网络结构结合知识蒸馏，有效提升了高度不完整点云的补全效果，为相关任务提供了新方案。

Abstract: Point clouds collected from real-world environments are often incomplete due
to factors such as limited sensor resolution, single viewpoints, occlusions,
and noise. These challenges make point cloud completion essential for various
applications. A key difficulty in this task is predicting the overall shape and
reconstructing missing regions from highly incomplete point clouds. To address
this, we introduce CasPoinTr, a novel point cloud completion framework using
cascaded networks and knowledge distillation. CasPoinTr decomposes the
completion task into two synergistic stages: Shape Reconstruction, which
generates auxiliary information, and Fused Completion, which leverages this
information alongside knowledge distillation to generate the final output.
Through knowledge distillation, a teacher model trained on denser point clouds
transfers incomplete-complete associative knowledge to the student model,
enhancing its ability to estimate the overall shape and predict missing
regions. Together, the cascaded networks and knowledge distillation enhance the
model's ability to capture global shape context while refining local details,
effectively bridging the gap between incomplete inputs and complete targets.
Experiments on ShapeNet-55 under different difficulty settings demonstrate that
CasPoinTr outperforms existing methods in shape recovery and detail
preservation, highlighting the effectiveness of our cascaded structure and
distillation strategy.

</details>


### [85] [UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation](https://arxiv.org/abs/2509.23376)
*Jinghong Zheng,Changlong Jiang,Jiaqi Li,Haohong Kuang,Hang Xu,Tingbing Yan*

Main category: cs.CV

TL;DR: 本文提出了UniPose方法，无需人工标注3D关键点，通过跨模态传播2D姿态先验，实现弱监督的3D人体姿态估计，在主流数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前3D人体姿态估计需要大量人工标注的3D数据，收集成本高，难以扩展，现有方法跨2D和3D域的适应性和鲁棒性有限。

Method: 提出UniPose，通过自监督学习，将大规模2D RGB姿态标注迁移到3D领域，在不需3D标注的RGB-D序列上训练。利用现成的2D姿态估计结果作为点云网络的弱监督信号，引入空间-时间约束。创新提出anchor-to-joint方法进行3D lifting，并结合多模态、伪标签和2D-3D反投影损失，提升鲁棒性和准确性。

Result: 在CMU Panoptic和ITOP数据集上，UniPose达到与全监督方法相当的性能。使用大规模未标注数据（如NTU RGB+D 60）可进一步提升在复杂条件下的效果，3D lifting方法取得最新最优结果。

Conclusion: UniPose无需昂贵的3D关键点标注即可实现高质量3D姿态估计，兼具多模态学习、自监督和跨域泛化能力，有望推动实际场景中的3D人体理解应用。

Abstract: In this paper, we present UniPose, a unified cross-modality pose prior
propagation method for weakly supervised 3D human pose estimation (HPE) using
unannotated single-view RGB-D sequences (RGB, depth, and point cloud data).
UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS
COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D
sequences, eliminating the need for labor-intensive 3D keypoint annotations.
This approach bridges the gap between 2D and 3D domains without suffering from
issues related to multi-view camera calibration or synthetic-to-real data
shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as
weak supervision for point cloud networks, incorporating spatial-temporal
constraints like body symmetry and joint motion. The 2D-to-3D back-projection
loss and cross-modality interaction further enhance this process. By treating
the point cloud network's 3D HPE results as pseudo ground truth, our
anchor-to-joint prediction method performs 3D lifting on RGB and depth
networks, making it more robust against inaccuracies in 2D HPE results compared
to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show
that UniPose achieves comparable performance to fully supervised methods.
Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its
performance under challenging conditions, demonstrating its potential for
practical applications. Our proposed 3D lifting method also achieves
state-of-the-art results.

</details>


### [86] [Generative Modeling of Shape-Dependent Self-Contact Human Poses](https://arxiv.org/abs/2509.23393)
*Takehiko Ohkawa,Jihyun Lee,Shunsuke Saito,Jason Saragih,Fabian Prado,Yichen Xu,Shoou-I Yu,Ryosuke Furuta,Yoichi Sato,Takaaki Shiratori*

Main category: cs.CV

TL;DR: 本文提出了首个包含精确人体形状登记的大规模自接触数据集Goliath-SC，并基于该数据提出了结合形状参数的生成式自接触姿态建模方法，有效提升了单视角下自接触姿态估计的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自接触相关数据集缺乏姿态多样性和精确体型数据，导致无法细致分析体型与自接触动作之间的关系，也无法准确建模和估计具有自接触的人体姿态。

Method: 作者收集并标注了包含130名被试、38.3万种自接触姿态及精确体型参数的数据集Goliath-SC。在此基础上，提出了一种基于body-part-wise latent diffusion及自注意力机制、受体型参数调控的自接触生成模型。此外，将该先验融入单视角人体姿态估计，并通过后处理优化体接触。

Result: 实验表明，接入体型参数的自接触先验对于成功建模自接触姿态分布至关重要，在自接触情境下能大幅提升单视角姿态估计的准确性。

Conclusion: 作者验证了结合体型参数对自接触姿态建模的重要作用，显著提升了单视角自接触姿态估计，为人体形状与动作关系的深入分析提供了数据和方法基础。

Abstract: One can hardly model self-contact of human poses without considering
underlying body shapes. For example, the pose of rubbing a belly for a person
with a low BMI leads to penetration of the hand into the belly for a person
with a high BMI. Despite its relevance, existing self-contact datasets lack the
variety of self-contact poses and precise body shapes, limiting conclusive
analysis between self-contact poses and shapes. To address this, we begin by
introducing the first extensive self-contact dataset with precise body shape
registration, Goliath-SC, consisting of 383K self-contact poses across 130
subjects. Using this dataset, we propose generative modeling of self-contact
prior conditioned by body shape parameters, based on a body-part-wise latent
diffusion with self-attention. We further incorporate this prior into
single-view human pose estimation while refining estimated poses to be in
contact. Our experiments suggest that shape conditioning is vital to the
successful modeling of self-contact pose distribution, hence improving
single-view pose estimation in self-contact.

</details>


### [87] [WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving](https://arxiv.org/abs/2509.23402)
*Ziyue Zhu,Zhanqian Wu,Zhenxin Zhu,Lijun Zhou,Haiyang Sun,Bing Wan,Kun Ma,Guang Chen,Hangjun Ye,Jin Xie,jian Yang*

Main category: cs.CV

TL;DR: WorldSplat是一个创新的4D驾驶场景生成方法，结合生成和重建优势，能生成高保真、多视角、一致性的驾驶视频。


<details>
  <summary>Details</summary>
Motivation: 以往生成方法难以兼顾高质量新视角合成和三维一致性，而重建方法虽有强NVS能力却缺少生成灵活性，现有方案不能两全。

Method: 提出WorldSplat，包括两步：首先用融合多模态信息的4D感知潜变量扩散模型，直接生成像素配准的4D高斯数据；接着用改进的视频扩散模型细化新视角视频质量。

Result: 在多个标准数据集上的大量实验证明，WorldSplat能生成高保真、时间和空间上一致的多轨新视角驾驶视频。

Conclusion: WorldSplat为4D驾驶场景数据的生成和合成带来突破，为自动驾驶系统提供高质量、可控的合成数据，兼顾生成能力和NVS表现。

Abstract: Recent advances in driving-scene generation and reconstruction have
demonstrated significant potential for enhancing autonomous driving systems by
producing scalable and controllable training data. Existing generation methods
primarily focus on synthesizing diverse and high-fidelity driving videos;
however, due to limited 3D consistency and sparse viewpoint coverage, they
struggle to support convenient and high-quality novel-view synthesis (NVS).
Conversely, recent 3D/4D reconstruction approaches have significantly improved
NVS for real-world driving scenes, yet inherently lack generative capabilities.
To overcome this dilemma between scene generation and reconstruction, we
propose \textbf{WorldSplat}, a novel feed-forward framework for 4D
driving-scene generation. Our approach effectively generates consistent
multi-track videos through two key steps: ((i)) We introduce a 4D-aware latent
diffusion model integrating multi-modal information to produce pixel-aligned 4D
Gaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel
view videos rendered from these Gaussians using a enhanced video diffusion
model. Extensive experiments conducted on benchmark datasets demonstrate that
\textbf{WorldSplat} effectively generates high-fidelity, temporally and
spatially consistent multi-track novel view driving videos.

</details>


### [88] [Enhanced Fracture Diagnosis Based on Critical Regional and Scale Aware in YOLO](https://arxiv.org/abs/2509.23408)
*Yuyang Sun,Junchuan Yu,Cuiming Zou*

Main category: cs.CV

TL;DR: 本研究提出了一种改进的YOLO模型（Fracture-YOLO），通过引入关键区域注意力模块和尺度感知头部，大幅提升了骨折检测的精度，实验结果显示其性能优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统骨折诊断依赖有经验医生的人工判断，速度和准确率受限。随着深度学习的发展，基于YOLO的骨折检测显示出提升效率和准确性的巨大潜力，但仍有提升空间。

Method: 该研究提出Fracture-YOLO模型，创新性地引入Critical-Region-Selector Attention(CRSelector)模块，利用全局纹理信息聚焦骨折关键区域特征，并采用Scale-Aware(ScA)模块对不同尺度特征动态调整权重；整体提升对多尺度骨折的检测能力。

Result: 实验表明，相较基线模型，Fracture-YOLO在检测精度上有显著提升，mAP50提升4个百分点，mAP50-95提升3个百分点，达到当前最新水平。

Conclusion: Fracture-YOLO有效提升了骨折检测的性能，实现了对多尺度目标的精确识别，推动了人工智能在医学影像分析领域的发展。

Abstract: Fracture detection plays a critical role in medical imaging analysis,
traditional fracture diagnosis relies on visual assessment by experienced
physicians, however the speed and accuracy of this approach are constrained by
the expertise. With the rapid advancements in artificial intelligence, deep
learning models based on the YOLO framework have been widely employed for
fracture detection, demonstrating significant potential in improving diagnostic
efficiency and accuracy. This study proposes an improved YOLO-based model,
termed Fracture-YOLO, which integrates novel Critical-Region-Selector Attention
(CRSelector) and Scale-Aware (ScA) heads to further enhance detection
performance. Specifically, the CRSelector module utilizes global texture
information to focus on critical features of fracture regions. Meanwhile, the
ScA module dynamically adjusts the weights of features at different scales,
enhancing the model's capacity to identify fracture targets at multiple scales.
Experimental results demonstrate that, compared to the baseline model,
Fracture-YOLO achieves a significant improvement in detection precision, with
mAP50 and mAP50-95 increasing by 4 and 3, surpassing the baseline model and
achieving state-of-the-art (SOTA) performance.

</details>


### [89] [FracDetNet: Advanced Fracture Detection via Dual-Focus Attention and Multi-scale Calibration in Medical X-ray Imaging](https://arxiv.org/abs/2509.23416)
*Yuyang Sun,Cuiming Zou*

Main category: cs.CV

TL;DR: 本文提出了FracDetNet，一个新型断裂检测框架，通过引入双重关注机制和多尺度校准，有效提升医学影像中的骨折检测精度，在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 准确检测骨折对临床诊断极为重要，但现有方法受限于成像角度变化和图像质量，难以识别细微或多样形态的骨折。作者旨在解决这些挑战，提升骨折检测技术的应用价值。

Method: 提出FracDetNet框架，融合Dual-Focus Attention（DFA）和Multi-scale Calibration（MC）模块。DFA结合全局与局部关注机制，捕捉丰富特征；MC自适应优化特征表征，提高检测效果。在公开医学影像数据集GRAZPEDWRI-DX上进行实验证明其优越性。

Result: FracDetNet在GRAZPEDWRI-DX数据集上取得mAP$_{50-95}$ 40.0%，比基线提升7.5%；mAP$_{50}$达到63.9%，提升4.2%。针对骨折的检测精度也提升2.9%。

Conclusion: FracDetNet通过创新的注意力机制和特征优化方法，显著提升了骨折检测的准确率，为医学图像分析提供了更有效的辅助工具。

Abstract: In this paper, an advanced fracture detection framework, FracDetNet, is
proposed to address challenges in medical imaging, as accurate fracture
detection is essential for enhancing diagnostic efficiency in clinical
practice. Despite recent advancements, existing methods still struggle with
detecting subtle and morphologically diverse fractures due to variable imaging
angles and suboptimal image quality. To overcome these limitations, FracDetNet
integrates Dual-Focus Attention (DFA) and Multi-scale Calibration (MC).
Specifically, the DFA module effectively captures detailed local features and
comprehensive global context through combined global and local attention
mechanisms. Additionally, the MC adaptively refines feature representations to
enhance detection performance. Experimental evaluations on the publicly
available GRAZPEDWRI-DX dataset demonstrate state-of-the-art performance, with
FracDetNet achieving a mAP$_{50-95}$ of 40.0\%, reflecting a \textbf{7.5\%}
improvement over the baseline model. Furthermore, the mAP$_{50}$ reaches
63.9\%, representing an increase of \textbf{4.2\%}, with fracture-specific
detection accuracy also enhanced by \textbf{2.9\%}.

</details>


### [90] [SPIKE-RL: Video-LLMs meet Bayesian Surprise](https://arxiv.org/abs/2509.23433)
*Sahithya Ravi,Aditya Chinchure,Raymond T. Ng,Leonid Sigal,Vered Shwartz*

Main category: cs.CV

TL;DR: 本文提出SPIKE框架，通过量化视频流中新出现的视觉信息对模型信念的贝叶斯式更新，精确定位视频中的“突发事件”，并提出SPIKE-RL进一步优化重要帧的采样方式，有效提升视频大模型处理和理解关键时刻的能力。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM（视频大语言模型）通常采用均匀帧采样方法，容易忽略视频中关键的突发和记忆点，影响对视频叙事的准确理解。

Method: 提出SPIKE推理框架，利用贝叶斯Surprise机制，度量视频流中新帧带来的信念更新，定位与先验信念冲突的'惊喜'瞬间。同时设计SPIKE-RL算法，结合强化学习GRPO方法，根据视频描述获得奖励信号，优化模型对帧重要性的判断，实现突发事件加权采样。

Result: SPIKE可有效定位视频‘惊喜时刻’，与人类评判高度相关。使用SPIKE和SPIKE-RL指导的帧采样方法在FunQA、Oops!等五个数据集上均优于传统均匀采样，在下游任务获得一致性能提升。

Conclusion: SPIKE与SPIKE-RL能够显著增强Video-LLM对突发事件的感知和理解能力，推动模型具备根据新信息自我修正和反思的能力，对构建更加健壮的多模态理解系统具有重要意义。

Abstract: Real-world videos often show routine activities punctuated by memorable,
surprising events. However, most Video-LLMs process videos by sampling frames
uniformly, likely missing critical moments that define a video's narrative. We
introduce SPIKE, an inference-time framework that quantifies Bayesian Surprise
as the belief update triggered by new visual evidence in the video stream,
identifying moments where new visual evidence conflicts with prior beliefs.
SPIKE effectively localizes surprise in videos, strongly correlated with humans
on positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs
of zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which
leverages GRPO to optimize belief hypotheses based on a reward signal from the
video caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame
sampling, which allocates more frames to interesting moments in the video. With
this strategy, we achieve consistent performance gains on five downstream
benchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and
register surprise, our work paves the way for more robust models that can
revise their understanding in response to new information.

</details>


### [91] [FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation](https://arxiv.org/abs/2509.23438)
*Mohammed Alsakabi,Wael Mobeirek,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 本文提出FM-SIREN和FM-FINER，通过为每个神经元分配基于Nyquist准则的特定频率乘子，显著减少INR模型中的特征冗余，提高表示能力，并在多个任务中持续优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于周期激活的隐式神经表示（INR）网络（如SIREN和FINER）因为使用固定频率乘子，导致同层神经元学习到重叠的频率成分，存在特征冗余，限制了MLP的表达能力。

Method: 受离散正弦变换（DST）等经典信号处理方法启发，作者为每个神经元分配根据Nyquist准则确定的特定频率乘子，从而为周期激活函数引入多样化频率，无需超参数搜索或增加网络深度。

Result: 所提方法在减少网络特征冗余近50%的同时，在1D音频、2D图像、3D模型拟合及NeRF合成等多种INR任务中均优于SIREN和FINER等基线，同时保持高效性。

Conclusion: 基于Nyquist准则设计的神经元特定频率乘子能够有效提升INR网络表达能力和性能，为神经表征方法的改进提供了一个简洁且有效的新方向。

Abstract: Existing periodic activation-based implicit neural representation (INR)
networks, such as SIREN and FINER, suffer from hidden feature redundancy, where
neurons within a layer capture overlapping frequency components due to the use
of a fixed frequency multiplier. This redundancy limits the expressive capacity
of multilayer perceptrons (MLPs). Drawing inspiration from classical signal
processing methods such as the Discrete Sine Transform (DST), we propose
FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency
multipliers to periodic activations. Unlike existing approaches, our design
introduces frequency diversity without requiring hyperparameter tuning or
additional network depth. This simple yet principled modification reduces the
redundancy of features by nearly 50% and consistently improves signal
reconstruction across diverse INR tasks, including fitting 1D audio, 2D image
and 3D shape, and synthesis of neural radiance fields (NeRF), outperforming
their baseline counterparts while maintaining efficiency.

</details>


### [92] [FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing](https://arxiv.org/abs/2509.23452)
*Tanawan Premsri,Parisa Kordjamshidi*

Main category: cs.CV

TL;DR: 提出了一种将参考系（Frame of Reference, FoR）引入文本到图像（T2I）生成的新方法FoR-SALE，有效提升了模型对空间表达的理解和图像调整能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成模型在遇到非相机视角的空间描述时表现不佳，缺乏对人类空间参考系（FoR）的理解和处理能力，因此需要新的机制优化空间语义与图像的一致性。

Method: 基于LLM的扩散编辑框架（SLD）进行扩展，提出FoR-SALE方法。该方法先用视觉模块提取初始生成图像中的空间结构，再将空间表达映射到相应相机视角，实现文字与视觉的对齐。如果检测到错位，则自动生成并应用所需的图像编辑操作，并在潜空间调整朝向与深度。

Result: 在两个专门针对FoR空间理解设计的基准数据集上，FoR-SALE能让主流T2I模型仅用一次修正迭代，其性能提高最多可达5.3%。

Conclusion: FoR-SALE显著提升了T2I模型在处理带有多样空间参考系描述时的生成能力，为后续空间语义与多模态生成深度结合提供了新路径。

Abstract: Frame of Reference (FoR) is a fundamental concept in spatial reasoning that
humans utilize to comprehend and describe space. With the rapid progress in
Multimodal Language models, the moment has come to integrate this
long-overlooked dimension into these models. In particular, in text-to-image
(T2I) generation, even state-of-the-art models exhibit a significant
performance gap when spatial descriptions are provided from perspectives other
than the camera. To address this limitation, we propose Frame of
Reference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),
an extension of the Self-correcting LLM-controlled Diffusion (SLD) framework
for T2I. For-Sale evaluates the alignment between a given text and an initially
generated image, and refines the image based on the Frame of Reference
specified in the spatial expressions. It employs vision modules to extract the
spatial configuration of the image, while simultaneously mapping the spatial
expression to a corresponding camera perspective. This unified perspective
enables direct evaluation of alignment between language and vision. When
misalignment is detected, the required editing operations are generated and
applied. FoR-SALE applies novel latent-space operations to adjust the facing
direction and depth of the generated images. We evaluate FoR-SALE on two
benchmarks specifically designed to assess spatial understanding with FoR. Our
framework improves the performance of state-of-the-art T2I models by up to 5.3%
using only a single round of correction.

</details>


### [93] [3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras](https://arxiv.org/abs/2509.23455)
*Tharindu Ekanayake,Constantino Álvarez Casado,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本论文提出了一种名为3DPCNet的新方法，可将单目3D姿态估计得到的摄影机中心骨架规范化为身体中心坐标，实现视角无关的运动分析。其核心是一个灵活小巧、与估计器无关的模块，能有效降低姿态估计误差。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计通常得到的是摄影机中心坐标的骨架结构，这导致同一个动作在不同视角下表现不同，影响了健康和运动科学等应用中的动作对比与分析。为了解决由视点依赖带来的分析困难，作者提出要将姿态规范到一致的身体中心坐标系。

Method: 提出3DPCNet模块，直接作用于3D关节点坐标，将任意输入骨架转换到标准的、体中心的规范坐标系中。该模块采用图卷积网络(GCN)提取局部骨骼结构信息，用Transformer结合门控跨注意力机制提取全局上下文信息，通过6D连续旋转表示（映射到SO(3)矩阵）实现骨架对齐。使用合成旋转的自监督训练机制和复合损失函数进行优化。

Result: 在MM-Fi数据集上，3DPCNet将平均旋转误差从20度降低至3.4度，将Mean Per Joint Position Error从约64 mm降至47 mm。在TotalCapture数据集的定性分析中，文中方法提取的加速度信号与IMU真值高度对应，显示了其规范视角带来的物理真实性和运动分析能力。

Conclusion: 3DPCNet模块能够显著减少由视点变化带来的误差，实现视角无关的运动分析，有助于健康和体育领域的3D姿态数据科学分析。

Abstract: Monocular 3D pose estimators produce camera-centered skeletons, creating
view-dependent kinematic signals that complicate comparative analysis in
applications such as health and sports science. We present 3DPCNet, a compact,
estimator-agnostic module that operates directly on 3D joint coordinates to
rectify any input pose into a consistent, body-centered canonical frame. Its
hybrid encoder fuses local skeletal features from a graph convolutional network
with global context from a transformer via a gated cross-attention mechanism.
From this representation, the model predicts a continuous 6D rotation that is
mapped to an $SO(3)$ matrix to align the pose. We train the model in a
self-supervised manner on the MM-Fi dataset using synthetically rotated poses,
guided by a composite loss ensuring both accurate rotation and pose
reconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error
from over 20$^{\circ}$ to 3.4$^{\circ}$ and the Mean Per Joint Position Error
from ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations
on the TotalCapture dataset further demonstrate that our method produces
acceleration signals from video that show strong visual correspondence to
ground-truth IMU sensor data, confirming that our module removes viewpoint
variability to enable physically plausible motion analysis.

</details>


### [94] [No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.23457)
*Mohammad Hossein Sameti,Amir M. Mansourian,Arash Marioriyad,Soheil Fadaee Oshyani,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度的测试时优化框架，以提升文本到图像（T2I）生成模型在应对复杂提示时的表达准确性。实验表明，该方法显著提升了模型对提示中不同概念的覆盖率与忠实度。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I模型进展迅速，但它们在处理复杂提示时，常常无法准确还原所有细节，尤其是在表现特定物体和属性时容易遗漏或错误。为了解决这一不足，亟需提升模型对复杂组合性提示的响应能力。

Method: 作者提出在测试时对T2I模型进行细粒度优化：首先将输入提示语分解为若干语义概念，在全局和单独概念层面分别评估生成图像的对应关系。引入细粒度CLIP模型，计算每个概念的匹配程度，获取关于遗漏或错误概念的详细反馈。利用这一反馈，通过循环优化提示语，使大语言模型生成更优提示。

Result: 在DrawBench和CompBench等数据集上的实验表明，该方法在概念覆盖率和人类评测的忠实度两个指标上，均明显优于标准的测试时优化方法及原始T2I模型。

Conclusion: 该方法有效提升了T2I模型对复杂或多概念提示的表达能力，并且无需重新训练基础模型，具有较高的实用价值。

Abstract: Despite recent advances in text-to-image (T2I) models, they often fail to
faithfully render all elements of complex prompts, frequently omitting or
misrepresenting specific objects and attributes. Test-time optimization has
emerged as a promising approach to address this limitation by refining
generation without the need for retraining. In this paper, we propose a
fine-grained test-time optimization framework that enhances compositional
faithfulness in T2I generation. Unlike most of prior approaches that rely
solely on a global image/text similarity score, our method decomposes the input
prompt into semantic concepts and evaluates alignment at both the global and
concept levels. A fine-grained variant of CLIP is used to compute concept-level
correspondence, producing detailed feedback on missing or inaccurate concepts.
This feedback is fed into an iterative prompt refinement loop, enabling the
large language model to propose improved prompts. Experiments on DrawBench and
CompBench prompts demonstrate that our method significantly improves concept
coverage and human-judged faithfulness over both standard test-time
optimization and the base T2I model. Code is available at:
https://github.com/AmirMansurian/NoConceptLeftBehind

</details>


### [95] [Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation](https://arxiv.org/abs/2509.23475)
*Ming-Tsung Hsu,Fang-Yu Hsu,Yi-Ting Lin,Kai-Heng Chien,Jun-Ren Chen,Cheng-Hsiang Su,Yi-Chen Ou,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 该文提出了MFAS-DANet框架，在多模态变换环境下，有效提升了人脸反欺诈检测能力，特别是对未知攻击和缺失模态的适应能力。实验验证方法达到了业内最优水平。


<details>
  <summary>Details</summary>
Motivation: 多模态人脸反欺诈系统在遇到新型攻击和新领域时表现不佳，尤其是部分模态缺失和标签噪声等现实挑战。因此，研究如何在迁移学习（Domain Adaptation，DA）场景下提升多模态FAS的泛化与鲁棒性成为迫切任务。

Method: 该文提出MFAS-DANet框架，针对三大难题：（1）用其他模态的互补特征补全或增强缺失模态特征；（2）利用多模态预测不确定性筛选可靠的伪标签，提升自适应过程的标签质量；（3）通过自适应机制在泛化不稳定时减小损失权重，稳定时增大，预防模型退化。

Result: 大量实验表明，MFAS-DANet在实际迁移学习场景中有效应对了缺失模态、标签噪声及模型退化等问题，性能优于现有方法，达到SOTA水平。

Conclusion: MFAS-DANet为多模态人脸反欺诈在迁移适配中，提供了有效且实用的解决思路，对复杂实际环境具有推广意义。

Abstract: Recent multi-modal face anti-spoofing (FAS) methods have investigated the
potential of leveraging multiple modalities to distinguish live and spoof
faces. However, pre-adapted multi-modal FAS models often fail to detect unseen
attacks from new target domains. Although a more realistic domain adaptation
(DA) scenario has been proposed for single-modal FAS to learn specific spoof
attacks during inference, DA remains unexplored in multi-modal FAS methods. In
this paper, we propose a novel framework, MFAS-DANet, to address three major
challenges in multi-modal FAS under the DA scenario: missing modalities, noisy
pseudo labels, and model degradation. First, to tackle the issue of missing
modalities, we propose extracting complementary features from other modalities
to substitute missing modality features or enhance existing ones. Next, to
reduce the impact of noisy pseudo labels during model adaptation, we propose
deriving reliable pseudo labels by leveraging prediction uncertainty across
different modalities. Finally, to prevent model degradation, we design an
adaptive mechanism that decreases the loss weight during unstable adaptations
and increasing it during stable ones. Extensive experiments demonstrate the
effectiveness and state-of-the-art performance of our proposed MFAS-DANet.

</details>


### [96] [RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation](https://arxiv.org/abs/2509.23480)
*Shourya Verma,Mengbo Wang,Nadia Atallah Lanman,Ananth Grama*

Main category: cs.CV

TL;DR: 论文提出了一种新的图像修复蒸馏方法RestoRect，结合物理分解、可学习扩散和新颖特征对齐机制，实现速度与质量的兼顾，在多个任务和数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有高性能图像修复模型速度慢、快模型效果差，特征蒸馏难以捕捉transformer的动态特性，需平衡模型速度、质量和有效知识迁移。

Method: 作者提出Latent Rectified Flow Feature Distillation，将特征蒸馏建模为生成过程，学生通过学习隐空间轨迹合成高质量特征。方法结合Retinex物理分解、可学习各向异性扩散约束、三角色彩极化，并设计了Feature Layer Extraction损失用于不同网络的鲁棒特征对齐和知识迁移。

Result: RestoRect在保持恢复质量的同时训练收敛更快、推理更快，泛化到不同架构间效果好，在15个数据集4类任务8项指标上表现优越。

Conclusion: RestoRect有效提升图像修复的速度、稳定性和质量，为高效知识迁移和模型部署带来新思路。

Abstract: Current approaches for restoration of degraded images face a critical
trade-off: high-performance models are too slow for practical use, while fast
models produce poor results. Knowledge distillation transfers teacher knowledge
to students, but existing static feature matching methods cannot capture how
modern transformer architectures dynamically generate features. We propose
'RestoRect', a novel Latent Rectified Flow Feature Distillation method for
restoring degraded images. We apply rectified flow to reformulate feature
distillation as a generative process where students learn to synthesize
teacher-quality features through learnable trajectories in latent space. Our
framework combines Retinex theory for physics-based decomposition with
learnable anisotropic diffusion constraints, and trigonometric color space
polarization. We introduce a Feature Layer Extraction loss for robust knowledge
transfer between different network architectures through cross-normalized
transformer feature alignment with percentile-based outlier detection.
RestoRect achieves better training stability, and faster convergence and
inference while preserving restoration quality. We demonstrate superior results
across 15 image restoration datasets, covering 4 tasks, on 8 metrics.

</details>


### [97] [Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos](https://arxiv.org/abs/2509.23492)
*Junyi Wu,Jiachen Tao,Haoxuan Wang,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为OriGS的高质量4D重建新方法，能够利用单目视频实现对动态场景的更真实还原，效果超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有将3D高斯Splatting方法扩展到动态场景的技术，在处理复杂、局部变化剧烈的动态时，依赖于低秩假设，导致不能充分捕捉非受限动态下的区域性形变。为了解决这一问题，作者希望设计一种新的表征方式，能够自适应地对复杂局部动态进行建模与重建。

Method: 本文方法核心为引入“方向锚定”的多维高斯Splatting。首先估算整个场景的全局方向场，作为动态建模的结构性引导。基于此，提出了方向感知的超高斯表示，将时间、空间、几何和方向信息统一嵌入到概率状态中，通过有条件切片操作，自适应地推断区域性形变，并对齐全局运动趋势。

Result: 实验结果表明，OriGS在具有挑战性的现实动态场景中，重建保真度明显优于主流的动态三维重建方法。

Conclusion: OriGS方法有效提升了基于单目视频的动态场景4D重建质量，特别是在复杂局部动态和全局运动同时存在时表现优异，为动态重建领域带来新突破。

Abstract: We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework
for high-quality 4D reconstruction from casually captured monocular videos.
While recent advances extend 3D Gaussian Splatting to dynamic scenes via
various motion anchors, such as graph nodes or spline control points, they
often rely on low-rank assumptions and fall short in modeling complex,
region-specific deformations inherent to unconstrained dynamics. OriGS
addresses this by introducing a hyperdimensional representation grounded in
scene orientation. We first estimate a Global Orientation Field that propagates
principal forward directions across space and time, serving as stable
structural guidance for dynamic modeling. Built upon this, we propose
Orientation-aware Hyper-Gaussian, a unified formulation that embeds time,
space, geometry, and orientation into a coherent probabilistic state. This
enables inferring region-specific deformation through principled conditioned
slicing, adaptively capturing diverse local dynamics in alignment with global
motion intent. Experiments demonstrate the superior reconstruction fidelity of
OriGS over mainstream methods in challenging real-world dynamic scenes.

</details>


### [98] [Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional](https://arxiv.org/abs/2509.23499)
*Divyam Madaan,Varshan Muhunthan,Kyunghyun Cho,Sumit Chopra*

Main category: cs.CV

TL;DR: 本文通过大规模实证研究分析了多模态大语言模型在视觉问答任务中不同模态之间的依赖关系，并揭示了现有基准中模态依赖分布存在的显著差异及其对多模态推理能力评价的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，单一模态与目标任务的关系（模内依赖）以及不同模态间与任务的综合关系（模间依赖）对于提升多模态理解至关重要，但目前基准评测中这些依赖关系及其互作尚缺乏系统性刻画。

Method: 作者在23个涵盖通用与专业知识推理、OCR、文档理解等领域的视觉问答基准上，利用多模态大语言模型进行系统实验，对比分析了模型对视觉、文本及其交互的依赖程度。

Result: 实验显示，不同基准之间、甚至同一基准内部，对视觉、文本及其交互的依赖表现出显著差异。部分本意减少文本偏置的基准，反而强化了对图像的单一依赖。同时，随着模型规模增大，高性能往往仅依赖模内特征，掩盖了模型实际的多模态推理能力不足。

Conclusion: 该工作为多模态数据集提供了量化依赖刻画手段，并为多模态基准的设计与评估提供了理论依据和实践指南。

Abstract: Understanding the interplay between intra-modality dependencies (the
contribution of an individual modality to a target task) and inter-modality
dependencies (the relationships between modalities and the target task) is
fundamental to advancing multi-modal learning. However, the nature of and
interaction between these dependencies within current benchmark evaluations
remains poorly characterized. In this work, we present a large-scale empirical
study to quantify these dependencies across 23 visual question-answering
benchmarks using multi-modal large language models (MLLMs) covering domains
such as general and expert knowledge reasoning, optical character recognition,
and document understanding. Our findings show that the reliance on vision,
question (text), and their interaction varies significantly, both across and
within benchmarks. We discover that numerous benchmarks intended to mitigate
text-only biases have inadvertently amplified image-only dependencies. This
characterization persists across model sizes, as larger models often use these
intra-modality dependencies to achieve high performance that mask an underlying
lack of multi-modal reasoning. We provide a quantitative characterization of
multi-modal datasets, enabling a principled approach to multi-modal benchmark
design and evaluation.

</details>


### [99] [Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update](https://arxiv.org/abs/2509.23502)
*Fatemeh Salahi Chashmi,Roya Sotoudeh*

Main category: cs.CV

TL;DR: 文章提出了一种结合动态核（DK）机制和全局编码器注意力（EA）模块的新框架，有效提升了结直肠息肉分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 息肉在结直肠癌筛查中至关重要，但其形状、大小多变且边界低对比，导致分割任务困难。作者希望提升分割精度并降低运算成本。

Method: 方法结合了动态核机制和全局编码器注意力模块。EA模块聚合多尺度特征，为DK机制提供全局上下文初始化，逐步优化分割预测。此外，采用统一通道自适应（UCA）以标准化解码阶段的特征维度，实现信息融合和运算效率提升。

Result: 在KvasirSEG和CVC ClinicDB数据集上进行大量实验证明，该方法在Dice和IoU等分割指标均超过现有多种最新方法。同时，UCA简化了解码器结构，减少了算力消耗。

Conclusion: 该方法为息肉分割任务提供了一种鲁棒且适应性强的方案，对临床与自动诊断系统有良好应用前景。

Abstract: Polyp segmentation is a critical step in colorectal cancer detection, yet it
remains challenging due to the diverse shapes, sizes, and low contrast
boundaries of polyps in medical imaging. In this work, we propose a novel
framework that improves segmentation accuracy and efficiency by integrating a
Dynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK
mechanism, initialized by a global context vector from the EA module,
iteratively refines segmentation predictions across decoding stages, enabling
the model to focus on and accurately delineate complex polyp boundaries. The EA
module enhances the network's ability to capture critical lesion features by
aggregating multi scale information from all encoder layers. In addition, we
employ Unified Channel Adaptation (UCA) in the decoder to standardize feature
dimensions across stages, ensuring consistent and computationally efficient
information fusion. Our approach extends the lesion-aware kernel framework by
introducing a more flexible, attention driven kernel initialization and a
unified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB
benchmark datasets demonstrate that our model outperforms several state of the
art segmentation methods, achieving superior Dice and Intersection over Union
scores. Moreover, UCA simplifies the decoder structure, reducing computational
cost without compromising accuracy. Overall, the proposed method provides a
robust and adaptable solution for polyp segmentation, with promising
applications in clinical and automated diagnostic systems.

</details>


### [100] [Evaluating point-light biological motion in multimodal large language models](https://arxiv.org/abs/2509.23517)
*Akila Kadambi,Marco Iacoboni,Lisa Aziz-Zadeh,Srini Narayanan*

Main category: cs.CV

TL;DR: 本文提出了ActPLD基准，用于评估多模态大型语言模型(MLLMs)对人体点光源(PLD)动作识别能力，结果显示现有模型在此任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 人类早期即可通过点光源(PLDs)提取丰富语义信息，而点光源刺激仅提供运动视觉线索，被广泛用作探究人类动作理解的范式。但目前尚无相关基准系统性测试多模态大模型对PLDs的动作识别能力。

Method: 作者提出 ActPLD，作为第一个专门针对人类PLDs动作理解的多模态大模型评测基准，涵盖单人动作与社交交互两种情境，对比评估了主流商用和开源多模态模型性能。

Result: 所有参测模型（包括顶尖商用及开源系统）在ActPLD任务上均表现较差，无论是单人还是互动场景，均存在动作和时空理解能力的根本短板。

Conclusion: 当前多模态大模型在点光源人体动作理解任务上表现不佳，暴露了其在动作和时空信息处理方面存在的基础性瓶颈，有待进一步改进。

Abstract: Humans can extract rich semantic information from minimal visual cues, as
demonstrated by point-light displays (PLDs), which consist of sparse sets of
dots localized to key joints of the human body. This ability emerges early in
development and is largely attributed to human embodied experience. Since PLDs
isolate body motion as the sole source of meaning, they represent key stimuli
for testing the constraints of action understanding in these systems. Here we
introduce ActPLD, the first benchmark to evaluate action processing in MLLMs
from human PLDs. Tested models include state-of-the-art proprietary and
open-source systems on single-actor and socially interacting PLDs. Our results
reveal consistently low performance across models, introducing fundamental gaps
in action and spatiotemporal understanding.

</details>


### [101] [Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis](https://arxiv.org/abs/2509.23530)
*Alec K. Peltekian,Karolina Senkow,Gorkem Durak,Kevin M. Grudzinski,Bradford C. Bemiss,Jane E. Dematte,Carrie Richardson,Nikolay S. Markov,Mary Carns,Kathleen Aren,Alexandra Soriano,Matthew Dapas,Harris Perlman,Aaron Gundersheimer,Kavitha C. Selvan,John Varga,Monique Hinchcliff,Krishnan Warrior,Catherine A. Gao,Richard G. Wunderink,GR Scott Budinger,Alok N. Choudhary,Anthony J. Esposito,Alexander V. Misharin,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 本研究提出了一种基于影像组学与深度学习的新型纵向胸部CT分析框架，并利用2125例SSc患者的胸部CT数据预测该病相关间质性肺病的死亡风险，模型在一年、三年与五年死亡率预测方面均表现良好。


<details>
  <summary>Details</summary>
Motivation: 系统性硬化症常并发间质性肺病，是主要的发病与死亡原因，但目前CT影像用于疾病进展和死亡预测的价值尚不明确。因此，研究亟需发展更精准、自动化的预测工具，提高早期检测和风险分层的效果。

Method: 收集纳入北美硬皮病登记的SSc患者共2125例胸部CT，采用影像组学及深度学习模型（ResNet-18、DenseNet-121、Swin Transformer预训练并微调）进行不同时间窗（一年、三年、五年）内死亡风险预测，死亡标签经专家确认，模型性能用AUC评价。

Result: 模型在一年、三年、五年死亡率预测中的AUC分别为0.769、0.801、0.709，显示出良好的预测能力，有效鉴别高风险患者。

Conclusion: 基于影像组学与深度学习的CT分析框架能有效预测SSc相关间质性肺病的死亡风险，为临床早期风险评估和干预提供科学依据，是该领域的重要进展。

Abstract: Interstitial lung disease (ILD) is a leading cause of morbidity and mortality
in systemic sclerosis (SSc). Chest computed tomography (CT) is the primary
imaging modality for diagnosing and monitoring lung complications in SSc
patients. However, its role in disease progression and mortality prediction has
not yet been fully clarified. This study introduces a novel, large-scale
longitudinal chest CT analysis framework that utilizes radiomics and deep
learning to predict mortality associated with lung complications of SSc. We
collected and analyzed 2,125 CT scans from SSc patients enrolled in the
Northwestern Scleroderma Registry, conducting mortality analyses at one, three,
and five years using advanced imaging analysis techniques. Death labels were
assigned based on recorded deaths over the one-, three-, and five-year
intervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of
the 2,125 CT scans were from patients who died within one, three, and five
years, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use
pre-trained models, and fine-tuned on 2,125 images of SSc patients. Models
achieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-,
three-, and five-years, respectively. Our findings highlight the potential of
both radiomics and deep learning computational methods to improve early
detection and risk assessment of SSc-related interstitial lung disease, marking
a significant advancement in the literature.

</details>


### [102] [Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis](https://arxiv.org/abs/2509.23535)
*Ibne Farabi Shihab,Weiheng Chai,Jiyang Wang,Sanjeda Akter,Senem Velipasalar Gursoy,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种面向资源感知的自适应超分辨率框架，有效提升了驾驶员监测系统在安全关键场景下的置信度校准和检测性能，优于传统低分辨率训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶员监测系统虽然在准确率上已取得良好成绩，但其置信度校准效果欠佳，特别是在安全关键应用下可能引发危险。针对直接采用低分辨率视频训练模型导致校准不足的问题，亟需提出更可靠的解决方法。

Method: 提出了一个资源感知型自适应超分辨率（SR）框架，结合SR和低分辨率训练，优化模型在关键事件上的置信度校准和高精度召回。还设计了轻量级伪影检测器，用于过滤由SR带来的伪影输出，确保系统输出可靠。

Result: 该方法在安全指标上表现优异：校准性能（ECE）达到5.8%，优于基线模型的6.2%；嗜睡检测的AUPR为0.78（基线为0.74）；用手机检测的精确率-召回为0.74（基线为0.71）；伪影检测器仅有0.3M参数，增加时延仅5.2ms。

Conclusion: 相比于低分辨率训练的通用模型，所提自适应框架在安全关键应用中展现出更优的可靠性和性能，是该领域的最新最优解。

Abstract: Driver monitoring systems require not just high accuracy but reliable,
well-calibrated confidence scores for safety-critical deployment. While direct
low-resolution training yields high overall accuracy, it produces poorly
calibrated predictions that can be dangerous in safety-critical scenarios. We
propose a resource-aware adaptive super-resolution framework that optimizes for
model calibration and high precision-recall on critical events. Our approach
achieves state-of-the-art performance on safety-centric metrics: best
calibration (ECE of 5.8\% vs 6.2\% for LR-trained baselines), highest AUPR for
drowsiness detection (0.78 vs 0.74), and superior precision-recall for phone
use detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters,
5.2ms overhead) provides additional safety by filtering SR-induced
hallucinations. While LR-trained video models serve as strong general-purpose
baselines, our adaptive framework represents the state-of-the-art solution for
safety-critical applications where reliability is paramount.

</details>


### [103] [OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction](https://arxiv.org/abs/2509.23541)
*Hongyang Li,Jinyuan Qu,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为OVSeg3R的训练方案，实现了基于2D感知模型和3D重建的开放词汇3D实例分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D实例分割大多依赖封闭词汇和人工注释，难以适应开放世界需求，且直接从2D到3D的转移存在噪声和边界模糊问题，影响分割精度。

Method: 1. 利用2D视频重建3D场景，直接作为输入避免了人工调整；2. 利用2D-3D关联，通过2D开放词汇分割模型，将2D实例掩码投影到3D空间，为3D模型生成标签；3. 针对2D到3D映射噪声引入View-wise Instance Partition算法，稳定训练；4. 提出2D Instance Boundary-aware Superpoint算法，在超点聚类时利用2D掩码约束，以避免超点跨越实例边界。

Result: 在ScanNet200基准上，提出的方法在整体上提升了2.3 mAP，在开放词汇新类别上的mAP提升达到7.1，相较于之前方法有较大优势。

Conclusion: OVSeg3R有效实现了无封闭词汇限制的3D实例分割，不仅提升了长尾类别性能，还整体改善了分割效果，为开放世界3D理解提供了新方向。

Abstract: In this paper, we propose a training scheme called OVSeg3R to learn
open-vocabulary 3D instance segmentation from well-studied 2D perception models
with the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes
from 2D videos as input, avoiding costly manual adjustment while aligning input
with real-world applications. By exploiting the 2D to 3D correspondences
provided by 3D reconstruction models, OVSeg3R projects each view's 2D instance
mask predictions, obtained from an open-vocabulary 2D model, onto 3D to
generate annotations for the view's corresponding sub-scene. To avoid
incorrectly introduced false positives as supervision due to partial
annotations from 2D to 3D, we propose a View-wise Instance Partition algorithm,
which partitions predictions to their respective views for supervision,
stabilizing the training process. Furthermore, since 3D reconstruction models
tend to over-smooth geometric details, clustering reconstructed points into
representative super-points based solely on geometry, as commonly done in
mainstream 3D segmentation methods, may overlook geometrically non-salient
objects. We therefore introduce 2D Instance Boundary-aware Superpoint, which
leverages 2D masks to constrain the superpoint clustering, preventing
superpoints from violating instance boundaries. With these designs, OVSeg3R not
only extends a state-of-the-art closed-vocabulary 3D instance segmentation
model to open-vocabulary, but also substantially narrows the performance gap
between tail and head classes, ultimately leading to an overall improvement of
+2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard
open-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP
on the novel classes, further validating its effectiveness.

</details>


### [104] [From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations](https://arxiv.org/abs/2509.23555)
*Javed Ahmad,Penggang Gao,Donatien Delehelle,Mennuti Canio,Nikhil Deshpande,Jesús Ortiz,Darwin G. Caldwell,Yonas Teodros Tefera*

Main category: cs.CV

TL;DR: 本文综述了3D高斯溅射（3DGS）在多个三维场景应用领域相较于NeRF的技术优势及最新进展。


<details>
  <summary>Details</summary>
Motivation: NeRF引入了高保真、视角一致的三维重建技术，但在效率和实际应用中有一定限制。为寻找更高效、易集成的方法，3DGS迅速获得关注，有望成为新的主流。

Method: 文章系统梳理了3DGS在SLAM、远程临场、遥操作、机器人操作和3D内容生成等领域的应用，总结并对比了这些领域中采用3DGS和NeRF的技术路线，分析3DGS的技术优势、适应性及瓶颈。

Result: 在上述应用领域，3DGS在渲染质量、结构还原精度及优化速度等方面比NeRF表现更优，实现了高效三维重建并满足多领域需求。

Conclusion: 3DGS凭借其光照真实感、几何精度和计算效率上的平衡，逐步取代NeRF成为主流，实现不只是图像合成，还能更好服务于三维感知、交互与内容创建等多种场景，具有广阔前景。

Abstract: Neural scene representations such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have transformed how 3D environments are modeled,
rendered, and interpreted. NeRF introduced view-consistent photorealism via
volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient
alternative that supports high-quality rendering, faster optimization, and
integration into hybrid pipelines for enhanced photorealism and task-driven
scene understanding. This survey examines how 3DGS is being adopted across
SLAM, telepresence and teleoperation, robotic manipulation, and 3D content
generation. Despite their differences, these domains share common goals:
photorealistic rendering, meaningful 3D structure, and accurate downstream
tasks. We organize the review around unified research questions that explain
why 3DGS is increasingly displacing NeRF-based approaches: What technical
advantages drive its adoption? How does it adapt to different input modalities
and domain-specific constraints? What limitations remain? By systematically
comparing domain-specific pipelines, we show that 3DGS balances photorealism,
geometric fidelity, and computational efficiency. The survey offers a roadmap
for leveraging neural rendering not only for image synthesis but also for
perception, interaction, and content creation across real and virtual
environments.

</details>


### [105] [Pancreas Part Segmentation under Federated Learning Paradigm](https://arxiv.org/abs/2509.23562)
*Ziliang Hong,Halil Ertugrul Aktas,Andrea Mia Bejar,Katherine Wu,Hongyi Pan,Gorkem Durak,Zheyuan Zhang,Sait Kayali,Temel Tirkes,Federica Proietto Salanitri,Concetto Spampinato,Michael Goggins,Tamas Gonda,Candice Bolan,Raj Keswani,Frank Miller,Michael Wallace,Ulas Bagci*

Main category: cs.CV

TL;DR: 该论文提出了首个用于MRI胰腺分区（头、体、尾）分割的联邦学习方法，证明了其在多中心异构数据上可行且有效。


<details>
  <summary>Details</summary>
Motivation: 准确将胰腺分为头、体、尾对诊断和治疗至关重要，但MRI图像的变异性、组织对比度差与数据稀缺性使得精确分割十分困难。

Method: 采用保护隐私的联邦学习框架，联合七家医学机构，在不共享原始数据情况下训练模型。系统评估了三种主流分割结构（U-Net、Attention U-Net、Swin UNETR）与两种联邦学习算法（FedAvg、FedProx）的组合，并提出基于解剖结构的损失函数，强化MRI区域特征。

Result: 综合实验结果证明，在面向711例T1W和726例T2W多中心MRI异构数据集上，该方法实现了临床可用的胰腺头/体/尾分割性能。Attention U-Net结合FedAvg表现最佳。

Conclusion: 所提联邦学习框架在保持数据隐私的前提下，实现了高效的胰腺分区分割，为实际临床应用和多中心医学AI协作提供了新途径。

Abstract: We present the first federated learning (FL) approach for pancreas part(head,
body and tail) segmentation in MRI, addressing a critical clinical challenge as
a significant innovation. Pancreatic diseases exhibit marked regional
heterogeneity cancers predominantly occur in the head region while chronic
pancreatitis causes tissue loss in the tail, making accurate segmentation of
the organ into head, body, and tail regions essential for precise diagnosis and
treatment planning. This segmentation task remains exceptionally challenging in
MRI due to variable morphology, poor soft-tissue contrast, and anatomical
variations across patients. Our novel contribution tackles two fundamental
challenges: first, the technical complexity of pancreas part delineation in
MRI, and second the data scarcity problem that has hindered prior approaches.
We introduce a privacy-preserving FL framework that enables collaborative model
training across seven medical institutions without direct data sharing,
leveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key
innovations include: (1) a systematic evaluation of three state-of-the-art
segmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two
FL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as
optimal for pancreatic heterogeneity, which was never been done before; (2) a
novel anatomically-informed loss function prioritizing region-specific texture
contrasts in MRI. Comprehensive evaluation demonstrates that our approach
achieves clinically viable performance despite training on distributed,
heterogeneous datasets.

</details>


### [106] [Towards Interpretable Visual Decoding with Attention to Brain Representations](https://arxiv.org/abs/2509.23566)
*Pinyuan Feng,Hossein Adeli,Wenxuan Guo,Fan Cheng,Ethan Hwang,Nikolaus Kriegeskorte*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉解码框架NeuroAdapter，可直接利用大脑活动重建复杂视觉刺激，且无需依赖中间特征空间。该方法不仅重建效果与现有方法相当，还提高了解释透明度，同时贡献了一个新颖的双向可解释性分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重建方法通常将脑信号映射到中间的图像或文本特征空间，这掩盖了不同脑区对最终重建结果的具体贡献。作者希望绕过这一过程，实现端到端的可解释性视觉重建，更好地理解大脑对现实场景的编码方式。

Method: 作者提出NeuroAdapter框架，可直接将大脑表征作为条件输入到潜在扩散模型，从而跳过中间特征空间。他们还设计了IBBI（Image-Brain BI-directional interpretability）框架，通过分析扩散去噪过程中的跨注意力机制，揭示不同皮层区域对生成过程的影响。

Result: 在公开fMRI数据集上，NeuroAdapter的视觉重建质量与以往方法相当，但提供了更高的可解释性。此外，IBBI框架展示了脑信号如何具体影响生成轨迹。

Conclusion: NeuroAdapter实现了具有竞争力的端到端脑-图像解码方法，同时强化了可解释性分析，为通过视觉神经科学视角解释扩散模型开辟了新道路。

Abstract: Recent work has demonstrated that complex visual stimuli can be decoded from
human brain activity using deep generative models, helping brain science
researchers interpret how the brain represents real-world scenes. However, most
current approaches leverage mapping brain signals into intermediate image or
text feature spaces before guiding the generative process, masking the effect
of contributions from different brain areas on the final reconstruction output.
In this work, we propose NeuroAdapter, a visual decoding framework that
directly conditions a latent diffusion model on brain representations,
bypassing the need for intermediate feature spaces. Our method demonstrates
competitive visual reconstruction quality on public fMRI datasets compared to
prior work, while providing greater transparency into how brain signals shape
the generation process. To this end, we contribute an Image-Brain
BI-directional interpretability framework (IBBI) which investigates
cross-attention mechanisms across diffusion denoising steps to reveal how
different cortical areas influence the unfolding generative trajectory. Our
results highlight the potential of end-to-end brain-to-image decoding and
establish a path toward interpreting diffusion models through the lens of
visual neuroscience.

</details>


### [107] [RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization](https://arxiv.org/abs/2509.23582)
*Kaicheng Yang,Xun Zhang,Haotong Qin,Yucheng Lin,Kaisen Yang,Xianglong Yan,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一套针对Diffusion Transformers（DiTs）量化感知训练（QAT）的方法，使其在低比特量化下依然能高效生成图像。


<details>
  <summary>Details</summary>
Motivation: DiTs在图像生成上优于U-Net，但其高算力和内存需求限制实际应用。QAT能降低U-Net算力消耗，但直接应用于DiT存在瓶颈，尤其是激活值的量化问题。

Method: 1. 首先建立三值权重量化（W1.58A4）DiT基线。2. 提出RobustQuantizer，结合Hadamard变换来强化激活量化的鲁棒性。3. 提出首个激活量化混合精度（AMPN）方案，为不同层分配不同激活精度，避免信息瓶颈，整个网络权重采用三值量化。

Result: 在非条件和条件图像生成任务的大量实验中，RobuQ在亚4比特量化配置下实现了目前DiT量化的最优性能，首度实现在大规模数据集ImageNet-1K上平均激活量化至2比特仍保持稳定且有竞争力的生成效果。

Conclusion: RobuQ实现了DiT在极低比特量化下性能与稳定性的突破，为DiT实际部署和能效优化奠定基础。代码与模型将开源，为后续研究提供支持。

Abstract: Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for image generation, demonstrating superior scalability and performance over
U-Net architectures. However, their practical deployment is hindered by
substantial computational and memory costs. While Quantization-Aware Training
(QAT) has shown promise for U-Nets, its application to DiTs faces unique
challenges, primarily due to the sensitivity and distributional complexity of
activations. In this work, we identify activation quantization as the primary
bottleneck for pushing DiTs to extremely low-bit settings. To address this, we
propose a systematic QAT framework for DiTs, named RobuQ. We start by
establishing a strong ternary weight (W1.58A4) DiT baseline. Building upon
this, we propose RobustQuantizer to achieve robust activation quantization. Our
theoretical analyses show that the Hadamard transform can convert unknown
per-token distributions into per-token normal distributions, providing a strong
foundation for this method. Furthermore, we propose AMPN, the first
Activation-only Mixed-Precision Network pipeline for DiTs. This method applies
ternary weights across the entire network while allocating different activation
precisions to each layer to eliminate information bottlenecks. Through
extensive experiments on unconditional and conditional image generation, our
RobuQ framework achieves state-of-the-art performance for DiT quantization in
sub-4-bit quantization configuration. To the best of our knowledge, RobuQ is
the first achieving stable and competitive image generation on large datasets
like ImageNet-1K with activations quantized to average 2 bits. The code and
models will be available at https://github.com/racoonykc/RobuQ .

</details>


### [108] [VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement](https://arxiv.org/abs/2509.23584)
*Shulian Zhang,Yong Guo,Long Peng,Ziyang Wang,Ye Chen,Wenbo Li,Xiao Zhang,Yulun Zhang,Jian Chen*

Main category: cs.CV

TL;DR: 提出了VividFace，一种高效的一步扩散网络，实现了视频人脸增强，提升了图像质量、身份保持和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸增强方法面临三大困难：1. 保持细腻人脸纹理且保证时序一致性难；2. 缺乏高质量训练数据，泛化性不足；3. 推理过程多步去噪，效率低。

Method: 构建在预训练WANX视频生成模型上的VividFace，通过单步流匹配直接将低质量输入映射为高质量输出。提出Joint Latent-Pixel Face-Focused Training策略，在训练阶段在像素和潜在空间对人脸局部和全局交替优化，并引入MLLM驱动的数据筛选流程，自动选取高质量人脸视频数据，提高泛化能力。

Result: 实验显示VividFace在感知质量、身份保持和时序稳定性方面达到SOTA（最优），提升了效率并有利于社区应用。

Conclusion: VividFace为视频人脸增强提供了一种高效、泛化性强且表现优异的新方案，同时推动了相关领域的数据建设和方法进步。

Abstract: Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions
from degraded video sequences, a capability that underpins numerous
applications including video conferencing, film restoration, and surveillance.
Despite substantial progress in the field, current methods that primarily rely
on video super-resolution and generative frameworks continue to face three
fundamental challenges: (1) faithfully modeling intricate facial textures while
preserving temporal consistency; (2) restricted model generalization due to the
lack of high-quality face video training data; and (3) low efficiency caused by
repeated denoising steps during inference. To address these challenges, we
propose VividFace, a novel and efficient one-step diffusion framework for video
face enhancement. Built upon the pretrained WANX video generation model, our
method leverages powerful spatiotemporal priors through a single-step flow
matching paradigm, enabling direct mapping from degraded inputs to high-quality
outputs with significantly reduced inference time. To further boost efficiency,
we propose a Joint Latent-Pixel Face-Focused Training strategy that employs
stochastic switching between facial region optimization and global
reconstruction, providing explicit supervision in both latent and pixel spaces
through a progressive two-stage training process. Additionally, we introduce an
MLLM-driven data curation pipeline for automated selection of high-quality
video face datasets, enhancing model generalization. Extensive experiments
demonstrate that VividFace achieves state-of-the-art results in perceptual
quality, identity preservation, and temporal stability, while offering
practical resources for the research community.

</details>


### [109] [From Static to Dynamic: a Survey of Topology-Aware Perception in Autonomous Driving](https://arxiv.org/abs/2509.23641)
*Yixiao Chen,Ruining Yang,Xin Chen,Jia He,Dongliang Xu,Yue Yao*

Main category: cs.CV

TL;DR: 论文综述了自动驾驶中与拓扑感知相关的四大研究方向，强调了从静态地图到动态、传感器驱动感知的范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶依赖的静态地图建设成本高、实时更新难、泛化能力有限，因此需要探索更动态、智能的环境感知与理解方式。

Method: 论文系统梳理了矢量化地图构建、拓扑结构建模、先验知识融合及基于语言模型的感知四大方向，并分析它们如何共同推动从静态到动态感知的转变。

Result: 四大方向分别在空间建模、语义推理、知识整合和多模态理解上带来突破，以实时传感器数据推动更灵活的场景感知。

Conclusion: 这些研究进展为实现更自适应、更具扩展性和可解释性的自动驾驶系统奠定了基础，标志着自动驾驶感知正迈向更高层次的智能化。

Abstract: The key to achieving autonomous driving lies in topology-aware perception,
the structured understanding of the driving environment with an emphasis on
lane topology and road semantics. This survey systematically reviews four core
research directions under this theme: vectorized map construction, topological
structure modeling, prior knowledge fusion, and language model-based
perception. Across these directions, we observe a unifying trend: a paradigm
shift from static, pre-built maps to dynamic, sensor-driven perception.
Specifically, traditional static maps have provided semantic context for
autonomous systems. However, they are costly to construct, difficult to update
in real time, and lack generalization across regions, limiting their
scalability. In contrast, dynamic representations leverage on-board sensor data
for real-time map construction and topology reasoning. Each of the four
research directions contributes to this shift through compact spatial modeling,
semantic relational reasoning, robust domain knowledge integration, and
multimodal scene understanding powered by pre-trained language models.
Together, they pave the way for more adaptive, scalable, and explainable
autonomous driving systems.

</details>


### [110] [Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR](https://arxiv.org/abs/2509.23596)
*Chenxi Zhao,Daochang Wang,Siqian Zhang,Gangyao Kuang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于前向散射中心模型(FSCM)的多层次异构知识迁移(MHKT)网络，实现更纯净有效的SAR目标识别知识迁移。通过引入任务相关信息选择器、最大判别散度和类别关系一致性约束，提高了知识迁移质量，在新构建的两套数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达(SAR)目标识别样本有限，现有方法多依赖仿真图像，但存在大量无关信息(如背景、噪声)，影响迁移效果，有效迁移关键目标知识仍是一大挑战。

Method: 作者提出用具有强物理意义和可解释性的FSCM数据迁移关键目标知识，设计了MHKT网络，包括：1) 任务相关信息选择器分离无效特征，2) 目标泛化知识迁移模块引入最大判别散度以兼顾可迁移性和区分性，3) 类别关系知识迁移模块利用类别一致性约束，缓解数据失衡引起的优化偏移。

Result: 在由FSCM数据和实测SAR图像组成的两个新数据集上，所提方法实验显示性能优越，优于现有方法，有效提升了知识迁移的质量和目标识别准确性。

Conclusion: 本文提出的基于FSCM和MHKT网络的知识迁移方法，能更纯净高效地迁移目标知识，显著提升有限样本条件下的SAR目标识别能力，验证了方案的物理合理性及实际效果。

Abstract: Simulated data-assisted SAR target recognition methods are the research
hotspot currently, devoted to solving the problem of limited samples. Existing
works revolve around simulated images, but the large amount of irrelevant
information embedded in the images, such as background, noise, etc., seriously
affects the quality of the migrated information. Our work explores a new
simulated data to migrate purer and key target knowledge, i.e., forward
scattering center model (FSCM) which models the actual local structure of the
target with strong physical meaning and interpretability. To achieve this
purpose, multi-level heterogeneous knowledge transfer (MHKT) network is
proposed, which fully migrates FSCM knowledge from the feature, distribution
and category levels, respectively. Specifically, we permit the more suitable
feature representations for the heterogeneous data and separate non-informative
knowledge by task-associated information selector (TAIS), to complete purer
target feature migration. In the distribution alignment, the new metric
function maximum discrimination divergence (MDD) in target generic knowledge
transfer (TGKT) module perceives transferable knowledge efficiently while
preserving discriminative structure about classes. Moreover, category relation
knowledge transfer (CRKT) module leverages the category relation consistency
constraint to break the dilemma of optimization bias towards simulation data
due to imbalance between simulated and measured data. Such stepwise knowledge
selection and migration will ensure the integrity of the migrated FSCM
knowledge. Notably, extensive experiments on two new datasets formed by FSCM
data and measured SAR images demonstrate the superior performance of our
method.

</details>


### [111] [Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices](https://arxiv.org/abs/2509.23647)
*Xingjian Yang,Ashis G. Banerjee*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于边缘设备的高效统一6D姿态估计框架，结合了鲁棒初始估计和快速跟踪，在光照变化下对未知物体表现优良。


<details>
  <summary>Details</summary>
Motivation: 在不理想光照条件下，对未知物体进行6D姿态估计是计算机视觉中的难题，尤其在边缘计算场景中需兼顾准确性与实时性。

Method: 方法上，提出了基于光照不变的颜色对特征表示，用于初始姿态估计（通过RGB-D和3D模型配准）和运动跟踪（验证时间一致性并回归物体运动），实现了特征与模型的统一高效利用。

Result: 在公开基准数据集上，实验结果显示该方法能够在姿态估计的准确性与跟踪的实时性之间取得良好平衡，且在姿态突变时表现稳定。

Conclusion: 整体框架在复杂光照和大幅运动的情况下展现出较强的鲁棒性和高效性，适用于边缘计算设备。

Abstract: Robust 6D pose estimation of novel objects under challenging illumination
remains a significant challenge, often requiring a trade-off between accurate
initial pose estimation and efficient real-time tracking. We present a unified
framework explicitly designed for efficient execution on edge devices, which
synergizes a robust initial estimation module with a fast motion-based tracker.
The key to our approach is a shared, lighting-invariant color-pair feature
representation that forms a consistent foundation for both stages. For initial
estimation, this feature facilitates robust registration between the live RGB-D
view and the object's 3D mesh. For tracking, the same feature logic validates
temporal correspondences, enabling a lightweight model to reliably regress the
object's motion. Extensive experiments on benchmark datasets demonstrate that
our integrated approach is both effective and robust, providing competitive
pose estimation accuracy while maintaining high-fidelity tracking even through
abrupt pose changes.

</details>


### [112] [VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration](https://arxiv.org/abs/2509.23601)
*Han Hu,Zhuoran Zheng,Liang Li,Chen Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种名为VAMamba的新型图像复原方法，通过引入自适应特征融合和扫描机制，有效提升了复原质量与计算效率，在多项任务上刷新了表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Mamba图像复原方法受限于固定的扫描模式和低效的特征利用，难以针对多样化退化情况自适应调整，导致复原效果和效率受限。

Method: 提出VAMamba架构，包含两项创新：（1）QCLAM模块：通过FIFO缓存历史特征，并根据当前与缓存特征的相似性智能融合，动态重用特征且有效控制内存增长；（2）GPS-SS2D模块：采用视觉Transformer生成像素重要性评分图，通过贪婪策略自适应确定扫描路径，替代传统固定扫描，实现针对有退化区域的特征提取。两模块结合实现自适应、精准且高效的图像复原。

Result: VAMamba在多种图像复原任务和基准数据集上，显著优于现有方法，无论在复原质量还是效率方面均取得新基准。

Conclusion: VAMamba通过自适应特征缓存与扫描，有效克服了传统Mamba方法的局限性，是一种高效、强大的自适应图像复原框架。

Abstract: Recent Mamba-based image restoration methods have achieved promising results
but remain
  limited by fixed scanning patterns and inefficient feature utilization.
Conventional Mamba
  architectures rely on predetermined paths that cannot adapt to diverse
degradations, constraining
  both restoration performance and computational efficiency. To overcome these
limitations, we
  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.
First,
  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha
  FIFO cache that stores historical representations. Similarity between current
LoRA-adapted and
  cached features guides intelligent fusion, enabling dynamic reuse while
effectively controlling
  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.
A
  Vision Transformer generates score maps to estimate pixel importance, and a
greedy strategy de termines optimal forward and backward scanning paths. These
learned trajectories replace rigid
  patterns, enabling SS2D to perform targeted feature extraction. The
integration of QCLAM and
  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while
maintaining high
  computational efficiency. Extensive experiments across diverse restoration
tasks demonstrate
  that VAMamba consistently outperforms existing approaches in both restoration
quality and
  efficiency, establishing new benchmarks for adaptive image restoration. Our
code is available
  at https://github.com/WaterHQH/VAMamba.

</details>


### [113] [FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention](https://arxiv.org/abs/2509.23733)
*Hangtian Zhao,Xiang Chen,Yizhe Li,Qianhao Wang,Haibo Lu,Fei Gao*

Main category: cs.CV

TL;DR: 本文提出了FastViDAR框架，能够利用四个鱼眼摄像头输入，生成全方位360°深度图及相关置信度评估。其核心创新包括高效特征融合的分层注意力机制，以及多视角深度投影的融合方法。结果在多种数据集和嵌入式硬件上表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前多摄像头全景深度估计系统存在特征融合效率低和部署难度大的问题。本文希望提高特征融合效率并简化360°深度图生成流程，使系统更适应实际应用，尤其是在嵌入式终端。

Method: 提出Alternative Hierarchical Attention（AHA）机制，将自注意力分为帧内和帧间两阶段，以高效实现跨视角特征混合。提出了一种将深度估计投影到公共等矩形坐标系的ERP融合方法用于输出最终深度图。同时构建了新的评测数据集。

Result: 在HM3D和2D3D-S数据集上评测显示，该方法在真实数据上实现了有竞争力的零样本深度估计精度，并在嵌入式硬件（NVIDIA Orin NX）上达到20 FPS实时性能。

Conclusion: FastViDAR框架兼顾了高精度和高效率，尤其适合于硬件资源受限的实时全景深度感知场景，为多摄像头视觉系统的发展提供了有力支撑。

Abstract: In this paper we propose FastViDAR, a novel framework that takes four fisheye
camera inputs and produces a full $360^\circ$ depth map along with per-camera
depth, fusion depth, and confidence estimates. Our main contributions are: (1)
We introduce Alternative Hierarchical Attention (AHA) mechanism that
efficiently fuses features across views through separate intra-frame and
inter-frame windowed self-attention, achieving cross-view feature mixing with
reduced overhead. (2) We propose a novel ERP fusion approach that projects
multi-view depth estimates to a shared equirectangular coordinate system to
obtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D
and 2D3D-S datasets for comprehensive evaluation, demonstrating competitive
zero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA
Orin NX embedded hardware. Project page:
\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}

</details>


### [114] [Deep Taxonomic Networks for Unsupervised Hierarchical Prototype Discovery](https://arxiv.org/abs/2509.23602)
*Zekun Wang,Ethan Haarer,Zhiyi Dai,Tianyi Zhu,Christopher J. MacLellan*

Main category: cs.CV

TL;DR: 本文提出了一种深度变分层次聚类方法（deep taxonomic networks），能够自动从无标签数据中学习出多层次、可解释的层次原型结构，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度层次聚类方法通常依赖类别数量预设，并未充分利用不同层级的原型信息，难以自动发现丰富的层次化结构。

Method: 作者提出了一种基于深度潜变量的结构，将先验建模为完全二叉树结构的高斯混合，通过变分推断优化，使模型在没有真实标签信息的情况下自动学习层级结构和多层原型。该方法最大化ELBO，理论和实证上鼓励原型层级关系的自动发现。

Result: 在多种图像分类数据集上，该方法以新颖评价机制在各层次聚类性能优于多种主流基线方法，并能获得更丰富和可解释的层次聚类结果。

Conclusion: 所提出的deep taxonomic networks能有效挖掘无标签数据中的多层次、语义丰富的层次结构，具备较强的实际应用和可解释性价值。

Abstract: Inspired by the human ability to learn and organize knowledge into
hierarchical taxonomies with prototypes, this paper addresses key limitations
in current deep hierarchical clustering methods. Existing methods often tie the
structure to the number of classes and underutilize the rich prototype
information available at intermediate hierarchical levels. We introduce deep
taxonomic networks, a novel deep latent variable approach designed to bridge
these gaps. Our method optimizes a large latent taxonomic hierarchy,
specifically a complete binary tree structured mixture-of-Gaussian prior within
a variational inference framework, to automatically discover taxonomic
structures and associated prototype clusters directly from unlabeled data
without assuming true label sizes. We analytically show that optimizing the
ELBO of our method encourages the discovery of hierarchical relationships among
prototypes. Empirically, our learned models demonstrate strong hierarchical
clustering performance, outperforming baselines across diverse image
classification datasets using our novel evaluation mechanism that leverages
prototype clusters discovered at all hierarchical levels. Qualitative results
further reveal that deep taxonomic networks discover rich and interpretable
hierarchical taxonomies, capturing both coarse-grained semantic categories and
fine-grained visual distinctions.

</details>


### [115] [GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State](https://arxiv.org/abs/2509.23737)
*Guole Shen,Tianchen Deng,Yanbo Wang,Yongtao Chen,Yilin Shen,Jiuming Liu,Jingchuan Wang*

Main category: cs.CV

TL;DR: 本文介绍了一种新型端到端SLAM系统GRS-SLAM3R，实现了无需先验知识与相机参数的RGB图像密集场景重建和位姿估计，并兼顾全球一致性与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有DUSt3R-based的SLAM方法大多只用图像对预测点云，忽视了时序记忆与全局一致性，影响了重建精度和稳定性。本研究旨在弥补这一不足。

Method: 提出GRS-SLAM3R框架，采用顺序输入，实现全局坐标下的累积点云重建。引入空间记忆的潜在状态，并用Transformer结构的门控模块维持和更新3D空间信息；将场景分割为子图进行局部对齐，并利用相对约束将所有子图对齐到统一世界坐标，实现全球一致性。

Result: 在多个数据集上实验表明，本文方法在重建精度上优于现有方法，同时保持实时性能。

Conclusion: GRS-SLAM3R显著提升了端到端密集场景重建的一致性与精度，为无先验RGB SLAM提供了更优选择。

Abstract: DUSt3R-based end-to-end scene reconstruction has recently shown promising
results in dense visual SLAM. However, most existing methods only use image
pairs to estimate pointmaps, overlooking spatial memory and global
consistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework
for dense scene reconstruction and pose estimation from RGB images without any
prior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based
frameworks, which operate on all image pairs and predict per-pair point maps in
local coordinate frames, our method supports sequentialized input and
incrementally estimates metric-scale point clouds in the global coordinate. In
order to improve consistent spatial correlation, we use a latent state for
spatial memory and design a transformer-based gated update module to reset and
update the spatial memory that continuously aggregates and tracks relevant 3D
information across frames. Furthermore, we partition the scene into submaps,
apply local alignment within each submap, and register all submaps into a
common world frame using relative constraints, producing a globally consistent
map. Experiments on various datasets show that our framework achieves superior
reconstruction accuracy while maintaining real-time performance.

</details>


### [116] [MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising](https://arxiv.org/abs/2509.23603)
*Tangtangfang Fang,Jingxi Hu,Xiangjian He,Jiaqi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MAN的多阶段抗噪声网络，在潜在空间通过扩散模型高效去噪低剂量CT图像，在显著提升速度的同时获得高质量成像，克服了现有扩散模型计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在低剂量CT（LDCT）去噪中效果突出，但推理时间极长（每张扫描需几千秒），严重影响临床应用，亟需兼顾高效与高质量的去噪方法。

Method: 提出MAN方法，基于感知优化的自编码器在压缩潜在空间中运行，结合注意力机制和条件U-Net，实现快速、确定性的条件扩散去噪过程，大大减少推理计算量。

Result: 在LDCT和Projection数据集上，MAN模型的感知质量优于CNN/GAN方法，与优秀但计算量大的DDPM、Dn-Dp扩散模型在重建质量上可比。同时，推理速度提升60倍以上，PSNR/SSIM分数保持竞争力。

Conclusion: MAN方法兼顾高保真成像与实际临床应用的可行性，为先进生成式模型在医学影像领域的落地应用提供了新方向。

Abstract: While diffusion models have set a new benchmark for quality in Low-Dose
Computed Tomography (LDCT) denoising, their clinical adoption is critically
hindered by extreme computational costs, with inference times often exceeding
thousands of seconds per scan. To overcome this barrier, we introduce MAN, a
Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and
High-Quality Low-Dose CT Image Denoising task. Our method operates in a
compressed latent space via a perceptually-optimized autoencoder, enabling an
attention-based conditional U-Net to perform the fast, deterministic
conditional denoising diffusion process with drastically reduced overhead. On
the LDCT and Projection dataset, our model achieves superior perceptual
quality, surpassing CNN/GAN-based methods while rivaling the reconstruction
fidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most
critically, in the inference stage, our model is over 60x faster than
representative pixel space diffusion denoisers, while remaining competitive on
PSNR/SSIM scores. By bridging the gap between high fidelity and clinical
viability, our work demonstrates a practical path forward for advanced
generative models in medical imaging.

</details>


### [117] [DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation](https://arxiv.org/abs/2509.23922)
*Haibao Yu,Wenxian Yang,Ruiyang Hao,Chuanye Wang,Jiaru Zhong,Ping Luo,Zaiqing Nie*

Main category: cs.CV

TL;DR: 本论文提出了一个更贴近真实世界的CARLA闭环自动驾驶评测框架，通过整合真实交通场景和数字孪生技术，提升了模拟评测的真实性和挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CARLA的自动驾驶闭环评测大多依赖人工设定场景，与现实情况差距较大，难以准确反映实际驾驶性能。作者希望通过引入真实世界的交通数据和环境，再现更加逼真的驾驶评测场景。

Method: 从高位基础设施传感器采集的100小时视频数据中提取800个动态交通场景，并为15个真实路口创建具有高度一致视觉效果的数字孪生静态资产。将这些真实路口及其场景集成到CARLA模拟器中，实现与真实环境高度贴合的自动驾驶测试。

Result: 成功在CARLA中复现了具有丰富多样行为、地理、天气、时段变化的复杂城市交叉路口交通场景。构建了更加真实且具挑战性的自动驾驶模型闭环评测基准。

Conclusion: 该方法有效提升了闭环自动驾驶评测的真实性和代表性，有助于更准确地评估端到端自动驾驶模型的实际性能。

Abstract: Closed-loop evaluation is increasingly critical for end-to-end autonomous
driving. Current closed-loop benchmarks using the CARLA simulator rely on
manually configured traffic scenarios, which can diverge from real-world
conditions, limiting their ability to reflect actual driving performance. To
address these limitations, we introduce a simple yet challenging closed-loop
evaluation framework that closely integrates real-world driving scenarios into
the CARLA simulator with infrastructure cooperation. Our approach involves
extracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour
video dataset captured by high-mounted infrastructure sensors, and creating
static digital twin assets for 15 real-world intersections with consistent
visual appearance. These digital twins accurately replicate the traffic and
environmental characteristics of their real-world counterparts, enabling more
realistic simulations in CARLA. This evaluation is challenging due to the
diversity of driving behaviors, locations, weather conditions, and times of day
at complex urban intersections. In addition, we provide a comprehensive
closed-loop benchmark for evaluating end-to-end autonomous driving models.
Project URL:
\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.

</details>


### [118] [VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis](https://arxiv.org/abs/2509.23605)
*Zeren Xiong,Yue Yu,Zedong Zhang,Shuo Chen,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出了一种名为Visual Mixing Diffusion (VMDiff) 的新方法，用于将多源视觉线索融合为单一连贯图像，在视觉质量和创造力上超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 多源图像融合生成新图像是图像到图像生成领域中的基础难题，现有方法存在对象未融合（共存生成）或语义偏置问题，限制了艺术创作和虚拟现实等应用。

Method: 设计了基于扩散模型的VMDiff框架，在噪声和潜在空间层面融合两张输入图像。包括：(1)混合采样过程（引导去噪、反演和球面插值结合并可调），实现结构感知融合，缓解共存生成问题；(2)高效自适应调整模块，引入基于相似性的评分，自动调整参数，减轻语义偏置。

Result: 在包含780对概念的扩展基准上，VMDiff在视觉质量、语义一致性和人类评定的创造力方面均优于强基线方法。

Conclusion: VMDiff能更有效融合多源视觉信息，生成连贯、具创意且语义一致的新图像，为艺术创作、虚拟现实等领域提供了有力工具。

Abstract: Creating novel images by fusing visual cues from multiple sources is a
fundamental yet underexplored problem in image-to-image generation, with broad
applications in artistic creation, virtual reality and visual media. Existing
methods often face two key challenges: coexistent generation, where multiple
objects are simply juxtaposed without true integration, and bias generation,
where one object dominates the output due to semantic imbalance. To address
these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet
effective diffusion-based framework that synthesizes a single, coherent object
by integrating two input images at both noise and latent levels. Our approach
comprises: (1) a hybrid sampling process that combines guided denoising,
inversion, and spherical interpolation with adjustable parameters to achieve
structure-aware fusion, mitigating coexistent generation; and (2) an efficient
adaptive adjustment module, which introduces a novel similarity-based score to
automatically and adaptively search for optimal parameters, countering semantic
bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that
our method outperforms strong baselines in visual quality, semantic
consistency, and human-rated creativity.

</details>


### [119] [Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.23993)
*Muleilan Pei,Shaoshuai Shi,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文针对自动驾驶场景下多智能体交通行为的真实模拟问题，提出了SMART-R1，一种基于R1范式的强化学习微调方法，通过混合有监督与强化微调策略，有效提升模拟器与真实世界场景的一致性，并在主流数据集和挑战中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据的多智能体交通行为模拟器主要依赖于有监督学习，但在实际测试环境中常因分布偏移导致泛化能力差，难以真实还原人类驾驶行为。因此，亟需一种能更好适应分布变化、提升模拟结果逼真度的新方法。

Method: 提出SMART-R1训练框架，融合了R1范式的强化学习微调方式，并创新性地引入基于评价指标的策略优化方法。具体采用“SFT-RFT-SFT”（有监督微调-强化微调-有监督微调）交替迭代训练，以提升模型行为与人类驾驶的对齐度。

Result: 在大规模Waymo Open Motion Dataset (WOMD)上进行了多项实验，结果表明SMART-R1框架有效提升了模型逼真度。在Waymo Open Sim Agents Challenge (WOSAC) 上以0.7858的整体真实度Meta分数排名榜首，达到了当前最优水平。

Conclusion: SMART-R1方法能显著缓解训练与测试分布间的偏移问题，通过创新的强化微调机制，提升了模拟模型在未见场景下的泛化能力和真实度，为自动驾驶技术中的多智能体仿真提供了更可靠和实用的工具。

Abstract: Scalable and realistic simulation of multi-agent traffic behavior is critical
for advancing autonomous driving technologies. Although existing data-driven
simulators have made significant strides in this domain, they predominantly
rely on supervised learning to align simulated distributions with real-world
driving scenarios. A persistent challenge, however, lies in the distributional
shift that arises between training and testing, which often undermines model
generalization in unseen environments. To address this limitation, we propose
SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for
next-token prediction models to better align agent behavior with human
preferences and evaluation metrics. Our approach introduces a metric-oriented
policy optimization algorithm to improve distribution alignment and an
iterative "SFT-RFT-SFT" training strategy that alternates between Supervised
Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance
gains. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) validate the effectiveness of this simple yet powerful R1-style training
framework in enhancing foundation models. The results on the Waymo Open Sim
Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art
performance with an overall realism meta score of 0.7858, ranking first on the
leaderboard at the time of submission.

</details>


### [120] [FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching](https://arxiv.org/abs/2509.23608)
*Liubing Hu,Chen Wu,Anrui Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出FlowLUT模型，通过结合多组3D LUTs、内容感知融合权重和新颖的流匹配结构，兼顾了图像增强任务中的效率与表现力。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像增强通常需在高效计算与强大表达能力间权衡，传统3D LUT高效却表现力有限。

Method: FlowLUT模型采用多个带不同先验的可微分3D LUTs，将输入图像在颜色空间中转换，并用轻量级网络预测内容相关的融合权重以自适应修正色彩。另外，引入迭代流匹配机制增强细节和去除伪影，并设计了综合损失函数联合训练模型。

Result: 在三个公开基准上的实验表明，FlowLUT显著优于传统3D LUT和相关方法，兼具效率与效果。

Conclusion: FlowLUT有效突破了传统LUT的表示力瓶颈，实现了高效、灵活且高质量的图像增强，适合实际应用场景。

Abstract: Deep learning-based image enhancement methods face a fundamental trade-off
between computational efficiency and representational capacity. For example,
although a conventional three-dimensional Look-Up Table (3D LUT) can process a
degraded image in real time, it lacks representational flexibility and depends
solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel
end-to-end model that integrates the efficiency of LUTs, multiple priors, and
the parameter-independent characteristic of flow-matched reconstructed images.
Specifically, firstly, the input image is transformed in color space by a
collection of differentiable 3D LUTs (containing a large number of 3D LUTs with
different priors). Subsequently, a lightweight content-aware dynamically
predicts fusion weights, enabling scene-adaptive color correction with
$\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs
on multiple 3D LUTs, with $\mathcal{O}(1)$ complexity for scene-adaptive color
correction.Furthermore, to address the inherent representation limitations of
LUTs, we design an innovative iterative flow matching method to restore local
structural details and eliminate artifacts. Finally, the entire model is
jointly optimized under a composite loss function enforcing perceptual and
structural fidelity. Extensive experimental results demonstrate the
effectiveness of our method on three benchmarks.

</details>


### [121] [Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO Platform](https://arxiv.org/abs/2509.24001)
*Matej Palider,Omar Eldardeer,Viktor Kocur*

Main category: cs.CV

TL;DR: 本文在HRI（人机交互）共享工作空间情境下，评估了四种主流凝视估计方法，并基于NICO机器人平台引入了新的标注数据集。评测结果显示，虽然角度误差与通用基准相近，但在实际空间中，最佳中位误差也有16.48厘米，反映出现有方法的实际局限。论文最后讨论了这些局限，并提出凝视估计在HRI系统中整合的建议。


<details>
  <summary>Details</summary>
Motivation: 目前凝视估计方法虽在标准数据集表现良好，但在真实人机协作空间中的实际可用性和局限并未充分探索，因此有必要进行应用层面的评估。

Method: 1. 使用NICO机器人收集并标注新的凝视数据集；2. 选取四种最先进的凝视估计模型进行评测；3. 对比模型在通用基准与实际共享空间场景中的表现。

Result: 在HRI共享空间中，最佳模型的凝视估计算法中位误差为16.48厘米，虽与标准基准的角度误差接近，但空间精度有限。

Conclusion: 当前凝视估计方法在实际HRI场景下仍有较大精度提升空间。研究提出了整合建议，以促进行为感知在机器人系统中的实际应用。

Abstract: This paper evaluates the current gaze estimation methods within an HRI
context of a shared workspace scenario. We introduce a new, annotated dataset
collected with the NICO robotic platform. We evaluate four state-of-the-art
gaze estimation models. The evaluation shows that the angular errors are close
to those reported on general-purpose benchmarks. However, when expressed in
terms of distance in the shared workspace the best median error is 16.48 cm
quantifying the practical limitations of current methods. We conclude by
discussing these limitations and offering recommendations on how to best
integrate gaze estimation as a modality in HRI systems.

</details>


### [122] [InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects](https://arxiv.org/abs/2509.23612)
*Xinhao Cai,Minghang Zheng,Xin Jin,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出一个全新任务：在包含可移动物体的3D场景中，根据文本生成受控的人体与物体交互，并建立了新的数据集与方法，实现了更丰富、更真实的交互生成。


<details>
  <summary>Details</summary>
Motivation: 现有的人体-场景交互数据集种类有限，且多关注与静态物体的交互，缺乏对可移动物体的支持，而构建此类数据集成本较高。因此，亟需一个能支持文本控制与可移动对象交互的三维数据集和解决方案。

Method: （1）构建InteractMove数据集：通过对已有交互数据与场景上下文的对齐，生成包含多可移动物体和丰富交互类型的新数据集；（2）提出新的三维视觉定位管线，首先确定交互目标物体，然后通过手-物联合affordance学习预测具体接触区域，最后引入局部场景建模与碰撞约束优化交互轨迹，确保运动物理合理无碰撞。

Result: 实验证明，所提方法在物理可行性与文本符合度均优于现有3D交互生成方法，能更真实、准确地生成复杂场景中的人体-物体交互。

Conclusion: 本文首次提出面向可移动物体、文本可控的人物-物体互动生成任务及数据集，并设计了有效的自动化生成管线，推动了三维人-物互动领域的发展。

Abstract: We propose a novel task of text-controlled human object interaction
generation in 3D scenes with movable objects. Existing human-scene interaction
datasets suffer from insufficient interaction categories and typically only
consider interactions with static objects (do not change object positions), and
the collection of such datasets with movable objects is difficult and costly.
To address this problem, we construct the InteractMove dataset for Movable
Human-Object Interaction in 3D Scenes by aligning existing human object
interaction data with scene contexts, featuring three key characteristics: 1)
scenes containing multiple movable objects with text-controlled interaction
specifications (including same-category distractors requiring spatial and 3D
scene context understanding), 2) diverse object types and sizes with varied
interaction patterns (one-hand, two-hand, etc.), and 3) physically plausible
object manipulation trajectories. With the introduction of various movable
objects, this task becomes more challenging, as the model needs to identify
objects to be interacted with accurately, learn to interact with objects of
different sizes and categories, and avoid collisions between movable objects
and the scene. To tackle such challenges, we propose a novel pipeline solution.
We first use 3D visual grounding models to identify the interaction object.
Then, we propose a hand-object joint affordance learning to predict contact
regions for different hand joints and object parts, enabling accurate grasping
and manipulation of diverse objects. Finally, we optimize interactions with
local-scene modeling and collision avoidance constraints, ensuring physically
plausible motions and avoiding collisions between objects and the scene.
Comprehensive experiments demonstrate our method's superiority in generating
physically plausible, text-compliant interactions compared to existing
approaches.

</details>


### [123] [FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation](https://arxiv.org/abs/2509.24241)
*Seungwook Kim,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: 该论文提出了两种无需训练、在推理时可用的新方法，使动作参数在扩散模型生成机器人视频时被更有效地利用，提高了动作连贯性和视频质量。


<details>
  <summary>Details</summary>
Motivation: 目前基于扩散模型的视频生成中，动作信息常作为被动条件输入，这导致合成机器人视频时对动作控制效果有限，因此需要探索更充分发挥显式动作轨迹的方法。

Method: 作者提出：(1) 动作缩放的无分类器引导方法，根据动作大小动态调整引导强度，增加对运动强度的控制；(2) 动作缩放的噪声截断方法，在扩散模型初始噪声采样时调整其分布，使其更符合期望运动动态。这两种方法均在推理阶段无须额外训练。

Result: 在实际机器人操作数据集上实验显示，两种方法能显著提升生成视频中动作的连贯性与视觉质量，适用于多种机器人环境。

Conclusion: 结合显式动作参数的推理时方法能够更好地利用扩散模型进行机器人视频生成，提升控制力与样本质量，为机器人世界模型和基础模型研究带来进步。

Abstract: Generating realistic robot videos from explicit action trajectories is a
critical step toward building effective world models and robotics foundation
models. We introduce two training-free, inference-time techniques that fully
exploit explicit action parameters in diffusion-based robot video generation.
Instead of treating action vectors as passive conditioning signals, our methods
actively incorporate them to guide both the classifier-free guidance process
and the initialization of Gaussian latents. First, action-scaled
classifier-free guidance dynamically modulates guidance strength in proportion
to action magnitude, enhancing controllability over motion intensity. Second,
action-scaled noise truncation adjusts the distribution of initially sampled
noise to better align with the desired motion dynamics. Experiments on real
robot manipulation datasets demonstrate that these techniques significantly
improve action coherence and visual quality across diverse robot environments.

</details>


### [124] [BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images](https://arxiv.org/abs/2509.23617)
*Cheng Huang,Weizheng Xie,Fan Gao,Yutong Liu,Ruoling Wu,Zeyu Han,Jingxi Qiu,Xiangxiang Wang,Zhenglin Yang,Hao Wang,Yongbin Yu*

Main category: cs.CV

TL;DR: 本文提出了一种无监督视网膜血管分割方法BioVessel-Net，结合了血管生物统计、对抗细化和基于半径的分割策略，无需标签即可实现高精度血管提取，并发布了高分辨率数据集RetinaMix。该方法在新老数据集上均大幅超越现有有监督与半监督方法。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管结构变化是青光眼等眼病的重要生物标志，而现有分割方法依赖大量人工标注，成本高且易出错，尤其在OCTA图像中难以获得标注数据。因此亟需无监督、高效和可解释的分割新方法。

Method: 提出BioVessel-Net框架，无需人工标签，结合血管生物统计建模、对抗学习精细化和半径引导的分割策略，实现血管结构的高效建模与分割。同时，发布多样化高分辨率2D/3D OCTA图像数据集RetinaMix用于评测。

Result: 在RetinaMix和现有公开数据集上，BioVessel-Net实现了接近完美的分割准确率，显著优于最先进的有监督和半监督方法。

Conclusion: BioVessel-Net和RetinaMix为视网膜血管分析提供了无需标注、高计算效率、临床可解释的新方案，对青光眼监测、血流建模和疾病预测具有广阔应用前景。

Abstract: Structural changes in retinal blood vessels are critical biomarkers for the
onset and progression of glaucoma and other ocular diseases. However, current
vessel segmentation approaches largely rely on supervised learning and
extensive manual annotations, which are costly, error-prone, and difficult to
obtain in optical coherence tomography angiography. Here we present
BioVessel-Net, an unsupervised generative framework that integrates vessel
biostatistics with adversarial refinement and a radius-guided segmentation
strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular
structures with biostatistical coherence, achieving accurate and explainable
vessel extraction without labeled data or high-performance computing. To
support training and evaluation, we introduce RetinaMix, a new benchmark
dataset of 2D and 3D OCTA images with high-resolution vessel details from
diverse populations. Experimental results demonstrate that BioVessel-Net
achieves near-perfect segmentation accuracy across RetinaMix and existing
datasets, substantially outperforming state-of-the-art supervised and
semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a
label-free, computationally efficient, and clinically interpretable solution
for retinal vessel analysis, with broad potential for glaucoma monitoring,
blood flow modeling, and progression prediction. Code and dataset are
available: https://github.com/VikiXie/SatMar8.

</details>


### [125] [SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics](https://arxiv.org/abs/2509.24572)
*Peter Hönig,Stefan Thalhammer,Jean-Baptiste Weibel,Matthias Hirschmanner,Markus Vincze*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的类别级物体位姿估计方法SCOPE，通过利用DINOv2特征作为连续语义先验，实现无需离散类别标签的物体位姿估计，提升了对未知类别的泛化能力和仿真到现实的性能。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，机器人对未知物体进行操作时，需要语义理解以泛化至已知和未知类别。然而，当前方案在类别泛化、对离散标签的依赖以及仿真到现实迁移等方面存在挑战。本文提出新方法以解决上述问题。

Method: SCOPE结合DINOv2的连续特征语义先验、逼真训练数据及点法向噪声模型，缩小仿真到现实的差距。通过跨注意力注入语义信息，使模型在类别分布之外学习到标准化的物体坐标系统。

Result: SCOPE在基于合成数据训练的类别级物体位姿估计任务上表现优于现有方法，在5°5cm指标上相对提升31.9%。在两个实例级数据集上的额外实验表明其对未知类别具备良好泛化能力，对未知类别物体抓取成功率可达100%。

Conclusion: SCOPE消除了对离散类别标签的需求，提升了类别级物体位姿估计和泛化能力，为机器人在开放环境中操作未知物体提供新手段。

Abstract: Object manipulation requires accurate object pose estimation. In open
environments, robots encounter unknown objects, which requires semantic
understanding in order to generalize both to known categories and beyond. To
resolve this challenge, we present SCOPE, a diffusion-based category-level
object pose estimation model that eliminates the need for discrete category
labels by leveraging DINOv2 features as continuous semantic priors. By
combining these DINOv2 features with photorealistic training data and a noise
model for point normals, we reduce the Sim2Real gap in category-level object
pose estimation. Furthermore, injecting the continuous semantic priors via
cross-attention enables SCOPE to learn canonicalized object coordinate systems
across object instances beyond the distribution of known categories. SCOPE
outperforms the current state of the art in synthetically trained
category-level object pose estimation, achieving a relative improvement of
31.9\% on the 5$^\circ$5cm metric. Additional experiments on two instance-level
datasets demonstrate generalization beyond known object categories, enabling
grasping of unseen objects from unknown categories with a success rate of up to
100\%. Code available: https://github.com/hoenigpeter/scope.

</details>


### [126] [DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation](https://arxiv.org/abs/2509.23624)
*Wei Pan,Huiguo He,Hiuyi Cheng,Yilin Shi,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出DiffInk，一个基于潜变量扩散与Transformer结构的端到端文字行级在线手写体生成模型，在准确性与风格一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到手写生成方法多聚焦于单字符或单词级生成，效率低下且难以建模整行文字的整体结构。作者希望解决文本行级生成的效率和结构完整性问题。

Method: 首先提出InkVAE，一种带有两种正则化损失（OCR损失保证字形准确、风格分类损失保持书写风格）的序列变分自编码器，实现字符内容和书写风格的解耦。然后提出InkDiT，一种潜变量扩散Transformer，融合目标文本与参考风格，生成连贯的笔迹轨迹。

Result: DiffInk在笔迹字形准确性和风格保持性上超过了当前最优方法，并大幅提高了生成效率。

Conclusion: DiffInk拓展了端到端文字行级手写体生成的能力，在文本内容与风格保真的前提下，实现更快更准确的生成，为在线手写体生成任务设立新基准。

Abstract: Deep generative models have advanced text-to-online handwriting generation
(TOHG), which aims to synthesize realistic pen trajectories conditioned on
textual input and style references. However, most existing methods still
primarily focus on character- or word-level generation, resulting in
inefficiency and a lack of holistic structural modeling when applied to full
text lines. To address these issues, we propose DiffInk, the first latent
diffusion Transformer framework for full-line handwriting generation. We first
introduce InkVAE, a novel sequential variational autoencoder enhanced with two
complementary latent-space regularization losses: (1) an OCR-based loss
enforcing glyph-level accuracy, and (2) a style-classification loss preserving
writing style. This dual regularization yields a semantically structured latent
space where character content and writer styles are effectively disentangled.
We then introduce InkDiT, a novel latent diffusion Transformer that integrates
target text and reference styles to generate coherent pen trajectories.
Experimental results demonstrate that DiffInk outperforms existing
state-of-the-art methods in both glyph accuracy and style fidelity, while
significantly improving generation efficiency. Code will be made publicly
available.

</details>


### [127] [Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic Environments](https://arxiv.org/abs/2509.24731)
*Luis F. W. Batista,Tom Bourbon,Cedric Pradalier*

Main category: cs.CV

TL;DR: 本文通过极化成像技术提升了水面漂浮物（如塑料瓶）分割的准确性，尤其在存在水面反光和光照变化的情况下，提出了一个新的极化图像数据集（PoTATO）并做了系统性性能对比。


<details>
  <summary>Details</summary>
Motivation: 在室外水域对漂浮垃圾进行目标分割时，水面反光和光照变化会显著降低分割效果。现有方法对反光干扰鲁棒性不足，需要探索新的成像与建模手段改善分割性能。

Method: 利用极化成像技术，通过PoTATO数据集（含有水域极化图像和相关标签），采用主流特征融合神经网络进行分割基准测试，并与标准RGB单图像方法做对比分析。

Result: 极化信息有助于还原低对比度目标及抑制反光带来的误检，极化方法下分割掩膜轮廓更清晰，平均IoU提升、轮廓误差下降。但引入极化通道后，模型规模增大，计算负担上升，且存在一定概率的新型误检。

Conclusion: 极化成像在水面漂浮物分割任务具有明显优势，也存在计算和误检的权衡。相关基准数据与代码公开有助于学术社区评估和采纳极化传感器方案，加速领域研究进展。

Abstract: Accurate segmentation of floating debris on water is often compromised by
surface glare and changing outdoor illumination. Polarimetric imaging offers a
single-sensor route to mitigate water-surface glare that disrupts semantic
segmentation of floating objects. We benchmark state-of-the-art fusion networks
on PoTATO, a public dataset of polarimetric images of plastic bottles in inland
waterways, and compare their performance with single-image baselines using
traditional models. Our results indicate that polarimetric cues help recover
low-contrast objects and suppress reflection-induced false positives, raising
mean IoU and lowering contour error relative to RGB inputs. These sharper masks
come at a cost: the additional channels enlarge the models increasing the
computational load and introducing the risk of new false positives. By
providing a reproducible, diagnostic benchmark and publicly available code, we
hope to help researchers choose if polarized cameras are suitable for their
applications and to accelerate related research.

</details>


### [128] [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/abs/2509.23625)
*YuQian Li,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的递归内省掩码扩散视觉语言模型（RIV），通过内省训练和递归推理机制，使模型具备自我纠错能力，并在多个基准任务上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码扩散的视觉语言模型虽然在多模态理解任务取得进展，但对生成内容存在错误时，缺乏发现并自行纠正这些错误（尤其是逻辑错误）的能力。为解决该短板，提升模型生成质量，作者提出了新的机制。

Method: 1）引入内省训练，将内省模型用于检测生成序列中的各类错误（包括语法、拼写及逻辑错误）；2）递归推理，从传统unmask步骤出发，模型通过内省检测再掩码错误输出，反复迭代unmask-检测-掩码过程，直到输出可靠结果。

Result: RIV模型在多个公共多模态基准数据集上表现优异，取得了比绝大多数现有MDVLMs更好的效果。

Conclusion: 通过引入内省模块和递归纠错机制，RIV大幅提升了生成内容的正确性和可靠性，推动了多模态视觉语言模型的进一步发展。

Abstract: Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable
progress in multimodal understanding tasks. However, these models are unable to
correct errors in generated tokens, meaning they lack self-correction
capability. In this paper, we propose Recursive Introspection Mask Diffusion
Vision Language Model (RIV), which equips the model with self-correction
ability through two novel mechanisms. The first is Introspection Training,
where an Introspection Model is introduced to identify errors within generated
sequences. Introspection Training enables the model to detect not only
grammatical and spelling mistakes, but more importantly, logical errors. The
second is Recursive Inference. Beginning with the standard unmasking step, the
learned Introspection Model helps to identify errors in the output sequence and
remask them. This alternating
($\text{unmask}\rightarrow\text{introspection}\rightarrow\text{remask}$)
process is repeated recursively until reliable results are obtained.
Experimental results on multiple benchmarks demonstrate that the proposed RIV
achieves state-of-the-art performance, outperforming most existing MDVLMs.

</details>


### [129] [ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation](https://arxiv.org/abs/2509.24878)
*Jiuhong Xiao,Roshan Nayak,Ning Zhang,Daniel Tortei,Giuseppe Loianno*

Main category: cs.CV

TL;DR: 本文提出了一种用于RGB到热成像图像转换的生成模型ThermalGen，并构建了大规模数据集支持训练。实验显示该方法优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: RGB-热成像配对数据稀缺，限制了多模态图像融合与检索等任务的发展。通过从RGB图像合成热成像图像，可缓解配对数据不足问题。

Method: 提出了一种基于自适应流的生成模型ThermalGen，该模型结合了RGB图像条件架构和风格解耦机制。作者还整理并新建了多个大规模RGB-热成像数据集来支持训练与评测。

Result: ThermalGen在多个RGB-热成像任务基准上，取得了与甚至优于现有GAN和扩散模型方法的表现。该方法能力涵盖不同视角、传感器和环境条件下的热成像图像合成。

Conclusion: ThermalGen是首个能在多样化场景下高质量合成热成像图像的RGB-热成像转换生成模型，有望推动多模态图像融合等实际应用发展。

Abstract: Paired RGB-thermal data is crucial for visual-thermal sensor fusion and
cross-modality tasks, including important applications such as multi-modal
image alignment and retrieval. However, the scarcity of synchronized and
calibrated RGB-thermal image pairs presents a major obstacle to progress in
these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image
translation has emerged as a promising solution, enabling the synthesis of
thermal images from abundant RGB datasets for training purposes. In this study,
we propose ThermalGen, an adaptive flow-based generative model for RGB-T image
translation, incorporating an RGB image conditioning architecture and a
style-disentangled mechanism. To support large-scale training, we curated eight
public satellite-aerial, aerial, and ground RGB-T paired datasets, and
introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day,
Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor
types, and geographic regions. Extensive evaluations across multiple RGB-T
benchmarks demonstrate that ThermalGen achieves comparable or superior
translation performance compared to existing GAN-based and diffusion-based
methods. To our knowledge, ThermalGen is the first RGB-T image translation
model capable of synthesizing thermal images that reflect significant
variations in viewpoints, sensor characteristics, and environmental conditions.
Project page: http://xjh19971.github.io/ThermalGen

</details>


### [130] [Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models](https://arxiv.org/abs/2509.23626)
*Beomseok Kang,Niluthpol Chowdhury Mithun,Mikhail Sizintsev,Han-Pang Chiu,Supun Samarasekera*

Main category: cs.CV

TL;DR: FAMDA是一种基于自训练和视觉基础模型的新型无监督领域自适应（UDA）多任务密集预测方法，专用于解决机器人语义分割和深度估计等任务中的领域迁移问题，表现出比现有方法更优异的效果，并且高效轻量。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测对机器人等实际应用场景至关重要，但模型在新环境（如从合成到真实、昼夜变化）部署时常因领域差异造成性能下降。虽然UDA方法能缓解单任务的领域迁移问题，但应用于多任务时，现有方法（如对抗训练）效果有限。近期自训练方法更有前景，因此需要新的方法将其优势带入多任务UDA。

Method: 提出FAMDA，将视觉基础模型（如分割和深度模型）作为教师，在自训练框架下为目标域生成高质量伪标签，通过蒸馏方式提升学生模型的泛化能力和效率。FAMDA无需对抗学习，整合基础模型优势，最终实现端到端训练。

Result: 在标准合成到真实和白天到夜晚的多任务UDA基准测试上，FAMDA取得了最新最优（SOTA）表现。轻量版能在体积大幅缩小的情况下仍保持SOTA准确率，远小于基础模型体积。

Conclusion: FAMDA极大提升了多任务UDA的性能和效率，能够高效部署于资源受限的机器人等实际应用场景，为领域自适应和模型轻量化提供了有效解决方案。

Abstract: Multi-task dense prediction, which aims to jointly solve tasks like semantic
segmentation and depth estimation, is crucial for robotics applications but
suffers from domain shift when deploying models in new environments. While
unsupervised domain adaptation (UDA) addresses this challenge for single tasks,
existing multi-task UDA methods primarily rely on adversarial learning
approaches that are less effective than recent self-training techniques. In
this paper, we introduce FAMDA, a simple yet effective UDA framework that
bridges this gap by leveraging Vision Foundation Models (VFMs) as powerful
teachers. Our approach integrates Segmentation and Depth foundation models into
a self-training paradigm to generate high-quality pseudo-labels for the target
domain, effectively distilling their robust generalization capabilities into a
single, efficient student network. Extensive experiments show that FAMDA
achieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA
multi-task learning (MTL) benchmarks and a challenging new day-to-night
adaptation task. Our framework enables the training of highly efficient models;
a lightweight variant achieves SOTA accuracy while being more than 10$\times$
smaller than foundation models, highlighting FAMDA's suitability for creating
domain-adaptive and efficient models for resource-constrained robotics
applications.

</details>


### [131] [Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events](https://arxiv.org/abs/2509.25146)
*Richeek Das,Kostas Daniilidis,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 本文提出了一种用于事件相机数据的高效表示方法 F^3，并展示其在多种下游视觉任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机生成的数据稀疏且动态，传统像素级表示方法难以充分利用其特点，缺乏高效、鲁棒的数据表示形式。作者希望开发一种新型表征方法，能高效处理事件相机数据并提升下游任务表现。

Method: 作者提出了 Fast Feature Field（F^3）表示，通过利用多分辨率哈希编码和深度集合思想，高效地将连续时空内的事件编码成多通道图像。该方法通过以过去事件预测未来事件为训练目标，学习到能同时保持场景结构和运动信息的稀疏鲁棒表征。

Result: F^3 可在 HD 分辨率下实现 120Hz、在 VGA 分辨率下实现 440Hz 的高效运算。该方法在三种机器人平台（汽车、四足机器人、无人机）上、不同环境（室内外、城市场景、越野）、不同光照条件及传感器下，在光流、语义分割和单目深度估计等任务上达到当前最佳性能（SOTA）。

Conclusion: F^3 是一种高效、鲁棒且具有优越下游任务表现的事件相机数据新型表示方法，为事件视觉感知提供了强大支持，适用于多种场景和设备。

Abstract: This paper develops a mathematical argument and algorithms for building
representations of data from event-based cameras, that we call Fast Feature
Field ($\text{F}^3$). We learn this representation by predicting future events
from past events and show that it preserves scene structure and motion
information. $\text{F}^3$ exploits the sparsity of event data and is robust to
noise and variations in event rates. It can be computed efficiently using ideas
from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and
440 Hz at VGA resolutions. $\text{F}^3$ represents events within a contiguous
spatiotemporal volume as a multi-channel image, enabling a range of downstream
tasks. We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates). Our implementations can predict these tasks at
25-75 Hz at HD resolution.

</details>


### [132] [MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing](https://arxiv.org/abs/2509.23635)
*Ruibing Hou,Mingshuang Luo,Hongyu Pan,Hong Chang,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出了MotionVerse，这是一个利用大语言模型（LLM）来理解、生成和编辑单人及多人动作的统一框架。通过高效的动作数据离散化和创新的并行建模策略，实现了高效与高性能兼得。


<details>
  <summary>Details</summary>
Motivation: 当前动作数据的理解和生成依赖于复杂的数据表征及多模态信息融合，但现有方法效率与效果难以兼得。本研究致力于解决动作与语言多模态融合时的干扰，并提升动作表征效率和多任务性能。

Method: 1. 采用残差量化的动作tokenizer，将连续动作转为多流离散tokens。
2. 提出“延迟并行”建模策略，分步编码残差token流，提升流间依赖建模效率。
3. 设计“双塔结构”，针对动作和语言采用独立参数，降低模态干扰，实现稳定融合。

Result: 消融实验验证了各组件的有效性，并通过大量任务展示该方法在动作理解、生成和编辑任务中的优越性能。

Conclusion: MotionVerse有效解决了多模态融合中的干扰问题，在保证效率的同时，显著提升了动作相关任务的表现，为动作生成和编辑等领域提供了强有力的工具。

Abstract: This paper proposes MotionVerse, a unified framework that harnesses the
capabilities of Large Language Models (LLMs) to comprehend, generate, and edit
human motion in both single-person and multi-person scenarios. To efficiently
represent motion data, we employ a motion tokenizer with residual quantization,
which converts continuous motion sequences into multi-stream discrete tokens.
Furthermore, we introduce a \textit{Delay Parallel} Modeling strategy, which
temporally staggers the encoding of residual token streams. This design enables
LLMs to effectively capture inter-stream dependencies while maintaining
computational efficiency comparable to single-stream modeling. Moreover, to
alleviate modality interference between motion and language, we design a
\textit{dual-tower architecture} with modality-specific parameters, ensuring
stable integration of motion information for both comprehension and generation
tasks. Comprehensive ablation studies demonstrate the effectiveness of each
component in MotionVerse, and extensive experiments showcase its superior
performance across a wide range of motion-relevant tasks.

</details>


### [133] [LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders](https://arxiv.org/abs/2509.23639)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Kangli Zi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的轻量化方法LightFair，通过调整文本编码器来提升文本到图像扩散模型（T2I DMs）的公平性。该方法只需微调文本嵌入，无需全模型重训练或辅助网络，极大降低了训练和采样负担，并取得了当前最佳去偏效果。


<details>
  <summary>Details</summary>
Motivation: 当前T2I扩散模型常因文本编码器产生偏差，影响生成图像的公平性。已有方法要么需要对整个模型重训练，要么需引入额外网络，导致训练和推理代价高且效果一般。作者期望以更高效、简单的方式提升模型公平性。

Method: 本文聚焦于文本编码器，通过分析其嵌入输出的偏斜现象，提出一种协同距离约束去偏方法（collaborative distance-constrained debiasing），只对文本嵌入参数微调以消除偏差。同时，为避免去偏带来图像质量下降，提出两阶段采样策略，只在特定阶段启用去偏文本编码器。

Result: 实验表明，该方法无需辅助网络，在Stable Diffusion v1.5模型上，仅用1/4训练量就达到了当前最优的去偏效果，基本不增加采样时间。

Conclusion: 通过对文本编码器微调嵌入和创新采样策略，LightFair方法能够高效提升T2I扩散模型的公平性，兼顾效率和生成质量，实用性强。

Abstract: This paper explores a novel lightweight approach LightFair to achieve fair
text-to-image diffusion models (T2I DMs) by addressing the adverse effects of
the text encoder. Most existing methods either couple different parts of the
diffusion model for full-parameter training or rely on auxiliary networks for
correction. They incur heavy training or sampling burden and unsatisfactory
performance. Since T2I DMs consist of multiple components, with the text
encoder being the most fine-tunable and front-end module, this paper focuses on
mitigating bias by fine-tuning text embeddings. To validate feasibility, we
observe that the text encoder's neutral embedding output shows substantial
skewness across image embeddings of various attributes in the CLIP space. More
importantly, the noise prediction network further amplifies this imbalance. To
finetune the text embedding, we propose a collaborative distance-constrained
debiasing strategy that balances embedding distances to improve fairness
without auxiliary references. However, mitigating bias can compromise the
original generation quality. To address this, we introduce a two-stage
text-guided sampling strategy to limit when the debiased text encoder
intervenes. Extensive experiments demonstrate that LightFair is effective and
efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA
debiasing at just $1/4$ of the training burden, with virtually no increase in
sampling burden. The code is available at https://github.com/boyuh/LightFair.

</details>


### [134] [EfficientMIL: Efficient Linear-Complexity MIL Method for WSI Classification](https://arxiv.org/abs/2509.23640)
*Chengying She,Ben Wang,Xinran Zhang,Dongjie Fan,Jialu Zhang,Chengwei Chen,Lizhuang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的WSI（全切片图像）分类方法EfficientMIL，利用自适应Patch选择模块（APS）和线性复杂度序列模型，极大提升了计算效率，同时在多个病理数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的MIL方法依赖Transformer自注意力机制，虽性能优异但计算量大，难以处理包含数十万Patch的WSI。该工作旨在解决计算瓶颈，提高MIL方法的效率和实用性。

Method: 提出EfficientMIL，在MIL结构中用自适应Patch选择器（APS）替代传统自注意力机制，采用GRU、LSTM和新型Mamba状态空间模型等线性复杂度序列模型，显著降低计算成本，同时提升Patch选择效果。

Result: 在TCGA-Lung和CAMELYON16等公开病理数据集上，EfficientMIL的不同变体（GRU、Mamba）均取得了优于现有方法的表现，如AUC最高达到0.990，准确率达0.975，并大幅提升计算效率。

Conclusion: EfficientMIL不仅突破了SOTA方法的计算瓶颈，还实现了更高的分类准确率与AUC分数，所提APS对于WSI中Patch的选择也更为高效，为病理图像快速自动分析提供了新方案。

Abstract: Whole slide images (WSIs) classification represents a fundamental challenge
in computational pathology, where multiple instance learning (MIL) has emerged
as the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on
attention mechanisms, achieving good performance but requiring substantial
computational resources due to quadratic complexity when processing hundreds of
thousands of patches. To address this computational bottleneck, we introduce
EfficientMIL, a novel linear-complexity MIL approach for WSIs classification
with the patches selection module Adaptive Patch Selector (APS) that we
designed, replacing the quadratic-complexity self-attention mechanisms in
Transformer-based MIL methods with efficient sequence models including
RNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves
significant computational efficiency improvements while outperforming other MIL
methods across multiple histopathology datasets. On TCGA-Lung dataset,
EfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on
CAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of
0.975, surpassing previous state-of-the-art methods. Extensive experiments
demonstrate that APS is also more effective for patches selection than
conventional selection strategies.

</details>


### [135] [Griffin: Generative Reference and Layout Guided Image Composition](https://arxiv.org/abs/2509.23643)
*Aryan Mikaeili,Amirhossein Alimohammadi,Negar Hassanpour,Ali Mahdavi-Amiri,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新方法，通过图像而非文本来指定多图像布局控制，实现对生成图像中内容及其位置的精确引导。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型虽然可以生成逼真的图像，但文本控制方式在需要更明确指导时存在局限性，特别是在需要精确控制内容与布局的场景下。

Method: 本方法通过输入参考图像（每种元素仅需一张），而不依赖文本描述，并对模型进行布局引导，显式且简单地实现了对象及其组成部分的控制。整个方案无需另行训练。

Result: 该方法在多种图像合成任务中均表现出良好效果，能够有效实现多元素内容按指定布局组合。

Conclusion: 利用该方法能够为图像生成任务提供更直接、灵活的控制方式，特别适用于需要复杂内容和布局指定的场景。

Abstract: Text-to-image models have achieved a level of realism that enables the
generation of highly convincing images. However, text-based control can be a
limiting factor when more explicit guidance is needed. Defining both the
content and its precise placement within an image is crucial for achieving
finer control. In this work, we address the challenge of multi-image layout
control, where the desired content is specified through images rather than
text, and the model is guided on where to place each element. Our approach is
training-free, requires a single image per reference, and provides explicit and
simple control for object and part-level composition. We demonstrate its
effectiveness across various image composition tasks.

</details>


### [136] [Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures](https://arxiv.org/abs/2509.23646)
*Lu Xiao,Jiale Zhang,Yang Liu,Taicheng Huang,Xin Tian*

Main category: cs.CV

TL;DR: 本文提出Sparse-Up框架，在生成高保真3D资产时，有效解决了高频细节损失和跨视角一致性之间的权衡问题，通过稀疏体素引导与创新上采样策略，大幅节省内存同时保留高细节。


<details>
  <summary>Details</summary>
Motivation: 现有3D资产建模方法在保持跨视角一致性和捕捉高频细节间难以兼顾，常常需要在两者中做权衡，导致纹理撕裂/漂移或细节丢失。

Method: 作者提出Sparse-Up框架，采用稀疏体素引导纹理重建以保障多视角一致性，并引入两大核心策略：1）表面锚定，对体素进行可学习的上采样，仅约束于网格表面，减少冗余体素；2）视图域划分，根据图像patch划分体素，仅对可见patch监督/反向传播。

Result: Sparse-Up利用上述策略，在大幅降低高分辨率体素训练内存消耗的同时，仍能保持几何一致性并有效保留纹理高频细节，比传统方法提升明显。

Conclusion: Sparse-Up为高保真3D纹理建模提供了一种高效、细节丰富且一致性强的解决方案，有望推动高质量3D资产的生产。

Abstract: The creation of high-fidelity 3D assets is often hindered by a 'pixel-level
pain point': the loss of high-frequency details. Existing methods often trade
off one aspect for another: either sacrificing cross-view consistency,
resulting in torn or drifting textures, or remaining trapped by the resolution
ceiling of explicit voxels, forfeiting fine texture detail. In this work, we
propose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework
that effectively preserves high-frequency details. We use sparse voxels to
guide texture reconstruction and ensure multi-view consistency, while
leveraging surface anchoring and view-domain partitioning to break through
resolution constraints. Surface anchoring employs a learnable upsampling
strategy to constrain voxels to the mesh surface, eliminating over 70% of
redundant voxels present in traditional voxel upsampling. View-domain
partitioning introduces an image patch-guided voxel partitioning scheme,
supervising and back-propagating gradients only on visible local patches.
Through these two strategies, we can significantly reduce memory consumption
during high-resolution voxel training without sacrificing geometric
consistency, while preserving high-frequency details in textures.

</details>


### [137] [ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis](https://arxiv.org/abs/2509.23652)
*Congzhi Zhang,Zhibin Wang,Yinchao Ma,Jiawei Peng,Yihan Wang,Qiang Zhou,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 本文针对大型视觉-语言模型（LVLMs）在复杂视频推理任务中的应用挑战，提出并构建了大规模ReWatch数据集，并用新颖的方法提升了视频推理能力，在多个基准测试中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的以可验证奖励为基础的强化学习（RLVR）在图片推理上有进展，但在视频推理领域受限于缺乏高质量、多步推理和视频依赖的链式思考（CoT）数据，导致其应用受阻。

Method: 提出了多阶段合成流程，构建出了包含字幕、问答和链式思考三部分的ReWatch大规模视频推理数据集。创新性地用多智能体ReAct框架，模拟“反复观看”过程，合成视频内容相关的推理轨迹。同时，基于该数据集将强基线LVLM用监督微调和RLVR框架（引入新的观测与推理奖励机制）后训练成ReWatch-R1。该机制不仅考察答案准确性，还直接惩罚无根据的推理。

Result: 在五项具有挑战性的视频推理基准测试上，ReWatch-R1达到了当前最优的平均性能。

Conclusion: 高质量、多步推理的视频数据集是提升模型视频推理能力的关键，通过新型奖励机制和数据合成方式，可以显著推动视频理解LVLM前沿发展。

Abstract: While Reinforcement Learning with Verifiable Reward (RLVR) significantly
advances image reasoning in Large Vision-Language Models (LVLMs), its
application to complex video reasoning remains underdeveloped. This gap stems
primarily from a critical data bottleneck: existing datasets lack the
challenging, multi-hop questions and high-quality, video-grounded
Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address
this, we introduce ReWatch, a large-scale dataset built to foster advanced
video reasoning. We propose a novel multi-stage synthesis pipeline to
synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.
A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which
simulates a human-like "re-watching" process to generate video-grounded
reasoning traces by explicitly modeling information retrieval and verification.
Building on this dataset, we develop ReWatch-R1 by post-training a strong
baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This
framework incorporates a novel Observation \& Reasoning (O\&R) reward mechanism
that evaluates both the final answer's correctness and the reasoning's
alignment with video content, directly penalizing hallucination. Our
experiments show that ReWatch-R1 achieves state-of-the-art average performance
on five challenging video reasoning benchmarks.

</details>


### [138] [LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training](https://arxiv.org/abs/2509.23661)
*Xiang An,Yin Xie,Kaicheng Yang,Wenkang Zhang,Xiuwei Zhao,Zheng Cheng,Yirui Wang,Songcen Xu,Changrui Chen,Chunsheng Wu,Huajie Tan,Chunyuan Li,Jing Yang,Jie Yu,Xiyao Wang,Bin Qin,Yumeng Wang,Zizhen Yan,Ziyong Feng,Ziwei Liu,Bo Li,Jiankang Deng*

Main category: cs.CV

TL;DR: 本论文提出了LLaVA-OneVision-1.5，一个全新的大规模多模态模型家族，在保证SOTA性能的同时大幅降低了训练成本。该方法从头开放、可复现地构建高质量视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型通常训练成本高昂并缺乏可复现实践。本研究旨在打造一种既高效、低成本又表现卓越的开放视觉-语言模型训练框架。

Method: 1) 构建大规模优质数据集：包括8,500万个预训练样本和2,600万个指令样本，共计640亿多模态token。2) 提出高效训练框架：采用离线并行数据压缩与打包策略，使整个训练费用控制在16,000美元内。3) 全面评测：与Qwen2.5-VL系列在27项基准上对比。

Result: LLaVA-OneVision-1.5-8B在27项指标中18项优于Qwen2.5-VL-7B，4B版本在27项指标中全部优于Qwen2.5-VL-3B，展现出极强的性能优势。

Conclusion: LLaVA-OneVision-1.5实现了开放、高效、低成本的多模态大模型训练方案，并在多项下游任务中取得新SOTA。后续还将发布LLaVA-OneVision-1.5-RL。

Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models
(LMMs) that achieve state-of-the-art performance with significantly reduced
computational and financial costs. Different from the existing works,
LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for
building high-quality vision-language models entirely from scratch. The
LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale
Curated Datasets: We construct an 85M concept-balanced pretraining dataset
LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction
dataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed
multimodal tokens. (2) Efficient Training Framework: We develop a complete
end-to-end efficient training framework leveraging an offline parallel data
packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a
$16,000 budget. (3) State-of-the-art Performance: Experimental results
demonstrate that LLaVA-OneVision1.5 yields exceptionally competitive
performance across a broad range of downstream tasks. Specifically,
LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and
LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We
anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community
to await further updates.

</details>


### [139] [HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score](https://arxiv.org/abs/2509.23663)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Peter A. Beerel*

Main category: cs.CV

TL;DR: 论文提出了一种名为HIVTP的免训练层次化视觉token剪枝方法，用于提升视觉-语言模型（VLMs）的推理效率，能显著减少计算成本且不损失甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在多模态任务上表现优异，但其视觉编码器产生大量视觉tokens，导致推理效率低。已有研究表明其中许多token不重要，可被安全剪枝。为此，提升token处理效率成为亟需解决的问题。

Method: HIVTP方法基于视觉编码器中间层注意力图来评估视觉token的重要性，反映细粒度和目标级别注意力。具体地：1）将视觉token序列重塑为空间二维排列；2）“全局保留阶段”按区域筛选高分token；3）“局部保留阶段”在每个小窗口内进一步筛选最重要token，从而分层次地保留全局和局部关键token。整个过程无需模型再训练。

Result: 实验表明，HIVTP方法将LLaVA-v1.5-7B和LLaVA-Next-7B的首token生成时间分别减少50.0%和55.1%，token生成吞吐量提升60.9%和47.3%。同时还能保持甚至提升模型在某些基准上的准确率。

Conclusion: HIVTP在无需额外训练的情况下，实现了比已有方法更高的推理效率和更优的准确率，展现出在视觉-语言模型中的实际应用价值。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities on diverse
multimodal tasks. However, the large number of visual tokens output by the
vision encoder severely hinders inference efficiency, and prior studies have
shown that many of these tokens are not important and can therefore be safely
pruned. In this work, we propose HIVTP, a training-free method to improve VLMs
efficiency via hierarchical visual token pruning using a novel
middle-layer-based importance score. Specifically, we utilize attention maps
extracted from the middle layers of the vision encoder, which better reflect
fine-grained and object-level attention, to estimate visual token importance.
Based on this, we propose a hierarchical visual token pruning method to retain
both globally and locally important visual tokens. Specifically, we reshape the
1-D visual token sequence output by the vision encoder into a 2-D spatial
layout. In the global retaining stage, we divide the image into regions and
retain tokens with higher importance scores in each region; in the local
retaining stage, we then divide the image into small windows and retain the
most important token in each local window. Experimental results show that our
proposed method, HIVTP, can reduce the time-to-first-token (TTFT) of
LLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and
improve the token generation throughput by up to 60.9% and 47.3%, without
sacrificing accuracy, and even achieving improvements on certain benchmarks.
Compared with prior works, HIVTP achieves better accuracy while offering higher
inference efficiency.

</details>


### [140] [Token Merging via Spatiotemporal Information Mining for Surgical Video Understanding](https://arxiv.org/abs/2509.23672)
*Xixi Jiang,Chen Yang,Dong Zhang,Pingcheng Dong,Xin Yang,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种针对手术视频理解的高效Token合并方法（STIM-TM），在极大减少计算量的同时仍保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 当前Vision Transformer用于手术视频理解虽有效，但因需要处理极大量时空Token，计算成本极高。以往Token合并工作未充分考虑视频的时空结构和信息分布异质性，导致性能不理想。

Method: 提出STIM-TM方法，分别沿时间（合并相邻帧空间对应的Token，通过显著性加权）与空间（通过时序稳定性分析优先合并静态Token）独立减少Token冗余。该方法无需额外训练，能够保证时序信息连续性及动态区域信息。

Result: STIM-TM使GFLOPs减少超过65%，且在多个手术视频任务上准确率保持竞争力。可高效训练长序列手术视频，极大缓解了计算瓶颈。

Conclusion: 提出的方法突破了手术视频Transformer的效率瓶颈，有效平衡了计算资源消耗和模型性能，并具有良好的实际应用前景。

Abstract: Vision Transformer models have shown impressive effectiveness in the surgical
video understanding tasks through long-range dependency modeling. However,
current methods suffer from prohibitive computational costs due to processing
massive spatiotemporal tokens across video frames. While prior work on token
merging has advanced model efficiency, they fail to adequately consider the
inherent spatiotemporal structure of video data and overlook the heterogeneous
nature of information distribution, leading to suboptimal performance. In this
paper, we propose a spatiotemporal information mining token merging (STIM-TM)
method, representing the first dedicated approach for surgical video
understanding. STIM-TM introduces a decoupled strategy that reduces token
redundancy along temporal and spatial dimensions independently. Specifically,
the temporal component merges spatially corresponding tokens from consecutive
frames using saliency weighting, preserving critical sequential information and
maintaining continuity. Meanwhile, the spatial component prioritizes merging
static tokens through temporal stability analysis, protecting dynamic regions
containing essential surgical information. Operating in a training-free manner,
STIM-TM achieves significant efficiency gains with over $65\%$ GFLOPs reduction
while preserving competitive accuracy across comprehensive surgical video
tasks. Our method also supports efficient training of long-sequence surgical
videos, addressing computational bottlenecks in surgical applications.

</details>


### [141] [RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks](https://arxiv.org/abs/2509.23673)
*Amit Agarwal,Hitesh Laxmichand Patel,Srikant Panda,Hansa Meghwani,Jyotika Singh,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 提出了一种新指标RCI，用于衡量多模态基准中任务对全局与局部视觉信息的依赖，为数据集和模型发展提供指引。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的评测基准无法区分模型是基于全局图像理解还是仅依赖局部视觉线索，这导致难以针对实际需求进行有效数据集构建和模型优化。

Method: 提出Region Comprehension Index（RCI），通过比较模型在完整图片和局部图像块上的表现，定量评估数据集任务对全局和局部视觉信息的依赖程度。RCI可以揭示模型是否真正具备整体图像推理能力。

Result: 对13个主流多模态基准应用RCI后发现，大多数任务偏向局部推理，存在显著空间偏置，说明在现实应用环境中有潜在风险。

Conclusion: RCI为学界和工业界提供了诊断和改进多模态系统偏差的工具，有助于创建更健壮、适用于实际场景的数据集和基准，推动多模态系统的可靠落地。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.

</details>


### [142] [MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain Segmentation via Multi-scale Self-Distilled Fusion Strategy](https://arxiv.org/abs/2509.23677)
*Dayu Tan,Ziwei Zhang,Yansan Su,Xin Peng,Yike Dai,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: 该文提出了一种新型的3D多模态图像分割框架MSD-KMamba，能在保证分割精度的同时，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 许多CNN-Transformer混合模型为了捕捉长距离依赖，依赖高复杂度的全局注意力机制，导致计算资源消耗大。现有的知识蒸馏和稀疏注意力虽可提升效率，但在复杂分割任务中精度常常不足，因此需要平衡模型性能和计算效率。

Method: MSD-KMamba引入双向空间感知分支以高效获取脑区空间上下文依赖，同时配套非线性特征提取机制以提升模型对复杂异质模式的学习能力。再结合多尺度自蒸馏融合策略，增强多层级特征表达与语义信息传递，从而优化分割表现并降低复杂度。

Result: 在多个标准基准数据集上的广泛实验表明，MSD-KMamba在分割精度、鲁棒性、泛化能力等方面均优于最新方法，同时具备高计算效率和良好可扩展性。

Conclusion: MSD-KMamba在分割精度与效率之间取得了优良平衡，为高效3D多模态医学图像分割提供了有力的新选择。

Abstract: Numerous CNN-Transformer hybrid models rely on high-complexity global
attention mechanisms to capture long-range dependencies, which introduces
non-linear computational complexity and leads to significant resource
consumption. Although knowledge distillation and sparse attention mechanisms
can improve efficiency, they often fall short of delivering the high
segmentation accuracy necessary for complex tasks. Balancing model performance
with computational efficiency remains a critical challenge. In this work, we
propose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba,
which integrates bidirectional spatial perception with multi-scale
self-distillation. The bidirectional spatial aware branch effectively captures
long-range spatial context dependencies across brain regions, while also
incorporating a powerful nonlinear feature extraction mechanism that further
enhances the model's ability to learn complex and heterogeneous patterns. In
addition, the proposed multi-scale self-distilled fusion strategy strengthens
hierarchical feature representations and improves the transfer of semantic
information at different resolution levels. By jointly leveraging the
bidirectional spatial perception branch and the multi-scale self-distilled
fusion strategy, our framework effectively mitigates the bottleneck of
quadratic computational complexity in volumetric segmentation, while
simultaneously addressing the limitation of insufficient global perception.
Extensive experiments on multiple standard benchmark datasets demonstrate that
MSD-KMamba consistently outperforms state-of-the-art methods in segmentation
accuracy, robustness, and generalization, while maintaining high computational
efficiency and favorable scalability. The source code of MSD-KMamba is publicly
available at https://github.com/daimao-zhang/MSD-KMamba.

</details>


### [143] [QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification](https://arxiv.org/abs/2509.23681)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Mingqiang Wu,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 本论文提出QuantSparse框架，通过结合量化和注意力稀疏化，有效压缩扩散Transformer视频生成模型，同时保持高视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer虽在视频生成方面表现优异，但其高昂计算和存储成本限制了实际应用。常规的模型量化和注意力稀疏化单独应用时在高压缩下都会显著降低性能，因此有必要探索两者结合的新方法。

Method: 作者提出QuantSparse框架，将模型量化与注意力稀疏化有机结合。具体创新点包括多尺度显著注意力蒸馏（整合全局结构和局部显著性监督缓解量化偏置）和二阶稀疏注意力重参数化（利用残差的时序稳定性高效恢复因稀疏化而丢失的信息）。

Result: 在HunyuanVideo-13B数据上，QuantSparse方法实现PSNR提升至20.88，较量化SOTA基线Q-VDiT（16.85 PSNR）大幅提升。同时，显著减少存储（3.68倍降幅）和推理时间（1.88倍加速）。

Conclusion: QuantSparse框架有效缓解了量化和稀疏化单独应用带来的质量损失。在保证视频生成质量大幅提升的同时，大幅降低计算和存储，推动了大规模视频生成模型的实际部署前景。

Abstract: Diffusion transformers exhibit remarkable video generation capability, yet
their prohibitive computational and memory costs hinder practical deployment.
Model quantization and attention sparsification are two promising directions
for compression, but each alone suffers severe performance degradation under
aggressive compression. Combining them promises compounded efficiency gains,
but naive integration is ineffective. The sparsity-induced information loss
exacerbates quantization noise, leading to amplified attention shifts. To
address this, we propose \textbf{QuantSparse}, a unified framework that
integrates model quantization with attention sparsification. Specifically, we
introduce \textit{Multi-Scale Salient Attention Distillation}, which leverages
both global structural guidance and local salient supervision to mitigate
quantization-induced bias. In addition, we develop \textit{Second-Order Sparse
Attention Reparameterization}, which exploits the temporal stability of
second-order residuals to efficiently recover information lost under sparsity.
Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88
PSNR, substantially outperforming the state-of-the-art quantization baseline
Q-VDiT (16.85 PSNR), while simultaneously delivering a \textbf{3.68$\times$}
reduction in storage and \textbf{1.88$\times$} acceleration in end-to-end
inference. Our code will be released in
https://github.com/wlfeng0509/QuantSparse.

</details>


### [144] [HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection](https://arxiv.org/abs/2509.23690)
*Siyuan Gao,Jiashu Yao,Haoyu Wen,Yuhang Guo,Zeming Liu,Heyan Huang*

Main category: cs.CV

TL;DR: 本文提出了HomeSafeBench，一种用于评估体感智能体在家庭安全检查任务中能力的新基准，利用动态第一人称视觉信息克服了现有基准的局限，并全面评估了主流视觉-语言模型（VLM）在家庭安全检查任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的家庭安全检查基准主要采用文本描述环境，缺乏真实的视觉信息，而且仅提供静态观察视角，限制了体感智能体的自由探索，导致部分安全隐患被漏检。这些局限阻碍了对基于视觉-语言模型（VLM）的智能体能力的真实评估。

Method: 作者提出了HomeSafeBench，覆盖12,900个数据点，涵盖五种常见家居安全隐患（火灾、电击、坠落物、绊倒、儿童安全），采用模拟家居环境中动态第一人称视角的图片，允许体感智能体自由探索并进行全面检查。

Result: 使用HomeSafeBench对主流VLM进行了系统性评估，结果显示即使表现最好的模型F1分数也仅为10.23%，表明现有VLM在家居安全检查任务中存在明显的局限性，尤其在安全隐患识别和探索策略选择方面表现不足。

Conclusion: HomeSafeBench能够为未来与家居安全检查相关的研究提供有价值的参考和支持。作者将公开数据集和代码，促进该方向的研究。

Abstract: Embodied agents can identify and report safety hazards in the home
environments. Accurately evaluating their capabilities in home safety
inspection tasks is curcial, but existing benchmarks suffer from two key
limitations. First, they oversimplify safety inspection tasks by using textual
descriptions of the environment instead of direct visual information, which
hinders the accurate evaluation of embodied agents based on Vision-Language
Models (VLMs). Second, they use a single, static viewpoint for environmental
observation, which restricts the agents' free exploration and cause the
omission of certain safety hazards, especially those that are occluded from a
fixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a
benchmark with 12,900 data points covering five common home safety hazards:
fire, electric shock, falling object, trips, and child safety. HomeSafeBench
provides dynamic first-person perspective images from simulated home
environments, enabling the evaluation of VLM capabilities for home safety
inspection. By allowing the embodied agents to freely explore the room,
HomeSafeBench provides multiple dynamic perspectives in complex environments
for a more thorough inspection. Our comprehensive evaluation of mainstream VLMs
on HomeSafeBench reveals that even the best-performing model achieves an
F1-score of only 10.23%, demonstrating significant limitations in current VLMs.
The models particularly struggle with identifying safety hazards and selecting
effective exploration strategies. We hope HomeSafeBench will provide valuable
reference and support for future research related to home security inspections.
Our dataset and code will be publicly available soon.

</details>


### [145] [Confidence Aware SSD Ensemble with Weighted Boxes Fusion for Weapon Detection](https://arxiv.org/abs/2509.23697)
*Atharva Jadhav,Arush Karekar,Manas Divekar,Shachi Natu*

Main category: cs.CV

TL;DR: 本文提出通过集成多种特征提取主干网络的SSD模型，并结合Weighted Boxes Fusion方法，提升复杂环境下的武器检测准确性，有效应对遮挡、光照变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前公共空间对安全监控需求强烈，但现有的单一目标检测模型在复杂条件下（如遮挡、光照变化与背景杂乱）表现不稳定，亟需提升检测的鲁棒性和精度。

Method: 论文训练了多种主干网络（VGG16、ResNet50、EfficientNet、MobileNetV3）的SSD模型，并对三类武器（枪械、重型武器、刀具）图像进行检测，最后采用加权框融合（WBF）方法进行集成，提高预测框的精度。

Result: 采用最大置信度策略的WBF集成方法，在mAP指标上达到0.838，较最优单一模型提升2.948%，并优于其他融合策略。

Conclusion: 多主干网络SSD模型的集成结合置信度感知的融合策略，大大提升了武器检测的鲁棒性和准确率，为公共安全监控中的实时武器检测提供了有效解决方案。

Abstract: The safety and security of public spaces is of vital importance, driving the
need for sophisticated surveillance systems capable of accurately detecting
weapons, which are often hampered by issues like partial occlusion, varying
lighting, and cluttered backgrounds. While single-model detectors are advanced,
they often lack robustness in these challenging conditions. This paper presents
the hypothesis that ensemble of Single Shot Multibox Detector (SSD) models with
diverse feature extraction backbones can significantly enhance detection
robustness. To leverage diverse feature representations, individual SSD models
were trained using a selection of backbone networks: VGG16, ResNet50,
EfficientNet, and MobileNetV3. The study is conducted on a dataset consisting
of images of three distinct weapon classes: guns, heavy weapons and knives. The
predictions from these models are combined using the Weighted Boxes Fusion
(WBF) method, an ensemble technique designed to optimize bounding box accuracy.
Our key finding is that the fusion strategy is as critical as the ensemble's
diversity, a WBF approach using a 'max' confidence scoring strategy achieved a
mean Average Precision (mAP) of 0.838. This represents a 2.948% relative
improvement over the best-performing single model and consistently outperforms
other fusion heuristics. This research offers a robust approach to enhancing
real-time weapon detection capabilities in surveillance applications by
demonstrating that confidence-aware fusion is a key mechanism for improving
accuracy metrics of ensembles.

</details>


### [146] [INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception](https://arxiv.org/abs/2509.23700)
*Yunjiang Xu,Lingzhi Li,Jin Wang,Yupeng Ouyang,Benyuan Yang*

Main category: cs.CV

TL;DR: 该论文提出了INSTINCT，一种新颖的协同感知框架，在确保高检测精度的同时极大降低了通信带宽需求。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知系统虽然通过整合多车传感器数据提升了远距离和遮挡场景下的检测效果，但频繁通信与实时性要求受限于带宽。尤其针对LiDAR数据，实例级交互虽能减少带宽，但实际性能仍不理想，需要进一步突破。

Method: 提出INSTINCT架构，包含三个核心：1）基于质量的实例特征筛选机制；2）双分支检测路由解耦与协作无关及相关的实例；3）跨智能体局部实例融合模块。此外，改进GT采样技术以增强多样特征的训练。

Result: 在DAIR-V2X与V2V4Real数据集上，INSTINCT的检测精度分别提升13.23%和33.08%，通信带宽降至1/281和1/264，显著优于现有方法。

Conclusion: INSTINCT有效提高了协同感知的精度与通信效率，为多智能体汽车环境感知提供了更优解决方案。

Abstract: Collaborative perception systems overcome single-vehicle limitations in
long-range detection and occlusion scenarios by integrating multi-agent sensory
data, improving accuracy and safety. However, frequent cooperative interactions
and real-time requirements impose stringent bandwidth constraints. Previous
works proves that query-based instance-level interaction reduces bandwidth
demands and manual priors, however, LiDAR-focused implementations in
collaborative perception remain underdeveloped, with performance still trailing
state-of-the-art approaches. To bridge this gap, we propose INSTINCT
(INSTance-level INteraCtion ArchiTecture), a novel collaborative perception
framework featuring three core components: 1) a quality-aware filtering
mechanism for high-quality instance feature selection; 2) a dual-branch
detection routing scheme to decouple collaboration-irrelevant and
collaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion
module to aggregate local hybrid instance features. Additionally, we enhance
the ground truth (GT) sampling technique to facilitate training with diverse
hybrid instance features. Extensive experiments across multiple datasets
demonstrate that INSTINCT achieves superior performance. Specifically, our
method achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and
V2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared
to state-of-the-art methods. The code is available at
https://github.com/CrazyShout/INSTINCT.

</details>


### [147] [CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement](https://arxiv.org/abs/2509.23708)
*Boseong Jeon,Junghyuk Lee,Jimin Park,Kwanyoung Kim,Jingi Jung,Sangwon Lee,Hyunbo Shim*

Main category: cs.CV

TL;DR: CrimEdit是一种利用分类器无关引导联合处理目标移除和插入及其相关效应（如阴影、反射）的扩散模型，能高效实现对象的移除、控制性插入及移动。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来通过对抗事实数据集训练的扩散模型已能处理对象效应（如阴影、反射），但对于如何在同一模型中利用分类器无关引导同时提升对象移除和插入任务效果的研究仍不足，影响了图像复合编辑的效率和统一性。

Method: 作者提出CrimEdit，将移除和插入作为任务嵌入联合在单模型内训练，并在分类器无关引导框架下处理对象及其效应的去除或插入。该方法还首次允许在空间上区分不同区域以支持对象位移操作，实现一步去噪完成目标移动。

Result: 实验表明，CrimEdit无须额外训练即可高效完成对象及其效应的移除、可控性插入和对象移动，优于现有方法。

Conclusion: CrimEdit实现了统一、高效且灵活的图像对象编辑，推动了对象移除、插入和移动在实际图像编辑任务中的进展。

Abstract: Recent works on object removal and insertion have enhanced their performance
by handling object effects such as shadows and reflections, using diffusion
models trained on counterfactual datasets. However, the performance impact of
applying classifier-free guidance to handle object effects across removal and
insertion tasks within a unified model remains largely unexplored. To address
this gap and improve efficiency in composite editing, we propose CrimEdit,
which jointly trains the task embeddings for removal and insertion within a
single model and leverages them in a classifier-free guidance scheme --
enhancing the removal of both objects and their effects, and enabling
controllable synthesis of object effects during insertion. CrimEdit also
extends these two task prompts to be applied to spatially distinct regions,
enabling object movement (repositioning) within a single denoising step. By
employing both guidance techniques, extensive experiments show that CrimEdit
achieves superior object removal, controllable effect insertion, and efficient
object movement without requiring additional training or separate removal and
insertion stages.

</details>


### [148] [PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease](https://arxiv.org/abs/2509.23719)
*Shuai Shao,Shu Jiang,Shiyuan Zhao,Di Yang,Yan Wang,Yutong Bai,Jianguo Zhang,Jiangtao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PD-Diag-Net的端到端自动化帕金森病（PD）诊断方法，能在原始MRI数据上进行风险评估和辅助诊断，大幅提升了诊断的准确率与早期检测能力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病发病率逐年上升，严重影响患者生活质量。现有的诊断流程依赖神经科专家，流程复杂且易导致早期漏诊，缺乏自动化、高效、客观的诊断手段。研究目标是通过深度学习自动分析MRI影像，简化诊断流程并提高早期检测率。

Method: 提出PD-Diag-Net框架，包括MRI预处理模块（整合多种成像预处理工具，减少数据变异），并引入两类临床先验知识（与PD高相关脑区及其加速老化特性），据此设计了两个功能模块：相关脑区引导特征聚合模块和脑龄差引导诊断模块，分别从群体和个体两个层面对影像特征进行强化。

Result: 该方法在外部测试集上取得了86%的诊断准确率，早期诊断准确率超过96%，比已有先进方法提升了20%以上。

Conclusion: PD-Diag-Net能显著提升帕金森病的自动化辅助诊断水平，尤其是在早期检测方面有突出表现，展示了临床推广的巨大潜力。

Abstract: Parkinson's disease (PD) is a common neurodegenerative disorder that severely
diminishes patients' quality of life. Its global prevalence has increased
markedly in recent decades. Current diagnostic workflows are complex and
heavily reliant on neurologists' expertise, often resulting in delays in early
detection and missed opportunities for timely intervention. To address these
issues, we propose an end-to-end automated diagnostic method for PD, termed
PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly
from raw MRI scans. This framework first introduces an MRI Pre-processing
Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability
by flexibly integrating established medical imaging preprocessing tools. It
then incorporates two forms of clinical prior knowledge: (1)
Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions
strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior),
which reflects the accelerated aging typically observed in PD-associated
regions. Building on these priors, we design two dedicated modules: the
Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides
the model to focus on PD-associated regions at the inter-subject level, and the
Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps
as auxiliary constraints at the intra-subject level to enhance diagnostic
accuracy and clinical interpretability. Furthermore, we collected external test
data from our collaborating hospital. Experimental results show that
PD-Diag-Net achieves 86\% accuracy on external tests and over 96% accuracy in
early-stage diagnosis, outperforming existing advanced methods by more than
20%.

</details>


### [149] [DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion](https://arxiv.org/abs/2509.23723)
*Zijun Li,Hongyu Yan,Shijie Li,Kunming Luo,Li Lu,Xulei Yang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种用于点云补全的分层扩散模型（DiffPCN），分为粗略生成和细致优化两个阶段，有效提升点云补全的完整性和准确性，达到了最新的性能水平。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在低级视觉任务中表现突出，但其在点云补全领域尚未广泛应用，主要由于点云无结构、分布不规则，难以直接建模。

Method: 方法分为两个阶段：首先将部分点云投影为深度图，借助DepthLDM完成多视角深度图生成，再合成粗略点云；其次设计点去噪网络去除异常点，并结合局部关联特征引导点云上采样，获得高密度、高保真结果。

Result: 在点云补全任务中，DiffPCN在几何精度和形状完整性上取得了最先进的效果，显著提升了补全的鲁棒性和一致性。

Conclusion: DiffPCN有效地将扩散模型能力引入点云补全，解决了点云结构无序的问题，实现了高质量的点云重建，为相关扩散建模任务提供了新工具和视角。

Abstract: Latent diffusion models (LDMs) have demonstrated remarkable generative
capabilities across various low-level vision tasks. However, their potential
for point cloud completion remains underexplored due to the unstructured and
irregular nature of point clouds. In this work, we propose DiffPCN, a novel
diffusion-based coarse-to-fine framework for point cloud completion. Our
approach comprises two stages: an initial stage for generating coarse point
clouds, and a refinement stage that improves their quality through point
denoising and upsampling. Specifically, we first project the unordered and
irregular partial point cloud into structured depth images, which serve as
conditions for a well-designed DepthLDM to synthesize completed multi-view
depth images that are used to form coarse point clouds. In this way, our
DiffPCN can yield high-quality and high-completeness coarse point clouds by
leveraging LDM' s powerful generation and comprehension capabilities. Then,
since LDMs inevitably introduce outliers into the generated depth maps, we
design a Point Denoising Network to remove artifacts from the coarse point
cloud by predicting a per-point distance score. Finally, we devise an
Association-Aware Point Upsampler, which guides the upsampling process by
leveraging local association features between the input point cloud and the
corresponding coarse points, further yielding a dense and high-fidelity output.
Experimental results demonstrate that our DiffPCN achieves state-of-the-art
performance in geometric accuracy and shape completeness, significantly
improving the robustness and consistency of point cloud completion.

</details>


### [150] [Video Panels for Long Video Understanding](https://arxiv.org/abs/2509.23724)
*Lars Doorenbos,Federico Spurio,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种针对长视频理解的新型视觉提示策略，不需额外训练或参数修改，即可显著提升现有视频-语言模型（VLM）在长视频理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在处理长视频时的效果远不如在图片或短视频任务上，因此社区普遍关注如何提升其长时上下文建模能力。以往方法大多引入复杂的新模块和额外训练，效率较低。该文欲在无需额外训练或更改模型结构前提下，直接提升现有VLM的长视频理解能力。

Method: 提出将多帧视频合成为一张图片面板的新颖视觉提示策略，让VLM以更高的时间分辨率建模长视频内容，牺牲空间细节以换取更长的时序上下文。该方法无需训练，不涉及新参数，应用灵活，适配各种VLM，并可直接集成。

Result: 在五个视频理解基准任务和多种VLM架构评测，方法稳定有效提升了长视频推理表现。尤其在TimeScope(Long)数据集上，视频问答准确率提升高达19.4%。

Conclusion: 提出的视觉提示方式可广泛提升各类VLM的长视频理解表现，使用简单，适用性强，无需训练成本，为该领域树立新标杆。代码将在论文接受后开源。

Abstract: Recent Video-Language Models (VLMs) achieve promising results on long-video
understanding, but their performance still lags behind that achieved on tasks
involving images or short videos. This has led to great interest in improving
the long context modeling of VLMs by introducing novel modules and additional
complexity. % additional training time. In this paper, we take a different
approach: rather than fine-tuning VLMs with the limited data available, we
attempt to maximize the performance of existing models. To this end, we propose
a novel visual prompting strategy specifically designed for long-video
understanding. By combining multiple frames as panels into one image, we
effectively trade off spatial details for temporal resolution. Our approach is
training-free, parameter-free, and model-agnostic, and can be seamlessly
integrated into existing VLMs. Extensive experiments on five established
benchmarks across a wide range of model architectures, sizes, and context
windows confirm the consistency of our approach. For the TimeScope (Long)
dataset, which has the longest videos, the accuracy for video question
answering is improved by up to 19.4\%. Overall, our method raises the bar for
long video understanding models. We will make our code available upon
acceptance.

</details>


### [151] [M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation](https://arxiv.org/abs/2509.23728)
*Yiheng Zhang,Zhuojiang Cai,Mingdao Wang,Meitong Guo,Tianxiao Li,Li Lin,Yuwang Wang*

Main category: cs.CV

TL;DR: 本文提出了M3DLayout，一个大规模多来源、高质量注释的文本驱动3D室内布局数据集，并建立基线模型验证其实用性，显著提升了布局生成多样性和细致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D室内布局生成模型受制于数据集规模有限、多样性不足和注释质量低，极大限制了模型学习能力和场景复杂性。

Method: 提出M3DLayout数据集，整合真实场景扫描、专业CAD设计和程序生成三类来源，共收录15,080个布局和25.8万个物体实例，每个布局均配以详细结构化文本描述。同时，基于文本条件扩散模型建立基线，系统评估数据集价值。

Result: 实验结果表明M3DLayout为训练室内布局生成模型提供了坚实基础。多源数据尤其是包含丰富小物体信息的Inf3DLayout子集，显著提升了场景的多样性和细节复杂程度。

Conclusion: M3DLayout作为全新大规模高质量数据集，有望极大推动文本驱动3D场景合成领域模型性能及研究进展。

Abstract: In text-driven 3D scene generation, object layout serves as a crucial
intermediate representation that bridges high-level language instructions with
detailed geometric output. It not only provides a structural blueprint for
ensuring physical plausibility but also supports semantic controllability and
interactive editing. However, the learning capabilities of current 3D indoor
layout generation models are constrained by the limited scale, diversity, and
annotation quality of existing datasets. To address this, we introduce
M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.
M3DLayout comprises 15,080 layouts and over 258k object instances, integrating
three distinct sources: real-world scans, professional CAD designs, and
procedurally generated scenes. Each layout is paired with detailed structured
text describing global scene summaries, relational placements of large
furniture, and fine-grained arrangements of smaller items. This diverse and
richly annotated resource enables models to learn complex spatial and semantic
patterns across a wide variety of indoor environments. To assess the potential
of M3DLayout, we establish a benchmark using a text-conditioned diffusion
model. Experimental results demonstrate that our dataset provides a solid
foundation for training layout generation models. Its multi-source composition
enhances diversity, notably through the Inf3DLayout subset which provides rich
small-object information, enabling the generation of more complex and detailed
scenes. We hope that M3DLayout can serve as a valuable resource for advancing
research in text-driven 3D scene synthesis.

</details>


### [152] [LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models](https://arxiv.org/abs/2509.23729)
*Shubhang Bhatnagar,Andy Xu,Kar-Han Tan,Narendra Ahuja*

Main category: cs.CV

TL;DR: 本文首次系统分析了多模态大语言模型（MLLMs）在超低比特（<4bit）量化下的表现，并提出分层选择量化方案（LUQ），大幅降低内存需求且性能损失有限。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型通常需要大量内存和计算资源，限制了其部署。而在单纯的语言模型中，极低位后训练量化（PTQ）已被证明效果良好，因此探索面向MLLM的极低位量化方法具有重要意义。

Method: 分析多模态token在不同层的激活分布，发现部分层对超低比特量化容忍度较高。基于此，提出LUQ方法：分层选择性地对容忍度高的层进行超低比特量化。同时，在PTQ时混合图像和文本token，有助提升低比特VQA效果。方法在LLaVA-1.5和Qwen-2.5-VL等多个VQA基准上验证有效性。

Result: LUQ量化后模型内存占用分别比4bit模型减少40%和31%，在MME基准上性能下降不到10%。混合模态token可进一步提升VQA低比特表现。

Conclusion: 分层超低比特量化（LUQ）能有效压缩多模态大模型且性能损失可控，为实际部署提供了新思路。

Abstract: Large Language Models (LLMs) with multimodal capabilities have revolutionized
vision-language tasks, but their deployment often requires huge memory and
computational resources. While post-training quantization (PTQ) has
successfully compressed language models to as low as 1-bit precision without
significant performance loss, its effectiveness for multimodal LLMs (MLLMs)
remains relatively unexplored. In this paper, we present the first study on
ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals
that multimodal tokens and intermediate layer activations produced by them
exhibit significantly higher statistical variance and entropy compared to text
tokens, making them less tolerant to ultra-low bit quantization. However, the
activation distributions of multimodal tokens varies significantly over
different layers, with some layers having lower entropy activation
distributions. We empirically show that such layers in these models can better
tolerate ultra-low bit quantization. Building on these insights, we propose a
novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit
Quantization, which selectively applies ultra-low bit quantization to layers
that are more resilient to it. Additionally, we also show that using a mix of
multimodal tokens (image and text) for PTQ boosts VQA performance in the
ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL
across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less
memory than their 4-bit counterparts, respectively, while exhibiting a
performance degradation of less than 10% on the MME benchmark.

</details>


### [153] [HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation](https://arxiv.org/abs/2509.23736)
*Cong Chen,Ziyuan Huang,Cheng Zou,Muzhi Zhu,Kaixiang Ji,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多尺度视觉Transformer（ViT）分词器HieraTok，通过引入多尺度下采样和尺度因果注意机制，有效提升了图像重建和生成的表现，显著优于传统单尺度ViT分词器。


<details>
  <summary>Details</summary>
Motivation: 现有ViT分词器只能捕捉单一尺度的图像特征，难以同时兼顾全局语义与局部结构，限制了其在视觉生成任务中的表现。因此，亟需一种多尺度建模方法以提取更丰富的图像信息。

Method: 作者设计了两项核心机制：(1) 在分词器编码器输出的Token图上进行多尺度下采样，生成多尺度Token序列；(2) 提出尺度因果注意力机制，使低分辨率全局特征能逐步传递至高分辨率，捕捉细粒度结构信息。

Result: 多尺度分词器相比单尺度，在图像重建rFID指标上提升27.2%（1.47→1.07）；在下游生成任务中，收敛速度提升1.38倍，gFID提升18.9%（16.4→13.3）。进一步扩大训练后，rFID达0.45，gFID达1.82，均为ViT分词器中的SOTA。

Conclusion: HieraTok作为首个多尺度ViT分词器，有效兼顾了全局与局部信息，在图像重建与生成任务中实现显著提升，为后续ViT分词器在视觉生成领域的研究和应用提供了新方向。

Abstract: In this work, we present HieraTok, a novel multi-scale Vision Transformer
(ViT)-based tokenizer that overcomes the inherent limitation of modeling
single-scale representations. This is realized through two key designs: (1)
multi-scale downsampling applied to the token map generated by the tokenizer
encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal
attention mechanism that enables the progressive flow of information from
low-resolution global semantic features to high-resolution structural details.
Coupling these designs, HieraTok achieves significant improvements in both
image reconstruction and generation tasks. Under identical settings, the
multi-scale visual tokenizer outperforms its single-scale counterpart by a
27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into
downstream generation frameworks, it achieves a $1.38\times$ faster convergence
rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be
attributed to the smoother and more uniformly distributed latent space.
Furthermore, by scaling up the tokenizer's training, we demonstrate its
potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To
the best of our knowledge, we are the first to introduce multi-scale ViT-based
tokenizer in image reconstruction and image generation. We hope our findings
and designs advance the ViT-based tokenizers in visual generation tasks.

</details>


### [154] [ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature Learning](https://arxiv.org/abs/2509.23741)
*Xincheng Yao,Chao Shi,Muming Zhao,Guangtao Zhai,Chongyang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的类别无关异常检测（class-agnostic anomaly detection, AD）方法ResAD及其改进版ResAD++，能够不经重训练或微调直接对新类别中的异常进行检测，并在八个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单类或多类AD方法由于特征表达仍存在类别相关性，导致在新类别上的泛化能力较差，难以实现真正的类别无关异常检测。因此，作者希望设计一种能够消除类别相关性的异常检测模型。

Method: 提出了一种剩余特征（residual feature）机制，通过特征匹配和减法消除特征相关性，并通过特征超球约束使得不同类别的特征尺度一致。此外，提出了对比性更强的logbarrier双向收缩损失和矢量量化特征分布匹配模块，进一步增强模型能力，形成ResAD++。

Result: 在八个真实世界AD数据集上的综合实验表明，ResAD++在无需针对新类别重训练或微调的情况下仍能实现优异表现，超越当前主流方法及自身基础版ResAD。

Conclusion: 提出的ResAD及其增强版ResAD++能够有效实现类别无关异常检测，在新类别未见数据上具有优异的泛化能力，推动了异常检测领域的发展。

Abstract: This paper explores the problem of class-agnostic anomaly detection (AD),
where the objective is to train one class-agnostic AD model that can generalize
to detect anomalies in diverse new classes from different domains without any
retraining or fine-tuning on the target data. When applied for new classes, the
performance of current single- and multi-class AD methods is still
unsatisfactory. One fundamental reason is that representation learning in
existing methods is still class-related, namely, feature correlation. To
address this issue, we propose residual features and construct a simple but
effective framework, termed ResAD. Our core insight is to learn the residual
feature distribution rather than the initial feature distribution. Residual
features are formed by matching and then subtracting normal reference features.
In this way, we can effectively realize feature decorrelation. Even in new
classes, the distribution of normal residual features would not remarkably
shift from the learned distribution. In addition, we think that residual
features still have one issue: scale correlation. To this end, we propose a
feature hypersphere constraining approach, which learns to constrain initial
normal residual features into a spatial hypersphere for enabling the feature
scales of different classes as consistent as possible. Furthermore, we propose
a novel logbarrier bidirectional contraction OCC loss and vector quantization
based feature distribution matching module to enhance ResAD, leading to the
improved version of ResAD (ResAD++). Comprehensive experiments on eight
real-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD
results when directly used in new classes, outperforming state-of-the-art
competing methods and also surpassing ResAD. The code is available at
https://github.com/xcyao00/ResAD.

</details>


### [155] [Poivre: Self-Refining Visual Pointing with Reinforcement Learning](https://arxiv.org/abs/2509.23746)
*Wenjie Yang,Zengfeng Huang*

Main category: cs.CV

TL;DR: 提出了一种新的视觉指向方法Poivre，通过多步自我修正显著提升视觉—语言模型的指点能力，并在权威基准上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉—语言模型在图像指向（即预测目标在图像中的坐标）任务上表现远逊于人类，一个主要原因是模型通常只允许一步完成指向，缺乏自我修正能力。

Method: 提出Poivre方法，即“指向、可视化、再修正”三步流程。模型先给出初步标注，接着可视化输出并根据结果进行迭代修正。训练阶段引入强化学习，设计特殊的过程奖励，引导模型学会自我修正。最后训练出Poivre-7B大模型。

Result: 在Point-Bench基准上，Poivre-7B超过Gemini-2.5-Pro和Molmo-72B等先进模型，性能提升3%以上，创下新SOTA。相关训练代码、数据集和模型权重也一并发布。

Conclusion: 多步自我修正能显著提升VLM在视觉指向任务上的表现，简单的自精炼流程结合强化学习也能带来很好的实际成效，对后续研究有重要参考价值。

Abstract: Visual pointing, which aims to localize a target by predicting its
coordinates on an image, has emerged as an important problem in the realm of
vision-language models (VLMs). Despite its broad applicability, recent
benchmarks show that current VLMs still fall far behind human performance on
this task. A key limitation is that VLMs are typically required to complete the
pointing task in a single step, akin to asking humans to point at an object
without seeing their own fingers. To address this issue, we propose a simple
yet effective self-refining procedure: Point, Visualize, then Refine (Poivre).
This procedure enables a VLM to first mark its estimated point, then
iteratively refine the coordinates if necessary. Inspired by advances of
reasoning models in the natural language domain, we employ reinforcement
learning (RL) to incentivize this self-refining ability. For the RL training,
we design a neat process reward that is not only empirically effective but also
grounded in appealing properties. Our trained model, Poivre-7B, sets a new
state of the art on Point-Bench, outperforming both proprietary models such as
Gemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To
support future research, we release our training and inference code, dataset,
and the Poivre-7B checkpoint.

</details>


### [156] [PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block](https://arxiv.org/abs/2509.23751)
*Arshia Yousefi Nezhad,Helia Aghaei,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习模型PVTAdpNet，用于结直肠癌息肉的高效分割，在多个数据集上表现优异，具有良好的临床应用前景。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌发病率高、死亡率高，早期检测至关重要。传统结肠镜检查因息肉形态多样，漏检率高，亟需更准确、智能的分割算法提升检测效果。

Method: 作者提出PVTAdpNet模型，将U-Net结构与金字塔视觉Transformer主干网络融合，采用创新的残差模块和适配器式跳连机制，增强特征提取和信息流动，并引入通道注意力机制提升特征精细度。模型可实现实时息肉分割。

Result: PVTAdpNet在多项基准数据集（含out-of-distribution）上实现了Dice 0.8851和mIoU 0.8167的高分，优于现有方法，PolypGen等数据集上也展现出实时高精度性能。

Conclusion: PVTAdpNet显著提升了息肉分割的准确性和实时性，满足临床实际需求，并已开放源码，便于后续研究和应用。

Abstract: Colorectal cancer ranks among the most common and deadly cancers, emphasizing
the need for effective early detection and treatment. To address the
limitations of traditional colonoscopy, including high miss rates due to polyp
variability, we introduce the Pyramid Vision Transformer Adapter Residual
Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder
structure with a Pyramid Vision Transformer backbone, novel residual blocks,
and adapter-based skip connections. The design enhances feature extraction,
dense prediction, and gradient flow, supported by squeeze-and-excitation
attention for improved channel-wise feature refinement. PVTAdpNet achieves
real-time, accurate polyp segmentation, demonstrating superior performance on
benchmark datasets with high mDice and mIoU scores, making it highly suitable
for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851
and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution
polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's
capability for real-time, accurate performance within familiar distributions.
The source code of our network is available at
https://github.com/ayousefinejad/PVTAdpNet.git

</details>


### [157] [UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception](https://arxiv.org/abs/2509.23760)
*Xinyang Song,Libin Wang,Weining Wang,Shaozhen Liu,Dandan Zheng,Jingdong Chen,Qi Li,Zhenan Sun*

Main category: cs.CV

TL;DR: 本文提出了UniAlignment框架，通过单一扩散模型实现统一的多模态生成，提升了视觉与文本的语义一致性及复杂指令理解能力，并推出了SemGen-Bench基准，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前多模态任务在利用扩散模型方面，通常需要依赖视觉-语言模型或模块化设计来实现语义指导，导致架构碎片化且计算效率低。因此，迫切需要统一架构来高效提升多模态语义理解，尤其是在复杂语义指令下。

Method: 提出UniAlignment架构，采用单一扩散变换器（transformer），基于双流扩散训练策略，不仅融合了内部模态语义对齐，还实现了跨模态的语义对齐，提升了模型在多模态任务中的一致性和指令跟随能力。同时提出SemGen-Bench基准用于测试复杂文本指令下的多模态语义一致性。

Result: 在多个任务与基准上，UniAlignment展现出超过现有方法的性能，表现出更好的多模态语义一致性和处理复杂指令的鲁棒性。

Conclusion: UniAlignment显示了统一多模态生成架构的巨大潜力，为实现更智能的多模态理解和生成提供了有效途径，有望推动扩散模型在多模态领域的发展。

Abstract: The remarkable success of diffusion models in text-to-image generation has
sparked growing interest in expanding their capabilities to a variety of
multi-modal tasks, including image understanding, manipulation, and perception.
These tasks require advanced semantic comprehension across both visual and
textual modalities, especially in scenarios involving complex semantic
instructions. However, existing approaches often rely heavily on
vision-language models (VLMs) or modular designs for semantic guidance, leading
to fragmented architectures and computational inefficiency. To address these
challenges, we propose UniAlignment, a unified multimodal generation framework
within a single diffusion transformer. UniAlignment introduces a dual-stream
diffusion training strategy that incorporates both intrinsic-modal semantic
alignment and cross-modal semantic alignment, thereby enhancing the model's
cross-modal consistency and instruction-following robustness. Additionally, we
present SemGen-Bench, a new benchmark specifically designed to evaluate
multimodal semantic consistency under complex textual instructions. Extensive
experiments across multiple tasks and benchmarks demonstrate that UniAlignment
outperforms existing baselines, underscoring the significant potential of
diffusion models in unified multimodal generation.

</details>


### [158] [GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning](https://arxiv.org/abs/2509.23770)
*Xiaojie Li,Bei Wang,Jianlong Wu,Yue Yu,Liqiang Nie,Min Zhang*

Main category: cs.CV

TL;DR: GenView++通过创新的数据增强和质量评估机制，提升了对比学习中的正样本对的构建与利用效率，在视觉和视觉-语言任务上均取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 对比学习效果依赖于高质量正样本对的构建。现有方法不论是手工还是生成式增强，都存在多样性不足和语义污染的问题，而且训练时未能区分样本对的质量，对性能造成限制。

Method: 提出GenView++框架，包含两大创新：（1）采用多源自适应视图生成机制，结合图像、文本和图文联合条件动态调整生成参数，确保生成样本既多样又语义一致；（2）引入基于质量的对比学习机制，对每对样本根据语义一致性和多样性动态赋权，优先利用高质量样本对。

Result: GenView++在大规模实验中优于主流方法。例如，视觉表示学习上提升MoCov2在ImageNet上的线性分类性能2.5%；在十个数据集上，视觉-语言学习的零样本分类准确率较CLIP提升12.31%，较SLIP提升5.31%；Flickr30k文本检索R@5提升3.2%。

Conclusion: GenView++显著提升了对比学习中正样本对构建与利用的有效性，在视觉和视觉-语言任务中均表现突出，有望推动对比学习方法的进一步发展。

Abstract: The success of contrastive learning depends on the construction and
utilization of high-quality positive pairs. However, current methods face
critical limitations on two fronts: on the construction side, both handcrafted
and generative augmentations often suffer from limited diversity and risk
semantic corruption; on the learning side, the absence of a quality assessment
mechanism leads to suboptimal supervision where all pairs are treated equally.
To tackle these challenges, we propose GenView++, a unified framework that
addresses both fronts by introducing two synergistic innovations. To improve
pair construction, GenView++ introduces a multi-source adaptive view generation
mechanism to synthesize diverse yet semantically coherent views by dynamically
modulating generative parameters across image-conditioned, text-conditioned,
and image-text-conditioned strategies. Second, a quality-driven contrastive
learning mechanism assesses each pair's semantic alignment and diversity to
dynamically reweight their training contribution, prioritizing high-quality
pairs while suppressing redundant or misaligned pairs. Extensive experiments
demonstrate the effectiveness of GenView++ across both vision and
vision-language tasks. For vision representation learning, it improves MoCov2
by +2.5% on ImageNet linear classification. For vision-language learning, it
raises the average zero-shot classification accuracy by +12.31% over CLIP and
+5.31% over SLIP across ten datasets, and further improves Flickr30k text
retrieval R@5 by +3.2%. The code is available at
https://github.com/xiaojieli0903/GenViewPlusPlus.

</details>


### [159] [A Modality-Tailored Graph Modeling Framework for Urban Region Representation via Contrastive Learning](https://arxiv.org/abs/2509.23772)
*Yaya Zhao,Kaiqi Zhao,Zixuan Tang,Zhiyuan Liu,Xiaoling Lu,Yalei Du*

Main category: cs.CV

TL;DR: 当前图结构建模在多模态城市数据中存在无法充分区分不同模态特征以及忽略空间异质性的问题。为此，作者提出了MTGRR框架，结合模态自适应GNN和空间感知多模态融合机制，在多个任务及数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态城市区域图神经网络方法普遍使用统一的模型结构，各模态特征未能得到有效挖掘；同时，多模态特征融合阶段忽略了不同区域间的空间和模态异质性，导致表达效果有限。作者旨在提升多模态表示的区分能力和空间适应性。

Method: 将模态划分为聚合级与点级两类，通过MoE专属专家GNN处理聚合级模态，点级模态用双层GNN提取细粒度语义特征。融合阶段引入空间感知机制，自适应推断各区域的模态权重。同时，结合自监督对比学习联合优化聚合级、点级与融合级目标。

Result: 在两个真实多模态城市数据集上，MTGRR在表示效果和下游任务准确率上都显著优于各类基线方法，体现了其在处理模态和空间异质性方面的先进性与有效性。

Conclusion: MTGRR能有效挖掘和融合多模态城市数据中的模态间与空间异质性，极大提升了区域表示学习的精度和泛化能力。该框架适用于当前城市智能分析中的多元复杂数据建模问题。

Abstract: Graph-based models have emerged as a powerful paradigm for modeling
multimodal urban data and learning region representations for various
downstream tasks. However, existing approaches face two major limitations. (1)
They typically employ identical graph neural network architectures across all
modalities, failing to capture modality-specific structures and
characteristics. (2) During the fusion stage, they often neglect spatial
heterogeneity by assuming that the aggregation weights of different modalities
remain invariant across regions, resulting in suboptimal representations. To
address these issues, we propose MTGRR, a modality-tailored graph modeling
framework for urban region representation, built upon a multimodal dataset
comprising point of interest (POI), taxi mobility, land use, road element,
remote sensing, and street view images. (1) MTGRR categorizes modalities into
two groups based on spatial density and data characteristics: aggregated-level
and point-level modalities. For aggregated-level modalities, MTGRR employs a
mixture-of-experts (MoE) graph architecture, where each modality is processed
by a dedicated expert GNN to capture distinct modality-specific
characteristics. For the point-level modality, a dual-level GNN is constructed
to extract fine-grained visual semantic features. (2) To obtain effective
region representations under spatial heterogeneity, a spatially-aware
multimodal fusion mechanism is designed to dynamically infer region-specific
modality fusion weights. Building on this graph modeling framework, MTGRR
further employs a joint contrastive learning strategy that integrates region
aggregated-level, point-level, and fusion-level objectives to optimize region
representations. Experiments on two real-world datasets across six modalities
and three tasks demonstrate that MTGRR consistently outperforms
state-of-the-art baselines, validating its effectiveness.

</details>


### [160] [Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution](https://arxiv.org/abs/2509.23774)
*Qifan Li,Jiale Zou,Jinhua Zhang,Wei Long,Xinyu Zhou,Shuhang Gu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于向量量化的超分辨率生成模型（TVQ&RAP），通过改进量化和预测方式，有效提升了重建图像的质量并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于向量量化的方法在视觉先验建模方面虽有潜力，但在处理丰富视觉信息时，会带来较大的量化误差；另外，仅利用编码级别监督训练预测器，无法优化最终的图像重建误差，导致建模效果不佳。为解决这些问题，提出了新方法。

Method: 1）提出纹理向量量化策略，仅对缺失纹理进行先验建模，避免对所有视觉特征直接量化，减少量化误差；2）提出重建感知预测策略，利用straight-through estimator，采用图像级别的监督直接训练索引预测器，从而优化最终的重建效果。

Result: 实验结果表明，该生成式超分模型（TVQ&RAP）不仅生成的超分辨率图像具有高真实感，同时还具备较低的计算开销。

Conclusion: 通过更细致的量化策略和直接优化重建误差，该方法克服了以往VQ模型的主要缺陷，在保持效率的前提下，生成质量显著提升。

Abstract: Vector-quantized based models have recently demonstrated strong potential for
visual prior modeling. However, existing VQ-based methods simply encode visual
features with nearest codebook items and train index predictor with code-level
supervision. Due to the richness of visual signal, VQ encoding often leads to
large quantization error. Furthermore, training predictor with code-level
supervision can not take the final reconstruction errors into consideration,
result in sub-optimal prior modeling accuracy. In this paper we address the
above two issues and propose a Texture Vector-Quantization and a Reconstruction
Aware Prediction strategy. The texture vector-quantization strategy leverages
the task character of super-resolution and only introduce codebook to model the
prior of missing textures. While the reconstruction aware prediction strategy
makes use of the straight-through estimator to directly train index predictor
with image-level supervision. Our proposed generative SR model (TVQ&RAP) is
able to deliver photo-realistic SR results with small computational cost.

</details>


### [161] [GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning](https://arxiv.org/abs/2509.23781)
*Nayeong Kim,Seong Joon Oh,Suha Kwak*

Main category: cs.CV

TL;DR: 本文针对基于视觉-语言模型（VLMs）的高效参数微调方法在处理组间数据不平衡时存在的偏差问题，提出了一种简单而有效的去偏差微调算法 GroupCoOp。该方法通过引入组特定文本提示提升模型对少数群体的鲁棒性，并在多个基准上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM参数高效微调方法在面对微调数据集中的亚组不平衡时，容易受到虚假相关性的影响，从而损害少数群体的性能表现。因此亟需一种方法提升微调VLM在组别不平衡时的鲁棒性。

Method: 提出Group Context Optimization（GroupCoOp）算法。核心思想是针对每个组生成特定文本提示，将其作为该组的特征代表，相当于为每一类别配置多个分类器。这种做法利用VLM文本编码器丰富的语义表达能力，实现即便训练样本较少也能生成有效群体提示。这样能缓解因数据分布不均带来的类别分散及少数组别被忽视等问题。

Result: 在五个基准数据集和五种CLIP架构上，GroupCoOp均取得了最佳的组鲁棒性表现，有时甚至超越了需要微调整个网络参数的先前方法。该方法只训练了0.016%网络参数，极大提高了效率。

Conclusion: GroupCoOp作为一种高效且去偏差的微调方法，在保持VLM高效参数利用的同时，显著提升了在组别不平衡情况下的泛化能力和鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)
excels in various vision tasks thanks to the rich knowledge and generalization
ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are
vulnerable to spurious correlations stemming from the subgroup imbalance in the
fine-tuning datasets. To resolve this issue, we propose Group Context
Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm
that enhances the group robustness of fine-tuned VLMs. Its key idea is to
employ group-specific text prompts as group representatives serving as multiple
classifiers for their target class. The rich semantic knowledge of the text
encoder of VLM enables the discovery of effective group prompts even for groups
with a small number of training samples. Leveraging the group prompts for each
class addresses the issues caused by the group-imbalanced training set, such as
the neglect of minority groups and the scattered distribution of each class in
the embedding space. GroupCoOp achieved the best results on five benchmarks
across five CLIP architectures and occasionally outperformed prior methods that
fine-tune the entire network, despite training only 0.016\% of the network's
parameters.

</details>


### [162] [From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation](https://arxiv.org/abs/2509.23787)
*Mahdi Farrokhimaleki,Parsa Rahmati,Richard Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种通过图像分割和视觉分析修复PCG生成不稳定游戏关卡的方法，在愤怒的小鸟游戏中有效提升了关卡稳定性和可玩性。


<details>
  <summary>Details</summary>
Motivation: PCG技术能够自动生成丰富复杂的环境，但现有PCG模型容易生成结构不稳定、质量不一致的关卡，难以满足工业标准。如何检测并修复这些问题，是实现高质量自动化内容生成的关键难题。

Method: 提出了一种基于图像分割和视觉分析的新方法，对PCG生成的关卡图像进行处理，通过检测结构漏洞进行定位，并采用有针对性的修复操作。实验中，评估和比较了多种图像分割模型，选择效果最佳的模型用于修复流程。以愤怒的小鸟为案例进行验证。

Result: 实验结果表明，该方法显著提升了AI自动生成关卡的结构稳定性和可玩性，修复效果良好。

Conclusion: 尽管本研究评估对象为愤怒的小鸟，该基于图像的检测和修复框架具有良好的通用性，可适用于结构相似的2D游戏自动关卡内容优化。

Abstract: Procedural Content Generation (PCG) techniques enable automatic creation of
diverse and complex environments. While PCG facilitates more efficient content
creation, ensuring consistently high-quality, industry-standard content remains
a significant challenge. In this research, we propose a method to identify and
repair unstable levels generated by existing PCG models. We use Angry Birds as
a case study, demonstrating our method on game levels produced by established
PCG approaches. Our method leverages object segmentation and visual analysis of
level images to detect structural gaps and perform targeted repairs. We
evaluate multiple object segmentation models and select the most effective one
as the basis for our repair pipeline. Experimental results show that our method
improves the stability and playability of AI-generated levels. Although our
evaluation is specific to Angry Birds, our image-based approach is designed to
be applicable to a wide range of 2D games with similar level structures.

</details>


### [163] [Controllable Generation of Large-Scale 3D Urban Layouts with Semantic and Structural Guidance](https://arxiv.org/abs/2509.23804)
*Mengyuan Niu,Xinxin Zhuo,Ruizhe Wang,Yuyue Huang,Junyan Yang,Qiao Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种可控的大规模三维城市布局生成方法，通过融合几何与语义信息，使输出更加真实且便于控制。


<details>
  <summary>Details</summary>
Motivation: 目前图像生成方法虽然可以产生多样的布局，但缺乏几何连续性和可扩展性；基于图的方法虽然能把握结构关系，但忽略了地块的语义信息。因此，迫切需要一种结合两者优势的建模方法。

Method: 作者提出将几何和语义属性融合，引入边权重，同时在图中嵌入建筑高度，将2D布局扩展到3D结构。用户还可以直接通过修改语义属性控制生成结果。

Result: 实验证明，该方法能生成有效的大规模3D城市模型，兼具几何连续性和语义多样性。

Conclusion: 本方法为数据驱动的城市规划和设计提供了有效工具，同时具备可控性和可扩展性。

Abstract: Urban modeling is essential for city planning, scene synthesis, and gaming.
Existing image-based methods generate diverse layouts but often lack geometric
continuity and scalability, while graph-based methods capture structural
relations yet overlook parcel semantics. We present a controllable framework
for large-scale 3D vector urban layout generation, conditioned on both geometry
and semantics. By fusing geometric and semantic attributes, introducing edge
weights, and embedding building height in the graph, our method extends 2D
layouts to realistic 3D structures. It also enables users to directly control
the output by modifying semantic attributes. Experiments show that it produces
valid, large-scale urban models, offering an effective tool for data-driven
planning and design.

</details>


### [164] [A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control](https://arxiv.org/abs/2509.23815)
*Ali Nazeri,Shashank Mishra,Achim Wagner,Martin Ruskowski,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本论文提出了一种新型多视角质控系统，整合多摄像头与先进检测算法，显著提升了生产线小部件装配的检测准确率，并公开了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 传统的质控方法依赖单视角成像或人工检测，易受遮挡、视角受限或光照不一致影响，导致检测错误，并且增加了生产线中检测工位与停机成本。

Method: 构建多视角质控模块，使用三摄像头系统获取全面组件图像，通过自定义的图像融合方法和先进目标检测算法整合多视角结果，并建立覆盖多种真实工况（包括不同光照、遮挡和角度）的标注数据集。

Result: 多视角系统在识别如螺钉等装配不良的小部件时，检测精度和召回率大幅高于单视角方法，展现出较强鲁棒性和适用性。

Conclusion: 该多视角质控方案克服了单视角的固有限制，为工业自动化提供了高效、可扩展和高准确率的质控机制，有助于保障生产线可靠性与安全性，同时促进相关领域研究（数据集已公开）。

Abstract: Quality control is a critical aspect of manufacturing, particularly in
ensuring the proper assembly of small components in production lines. Existing
solutions often rely on single-view imaging or manual inspection, which are
prone to errors due to occlusions, restricted perspectives, or lighting
inconsistencies. These limitations require the installation of additional
inspection stations, which could disrupt the assembly line and lead to
increased downtime and costs. This paper introduces a novel multi-view quality
control module designed to address these challenges, integrating a multi-camera
imaging system with advanced object detection algorithms. By capturing images
from three camera views, the system provides comprehensive visual coverage of
components of an assembly process. A tailored image fusion methodology combines
results from multiple views, effectively resolving ambiguities and enhancing
detection reliability. To support this system, we developed a unique dataset
comprising annotated images across diverse scenarios, including varied lighting
conditions, occlusions, and angles, to enhance applicability in real-world
manufacturing environments. Experimental results show that our approach
significantly outperforms single-view methods, achieving high precision and
recall rates in the identification of improperly fastened small assembly parts
such as screws. This work contributes to industrial automation by overcoming
single-view limitations, and providing a scalable, cost-effective, and accurate
quality control mechanism that ensures the reliability and safety of the
assembly line. The dataset used in this study is publicly available to
facilitate further research in this domain.

</details>


### [165] [Assessing Visual Privacy Risks in Multimodal AI: A Novel Taxonomy-Grounded Evaluation of Vision-Language Models](https://arxiv.org/abs/2509.23827)
*Efthymios Tsaprazlis,Tiantian Feng,Anil Ramakrishna,Rahul Gupta,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 本文关注大模型在隐私理解与保护上的不足，提出了视觉隐私分类体系，并评估了当前模型在隐私识别上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在理解和推理等能力上的进步，其在隐私保护上的不足愈发突出，尤其缺乏能够测试和指导AI体系隐私原则实现的资源。亟需评价VLMs在隐私理解上的能力，并为未来研究建立标准。

Method: 作者构建了一个全面、多层级和可扩展的视觉隐私分类法。随后，基于该体系对多种主流视觉-语言模型（VLMs）进行系统评测，分析它们在不同隐私场景下的表现。

Result: 实验证明，当前主流视觉-语言模型在上下文隐私理解上表现不一致，普遍存在漏洞和局限性。

Conclusion: 本文拓展了隐私保护研究的基础，提出的视觉隐私分类法可为后续研究提供标准框架，并强调了发展更为健全、具备隐私感知的AI系统的紧迫需求。

Abstract: Artificial Intelligence have profoundly transformed the technological
landscape in recent years. Large Language Models (LLMs) have demonstrated
impressive abilities in reasoning, text comprehension, contextual pattern
recognition, and integrating language with visual understanding. While these
advances offer significant benefits, they also reveal critical limitations in
the models' ability to grasp the notion of privacy. There is hence substantial
interest in determining if and how these models can understand and enforce
privacy principles, particularly given the lack of supporting resources to test
such a task. In this work, we address these challenges by examining how legal
frameworks can inform the capabilities of these emerging technologies. To this
end, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that
captures a wide range of privacy issues, designed to be scalable and adaptable
to existing and future research needs. Furthermore, we evaluate the
capabilities of several state-of-the-art Vision-Language Models (VLMs),
revealing significant inconsistencies in their understanding of contextual
privacy. Our work contributes both a foundational taxonomy for future research
and a critical benchmark of current model limitations, demonstrating the urgent
need for more robust, privacy-aware AI systems.

</details>


### [166] [Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation](https://arxiv.org/abs/2509.23828)
*Hanyu Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了Uni4D-LLM，一个具备时空感知能力的统一视觉-语言模型，实现了对4D场景的理解和生成，大大提升了对动态场景的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在2D任务取得重大进展，但在3D和4D（动态场景）上的语义理解与内容生成无法统一，主要因理解与生成任务采用了不同的建模方法，导致无法用一个模型高效处理两者。

Method: 提出了Uni4D-LLM，创新性地融合了4D场景的语义特征和注入噪声的外观特征，并结合时空几何信息，通过自适应交互注意力机制整合为具有时空感知能力的视觉表示。模型架构以Transformer为基础，将自回归和扩散模型融合于同一大语言模型中，配合任务定制化头部输出。

Result: 在多个公开基准测试上，Uni4D-LLM获得了优于现有方法的理解与生成表现，验证了方法的有效性和泛化能力。

Conclusion: Uni4D-LLM首次实现了对4D场景理解与生成的真正统一，为视觉-语言多模态大模型在复杂动态场景的应用奠定了基础。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in 2D
scene understanding and generation, but extending this unification to the
physical world remains an open challenge. Existing 3D and 4D approaches
typically embed scene geometry into autoregressive model for semantic
understanding and diffusion model for content generation. This paradigm gap
prevents a single model from jointly handling both tasks, especially in dynamic
4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,
the first unified VLM framework with spatiotemporal awareness for 4D scene
understanding and generation. Our design is guided by two key insights: 1)
Unification requires a shared representation. We extract semantic features for
understanding and noisy-injected appearance features for generation,
incorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual
representation through adaptive cross-attention. 2) Unification requires a
shared architecture. Both autoregression and diffusion are built on Transformer
backbones, and this enables integration into a single LLM with task-specific
heads. By aligning visual and linguistic representations, our Uni4D-LLM
produces predictions for both understanding and generation within one
Transformer-based framework. We further apply instruction fine-tuning on
diverse 4D vision-language datasets to improve generalization across tasks.
Extensive experiments on multiple benchmarks demonstrate that Uni4D-LLM
achieves competitive or superior results compared to state-of-the-art models
and offers the first true unification of 4D scene understanding and generation.

</details>


### [167] [2nd Place Report of MOSEv2 Challenge 2025: Concept Guided Video Object Segmentation via SeC](https://arxiv.org/abs/2509.23838)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了使用大型视觉-语言模型（LVLM）来改进视频目标切割中的语义理解能力，从而提升分割的鲁棒性，并在MOSEv2数据集上进行零样本评估，取得了39.7分，排名第二。


<details>
  <summary>Details</summary>
Motivation: 以往的方法过于依赖外观模式匹配，面对剧烈变化、遮挡和场景切换时鲁棒性较差，缺乏目标概念层面的理解能力。作者希望借助视觉-语言模型的高层次语义能力，提升分割模型在复杂环境中的表现和泛化能力。

Method: 采用Segment Concept（SeC）框架，通过大型视觉-语言模型对目标建立深层语义理解，无需针对数据集进行微调，实现更持久、稳健的目标分割。该方法在MOSEv2数据集上进行了零样本评价。

Result: 在没有对训练集进行Fine-tune的情况下，SeC方法在MOSEv2测试集上获得了39.7分，在第七届大规模视频目标分割挑战赛的Complex VOS track中排名第二。

Conclusion: 基于大型视觉-语言模型的深层语义理解能力显著提升了复杂场景下视频目标分割的鲁棒性和泛化能力，无需额外训练即可获得优异表现，验证了该方法的有效性与前景。

Abstract: Semi-supervised Video Object Segmentation aims to segment a specified target
throughout a video sequence, initialized by a first-frame mask. Previous
methods rely heavily on appearance-based pattern matching and thus exhibit
limited robustness against challenges such as drastic visual changes,
occlusions, and scene shifts. This failure is often attributed to a lack of
high-level conceptual understanding of the target. The recently proposed
Segment Concept (SeC) framework mitigated this limitation by using a Large
Vision-Language Model (LVLM) to establish a deep semantic understanding of the
object for more persistent segmentation. In this work, we evaluate its
zero-shot performance on the challenging coMplex video Object SEgmentation v2
(MOSEv2) dataset. Without any fine-tuning on the training set, SeC achieved
39.7 \JFn on the test set and ranked 2nd place in the Complex VOS track of the
7th Large-scale Video Object Segmentation Challenge.

</details>


### [168] [Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric](https://arxiv.org/abs/2509.23841)
*Bingyang Cui,Yujie Zhang,Qi Yang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 本文提出了一个新的Text-to-3D（T23D）质量评测基准T23D-CompBench，并开发了评测方法Rank2Score，显著提升了T23D模型的质量评估精度。


<details>
  <summary>Details</summary>
Motivation: 目前T23D生成模型层出不穷，但它们的质量评估基准和指标存在过时、分散和粒度粗糙的问题，导致缺乏有效的细致量化评测方法；现有的客观评价指标也存在结构性限制，难以准确反映生成质量。

Method: 1）提出全新的T23D-CompBench基准，涵盖5大组成成分和12个子成分，通过10种主流模型生成3600个带纹理的网格，并基于大规模主观实验采集12.96万个人类评分。2）提出Rank2Score评测方法，采用两阶段训练：第一阶段利用有监督对比回归和课程学习强化成对训练，第二阶段通过主观均分进行预测细化，提高与人类评价的一致性。

Result: Rank2Score在各项实验和下游应用中都优于现有指标，可更准确反映人类对T23D生成质量的判断，并可作为奖励函数优化生成模型。

Conclusion: T23D-CompBench为T23D质量评估提供了全面、细粒度的基准，Rank2Score则是当前最有效的自动化T23D质量评估方法，对T23D生成和优化具有重要价值。

Abstract: Recent advances in Text-to-3D (T23D) generative models have enabled the
synthesis of diverse, high-fidelity 3D assets from textual prompts. However,
existing challenges restrict the development of reliable T23D quality
assessment (T23DQA). First, existing benchmarks are outdated, fragmented, and
coarse-grained, making fine-grained metric training infeasible. Moreover,
current objective metrics exhibit inherent design limitations, resulting in
non-representative feature extraction and diminished metric robustness. To
address these limitations, we introduce T23D-CompBench, a comprehensive
benchmark for compositional T23D generation. We define five components with
twelve sub-components for compositional prompts, which are used to generate
3,600 textured meshes from ten state-of-the-art generative models. A
large-scale subjective experiment is conducted to collect 129,600 reliable
human ratings across different perspectives. Based on T23D-CompBench, we
further propose Rank2Score, an effective evaluator with two-stage training for
T23DQA. Rank2Score enhances pairwise training via supervised contrastive
regression and curriculum learning in the first stage, and subsequently refines
predictions using mean opinion scores to achieve closer alignment with human
judgments in the second stage. Extensive experiments and downstream
applications demonstrate that Rank2Score consistently outperforms existing
metrics across multiple dimensions and can additionally serve as a reward
function to optimize generative models. The project is available at
https://cbysjtu.github.io/Rank2Score/.

</details>


### [169] [CE-FAM: Concept-Based Explanation via Fusion of Activation Maps](https://arxiv.org/abs/2509.23849)
*Michihiro Kuroki,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出了一种新的基于概念的解释方法CE-FAM，用于展示分类模型学到的概念、对应区域及其贡献，并能在无需标注数据集的情况下处理任意概念。


<details>
  <summary>Details</summary>
Motivation: 现有显著性图可显示分类模型关注的区域，但难以解释这些区域的实际含义。基于概念的解释虽然能将AI决策分解为人类可理解的概念，却很少有方法能同时揭示模型学到的概念、相关区域以及对结果的具体贡献。

Method: 方法CE-FAM采用与图像分类器共享激活图的分支网络，该分支网络学习模仿视觉-语言模型的嵌入，从而预测图像中的概念。概念对应区域通过激活图的加权和（权重由概念预测分数的梯度给出）决定，贡献则按其对分类分数的影响量化。此外，该方法无需标注数据集即可处理任意概念，并提出了新的评估指标来衡量概念区域的准确性。

Result: 在定性和定量实验中，CE-FAM优于现有方法，特别是在零样本推断未知概念时表现突出。

Conclusion: CE-FAM作为一个通用的解释框架，不仅能标注出分类模型学到的概念和相应区域，还能量化其贡献，继承了VLM处理任意概念的能力。

Abstract: Although saliency maps can highlight important regions to explain the
reasoning behind image classification in artificial intelligence (AI), the
meaning of these regions is left to the user's interpretation. In contrast,
conceptbased explanations decompose AI predictions into humanunderstandable
concepts, clarifying their contributions. However, few methods can
simultaneously reveal what concepts an image classifier learns, which regions
are associated with them, and how they contribute to predictions. We propose a
novel concept-based explanation method, Concept-based Explanation via Fusion of
Activation Maps (CE-FAM). It employs a branched network that shares activation
maps with an image classifier and learns to mimic the embeddings of a Vision
and Language Model (VLM). The branch network predicts concepts in an image, and
their corresponding regions are represented by a weighted sum of activation
maps, with weights given by the gradients of the concept prediction scores.
Their contributions are quantified based on their impact on the image
classification score. Our method provides a general framework for identifying
the concept regions and their contributions while leveraging VLM knowledge to
handle arbitrary concepts without requiring an annotated dataset. Furthermore,
we introduce a novel evaluation metric to assess the accuracy of the concept
regions. Our qualitative and quantitative evaluations demonstrate our method
outperforms existing approaches and excels in zero-shot inference for unseen
concepts.

</details>


### [170] [FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction](https://arxiv.org/abs/2509.23859)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出了一种融合CNN和ViT的新型面部美学预测框架FairViT-GAN，通过对抗性去偏机制实现高精度与显著公平性提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在面部美学预测中取得进展，但普遍面临架构局限、人口统计偏见与缺乏透明度等问题，特别是在少数群体中的不公平表现。

Method: 提出FairViT-GAN框架，将CNN用于局部特征提取，ViT用于全局语义建模，同时引入对抗性去偏机制，使特征对受保护属性（如种族）不敏感，并可视化各分支的特征关注区域增强透明度。

Result: 在SCUT-FBP5500数据集上，FairViT-GAN预测相关性达到0.9230，RMSE为0.2650。公平性方面，族群间性能差距缩小82.9%，对抗识别准确率降至接近随机（52.1%）。

Conclusion: FairViT-GAN不仅显著提升精度，还有效缓解算法偏见，提供了开发负责任AI系统在主观视觉评估领域的有力方案。

Abstract: Facial Beauty Prediction (FBP) has made significant strides with the
application of deep learning, yet state-of-the-art models often exhibit
critical limitations, including architectural constraints, inherent demographic
biases, and a lack of transparency. Existing methods, primarily based on
Convolutional Neural Networks (CNNs), excel at capturing local texture but
struggle with global facial harmony, while Vision Transformers (ViTs)
effectively model long-range dependencies but can miss fine-grained details.
Furthermore, models trained on benchmark datasets can inadvertently learn and
perpetuate societal biases related to protected attributes like ethnicity. To
address these interconnected challenges, we propose \textbf{FairViT-GAN}, a
novel hybrid framework that synergistically integrates a CNN branch for local
feature extraction and a ViT branch for global context modeling. More
significantly, we introduce an adversarial debiasing mechanism where the
feature extractor is explicitly trained to produce representations that are
invariant to protected attributes, thereby actively mitigating algorithmic
bias. Our framework's transparency is enhanced by visualizing the distinct
focus of each architectural branch. Extensive experiments on the SCUT-FBP5500
benchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in
predictive accuracy, achieving a Pearson Correlation of \textbf{0.9230} and
reducing RMSE to \textbf{0.2650}, but also excels in fairness. Our analysis
reveals a remarkable \textbf{82.9\% reduction in the performance gap} between
ethnic subgroups, with the adversary's classification accuracy dropping to
near-random chance (52.1\%). We believe FairViT-GAN provides a robust,
transparent, and significantly fairer blueprint for developing responsible AI
systems for subjective visual assessment.

</details>


### [171] [Sim-DETR: Unlock DETR for Temporal Sentence Grounding](https://arxiv.org/abs/2509.23867)
*Jiajin Tang,Zhengxuan Wei,Yuchen Zhu,Cheng Shi,Guanbin Li,Liang Lin,Sibei Yang*

Main category: cs.CV

TL;DR: 本论文针对视频中的时序句子定位任务，发现标准DETR方案在此任务下效果反常，提出了Sim-DETR模型作为改进。


<details>
  <summary>Details</summary>
Motivation: 虽然DETR在时序句子定位中被广泛使用，但常规的增强策略却未能提升甚至还降低了其性能。作者想要找出原因，并解决这一问题。

Method: 本文系统分析了标准DETR在该任务表现异常的两个主要原因：1）来自相似目标时刻的查询冲突；2）全局语义与局部定位之间的内部查询冲突。在此基础上，提出Sim-DETR，主要有两个修改：（1）在解码器中根据语义和位置重叠约束查询之间的自注意力；（2）加入查询与帧对齐增强全局和本地上下文的联系。

Result: 实验结果表明，Sim-DETR极大提升了DETR在视频时序句子定位任务上的表现，有效释放了其潜力。

Conclusion: Sim-DETR为时序句子定位提供了一个强有力的新基线，并为后续研究提供了有益的探索方向。

Abstract: Temporal sentence grounding aims to identify exact moments in a video that
correspond to a given textual query, typically addressed with detection
transformer (DETR) solutions. However, we find that typical strategies designed
to enhance DETR do not improve, and may even degrade, its performance in this
task. We systematically analyze and identify the root causes of this abnormal
behavior: (1) conflicts between queries from similar target moments and (2)
internal query conflicts due to the tension between global semantics and local
localization. Building on these insights, we propose a simple yet powerful
baseline, Sim-DETR, which extends the standard DETR with two minor
modifications in the decoder layers: (1) constraining self-attention between
queries based on their semantic and positional overlap and (2) adding
query-to-frame alignment to bridge the global and local contexts. Experiments
demonstrate that Sim-DETR unlocks the full potential of DETR for temporal
sentence grounding, offering a strong baseline for future research.

</details>


### [172] [Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models](https://arxiv.org/abs/2509.23876)
*Ky Dan Nguyen,Hoang Lam Tran,Anh-Dung Dinh,Daochang Liu,Weidong Cai,Xiuying Wang,Chang Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的信息引导机制，解决了分辨率递进式自回归图像生成模型中信息不一致的问题，大幅提升了生成图像的准确性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归的图像生成模型在逐步提高分辨率时，各patch之间会出现信息不一致，导致生成的图像特征模糊且与条件信息对不上，影响生成质量。

Method: 提出Information-Grounding Guidance (IGG)机制，在生成过程中通过注意力机制将引导信号锚定到语义重要区域，动态强化有信息量的patch，从而保证条件信息与内容紧密对齐。

Result: 在类别条件和文本到图像等多种生成任务中，IGG机制使得生成的图像更清晰、一致且语义准确，性能优于现有自回归方法，树立了新的基准。

Conclusion: IGG有效解决了自回归图像生成模型中的语义漂移和内容不符问题，是提升这类模型生成质量的有力方法。

Abstract: Autoregressive (AR) models based on next-scale prediction are rapidly
emerging as a powerful tool for image generation, but they face a critical
weakness: information inconsistencies between patches across timesteps
introduced by progressive resolution scaling. These inconsistencies scatter
guidance signals, causing them to drift away from conditioning information and
leaving behind ambiguous, unfaithful features. We tackle this challenge with
Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance
to semantically important regions through attention. By adaptively reinforcing
informative patches during sampling, IGG ensures that guidance and content
remain tightly aligned. Across both class-conditioned and text-to-image
generation tasks, IGG delivers sharper, more coherent, and semantically
grounded images, setting a new benchmark for AR-based methods.

</details>


### [173] [PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications](https://arxiv.org/abs/2509.23879)
*Hitesh Laxmichand Patel,Amit Agarwal,Srikant Panda,Hansa Meghwani,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 本文提出了首个系统化、可解释的大模型视觉上下文鲁棒性指标PCRI，并用该指标对19个先进多模态大模型在15个基准测试中进行了比较，发现大多数模型对无关背景变化仍然脆弱，仅有极少数展现出高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在真实环境中对无关或干扰视觉内容过于敏感，而目前的评价指标无法衡量这一点。因此，需要新的评价工具量化模型对视觉上下文变动的鲁棒性。

Method: 作者提出了Patch Context Robustness Index (PCRI) 指标，系统量化模型在局部图片与完整图片任务中的性能变化。作者采用PCRI对19个SOTA模型在15项视觉-语言任务上进行了实测分析。

Result: 绝大多数主流模型对背景噪声仍表现出脆弱性，仅有如InternVL2-26B和Qwen2VL-72B在不同任务下展现了稳定的上下文鲁棒性。不同模型架构对视觉上下文的敏感性也有差异，PCRI可辅助进一步分析。

Conclusion: PCRI为MLLM的上下文鲁棒性提供了标准化对比和诊断工具，有助于科学选型及未来鲁棒模型设计和训练方法的改进，促进多模态模型实际部署。

Abstract: The reliability of Multimodal Large Language Models (MLLMs) in real-world
settings is often undermined by sensitivity to irrelevant or distracting visual
context, an aspect not captured by existing evaluation metrics. We introduce
the \textbf{Patch Context Robustness Index (PCRI)}, the first systematic and
interpretable score for quantifying MLLM robustness to variations in visual
context granularity, measuring performance changes between localized image
patches and full-image input.
  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language
benchmarks, we find that most leading models remain brittle to background
noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating
consistent robustness across tasks. PCRI analysis also highlights how different
model architectures handle and integrate visual context, offering actionable
diagnostic insight for both researchers and practitioners.
  PCRI enables rigorous comparison of context robustness, supporting principled
model selection and guiding the development of future architectures and
training strategies for robust, real-world deployment.

</details>


### [174] [Learning Adaptive Pseudo-Label Selection for Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2509.23880)
*Taehun Kong,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于半监督3D目标检测的新框架，通过引入可学习的伪标签筛选模块和软监督策略，自动且自适应地挑选高质量伪标签，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 3D目标检测需要大量有标注的数据，代价高昂。半监督方法希望利用未标注数据，但现有伪标签筛选方式主要依赖人工设定阈值，忽略了上下文信息，导致伪标签质量评估不足。

Method: 本文设计了一个新的半监督3D目标检测框架，在教师网络输出端引入两个新网络，通过分数融合与上下文自适应阈值自动评估与筛选伪标签质量，并通过真实标签与伪标签的一致性进行监督。同时，提出软监督策略，使学生网络在训练时更加重视高质量标签，降低噪音伪标签的影响。

Result: 在KITTI和Waymo等主流3D目标检测数据集上进行大量实验，结果表明新方法相比现有方法能自动选出高精度伪标签且覆盖更多场景，召回率更高，并显著提升了半监督3D目标检测的整体性能。

Conclusion: 该方法通过可学习的伪标签筛选和上下文自适应机制，有效提升了半监督3D目标检测的精度与鲁棒性，为后续相关研究提供了一种新的思路。

Abstract: Semi-supervised 3D object detection (SS3DOD) aims to reduce costly 3D
annotations utilizing unlabeled data. Recent studies adopt pseudo-label-based
teacher-student frameworks and demonstrate impressive performance. The main
challenge of these frameworks is in selecting high-quality pseudo-labels from
the teacher's predictions. Most previous methods, however, select pseudo-labels
by comparing confidence scores over thresholds manually set. The latest works
tackle the challenge either by dynamic thresholding or refining the quality of
pseudo-labels. Such methods still overlook contextual information e.g. object
distances, classes, and learning states, and inadequately assess the
pseudo-label quality using partial information available from the networks. In
this work, we propose a novel SS3DOD framework featuring a learnable
pseudo-labeling module designed to automatically and adaptively select
high-quality pseudo-labels. Our approach introduces two networks at the teacher
output level. These networks reliably assess the quality of pseudo-labels by
the score fusion and determine context-adaptive thresholds, which are
supervised by the alignment of pseudo-labels over GT bounding boxes.
Additionally, we introduce a soft supervision strategy that can learn robustly
under pseudo-label noises. This helps the student network prioritize cleaner
labels over noisy ones in semi-supervised learning. Extensive experiments on
the KITTI and Waymo datasets demonstrate the effectiveness of our method. The
proposed method selects high-precision pseudo-labels while maintaining a wider
coverage of contexts and a higher recall rate, significantly improving relevant
SS3DOD methods.

</details>


### [175] [Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction](https://arxiv.org/abs/2509.23885)
*Guoquan Wei,Zekun Zhou,Liu Shi,Wenzhe Shan,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督扩散模型SuperDiff，用于低剂量CT（LDCT）重建，克服了现有方法对配对数据和泛化能力不足的问题，并且在多数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的低剂量CT去噪方法依赖于成对数据且泛化性能较差，即使是扩散模型也难以满足临床洁净数据分布学习的要求；自监督方法泛化拓展到未见剂量时表现大幅下降。

Method: 提出SuperDiff方法，核心包括：1）上下文子数据相似性自适应感知策略用于提供初始先验；2）结合知识蒸馏和潜变量扩散模型优化图像细节；3）像素级自我校正融合增强细粒度保真度，实现灵活泛化到更多剂量；4）训练及测试阶段仅需LDCT投影域数据。

Result: 在多个数据集和真实数据上完成充分的定量和定性评估，SuperDiff无论在重建质量还是泛化能力上均优于现有主流方法。

Conclusion: SuperDiff方法能实现无需成对数据的低剂量CT去噪，在泛化至不同剂量甚至未见剂量下依然保持先进性能，为临床医学成像提供新的技术手段。

Abstract: Current models based on deep learning for low-dose CT denoising rely heavily
on paired data and generalize poorly. Even the more concerned diffusion models
need to learn the distribution of clean data for reconstruction, which is
difficult to satisfy in medical clinical applications. At the same time,
self-supervised-based methods face the challenge of significant degradation of
generalizability of models pre-trained for the current dose to expand to other
doses. To address these issues, this paper proposes a novel method of
tunable-generalization diffusion powered by self-supervised contextual sub-data
for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata
similarity adaptive sensing strategy is designed for denoising centered on the
LDCT projection domain, which provides an initial prior for the subsequent
progress. Subsequently, the initial prior is used to combine knowledge
distillation with a deep combination of latent diffusion models for optimizing
image details. The pre-trained model is used for inference reconstruction, and
the pixel-level self-correcting fusion technique is proposed for fine-grained
reconstruction of the image domain to enhance the image fidelity, using the
initial prior and the LDCT image as a guide. In addition, the technique is
flexibly applied to the generalization of upper and lower doses or even unseen
doses. Dual-domain strategy cascade for self-supervised LDCT denoising,
SuperDiff requires only LDCT projection domain data for training and testing.
Full qualitative and quantitative evaluations on both datasets and real data
show that SuperDiff consistently outperforms existing state-of-the-art methods
in terms of reconstruction and generalization performance.

</details>


### [176] [AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities](https://arxiv.org/abs/2509.23888)
*Tatsuro Banno,Takehiko Ohkawa,Ruicong Liu,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了AssemblyHands-X，这是首个用于双手活动的无标记3D手-身体数据集，专注于手与身体协调对动作识别的影响。实验表明，联合建模手和身体的姿态比仅用部分或视频更高效且准确。


<details>
  <summary>Details</summary>
Motivation: 现有3D动作数据集要么只有手部要么只有身体姿态标注，缺乏能够系统研究手-身体协调对动作理解影响的高质量数据集。基于标记的采集方法又存在视觉伪影，限制了模型泛化能力。

Method: 建立了一个多视角同步视频的3D姿态注释流程，将多视角三角测量与SMPL-X网格拟合相结合，实现手与上半身的可靠3D配准。随后用不同输入（视频、手姿、身体姿势或联合姿势）在图卷积与时空注意的动作识别模型上测试。

Result: 实验显示，基于姿态的动作识别比基于原始视频更高效且准确。手和身体联合建模明显优于单独使用手部或身体提示。

Conclusion: 建模手和身体的互依动态对于全面理解双手活动极其重要。该数据集和方法推动了手-身体协调在动作识别中的系统研究。

Abstract: Bimanual human activities inherently involve coordinated movements of both
hands and body. However, the impact of this coordination in activity
understanding has not been systematically evaluated due to the lack of suitable
datasets. Such evaluation demands kinematic-level annotations (e.g., 3D pose)
for the hands and body, yet existing 3D activity datasets typically annotate
either hand or body pose. Another line of work employs marker-based motion
capture to provide full-body pose, but the physical markers introduce visual
artifacts, thereby limiting models' generalization to natural, markerless
videos. To address these limitations, we present AssemblyHands-X, the first
markerless 3D hand-body benchmark for bimanual activities, designed to study
the effect of hand-body coordination for action recognition. We begin by
constructing a pipeline for 3D pose annotation from synchronized multi-view
videos. Our approach combines multi-view triangulation with SMPL-X mesh
fitting, yielding reliable 3D registration of hands and upper body. We then
validate different input representations (e.g., video, hand pose, body pose, or
hand-body pose) across recent action recognition models based on graph
convolution or spatio-temporal attention. Our extensive experiments show that
pose-based action inference is more efficient and accurate than video
baselines. Moreover, joint modeling of hand and body cues improves action
recognition over using hands or upper body alone, highlighting the importance
of modeling interdependent hand-body dynamics for a holistic understanding of
bimanual activities.

</details>


### [177] [LifeCLEF Plant Identification Task 2015](https://arxiv.org/abs/2509.23891)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2015植物识别挑战赛评估了基于大型真实场景的植物自动识别方法，数据集包含了10万张图片，涉及1000种西欧植物，通过众包平台收集。文章介绍了资源、评估体系、参赛方法总结以及主要成果分析。


<details>
  <summary>Details</summary>
Motivation: 推动自动化植物识别方法在大规模、接近实际生物多样性监测场景中的应用与发展，填补传统小规模植物识别研究与实际需求之间的鸿沟。

Method: 利用2011年启动的大规模众包采集平台收集植物图像，构建包含100K图片、1000种西欧植物的数据集。举办竞赛，邀请多组研究者基于该数据集提交不同识别方法与系统，对比其性能并加以评测。

Result: 吸引了众多研究小组参与，展示了多种前沿植物识别方法的效果，系统性评估了各方法在大规模、真实场景下的表现，同时揭示了当前解决方案的优劣。

Conclusion: 大型众包数据集有助于更真实、全面地评估植物识别技术，参与团队的方法显示出自动化识别的巨大潜力，但也暴露了复杂自然环境下的挑战，有待进一步研究提升识别良率和适应性。

Abstract: The LifeCLEF plant identification challenge aims at eval- uating plant
identification methods and systems at a very large scale, close to the
conditions of a real-world biodiversity monitoring scenario. The 2015
evaluation was actually conducted on a set of more than 100K images
illustrating 1000 plant species living in West Europe. The main originality of
this dataset is that it was built through a large-scale partic- ipatory sensing
plateform initiated in 2011 and which now involves tens of thousands of
contributors. This overview presents more precisely the resources and
assessments of the challenge, summarizes the approaches and systems employed by
the participating research groups, and provides an analysis of the main
outcomes.

</details>


### [178] [Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios](https://arxiv.org/abs/2509.23895)
*Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li*

Main category: cs.CV

TL;DR: 提出了一种新颖的跨模态对比遗忘（CCU）框架，以解决视觉模态数据隐私泄漏和遗忘过程中保持多模态性能的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉模态在实际多模态应用（如自动驾驶）中容易发生隐私泄漏，现有的机器遗忘（Machine Unlearning）方法不能很好地维持跨模态知识和保留数据的结构稳定性，导致整体和其他模态性能下降。

Method: 提出CCU框架：1) 选择性视觉遗忘，利用逆对比学习使视觉表示与原始语义解耦；2) 跨模态知识保持，通过语义一致性维持其他模态判别能力；3) 双集合对比分离，隔离遗忘与保留集合间的结构扰动以保持模型性能。

Result: 在三个数据集上进行大量实验，CCU方法在只需7%遗忘时间的情况下，实现了比最优基线高7.12%的准确率提升。

Conclusion: CCU在快速高效解决视觉模态数据遗忘的同时，兼顾了整体和其他模态的性能，优于现有方法，适合需要高效隐私保护的多模态应用。

Abstract: Visual modality is the most vulnerable to privacy leakage in real-world
multimodal applications like autonomous driving with visual and radar data;
Machine unlearning removes specific training data from pre-trained models to
address privacy leakage, however, existing methods fail to preserve cross-modal
knowledge and maintain intra-class structural stability of retain data, leading
to reduced overall and other modalities' performance during visual unlearning;
to address these challenges, we propose a Cross-modal Contrastive Unlearning
(CCU) framework, which integrates three key components: (a) selective visual
unlearning: employing inverse contrastive learning to dissociate visual
representations from their original semantics, (b) cross-modal knowledge
retention: preserving other modalities' discriminability through semantic
consistency, and (c) dual-set contrastive separation: preserving the model
performance via isolation of structural perturbations between the unlearn set
and retain set; extensive experiments on three datasets demonstrate the
superiority of CCU, and our method achieves a 7.12% accuracy improvement with
only 7% of the unlearning time compared to the top-accuracy baseline.

</details>


### [179] [Q-FSRU: Quantum-Augmented Frequency-Spectral For Medical Visual Question Answering](https://arxiv.org/abs/2509.23899)
*Rakesh Thakur,Yusra Tariq,Rakesh Chandra Joshi*

Main category: cs.CV

TL;DR: 本文提出了Q-FSRU模型，将频域特征融合与量子检索机制结合，用于提高医学视觉问答任务（VQA）的性能和可解释性。模型在VQA-RAD数据集上表现优于先前方法，特别是在需要综合图像与文本推理的复杂问题上。


<details>
  <summary>Details</summary>
Motivation: 医学领域内，处理需兼顾图像与文本理解的复杂临床问题仍是一大挑战。现有VQA模型在高复杂性和可解释性需求下，效果有限，因此需要新的方法提升推理能力和答案质量。

Method: Q-FSRU模型将医学图像和相关文本特征通过快速傅立叶变换（FFT）转化到频域，以提取更有意义的信息并过滤噪声。同时引入量子检索增强生成（Quantum RAG）方法，利用量子相似性检索外部医学知识并融合至特征推理流程中。

Result: 在VQA-RAD（包含实际放射学影像及相应问题）数据集上评测结果显示，Q-FSRU不仅整体优于以往模型，尤其在复杂影像文本联合推理任务上提升明显。

Conclusion: Q-FSRU创新性将频谱特征与量子检索机制结合，提升了医学VQA的准确率和解释性。该方法展示出构建智能、透明、实用AI临床助手的潜力。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [180] [LifeCLEF Plant Identification Task 2014](https://arxiv.org/abs/2509.23900)
*Herve Goeau,Alexis Joly,Pierre Bonnet,Souheil Selmi,Jean-Francois Molino,Daniel Barthelemy,Nozha Boujemaa*

Main category: cs.CV

TL;DR: 本论文介绍了LifeCLEFs植物识别任务，评估了约500种树木和草本植物的识别方法，数据集由公民科学项目收集，真实度高，并分析了各团队的检索方法和结果。


<details>
  <summary>Details</summary>
Motivation: 植物自动识别在生物多样性保护和实际应用中具重要意义，但现有公开标准化测试数据有限，缺乏模拟真实应用环境的数据集，因此需要一个接近真实场景的大规模测试平台。

Method: 任务使用由法国公民科学项目Tela Botanica收集的植物图片数据，包含7种图片类型，参赛团队需提交各自的检索与识别方案，通过标准评估方法对结果进行比较和分析。

Result: 共有来自6个国家的10个团队参与，提交了27次各具特色的方法，经过对识别结果的详细分析，展现了当前图像与多媒体检索技术在植物识别上的能力和局限。

Conclusion: 本次任务反映了多媒体检索领域对生物多样性与植物学的研究兴趣，突出显示了植物自动识别的挑战性，并为未来相关研究提供了宝贵的数据资源和参考基线。

Abstract: The LifeCLEFs plant identification task provides a testbed for a
system-oriented evaluation of plant identification about 500 species trees and
herbaceous plants. Seven types of image content are considered: scan and
scan-like pictures of leaf, and 6 kinds of detailed views with un- constrained
conditions, directly photographed on the plant: flower, fruit, stem & bark,
branch, leaf and entire view. The main originality of this data is that it was
specifically built through a citizen sciences initiative conducted by Tela
Botanica, a French social network of amateur and expert botanists. This makes
the task closer to the conditions of a real- world application. This overview
presents more precisely the resources and assessments of task, summarizes the
retrieval approaches employed by the participating groups, and provides an
analysis of the main eval- uation results. With a total of ten groups from six
countries and with a total of twenty seven submitted runs, involving distinct
and original methods, this fourth year task confirms Image & Multimedia
Retrieval community interest for biodiversity and botany, and highlights
further challenging studies in plant identification.

</details>


### [181] [EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging](https://arxiv.org/abs/2509.23906)
*Anoushka Harit,William Prew,Zhongtian Sun,Florian Markowetz*

Main category: cs.CV

TL;DR: 该论文提出了一种用于医学影像基础模型的持续学习框架，无需保存患者数据样本，在保证隐私和高效的前提下，有效减少灾难性遗忘并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像模型需要不断适应新数据，但完整的重训练受限于数据隐私和高成本，因此亟需高效且保护隐私的持续学习方法。

Method: 提出结合类条件扩散重放（class conditional diffusion replay）与弹性权重固化（Elastic Weight Consolidation, EWC）的方法，并采用紧凑的Vision Transformer骨干网络，对MedMNIST v2和CheXpert任务进行评估。

Result: 在CheXpert数据集上，方法达到0.851 AUROC，将遗忘减少30%以上，接近联合训练（0.869 AUROC），同时维持高效与隐私保护。分析显示模型遗忘与重放的保真度及参数漂移密切相关，验证了扩散重放与参数稳定性的互补作用。

Conclusion: 该方法为临床影像模型可扩展、隐私友好的持续适应提供了切实可行的路径，推动医学影像AI的实际落地和持续优化。

Abstract: Medical imaging foundation models must adapt over time, yet full retraining
is often blocked by privacy constraints and cost. We present a continual
learning framework that avoids storing patient exemplars by pairing class
conditional diffusion replay with Elastic Weight Consolidation. Using a compact
Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and
CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by
more than 30\% relative to DER\texttt{++}, and approaches joint training at
0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect
forgetting to two measurable factors: fidelity of replay and Fisher weighted
parameter drift, highlighting the complementary roles of replay diffusion and
synaptic stability. The results indicate a practical route for scalable,
privacy aware continual adaptation of clinical imaging models.

</details>


### [182] [Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation](https://arxiv.org/abs/2509.23907)
*You Zhou,Lijiang Chen,Shuchang Lyu,Guangxia Cui,Wenpei Bai,Zheng Zhou,Meng Li,Guangliang Cheng,Huiyu Zhou,Qi Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦领域自适应（FedDA）分割训练框架，用于解决医疗图像分割中因不同客户端拥有不同模态数据带来的异质性问题。该方法通过特征级对抗学习实现跨客户端特征对齐，提升了模型的跨域泛化能力。实验证明FedDA在多模态数据、多个数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗数据隐私与数据孤岛问题促使联邦学习的发展，但现实中不同医疗机构拥有不同模态的医学图像，导致数据分布不一致（领域异质性）。现有联邦学习方法难以有效整合不同模态数据，严重影响模型性能与泛化能力，亟需新的方法突破这一难题。

Method: 提出FedDA框架，在联邦学习中引入了特征级的对抗训练机制。具体做法为：通过对抗损失，对来自不同客户端（模态）的特征图进行对齐，使模型对不同模态数据具备更好的适应与泛化能力。训练过程嵌入对抗机制，促进模型学习跨域共享表征。

Result: 在三个医学图像数据集上进行了全面实验，FedDA模型表现出显著的跨域泛化能力。单一模态客户端也能获得跨模态的处理能力，且在客观评测指标和主观评测方面均优于现有主流联邦聚合算法。

Conclusion: FedDA能够有效缓解联邦医疗图像分割中的模态异质性问题，实现更有效的跨域联邦聚合，提升了不同模态下的分割性能，为实际多中心医疗协作提供了新的解决思路。

Abstract: Federated learning enables collaborative training of machine learning models
among different clients while ensuring data privacy, emerging as the mainstream
for breaking data silos in the healthcare domain. However, the imbalance of
medical resources, data corruption or improper data preservation may lead to a
situation where different clients possess medical images of different modality.
This heterogeneity poses a significant challenge for cross-domain medical image
segmentation within the federated learning framework. To address this
challenge, we propose a new Federated Domain Adaptation (FedDA) segmentation
training framework. Specifically, we propose a feature-level adversarial
learning among clients by aligning feature maps across clients through
embedding an adversarial training mechanism. This design can enhance the
model's generalization on multiple domains and alleviate the negative impact
from domain-shift. Comprehensive experiments on three medical image datasets
demonstrate that our proposed FedDA substantially achieves cross-domain
federated aggregation, endowing single modality client with cross-modality
processing capabilities, and consistently delivers robust performance compared
to state-of-the-art federated aggregation algorithms in objective and
subjective assessment. Our code are available at
https://github.com/GGbond-study/FedDA.

</details>


### [183] [EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling](https://arxiv.org/abs/2509.23909)
*Xin Luo,Jiahao Wang,Chenyuan Wu,Shitao Xiao,Xiyan Jiang,Defu Lian,Jiajun Zhang,Dong Liu,Zheng liu*

Main category: cs.CV

TL;DR: 本论文提出了一套完整的从基准测试到奖励建模再到强化学习训练的图像编辑流程，并证明了高保真度、专业化的奖励模型是实现强化学习在图像编辑任务中突破的关键。


<details>
  <summary>Details</summary>
Motivation: 尽管基于指令的图像编辑取得了显著进展，但面对复杂指令时现有方法效果有限，且通常需要多次采样才能得到满意结果。强化学习能够缓解此问题，但由于缺乏高效高保真的奖励信号，强化学习在图像编辑中的应用受限。

Method: 本文首先提出了系统性奖励模型评测基准EditReward-Bench，然后构建了多个规模的奖励模型EditScore（含7B至72B参数量），通过精细化数据筛选提升其匹配主流闭源VLM性能，并通过自我集成策略提升表现；验证高保真奖励模型在Unlock RL for image editing的重要性，并将方法应用在OmniGen2模型上。

Result: EditScore奖励模型在EditReward-Bench基准上表现优异，最大规模的版本甚至超越了GPT-5。EditScore提供了有效的学习信号，使基于RL的图像编辑策略优化变得高效且鲁棒。系统性实验表明该方法带来了显著且稳定的性能提升。

Conclusion: 高保真、领域专业化的奖励模型是强化学习充分释放在图像编辑领域潜力的关键，本文方法实现了系统的评测、建模及强化学习一体化流程，为相关研究提供了范式。

Abstract: Instruction-guided image editing has achieved remarkable progress, yet
current models still face challenges with complex instructions and often
require multiple samples to produce a desired result. Reinforcement Learning
(RL) offers a promising solution, but its adoption in image editing has been
severely hindered by the lack of a high-fidelity, efficient reward signal. In
this work, we present a comprehensive methodology to overcome this barrier,
centered on the development of a state-of-the-art, specialized reward model. We
first introduce EditReward-Bench, a comprehensive benchmark to systematically
evaluate reward models on editing quality. Building on this benchmark, we
develop EditScore, a series of reward models (7B-72B) for evaluating the
quality of instruction-guided image editing. Through meticulous data curation
and filtering, EditScore effectively matches the performance of learning
proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy
tailored for the generative nature of EditScore, our largest variant even
surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity
reward model is the key to unlocking online RL for image editing. Our
experiments show that, while even the largest open-source VLMs fail to provide
an effective learning signal, EditScore enables efficient and robust policy
optimization. Applying our framework to a strong base model, OmniGen2, results
in a final model that shows a substantial and consistent performance uplift.
Overall, this work provides the first systematic path from benchmarking to
reward modeling to RL training in image editing, showing that a high-fidelity,
domain-specialized reward model is the key to unlocking the full potential of
RL in this domain.

</details>


### [184] [MoReact: Generating Reactive Motion from Textual Descriptions](https://arxiv.org/abs/2509.23911)
*Xiyan Xu,Sirui Xu,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于文本驱动的人体反应生成方法MoReact，通过分离全局轨迹与局部动作的生成，实现了更真实、可控且符合文本描述的两人交互动作建模。


<details>
  <summary>Details</summary>
Motivation: 现有的人体反应生成方法，要么将多个人视为一个整体，缺乏个体适应性，要么只用一方动作生成另一方反应，忽略了丰富的语义信息，致使对多样和动态交互情景的应对不足。

Method: 提出了一种基于扩散模型（diffusion-based）的MoReact方法，先生成全局轨迹再生成局部动作，并引入新的交互损失函数提升近距离交互的真实感。整个过程结合了动作对手和文本描述，确保生成结果动作与语义高度契合。

Result: 在基于两人动作数据集上实验，MoReact可生成更加真实、多样且可控的反应动作，不仅与对手动作匹配良好，也能依照文本指导实现合理交互。

Conclusion: 该方法突破了现有模型的局限，显著提升了人体反应动作的多样性、真实感和可控性，为计算机视觉及人机交互等方向提供了新的解决思路。

Abstract: Modeling and generating human reactions poses a significant challenge with
broad applications for computer vision and human-computer interaction. Existing
methods either treat multiple individuals as a single entity, directly
generating interactions, or rely solely on one person's motion to generate the
other's reaction, failing to integrate the rich semantic information that
underpins human interactions. Yet, these methods often fall short in adaptive
responsiveness, i.e., the ability to accurately respond to diverse and dynamic
interaction scenarios. Recognizing this gap, our work introduces an approach
tailored to address the limitations of existing models by focusing on
text-driven human reaction generation. Our model specifically generates
realistic motion sequences for individuals that responding to the other's
actions based on a descriptive text of the interaction scenario. The goal is to
produce motion sequences that not only complement the opponent's movements but
also semantically fit the described interactions. To achieve this, we present
MoReact, a diffusion-based method designed to disentangle the generation of
global trajectories and local motions sequentially. This approach stems from
the observation that generating global trajectories first is crucial for
guiding local motion, ensuring better alignment with given action and text.
Furthermore, we introduce a novel interaction loss to enhance the realism of
generated close interactions. Our experiments, utilizing data adapted from a
two-person motion dataset, demonstrate the efficacy of our approach for this
novel task, which is capable of producing realistic, diverse, and controllable
reactions that not only closely match the movements of the counterpart but also
adhere to the textual guidance. Please find our webpage at
https://xiyan-xu.github.io/MoReactWebPage.

</details>


### [185] [Revisit the Imbalance Optimization in Multi-task Learning: An Experimental Analysis](https://arxiv.org/abs/2509.23915)
*Yihang Guo,Tianyuan Yu,Liang Bai,Yanming Guo,Yirun Ruan,William Li,Weishi Zheng*

Main category: cs.CV

TL;DR: 本文系统性分析了多任务学习（MTL）中优化失衡问题的成因，并提出只需简单按梯度范数缩放任务损失，即可带来与复杂调参相当的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多任务学习虽然能提升视觉系统的通用性，但受制于任务间相互干扰，往往不如单任务表现好，急需厘清导致该现象的根本原因并寻找更优解决方案。

Method: 作者在不同数据集及主流优化方法上进行了系统性实验，深入分析现有方法在优化任务损失权重时的表现，并研究了基础模型强初始化和数据量扩展对优化平衡的作用。同时，定量追踪任务梯度范数，揭示优化失衡与梯度范数之间的关联，并根据梯度范数自适应缩放损失权重。

Result: 实验证明：1）现有方法在不同数据集下表现不一致且依赖于耗时的损失权重调参。2）强初始化和大规模数据并不能根除优化失衡。3）任务梯度范数与优化失衡高度相关，采用梯度范数自动调节损失权重，能在不搜索参数的前提下取得与最优调参方案相当的结果。

Conclusion: 与其不断设计更复杂的多任务优化方法，不如直接关注和控制梯度动态，利用梯度范数理顺任务间优化过程，提升多任务学习稳定性和效果。

Abstract: Multi-task learning (MTL) aims to build general-purpose vision systems by
training a single network to perform multiple tasks jointly. While promising,
its potential is often hindered by "unbalanced optimization", where task
interference leads to subpar performance compared to single-task models. To
facilitate research in MTL, this paper presents a systematic experimental
analysis to dissect the factors contributing to this persistent problem. Our
investigation confirms that the performance of existing optimization methods
varies inconsistently across datasets, and advanced architectures still rely on
costly grid-searched loss weights. Furthermore, we show that while powerful
Vision Foundation Models (VFMs) provide strong initialization, they do not
inherently resolve the optimization imbalance, and merely increasing data
quantity offers limited benefits. A crucial finding emerges from our analysis:
a strong correlation exists between the optimization imbalance and the norm of
task-specific gradients. We demonstrate that this insight is directly
applicable, showing that a straightforward strategy of scaling task losses
according to their gradient norms can achieve performance comparable to that of
an extensive and computationally expensive grid search. Our comprehensive
analysis suggests that understanding and controlling gradient dynamics is a
more direct path to stable MTL than developing increasingly complex methods.

</details>


### [186] [Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives](https://arxiv.org/abs/2509.23917)
*Kuanrong Liu,Siyuan Liang,Cheng Qian,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文分析了CLIP模型在多任务（图像文本检索、目标检测和语义分割）下，对抗样本的跨任务迁移行为，并提出了MT-AdvCLIP框架以提升对抗样本的迁移攻击能力。实验表明，该方法在不增加扰动预算的情况下大幅提升了多任务的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然在图像-文本检索等粗粒度任务表现优秀，但在细粒度任务如目标检测和语义分割中表现较弱，对抗鲁棒性研究也不充分。研究CLIP在多任务下对抗样本的迁移性，能更好地评估其泛化极限和安全风险。

Method: 作者对CLIP及其衍生模型在多任务下（图像文本检索、目标检测和语义分割）生成和迁移对抗样本进行了系统实证分析。基于发现，提出了MT-AdvCLIP框架，通过任务感知的特征聚合损失，生成更具跨任务泛化能力的对抗扰动。

Result: 实验表明，MT-AdvCLIP在多个公开数据集上，能在不增加扰动预算的情况下，将对抗样本在多任务中的攻击成功率平均提升超过39%。特别是在使用细粒度任务生成的对抗样本时，迁移攻击效果更佳。

Conclusion: 本文系统揭示了CLIP多任务模型中对抗样本的迁移机制，并提出了一种有效提升跨任务对抗攻击能力的新框架，为多任务鲁棒性评估和对抗样本设计提供了新视角。

Abstract: As a general-purpose vision-language pretraining model, CLIP demonstrates
strong generalization ability in image-text alignment tasks and has been widely
adopted in downstream applications such as image classification and image-text
retrieval. However, it struggles with fine-grained tasks such as object
detection and semantic segmentation. While many variants aim to improve CLIP on
these tasks, its robustness to adversarial perturbations remains underexplored.
Understanding how adversarial examples transfer across tasks is key to
assessing CLIP's generalization limits and security risks. In this work, we
conduct a systematic empirical analysis of the cross-task transfer behavior of
CLIP-based models on image-text retrieval, object detection, and semantic
segmentation under adversarial perturbations. We find that adversarial examples
generated from fine-grained tasks (e.g., object detection and semantic
segmentation) often exhibit stronger transfer potential than those from
coarse-grained tasks, enabling more effective attacks against the original CLIP
model. Motivated by this observation, we propose a novel framework, Multi-Task
Adversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature
aggregation loss and generates perturbations with enhanced cross-task
generalization capability. This design strengthens the attack effectiveness of
fine-grained task models on the shared CLIP backbone. Experimental results on
multiple public datasets show that MT-AdvCLIP significantly improves the
adversarial transfer success rate (The average attack success rate across
multiple tasks is improved by over 39%.) against various CLIP-derived models,
without increasing the perturbation budget. This study reveals the transfer
mechanism of adversarial examples in multi-task CLIP models, offering new
insights into multi-task robustness evaluation and adversarial example design.

</details>


### [187] [Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models](https://arxiv.org/abs/2509.23919)
*Longtao Jiang,Mingfei Han,Lei Chen,Yongqiang Yu,Feng Zhao,Xiaojun Chang,Zhihui Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Mask AutoRegressive（MAR）模型的文本引导图像修复新方法Token Painter，实现了更加准确的文本内容融入和背景一致性。通过两个关键模块，提升了修复区域与文本和背景的协调性，且无需额外训练，在各项指标上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在潜变量空间建模整个图像，导致修复区域难与文本提示精确对齐，并且与原有背景难以保持一致性。如何同时提升文本内容结合度与背景协调性，是亟需解决的问题。

Method: 方法基于MAR模型，提出了Token Painter方案，无需训练。其核心包括：（1）Dual-Stream Encoder Information Fusion（DEIF），用频域信息融合文本与背景语义生成引导token，实现内容与语境的双重约束；（2）Adaptive Decoder Attention Score Enhancing（ADAE），自适应提升对引导token和修复token的注意力分数，增强细节对齐和视觉质量。

Result: 通过广泛实验，该方法在几乎所有评测指标上都优于现有SOTA算法，且生成的修复图像在视觉效果上也更佳。

Conclusion: Token Painter有效提升了文本一致性和背景和谐度，在无需额外训练的前提下超越了当前主流方法，为文本引导图像修复提供了更优的解决方案。

Abstract: Text-guided image inpainting aims to inpaint masked image regions based on a
textual prompt while preserving the background. Although diffusion-based
methods have become dominant, their property of modeling the entire image in
latent space makes it challenging for the results to align well with prompt
details and maintain a consistent background. To address these issues, we
explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports
image inpainting by generating latent tokens corresponding to mask regions,
enabling better local controllability without altering the background. However,
directly applying MAR to this task makes the inpainting content either ignore
the prompts or be disharmonious with the background context. Through analysis
of the attention maps from the inpainting images, we identify the impact of
background tokens on text tokens during the MAR generation, and leverage this
to design \textbf{Token Painter}, a training-free text-guided image inpainting
method based on MAR. Our approach introduces two key components: (1)
Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and
context information from text and background in frequency domain to produce
novel guidance tokens, allowing MAR to generate text-faithful inpainting
content while keeping harmonious with background context. (2) Adaptive Decoder
Attention Score Enhancing (ADAE), which adaptively enhances attention scores on
guidance tokens and inpainting tokens to further enhance the alignment of
prompt details and the content visual quality. Extensive experiments
demonstrate that our training-free method outperforms prior state-of-the-art
methods across almost all metrics and delivers superior visual results. Codes
will be released.

</details>


### [188] [Learning Encoding-Decoding Direction Pairs to Unveil Concepts of Influence in Deep Vision Networks](https://arxiv.org/abs/2509.23926)
*Alexandros Doumanoglou,Kurt Driessens,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，揭示深度视觉网络如何通过潜在空间方向（概念嵌入）编码、解码输入中的语义概念，从而提升模型解释性、可调试性和纠错能力。


<details>
  <summary>Details</summary>
Motivation: 深度视觉网络普遍被视为“黑箱”，缺乏可解释性。本研究希望解析网络内部，理解其是如何表征和组合语义概念，并以此提升网络的可解释性、调试和改进模型预测。

Method: 作者将每个语义概念表征为潜在空间中的一个方向（概念嵌入），引入编码方向与解码方向的配对思路。不同于传统特征重构方法，本文提出通过激活的方向性聚类来识别解码方向，通过概率角度下的信号向量来估算编码方向，并创新性利用了一种称为“不确定区域对齐”（Uncertainty Region Alignment）的技术结合网络权重，发现影响预测的可解释方向。

Result: 实验表明，方法在合成数据集上可以准确恢复真实的方向对；在真实数据上，获得的解码方向与语义清晰概念高度对应，超越了无监督基线方法。同时信号向量对编码方向的估计在激活最大化验证下是准确的。

Conclusion: 最终，该方法不仅能够帮助理解模型整体和单个预测的行为，还可以通过对概念的（去）学习，实现模型预测的纠错和生成反事实解释。

Abstract: Empirical evidence shows that deep vision networks represent concepts as
directions in latent space, vectors we call concept embeddings. Each concept
has a latent factor-a scalar-indicating its presence in an input patch. For a
given patch, multiple latent factors are encoded into a compact representation
by linearly combining concept embeddings, with the factors as coefficients.
Since these embeddings enable such encoding, we call them encoding directions.
A latent factor can be recovered via the inner product with a filter, a vector
we call a decoding direction. These encoding-decoding direction pairs are not
directly accessible, but recovering them helps open the black box of deep
networks, enabling understanding, debugging, and improving models. Decoder
directions attribute meaning to latent codes, while encoding directions assess
concept influence on predictions, with both enabling model correction by
unlearning irrelevant concepts. Unlike prior matrix decomposition, autoencoder,
or dictionary learning methods that rely on feature reconstruction, we propose
a new perspective: decoding directions are identified via directional
clustering of activations, and encoding directions are estimated with signal
vectors under a probabilistic view. We further leverage network weights through
a novel technique, Uncertainty Region Alignment, which reveals interpretable
directions affecting predictions. Our analysis shows that (a) on synthetic
data, our method recovers ground-truth direction pairs; (b) on real data,
decoding directions map to monosemantic, interpretable concepts and outperform
unsupervised baselines; and (c) signal vectors faithfully estimate encoding
directions, validated via activation maximization. Finally, we demonstrate
applications in understanding global model behavior, explaining individual
predictions, and intervening to produce counterfactuals or correct errors.

</details>


### [189] [SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing](https://arxiv.org/abs/2509.23927)
*Yi Yang,Xiaokun Zhang,Qingchen Fang,Ziqi Ye,Rui Li,Li Liu,Haipeng Wang*

Main category: cs.CV

TL;DR: 本论文提出了首个通用SAR多模态基础模型SAR-KnowLIP，填补了合成孔径雷达（SAR）影像在多模态智能领域的空白，并附带大规模数据集与标准化评测基准。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态人工智能近年来取得进展，但主要集中于RGB影像，合成孔径雷达影像则长期被忽视。而SAR凭借全天候成像能力在遥感场景理解中有独特价值，因此迫切需要一种适用于SAR的多模态基础模型及相关数据与评测体系。

Method: 1）首次引入并构建具地理信息投影属性的大规模SAR数据集SAR-GEOVL-1M（含120,000幅图片/135城市）；2）基于分层认知的chain-of-thought方法，生成结构化语义标签，覆盖地貌、区域功能、目标属性与空间关系；3）提出自洽迭代优化机制，通过自监督闭环（对比、匹配、重构）不断提升跨模态对齐效果；4）建立涵盖11个视觉与视觉-语言任务的统一评测基准，与14个主流基础模型比较。

Result: SAR-KnowLIP在物体计数和土地覆盖分类等任务上表现优异，显著超过现有主流多模态基础模型。

Conclusion: SAR-KnowLIP通过大规模多模态数据、可迁移模型架构和系统性评测，有望极大推动SAR多模态基础模型的研究与应用发展。

Abstract: Cross-modal artificial intelligence has garnered widespread attention in
recent years, achieving significant progress in the study of natural images.
However, existing methods are mostly designed for RGB imagery, leaving a
significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with
its all-day, all-weather imaging capabilities, plays an irreplaceable role in
remote sensing scene understanding. To address this gap, this paper proposes
SAR-KnowLIP, the first universal SAR multimodal foundational model, along with
reusable data and evaluation baselines. Specifically: (1) This work introduces
the critical yet long-overlooked attribute of geographic information into
remote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR
dataset with complete geographic projection properties), covering multiple
satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured
text is generated through a hierarchical cognitive chain-of-thought (HCoT),
providing more than one million multi-dimensional semantic annotations of
landforms, regional functions, target attributes, and spatial relationships.
(3) We design a Self-Consistent Iterative Optimization mechanism that
continuously enhances cross-modal alignment through a self-supervised closed
loop of contrastive, matching, and reconstruction learning on a transferable
multimodal encoder. (4) A unified evaluation benchmark is established across 11
representative downstream vision and vision-language tasks, with comparisons
against 14 leading foundation models, where SAR-KnowLIP demonstrates leading
performance, particularly in object counting and land-cover classification. We
expect that SAR-KnowLIP's large-scale multimodal data, transferable model
architecture, and comprehensive experimental benchmark will significantly
advance the development of SAR multimodal baseline models.

</details>


### [190] [AutoPrune: Each Complexity Deserves a Pruning Policy](https://arxiv.org/abs/2509.23931)
*Hanshi Wang,Yuhao Xu,Zekun Xu,Jin Gao,Yufan Liu,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种适应复杂度的视觉Token剪枝方法AutoPrune，在大规模视觉-语言模型中按样本与任务复杂度智能地剪枝，大幅降低计算消耗，同时几乎不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型中的视觉token存在大量冗余。传统剪枝方法通常依赖启发式和固定的剪枝策略，无法适应不同输入和任务的多样复杂性，未能与模型的整体推理过程高度协同，这限制了计算资源的优化能力。

Method: 作者提出了AutoPrune框架，无需额外训练，可作为即插即用的剪枝方式。AutoPrune通过量化视觉token与文本token的互信息，并将该信号映射到受预算约束的Logistic保留曲线上，为不同复杂度的样本和任务自适应地分配剪枝策略，并确保计算资源限制下的最优性能。

Result: 在视觉-语言标准任务和自动驾驶的视觉-语言-动作模型上，AutoPrune在LLaVA-1.5-7B模型上实现了剪掉89%的视觉token，推理FLOPs减少76.8%，同时保持96.7%的原始准确率，优于最新的PDrop方法9.1%。

Conclusion: AutoPrune根据样本和任务复杂度自适应分配剪枝策略，能极大提升视觉-语言模型的推理效率且基本不影响性能，具有很强的实际应用前景。

Abstract: The established redundancy in visual tokens within large vision-language
models allows pruning to effectively reduce their substantial computational
demands. Previous methods typically employ heuristic layer-specific pruning
strategies where, although the number of tokens removed may differ across
decoder layers, the overall pruning schedule is fixed and applied uniformly to
all input samples and tasks, failing to align token elimination with the
model's holistic reasoning trajectory. Cognitive science indicates that human
visual processing often begins with broad exploration to accumulate evidence
before narrowing focus as the target becomes distinct. Our experiments reveal
an analogous pattern in these models. This observation suggests that neither a
fixed pruning schedule nor a heuristic layer-wise strategy can optimally
accommodate the diverse complexities inherent in different inputs. To overcome
this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a
training-free, plug-and-play framework that tailors pruning policies to varying
sample and task complexities. Specifically, AutoPrune quantifies the mutual
information between visual and textual tokens, then projects this signal to a
budget-constrained logistic retention curve. Each such logistic curve, defined
by its unique shape, corresponds to the specific complexity of different tasks
and can guarantee adherence to predefined computational constraints. We
evaluate AutoPrune on standard vision-language tasks and on
Vision-Language-Action models for autonomous driving. Notably, when applied to
LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference
FLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all
tasks. This corresponds to a 9.1% improvement over the recent work PDrop,
demonstrating the effectiveness. Code is available at
https://github.com/AutoLab-SAI-SJTU/AutoPrune.

</details>


### [191] [CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting](https://arxiv.org/abs/2509.23947)
*Dragoş-Andrei Chileban,Andrei-Ştefan Bulzan,Cosmin Cernǎzanu-Glǎvan*

Main category: cs.CV

TL;DR: 本论文提出了一个基于3D高斯喷洒（3D-GS）的自动汽车损伤检测管线，能够从有限视角实现高效的3D损伤分割，提升了损伤评估的精度与全面性。


<details>
  <summary>Details</summary>
Motivation: 现有自动车损检测主要依赖2D图像，难以准确反映损伤的空间分布和几何特征。最新的3D重建方法，尤其是3D高斯喷洒（3D-GS）技术，可以用少量视角重建出真实的三维形态，为更细致和准确的损伤检测带来了新机会。

Method: 1. 首先利用SfM（结构光束）获得摄像参数和三维场景；2. 通过提升2D分割掩码至3D空间，实现损伤空间定位与分割；3. 提出不依赖学习的单视角3D-GS分割方法，将高斯体投影到图像平面，结合Z-buffering与深度、透明度正态分布模型，筛选和定位损伤区域。

Result: 实验证明，该方法在车损难以多视角捕捉，仅有单视角明确可见的极端场景下依然表现出色，能够高效分割划痕、小凹陷等微小损伤。

Conclusion: 作者提出的自动车损3D检测方法，无需多视角一致性，仅凭单视角即可实现有效损伤分割，为工业应用带来了易用、鲁棒且精准的解决方案，且已开源实现。

Abstract: Automatic car damage detection has been a topic of significant interest for
the auto insurance industry as it promises faster, accurate, and cost-effective
damage assessments. However, few works have gone beyond 2D image analysis to
leverage 3D reconstruction methods, which have the potential to provide a more
comprehensive and geometrically accurate representation of the damage.
Moreover, recent methods employing 3D representations for novel view synthesis,
particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to
generate accurate and coherent 3D reconstructions from a limited number of
views. In this work we introduce an automatic car damage detection pipeline
that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we
propose a simple yet effective learning-free approach for single-view 3D-GS
segmentation. Specifically, Gaussians are projected onto the image plane using
camera parameters obtained via Structure from Motion (SfM). They are then
filtered through an algorithm that utilizes Z-buffering along with a normal
distribution model of depth and opacities. Through experiments we found that
this method is particularly effective for challenging scenarios like car damage
detection, where target objects (e.g., scratches, small dents) may only be
clearly visible in a single view, making multi-view consistency approaches
impractical or impossible. The code is publicly available at:
https://github.com/DragosChileban/CrashSplat.

</details>


### [192] [HunyuanImage 3.0 Technical Report](https://arxiv.org/abs/2509.23951)
*Siyu Cao,Hangting Chen,Peng Chen,Yiji Cheng,Yutao Cui,Xinchi Deng,Ying Dong,Kipper Gong,Tianpeng Gu,Xiusen Gu,Tiankai Hang,Duojun Huang,Jie Jiang,Zhengkai Jiang,Weijie Kong,Changlin Li,Donghao Li,Junzhe Li,Xin Li,Yang Li,Zhenxi Li,Zhimin Li,Jiaxin Lin,Linus,Lucaz Liu,Shu Liu,Songtao Liu,Yu Liu,Yuhong Liu,Yanxin Long,Fanbin Lu,Qinglin Lu,Yuyang Peng,Yuanbo Peng,Xiangwei Shen,Yixuan Shi,Jiale Tao,Yangyu Tao,Qi Tian,Pengfei Wan,Chunyu Wang,Kai Wang,Lei Wang,Linqing Wang,Lucas Wang,Qixun Wang,Weiyan Wang,Hao Wen,Bing Wu,Jianbing Wu,Yue Wu,Senhao Xie,Fang Yang,Miles Yang,Xiaofeng Yang,Xuan Yang,Zhantao Yang,Jingmiao Yu,Zheng Yuan,Chao Zhang,Jian-Wei Zhang,Peizhen Zhang,Shi-Xue Zhang,Tao Zhang,Weigang Zhang,Yepeng Zhang,Yingfang Zhang,Zihao Zhang,Zijian Zhang,Penghao Zhao,Zhiyuan Zhao,Xuefei Zhe,Jianchen Zhu,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanImage 3.0 是一个统一多模态理解和生成的大模型，具备业内领先的图像生成能力，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在理解和生成之间存在割裂，且大规模高性能、开源的生成模型较少。作者希望构建一个强大的、统一的多模态生成模型，并对外开放，推动整个社区的发展。

Method: 作者提出了HunyuanImage 3.0，基于自回归框架，整合了精细化数据筛选、架构设计、本地化思维链机制、分阶段预训练和后训练，以及高效的训练推理基础设施。模型采用了超大规模的Mixture-of-Experts机制，总参数量超过800亿，推理时激活130亿参数。

Result: 模型在多项自动和人工评测中，文本-图像对齐和视觉质量均达到了与业界最佳模型媲美的水平，成为目前参数规模最大且性能最强的开源图像生成模型。

Conclusion: HunyuanImage 3.0推动了多模态领域的发展，通过开源代码和权重，为社区的创新和研究提供了强有力的基础，大幅促进多模态生态的繁荣。

Abstract: We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available. The achievement of
HunyuanImage 3.0 relies on several key components, including meticulous data
curation, advanced architecture design, a native Chain-of-Thoughts schema,
progressive model pre-training, aggressive model post-training, and an
efficient infrastructure that enables large-scale training and inference. With
these advancements, we successfully trained a Mixture-of-Experts (MoE) model
comprising over 80 billion parameters in total, with 13 billion parameters
activated per token during inference, making it the largest and most powerful
open-source image generative model to date. We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem. All
open source assets are publicly available at
https://github.com/Tencent-Hunyuan/HunyuanImage-3.0

</details>


### [193] [ColLab: A Collaborative Spatial Progressive Data Engine for Referring Expression Comprehension and Generation](https://arxiv.org/abs/2509.23955)
*Shilan Zhang,Jirui Huang,Ruilin Yao,Cong Wang,Yaxiong Chen,Peng Xu,Shengwu Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种名为ColLab的新方法，实现了无人工参与下的Referring Expression Comprehension（REC）和Referring Expression Generation（REG）数据自动生成。该方法提升了数据注释效率和表达质量。


<details>
  <summary>Details</summary>
Motivation: 现有的REC和REG数据集依赖人工标注，过程耗时且难以大规模扩展。因此亟需自动化、高效且无需人工的解决方案。

Method: 提出了ColLab数据引擎，采用协同多模态模型交互（CMMI）策略，结合多模态大语言模型（MLLMs）与大语言模型（LLMs）生成表达。同时设计了空间渐进增强模块（SPA），增强描述在重复实例中的空间表达能力。

Result: 实验表明，ColLab大幅提升了注释效率，并提升了生成描述的质量与区分性。部分成果已在ICCV 2025 MARS2大赛数据生成中应用，有效丰富了数据样本的多样性与挑战性。

Conclusion: ColLab为REC和REG任务的数据生成提供了高效、自动化的新方案，有助于大规模多模态理解和推理任务的数据基础建设。

Abstract: Referring Expression Comprehension (REC) and Referring Expression Generation
(REG) are fundamental tasks in multimodal understanding, supporting precise
object localization through natural language. However, existing REC and REG
datasets rely heavily on manual annotation, which is labor-intensive and
difficult to scale. In this paper, we propose ColLab, a collaborative spatial
progressive data engine that enables fully automated REC and REG data
generation without human supervision. Specifically, our method introduces a
Collaborative Multimodal Model Interaction (CMMI) strategy, which leverages the
semantic understanding of multimodal large language models (MLLMs) and large
language models (LLMs) to generate descriptions. Furthermore, we design a
module termed Spatial Progressive Augmentation (SPA) to enhance spatial
expressiveness among duplicate instances. Experiments demonstrate that ColLab
significantly accelerates the annotation process of REC and REG while improving
the quality and discriminability of the generated expressions. In addition to
the core methodological contribution, our framework was partially adopted in
the data generation pipeline of the ICCV 2025 MARS2 Challenge on Multimodal
Reasoning, enriching the dataset with diverse and challenging samples that
better reflect real-world reasoning demands.

</details>


### [194] [Reinforcement Learning with Inverse Rewards for World Model Post-training](https://arxiv.org/abs/2509.23958)
*Yang Ye,Tianyu He,Shuo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: 提出了一种新的后训练框架RLIR，显著提升了视频世界模型在动作跟随性和视觉质量等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 视频世界模型在生成过程中，虽然视觉和时序一致性有进步，但对于人类指定动作的准确跟随能力仍有不足，现有的强化学习后训练方法又因高标注成本和缺乏检验机制而难以直接应用。

Method: 提出RLIR方法，利用逆动力学模型从生成视频中恢复输入动作，从而将高维视频信息映射到低维动作空间，得到可验证的奖励信号，并通过群体相对策略优化算法进行训练。

Result: 在自回归和扩散两类世界模型中的实验表明，RLIR在动作跟随上提升5-10%，视觉质量最高提升10%，并获得了更高的人类偏好评分。

Conclusion: RLIR是首个专为提升视频世界模型的动作跟随能力设计的后训练方法，在不依赖大规模人工标注的前提下，有效提升了模型表现。

Abstract: World models simulate dynamic environments, enabling agents to interact with
diverse input modalities. Although recent advances have improved the visual
quality and temporal consistency of video world models, their ability of
accurately modeling human-specified actions remains under-explored.
Reinforcement learning presents a promising approach for directly improving the
suboptimal action-following capability of pre-trained models, assuming that an
appropriate reward function can be defined. However, transferring reinforcement
learning post-training methods to world model is impractical due to the
prohibitive cost of large-scale preference annotations and the infeasibility of
constructing rule-based video verifiers. To address this gap, we propose
Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework
that derives verifiable reward signals by recovering input actions from
generated videos using an Inverse Dynamics Model. By mapping high-dimensional
video modality to a low-dimensional action space, RLIR provides an objective
and verifiable reward for optimization via Group Relative Policy Optimization.
Experiments across autoregressive and diffusion paradigms demonstrate 5-10%
gains in action-following, up to 10% improvements in visual quality, and higher
human preference scores, establishing RLIR as the first post-training method
specifically designed to enhance action-following in video world models.

</details>


### [195] [A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid Cancer Classification](https://arxiv.org/abs/2509.23968)
*Nada Bouchekout,Abdelkrim Boukabou,Morad Grimes,Yassine Habchi,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 该论文提出了一种结合自适应卷积神经网络（CNN）与Cohen-Daubechies-Feauveau (CDF9/7)小波和n-scroll混沌系统的新型甲状腺癌超声图像分类方法，在公开数据集上取得了98.17%的准确率，优于当前主流模型，具备很好的泛化能力和解释性。


<details>
  <summary>Details</summary>
Motivation: 全球甲状腺癌发病率上升，需要及时且准确的诊断手段以提升治疗效果和患者预后。目前常用的深度学习方法仍有提升空间，尤其是在提升区分特征、增强模型泛化能力及可解释性方面。

Method: 作者设计了一种“自适应CNN+小波分析（CDF9/7）+混沌系统特征调制”的智能分类方法。通过对小波细节系数引入n-scroll混沌系统来增强特征判别力，并在DDTI公开超声数据集上进行五折交叉验证，同时与多种主流深度学习模型（EfficientNetV2-S、Swin-T、ViT-B/16、ConvNeXt-T）进行了对比和消融实验；还测试了模型的跨数据集泛化能力和可解释性。

Result: 该方法在DDTI数据集上实现了98.17%的准确率、98.76%的敏感性、97.58%的特异性、97.55%的F1-score和0.9912的AUC。小波+混沌模块相比单一小波，准确率提升8.79%。与最新主流模型对比，本模型准确率和AUC均有提升。交叉数据集（TCIA）准确率为95.82%，在皮肤病变ISIC子集上准确率为97.31%。

Conclusion: 结合小波分析和混沌系统的自适应CNN能有效提升甲状腺癌超声诊断的准确率与泛化性能，运行高效、具有良好解释性，显示出临床集成应用的潜力。

Abstract: Timely and accurate diagnosis is crucial in addressing the global rise in
thyroid cancer, ensuring effective treatment strategies and improved patient
outcomes. We present an intelligent classification method that couples an
Adaptive Convolutional Neural Network (CNN) with Cohen-Daubechies-Feauveau
(CDF9/7) wavelets whose detail coefficients are modulated by an n-scroll
chaotic system to enrich discriminative features. We evaluate on the public
DDTI thyroid ultrasound dataset (n = 1,638 images; 819 malignant / 819 benign)
using 5-fold cross-validation, where the proposed method attains 98.17%
accuracy, 98.76% sensitivity, 97.58% specificity, 97.55% F1-score, and an AUC
of 0.9912. A controlled ablation shows that adding chaotic modulation to CDF9/7
improves accuracy by +8.79 percentage points over a CDF9/7-only CNN (from
89.38% to 98.17%). To objectively position our approach, we trained
state-of-the-art backbones on the same data and splits: EfficientNetV2-S
(96.58% accuracy; AUC 0.987), Swin-T (96.41%; 0.986), ViT-B/16 (95.72%; 0.983),
and ConvNeXt-T (96.94%; 0.987). Our method outperforms the best of these by
+1.23 points in accuracy and +0.0042 in AUC, while remaining computationally
efficient (28.7 ms per image; 1,125 MB peak VRAM). Robustness is further
supported by cross-dataset testing on TCIA (accuracy 95.82%) and transfer to an
ISIC skin-lesion subset (n = 28 unique images, augmented to 2,048; accuracy
97.31%). Explainability analyses (Grad-CAM, SHAP, LIME) highlight clinically
relevant regions. Altogether, the wavelet-chaos-CNN pipeline delivers
state-of-the-art thyroid ultrasound classification with strong generalization
and practical runtime characteristics suitable for clinical integration.

</details>


### [196] [VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion](https://arxiv.org/abs/2509.23971)
*Kargi Chauhan,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 当前先进的扩散模型可以生成现实感强的交通仿真，但常常违反物理约束，如车辆碰撞、驶离道路等。该文提出了一种不需重新训练的能量引导采样方法，有效提升了物理有效性和仿真真实度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然能够生成现实风格的交通仿真轨迹，却经常无视基本物理定律，将物理有效性当做模型自然产生的副产物而非设计要求。这大大降低了仿真的可信度和实用价值。

Method: 提出“物理有效性优先的空间智能（VFSI）”，在扩散采样阶段通过能量函数强行施加碰撞规避与运动学约束，无需模型重新训练，引导生成轨迹趋向物理有效。

Result: 在Waymo大规模数据集上实验，碰撞率降低67%（24.6%到8.1%），总体有效性提高87%（50.3%到94.2%），仿真准确度(ADE)也有所提升。

Conclusion: 只要在推断阶段通过能量引导显式约束，便能实现高物理有效性的交通仿真，无需修改或重训练原有模型，适用于多种交通扩散模型。

Abstract: Modern diffusion models generate realistic traffic simulations but
systematically violate physical constraints. In a large-scale evaluation of
SceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of
generated trajectories violate basic physical laws - vehicles collide, drive
off roads, and spawn inside buildings. This reveals a fundamental limitation:
current models treat physical validity as an emergent property rather than an
architectural requirement. We propose Validity-First Spatial Intelligence
(VFSI), which enforces constraints through energy-based guidance during
diffusion sampling, without model retraining. By incorporating collision
avoidance and kinematic constraints as energy functions, we guide the denoising
process toward physically valid trajectories. Across 200 urban scenarios from
the Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to
8.1%) and improves overall validity by 87% (50.3% to 94.2%), while
simultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our
model-agnostic approach demonstrates that explicit constraint enforcement
during inference is both necessary and sufficient for physically valid traffic
simulation.

</details>


### [197] [Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution](https://arxiv.org/abs/2509.23980)
*Jinpei Guo,Yifei Ji,Zheng Chen,Yufei Wang,Sizhuo Ma,Yong Guo,Yulun Zhang,Jian Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频超分辨率扩散模型OASIS，在准确率和推理速度上均优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频超分任务上效果优秀，但传统方法存在信息冗余和不必要的计算开销。目标是减少冗余、提高效率并保留预训练知识。

Method: OASIS采用一步扩散架构，创新性地引入了注意力专用路由，使不同注意力头根据自身特性学习不同模式，从而降低冗余和提升有效性。同时提出了逐步训练策略，先从时间一致的退化入手，再过渡到更复杂的退化设置。

Result: OASIS在合成和真实数据集上均取得了最先进的性能，推理速度相比SeedVR2等一步扩散基线快6.2倍。

Conclusion: OASIS显著提升了VSR任务中的效率与效果，为实用型视频超分辨率系统提供了新思路。

Abstract: Diffusion models have recently shown promising results for video
super-resolution (VSR). However, directly adapting generative diffusion models
to VSR can result in redundancy, since low-quality videos already preserve
substantial content information. Such redundancy leads to increased
computational overhead and learning burden, as the model performs superfluous
operations and must learn to filter out irrelevant information. To address this
problem, we propose OASIS, an efficient $\textbf{o}$ne-step diffusion model
with $\textbf{a}$ttention $\textbf{s}$pecialization for real-world
v$\textbf{i}$deo $\textbf{s}$uper-resolution. OASIS incorporates an attention
specialization routing that assigns attention heads to different patterns
according to their intrinsic behaviors. This routing mitigates redundancy while
effectively preserving pretrained knowledge, allowing diffusion models to
better adapt to VSR and achieve stronger performance. Moreover, we propose a
simple yet effective progressive training strategy, which starts with
temporally consistent degradations and then shifts to inconsistent settings.
This strategy facilitates learning under complex degradations. Extensive
experiments demonstrate that OASIS achieves state-of-the-art performance on
both synthetic and real-world datasets. OASIS also provides superior inference
speed, offering a $\textbf{6.2$\times$}$ speedup over one-step diffusion
baselines such as SeedVR2. The code will be available at
\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.

</details>


### [198] [RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization](https://arxiv.org/abs/2509.23991)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Dinesh Manocha*

Main category: cs.CV

TL;DR: RPG360提出了一种无需训练且高鲁棒性的360度单目深度估计方法，通过cubemap转换、基础模型和图优化实现尺度一致性，大幅提升不同数据集表现，并助力相关下游任务。


<details>
  <summary>Details</summary>
Motivation: 360度图像在多个领域应用广泛，但其深度估计因缺乏大规模标注数据面临问题，现有方法难以兼顾适应性和精准度。

Method: 将360图像转为六面体cubemap后，使用透视基础模型分别预测每一面深度与法向量。通过新颖的图优化方法，基于参数化的深度与法向图以及每面独立尺度参数，实现六面体间一致且结构完整的深度尺度对齐。

Result: RPG360在Matterport3D、Stanford2D3D和360Loc等多数据集上表现优越。在无监督、零样本设定下能显著提升表现，也在特征匹配与运动结构恢复等下游任务中带来3.2~5.4%和0.2~9.7%的提升。

Conclusion: RPG360无需大规模标注数据，通过结合基础模型和图优化，实现了高精度且鲁棒的360度深度估计，并对多个下游任务输出有益影响，展现了方法的通用性和实用性。

Abstract: The increasing use of 360 images across various domains has emphasized the
need for robust depth estimation techniques tailored for omnidirectional
images. However, obtaining large-scale labeled datasets for 360 depth
estimation remains a significant challenge. In this paper, we propose RPG360, a
training-free robust 360 monocular depth estimation method that leverages
perspective foundation models and graph optimization. Our approach converts 360
images into six-face cubemap representations, where a perspective foundation
model is employed to estimate depth and surface normals. To address depth scale
inconsistencies across different faces of the cubemap, we introduce a novel
depth scale alignment technique using graph-based optimization, which
parameterizes the predicted depth and normal maps while incorporating an
additional per-face scale parameter. This optimization ensures depth scale
consistency across the six-face cubemap while preserving 3D structural
integrity. Furthermore, as foundation models exhibit inherent robustness in
zero-shot settings, our method achieves superior performance across diverse
datasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate
the versatility of our depth estimation approach by validating its benefits in
downstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion
0.2 ~ 9.7% in AUC@5.

</details>


### [199] [TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute Coronary Syndrome Treatment Prediction](https://arxiv.org/abs/2509.23999)
*Diane Kim,Minh Nguyen Nhat To,Sherif Abdalla,Teresa S. M. Tsang,Purang Abolmaesumi,and Christina Luong*

Main category: cs.CV

TL;DR: 本文提出了一种名为TREAT-Net的多模态深度学习框架，利用超声心动图视频和临床数据，无创预测ACS（急性冠状动脉综合征）治疗方案，模型在大规模数据上表现优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉造影虽然是诊断ACS的金标准，但属于有创且资源消耗大的操作，容易带来风险和延误治疗，因此有必要开发无创和高效的替代工具。

Method: 作者提出TREAT-Net多模态深度学习框架，综合处理超声心动图视频与结构化临床记录，通过表格引导型交叉注意力方法增强视频解读能力，并采用晚期融合机制整合多模态预测结果。

Result: TREAT-Net在9000多例ACS数据上训练，取得67.6%的balanced accuracy和71.1%的AUROC，干预预测的跨模态一致性达88.6%，明显优于只用单一模态或无融合策略的基线模型。

Conclusion: TREAT-Net能够在无需有创操作的情况下，为ACS病人提供及时和准确的治疗分流预测，尤其适用于难以获得冠状动脉造影的群体。

Abstract: Coronary angiography remains the gold standard for diagnosing Acute Coronary
Syndrome (ACS). However, its resource-intensive and invasive nature can expose
patients to procedural risks and diagnostic delays, leading to postponed
treatment initiation. In this work, we introduce TREAT-Net, a multimodal deep
learning framework for ACS treatment prediction that leverages non-invasive
modalities, including echocardiography videos and structured clinical records.
TREAT-Net integrates tabular-guided cross-attention to enhance video
interpretation, along with a late fusion mechanism to align predictions across
modalities. Trained on a dataset of over 9000 ACS cases, the model outperforms
unimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an
AUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy
for intervention prediction. These findings highlight the potential of
TREAT-Net as a non-invasive tool for timely and accurate patient triage,
particularly in underserved populations with limited access to coronary
angiography.

</details>


### [200] [SIE3D: Single-image Expressive 3D Avatar generation via Semantic Embedding and Perceptual Expression Loss](https://arxiv.org/abs/2509.24004)
*Zhiqi Huang,Dulongkai Cui,Jinglu Hu*

Main category: cs.CV

TL;DR: SIE3D框架能够通过单张图片和文本描述生成高逼真、可控的3D头部头像，提升了表情控制的精度和真实度。


<details>
  <summary>Details</summary>
Motivation: 当前3D头像生成方法无法通过文本实现细致、直观的表情控制，限制了实用性和灵活性。

Method: SIE3D将图片提取的身份特征与文本的语义嵌入结合，通过新颖的条件融合机制实现精确控制。同时提出创新的感知表情损失函数，利用预训练表情分类器引导生成过程，提升表情与文本描述的一致性。

Result: 大量实验显示，在消费级GPU上，SIE3D在身份保真和表情精准度上均超过了现有竞品，表现出更好的可控性和真实感。

Conclusion: SIE3D为单图驱动的3D头像生成带来更直观、细腻的文本控制能力，为虚拟形象等应用提供了更优解决方案。

Abstract: Generating high-fidelity 3D head avatars from a single image is challenging,
as current methods lack fine-grained, intuitive control over expressions via
text. This paper proposes SIE3D, a framework that generates expressive 3D
avatars from a single image and descriptive text. SIE3D fuses identity features
from the image with semantic embedding from text through a novel conditioning
scheme, enabling detailed control. To ensure generated expressions accurately
match the text, it introduces an innovative perceptual expression loss
function. This loss uses a pre-trained expression classifier to regularize the
generation process, guaranteeing expression accuracy. Extensive experiments
show SIE3D significantly improves controllability and realism, outperforming
competitive methods in identity preservation and expression fidelity on a
single consumer-grade GPU. Project page:
https://blazingcrystal1747.github.io/SIE3D/

</details>


### [201] [FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.24008)
*Haonan Ge,Yiwei Wang,Kai-Wei Chang,Hang Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了FrameMind框架，结合强化学习和动态采样策略，使视频理解模型能在推理过程中主动选择和请求关键视频帧，大幅提升了灵活性和效率，并在多项基准下取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型采用固定抽帧策略，不能针对不同问题灵活地调整视觉信息采集，导致在需要广泛时域或细粒度空间分辨率的任务中表现欠佳。作者希望通过主动和动态的视觉信息获取机制，提升模型的推理能力。

Method: 提出FrameMind端到端框架，引入Frame-Interleaved Chain-of-Thought（FiCOT）机制，通过多轮交互式推理，在文字推理和视觉信息主动感知之间切换，并通过工具选取目标帧或片段。为训练动态采样策略，设计Dynamic Resolution Frame Sampling（DRFS）和DRFS-GRPO算法，实现无须帧级标注的动态策略优化。

Result: 在MLVU和VideoMME等高难度视频理解基准测试上，FrameMind在各项指标上均显著优于现有方法，刷新了当前最优纪录。

Conclusion: FrameMind通过动态和交互式的视频信息采集策略，有效提升了视频理解模型的灵活性和推理能力，为高效视频理解任务树立了新范例。

Abstract: Current video understanding models rely on fixed frame sampling strategies,
processing predetermined visual inputs regardless of the specific reasoning
requirements of each question. This static approach limits their ability to
adaptively gather visual evidence, leading to suboptimal performance on tasks
that require either broad temporal coverage or fine-grained spatial detail. In
this paper, we introduce FrameMind, an end-to-end framework trained with
reinforcement learning that enables models to dynamically request visual
information during reasoning through Frame-Interleaved Chain-of-Thought
(FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns
where the model alternates between textual reasoning and active visual
perception, using tools to extract targeted frames or video clips based on
identified knowledge gaps. To train effective dynamic sampling policies, we
propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to
diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a
group-relative policy optimization algorithm that learns from outcome-based
rewards without requiring frame-level annotations. Extensive experiments on
challenging benchmarks like MLVU and VideoMME demonstrate that our method
significantly outperforms existing models, advancing the state of the art in
flexible and efficient video understanding.

</details>


### [202] [Generalized Category Discovery in Hyperspectral Images via Prototype Subspace Modeling](https://arxiv.org/abs/2509.24017)
*Xianlu Li,Nicolas Nadisic,Shaoguang Huang,Aleksandra Pizurica*

Main category: cs.CV

TL;DR: 本论文提出了首个针对高光谱图像（HSI）的通用类别发现（GCD） framework，通过原型子空间建模显著提升了新旧类别识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用类别发现方法大多针对RGB图像，不适用于高维、光谱结构复杂的高光谱图像，存在类别区分和建模能力弱的问题。

Method: 提出一种针对HSI的GCD framework。与以往每类仅用一个原型向量不同，作者使用一组基向量定义每个类别，构建子空间表达。此外引入基正交性约束，提升类别可分性，并用重建约束保证每类子空间能重构其样本。

Result: 在真实高光谱数据集上，所提方法在类别发现准确率方面显著优于现有先进方法。

Conclusion: 原型子空间建模和创新约束有效提升了HSI下类别发现性能，为高光谱通用类别发现提供了新思路。

Abstract: Generalized category discovery~(GCD) seeks to jointly identify both known and
novel categories in unlabeled data. While prior works have mainly focused on
RGB images, their assumptions and modeling strategies do not generalize well to
hyperspectral images~(HSI), which are inherently high-dimensional and exhibit
complex spectral structures. In this paper, we propose the first GCD framework
tailored for HSI, introducing a prototype subspace modeling model to better
capture class structure. Instead of learning a single prototype vector for each
category as in existing methods such as SimGCD, we model each category using a
set of basis vectors, forming a subspace representation that enables greater
expressiveness and discrimination in a high-dimensional feature space. To guide
the learning of such bases, we enforce two key constraints: (1) a basis
orthogonality constraint that promotes inter-class separability, and (2) a
reconstruction constraint that ensures each prototype basis can effectively
reconstruct its corresponding class samples. Experimental results on real-world
HSI demonstrate that our method significantly outperforms state-of-the-art GCD
methods, establishing a strong foundation for generalized category discovery in
hyperspectral settings.

</details>


### [203] [Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba](https://arxiv.org/abs/2509.24020)
*Jian Chen,Zhuoran Zheng,Han Hu,Guijuan Zhang,Dianjie Lu,Liang Li,Chen Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种结合物理先验（大气散射模型）和行人关系拓扑建模的深度学习模型，用于解决雾霾天气下行人轨迹预测的物理信息退化与交互建模无效问题。在新构建的数据集和极端雾霾条件下，该方法显著优于现有 SOTA 模型。


<details>
  <summary>Details</summary>
Motivation: 雾霾天气导致视觉信息丢失，现有行人轨迹预测方法难以应对极端恶劣气象下物理退化和行人交互的建模难题。为实现智能交通系统在复杂环境下的可靠感知，需要有针对性的创新方法。

Method: 1）构建可微分的大气散射物理模型，通过神经网络估计物理参数，学习去雾特征表征。2）设计自适应扫描状态空间模型（改进版Mamba），在保持长依赖建模能力的前提下大大加快推理速度。3）开发异构图注意力网络与时空融合模块，建模行人与群体跨粒度的多层次交互，并捕捉协同演化模式。4）在 ETH/UCY 数据集基础上构建雾霾场景数据集进行评测。

Result: 相比最先进方法，在致密雾霾（能见度<30m）下，模型的 minADE/minFDE 分别降低了37.2%和41.5%，且推理速度提升了78%。

Conclusion: 该方法为恶劣气候下智能交通系统中的可靠感知与预测提供了一种新的建模范式，有效缓解了物理信息退化带来的影响，提升了行人轨迹预测的精度与实用性。

Abstract: To address the issues of physical information degradation and ineffective
pedestrian interaction modeling in pedestrian trajectory prediction under hazy
weather conditions, we propose a deep learning model that combines physical
priors of atmospheric scattering with topological modeling of pedestrian
relationships. Specifically, we first construct a differentiable atmospheric
scattering model that decouples haze concentration from light degradation
through a network with physical parameter estimation, enabling the learning of
haze-mitigated feature representations. Second, we design an adaptive scanning
state space model for feature extraction. Our adaptive Mamba variant achieves a
78% inference speed increase over native Mamba while preserving long-range
dependency modeling.
  Finally, to efficiently model pedestrian relationships, we develop a
heterogeneous graph attention network, using graph matrices to model
multi-granularity interactions between pedestrians and groups, combined with a
spatio-temporal fusion module to capture the collaborative evolution patterns
of pedestrian movements. Furthermore, we constructed a new pedestrian
trajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of
the proposed method. Experiments show that our method reduces the minADE /
minFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in
dense haze scenarios (visibility < 30m), providing a new modeling paradigm for
reliable perception in intelligent transportation systems in adverse
environments.

</details>


### [204] [$\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain](https://arxiv.org/abs/2509.24022)
*Nate Rothschild,Moshe Kimhi,Avi Mendelson,Chaim Baskin*

Main category: cs.CV

TL;DR: 本文提出直接在原始Bayer马赛克图片上学习，可有效提升雨天图像重建质量，优于传统基于后ISP（sRGB）图像的方法。


<details>
  <summary>Details</summary>
Motivation: 大部分图像重建网络都在经过图像信号处理（ISP）的sRGB图像上训练，但ISP不可逆地混合颜色、裁剪动态范围、模糊细节，导致重建质量下降。作者希望避免这些损失，探索是否能在原始数据上获得更好的重建效果。

Method: 以降雨退化图像为研究案例，比较后ISP和Bayer重建流程；构建Raw-Rain数据集，内含真实雨景的12-bit Bayer和相应sRGB图像；提出信息保留评分（ICS）指标，更契合人类主观感受。

Result: 原始域模型在测试集中，PSNR提升最多0.99 dB，ICS提升1.2%，且计算量更低、运行更快。

Conclusion: 建议低层视觉任务采用ISP-last范式，推动端到端可学习的相机流程。

Abstract: Image reconstruction from corrupted images is crucial across many domains.
Most reconstruction networks are trained on post-ISP sRGB images, even though
the image-signal-processing pipeline irreversibly mixes colors, clips dynamic
range, and blurs fine detail. This paper uses the rain degradation problem as a
use case to show that these losses are avoidable, and demonstrates that
learning directly on raw Bayer mosaics yields superior reconstructions. To
substantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction
pipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy
scenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii)
introduce Information Conservation Score (ICS), a color-invariant metric that
aligns more closely with human opinion than PSNR or SSIM. On the test split,
our raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2%
ICS, while running faster with half of the GFLOPs. The results advocate an
ISP-last paradigm for low-level vision and open the door to end-to-end
learnable camera pipelines.

</details>


### [205] [Joint Superpixel and Self-Representation Learning for Scalable Hyperspectral Image Clustering](https://arxiv.org/abs/2509.24027)
*Xianlu Li,Nicolas Nadisic,Shaoguang Huang,Aleksandra Pizurica*

Main category: cs.CV

TL;DR: 本文提出了一种全新的端到端框架，将超像素分割与子空间聚类联合优化，显著提升高光谱图像分析的聚类精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统高光谱图像（HSI）子空间聚类方法因计算与内存消耗大而不易扩展。通过超像素分割降低处理点数量虽可提升效率，但现有方法往往独立于聚类任务进行分割，导致最终分块与聚类目标不一致。需要一种能兼顾聚类目标的有效超像素分割策略。

Method: 构建端到端联合优化框架，将可微超像素分割模块与基于ADMM的自表达网络结合，并通过反馈机制对超像素分割模块给出聚类指导信号。同时，为每个超像素学习独特的紧致度参数，实现自适应分割。

Result: 在多个高光谱图像数据集上的大量实验表明，该方法相较于当前最优聚类方法在聚类准确率上表现更优。

Conclusion: 联合优化超像素分割与子空间聚类不仅提升聚类效果，还能更好地兼顾谱-空结构，为HSI聚类提供了高效且精度优异的新范式。

Abstract: Subspace clustering is a powerful unsupervised approach for hyperspectral
image (HSI) analysis, but its high computational and memory costs limit
scalability. Superpixel segmentation can improve efficiency by reducing the
number of data points to process. However, existing superpixel-based methods
usually perform segmentation independently of the clustering task, often
producing partitions that do not align with the subsequent clustering
objective. To address this, we propose a unified end-to-end framework that
jointly optimizes superpixel segmentation and subspace clustering. Its core is
a feedback mechanism: a self-representation network based on unfolded
Alternating Direction Method of Multipliers (ADMM) provides a model-driven
signal to guide a differentiable superpixel module. This joint optimization
yields clustering-aware partitions that preserve both spectral and spatial
structure. Furthermore, our superpixel network learns a unique compactness
parameter for each superpixel, enabling more flexible and adaptive
segmentation. Extensive experiments on benchmark HSI datasets demonstrate that
our method consistently achieves superior accuracy compared with
state-of-the-art clustering approaches.

</details>


### [206] [A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer](https://arxiv.org/abs/2509.24066)
*Leonardo Iurada,Beatrice Occhiena,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 本文研究了预训练视觉模型的初始化剪枝方法，发现即使在未知下游任务下进行数据驱动剪枝，剪枝后模型在零样本及微调时仍能保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练视觉模型表现出色，但其高昂的计算与存储成本制约了实际应用。剪枝技术可以减少这些成本，但传统观念认为要获得有效剪枝需依赖特定任务数据，这在下游任务未知前难以实现。

Method: 作者考察了数据对预训练模型剪枝的影响，通过对一个任务的剪枝，并测试剪枝模型在其他未见任务上的表现，结合微调评估剪枝模型的迁移与适应能力。

Result: 实验证明，即使使用单一任务数据进行剪枝，模型在零样本和迁移到未见任务时依然能保持高性能；对剪枝后模型微调，既能提升原任务性能，也能恢复对新任务的表现。

Conclusion: 剪枝时不一定需要任务特定的数据，强大的预训练本身塑造了有利的损失景观，使剪枝模型易于适应新任务，这降低了大模型剪枝及部署的门槛。

Abstract: The widespread availability of pre-trained vision models has enabled numerous
deep learning applications through their transferable representations. However,
their computational and storage costs often limit practical deployment.
Pruning-at-Initialization has emerged as a promising approach to compress
models before training, enabling efficient task-specific adaptation. While
conventional wisdom suggests that effective pruning requires task-specific
data, this creates a challenge when downstream tasks are unknown in advance. In
this paper, we investigate how data influences the pruning of pre-trained
vision models. Surprisingly, pruning on one task retains the model's zero-shot
performance also on unseen tasks. Furthermore, fine-tuning these pruned models
not only improves performance on original seen tasks but can recover held-out
tasks' performance. We attribute this phenomenon to the favorable loss
landscapes induced by extensive pre-training on large-scale datasets.

</details>


### [207] [Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding](https://arxiv.org/abs/2509.24133)
*Zhecheng Li,Guoxian Song,Yiwei Wang,Zhen Xiong,Junsong Yuan,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了一个结合通用视觉-语言模型与专用GUI定位模型的协同框架GMS，有效提升了自然语言在图形界面中的指令定位准确率，准确率提升至35.7%。


<details>
  <summary>Details</summary>
Motivation: 现有模型在理解复杂、多样的图形用户界面并准确预测操作目标的空间坐标时表现有限，需要更有效的方法来增强GUI定位能力。

Method: 提出GMS框架，将通用视觉-语言模型作为“扫描器”初步筛选感兴趣区域，再用经过微调的特定UI定位模型作为“定位器”精确确定位置。整个流程共分五个阶段，并采用层次化搜索与跨模态通信方式增强性能。

Result: 在ScreenSpot-Pro数据集上，单独使用“扫描器”或“定位器”准确率仅为2.0%和3.7%，而二者结合的GMS框架准确率达到35.7%，为10倍提升，并明显优于其他强基线方法。

Conclusion: GMS框架综合利用通用和专用模型优势，在多种设置下展现出强鲁棒性和普适性，为GUI语义理解和定位任务提供有效解决方案。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs)
presents a challenging task that requires models to comprehend diverse UI
elements across various applications and systems, while also accurately
predicting the spatial coordinates for the intended operation. To tackle this
problem, we propose GMS: Generalist Scanner Meets Specialist Locator, a
synergistic coarse-to-fine framework that effectively improves GUI grounding
performance. GMS leverages the complementary strengths of general
vision-language models (VLMs) and small, task-specific GUI grounding models by
assigning them distinct roles within the framework. Specifically, the general
VLM acts as a 'Scanner' to identify potential regions of interest, while the
fine-tuned grounding model serves as a 'Locator' that outputs precise
coordinates within these regions. This design is inspired by how humans perform
GUI grounding, where the eyes scan the interface and the brain focuses on
interpretation and localization. Our whole framework consists of five stages
and incorporates hierarchical search with cross-modal communication to achieve
promising prediction results. Experimental results on the ScreenSpot-Pro
dataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\%$
and $3.7\%$ accuracy respectively when used independently, their integration
within GMS framework yields an overall accuracy of $35.7\%$, representing a $10
\times$ improvement. Additionally, GMS significantly outperforms other strong
baselines under various settings, demonstrating its robustness and potential
for general-purpose GUI grounding.

</details>


### [208] [Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding](https://arxiv.org/abs/2509.24072)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文提出Grounding IDs，可以提升大型视觉-语言模型（LVLMs）在多模态任务中的表现，增强模型跨模态绑定和推理能力，并降低幻觉和不准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLMs在多模态任务表现良好，但在结构化推理和精确绑定上仍有限。近期研究发现添加简单视觉结构如分区和注释可以提升模型表现，但具体机制尚不明确，需要深入探究。

Method: 作者提出“Grounding IDs”的概念，即通过外部线索激发的隐式标识符，将对象和其所属分区绑定在视觉和语言模态间。通过表征分析和因果干预实验，探究这些标识符如何在嵌入空间内对齐分区对象，并验证其在多模态绑定中的中介作用。

Result: 实验证明Grounding IDs可实现分区内的鲁棒对齐，缩小图像与文本模态间的差距，增强相关组件间的注意力，提升跨模态绑定能力，并显著减少模型幻觉。

Conclusion: Grounding IDs是解释外部线索如何增强多模态绑定的关键机制。该机制提升了模型可解释性和鲁棒性，在多模态应用中具有实际价值。

Abstract: Large vision-language models (LVLMs) show strong performance across
multimodal benchmarks but remain limited in structured reasoning and precise
grounding. Recent work has demonstrated that adding simple visual structures,
such as partitions and annotations, improves accuracy, yet the internal
mechanisms underlying these gains remain unclear. We investigate this
phenomenon and propose the concept of Grounding IDs, latent identifiers induced
by external cues that bind objects to their designated partitions across
modalities. Through representation analysis, we find that these identifiers
emerge as robust within-partition alignment in embedding space and reduce the
modality gap between image and text. Causal interventions further confirm that
these identifiers mediate binding between objects and symbolic cues. We show
that Grounding IDs strengthen attention between related components, which in
turn improves cross-modal grounding and reduces hallucinations. Taken together,
our results identify Grounding IDs as a key symbolic mechanism explaining how
external cues enhance multimodal binding, offering both interpretability and
practical improvements in robustness.

</details>


### [209] [Latent Visual Reasoning](https://arxiv.org/abs/2509.24251)
*Bangzheng Li,Ximeng Sun,Jiang Liu,Ze Wang,Jialian Wu,Xiaodong Yu,Hao Chen,Emad Barsoum,Muhao Chen,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种新的视觉推理范式Latent Visual Reasoning (LVR)，实现了多模态大模型在视觉推理任务上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视觉推理时，尽管引入了语言的推理链条（CoT），但视觉信息作为静态输入，推理真正发生的空间依然局限于语言空间，限制了视觉理解能力。

Method: 提出LVR方法，将图像通过视觉编码器编码为和语言模型共享语义空间的视觉token，由语言模型自回归地生成与任务相关的潜在视觉状态，并与文本生成过程交替进行；同时引入GRPO算法做潜在空间上的强化学习，进一步平衡视觉与文本推理。

Result: 在对感知能力要求高的视觉问答任务上，LVR方法明显优于传统方式，在MMVP任务上取得71.67%的成绩，超过了Qwen2.5-VL的66.67%。

Conclusion: LVR显著提升了模型对细粒度视觉理解和感知能力，证实了在视觉token空间中自回归推理的有效性，有望推动多模态推理新范式的发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in
various tasks by incorporating Chain-of-Thought (CoT) reasoning in language
spaces. Recent work extends this direction by leveraging external tools for
visual editing, thereby enhancing the visual signal along the reasoning
trajectories. Nevertheless, these approaches remain fundamentally constrained:
reasoning is still confined to the language space, with visual information
treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a
new paradigm that enables autoregressive reasoning directly in the visual
embedding space. A visual encoder first projects images into visual tokens
within a joint semantic space shared with the language model. The language
model is then trained to generate latent states that reconstruct key visual
tokens critical for answering the query, constituting the process of latent
visual reasoning. By interleaving LVR with standard text generation, our model
achieves substantial gains on perception-intensive visual question answering
tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement
learning on latent reasoning, further balancing LVR and textual generation. We
show that LVR substantially improves fine-grained visual understanding and
perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code
base and model weights will be released later.

</details>


### [210] [Autoregressive Video Generation beyond Next Frames Prediction](https://arxiv.org/abs/2509.24081)
*Sucheng Ren,Chen Chen,Zhenbang Wang,Liangchen Song,Xiangxin Zhu,Alan Yuille,Yinfei Yang,Jiasen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成自回归模型VideoAR，创新性地将传统的逐帧预测拓展为多种预测单元（包括帧、关键细节帧、多尺度细化、时空立方体），其中以时空立方体为单元的预测在质量、速度、时序连贯性方面更优，突破了帧作为原子单元的既有假设。


<details>
  <summary>Details</summary>
Motivation: 目前主流视频自回归生成模型普遍以“帧”为预测基本单元，延续了语言建模中token的思想。然而，帧是否适合作为视频预测的原子单元值得质疑。该工作动机在于探索除了帧之外，是否存在更优的视频建模粒度，从而提升视频生成的性能与效率。

Method: 作者提出了VideoAR框架，支持多种预测单元（全帧、关键-细节帧、多尺度、时空立方体），并特别设计、实现了以“时空立方体”为预测单元的自回归模型，使得模型能在空间与时间维度上同时推进预测，打破逐帧生成的限制。模型在不同预测粒度上进行对比实验。

Result: 实验显示，基于时空立方体预测单元的方法，在视频生成的质量、生成速度以及时序连贯性方面均优于以帧为基础的传统方法。模型在VBench等数据集上的表现超越了现有主流自回归视频生成基线，并能够高效扩展至分钟级长序列视频生成。

Conclusion: 本文论证并实验证明了帧并不是视频自回归建模的唯一或最优单元，基于时空立方体的自回归生成方式表现更佳。作者期望本研究能够推动更深入地思考视频及其他时空领域的序列分解方式。

Abstract: Autoregressive models for video generation typically operate frame-by-frame,
extending next-token prediction from language to video's temporal dimension. We
question that unlike word as token is universally agreed in language if frame
is a appropriate prediction unit? To address this, we present VideoAR, a
unified framework that supports a spectrum of prediction units including full
frames, key-detail frames, multiscale refinements, and spatiotemporal cubes.
Among these designs, we find model video generation using
\textit{spatiotemporal} cubes as prediction units, which allows autoregressive
models to operate across both spatial and temporal dimensions simultaneously.
This approach eliminates the assumption that frames are the natural atomic
units for video autoregression. We evaluate VideoAR across diverse prediction
strategies, finding that cube-based prediction consistently delivers superior
quality, speed, and temporal coherence. By removing the frame-by-frame
constraint, our video generator surpasses state-of-the-art baselines on VBench
while achieving faster inference and enabling seamless scaling to minute-long
sequences. We hope this work will motivate rethinking sequence decomposition in
video and other spatiotemporal domains.

</details>


### [211] [Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA](https://arxiv.org/abs/2509.24445)
*Jianxin Liang,Tan Yue,Yuxuan Wang,Yueqian Wang,Zhihan Yin,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出通过合成更丰富的监督信号，提升视频问答（VideoQA）模型的表现，并验证了新方法能显著提高准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统VideoQA模型多数仅依赖于分离的、事实型问答对作为监督信号，这种“事实袋”的方式无法反映视频事件的叙事和因果结构，限制了模型对内容的深层理解。

Method: 提出两种互补策略：1）基于问题释义（QBP），将视频中多样化（what、how、why）的问题整合为叙述性段落，重构事件结构；2）基于问题标注（QBC），为每个问题生成细粒度的视觉理据，将答案与具体证据对应。利用大模型合成数据后，采用统一的预测目标训练VideoQA。

Result: 在STAR与NExT-QA数据集上，提出的方法显著提升了模型准确率。例如，3B模型在STAR上提升4.9%至72.5%，7B模型在NExT-QA上提升至80.8%。此外，QBP与QBC都提升了跨数据集泛化能力，且QBP可让模型训练收敛速度提升2.5倍以上。

Conclusion: 将数据合成从分离事实转向叙事连贯性和有理据的训练方式，显著提升了VideoQA的准确率、效率和泛化能力。

Abstract: The performance of Video Question Answering (VideoQA) models is fundamentally
constrained by the nature of their supervision, which typically consists of
isolated, factual question-answer pairs. This "bag-of-facts" approach fails to
capture the underlying narrative and causal structure of events, limiting
models to a shallow understanding of video content. To move beyond this
paradigm, we introduce a framework to synthesize richer supervisory signals. We
propose two complementary strategies: Question-Based Paraphrasing (QBP), which
synthesizes the diverse inquiries (what, how, why) from a video's existing set
of question-answer pairs into a holistic narrative paragraph that reconstructs
the video's event structure; and Question-Based Captioning (QBC), which
generates fine-grained visual rationales, grounding the answer to each question
in specific, relevant evidence. Leveraging powerful generative models, we use
this synthetic data to train VideoQA models under a unified next-token
prediction objective. Extensive experiments on STAR and NExT-QA validate our
approach, demonstrating significant accuracy gains and establishing new
state-of-the-art results, such as improving a 3B model to 72.5\% on STAR
(+4.9\%) and a 7B model to 80.8\% on NExT-QA. Beyond accuracy, our analysis
reveals that both QBP and QBC substantially enhance cross-dataset
generalization, with QBP additionally accelerating model convergence by over
2.5x. These results demonstrate that shifting data synthesis from isolated
facts to narrative coherence and grounded rationales yields a more accurate,
efficient, and generalizable training paradigm.

</details>


### [212] [Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow](https://arxiv.org/abs/2509.24099)
*Prerit Gupta,Shourya Verma,Ananth Grama,Aniket Bera*

Main category: cs.CV

TL;DR: DualFlow提出了一种高效的多模态双人动作生成框架，可根据文本、音乐及先前动作序列生成时序连贯、语义丰富、同步协调的3D动作，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 多模态、情境感知的双人动作生成在计算机图形学、动画以及人机交互领域非常具有挑战性，现有方法在效率、动作质量和多模态处理能力等方面存在不足。

Method: DualFlow框架利用rectified flow生成 3D动作，采用直线采样路径提升推理速度并减少误差；通过RAG模块结合音乐特征和大模型分解的文本语义检索动作样例；采用对比损失增强与条件信号的对齐，并引入同步损失以提升双人动作协调性。

Result: 在文本-动作、音乐-动作及多模态交互等多项基准测试中，DualFlow展现出动作质量、响应速度和效率上的一致优势，生成动作更具时间连贯性和韵律同步性。

Conclusion: DualFlow在多模态人体动作生成领域达到了新的最先进水平，能高效生成符合上下文且协调同步的双人动作，推动人机交互和动画生成的研究和应用。

Abstract: Generating realistic, context-aware two-person motion conditioned on diverse
modalities remains a central challenge in computer graphics, animation, and
human-computer interaction. We introduce DualFlow, a unified and efficient
framework for multi-modal two-person motion generation. DualFlow conditions 3D
motion synthesis on diverse inputs, including text, music, and prior motion
sequences. Leveraging rectified flow, it achieves deterministic straight-line
sampling paths between noise and data, reducing inference time and mitigating
error accumulation common in diffusion-based models. To enhance semantic
grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that
retrieves motion exemplars using music features and LLM-based text
decompositions of spatial relations, body movements, and rhythmic patterns. We
use contrastive objective that further strengthens alignment with conditioning
signals and introduce synchronization loss that improves inter-person
coordination. Extensive evaluations across text-to-motion, music-to-motion, and
multi-modal interactive benchmarks show consistent gains in motion quality,
responsiveness, and efficiency. DualFlow produces temporally coherent and
rhythmically synchronized motions, setting state-of-the-art in multi-modal
human motion generation.

</details>


### [213] [Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473)
*Shijie Lian,Changti Wu,Laurence Tianruo Yang,Hang Yuan,Bin Yu,Lei Zhang,Kai Chen*

Main category: cs.CV

TL;DR: 本论文提出了通过欧几里得几何问题来提升多模态大模型（MLLMs）空间智能能力，并构建了欧几里得30K数据集，通过创新训练方法显著提升了模型在空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 空间智能能力（如空间想象、物体旋转、空间关系判断等）对多模态大语言模型（MLLMs）来说仍是难以攻克的难题，缺乏系统训练与评测任务。作者希望通过几何领域的系统问题训练，弥补现有MLLMs在空间推理方面的不足。

Method: 作者自建了一个包含约3万道平面与立体几何题的多模态数据集Euclid30K，以欧几里得几何问题为代理任务（surrogate task）。利用Group Relative Policy Optimization（GRPO）方法，对Qwen2.5VL和RoboBrain2.0多模态模型家族进行微调，促使模型识别几何图形、计数、进行实体关系判断及多步推理训练。

Result: 微调后的模型在四个空间推理基准（Super-CLEVR、Omni3DBench、VSI-Bench和MindCube）上，未针对具体任务调整的情况下，取得了显著的zero-shot效果提升。例如在VSI-Bench基准上，所有模型的平均准确率从34.5%提升到40.5%，其中RoboBrain2.0-Euclid-7B达到了49.6%，超过了之前的最佳模型。

Conclusion: 本文首次系统验证了以几何为中心的微调策略可以赋予多模态视觉语言模型更强的空间迁移能力，对提升MLLM空间推理能力具有重要价值。

Abstract: Spatial intelligence spans a rich suite of abilities, including visualising
and transforming shapes, mentally rotating objects, judging relational
positions and containment, and estimating numerosity. However, it still remains
a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To
fill this gap, we propose to treat Euclidean geometry problem-solving as a
surrogate task. Specifically, we meticulously constructed a curated multimodal
dataset, called Euclid30K, comprising approximately 30K plane and solid
geometry problems. To enable the model to acquire and apply Euclidean
principles from these geometry problems, we employed Group Relative Policy
Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,
inspiring the models to identify shapes, count, and relate entities, and
perform multi-step deductive reasoning using Euclidean principles. Our
experiments demonstrate that the resulting models achieve substantial zero-shot
gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,
VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after
training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models
rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,
RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous
state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first
systematic study showing that geometry-centric fine-tuning can confer
vision-language models with broadly transferable spatial skills. Code and
Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.

</details>


### [214] [SVAC: Scaling Is All You Need For Referring Video Object Segmentation](https://arxiv.org/abs/2509.24109)
*Li Zhang,Haoxiang Gao,Zhihao Zhang,Luoxiao Huang,Tao Zhang*

Main category: cs.CV

TL;DR: 本论文提出SVAC模型，通过扩展输入帧和分割token提升视频-语言交互和分割精度，并结合两项新策略（ASTC与CSA），在多个RVOS基准上实现了高效且领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然促进了RVOS发展，但依然存在三个问题：（1）未充分利用MLLMs的先验知识；（2）长视频处理计算和内存代价高昂；（3）对复杂时序动态处理不佳。因此需要新的方法提升效率和分割效果。

Method: SVAC模型通过扩大输入帧数和分割token数量增强视频与语言的交互和精度。为减少因扩展带来的计算负担，设计了Anchor-Based Spatio-Temporal Compression（ASTC）模块对视觉token压缩，同时保留主要时空信息；并提出Clip-Specific Allocation（CSA）策略，更好应对视频中动态对象行为。

Result: SVAC在多个Refering Video Object Segmentation基准数据集上取得了最新的最优成绩，且在效率上也表现优异。

Conclusion: SVAC有效解决了MLLMs先验知识利用不足、长视频高计算代价及复杂动态处理不足等问题，实现了更精准、高效的视频对象分割。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment target objects in
video sequences based on natural language descriptions. While recent advances
in Multi-modal Large Language Models (MLLMs) have improved RVOS performance
through enhanced text-video understanding, several challenges remain, including
insufficient exploitation of MLLMs' prior knowledge, prohibitive computational
and memory costs for long-duration videos, and inadequate handling of complex
temporal dynamics. In this work, we propose SVAC, a unified model that improves
RVOS by scaling up input frames and segmentation tokens to enhance
video-language interaction and segmentation precision. To address the resulting
computational challenges, SVAC incorporates the Anchor-Based Spatio-Temporal
Compression (ASTC) module to compress visual tokens while preserving essential
spatio-temporal structure. Moreover, the Clip-Specific Allocation (CSA)
strategy is introduced to better handle dynamic object behaviors across video
clips. Experimental results demonstrate that SVAC achieves state-of-the-art
performance on multiple RVOS benchmarks with competitive efficiency. Our code
is available at https://github.com/lizhang1998/SVAC.

</details>


### [215] [NeMo: Needle in a Montage for Video-Language Understanding](https://arxiv.org/abs/2509.24563)
*Zi-Yuan Hu,Shuo Liang,Duo Zheng,Yanyang Li,Yeyao Tao,Shijia Huang,Wei Feng,Jia Qin,Jianguang Yu,Jing Huang,Meng Fang,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了针对视频大模型（VideoLLMs）时序复杂性推理的新型评测任务Needle in a Montage（NeMo），并据此构建了大规模基准数据集NeMoBench，以及自动化数据生成管线。


<details>
  <summary>Details</summary>
Motivation: 当前视频大模型在视频与语言理解领域取得了进展，但缺乏能够系统评估其长上下文记忆和时序推理能力的任务和数据集。受LLM领域needle in a haystack测试的启发，作者希望提出适配视频场景的对应评测方法。

Method: 作者设计了NeMo任务，用以检验模型在长时间视频中的关键细节回忆与时序定位能力。为此，开发了海量高质量自动化生成问答数据的管线。基于该管线，构建了NeMoBench数据集，包含31,378个QA对，覆盖13,486个不同长度视频。随后对20个先进模型进行了评测。

Result: 实验表明，作者的自动化数据生成管线能够稳定、高质量地生成评测数据，NeMoBench可不断迭代更新扩充视频内容。对比评测揭示了当前模型在时序推理等方面的能力上限与不足。

Conclusion: NeMoBench为VideoLLMs的时序推理和长上下文理解能力评测提供了新的标准和平台，有助于推动相关模型能力的持续提升。

Abstract: Recent advances in video large language models (VideoLLMs) call for new
evaluation protocols and benchmarks for complex temporal reasoning in
video-language understanding. Inspired by the needle in a haystack test widely
used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed
to assess VideoLLMs' critical reasoning capabilities, including long-context
recall and temporal grounding. To generate video question answering data for
our task, we develop a scalable automated data generation pipeline that
facilitates high-quality data synthesis. Built upon the proposed pipeline, we
present NeMoBench, a video-language benchmark centered on our task.
Specifically, our full set of NeMoBench features 31,378 automatically generated
question-answer (QA) pairs from 13,486 videos with various durations ranging
from seconds to hours. Experiments demonstrate that our pipeline can reliably
and automatically generate high-quality evaluation data, enabling NeMoBench to
be continuously updated with the latest videos. We evaluate 20 state-of-the-art
models on our benchmark, providing extensive results and key insights into
their capabilities and limitations. Our project page is available at:
https://lavi-lab.github.io/NeMoBench.

</details>


### [216] [GANji: A Framework for Introductory AI Image Generation](https://arxiv.org/abs/2509.24128)
*Chandon Hamel,Mike Busch*

Main category: cs.CV

TL;DR: 本文提出了GANji框架，用于针对日文汉字图像数据集对主流生成模型进行便捷对比，兼顾验证效率与结果分析。


<details>
  <summary>Details</summary>
Motivation: 当前主流生成模型的对比评测通常需要大量计算资源，降低了研究和应用的门槛。

Method: 作者提出了GANji框架，基于包含10,314个日文汉字的数据集，系统对比了三大生成模型：VAE、GAN和DDPM，并通过FID等指标量化性能差异。

Result: 结果显示，DDPM在图像质量（FID=26.2）上最佳，但采样速度比其他模型慢2000倍。

Conclusion: GANji框架以低计算成本揭示了生成模型在架构、计算效率和视觉质量间的核心权衡，适合教学和研究场景使用。

Abstract: The comparative study of generative models often requires significant
computational resources, creating a barrier for researchers and practitioners.
This paper introduces GANji, a lightweight framework for benchmarking
foundational AI image generation techniques using a dataset of 10,314 Japanese
Kanji characters. It systematically compares the performance of a Variational
Autoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising
Diffusion Probabilistic Model (DDPM). The results demonstrate that while the
DDPM achieves the highest image fidelity, with a Fr\'echet Inception Distance
(FID) score of 26.2, its sampling time is over 2,000 times slower than the
other models. The GANji framework is an effective and accessible tool for
revealing the fundamental trade-offs between model architecture, computational
cost, and visual quality, making it ideal for both educational and research
purposes.

</details>


### [217] [MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment](https://arxiv.org/abs/2509.24888)
*Fankai Jia,Daisong Gan,Zhe Zhang,Zhaochi Wen,Chenchen Dan,Dong Liang,Haifeng Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MMRQA的多模态MRI质量评估框架，融合了大语言模型与信号处理以提升医学影像评估的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: MRI质量评估对于临床决策至关重要，但受限于数据稀缺和协议多样。现有信号方法虽能量化但缺乏语义理解，深度学习方法准确但可解释性差。作者旨在突破这一平衡困境。

Method: MMRQA框架有三大创新：利用MRQy结合模拟伪影进行健壮的指标提取；使用Qwen将指标结构化为问答对；通过LoRA参数高效融入LLaVA-OneVision，实现多模态融合。

Result: 在MR-ART、FastMRI和MyConnectome等多个基准上，MMRQA表现为最优，尤其是在零样本泛化能力上表现突出，并通过消融实验得到验证。

Conclusion: 该框架实现了定量分析与语义推理的结合，输出具有临床可解释性的评估结果，为医学影像动态质量控制带来新的提升。

Abstract: Magnetic resonance imaging (MRI) quality assessment is crucial for clinical
decision-making, yet remains challenging due to data scarcity and protocol
variability. Traditional approaches face fundamental trade-offs: signal-based
methods like MRIQC provide quantitative metrics but lack semantic
understanding, while deep learning approaches achieve high accuracy but
sacrifice interpretability. To address these limitations, we introduce the
Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration
of multimodal large language models (MLLMs) with acquisition-aware signal
processing. MMRQA combines three key innovations: robust metric extraction via
MRQy augmented with simulated artifacts, structured transformation of metrics
into question-answer pairs using Qwen, and parameter-efficient fusion through
Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,
and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with
strong zero-shot generalization, as validated by comprehensive ablation
studies. By bridging quantitative analysis with semantic reasoning, our
framework generates clinically interpretable outputs that enhance quality
control in dynamic medical settings.

</details>


### [218] [TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.25143)
*Junyi Zhang,Jia-Chen Gu,Wenbo Hu,Yu Zhou,Robinson Piramuthu,Nanyun Peng*

Main category: cs.CV

TL;DR: 该论文提出了首个用于分析患者跨临床访问间病情变化的医疗视觉-语言基准TemMed-Bench，并评测了多种大视觉语言模型（LVLMs）在此任务上的表现，发现当前主流模型在时序医疗影像推理中大多表现有限。作者还探讨了多模态检索增强方式在此场景下的效果，显示性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉-语言模型评测主要针对单次就诊的静态图像，未能反映真实临床医生需要依据病人历史多次就诊数据综合判断的场景。为了更贴近期实际应用，需建立能够考查模型对患者病情演变推理能力的基准。

Method: 作者提出了TemMed-Bench基准，涵盖视觉问答、报告生成及图像对选择三项任务，并提供包含17,000多个实例的知识语料库。使用该基准，系统评测了12个主流大视觉语言模型，并探索了以视觉和文本双模态检索增强输入的方式提升模型表现。

Result: 实验发现，绝大多数LVLMs难以有效分析患者病情随时间变化，在无外部知识检索下表现接近随机水平。GPT o3/o4-mini和Claude 3.5 Sonnet表现相对较好。利用多模态检索增强提升了多数模型在基准上的表现，VQA任务平均提升2.59%。

Conclusion: 论文构建了符合临床实际需求的时序医疗影像推理基准，揭示了当前LVLMs在此类任务上的明显瓶颈，并验证了多模态检索增强方法的应用前景，为后续相关研究指明了方向。

Abstract: Existing medical reasoning benchmarks for vision-language models primarily
focus on analyzing a patient's condition based on an image from a single visit.
However, this setting deviates significantly from real-world clinical practice,
where doctors typically refer to a patient's historical conditions to provide a
comprehensive assessment by tracking their changes over time. In this paper, we
introduce TemMed-Bench, the first benchmark designed for analyzing changes in
patients' conditions between different clinical visits, which challenges large
vision-language models (LVLMs) to reason over temporal medical images.
TemMed-Bench consists of a test set comprising three tasks - visual
question-answering (VQA), report generation, and image-pair selection - and a
supplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we
conduct an evaluation of six proprietary and six open-source LVLMs. Our results
show that most LVLMs lack the ability to analyze patients' condition changes
over temporal medical images, and a large proportion perform only at a
random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini
and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they
have yet to reach the desired level. Furthermore, we explore augmenting the
input with both retrieved visual and textual modalities in the medical domain.
We also show that multi-modal retrieval augmentation yields notably higher
performance gains than no retrieval and textual retrieval alone across most
models on our benchmark, with the VQA task showing an average improvement of
2.59%. Overall, we compose a benchmark grounded on real-world clinical
practice, and it reveals LVLMs' limitations in temporal medical image
reasoning, as well as highlighting the use of multi-modal retrieval
augmentation as a potentially promising direction worth exploring to address
this challenge.

</details>


### [219] [EYE-DEX: Eye Disease Detection and EXplanation System](https://arxiv.org/abs/2509.24136)
*Youssef Sabiri,Walid Houmaidi,Amine Abouaomar*

Main category: cs.CV

TL;DR: 本论文提出EYE-DEX自动化框架，利用深度学习对眼底图像中的10种视网膜疾病进行分类，取得了92.36%的测试准确率，并通过Grad-CAM实现了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统眼科医生手动判读眼底图像既耗时又带有主观性，难以满足全球范围大量眼科筛查的需求。深度学习有望提升诊断效率与准确率，缓解眼科诊疗资源压力。

Method: 构建包含21,577张眼底图像的大规模Retinal Disease Dataset，采用VGG16、VGG19和ResNet50三种预训练卷积神经网络（CNN），进行微调并对10种视网膜疾病进行分类。使用Grad-CAM技术生成可视化的疾病区域解释结果。

Result: 微调后的VGG16模型取得了92.36%的全球基准测试准确率，优于其他模型。通过Grad-CAM实现了疾病特征区域的显著可视化，提高了AI模型的透明度和信任度。

Conclusion: EYE-DEX框架实现了高准确率和良好可解释性的视网膜疾病自动分类，展现了深度学习在眼科辅助诊断中的巨大潜力，有助于推动AI工具在临床中的广泛应用。

Abstract: Retinal disease diagnosis is critical in preventing vision loss and reducing
socioeconomic burdens. Globally, over 2.2 billion people are affected by some
form of vision impairment, resulting in annual productivity losses estimated at
$411 billion. Traditional manual grading of retinal fundus images by
ophthalmologists is time-consuming and subjective. In contrast, deep learning
has revolutionized medical diagnostics by automating retinal image analysis and
achieving expert-level performance. In this study, we present EYE-DEX, an
automated framework for classifying 10 retinal conditions using the large-scale
Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three
pre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and
ResNet50--with our finetuned VGG16 achieving a state-of-the-art global
benchmark test accuracy of 92.36%. To enhance transparency and explainability,
we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM)
technique to generate visual explanations highlighting disease-specific
regions, thereby fostering clinician trust and reliability in AI-assisted
diagnostics.

</details>


### [220] [GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts](https://arxiv.org/abs/2509.25160)
*Fan Yuan,Yuchen Yan,Yifan Jiang,Haoran Zhao,Tao Feng,Jinyan Chen,Yanwei Lou,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出了GSM8K-V数据集，这是一个纯视觉化、多图像的数学推理新基准，通过系统地将文本版GSM8K转化为图片形式，用于评价视觉语言模型（VLMs）的复杂视觉数学推理能力。实验证明现有VLMs在视觉数学推理任务上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有视觉数学推理基准大多只覆盖几何领域，覆盖面狭窄，且缺乏对数学文字题以及多图像间联合推理的考察。为推动VLMs在更具挑战性的视觉数学推理领域的发展，亟需一个综合、更具代表性的新基准。

Method: 作者将主流文本数学推理数据集GSM8K通过自动图像生成与人工标注相结合的方法，系统性转化为图像版（GSM8K-V），涵盖1,319个高质量样本，对多种主流开源与闭源VLMs进行了系统性测试和对比分析。

Result: 文本版GSM8K上的最佳模型准确率高达95.22%，而在新提出的视觉版GSM8K-V上，最高准确率仅为46.93%。展示出现有VLMs在复杂视觉数学推理任务方面仍有明显短板。

Conclusion: 提出的GSM8K-V为视觉数学推理任务提供了新视角和新基准，展现和揭示了当前VLMs能力的显著局限性，为后续更强健、泛化性更佳的视觉语言模型研发指明了方向。

Abstract: Vision language models (VLMs) achieve unified modeling of images and text,
enabling them to accomplish complex real-world tasks through perception,
planning, and reasoning. Among these tasks, reasoning is particularly
representative, with mathematical reasoning serving as a prominent example. It
highlights the high-level capability of VLMs to comprehend mathematical
information in images and to perform sophisticated reasoning. Recently,
numerous visual mathematical reasoning benchmarks have been proposed, but they
are often restricted to geometry, lack coverage of math word problems, and
rarely assess reasoning across multiple images. To address these gaps, we
introduce GSM8K-V, a purely visual multi-image mathematical reasoning
benchmark. GSM8K-V is built by systematically mapping each sample from the
widely used text-based GSM8K into visual form. Through a carefully designed
automated image-generation pipeline combined with meticulous human annotation,
we curate 1,319 high-quality samples. We evaluate a wide range of open-source
and closed-source models on GSM8K-V. Results show that although existing VLMs
have nearly saturated performance on text-based GSM8K, there remains
substantial room for improvement on GSM8K-V. For example, the best-performing
model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on
GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the
limitations of current models as well as potential directions for improvement.
GSM8K-V offers a new perspective on visual mathematical reasoning and
establishes a benchmark to guide the development of more robust and
generalizable VLMs.

</details>


### [221] [Analysis of Bias in Deep Learning Facial Beauty Regressors](https://arxiv.org/abs/2509.24138)
*Chandon Hamel,Mike Busch*

Main category: cs.CV

TL;DR: 该论文指出，AI美貌预测系统即使在看似平衡的数据集上训练，也会表现出族裔偏差。通过对比分析两个人脸美貌预测模型，发现算法结果在各族裔间存在显著差异，并提出当前技术难以实现公平性，同时讨论了可能的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI在美学领域的广泛应用，基于面部特征的美貌预测模型应避免加剧社会偏见。本文关注于揭示和分析AI美貌预测中的族裔偏见问题。

Method: 选取SCUT-FBP5500和MEBeauty两个流行人脸美貌数据库训练模型，利用Kruskal-Wallis H检验和Dunn事后分析等严格统计方法对不同族裔的预测表现进行比较。此外，在FairFace这种分布均衡的数据集上做评估，以及交叉数据集验证，以检测偏差扩散情况。

Result: 结果显示，不同族裔间模型的美貌评分差异显著（p < 0.001），而且只有4.8%-9.5%的族裔组对满足分布一致性标准。交叉验证表明算法对社会美貌偏见不但未能减轻，反而有放大趋势。

Conclusion: 现有AI美貌预测模型在公平性方面存在严重不足。论文提出并详细讨论了缓解这些偏见的潜在方法，为构建更公正的美学AI系统指明了方向。

Abstract: Bias can be introduced to AI systems even from seemingly balanced sources,
and AI facial beauty prediction is subject to ethnicity-based bias. This work
sounds warnings about AI's role in shaping aesthetic norms while providing
potential pathways toward equitable beauty technologies through comparative
analysis of models trained on SCUT-FBP5500 and MEBeauty datasets. Employing
rigorous statistical validation (Kruskal-Wallis H-tests, post hoc Dunn
analyses). It is demonstrated that both models exhibit significant prediction
disparities across ethnic groups $(p < 0.001)$, even when evaluated on the
balanced FairFace dataset. Cross-dataset validation shows algorithmic
amplification of societal beauty biases rather than mitigation based on
prediction and error parity. The findings underscore the inadequacy of current
AI beauty prediction approaches, with only 4.8-9.5\% of inter-group comparisons
satisfying distributional parity criteria. Mitigation strategies are proposed
and discussed in detail.

</details>


### [222] [Asymmetric VAE for One-Step Video Super-Resolution Acceleration](https://arxiv.org/abs/2509.24142)
*Jianze Li,Yong Guo,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了一种高效的视频超分辨率模型FastVSR，在大幅提升推理速度的同时保证效果。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散模型的视频超分辨率方法取得了优异表现，但现有方法在推理效率上仍有很大提升空间，特别是需要进一步减少计算量以实现实际应用。

Method: 设计了高压缩率（f16）的VAE结构并引入稳定的训练框架，利用像素shuffle和通道复制实现额外上采样；提出以性能下界为指导的训练目标以提高训练稳定性和收敛速度。

Result: FastVSR在推理速度上比多步模型快111.9倍，比现有一步模型快3.92倍。

Conclusion: FastVSR在保持较好性能的同时极大提升了效率，有望推动扩散模型在视频超分辨率领域的实际应用。

Abstract: Diffusion models have significant advantages in the field of real-world video
super-resolution and have demonstrated strong performance in past research. In
recent diffusion-based video super-resolution (VSR) models, the number of
sampling steps has been reduced to just one, yet there remains significant room
for further optimization in inference efficiency. In this paper, we propose
FastVSR, which achieves substantial reductions in computational cost by
implementing a high compression VAE (spatial compression ratio of 16, denoted
as f16). We design the structure of the f16 VAE and introduce a stable training
framework. We employ pixel shuffle and channel replication to achieve
additional upsampling. Furthermore, we propose a lower-bound-guided training
strategy, which introduces a simpler training objective as a lower bound for
the VAE's performance. It makes the training process more stable and easier to
converge. Experimental results show that FastVSR achieves speedups of 111.9
times compared to multi-step models and 3.92 times compared to existing
one-step models. We will release code and models at
https://github.com/JianzeLi-114/FastVSR.

</details>


### [223] [Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework](https://arxiv.org/abs/2509.24149)
*Walid Houmaidi,Youssef Sabiri,Salmane El Mansour Billah,Amine Abouaomar*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BrainFusion的新方法，通过结合多个卷积神经网络和YOLOv8，实现了脑肿瘤MRI的高精度分类与定位，极大提升了诊断性能和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的早期和准确分类对于制定有效治疗和提高患者生存率至关重要，现有自动化MRI分析方法在准确率和定位精度上存在局限。

Method: 作者采用了VGG16、ResNet50和Xception三个经过微调的CNN模型对脑肿瘤进行分类，并结合YOLOv8实现肿瘤的精确定位，实现了分类与定位一体化。

Result: 实验结果显示，微调后的VGG16模型在测试集上取得了99.86%的准确率，显著高于以往研究。另外，系统集成的定位功能和可解释性AI方法提升了模型在临床中的可用性和可信度。

Conclusion: 该方法显著提升了脑肿瘤MRI智能分析的准确性和可解释性，有望加速并优化临床决策流程，改善患者预后和生存率。

Abstract: The early and accurate classification of brain tumors is crucial for guiding
effective treatment strategies and improving patient outcomes. This study
presents BrainFusion, a significant advancement in brain tumor analysis using
magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural
networks (CNNs) for tumor classification--including VGG16, ResNet50, and
Xception--with YOLOv8 for precise tumor localization with bounding boxes.
Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the
fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially
exceeding previous benchmarks. Beyond setting a new accuracy standard, the
integration of bounding-box localization and explainable AI techniques further
enhances both the clinical interpretability and trustworthiness of the system's
outputs. Overall, this approach underscores the transformative potential of
deep learning in delivering faster, more reliable diagnoses, ultimately
contributing to improved patient care and survival rates.

</details>


### [224] [LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis](https://arxiv.org/abs/2509.24165)
*Moxin Zhao,Nan Meng,Jason Pui Yin Cheung,Chris Yuk Kwan Tang,Chenxi Yu,Wenting Zhong,Pengyu Lu,Chang Shi,Yipeng Zhuang,Teng Zhang*

Main category: cs.CV

TL;DR: 本文提出LatXGen，一种可由无放射性的RGBD图像合成真实侧位脊柱X光片的新方法，用于青少年特发性脊柱侧弯（AIS）的矢状面测量。实验表明其准确性高，优于现有GAN方法。


<details>
  <summary>Details</summary>
Motivation: AIS是一种复杂的三维脊柱畸形，矢状面评估对诊断和治疗极为重要。目前虽有无辐射冠状面测量进展，矢状面评估尚缺有效的无放射性方法。受到辐射风险和对全面评估需求双重驱动，作者致力于提供更安全的AIS矢状面测量工具。

Method: LatXGen采用双阶段架构，利用后位RGBD裸背照片合成逼真的侧位脊柱X光片。创新点包含：引入注意力型快速傅里叶卷积（FFC）模块融合RGBD与3D解剖特征，利用空间变形网络（SDN）建模矢状面形态变化，并构建了配对的大规模RGBD-侧位X光片数据集（3,264对）。

Result: 实验显示LatXGen生成的X光片在解剖准确性和视觉质量上均优于现有GAN生成方法，在定量指标和感官对比方面表现突出。

Conclusion: LatXGen为AIS矢状面无放射性评估提供了新颖且高效的解决方案，有助于推进AIS的全面且安全的形态学测量和临床应用。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal
deformity, and accurate morphological assessment requires evaluating both
coronal and sagittal alignment. While previous research has made significant
progress in developing radiation-free methods for coronal plane assessment,
reliable and accurate evaluation of sagittal alignment without ionizing
radiation remains largely underexplored. To address this gap, we propose
LatXGen, a novel generative framework that synthesizes realistic lateral spinal
radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed
backs. This enables accurate, radiation-free estimation of sagittal spinal
alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal
morphology changes from a lateral perspective based on posteroanterior surface
geometry, and (2) performing cross-modality translation from RGBD input to the
radiographic domain. The framework adopts a dual-stage architecture that
progressively estimates lateral spinal structure and synthesizes corresponding
radiographs. To enhance anatomical consistency, we introduce an attention-based
Fast Fourier Convolution (FFC) module for integrating anatomical features from
RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model
morphological variations in the lateral view. Additionally, we construct the
first large-scale paired dataset for this task, comprising 3,264 RGBD and
lateral radiograph pairs. Experimental results demonstrate that LatXGen
produces anatomically accurate radiographs and outperforms existing GAN-based
methods in both visual fidelity and quantitative metrics. This study offers a
promising, radiation-free solution for sagittal spine assessment and advances
comprehensive AIS evaluation.

</details>


### [225] [High-Order Progressive Trajectory Matching for Medical Image Dataset Distillation](https://arxiv.org/abs/2509.24177)
*Le Dong,Jinghao Bian,Jingyang Hou,Jingliang Hu,Yilei Shi,Weisheng Dong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 本文针对医疗影像分析中因隐私受限导致的数据共享难题，提出了一种新的数据集蒸馏方法来合成小型数据集，并通过几何结构感知和递进式匹配提升医学数据集蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据难以共享，影响AI模型开发和验证。数据集蒸馏可压缩信息，同时保护隐私，但现有基于轨迹匹配方法只关注末端状态，忽略了优化过程中的重要中间信息。

Method: 提出了shape-wise potential（几何结构感知潜能），捕捉参数轨迹的几何结构；并采用easy-to-complex匹配策略，根据参数复杂度递进式优化，逐步对齐优化路径中的关键点。

Result: 在医学影像分类任务上，所提方法能生成极小但有效的合成数据集，提升了蒸馏效果，同时在保护隐私的前提下，模型精度基本等同于原始完整数据集训练结果。

Conclusion: 新方法在不泄露隐私前提下有效提升了医学图像数据集的蒸馏性能，为医学人工智能模型开发、验证和数据共享提供了实用途径。

Abstract: Medical image analysis faces significant challenges in data sharing due to
privacy regulations and complex institutional protocols. Dataset distillation
offers a solution to address these challenges by synthesizing compact datasets
that capture essential information from real, large medical datasets.
Trajectory matching has emerged as a promising methodology for dataset
distillation; however, existing methods primarily focus on terminal states,
overlooking crucial information in intermediate optimization states. We address
this limitation by proposing a shape-wise potential that captures the geometric
structure of parameter trajectories, and an easy-to-complex matching strategy
that progressively addresses parameters based on their complexity. Experiments
on medical image classification tasks demonstrate that our method improves
distillation performance while preserving privacy and maintaining model
accuracy comparable to training on the original datasets. Our code is available
at https://github.com/Bian-jh/HoP-TM.

</details>


### [226] [Combining Discrepancy-Confusion Uncertainty and Calibration Diversity for Active Fine-Grained Image Classification](https://arxiv.org/abs/2509.24181)
*Yinghao Jin,Xi Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的主动学习方法DECERN，通过结合不确定性和多样性，更有效地在细粒度图像分类任务中选取有价值样本。实验结果优于当前先进方法。


<details>
  <summary>Details</summary>
Motivation: 细粒度图像分类中，类别间差异细微，难以评估样本的信息量，影响主动学习的效果。因此，亟需更有效的判别和选择方法。

Method: DECERN结合了不确定性和多样性两方面：一是提出不确定性融合（discrepancy-confusion uncertainty），量化未标记样本的类别方向性和结构稳定性，二是通过加权聚类实现不确定样本的多样化选择，并进一步校准多样性，确保全局多样性和局部代表性。

Result: 在7个细粒度图像数据集和26种实验设置中，DECERN方法在主动学习任务上取得了优于以往主流方法的性能。

Conclusion: DECERN能更好评估与选择细粒度数据集中的有价值样本，显著提升主动学习下的图像分类性能。

Abstract: Active learning (AL) aims to build high-quality labeled datasets by
iteratively selecting the most informative samples from an unlabeled pool under
limited annotation budgets. However, in fine-grained image classification,
assessing this informativeness is especially challenging due to subtle
inter-class differences. In this paper, we introduce a novel method, combining
discrepancy-confusion uncertainty and calibration diversity for active
fine-grained image classification (DECERN), to effectively perceive the
distinctiveness between fine-grained images and evaluate the sample value.
DECERN introduces a multifaceted informativeness measure that combines
discrepancy-confusion uncertainty and calibration diversity. The
discrepancy-confusion uncertainty quantifies the category directionality and
structural stability of fine-grained unlabeled data during local feature
fusion. Subsequently, uncertainty-weighted clustering is performed to diversify
the uncertainty samples. Then we calibrate the diversity to maximize the global
diversity of the selected sample while maintaining its local
representativeness. Extensive experiments conducted on 7 fine-grained image
datasets across 26 distinct experimental settings demonstrate that our method
achieves superior performance compared to state-of-the-art methods.

</details>


### [227] [Tumor Synthesis conditioned on Radiomics](https://arxiv.org/abs/2509.24182)
*Jonghun Kim,Inye Na,Eun Sook Ko,Hyunjin Park*

Main category: cs.CV

TL;DR: 本文提出了一种基于放射组学特征的肿瘤生成模型，结合了GAN和扩散模型，可根据用户指定的肿瘤特征在不同位置合成多样化、真实感强的3D医学肿瘤影像，已在多器官CT/MRI数据集上验证，并在下游任务和专家评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析领域由于隐私问题很难获得大规模高质量数据，特别是在3D CT和MRI影像上。现有的生成模型生成能力有限，难以满足医学肿瘤影像的多样性和真实性需求。因此，亟需能依据丰富肿瘤语义特征生成多样化医学影像的新方法，以辅助研究和临床应用。

Method: 作者提出利用放射组学（radiomics）手工设计的高维生物语义特征作为生成条件，结合GAN生成肿瘤mask，扩散模型生成具有指定特征肿瘤纹理，实现以用户指定特征（如大小、形状、纹理）在任意位置生成肿瘤。支持肿瘤的去除、变换和重定位，可生成不同类型和场景下的肿瘤影像。模型在四种器官（肾、肺、乳腺、脑）的CT和MRI数据上验证。

Result: 合成的肿瘤影像已有效辅助下游任务训练，并通过专家评估验证了其真实性和有效性，显示出较高的实用价值。

Conclusion: 该方法可依据放射组学特征合成多样化、真实可信的3D肿瘤影像，支持个性化调整肿瘤位置和特征。模型有潜力支持下游AI任务和临床治疗方案规划，解决了医学影像数据短缺和多样性不足的难题。

Abstract: Due to privacy concerns, obtaining large datasets is challenging in medical
image analysis, especially with 3D modalities like Computed Tomography (CT) and
Magnetic Resonance Imaging (MRI). Existing generative models, developed to
address this issue, often face limitations in output diversity and thus cannot
accurately represent 3D medical images. We propose a tumor-generation model
that utilizes radiomics features as generative conditions. Radiomics features
are high-dimensional handcrafted semantic features that are biologically
well-grounded and thus are good candidates for conditioning. Our model employs
a GAN-based model to generate tumor masks and a diffusion-based approach to
generate tumor texture conditioned on radiomics features. Our method allows the
user to generate tumor images according to user-specified radiomics features
such as size, shape, and texture at an arbitrary location. This enables the
physicians to easily visualize tumor images to better understand tumors
according to changing radiomics features. Our approach allows for the removal,
manipulation, and repositioning of tumors, generating various tumor types in
different scenarios. The model has been tested on tumors in four different
organs (kidney, lung, breast, and brain) across CT and MRI. The synthesized
images are shown to effectively aid in training for downstream tasks and their
authenticity was also evaluated through expert evaluations. Our method has
potential usage in treatment planning with diverse synthesized tumors.

</details>


### [228] [Simulating Post-Neoadjuvant Chemotherapy Breast Cancer MRI via Diffusion Model with Prompt Tuning](https://arxiv.org/abs/2509.24185)
*Jonghun Kim,Hyunjin Park*

Main category: cs.CV

TL;DR: 本研究提出一种基于扩散模型的方法，通过乳腺癌新辅助化疗（NAC）前后的DCE-MRI影像，来预测化疗后的影像变化，并能更好地反映肿瘤治疗反应。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌患者在手术前常接受新辅助化疗，其疗效通过动态对比增强MRI（DCE-MRI）随访观察。准确预测NAC反应有助于优化后续治疗方案，需要提升影像生成和疗效反应反映的准确性。

Method: 采用DCE-MRI的最大强度投影影像，利用扩散模型从化疗前影像生成化疗后（3周或12周）影像。引入prompt tuning以考虑已知影响NAC反应的临床因素，并与其他生成模型进行对比。

Result: 提出的模型在影像质量指标上优于其他生成方法，且在反映肿瘤pCR（病理完全缓解）相关肿瘤体积变化方面亦表现更佳。消融实验支持所选的设计方案有效性。

Conclusion: 该方法可望提升乳腺癌新辅助化疗反应的个体化预测，并有助于推动精准医疗的发展。

Abstract: Neoadjuvant chemotherapy (NAC) is a common therapy option before the main
surgery for breast cancer. Response to NAC is monitored using follow-up dynamic
contrast-enhanced magnetic resonance imaging (DCE-MRI). Accurate prediction of
NAC response helps with treatment planning. Here, we adopt maximum intensity
projection images from DCE-MRI to generate post-treatment images (i.e., 3 or 12
weeks after NAC) from pre-treatment images leveraging the emerging diffusion
model. We introduce prompt tuning to account for the known clinical factors
affecting response to NAC. Our model performed better than other generative
models in image quality metrics. Our model was better at generating images that
reflected changes in tumor size according to pCR compared to other models.
Ablation study confirmed the design choices of our method. Our study has the
potential to help with precision medicine.

</details>


### [229] [Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection](https://arxiv.org/abs/2509.24192)
*Sojung An,Kwanyong Park,Yong Jae Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 本文发现现有视觉语言模型（VLMs）在处理包含描述性属性和关系从句的复杂语言查询时表现有限，原因在于其文本编码器未有效区分目标对象、属性和关系。作者提出了TaSe框架，通过对句子结构的分层重组，实现了对象、属性、关系的有效解耦和聚合，从而提升了模型性能，实验在OmniLabel基准上性能提升24%。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs尽管对于简单查询有优异表现，但遇到涉及描述性属性和关系的复杂查询时效果较差。分析发现，问题主要源自文本编码器未能处理句子层级关系，导致目标、属性、关系混淆，影响检测精准性。解决这一挑战，对于提升多模态感知能力和实际应用有重要意义。

Method: 作者提出基于分层语言结构的TaSe框架，包括三大创新：一是构造层次化合成描述数据集，覆盖类别到复杂句子的多个层次；二是'Talk in Pieces'模块，通过新颖的解耦损失函数，将文本嵌入分解为对象、属性、关系三个子空间；三是'See in Whole'模块，将上述分离的成分按句子层级有序聚合，利用分层目标学习更具判别性的语言建模。

Result: 在OmniLabel基准评测中，TaSe框架显著提升了复杂语言查询下的目标检测能力，表现为24%的性能提升，优于现有主流VLMs方法，验证了层次化语言结构建模的重要性。

Conclusion: TaSe框架通过对句子结构的分层解耦和整合，强化了VLMs对语言层级组合性的学习，显著提高了复杂语言条件下的多模态目标检测能力。此方法为视觉语言模型引入更强的语言结构归纳偏置，对实现更精准的多模态感知具有理论与应用价值。

Abstract: While vision-language models (VLMs) have made significant progress in
multimodal perception (e.g., open-vocabulary object detection) with simple
language queries, state-of-the-art VLMs still show limited ability to perceive
complex queries involving descriptive attributes and relational clauses. Our
in-depth analysis shows that these limitations mainly stem from text encoders
in VLMs. Such text encoders behave like bags-of-words and fail to separate
target objects from their descriptive attributes and relations in complex
queries, resulting in frequent false positives. To address this, we propose
restructuring linguistic representations according to the hierarchical
relations within sentences for language-based object detection. A key insight
is the necessity of disentangling textual tokens into core components-objects,
attributes, and relations ("talk in pieces")-and subsequently aggregating them
into hierarchically structured sentence-level representations ("see in whole").
Building on this principle, we introduce the TaSe framework with three main
contributions: (1) a hierarchical synthetic captioning dataset spanning three
tiers from category names to descriptive sentences; (2) Talk in Pieces, the
three-component disentanglement module guided by a novel disentanglement loss
function, transforms text embeddings into subspace compositions; and (3) See in
Whole, which learns to aggregate disentangled components into hierarchically
structured embeddings with the guide of proposed hierarchical objectives. The
proposed TaSe framework strengthens the inductive bias of hierarchical
linguistic structures, resulting in fine-grained multimodal representations for
language-based object detection. Experimental results under the OmniLabel
benchmark show a 24% performance improvement, demonstrating the importance of
linguistic compositionality.

</details>


### [230] [An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation](https://arxiv.org/abs/2509.24194)
*Zach Eidex,Mojtaba Safari,Jie Ding,Richard Qiu,Justin Roper,David Yu,Hui-Kuo Shu,Zhen Tian,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D深度学习的高效生成框架（T1C-RFlow），能够利用预对比MRI数据合成类似T1-对比增强（T1C）MRI图像，为不能使用造影剂的患者提供替代方案，并具备优越的重建效果与大幅度提升的运算速度。


<details>
  <summary>Details</summary>
Motivation: 传统的钆基对比剂（GBCA）增强MRI因对某些肾功能不全患者存在风险，且造影剂使用的一致性问题影响影像结果，亟需一种无需对比剂、但仍能精准获取T1C图像的替代方法。

Method: 提出了一种基于3D潜在空间矫正流（Rectified Flow）扩散模型（T1C-RFlow）。具体流程为将T1w和T2-FLAIR输入预训练自编码器获得潜在表示，再在该潜在空间中训练扩散生成模型，最终实现高效T1C图像合成。数据来自BraTS 2024的三类脑肿瘤数据集，分为训练、验证和测试集进行评估。

Result: 无论是定性还是定量上，T1C-RFlow均超越了在相同潜在空间训练的3D基线模型（包括pix2pix、DDPM、DiT-3D）。在三个数据集上，该方法的NMSE和SSIM指标均表现突出，且重建速度远快于传统方法，单体积平均仅需6.9秒。

Conclusion: T1C-RFlow能在无造影剂情况下高效生成与真实T1C极为接近的合成图像，并明显加快重建速度，为免造影剂脑肿瘤MRI提供了实用的发展方向。

Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed
with T1w MRI to enhance lesion visualization but are restricted in patients at
risk of nephrogenic systemic fibrosis and variations in GBCA administration can
introduce imaging inconsistencies. This study develops an efficient 3D
deep-learning framework to generate T1-contrast enhanced images (T1C) from
pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified
flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and
T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient
latent space representation. A rectified flow diffusion model is then trained
in this latent space representation. The T1C-RFlow model was trained on a
curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients),
meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets.
Selected patients were split into train (N=2860), validation (N=612), and test
(N=614) sets. Results: Both qualitative and quantitative results demonstrate
that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM,
Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow
achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/-
0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/-
0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction
performance and significantly faster denoising times (6.9 s/volume, 200 steps)
than conventional DDPM models in both latent space (37.7s, 1000 steps) and
patch-based in image space (4.3 hr/volume). Significance: Our proposed method
generates synthetic T1C images that closely resemble ground truth T1C in much
less time than previous diffusion models. Further development may permit a
practical method for contrast-agent-free MRI for brain tumors.

</details>


### [231] [UniVid: The Open-Source Unified Video Model](https://arxiv.org/abs/2509.24200)
*Jiabin Luo,Junhui Lin,Zeyu Zhang,Biao Wu,Meng Fang,Ling Chen,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了UniVid架构，通过轻量级适配器将多模态大模型(MLLM)与扩散解码器结合，实现了视频理解与生成的统一。UniVid引入了温度模态对齐及金字塔反射机制，有效提升了模型对于提示的依从性和时序推理能力，在多个基准测试中达到了最新的最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成-理解统一模型面临两大挑战：一是流动生成中语义保持困难，二是高效迁移图像中心型MLLM到视频领域的成本高昂。因此，亟需一种能兼顾生成与理解效果、且能高效利用现有MLLM的视频统一模型。

Method: 提出UniVid架构，利用轻量适配器将MLLM与扩散解码器结合，支持视频生成和理解任务。引入温度模态对齐机制以缓解文本-视觉token不平衡、增强指令遵循性，并通过金字塔反射机制动态选取关键帧，实现高效的时序信息建模。

Result: UniVid在VBench-Long、MSVD-QA 及 ActivityNet-QA 等标准基准测试上取得优异成绩，分别在VBench-Long总分上超越EasyAnimateV5.1 2.2%，在MSVD-QA和ActivityNet-QA上比最好7B基线提高了1.0%和3.3%的准确率。

Conclusion: UniVid成功实现了视频生成与理解能力的统一，在保持或提升语义及时序建模效果的同时，具备优良的泛化能力且适应多种下游任务，达到了当前该领域的最新水平。

Abstract: Unified video modeling that combines generation and understanding
capabilities is increasingly important but faces two key challenges:
maintaining semantic faithfulness during flow-based generation due to
text-visual token imbalance and the limitations of uniform cross-modal
attention across the flow trajectory, and efficiently extending image-centric
MLLMs to video without costly retraining. We present UniVid, a unified
architecture that couples an MLLM with a diffusion decoder through a
lightweight adapter, enabling both video understanding and generation. We
introduce Temperature Modality Alignment to improve prompt adherence and
Pyramid Reflection for efficient temporal reasoning via dynamic keyframe
selection. Extensive experiments on standard benchmarks demonstrate
state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total
score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA
and ActivityNet-QA, respectively, compared with the best prior 7B baselines.

</details>


### [232] [BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation](https://arxiv.org/abs/2509.24204)
*Zelin Liu,Sicheng Dong,Bocheng Li,Yixuan Yang,Jiacheng Ruan,Chenxu Zhou,Suncheng Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种结构创新的适配方法（BALR-SAM），让SAM等大型通用视觉基础模型能高效适应医学影像分割场景，且只需极少参数就能达到甚至超越现有主流方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大规模预训练的视觉模型（如SAM）在自然图像上效果优异，但因缺乏医学领域特定适应性，直接用于医学影像分割表现并不理想。而在临床中，如何以最小的资源消耗和参数更新，实现模型高性能适应医学任务，是实际亟需解决的难题。

Method: 提出BALR-SAM，包含三个核心模块：1）基于深度可分离卷积与多尺度融合的边界感知细节增强网络（CDEN），强化医学图像分割中关键的边界特征；2）在SAM主干Vision Transformer插入低秩适配器模块，优化特征表达与注意力结构，同时极大压缩参数空间；3）在掩码解码器引入低秩张量注意力机制，减少75%内存用量并加速推理。

Result: 在多个标准医学分割数据集上，BALR-SAM无需额外提示词，仅更新1.8%（约1170万）参数，即超越全参数微调的MedSAM等多种SOTA方法。

Conclusion: BALR-SAM能极低成本、高效率地将通用视觉基础大模型适配到医学影像分割领域，显著提升了分割精度和资源利用效率。

Abstract: Vision foundation models like the Segment Anything Model (SAM), pretrained on
large-scale natural image datasets, often struggle in medical image
segmentation due to a lack of domain-specific adaptation. In clinical practice,
fine-tuning such models efficiently for medical downstream tasks with minimal
resource demands, while maintaining strong performance, is challenging. To
address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation
framework that enhances SAM for medical imaging. It combines three tailored
components: (1) a Complementary Detail Enhancement Network (CDEN) using
depthwise separable convolutions and multi-scale fusion to capture
boundary-sensitive features essential for accurate segmentation; (2) low-rank
adapters integrated into SAM's Vision Transformer blocks to optimize feature
representation and attention for medical contexts, while simultaneously
significantly reducing the parameter space; and (3) a low-rank tensor attention
mechanism in the mask decoder, cutting memory usage by 75% and boosting
inference speed. Experiments on standard medical segmentation datasets show
that BALR-SAM, without requiring prompts, outperforms several state-of-the-art
(SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8%
(11.7M) of its parameters.

</details>


### [233] [Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos](https://arxiv.org/abs/2509.24209)
*Yingdong Hu,Yisheng He,Jinnan Chen,Weihao Yuan,Kejie Qiu,Zehong Lin,Siyu Zhu,Zilong Dong,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出Forge4D，一种面向动态图像流的高效4D人体重建与插值方法，可从未经标定的稀疏视角视频中即时重建和插值4D人体，实现新视角与新时刻的生成。


<details>
  <summary>Details</summary>
Motivation: 当前的动态3D人体重建方法要么重建速度慢，要么无法生成新时刻的数据，限制了其实用性，因此亟需一种支持高效且时序一致的4D人体重建框架。

Method: Forge4D将4D重建与插值问题简化为流式3D高斯重建与稠密运动预测的联合任务。具体方法包括：(1) 首先用稀疏、未经标定的多视角图像静态重建3D高斯点云；(2) 引入可学习状态token，高效保持不同时间点间的信息一致性；(3) 设计新型运动预测模块，预测每个高斯点在相邻帧之间的稠密运动，还结合遮挡感知的高斯融合，以实现任意时刻的插值重建；(4) 提出自监督retargeting损失和基于遮挡的光流损失，在无稠密运动GT下优化运动预测模块。

Result: 在多组数据集（包括数据集内和外部测试集）上，Forge4D在速度和时序插值方面均表现出明显优势，实验结果展示了其高效性与泛化能力。

Conclusion: Forge4D不仅提升了动态4D人体重建的速度和插值表现，还支持新视角与新时刻的生成，弥补了现有方法的不足，具有很强的应用前景。

Abstract: Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view
videos is critical for numerous downstream applications. Existing methods,
however, are either limited by the slow reconstruction speeds or incapable of
generating novel-time representations. To address these challenges, we propose
Forge4D, a feed-forward 4D human reconstruction and interpolation model that
efficiently reconstructs temporally aligned representations from uncalibrated
sparse-view videos, enabling both novel view and novel time synthesis. Our
model simplifies the 4D reconstruction and interpolation problem as a joint
task of streaming 3D Gaussian reconstruction and dense motion prediction. For
the task of streaming 3D Gaussian reconstruction, we first reconstruct static
3D Gaussians from uncalibrated sparse-view images and then introduce learnable
state tokens to enforce temporal consistency in a memory-friendly manner by
interactively updating shared information across different timestamps. For
novel time synthesis, we design a novel motion prediction module to predict
dense motions for each 3D Gaussian between two adjacent frames, coupled with an
occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at
arbitrary timestamps. To overcome the lack of the ground truth for dense motion
supervision, we formulate dense motion prediction as a dense point matching
task and introduce a self-supervised retargeting loss to optimize this module.
An additional occlusion-aware optical flow loss is introduced to ensure motion
consistency with plausible human movement, providing stronger regularization.
Extensive experiments demonstrate the effectiveness of our model on both
in-domain and out-of-domain datasets. Project page and code at:
https://zhenliuzju.github.io/huyingdong/Forge4D.

</details>


### [234] [Scalable Audio-Visual Masked Autoencoders for Efficient Affective Video Facial Analysis](https://arxiv.org/abs/2509.24214)
*Xuecheng Wu,Junxiao Xue,Xinyi Yin,Yunyun Shi,Liangyu Fu,Danlei Huang,Yifan Wang,Jia Zhang,Jiayu Nie,Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于情感视频人脸分析（AVFA）的新型音视频自监督学习模型AVF-MAE++，通过多项创新方法有效提升该领域的性能，并在17个数据集、多项任务上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: AVFA领域因数据有限，难以提升情感识别性能，同时缺乏对自监督Masked Autoencoder（MAE）扩展规模性以及音视频跨模态协同建模的深入探究。

Method: 提出AVF-MAE++模型，包含：1）创新的双模态掩码策略，同时对音频和视觉数据掩码学习；2）增强的模态编码器设计，提升跨模态表达能力及可扩展性；3）引入迭代式音视频相关性学习模块，提高SSL下模态间协同能力；4）采用渐进式语义注入机制，将训练过程分为三阶段，平滑适应不同任务并减少过拟合。

Result: 在17个数据集、涵盖三大AVFA任务的大量实验中，AVF-MAE++在各基准数据上均达到一致的SOTA（最优）表现。消融实验显示每一创新模块对最终提升均有关键贡献。

Conclusion: AVF-MAE++能够有效缓解AVFA领域数据不足与跨模态学习难题，显著提高音视频情感理解的性能，并为音视频自监督、跨模态学习提供了新范式。代码已开源，有望推动情感智能系统发展。

Abstract: Affective video facial analysis (AVFA) has emerged as a key research field
for building emotion-aware intelligent systems, yet this field continues to
suffer from limited data availability. In recent years, the self-supervised
learning (SSL) technique of Masked Autoencoders (MAE) has gained momentum, with
growing adaptations in its audio-visual contexts. While scaling has proven
essential for breakthroughs in general multi-modal learning domains, its
specific impact on AVFA remains largely unexplored. Another core challenge in
this field is capturing both intra- and inter-modal correlations through
scalable audio-visual representations. To tackle these issues, we propose
AVF-MAE++, a family of audio-visual MAE models designed to efficiently
investigate the scaling properties in AVFA while enhancing cross-modal
correlation modeling. Our framework introduces a novel dual masking strategy
across audio and visual modalities and strengthens modality encoders with a
more holistic design to better support scalable pre-training. Additionally, we
present the Iterative Audio-Visual Correlation Learning Module, which improves
correlation learning within the SSL paradigm, bridging the limitations of
previous methods. To support smooth adaptation and reduce overfitting risks, we
further introduce a progressive semantic injection strategy, organizing the
model training into three structured stages. Extensive experiments conducted on
17 datasets, covering three major AVFA tasks, demonstrate that AVF-MAE++
achieves consistent state-of-the-art performance across multiple benchmarks.
Comprehensive ablation studies further highlight the importance of each
proposed component and provide deeper insights into the design choices driving
these improvements. Our code and models have been publicly released at Github.

</details>


### [235] [EVLF-FM: Explainable Vision Language Foundation Model for Medicine](https://arxiv.org/abs/2509.24231)
*Yang Bai,Haoran Cheng,Yang Zhou,Jun Zhou,Arun Thirunavukarasu,Yuhe Ke,Jie Yao,Kanae Fukutsu,Chrystie Wan Ning Quek,Ashley Hong,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Hiok Hong Chan,Victor Koh,Marcus Tan,Kelvin Z. Li,Leonard Yip,Ching Yu Cheng,Yih Chung Tham,Gavin Siew Wei Tan,Leopold Schmetterer,Marcus Ang,Rahat Hussain,Jod Mehta,Tin Aung,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Soon Thye Lim,Eyal Klang,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: cs.CV

TL;DR: EVLF-FM是一个多模态视觉-语言基础模型，能在多种医学影像上进行多疾病诊断和可解释性推理，其性能优于当前主流模型，并具备细粒度可视化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前医疗AI基础模型多为单一模态，且缺乏透明、可解释的推理过程，限制了在临床上的实际应用。本文致力于弥补这一空白，开发一种兼具多模态诊断能力与细粒度可解释性的基础模型。

Method: 作者开发了EVLF-FM模型，涵盖六大学科、十一种影像模态，使用全球23个数据集共130万样本进行训练与内部验证，并在10个新数据集进行外部测试。模型支持多疾病诊断、医学视觉问答、像素级视觉定位与推理，通过混合监督学习与视觉强化微调获得能力。

Result: EVLF-FM在疾病诊断方面，平均准确率（0.858）和F1分数（0.797）均优于现有主流通用与专科模型。视觉定位方面，9种模态平均mIOU为0.743及Acc@0.5为0.837。外部验证显示该模型在zero-shot和few-shot情景下依旧具备较强性能。

Conclusion: EVLF-FM具备可扩展的多疾病诊断、可解释性和推理能力，有望推动基础模型在实际临床中的采纳和信任，是一个具备开创意义的多模态、可解释医学视觉-语言模型。

Abstract: Despite the promise of foundation models in medical AI, current systems
remain limited - they are modality-specific and lack transparent reasoning
processes, hindering clinical adoption. To address this gap, we present
EVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify
broad diagnostic capability with fine-grain explainability. The development and
testing of EVLF-FM encompassed over 1.3 million total samples from 23 global
datasets across eleven imaging modalities related to six clinical specialties:
dermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology.
External validation employed 8,884 independent test samples from 10 additional
datasets across five imaging modalities. Technically, EVLF-FM is developed to
assist with multiple disease diagnosis and visual question answering with
pixel-level visual grounding and reasoning capabilities. In internal validation
for disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858)
and F1-score (0.797), outperforming leading generalist and specialist models.
In medical visual grounding, EVLF-FM also achieved stellar performance across
nine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External
validations further confirmed strong zero-shot and few-shot performance, with
competitive F1-scores despite a smaller model size. Through a hybrid training
strategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not
only achieves state-of-the-art accuracy but also exhibits step-by-step
reasoning, aligning outputs with visual evidence. EVLF-FM is an early
multi-disease VLM model with explainability and reasoning capabilities that
could advance adoption of and trust in foundation models for real-world
clinical deployment.

</details>


### [236] [When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs](https://arxiv.org/abs/2509.24258)
*Jinming Liu,Zhaoyang Jia,Jiahao Li,Bin Li,Xin Jin,Wenjun Zeng,Yan Lu*

Main category: cs.CV

TL;DR: 本文关注压缩技术在多模态大语言模型（MLLMs）边缘部署场景中的有效性，提出了一种针对MLLMs特点的自适应图像压缩编解码方法CoTAM，能够在大幅节省带宽的同时维持下游任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法主要服务于人眼系统（HVS），追求还原视觉保真度，却不能很好地适应以MLLMs为核心的下游多任务需求。而上传到云端MLLMs的信号数据若不高效压缩，将消耗过多带宽资源，影响端云协同效率。因此，亟需开发专项针对MLLMs任务表现而优化的压缩方法。

Method: 作者系统地分析了压缩失真对主流MLLMs的影响，发现不同特征层受压缩影响不一，进而影响不同下游任务表现。基于此，提出了CoTAM编解码器：其编码器利用CLIP浅层注意力生成特征重要性分布，引导码率分配以保护关键语义区域；解码器结合轻量级适配器与多层次损失函数，兼顾低级细节与高级语境还原，确保跨层特征的鲁棒合成。

Result: 实验表明，在保持MLLM下游任务性能不变的前提下，CoTAM较现有SOTA神经编解码器可节省最高35.99%的比特率。

Conclusion: CoTAM是一种针对MLLM特征与任务优化的编解码架构，在边缘到云端信号输入场景下，可显著节省带宽并维持优异的多任务表现，优于传统以及最新SOTA神经压缩方法。

Abstract: The increasing deployment of powerful Multimodal Large Language Models
(MLLMs), typically hosted on cloud platforms, urgently requires effective
compression techniques to efficiently transmit signal inputs (e.g., images,
videos) from edge devices with minimal bandwidth usage. However, conventional
image codecs are optimized for fidelity to serve the Human Visual System (HVS)
and ill-suited for MLLMs, in which diverse downstream tasks are jointly
considered. In this paper, we first systematically analyze the impact of
compression artifacts on several mainstream MLLMs. We find that: Compression
distortion unevenly impacts different-level image features, leading to varying
effects on MLLMs' downstream tasks depending on their feature-level reliance.
Motivated by this discovery, we propose an image Codec TAilored to MLLMs
(CoTAM) designed to adaptively protect multi-level features and suit different
demands of downstream tasks. The encoder leverages CLIP's shallow-layer
attention to generate an importance map for bit allocation, preserving critical
semantic regions. Concurrently, the decoder integrates a lightweight adapter
with a multi-level loss function to ensure the faithful reconstruction both of
low-level details and high-level semantic context for robust synthesis of
cross-level features. Extensive experiments validate that our method achieves
up to 35.99\% bitrate saving while maintaining the same performance on the MLLM
tasks, outperforming previous SOTA neural codecs.

</details>


### [237] [S$^2$NN: Sub-bit Spiking Neural Networks](https://arxiv.org/abs/2509.24266)
*Wenjie Wei,Malu Zhang,Jieyuan Zhang,Ammar Belatreche,Shuai Wang,Yimeng Shan,Hanwen Liu,Honglin Cao,Guoqing Wang,Yang Yang,Haizhou Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型亚比特脉冲神经网络（S²NNs），通过将权重压缩到低于1位，极大提升了SNN的存储和计算效率。研究还引入了OS-Quant和MPFD方法，能在保证模型高效的同时缓解性能损失，适用于端侧计算场景。


<details>
  <summary>Details</summary>
Motivation: 随着SNN在类脑智能中的发展，其能效优势逐渐受到重视，但大规模网络在存储和计算上仍有较高需求。现有二值SNN虽然压缩了模型，但距离极致高效还有差距。探索更激进的压缩方法能推动SNN在算力受限设备上的部署。

Method: 1. 利用二值SNN权重聚类模式提出了亚比特权重表示，建立S²NN基线模型。2. 针对训练过程中异常值导致的编码偏差，提出异常感知子比特权重量化（OS-Quant）方法，动态调整权重编码。3. 为弥补高压缩带来的性能损失，设计基于膜电位的特征蒸馏（MPFD）辅助高精度教师模型引导学生模型训练。

Result: 大量视觉与非视觉任务实验证明，S²NN压缩程度优于现有SNN量化方法，同时在准确率和推理效率上也取得更好表现。

Conclusion: S²NN通过亚比特量化和特征蒸馏显著推动了脉冲神经网络的极致高效和实用性，尤其适合边缘计算等算力资源有限的应用场景。

Abstract: Spiking Neural Networks (SNNs) offer an energy-efficient paradigm for machine
intelligence, but their continued scaling poses challenges for resource-limited
deployment. Despite recent advances in binary SNNs, the storage and
computational demands remain substantial for large-scale networks. To further
explore the compression and acceleration potential of SNNs, we propose Sub-bit
Spiking Neural Networks (S$^2$NNs) that represent weights with less than one
bit. Specifically, we first establish an S$^2$NN baseline by leveraging the
clustering patterns of kernels in well-trained binary SNNs. This baseline is
highly efficient but suffers from \textit{outlier-induced codeword selection
bias} during training. To mitigate this issue, we propose an
\textit{outlier-aware sub-bit weight quantization} (OS-Quant) method, which
optimizes codeword selection by identifying and adaptively scaling outliers.
Furthermore, we propose a \textit{membrane potential-based feature
distillation} (MPFD) method, improving the performance of highly compressed
S$^2$NN via more precise guidance from a teacher model. Extensive results on
vision and non-vision tasks reveal that S$^2$NN outperforms existing quantized
SNNs in both performance and efficiency, making it promising for edge computing
applications.

</details>


### [238] [Cycle Diffusion Model for Counterfactual Image Generation](https://arxiv.org/abs/2509.24267)
*Fangrui Huang,Alan Wang,Binxu Li,Bailey Trang,Ridvan Yesiloglu,Tianyu Hua,Wei Peng,Ehsan Adeli*

Main category: cs.CV

TL;DR: 论文提出了一种循环扩散模型（CDM），通过循环约束优化扩散模型在医学影像合成中的条件遵循性及图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散生成模型在医学影像合成中虽然效果突出，但难以兼顾对条件的严格遵循与合成图像的高质量，尤其是在直接或反事实图像生成任务中依然存在挑战。

Method: 作者提出了循环训练框架（CDM），通过引入循环约束，在生成图像与原始图像之间保持一致性，细致地微调扩散模型，以提升生成结果对输入条件的忠实度和图像质量。

Result: 在多个3D脑MRI数据集（包括ABCD、HCP、ADNI、PPMI）的实验表明，CDM方法在FID和SSIM等指标上有效提升了条件遵循性和生成图像的质量。

Conclusion: CDM的循环策略是优化基于扩散模型的医学影像生成的有效手段，对数据增强、反事实分析和疾病进展建模等任务有广泛应用价值。

Abstract: Deep generative models have demonstrated remarkable success in medical image
synthesis. However, ensuring conditioning faithfulness and high-quality
synthetic images for direct or counterfactual generation remains a challenge.
In this work, we introduce a cycle training framework to fine-tune diffusion
models for improved conditioning adherence and enhanced synthetic image
realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency
between generated and original images by incorporating cycle constraints,
enabling more reliable direct and counterfactual generation. Experiments on a
combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and
PPMI) show that our method improves conditioning accuracy and enhances image
quality as measured by FID and SSIM. The results suggest that the cycle
strategy used in CDM can be an effective method for refining diffusion-based
medical image generation, with applications in data augmentation,
counterfactual, and disease progression modeling.

</details>


### [239] [Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds](https://arxiv.org/abs/2509.24273)
*Yongqiang Wang,Weigang Li,Wenping Liu,Zhiqiang Tian,Jinling Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架结构的鲁棒点云配准框架SRRF，相较于现有方法，对点云数据中的畸变、噪声及预处理错误具有更强的鲁棒性和更高的配准精度。


<details>
  <summary>Details</summary>
Motivation: 现实中的点云数据由于受传感器限制、环境噪声及预处理误差等因素影响，常常出现密度失衡、几何畸变、噪声污染等问题，现有方法对此鲁棒性不足、配准精度降低，因此需要更加坚固的配准方法。

Method: 该方法提出骨架结构驱动的配准思路，利用骨架提取得到腐蚀自适应的点云骨架，结合原始点云与骨架的双重配准结果并设计分布距离损失，保障源点云与目标点云骨架结构的一致性。

Result: 在多种含有不同类型腐蚀的公开数据集上，SRRF方法比当前主流的点云配准算法表现更优，在密度失衡、噪声、几何畸变等多种场景下均展现出更高的鲁棒性和配准精度。

Conclusion: SRRF方法充分融合了局部几何与全局骨架稳定性，能有效配准遭受腐蚀的点云数据，为实际3D感知任务如自动驾驶、机器人与医疗成像等领域提供了一种更加鲁棒的解决方案。

Abstract: Point cloud registration is fundamental in 3D vision applications, including
autonomous driving, robotics, and medical imaging, where precise alignment of
multiple point clouds is essential for accurate environment reconstruction.
However, real-world point clouds are often affected by sensor limitations,
environmental noise, and preprocessing errors, making registration challenging
due to density distortions, noise contamination, and geometric deformations.
Existing registration methods rely on direct point matching or surface feature
extraction, which are highly susceptible to these corruptions and lead to
reduced alignment accuracy. To address these challenges, a skeleton-based
robust registration framework is presented, which introduces a
corruption-resilient skeletal representation to improve registration robustness
and accuracy. The framework integrates skeletal structures into the
registration process and combines the transformations obtained from both the
corrupted point cloud alignment and its skeleton alignment to achieve optimal
registration. In addition, a distribution distance loss function is designed to
enforce the consistency between the source and target skeletons, which
significantly improves the registration performance. This framework ensures
that the alignment considers both the original local geometric features and the
global stability of the skeleton structure, resulting in robust and accurate
registration results. Experimental evaluations on diverse corrupted datasets
demonstrate that SRRF consistently outperforms state-of-the-art registration
methods across various corruption scenarios, including density distortions,
noise contamination, and geometric deformations. The results confirm the
robustness of SRRF in handling corrupted point clouds, making it a potential
approach for 3D perception tasks in real-world scenarios.

</details>


### [240] [Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context](https://arxiv.org/abs/2509.24275)
*Yongqiang Wang,Weigang Li,Wenping Liu,Zhe Xu,Zhiqiang Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为CEGC的新方法，有效提升了部分点云配准任务在复杂环境下的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 部分点云配准在自动感知和三维场景理解中非常重要，但受到结构模糊、部分可见性和噪声的影响，现有方法效果有限。作者旨在解决这些核心挑战，提升部分点云配准的稳健性和精度。

Method: 提出了CEGC统一框架，通过融合全局上下文自信度估计与对匹配点可靠性的评估，提升了配准能力。其关键模块包括：重叠自信度估计模块（结合语义描述符和几何相似度，识别重叠区域并抑制异常值），以及基于全局注意力的上下文感知匹配策略（对匹配点赋予软性自信分数），并利用可微分加权SVD求解器进行精确变换估计。

Result: 在ModelNet40、ScanObjectNN和7Scenes等多个数据集上的实验，CEGC在配准精度、鲁棒性和泛化能力方面均优于当前最新方法。

Conclusion: CEGC为复杂和具有挑战性的部分点云配准任务提供了解释性强、可扩展的新解决方案，表现出优越的性能。

Abstract: Partial point cloud registration is essential for autonomous perception and
3D scene understanding, yet it remains challenging owing to structural
ambiguity, partial visibility, and noise. We address these issues by proposing
Confidence Estimation under Global Context (CEGC), a unified, confidence-driven
framework for robust partial 3D registration. CEGC enables accurate alignment
in complex scenes by jointly modeling overlap confidence and correspondence
reliability within a shared global context. Specifically, the hybrid overlap
confidence estimation module integrates semantic descriptors and geometric
similarity to detect overlapping regions and suppress outliers early. The
context-aware matching strategy smitigates ambiguity by employing global
attention to assign soft confidence scores to correspondences, improving
robustness. These scores guide a differentiable weighted singular value
decomposition solver to compute precise transformations. This tightly coupled
pipeline adaptively down-weights uncertain regions and emphasizes contextually
reliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D
vision datasets demonstrate that CEGC outperforms state-of-the-art methods in
accuracy, robustness, and generalization. Overall, CEGC offers an interpretable
and scalable solution to partial point cloud registration under challenging
conditions.

</details>


### [241] [ASIA: Adaptive 3D Segmentation using Few Image Annotations](https://arxiv.org/abs/2509.24288)
*Sai Raj Kishore Perla,Aditya Vora,Sauradip Nag,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为ASIA的新方法，用少量随拍图片标注，结合扩散模型，实现对3D对象部分的自适应分割，解决了现有方法对数据和文本描述依赖过强问题，并在实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D分割方法依赖于多视角图片、复杂3D标注或易产生歧义的文本描述，实际操作繁琐且精确度有限。论文希望以更便捷、更精确的方法实现3D对象中语义性或非语义性部分的自适应分割。

Method: 方法利用极少量用户随拍图片标注，通过Stable Diffusion等文本到图像扩散模型的先验将分割信息从2D转移到3D。训练时为每个分割部分优化独立文本token，并引入跨视角部分对应损失。推理阶段，方法通过多视角渲染并分割3D网格，采用投票在UV空间融合标签，利用Noise Optimization技术进一步优化，最后将标签映射回3D模型。

Result: ASIA在3D分割任务上，无论是定量还是定性评测，均优于现有主流方法。尤其在语义性与非语义性分割上，都表现出更好的泛化性和准确性。

Conclusion: ASIA提出了一种高效、实用且易泛化的3D分割框架，极大减少了数据采集和标注成本，同时解决了传统方法对文本描述或多视角数据的强依赖，为实际3D分割应用提供了更优解决方案。

Abstract: We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a
novel framework that enables segmentation of possibly non-semantic and
non-text-describable "parts" in 3D. Our segmentation is controllable through a
few user-annotated in-the-wild images, which are easier to collect than
multi-view images, less demanding to annotate than 3D models, and more precise
than potentially ambiguous text descriptions. Our method leverages the rich
priors of text-to-image diffusion models, such as Stable Diffusion (SD), to
transfer segmentations from image space to 3D, even when the annotated and
target objects differ significantly in geometry or structure. During training,
we optimize a text token for each segment and fine-tune our model with a novel
cross-view part correspondence loss. At inference, we segment multi-view
renderings of the 3D mesh, fuse the labels in UV-space via voting, refine them
with our novel Noise Optimization technique, and finally map the UV-labels back
onto the mesh. ASIA provides a practical and generalizable solution for both
semantic and non-semantic 3D segmentation tasks, outperforming existing methods
by a noticeable margin in both quantitative and qualitative evaluations.

</details>


### [242] [SVGThinker: Instruction-Aligned and Reasoning-Driven Text-to-SVG Generation](https://arxiv.org/abs/2509.24299)
*Hanqi Chen,Zhongyin Zhao,Ye Chen,Zhujin Liang,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出了一种新方法SVGThinker，用于提升文本到SVG生成的泛化能力和对指令的遵循性，生成的SVG质量高、易于编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SVG生成方法在泛化性和严格遵循输入指令方面存在明显不足，无法充分利用SVG结构化、可编辑的优势。

Method: SVGThinker通过推理驱动的流程实现SVG生成。具体流程是：依次渲染每个SVG基本图元，利用多模态模型对图片和代码都进行标注，然后构建逐步递增的更新数据。在该数据集上用监督微调训练大语言模型，让其将推理过程公开成为中间步骤（chain-of-thought），增强鲁棒性、减少错误和幻觉。

Result: 实验表明，SVGThinker比现有最优基线方法生成的SVG更加稳定、可编辑、质量高，同时保留了矢量图的结构优势。

Conclusion: SVGThinker不仅提升了SVG生成的质量和编辑性，还为智能化设计、内容创作和自动化图形生成开辟了新方向，优于传统基于图像的方法。

Abstract: Scalable Vector Graphics (SVG) is a code-based representation for 2D visuals.
Leveraging recent advances in large language models (LLMs), we study
text-to-SVG generation and address two persistent gaps: weak generalization and
poor adherence to input instructions. We present SVGThinker, a reasoning-driven
framework that aligns the production of SVG code with the visualization process
and supports the full set of SVG primitives. Our pipeline first renders each
primitive in sequence and uses a multimodal model to annotate the image and
code; we then build stepwise updates that mirror the incremental addition of
primitives. On this data, we train an LLM with supervised fine-tuning that
exposes its chain-of-thought as intermediate reasoning, improving robustness
and reducing errors and hallucinations. Experiments against state-of-the-art
baselines show that SVGThinker produces more stable, editable, and
higher-quality SVGs while preserving the structural advantages of vector
graphics. Unlike image-based methods, our outputs enable precise and
hierarchical editing, opening new directions for design, content creation, and
automated graphics generation.

</details>


### [243] [FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting](https://arxiv.org/abs/2509.24304)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为FrameThinker的新型大规模视觉-语言模型（LVLMs）推理框架，通过迭代视频内容和动作选择，实现了高效且有针对性的长视频推理能力，在多个长视频理解基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在长视频推理任务中表现有限，主要受限于均匀帧采样和静态文本推理方法，导致效率低下，难以处理高视觉复杂度的视频任务，因此需要一种更智能高效的长视频推理方法。

Method: 提出了FrameThinker框架，使LVLMs可以迭代地探询视频内容并作出决策。训练方法分为两阶段：首先用有监督微调（SFT）赋予模型新动作能力（如选帧），然后用强化学习（RL）优化模型决策策略，并深入研究了奖励函数设计以有效引导模型行为。

Result: 在Video-Holmes、LongVideo-Reason等推理基准，以及LongVideoBench、MLVU、VideoMME、LVBench等长视频理解基准上，FrameThinker的平均表现超过基线+10.4%；其中7B模型在LongVideo-Reason上用仅20.6帧达到了76.1%的新SOTA准确率，远超主流方法且实现了20倍以上的帧数效率提升。

Conclusion: FrameThinker显著提升了长视频推理效率和准确率，为LVLMs高效处理复杂长视频任务提供了新的解决方案，展现了在长视频理解和推理领域的巨大潜力。

Abstract: While Large Vision-Language Models (LVLMs) have achieved substantial progress
in video understanding, their application to long video reasoning is hindered
by uniform frame sampling and static textual reasoning, which are inefficient
and struggle to handle visually intensive video tasks. To overcome these
challenges, in this paper, we introduce the concept of thinking with long
videos and propose a novel framework FrameThinker. Within this framework, LVLMs
are able to iteratively interrogate video content. Developing such video
reasoning capabilities in LVLMs presents notable challenges, particularly in
adapting the model to new video actions (e.g. select frame), and designing
reward functions to guide LVLMs to adopt the newly introduced action. To solve
these challenges, we propose a two-phase training strategy, first employing
Supervised Fine-Tuning (SFT) to instill fundamental action capabilities,
followed by Reinforcement Learning (RL) to optimize a strategic decision-making
policy.Notably, in this RL phase, we conduct an in-depth and comprehensive
exploration of the reward design for each action and format reward. Extensive
experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and
long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and
LVBench, demonstrate that FrameThinker achieves a significant average
improvement of +10.4% over baselines while drastically reducing the number of
processed frames. Most notably, our 7B model, FrameThinker establishes a new
state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average
of only 20.6 frames. This not only outperforms the competitive LongVILA-R1
(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating
unparalleled efficiency and effectiveness.

</details>


### [244] [OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction](https://arxiv.org/abs/2509.24308)
*Yuhang Cao,Haojun Yan,Danya Yao*

Main category: cs.CV

TL;DR: 本文提出了OMeGa框架，通过联合优化三角网格和二维高斯点，实现了更准确高效的室内三维重建，并显著提升了几何质量和新视角渲染表现。


<details>
  <summary>Details</summary>
Motivation: 当前高斯渲染和表面重建方法在室内无纹理区域几何不准，以及网格提取与优化割裂，不能让网格结构反向指导重建，影响精度和效果。

Method: 提出OMeGa端到端框架，联动优化显式三角网格和2D高斯点，通过灵活绑定融合空间和纹理属性。引入网格约束和单目法线监督正则化几何学习，并设计启发式迭代网格细化（分裂高误差面，剪除不可靠面），提升网格质量和细节。

Result: OMeGa在室内重建基准上表现出色，Chamfer-L1误差相对2DGS基线降低47.3%，同时保持新视角渲染质量。

Conclusion: OMeGa能有效克服以往方法在无纹理室内区域几何重建难题，达到业界领先的重建与渲染性能。

Abstract: Neural rendering with Gaussian splatting has advanced novel view synthesis,
and most methods reconstruct surfaces via post-hoc mesh extraction. However,
existing methods suffer from two limitations: (i) inaccurate geometry in
texture-less indoor regions, and (ii) the decoupling of mesh extraction from
optimization, thereby missing the opportunity to leverage mesh geometry to
guide splat optimization. In this paper, we present OMeGa, an end-to-end
framework that jointly optimizes an explicit triangle mesh and 2D Gaussian
splats via a flexible binding strategy, where spatial attributes of Gaussian
Splats are expressed in the mesh frame and texture attributes are retained on
splats. To further improve reconstruction accuracy, we integrate mesh
constraints and monocular normal supervision into the optimization, thereby
regularizing geometry learning. In addition, we propose a heuristic, iterative
mesh-refinement strategy that splits high-error faces and prunes unreliable
ones to further improve the detail and accuracy of the reconstructed mesh.
OMeGa achieves state-of-the-art performance on challenging indoor
reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\% over the 2DGS
baseline while maintaining competitive novel-view rendering quality. The
experimental results demonstrate that OMeGa effectively addresses prior
limitations in indoor texture-less reconstruction.

</details>


### [245] [Towards Foundation Models for Cryo-ET Subtomogram Analysis](https://arxiv.org/abs/2509.24311)
*Runmin Jiang,Wanyue Feng,Yuntian Yang,Shriya Pingulkar,Hong Wang,Xi Xiao,Xiaoyu Cao,Genpei Zhang,Xiao Wang,Xiaolong Wu,Tianyang Wang,Yang Liu,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了面向冷冻电镜断层（cryo-ET）亚断层图像分析的通用基础大模型框架，包括数据生成、模型结构和噪声鲁棒性训练方法，全面提升了任务性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 亚断层分析（如分类、对齐、平均）对分子结构解析至关重要，但现有方法受限于数据标注稀缺、噪声严重及泛化能力差。作者希望通过引入基础模型理念，突破这些限制，实现更大规模和鲁棒的自动化分析。

Method: 1）提出CryoEngine合成数据生成器，利用452类粒子生成90万余个亚断层供预训练；2）设计自适应相位分块增强视觉Transformer（APT-ViT），提升几何与语义变化的等变鲁棒性；3）采用噪声自适应对比学习（NRCL）策略，强化高噪声下表示学习的稳定性。

Result: 在24个合成及真实数据集上的三项亚断层核心任务均达到SOTA性能，并展示出对未见数据集的强泛化能力。

Conclusion: 该工作首次将基础模型理念应用于cryo-ET亚断层分析，通过数据生成、模型创新和训练策略三管齐下，极大提升了分析性能和泛化，为断层图像自动化、规模化解析奠定基础。

Abstract: Cryo-electron tomography (cryo-ET) enables in situ visualization of
macromolecular structures, where subtomogram analysis tasks such as
classification, alignment, and averaging are critical for structural
determination. However, effective analysis is hindered by scarce annotations,
severe noise, and poor generalization. To address these challenges, we take the
first step towards foundation models for cryo-ET subtomograms. First, we
introduce CryoEngine, a large-scale synthetic data generator that produces over
904k subtomograms from 452 particle classes for pretraining. Second, we design
an Adaptive Phase Tokenization-enhanced Vision Transformer (APT-ViT), which
incorporates adaptive phase tokenization as an equivariance-enhancing module
that improves robustness to both geometric and semantic variations. Third, we
introduce a Noise-Resilient Contrastive Learning (NRCL) strategy to stabilize
representation learning under severe noise conditions. Evaluations across 24
synthetic and real datasets demonstrate state-of-the-art (SOTA) performance on
all three major subtomogram tasks and strong generalization to unseen datasets,
advancing scalable and robust subtomogram analysis in cryo-ET.

</details>


### [246] [Similarity-Aware Selective State-Space Modeling for Semantic Correspondence](https://arxiv.org/abs/2509.24318)
*Seungwook Kim,Minsu Cho*

Main category: cs.CV

TL;DR: MambaMatcher是一种用于图像间语义对应的新方法，能够高效且准确地建模高维相关性，实现了性能突破。


<details>
  <summary>Details</summary>
Motivation: 以往方法—无论是以特征为主还是以相关性为主—要么忽略复杂关系，要么计算开销巨大。需要一种既高效又能充分捕捉高维相关性的技术。

Method: 提出MambaMatcher，结合选择性状态空间模型（SSMs）与类似Mamba的线性复杂度算法，利用选择性扫描机制高效优化4D相关性图，无需降低特征分辨率。

Result: 在标准图像语义对应基准上，MambaMatcher表现优异，取得了当前最好的性能（SOTA）。

Conclusion: MambaMatcher能够弥补传统特征法与高相关性法的不足，在效率和效果之间实现了平衡，为图像语义对应任务开辟了新途径。

Abstract: Establishing semantic correspondences between images is a fundamental yet
challenging task in computer vision. Traditional feature-metric methods enhance
visual features but may miss complex inter-correlation relationships, while
recent correlation-metric approaches are hindered by high computational costs
due to processing 4D correlation maps. We introduce MambaMatcher, a novel
method that overcomes these limitations by efficiently modeling
high-dimensional correlations using selective state-space models (SSMs). By
implementing a similarity-aware selective scan mechanism adapted from Mamba's
linear-complexity algorithm, MambaMatcher refines the 4D correlation map
effectively without compromising feature map resolution or receptive field.
Experiments on standard semantic correspondence benchmarks demonstrate that
MambaMatcher achieves state-of-the-art performance.

</details>


### [247] [TP-MVCC: Tri-plane Multi-view Fusion Model for Silkie Chicken Counting](https://arxiv.org/abs/2509.24329)
*Sirui Chen,Yuhong Feng,Yifeng Wang,Jianghai Liao,Qi Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于三平面的多视角鸡只计数模型（TP-MVCC），极大提升了密集养鸡场景中的计数精度。


<details>
  <summary>Details</summary>
Motivation: 在智能农业中，准确计数鸡只对于生产管理至关重要，但在密集和遮挡严重的情况下，传统方法效果不佳。

Method: 该文提出TP-MVCC模型，通过多摄像头拍摄图像，采用几何投影和三平面特征融合，将不同视角信息融合到同一地面平面上，并解码为场景级密度图，从而实现高精度计数。此外，还构建了首个真实养殖环境下的多视角鸡只数据集。

Result: 实验结果显示，TP-MVCC在密集遮挡场景下，可显著优于单视角和传统融合方法，取得了95.1%的计数准确率，且鲁棒性强。

Conclusion: 该方法在智能农业中的实际应用潜力强，为提升生产效率和动物管理水平提供了有力工具。

Abstract: Accurate animal counting is essential for smart farming but remains difficult
in crowded scenes due to occlusions and limited camera views. To address this,
we propose a tri-plane-based multi-view chicken counting model (TP-MVCC), which
leverages geometric projection and tri-plane fusion to integrate features from
multiple cameras onto a unified ground plane. The framework extracts
single-view features, aligns them via spatial transformation, and decodes a
scene-level density map for precise chicken counting. In addition, we construct
the first multi-view dataset of silkie chickens under real farming conditions.
Experiments show that TP-MVCC significantly outperforms single-view and
conventional fusion comparisons, achieving 95.1\% accuracy and strong
robustness in dense, occluded scenarios, demonstrating its practical potential
for intelligent agriculture.

</details>


### [248] [Hyperspherical Latents Improve Continuous-Token Autoregressive Generation](https://arxiv.org/abs/2509.24335)
*Guolin Ke,Hui Xue*

Main category: cs.CV

TL;DR: 提出了一种新型自回归（AR）图像生成方法SphereAR，通过将所有输入输出限制在定半径超球面上，提升了AR模型在ImageNet上的生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有的连续token自回归模型在图像生成上性能落后于潜变量扩散模型和掩码生成模型，主要瓶颈在于变分自编码器（VAE）潜变量的异构方差问题在解码时被放大，导致方差塌缩。

Method: 引入SphereAR，将AR所有输入输出（包括无分类器引导后的结果）限定在固定半径的超球面（即l2范数恒定）上，利用超球VAE理论降低尺度分量对解码稳定性的影响。

Result: SphereAR-H（943M参数）在ImageNet上取得了1.34的FID分数，创下AR模型新纪录。更小规模的SphereAR-L（479M参数）和SphereAR-B（208M参数）也达到（或超过）同等规模下的主流自回归基线性能。

Conclusion: SphereAR首次使纯粹的自回归图像生成模型在相似参数规模下超越了扩散和掩码生成模型，验证了超球面约束在提升模型稳定性和生成质量方面的有效性。

Abstract: Autoregressive (AR) models are promising for image generation, yet
continuous-token AR variants often trail latent diffusion and masked-generation
models. The core issue is heterogeneous variance in VAE latents, which is
amplified during AR decoding, especially under classifier-free guidance (CFG),
and can cause variance collapse. We propose SphereAR to address this issue. Its
core design is to constrain all AR inputs and outputs -- including after CFG --
to lie on a fixed-radius hypersphere (constant $\ell_2$ norm), leveraging
hyperspherical VAEs. Our theoretical analysis shows that hyperspherical
constraint removes the scale component (the primary cause of variance
collapse), thereby stabilizing AR decoding. Empirically, on ImageNet
generation, SphereAR-H (943M) sets a new state of the art for AR models,
achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54
and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger
baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,
this is the first time a pure next-token AR image generator with raster order
surpasses diffusion and masked-generation models at comparable parameter
scales.

</details>


### [249] [Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA](https://arxiv.org/abs/2509.24350)
*Yan Ke,Xin Yu,Heming Du,Scott Chapman,Helen Huang*

Main category: cs.CV

TL;DR: 该论文提出了一个多代理协作的视觉问答系统，用于应对真实农业场景的多图像、多视角问题，并在公开基准上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的农业视觉问答方法多数只能处理单一文本或图像，难以适应实际应用中多图像、不同阶段和视角的数据需求。同时，这些系统缺乏对外部最新农业知识的整合能力，且质量控制机制不完善。

Method: 提出了一个包含四个代理（信息检索者、反思者、答题者、改进者）的多智能体框架。检索者负责获取外部信息，反思者评估信息充足性并触发查询优化，两位答题者并行生成答案减小偏差，改进者则多轮迭代比对和整合多图像信息、提升答案质量。

Result: 在AgMMU多图像农业视觉问答基准数据集上，该方法取得了有竞争力的性能。

Conclusion: 结合多代理、多轮自反思和改进机制的框架有效提升了多图像农业视觉问答的表现，对实际农业场景具有较强适应性与应用前景。

Abstract: Agricultural visual question answering is essential for providing farmers and
researchers with accurate and timely knowledge. However, many existing
approaches are predominantly developed for evidence-constrained settings such
as text-only queries or single-image cases. This design prevents them from
coping with real-world agricultural scenarios that often require multi-image
inputs with complementary views across spatial scales, and growth stages.
Moreover, limited access to up-to-date external agricultural context makes
these systems struggle to adapt when evidence is incomplete. In addition, rigid
pipelines often lack systematic quality control. To address this gap, we
propose a self-reflective and self-improving multi-agent framework that
integrates four roles, the Retriever, the Reflector, the Answerer, and the
Improver. They collaborate to enable context enrichment, reflective reasoning,
answer drafting, and iterative improvement.
  A Retriever formulates queries and gathers external information, while a
Reflector assesses adequacy and triggers sequential reformulation and renewed
retrieval. Two Answerers draft candidate responses in parallel to reduce bias.
The Improver refines them through iterative checks while ensuring that
information from multiple images is effectively aligned and utilized.
Experiments on the AgMMU benchmark show that our framework achieves competitive
performance on multi-image agricultural QA.

</details>


### [250] [NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis](https://arxiv.org/abs/2509.24353)
*Yixuan Ren,Hanyu Wang,Hao Chen,Bo He,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出了一种名为NeRV-Diffusion的隐式潜变量视频扩散模型，通过生成神经网络权重的方式合成高质量视频，效果优于现有隐式模型，并接近最新主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型很难高效压缩并整体生成视频，经常需要对每一帧分别编码或计算跨帧注意力，导致计算成本高且时序信息难以整体捕捉，隐式表示的生成质量有限。

Method: 1）使用hypernetwork构建的tokenizer，将原始视频像素编码为神经网络权重（INR参数），实现视频的整体压缩；2）使用隐式扩散transformer在INR权重空间去噪生成，实现高效视频生成。创新点包括权重分配、上采样和输入坐标的改进，SNR自适应损失加权和定时采样策略优化训练。

Result: 在UCF-101和Kinetics-600数据集上，模型生成的视频质量优于所有现有INR模型，并与当前最新非隐式模型性能接近。同时模型权重空间平滑，支持视频与帧间顺滑插值。

Conclusion: NeRV-Diffusion首次将视频以整个神经网络权重的形式统一建模，实现高效、高质量的视频生成，推进了隐式神经表示法在视频领域的进展。

Abstract: We present NeRV-Diffusion, an implicit latent video diffusion model that
synthesizes videos via generating neural network weights. The generated weights
can be rearranged as the parameters of a convolutional neural network, which
forms an implicit neural representation (INR), and decodes into videos with
frame indices as the input. Our framework consists of two stages: 1) A
hypernetworkbased tokenizer that encodes raw videos from pixel space to neural
parameter space, where the bottleneck latent serves as INR weights to decode.
2) An implicit diffusion transformer that denoises on the latent INR weights.
In contrast to traditional video tokenizers that encode videos into frame-wise
feature maps, NeRV-Diffusion compresses and generates a video holistically as a
unified neural network. This enables efficient and high-quality video synthesis
via obviating temporal cross-frame attentions in the denoiser and decoding
video latent with dedicated decoders. To achieve Gaussian-distributed INR
weights with high expressiveness, we reuse the bottleneck latent across all
NeRV layers, as well as reform its weight assignment, upsampling connection and
input coordinates. We also introduce SNR-adaptive loss weighting and scheduled
sampling for effective training of the implicit diffusion model. NeRV-Diffusion
reaches superior video generation quality over previous INR-based models and
comparable performance to most recent state-of-the-art non-implicit models on
real-world video benchmarks including UCF-101 and Kinetics-600. It also brings
a smooth INR weight space that facilitates seamless interpolations between
frames or videos.

</details>


### [251] [An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation](https://arxiv.org/abs/2509.24358)
*Dayu Tan,Cheng Kong,Yansen Su,Hai Chen,Dongliang Yang,Junfeng Xia,Chunhou Zheng*

Main category: cs.CV

TL;DR: 本文提出一种新的多器官医学图像分割网络LamFormer，有效解决了Transformer计算量大和局部细节信息提取不足的问题，并在多个复杂数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有采用Transformer的分割方法虽然善于捕获长距离依赖，但计算资源消耗高且对局部细节建模不足。急需一种兼顾全局建模和细节捕获，且计算效率更高的新型架构。

Method: 作者设计了LamFormer，一种U型网络结构，主要创新包括：(1) 在编码器中引入Linear Attention Mamba (LAM)以多尺度捕获长距依赖；(2) 构建了平行层次特征聚合模块(PHFA)以融合不同编码层特征并过滤信息；(3) 设计了Reduced Transformer(RT)，用新计算方式全局建模上采样特征以增强细节信息提取和远程依赖能力。

Result: LamFormer在七个具有挑战性和多样性的数据集上均优于现有主流分割方法，展示了卓越的分割性能。

Conclusion: LamFormer不仅显著提升了多器官分割精度，还在保持高性能的同时有效控制了模型复杂度，实现了性能与效率的平衡。

Abstract: In the field of multi-organ medical image segmentation, recent methods
frequently employ Transformers to capture long-range dependencies from image
features. However, these methods overlook the high computational cost of
Transformers and their deficiencies in extracting local detailed information.
To address high computational costs and inadequate local detail information, we
reassess the design of feature extraction modules and propose a new
deep-learning network called LamFormer for fine-grained segmentation tasks
across multiple organs. LamFormer is a novel U-shaped network that employs
Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture
multi-scale long-range dependencies. We construct the Parallel Hierarchical
Feature Aggregation (PHFA) module to aggregate features from different layers
of the encoder, narrowing the semantic gap among features while filtering
information. Finally, we design the Reduced Transformer (RT), which utilizes a
distinct computational approach to globally model up-sampled features. RRT
enhances the extraction of detailed local information and improves the
network's capability to capture long-range dependencies. LamFormer outperforms
existing segmentation methods on seven complex and diverse datasets,
demonstrating exceptional performance. Moreover, the proposed network achieves
a balance between model performance and model complexity.

</details>


### [252] [DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense](https://arxiv.org/abs/2509.24359)
*Amira Guesmi,Muhammad Shafique*

Main category: cs.CV

TL;DR: 该论文提出了一种新的对抗攻击防御方法DRIFT，通过引入随机滤波器并主动打散梯度共识，有效提升了深度神经网络在多类攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易受到对抗样本攻击，现有的防御方法在攻击者能估计梯度的情况下往往失效。作者发现攻击者能够利用随机变换中梯度的一致性（共识性）提升攻击迁移性，因此有必要研究并打破这种梯度共识以增强健壮性。

Method: 作者提出DRIFT防御方法，构建一组轻量、可学习的随机滤波器，对输入进行变换。通过最大化Jacobians和logits空间响应的差异，破坏梯度共识，同时保持模型对自然样本的预测一致。训练过程中结合了预测一致性、Jacobian分离、logit空间分离和对抗鲁棒性损失。

Result: 在ImageNet上，DRIFT在卷积神经网络和视觉Transformer上均显著提升了对抗鲁棒性，在自适应白盒、迁移与无梯度攻击下均优于最新的预处理、对抗训练和扩散模型等防御方法。且DRIFT几乎不增加额外运行和内存开销。

Conclusion: DRIFT证明打破梯度共识是提升对抗防御的有效手段，具有实用性与通用性，拓展了对抗鲁棒防御的新思路。

Abstract: Deep neural networks remain highly vulnerable to adversarial examples, and
most defenses collapse once gradients can be reliably estimated. We identify
\emph{gradient consensus}--the tendency of randomized transformations to yield
aligned gradients--as a key driver of adversarial transferability. Attackers
exploit this consensus to construct perturbations that remain effective across
transformations. We introduce \textbf{DRIFT} (Divergent Response in Filtered
Transformations), a stochastic ensemble of lightweight, learnable filters
trained to actively disrupt gradient consensus. Unlike prior randomized
defenses that rely on gradient masking, DRIFT enforces \emph{gradient
dissonance} by maximizing divergence in Jacobian- and logit-space responses
while preserving natural predictions. Our contributions are threefold: (i) we
formalize gradient consensus and provide a theoretical analysis linking
consensus to transferability; (ii) we propose a consensus-divergence training
strategy combining prediction consistency, Jacobian separation, logit-space
separation, and adversarial robustness; and (iii) we show that DRIFT achieves
substantial robustness gains on ImageNet across CNNs and Vision Transformers,
outperforming state-of-the-art preprocessing, adversarial training, and
diffusion-based defenses under adaptive white-box, transfer-based, and
gradient-free attacks. DRIFT delivers these improvements with negligible
runtime and memory cost, establishing gradient divergence as a practical and
generalizable principle for adversarial defense.

</details>


### [253] [UI-UG: A Unified MLLM for UI Understanding and Generation](https://arxiv.org/abs/2509.24361)
*Hao Yang,Weijie Qiu,Ru Zhang,Zhou Fang,Ruichao Mao,Xiaoyu Lin,Maji Huang,Zhaosong Huang,Teng Guo,Shuoyang Liu,Hai Rao*

Main category: cs.CV

TL;DR: 该论文提出了一种新型多模态大模型UI-UG，能够同时实现UI界面理解与生成，并在多个任务上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在特定领域（如用户界面）上的理解和生成仍存在不足，需要专门设计提升其在该领域的能力。

Method: UI-UG模型整合了UI理解与生成能力。理解任务上结合了有监督微调和GRPO策略，提升模型对复杂UI的细粒度理解。生成任务上使用DPO优化，使生成结果更贴合人类偏好。同时提出DSL设计、训练策略、渲染流程和评估方法，形成工业级整体流程。

Result: 在UI理解任务上，UI-UG超过了更大规模的通用MLLMs和同等规模的UI专项模型，达成SOTA表现。UI生成性能与更大模型持平，但资源消耗更低。将理解与生成任务集成后，二者的准确度和质量均有所提升。

Conclusion: UI-UG模型在UI理解和生成领域体现了高效、优质和资源节约的优势，为工业界提供了可行的新范式。

Abstract: Although Multimodal Large Language Models (MLLMs) have been widely applied
across domains, they are still facing challenges in domain-specific tasks, such
as User Interface (UI) understanding accuracy and UI generation quality. In
this paper, we introduce UI-UG (a unified MLLM for UI Understanding and
Generation), integrating both capabilities. For understanding tasks, we employ
Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization
(GRPO) to enhance fine-grained understanding on the modern complex UI data. For
generation tasks, we further use Direct Preference Optimization (DPO) to make
our model generate human-preferred UIs. In addition, we propose an industrially
effective workflow, including the design of an LLM-friendly domain-specific
language (DSL), training strategies, rendering processes, and evaluation
metrics. In experiments, our model achieves state-of-the-art (SOTA) performance
on understanding tasks, outperforming both larger general-purpose MLLMs and
similarly-sized UI-specialized models. Our model is also on par with these
larger MLLMs in UI generation performance at a fraction of the computational
cost. We also demonstrate that integrating understanding and generation tasks
can improve accuracy and quality for both tasks.

</details>


### [254] [Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models](https://arxiv.org/abs/2509.24365)
*Jitai Hao,Hao Liu,Xinyan Xiao,Qiang Huang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态模型结构Uni-X，通过在模型两端采用模态专用处理，在中间层共享参数，解决了传统共享自回归Transformer在文本与图像输入时的严重梯度冲突问题，显著提升了多模态融合效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于共享自回归Transformer的统一多模态模型，虽然结构简单，但在训练时不同模态（如图像与文本）在浅层和深层会产生严重的梯度冲突，影响模型效果。追根溯源，这一问题主要源自不同模态在底层统计特性上的巨大差异。因此，亟需新结构来缓解或消除多模态输入下的梯度冲突。

Method: 本文提出了Uni-X结构：模型首尾分别采用模态专用Transformer层用于处理各自特性的低/高层表示，在中间层实现参数共享，用于抽象层次的跨模态融合。这种“X形”结构旨在兼顾模态适应性和高层语义对齐，有效缓解梯度冲突。

Result: 在多个实验中，Uni-X在相同训练条件下的训练效率优于现有强基线模型。当参数规模达到3B并训练更大数据时，Uni-X取得了与7B参数传统UMM相当或更优的表现，图像生成任务的GenEval得分达到82，在文本和视觉理解任务上也表现出色。

Conclusion: Uni-X在参数效率和模型可扩展性上表现突出，是未来统一多模态建模的优选基础架构。该结构有效解决了梯度冲突问题，为多模态理解和生成任务带来了性能突破。

Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR)
transformers are attractive for their architectural simplicity. However, we
identify a critical limitation: when trained on multimodal inputs,
modality-shared transformers suffer from severe gradient conflicts between
vision and text, particularly in shallow and deep layers. We trace this issue
to the fundamentally different low-level statistical properties of images and
text, while noting that conflicts diminish in middle layers where
representations become more abstract and semantically aligned. To overcome this
challenge, we propose Uni-X, a two-end-separated, middle-shared architecture.
Uni-X dedicates its initial and final layers to modality-specific processing,
while maintaining shared parameters in the middle layers for high-level
semantic fusion. This X-shaped design not only eliminates gradient conflicts at
both ends but also further alleviates residual conflicts in the shared layers.
Extensive experiments validate the effectiveness of Uni-X. Under identical
training conditions, Uni-X achieves superior training efficiency compared to
strong baselines. When scaled to 3B parameters with larger training data, Uni-X
matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for
image generation alongside strong performance in text and vision understanding
tasks. These results establish Uni-X as a parameter-efficient and scalable
foundation for future unified multimodal modeling. Our code is available at
https://github.com/CURRENTF/Uni-X

</details>


### [255] [Real-Aware Residual Model Merging for Deepfake Detection](https://arxiv.org/abs/2509.24367)
*Jinhee Park,Guisik Kim,Choongsang Cho,Junseok Kwon*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度伪造检测方法Real-aware Residual Model Merging（R$^2$M），能够在无需重新训练的前提下将不同深度伪造检测专家模型合并，有效应对不断演化的伪造生成器。实验证明新方法在多种测试场景下优于联合训练和其他合并方法。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成器快速演化，导致伪造检测需要频繁收集新数据和重复训练模型，成本高昂且效率低下。作者希望提出新的模型合并思路，以避免重复训练，并提升对新型伪造检测的适应能力。

Method: 作者分析了深度伪造检测专家模型的特性，提出通过低秩分解得到共享的真实（Real）特征，将每个专家模型分解为真实对齐部分和伪造残差，再对残差进行层级降秩去噪，并通过任务范数匹配进行加权合并，以防止某一伪造类型主导合并结果。此外，该方法可以无训练直接合并，具备模型可组合性。

Result: 实验证明，R$^2$M在同分布、跨数据集和新数据集的伪造检测任务上，均优于传统联合训练和已有的模型合并方法。此外，当出现新的生成器类型时，仅需微调一个专家模型并重新合并即可，极大简化了维护流程。

Conclusion: R$^2$M是一种高效、可扩展且免训练的深度伪造检测模型合并框架，能有效应对快速变化的伪造生成威胁，减少重复训练成本，对未来深度伪造检测系统具有现实应用价值。

Abstract: Deepfake generators evolve quickly, making exhaustive data collection and
repeated retraining impractical. We argue that model merging is a natural fit
for deepfake detection: unlike generic multi-task settings with disjoint
labels, deepfake specialists share the same binary decision and differ in
generator-specific artifacts. Empirically, we show that simple weight averaging
preserves Real representations while attenuating Fake-specific cues. Building
upon these findings, we propose Real-aware Residual Model Merging (R$^2$M), a
training-free parameter-space merging framework. R$^2$M estimates a shared Real
component via a low-rank factorization of task vectors, decomposes each
specialist into a Real-aligned part and a Fake residual, denoises residuals
with layerwise rank truncation, and aggregates them with per-task norm matching
to prevent any single generator from dominating. A concise rationale explains
why a simple head suffices: the Real component induces a common separation
direction in feature space, while truncated residuals contribute only minor
off-axis variations. Across in-distribution, cross-dataset, and unseen-dataset,
R$^2$M outperforms joint training and other merging baselines. Importantly,
R$^2$M is also composable: when a new forgery family appears, we fine-tune one
specialist and re-merge, eliminating the need for retraining.

</details>


### [256] [From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis](https://arxiv.org/abs/2509.24369)
*Khawlah Bajbaa,Abbas Anwar,Muhammad Saqib,Hafeez Anwar,Nabin Sharma,Muhammad Usman*

Main category: cs.CV

TL;DR: 本论文提出了一种结合扩散模型与条件生成对抗网络（GAN）的混合框架，用于从卫星影像合成地理一致性的街景图像，并且在CVUSA基准数据集上实现了优于纯扩散方法和与先进GAN方法媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 从卫星影像生成街景图像有助于城市分析与地理空间数据采集，但由于视角和外观的巨大差异，这一任务面临很大挑战。作者希望突破现有方法在几何一致性和细节保留方面的局限。

Method: 作者设计了一个双分支架构，将Stable Diffusion模型作为核心，结合条件GAN，通过多阶段训练和融合策略，提升合成街景图像的几何一致性和视觉质量。

Result: 在CVUSA数据集上，所提出的混合方法在多个评估指标上优于仅用扩散模型的方法，并且与现有最先进的GAN法有竞争力，生成的图像在局部细节和几何结构上表现更好。

Conclusion: 该混合框架能够将卫星影像高质量地转化为真实且细节丰富的街景图像，为城市分析等应用提供了有力工具，并在跨视角图像合成领域推进了技术进步。

Abstract: Street view imagery has become an essential source for geospatial data
collection and urban analytics, enabling the extraction of valuable insights
that support informed decision-making. However, synthesizing street-view images
from corresponding satellite imagery presents significant challenges due to
substantial differences in appearance and viewing perspective between these two
domains. This paper presents a hybrid framework that integrates diffusion-based
models and conditional generative adversarial networks to generate
geographically consistent street-view images from satellite imagery. Our
approach uses a multi-stage training strategy that incorporates Stable
Diffusion as the core component within a dual-branch architecture. To enhance
the framework's capabilities, we integrate a conditional Generative Adversarial
Network (GAN) that enables the generation of geographically consistent
panoramic street views. Furthermore, we implement a fusion strategy that
leverages the strengths of both models to create robust representations,
thereby improving the geometric consistency and visual quality of the generated
street-view images. The proposed framework is evaluated on the challenging
Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image
synthesis. Experimental results demonstrate that our hybrid approach
outperforms diffusion-only methods across multiple evaluation metrics and
achieves competitive performance compared to state-of-the-art GAN-based
methods. The framework successfully generates realistic and geometrically
consistent street-view images while preserving fine-grained local details,
including street markings, secondary roads, and atmospheric elements such as
clouds.

</details>


### [257] [DINOReg: Strong Point Cloud Registration with Vision Foundation Model](https://arxiv.org/abs/2509.24370)
*Congjia Chen,Yufu Qu*

Main category: cs.CV

TL;DR: 本论文提出DINOReg，一种有效融合视觉与几何信息的点云配准网络，利用DINOv2视觉模型和混合位置编码，在多个数据集上取得大幅度性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统点云配准方法仅利用几何信息，或者简单融合颜色信息，未能充分开发图像中的丰富纹理与语义，且特征融合过程存在信息损失，限制了配准性能。

Method: 作者提出了DINOReg网络。方法上，借助DINOv2视觉基础模型，从图像中提取丰富的视觉特征（包含纹理和全局语义），并在patch级别与点云的几何特征高效融合；同时提出混合位置编码，将图像空间和点云空间的位置信息结合，增强网络对空间关系的感知能力。

Result: 在RGBD-3DMatch和RGBD-3DLoMatch数据集上，DINOReg相比最新几何或多模态配准方法，patch内点比率提升了14.2%，配准召回率提升了15.7%。

Conclusion: DINOReg能够充分利用视觉和几何特征，大幅提升RGB-D点云配准效果，相关代码已开源。

Abstract: Point cloud registration is a fundamental task in 3D computer vision. Most
existing methods rely solely on geometric information for feature extraction
and matching. Recently, several studies have incorporated color information
from RGB-D data into feature extraction. Although these methods achieve
remarkable improvements, they have not fully exploited the abundant texture and
semantic information in images, and the feature fusion is performed in an
image-lossy manner, which limit their performance. In this paper, we propose
DINOReg, a registration network that sufficiently utilizes both visual and
geometric information to solve the point cloud registration problem. Inspired
by advances in vision foundation models, we employ DINOv2 to extract
informative visual features from images, and fuse visual and geometric features
at the patch level. This design effectively combines the rich texture and
global semantic information extracted by DINOv2 with the detailed geometric
structure information captured by the geometric backbone. Additionally, a mixed
positional embedding is proposed to encode positional information from both
image space and point cloud space, which enhances the model's ability to
perceive spatial relationships between patches. Extensive experiments on the
RGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves
significant improvements over state-of-the-art geometry-only and multi-modal
registration methods, with a 14.2% increase in patch inlier ratio and a 15.7%
increase in registration recall. The code is publicly available at
https://github.com/ccjccjccj/DINOReg.

</details>


### [258] [Mask Clustering-based Annotation Engine for Large-Scale Submeter Land Cover Mapping](https://arxiv.org/abs/2509.24374)
*Hao Chen,Fang Xu,Tamer Saleh,Weifeng Hao,Gui-Song Xia*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的亚米级遥感影像标注方法MCAE，并构建了大规模城市地表覆盖数据集HiCity-LC。


<details>
  <summary>Details</summary>
Motivation: 亚米级遥感影像为地表覆盖精细分析提供了可能，但缺乏高质量大规模标注数据集限制了其有效利用。传统的标注方法成本高且效率低。

Method: 作者受空间自相关原理启发，提出Mask Clustering-based Annotation Engine (MCAE)方法，将语义一致的掩码分组作为最小标注单元，同时高效标记多个实例。

Result: MCAE极大提升了标注效率（提升1至2个数量级），保证了标签质量、多样性和空间代表性。利用MCAE构建了约140亿像素、覆盖五座中国主要城市的HiCity-LC数据集，分类准确率超过85%。

Conclusion: MCAE方法证实了其在大规模亚米级遥感影像标注上的可扩展性和实用性，HiCity-LC成为首个公开的亚米级城市地表覆盖基准数据集。

Abstract: Recent advances in remote sensing technology have made submeter resolution
imagery increasingly accessible, offering remarkable detail for fine-grained
land cover analysis. However, its full potential remains underutilized -
particularly for large-scale land cover mapping - due to the lack of
sufficient, high-quality annotated datasets. Existing labels are typically
derived from pre-existing products or manual annotation, which are often
unreliable or prohibitively expensive, particularly given the rich visual
detail and massive data volumes of submeter imagery. Inspired by the spatial
autocorrelation principle, which suggests that objects of the same class tend
to co-occur with similar visual features in local neighborhoods, we propose the
Mask Clustering-based Annotation Engine (MCAE), which treats semantically
consistent mask groups as the minimal annotating units to enable efficient,
simultaneous annotation of multiple instances. It significantly improves
annotation efficiency by one to two orders of magnitude, while preserving label
quality, semantic diversity, and spatial representativeness. With MCAE, we
build a high-quality annotated dataset of about 14 billion labeled pixels,
referred to as HiCity-LC, which supports the generation of city-scale land
cover maps across five major Chinese cities with classification accuracies
above 85%. It is the first publicly available submeter resolution city-level
land cover benchmark, highlighting the scalability and practical utility of
MCAE for large-scale, submeter resolution mapping. The dataset is available at
https://github.com/chenhaocs/MCAE

</details>


### [259] [REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2509.24382)
*Soumyadeep Chandra,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出了一种全新自监督框架REALIGN，能更有效地从过程类视频中学习高层次时序结构，实现对步骤乱序、重复动作、无关片段的鲁棒处理，显著优于此前方法。


<details>
  <summary>Details</summary>
Motivation: 以往基于Kantorovich最优传输（KOT）的方法只关注帧间特征相似度，无法捕捉任务的高阶时序结构，且难以应对现实教学视频中常见的步骤错乱、重复、背景干扰等情形。

Method: 提出基于正则化融合部分Gromov-Wasserstein最优传输（R-FPGWOT）的自监督学习方法REALIGN；该方法结合视觉对应与时序结构建模，可部分对齐序列，有效忽略噪声帧及错序步骤，并整合序列间对比学习以稳定训练。

Result: 在EgoProceL、ProceL、CrossTask等数据集上，REALIGN平均F1分数提升最高18.9%，时序IoU提升超30%；同时能得到更易解释的传输映射，有效保留关键步骤顺序并去除无关信息。

Conclusion: REALIGN突破了现有方法的单一特征相似性限制，实现了对过程类视频中任务结构的强鲁棒建模，将自监督步骤学习推向新高度。

Abstract: Learning from procedural videos remains a core challenge in self-supervised
representation learning, as real-world instructional data often contains
background segments, repeated actions, and steps presented out of order. Such
variability violates the strong monotonicity assumptions underlying many
alignment methods. Prior state-of-the-art approaches, such as OPEL, leverage
Kantorovich Optimal Transport (KOT) to build frame-to-frame correspondences,
but rely solely on feature similarity and fail to capture the higher-order
temporal structure of a task. In this paper, we introduce REALIGN, a
self-supervised framework for procedure learning based on Regularized Fused
Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT,
our formulation jointly models visual correspondences and temporal relations
under a partial alignment scheme, enabling robust handling of irrelevant
frames, repeated actions, and non-monotonic step orders common in instructional
videos. To stabilize training, we integrate FPGWOT distances with
inter-sequence contrastive learning, avoiding the need for multiple
regularizers and preventing collapse to degenerate solutions. Across egocentric
(EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves
up to 18.9% average F1-score improvements and over 30% temporal IoU gains,
while producing more interpretable transport maps that preserve key-step
orderings and filter out noise.

</details>


### [260] [Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy](https://arxiv.org/abs/2509.24385)
*Haijier Chen,Bo Xu,Shoujian Zhang,Haoze Liu,Jiaxuan Lin,Jingrong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频输入的3D多模态大语言模型Vid-LLM，能够无需外部3D数据直接理解3D场景，并在多个3D任务上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在2D视觉-语言推理方面已有较大进展，但将其扩展到3D场景理解存在挑战。现有3D-MLLM通常依赖3D数据，限制了其可扩展性与泛化能力，因此亟需能用更普适输入（如视频）实现3D理解的方法。

Method: Vid-LLM直接利用视频输入，结合几何先验来提升场景感知能力。方法上，设计了一种Cross-Task Adapter（CTA）模块，将3D几何信息与视觉-语言特征对齐；同时引入Metric Depth Model以恢复真实尺度的几何信息。最后，采用两阶段蒸馏策略优化模型，实现快速收敛与训练稳定。

Result: 在多个基准任务（包括3D问答、3D密集描述和3D视觉指代）中进行实验，结果表明该方法在多任务表现上优于现有方法。

Conclusion: Vid-LLM证明了基于视频输入的3D-MLLM无需底层3D数据也能高效完成3D理解任务，兼具实用性和多任务能力，对实际应用具有较大潜力。

Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have
significantly improved Vision-Language (VL) reasoning in 2D domains. However,
extending these capabilities to 3D scene understanding remains a major
challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend
on 3D data inputs, which limits scalability and generalization. To address this
limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes
video inputs without requiring external 3D data, making it practical for
real-world deployment. In our method, the geometric prior are directly used to
improve the performance of the sceen perception. To integrate the geometric
cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to
align the 3D geometric priors with the vision-language representations. To
ensure geometric consistency and integrity, we introduce a Metric Depth Model
that recovers real-scale geometry from the reconstruction outputs. Finally, the
model is fine-tuned with a two-stage distillation optimization strategy,
realizing fast convergence and stabilizes training. Extensive experiments
across diverse benchmarks verified the effectiveness of our method on 3D
Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks,
demonstrating the superior multi-task capabilities.

</details>


### [261] [PCICF: A Pedestrian Crossing Identification and Classification Framework](https://arxiv.org/abs/2509.24386)
*Junyi Gu,Beatriz Cabrero-Daniel,Ali Nouri,Lydia Armini,Christian Berger*

Main category: cs.CV

TL;DR: 本文提出了一套系统化识别和分类弱势交通参与者（VRU）场景（如行人、自行车骑行者）的方法PCICF，并通过改进合成数据集构建多行人过街情形的结构化字典（MoreSMIRK），结合高效的空间填充曲线算法与现实世界数据集验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着Robotaxi在世界多地商用落地，对城市中弱势交通参与者（如行人等）高可靠感知成为核心安全需求。现有数据集和场景识别方法在复杂多行人情形下存在局限，急需系统化工具辅助运营域内（ODD）的事件分析和AI训练评估。

Method: 作者基于现有合成数据集SMIRK扩展，系统构建了多行人过街情境的MoreSMIRK数据集字典。提出利用空间填充曲线（SFC）将多维场景特征压缩为可归类特征模式，并设计PCICF框架用于从多源传感数据发现并分类实际VRU场景。PCICF在手工标注的真实行人过街数据集PIE上进行验证。

Result: PCICF能够有效识别和分类复杂的行人过街事件，包括多行人分合等动态场景。算法计算高效，具备实际Robotaxi车载应用的潜力。相关数据与代码开源。

Conclusion: PCICF框架为Robotaxi等自动驾驶运营域内弱势交通参与者场景识别提供了系统化、高效、可扩展的工具基础。同时丰富的开放式MoreSMIRK数据集和方法，可促进行业对安全关键事件检测能力的提升。

Abstract: We have recently observed the commercial roll-out of robotaxis in various
countries. They are deployed within an operational design domain (ODD) on
specific routes and environmental conditions, and are subject to continuous
monitoring to regain control in safety-critical situations. Since ODDs
typically cover urban areas, robotaxis must reliably detect vulnerable road
users (VRUs) such as pedestrians, bicyclists, or e-scooter riders. To better
handle such varied traffic situations, end-to-end AI, which directly compute
vehicle control actions from multi-modal sensor data instead of only for
perception, is on the rise. High quality data is needed for systematically
training and evaluating such systems within their OOD. In this work, we propose
PCICF, a framework to systematically identify and classify VRU situations to
support ODD's incident analysis. We base our work on the existing synthetic
dataset SMIRK, and enhance it by extending its single-pedestrian-only design
into the MoreSMIRK dataset, a structured dictionary of multi-pedestrian
crossing situations constructed systematically. We then use space-filling
curves (SFCs) to transform multi-dimensional features of scenarios into
characteristic patterns, which we match with corresponding entries in
MoreSMIRK. We evaluate PCICF with the large real-world dataset PIE, which
contains more than 150 manually annotated pedestrian crossing videos. We show
that PCICF can successfully identify and classify complex pedestrian crossings,
even when groups of pedestrians merge or split. By leveraging computationally
efficient components like SFCs, PCICF has even potential to be used onboard of
robotaxis for OOD detection for example. We share an open-source replication
package for PCICF containing its algorithms, the complete MoreSMIRK dataset and
dictionary, as well as our experiment results presented in:
https://github.com/Claud1234/PCICF

</details>


### [262] [RapidMV: Leveraging Spatio-Angular Representations for Efficient and Consistent Text-to-Multi-View Synthesis](https://arxiv.org/abs/2509.24410)
*Seungwook Kim,Yichun Shi,Kejie Li,Minsu Cho,Peng Wang*

Main category: cs.CV

TL;DR: RapidMV是一种高效的文本到多视图合成图像生成模型，可以在约5秒内生成32张多视图图片，并且在一致性和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视图合成图像（从文本描述快速生成高一致性的多角度图片）对于虚拟三维资产生成和相关应用非常重要。目前相关方法在生成效率、一致性和语义对齐上存在不足。

Method: 作者提出了一个新的空间-视角潜在空间，将空间外观和视角变化编码到单一潜变量中，以提升多视图图像的一致性和生成效率。同时，训练过程被拆解为多个阶段以保证训练效果。

Result: RapidMV在多视图一致性和推理速度方面超越现有方法，并在生成质量和文本-图像对齐方面表现出有竞争力的结果。

Conclusion: RapidMV为高效且高一致性的文本到多视图生成提供了新思路，有望推动合成3D资产等后续应用的发展。

Abstract: Generating synthetic multi-view images from a text prompt is an essential
bridge to generating synthetic 3D assets. In this work, we introduce RapidMV, a
novel text-to-multi-view generative model that can produce 32 multi-view
synthetic images in just around 5 seconds. In essence, we propose a novel
spatio-angular latent space, encoding both the spatial appearance and angular
viewpoint deviations into a single latent for improved efficiency and
multi-view consistency. We achieve effective training of RapidMV by
strategically decomposing our training process into multiple steps. We
demonstrate that RapidMV outperforms existing methods in terms of consistency
and latency, with competitive quality and text-image alignment.

</details>


### [263] [CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers](https://arxiv.org/abs/2509.24416)
*Kai Liu,Shaoqiu Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于扩散Transformer（DiTs）模型的高效后训练量化方法CLQ，有效在保证生成质量的同时，实现了模型显著压缩和加速。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer模型由于体积大和复杂性高，限制了其在边缘设备上的应用。因此需要有效的压缩和加速方法，而目前的后训练量化（PTQ）方法不可避免地会带来性能下降。

Method: 提出了跨层引导正交量化方法CLQ，包含三大设计：1）跨块校准（CBC），获得更真实的校准数据，提升量化精度；2）基于正交的平滑方法（OBS），利用Hadamard矩阵平滑激活的异常值；3）跨层参数搜索（CLPS），优化各层量化参数。

Result: 在图像生成和视频生成模型上测试CLQ，可以将模型有效压缩为W4A4（权重量化4位，激活量化4位），实现近4倍的内存节约和推理加速，视觉质量和指标损失极小。

Conclusion: CLQ为DiTs在资源受限设备上的部署提供了切实可行的解决方案，在保持高生成质量的前提下，实现了高效压缩和显著加速。

Abstract: Visual generation quality has been greatly promoted with the rapid advances
in diffusion transformers (DiTs), which is attributed to the scaling of model
size and complexity. However, these attributions also hinder the practical
deployment of DiTs on edge devices, limiting their development and application.
Serve as an efficient model compression technique, model post-training
quantization (PTQ) can reduce the memory consumption and speed up the
inference, with inevitable performance degradation. To alleviate the
degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization
method for DiTs. To be specific, CLQ consists of three key designs. First, we
observe that the calibration data used by most of the PTQ methods can not
honestly represent the distribution of the activations. Therefore, we propose
cross-block calibration (CBC) to obtain accurate calibration data, with which
the quantization can be better guided. Second, we propose orthogonal-based
smoothing (OBS), which quantifies the outlier score of each channel and
leverages block Hadamard matrix to smooth the outliers with negligible
overhead. Third, we propose cross-layer parameter searching (CLPS) to search.
We evaluate CLQ with both image generation and video generation models and
successfully compress the model into W4A4 with negligible degradation in visual
quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our
code is available at
\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.

</details>


### [264] [A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models](https://arxiv.org/abs/2509.24420)
*Pei-Han Chen,Szu-Chi Chung*

Main category: cs.CV

TL;DR: 本文关注机器学习中图像训练数据质量评估，提出并改进数据清洗工具管线，显著提升低质量及重复图像检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着模型结构日益成熟，模型性能提升空间有限，数据质量成为影响模型表现的重要因素。然而，目前系统性评估与提升图像数据集质量的研究尚不充分，急需有效的质量检测和优化方法。

Method: 作者系统分析CIFAKE图像数据集中的常见质量问题，并基于CleanVision和Fastdup两个开源工具构建整合清洗管道，提出自动阈值选择方案用于无人工干预地检测低质量图片，将质量检测问题转为二分类任务，以F1分数作为主要衡量标准。

Result: 实验显示，不同质量问题对模型影响差异明显。提出的自动阈值方法，在单一扰动下F1分数从0.6794提升到0.9468，在双扰动下从0.7447提升到0.8557。对于重复图片检测，F1分数从0.4576提升到0.7928。

Conclusion: 作者提出的工作流有效提升了图像数据集自动化质量检测与去重能力，为提升基于图像的机器学习数据质量评估提供了基础和方法支持。

Abstract: In machine learning, research has traditionally focused on model development,
with relatively less attention paid to training data. As model architectures
have matured and marginal gains from further refinements diminish, data quality
has emerged as a critical factor. However, systematic studies on evaluating and
ensuring dataset quality in the image domain remain limited.
  This study investigates methods for systematically assessing image dataset
quality and examines how various image quality factors influence model
performance. Using the publicly available and relatively clean CIFAKE dataset,
we identify common quality issues and quantify their impact on training.
Building on these findings, we develop a pipeline that integrates two
community-developed tools, CleanVision and Fastdup. We analyze their underlying
mechanisms and introduce several enhancements, including automatic threshold
selection to detect problematic images without manual tuning.
  Experimental results demonstrate that not all quality issues exert the same
level of impact. While convolutional neural networks show resilience to certain
distortions, they are particularly vulnerable to degradations that obscure
critical visual features, such as blurring and severe downscaling. To assess
the performance of existing tools and the effectiveness of our proposed
enhancements, we formulate the detection of low-quality images as a binary
classification task and use the F1 score as the evaluation metric. Our
automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under
single perturbations and from 0.7447 to 0.8557 under dual perturbations. For
near-duplicate detection, our deduplication strategy increases the F1 score
from 0.4576 to 0.7928. These results underscore the effectiveness of our
workflow and provide a foundation for advancing data quality assessment in
image-based machine learning.

</details>


### [265] [Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh](https://arxiv.org/abs/2509.24421)
*Yuanyuan Gao,Yuning Gong,Yifei Liu,Li Jingfeng,Zhihang Zhong,Dingwen Zhang,Yanci Zhang,Dan Xu,Xiao Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为Proxy-GS的新方法，通过引入代理系统实现对高斯点遮挡的感知，在大规模场景下实现更快且高质量的3D渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点渲染（3DGS）及其MLP扩展方案，提高了画面质量但导致解码计算量大。尽管存在剪枝、细节层次（LOD）等优化手段，但由于缺乏遮挡感知，冗余依然严重。作者希望解决遮挡不敏感造成的效率与质量问题。

Method: 作者设计了Proxy-GS方法，引入一个高效代理系统，能在1ms内生成1000x1000分辨率的精确遮挡深度图。该代理在渲染时用于引导高斯点以及锚点的剔除，加速渲染；在训练时引导稠密化，避免遮挡区域的不一致，提高质量。

Result: Proxy-GS在强遮挡场景（如MatrixCity Streets数据集）下，既显著提升MLP-高斯渲染效果，又实现了超过Octree-GS 2.5倍的渲染速度，渲染质量也显著更高。

Conclusion: Proxy-GS有效解决了大规模场景高斯渲染的速度和遮挡冗余问题，在提升计算效率的同时确保了出色的渲染效果，为相关领域提供了新的高效方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for
achieving photorealistic rendering. Recent MLP-based variants further improve
visual fidelity but introduce substantial decoding overhead during rendering.
To alleviate computation cost, several pruning strategies and level-of-detail
(LOD) techniques have been introduced, aiming to effectively reduce the number
of Gaussian primitives in large-scale scenes. However, our analysis reveals
that significant redundancy still remains due to the lack of occlusion
awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a
proxy to introduce Gaussian occlusion awareness from any view. At the core of
our approach is a fast proxy system capable of producing precise occlusion
depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles:
first, it guides the culling of anchors and Gaussians to accelerate rendering
speed. Second, it guides the densification towards surfaces during training,
avoiding inconsistencies in occluded regions, and improving the rendering
quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset,
Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering
capability but also achieves faster rendering speed. Specifically, it achieves
more than 2.5x speedup over Octree-GS, and consistently delivers substantially
higher rendering quality. Code will be public upon acceptance.

</details>


### [266] [Rethinking Unsupervised Cross-modal Flow Estimation: Learning from Decoupled Optimization and Consistency Constraint](https://arxiv.org/abs/2509.24423)
*Runmin Zhang,Jialiang Wang,Si-Yuan Cao,Zhu Yu,Junchen Yu,Guangyi Zhang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 本文提出了DCFlow，一种创新的无监督跨模态流估计框架，通过解耦优化和跨模态一致性约束提升流估计精度，并在无监督条件下实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖外观相似性进行流估计，难以应对不同模态之间的差异和几何失配。为提升跨模态流估计的鲁棒性和精确度，亟需新的优化策略和一致性约束。

Method: DCFlow采用了解耦优化策略，将模态迁移网络与流估计网络协同训练，分别处理模态差异和几何失配问题。结合几何感知的数据合成流程及鲁棒损失函数为网络提供无标注运动监督，并引入跨模态一致性约束联合优化两网络。

Result: DCFlow可以与不同类型的流估计网络集成，并在作者构建的综合跨模态流基准上取得了领先的无监督性能。

Conclusion: DCFlow有效提升了跨模态流估计的准确性，有望成为未来相关研究的基础方法。

Abstract: This work presents DCFlow, a novel unsupervised cross-modal flow estimation
framework that integrates a decoupled optimization strategy and a cross-modal
consistency constraint. Unlike previous approaches that implicitly learn flow
estimation solely from appearance similarity, we introduce a decoupled
optimization strategy with task-specific supervision to address modality
discrepancy and geometric misalignment distinctly. This is achieved by
collaboratively training a modality transfer network and a flow estimation
network. To enable reliable motion supervision without ground-truth flow, we
propose a geometry-aware data synthesis pipeline combined with an
outlier-robust loss. Additionally, we introduce a cross-modal consistency
constraint to jointly optimize both networks, significantly improving flow
prediction accuracy. For evaluation, we construct a comprehensive cross-modal
flow benchmark by repurposing public datasets. Experimental results demonstrate
that DCFlow can be integrated with various flow estimation networks and
achieves state-of-the-art performance among unsupervised approaches.

</details>


### [267] [UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark](https://arxiv.org/abs/2509.24427)
*Ailing Zhang,Lina Lei,Dehong Kong,Zhixin Wang,Jiaqi Xu,Fenglong Song,Chun-Le Guo,Chang Liu,Fan Li,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一个全新的视频合成模型评测基准UI2V-Bench，重点考察模型对图像语义及推理能力的理解，弥补了以往只关注画质和时序一致性的不足。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频（I2V）生成模型热点突出，但相关评测体系主要关注视频质量和时序一致性，缺乏对模型语义理解和推理等能力的系统评估，因此急需新的评价标准来推动该领域的深入发展。

Method: 作者提出UI2V-Bench基准，围绕空间理解、属性绑定、类别认知和推理四大维度设计评测方案，采用多模态大语言模型（MLLM）分别实现实例级细粒度语义理解和基于反馈的因果推理自动化评测，包含约500组精心设计的文本-图像对，并对多种业内I2V模型进行测试。同时辅以人工评价校验自动化结果的有效性。

Result: 通过评测发现，UI2V-Bench能够有效区分和检验不同I2V模型在语义理解与推理能力上的优劣，并且MLLM自动评测指标与人工评价高度一致。

Conclusion: UI2V-Bench为I2V模型的语义和推理能力评测提供了系统、强有力的参考框架和数据集，填补了现有基准的空白，并助力未来模型研发和学术探索。

Abstract: Generative diffusion models are developing rapidly and attracting increasing
attention due to their wide range of applications. Image-to-Video (I2V)
generation has become a major focus in the field of video synthesis. However,
existing evaluation benchmarks primarily focus on aspects such as video quality
and temporal consistency, while largely overlooking the model's ability to
understand the semantics of specific subjects in the input image or to ensure
that the generated video aligns with physical laws and human commonsense. To
address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V
models with a focus on semantic understanding and reasoning. It introduces four
primary evaluation dimensions: spatial understanding, attribute binding,
category understanding, and reasoning. To assess these dimensions, we design
two evaluation methods based on Multimodal Large Language Models (MLLMs): an
instance-level pipeline for fine-grained semantic understanding, and a
feedback-based reasoning pipeline that enables step-by-step causal assessment
for more accurate evaluation. UI2V-Bench includes approximately 500 carefully
constructed text-image pairs and evaluates a range of both open source and
closed-source I2V models across all defined dimensions. We further incorporate
human evaluations, which show strong alignment with the proposed MLLM-based
metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by
emphasizing semantic comprehension and reasoning ability, offering a robust
framework and dataset to support future research and model development in the
field.

</details>


### [268] [NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding](https://arxiv.org/abs/2509.24441)
*Yanpeng Zhao,Shanyan Guan,Yunbo Wang,Yanhao Ge,Wei Li,Xiaokang Yang*

Main category: cs.CV

TL;DR: NeoWorld是一个可以通过单张图片生成交互式3D虚拟世界的深度学习框架，采用三维建模前景、二维合成背景的混合方式，其在效率和真实感上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于单张图片生成虚拟世界的方法常常受限于全局生成或纯2D处理，难以实现高度真实和交互性强的3D虚拟环境。为解决视觉真实感、效率和用户交互性之间的平衡问题，提出一种按需渲染且用户可交互的3D虚拟世界生成框架。

Method: NeoWorld采用前景对象3D建模、背景及非交互区域用2D合成的混合场景结构，结合最新表示学习和对象转3D技术，将用户关注区域三维高保真渲染，同时允许自然语言操控对象外观与动作。

Result: 在WorldScore基准测试中，NeoWorld在生成的动态性、沉浸感和视觉一致性等方面显著优于现有2D与2.5D深度层分方法。

Conclusion: NeoWorld实现了高效、动态且沉浸式的3D场景生成，推动了基于图像的交互式虚拟世界技术的发展，在实时虚拟世界探索与用户自定义体验方面具有重大应用前景。

Abstract: We introduce NeoWorld, a deep learning framework for generating interactive
3D virtual worlds from a single input image. Inspired by the on-demand
worldbuilding concept in the science fiction novel Simulacron-3 (1964), our
system constructs expansive environments where only the regions actively
explored by the user are rendered with high visual realism through
object-centric 3D representations. Unlike previous approaches that rely on
global world generation or 2D hallucination, NeoWorld models key foreground
objects in full 3D, while synthesizing backgrounds and non-interacted regions
in 2D to ensure efficiency. This hybrid scene structure, implemented with
cutting-edge representation learning and object-to-3D techniques, enables
flexible viewpoint manipulation and physically plausible scene animation,
allowing users to control object appearance and dynamics using natural language
commands. As users interact with the environment, the virtual world
progressively unfolds with increasing 3D detail, delivering a dynamic,
immersive, and visually coherent exploration experience. NeoWorld significantly
outperforms existing 2D and depth-layered 2.5D methods on the WorldScore
benchmark.

</details>


### [269] [Generalist Multi-Class Anomaly Detection via Distillation to Two Heterogeneous Student Networks](https://arxiv.org/abs/2509.24448)
*Hangil Park,Yongmin Seo,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于知识蒸馏的新颖双模型集成方法，实现了对工业检测与语义异常检测的广域异常检测，并在多个公开数据集上取得了突出表现。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测方法通常局限于特定场景（如工业检测或语义检测），缺乏对多样化任务的泛化能力，并且对数据集设置较为敏感。因此，亟需一种能跨多领域、任务鲁棒性的异常检测框架。

Method: 论文提出了一个由教师模型及两个学生模型（Encoder-Decoder用于工业细粒度检测，Encoder-Encoder用于语义异常检测）组成的框架，所有模型共用预训练特征提取器DINOv2。通过Noisy-OR目标函数联合训练，并融合本地与全局异常得分以获得最终异常评分。

Result: 该方法在MVTec-AD、MVTec-LOCO、VisA、Real-IAD等工业类数据集以及CIFAR-10/100、FMNIST、View等语义类数据集上，单类与多类任务均取得了SOTA结果。如MVTec-AD图像级AUROC达到99.7%，CIFAR-10上为97.8%，显著优于通用及专业方法。

Conclusion: 提出的双模型KD集成方案在兼顾工业与语义场景的异常检测能力方面，实现了跨领域的泛化能力和高准确率，推动了异常检测模型的通用化发展。

Abstract: Anomaly detection (AD) plays an important role in various real-world
applications. Recent advancements in AD, however, are often biased towards
industrial inspection, struggle to generalize to broader tasks like semantic
anomaly detection and vice versa. Although recent methods have attempted to
address general anomaly detection, their performance remains sensitive to
dataset-specific settings and single-class tasks. In this paper, we propose a
novel dual-model ensemble approach based on knowledge distillation (KD) to
bridge this gap. Our framework consists of a teacher and two student models: an
Encoder-Decoder model, specialized in detecting patch-level minor defects for
industrial AD and an Encoder-Encoder model, optimized for semantic AD. Both
models leverage a shared pre-trained encoder (DINOv2) to extract high-quality
feature representations. The dual models are jointly learned using the Noisy-OR
objective, and the final anomaly score is obtained using the joint probability
via local and semantic anomaly scores derived from the respective models. We
evaluate our method on eight public benchmarks under both single-class and
multi-class settings: MVTec-AD, MVTec-LOCO, VisA and Real-IAD for industrial
inspection and CIFAR-10/100, FMNIST and View for semantic anomaly detection.
The proposed method achieved state-of-the-art accuracies in both domains, in
multi-class as well as single-class settings, demonstrating generalization
across multiple domains of anomaly detection. Our model achieved an image-level
AUROC of 99.7% on MVTec-AD and 97.8% on CIFAR-10, which is significantly better
than the prior general AD models in multi-class settings and even higher than
the best specialist models on individual benchmarks.

</details>


### [270] [LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation](https://arxiv.org/abs/2509.24469)
*Heechang Kim,Gwanghyun Kim,Se Young Chun*

Main category: cs.CV

TL;DR: 本论文提出了一种结合Laban运动分析以实现更可控、更具表现力的人体动作生成方法，在文本驱动的扩散模型基础上，通过优化推理过程提升动作风格的多样性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的动作生成模型难以实现细粒度、富有表现力的动作控制，主要受限于语料数据的风格单一和自然语言难以描述定量动作特征，而Laban运动分析为动作质量描述提供了专业且一致性的量化工具。

Method: 作者创新性地将Laban Effort与Shape成分的量化方法集成到文本引导的动作生成扩散模型中。在推理阶段，通过优化文本嵌入，无需额外动作数据，实现对预训练模型动作属性的零样本控制。

Result: 实验表明，该方法成功依据目标Laban标签控制和操作动作属性，在保持动作身份的同时，生成了多样且富有表现力的动作质量。

Conclusion: 本文提出的零样本推理优化方法，为人类动作生成带来了更高的可解释性和表现力，对动作合成、交互等领域具有潜在广泛应用价值。

Abstract: Diverse human motion generation is an increasingly important task, having
various applications in computer vision, human-computer interaction and
animation. While text-to-motion synthesis using diffusion models has shown
success in generating high-quality motions, achieving fine-grained expressive
motion control remains a significant challenge. This is due to the lack of
motion style diversity in datasets and the difficulty of expressing
quantitative characteristics in natural language. Laban movement analysis has
been widely used by dance experts to express the details of motion including
motion quality as consistent as possible. Inspired by that, this work aims for
interpretable and expressive control of human motion generation by seamlessly
integrating the quantification methods of Laban Effort and Shape components
into the text-guided motion generation models. Our proposed zero-shot,
inference-time optimization method guides the motion generation model to have
desired Laban Effort and Shape components without any additional motion data by
updating the text embedding of pretrained diffusion models during the sampling
step. We demonstrate that our approach yields diverse expressive motion
qualities while preserving motion identity by successfully manipulating motion
attributes according to target Laban tags.

</details>


### [271] [Performance-Efficiency Trade-off for Fashion Image Retrieval](https://arxiv.org/abs/2509.24477)
*Julio Hurtado,Haoran Ni,Duygu Sap,Connor Mattinson,Martin Lotz*

Main category: cs.CV

TL;DR: 本文提出了一种选择性表征框架，有效压缩二手服装图像检索数据库至原来的10%，而检索准确率几乎不变。通过聚类与核心集选择筛选具代表性的样本，并引入基于邻域一致性的异常值剔除方法，从而提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 时尚产业对资源浪费和碳排放贡献巨大，促进二手市场可有效应对环境压力。机器学习可扩展二手服装估价，但如何高效进行大规模图像检索仍是挑战。作者希望通过数据库压缩提高系统检索效率，降低计算成本。

Method: 本文首先采用聚类和核心集选择（coreset selection）方法，从数据库中筛选代表性样本以保留关键信息和多样性。进一步提出一种基于邻域一致性评分的高效异常值剔除方法，先于样本选择阶段过滤无代表性的图像，提升选择质量。方法在DeepFashion系列三个公开数据集上进行评估。

Result: 通过策略性剪枝及代表性向量选择，数据库规模缩减至10%，检索系统的准确率基本未受影响，大幅减少了算力开销。异常值剔除方法结合聚类后，检索性能进一步提高。

Conclusion: 选择性表征与异常值剔除相结合，可大幅压缩数据库规模且保持高检索性能，降低了二手市场在图像检索上的计算资源消耗。该方法有助于二手时尚行业的可持续发展。

Abstract: The fashion industry has been identified as a major contributor to waste and
emissions, leading to an increased interest in promoting the second-hand
market. Machine learning methods play an important role in facilitating the
creation and expansion of second-hand marketplaces by enabling the large-scale
valuation of used garments. We contribute to this line of work by addressing
the scalability of second-hand image retrieval from databases. By introducing a
selective representation framework, we can shrink databases to 10% of their
original size without sacrificing retrieval accuracy. We first explore
clustering and coreset selection methods to identify representative samples
that capture the key features of each garment and its internal variability.
Then, we introduce an efficient outlier removal method, based on a
neighbour-homogeneity consistency score measure, that filters out
uncharacteristic samples prior to selection. We evaluate our approach on three
public datasets: DeepFashion Attribute, DeepFashion Con2Shop, and DeepFashion2.
The results demonstrate a clear performance-efficiency trade-off by
strategically pruning and selecting representative vectors of images. The
retrieval system maintains near-optimal accuracy, while greatly reducing
computational costs by reducing the images added to the vector database.
Furthermore, applying our outlier removal method to clustering techniques
yields even higher retrieval performance by removing non-discriminative samples
before the selection.

</details>


### [272] [Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs](https://arxiv.org/abs/2509.24491)
*Yuanshuai Li,Yuping Yan,Junfeng Tang,Yunxuan Li,Zeqi Zheng,Yaochu Jin*

Main category: cs.CV

TL;DR: 提出了一种新的MLLMs（多模态大模型）对齐方法SCPO，通过语义课程偏好数据集，实现细粒度对齐，大幅降低视觉幻觉问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管当前多模态大模型在许多任务上表现出色，但在生成与视觉证据矛盾的内容（即视觉幻觉）方面依然表现不佳。主流的DPO方法对细粒度语义对齐不足，且鼓励捷径学习，因此急需更有效的对齐方式来缓解视觉幻觉。

Method: 作者提出了SCPO框架，通过构建一个基于语义课程的偏好对齐数据集，实现由易到难的偏好对齐训练。训练中利用动态参考模型和新颖的对称双向目标，同时学习文本与视觉偏好，对齐多模态大模型的细粒度语义。

Result: 在多个LLaVA系列模型和不同规模版本上，SCPO显著优于baseline，在多个幻想基准上的幻觉率最高下降62.9%。此外，SCPO在通用基准上提升了事实正确性，同时保持了模型原有的综合能力，性能表现稳定。

Conclusion: SCPO首次将语义、对称性和课程学习相结合，实现了多模态大模型的高效对齐，能有效缓解视觉幻觉问题，为MLLMs实际应用提供了更可靠的方法。

Abstract: Multimodal Large Language Models (MLLMs) have significantly improved the
performance of various tasks, but continue to suffer from visual
hallucinations, a critical issue where generated responses contradict visual
evidence. While Direct Preference Optimization(DPO) is widely used for
alignment, its application to MLLMs often fails to capture fine-grained
semantic differences and encourages shortcut learning. To address these
challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a
novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard
curriculum built upon our Semantic Curriculum Preference Pairs dataset, which
provides fine-grained semantic contrasts sorted by difficulty. This curriculum
is trained with a dynamic reference model and a novel symmetric, bidirectional
objective to facilitate simultaneous learning from both textual and visual
preferences. To our knowledge, SCPO is the first framework to unify semantics,
symmetry, and curriculum for MLLMs alignment, effectively mitigating visual
hallucinations. Extensive experiments on LLaVA models across various scales and
versions validate that SCPO demonstrates superior performance compared to
baseline models on multiple hallucination benchmarks, reducing the
hallucination rate by up to 62.9%. Moreover, evaluations on generalized
benchmarks show that SCPO improves factuality while preserving general
capabilities, with its performance remaining stable across general
vision-language benchmarks.

</details>


### [273] [Robust Multimodal Semantic Segmentation with Balanced Modality Contributions](https://arxiv.org/abs/2509.24505)
*Jiaqi Tan,Xu Zheng,Fangyu Li,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为EQUISeg的新型多模态语义分割框架，通过均衡编码不同模态，提升模型在各模态退化时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前多模态分割方法在实际应用中会因依赖单一主导模态（如图像、深度信息等）而导致性能不稳，一旦主导模态受损，整体表现大幅下降。因此，实现各模态的有效平衡成为关键挑战。

Method: 作者提出了EQUISeg框架，基于四阶段跨模态Transformer模块（CMTB），实现高效的多模态融合和层次化选择。同时，设计了自引导模块（SGM），通过引入模态间的互导机制，自动调节各模态贡献，缓解模态失衡问题。

Result: 在多个数据集上进行的实验显示，EQUISeg在分割精度方面取得了显著提升，并有效减轻了因模态失衡带来的负面影响。

Conclusion: EQUISeg框架能够平衡多模态分割中的模态依赖性，提高模型在实际复杂场景下的鲁棒性和应用价值。

Abstract: Multimodal semantic segmentation enhances model robustness by exploiting
cross-modal complementarities. However, existing methods often suffer from
imbalanced modal dependencies, where overall performance degrades significantly
once a dominant modality deteriorates in real-world scenarios. Thus, modality
balance has become acritical challenge for practical multimodal segmentation.
To address this issue, we propose EQUISeg, a multimodal segmentation framework
that balances modality contributions through equal encoding of modalities.
Built upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables
efficient multimodal fusion and hierarchical selection. Furthermore, we design
a Self-guided Module(SGM) that mitigates modality imbalance by introducing a
mutual guidance mechanism, enabling each modality to adaptively adjust its
contribution and enhance robustness under degraded conditions. Extensive
experiments on multiple datasets demonstrate that EQUISeg achieves significant
performance gains and effectively alleviates the adverse effects of modality
imbalance in segmentation tasks.

</details>


### [274] [Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency](https://arxiv.org/abs/2509.24514)
*Jiaqi Tan,Fangyu Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出了QL-Adapter框架，通过引入ILFM和CMAM模块，提升了复杂场景下多目标图像编辑的能力，并建立了新的数据集和任务，在多个维度上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用CLIP文本编码器的图像编辑方法在场景复杂、目标多样时效果不佳，主要难点在于如何准确控制物体数量、空间布局及处理多类别目标，因此作者希望提出新的方法解决这两个实际挑战。

Method: 提出QL-Adapter框架，包含两个核心模块：（1）ILFM模块，将布局先验与来自CLIP的ViT图像patch特征融合，加强模型对空间结构的理解；（2）CMAM模块，将图像特征注入文本分支，丰富文本特征表达，提高模型指令理解与执行能力。同时构建了全新的QL-Dataset数据集，涵盖多类别、多布局和多数量变化，并定义了'数量与布局一致的图像编辑'（QL-Edit）任务。

Result: 在QL-Edit任务上，QL-Adapter获得了最新最强的性能表现，显著优于当前已有的多目标图像编辑模型。

Conclusion: QL-Adapter显著提升了对复杂多目标场景下的图像编辑能力，在数量和空间布局等方面表现突出；新建的数据集和任务为该领域提供了更具挑战性和代表性的基准，推动了多目标图像编辑研究的发展。

Abstract: Instruction driven image editing with standard CLIP text encoders often fails
in complex scenes with many objects. We present QL-Adapter, a framework for
multiple object editing that tackles two challenges: enforcing object counts
and spatial layouts, and accommodating diverse categories. QL-Adapter consists
of two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal
Augmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from
the CLIP image encoder to strengthen spatial structure understanding. CMAM
injects image features into the text branch to enrich textual embeddings and
improve instruction following. We further build QL-Dataset, a benchmark that
spans broad category, layout, and count variations, and define the task of
quantity and layout consistent image editing (QL-Edit). Extensive experiments
show that QL-Adapter achieves state of the art performance on QL-Edit and
significantly outperforms existing models.

</details>


### [275] [CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models](https://arxiv.org/abs/2509.24526)
*Zheyuan Hu,Chieh-Hsin Lai,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出了Consistency Mid-Training (CMT)方法，作为Flow map模型训练中的中间阶段，提升了训练效率和模型稳定性，显著减少了数据和计算需求，并在多个数据集上取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: CM和MF等Flow map模型能实现少步生成，但训练过程不稳定，对超参数敏感且耗费高。现有的从预训练扩散模型初始化的方法未能完全解决转换带来的不稳定性。作者旨在提升flow map训练的稳定性和效率。

Method: 提出在扩散模型预训练和flow map后训练之间插入一个轻量的中间训练阶段（CMT），通过沿预训练模型求解器轨迹直接映射到干净样本，实现轨迹一致且稳定的初始化，简化后续训练。

Result: CMT在CIFAR-10、ImageNet 64x64和512x512等数据集的两步FID分别达1.97、1.32、1.84，比传统方法减少高达98%的训练数据和GPU时间。在ImageNet 256x256上，1步FID为3.34，训练时间比从零训练的MF减少约50%。

Conclusion: CMT为flow map模型训练提供了稳定、高效且通用的框架，并在多个视觉生成任务中取得最佳性能和资源利用。

Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable
few-step generation by learning the long jump of the ODE solution of diffusion
models, yet training remains unstable, sensitive to hyperparameters, and
costly. Initializing from a pre-trained diffusion model helps, but still
requires converting infinitesimal steps into a long-jump map, leaving
instability unresolved. We introduce mid-training, the first concept and
practical method that inserts a lightweight intermediate stage between the
(diffusion) pre-training and the final flow map training (i.e., post-training)
for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact
and principled stage that trains a model to map points along a solver
trajectory from a pre-trained model, starting from a prior sample, directly to
the solver-generated clean sample. It yields a trajectory-consistent and stable
initialization. This initializer outperforms random and diffusion-based
baselines and enables fast, robust convergence without heuristics. Initializing
post-training with CMT weights further simplifies flow map learning.
Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10,
1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98%
less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT
reaches 1-step FID 3.34 while cutting total training time by about 50% compared
to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient,
and general framework for training flow map models.

</details>


### [276] [CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D](https://arxiv.org/abs/2509.24528)
*Mohamad Amin Mirzaei,Pantea Amoie,Ali Ekhterachian,Matin Mirzababaei*

Main category: cs.CV

TL;DR: 本文提出一种改进的3D场景理解方法，通过更精细的掩码生成与上下文感知的视觉编码提升三维语义分割和基于语言的物体检索效果，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于视觉语言模型的3D语义建图方法往往产生碎片化的掩码和不准确的语义分配，难以在复杂环境下实现准确的三维语义理解，有效性受限。

Method: 作者提出利用SemanticSAM并结合逐步细化机制，生成更准确、更丰富的物体级掩码，减轻原始SAM中常见的过度分割问题。同时，采用上下文感知的CLIP编码对每个掩码整合多种上下文视角并赋予经验权重，丰富视觉上下文信息。该方法可提升3D语义分割与基于语言的物体检索性能。

Result: 方法在多个3D场景理解相关任务（如3D语义分割、语言检索物体）和公开基准数据集上进行了评测，实验结果显示本方法在准确性和表现上均显著超过现有主流方法。

Conclusion: 改进的掩码生成和多上下文视觉语义融合机制显著提升了3D场景理解的效果，为机器人和智能体在复杂环境中的感知与交互提供了更可靠的语义支持。

Abstract: 3D scene understanding is fundamental for embodied AI and robotics,
supporting reliable perception for interaction and navigation. Recent
approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning
embedding vectors to 2D class-agnostic masks generated via vision-language
models (VLMs) and projecting these into 3D. However, these methods often
produce fragmented masks and inaccurate semantic assignments due to the direct
use of raw masks, limiting their effectiveness in complex environments. To
address this, we leverage SemanticSAM with progressive granularity refinement
to generate more accurate and numerous object-level masks, mitigating the
over-segmentation commonly observed in mask generation models such as vanilla
SAM, and improving downstream 3D semantic segmentation. To further enhance
semantic context, we employ a context-aware CLIP encoding strategy that
integrates multiple contextual views of each mask using empirically determined
weighting, providing much richer visual context. We evaluate our approach on
multiple 3D scene understanding tasks, including 3D semantic segmentation and
object retrieval from language queries, across several benchmark datasets.
Experimental results demonstrate significant improvements over existing
methods, highlighting the effectiveness of our approach.

</details>


### [277] [Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative Analysis](https://arxiv.org/abs/2509.24531)
*Kaizhen Zhu,Mokai Pan,Zhechuan Yu,Jingya Wang,Jingyi Yu,Ye Shi*

Main category: cs.CV

TL;DR: 本文统一分析了Diffusion Bridge与Flow Matching两种分布变换方法的理论和实验优劣，并提出了基于Transformer的Diffusion Bridge新结构，系统对比二者性能。


<details>
  <summary>Details</summary>
Motivation: Diffusion Bridge（扩散桥）与Flow Matching均在分布转换任务中取得良好表现，但两者理论假设与具体实现差异大，当前缺乏对其优劣统一的理论与实践分析。作者希望厘清何时应优选哪一方法，并解释其背后原因。

Method: 作者将两种方法均基于随机最优控制（Stochastic Optimal Control）视角重新表述，证明Diffusion Bridge在此框架下有更低的代价函数，并对Flow Matching在训练数据少时插值系数失效现象作出解释。为公平对比，作者提出了一种基于Transformer的新型Diffusion Bridge结构，并用相同架构实现Flow Matching。接着，作者围绕多项任务（如图像修复、超分辨、去模糊、去噪、翻译与风格迁移），系统测试不同分布差异和数据量下两者性能。

Result: 理论结果显示Diffusion Bridge具有更优的轨迹稳定性和自然性，Flow Matching在数据量降低时插值效果不佳。所有实验结果全面支持理论分析，揭示了两者在不同任务和数据条件下的适用性和局限性。

Conclusion: Diffusion Bridge与Flow Matching各有优劣，Diffusion Bridge在理论与实际尤其中小数据下更优。论文首次系统统一分析二者，为后续分布变换方法的选择和改进提供理论基础及实验依据。

Abstract: Diffusion Bridge and Flow Matching have both demonstrated compelling
empirical performance in transformation between arbitrary distributions.
However, there remains confusion about which approach is generally preferable,
and the substantial discrepancies in their modeling assumptions and practical
implementations have hindered a unified theoretical account of their relative
merits. We have, for the first time, provided a unified theoretical and
experimental validation of these two models. We recast their frameworks through
the lens of Stochastic Optimal Control and prove that the cost function of the
Diffusion Bridge is lower, guiding the system toward more stable and natural
trajectories. Simultaneously, from the perspective of Optimal Transport,
interpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly
ineffective when the training data size is reduced. To corroborate these
theoretical claims, we propose a novel, powerful architecture for Diffusion
Bridge built on a latent Transformer, and implement a Flow Matching model with
the same structure to enable a fair performance comparison in various
experiments. Comprehensive experiments are conducted across Image Inpainting,
Super-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks,
systematically varying both the distributional discrepancy (different
difficulty) and the training data size. Extensive empirical results align
perfectly with our theoretical predictions and allow us to delineate the
respective advantages and disadvantages of these two models. Our code is
available at https://anonymous.4open.science/r/DBFM-3E8E/.

</details>


### [278] [Foggy Crowd Counting: Combining Physical Priors and KAN-Graph](https://arxiv.org/abs/2509.24545)
*Yuhao Wang,Zhuoran Zheng,Han Hu,Dianjie Lu,Guijuan Zhang,Chen Lyu*

Main category: cs.CV

TL;DR: 本文针对雾天环境下人群计数的主要难点，提出了一种结合物理先验和数据驱动优化的人群计数方法，在四个公开数据集的实验中显著提升了计数精度。


<details>
  <summary>Details</summary>
Motivation: 在雾天等复杂气象条件下，传统人群计数面临如远距离目标模糊、局部特征退化和图像对比度减弱等挑战，导致准确率下降。因此，亟需一种既能结合物理机制，又能利用深度学习优势的方法来提升雾天人群计数的准确性。

Method: 方法首先引入可微分的大气散射模型，通过透射率动态估计和散射参数自适应校准，定量描述雾对不同景深目标的非线性衰减规律。其次，设计了基于Kolmogorov-Arnold表征定理的MSA-KAN模块，结合多层渐进式结构和自适应跳跃连接，提升退化区域的特征表达与边缘激活能力。最后提出气象感知GCN，结合MSA-KAN提取的深度特征动态构建空间邻接矩阵，加强复杂气候下空间关系建模。

Result: 在四个公开数据集的密集雾霾场景下，相比主流算法在MAE（平均绝对误差）指标上降低了12.2%至27.5%。

Conclusion: 该方法有效克服了雾天人群计数中的视野模糊、特征混淆等问题，在复杂气象环境下显著提升了计数准确性，具备良好的应用前景。

Abstract: Aiming at the key challenges of crowd counting in foggy environments, such as
long-range target blurring, local feature degradation, and image contrast
attenuation, this paper proposes a crowd-counting method with a physical a
priori of atmospheric scattering, which improves crowd counting accuracy under
complex meteorological conditions through the synergistic optimization of the
physical mechanism and data-driven.Specifically, first, the method introduces a
differentiable atmospheric scattering model and employs transmittance dynamic
estimation and scattering parameter adaptive calibration techniques to
accurately quantify the nonlinear attenuation laws of haze on targets with
different depths of field.Secondly, the MSA-KAN was designed based on the
Kolmogorov-Arnold Representation Theorem to construct a learnable edge
activation function. By integrating a multi-layer progressive architecture with
adaptive skip connections, it significantly enhances the model's nonlinear
representation capability in feature-degraded regions, effectively suppressing
feature confusion under fog interference.Finally, we further propose a
weather-aware GCN that dynamically constructs spatial adjacency matrices using
deep features extracted by MSA-KAN. Experiments on four public datasets
demonstrate that our method achieves a 12.2\%-27.5\% reduction in MAE metrics
compared to mainstream algorithms in dense fog scenarios.

</details>


### [279] [TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models](https://arxiv.org/abs/2509.24566)
*Zhifang Zhang,Qiqi Tao,Jiaqi Lv,Na Zhao,Lei Feng,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为TokenSwap的新型隐蔽后门攻击方法，能够更隐蔽地攻击大规模视觉-语言模型（LVLMs），显著提升攻击的成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有针对LVLMs的后门攻击大多采用固定目标模式，易被模型记忆和检测，并表现出明显的过度自信特征。因此亟需更隐蔽、更难以检测的攻击方式。

Method: TokenSwap方法并非让模型生成固定目标内容，而是干扰模型对物体关系的理解，具体为：训练时在输入图片中注入视觉触发器，并对文本答案的关键内容语法成分进行交换，导致模型生成内容时容易错误地表述物体间关系（袋装词行为）。为提升后门学习效率，引入了自适应token加权损失函数，突出相关token的学习。

Result: 实验结果表明，TokenSwap在多个基准和多种LVLM架构下均能实现高攻击成功率，同时大幅提升了攻击的隐蔽性和难以检测性。

Conclusion: TokenSwap有效填补了现有LVLM后门攻击在隐蔽性方面的不足，为视觉-语言模型的安全带来新的挑战。

Abstract: Large vision-language models (LVLMs) have achieved impressive performance
across a wide range of vision-language tasks, while they remain vulnerable to
backdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim
model to generate a predefined target pattern, which is either inserted into or
replaces the original content. We find that these fixed-pattern attacks are
relatively easy to detect, because the attacked LVLM tends to memorize such
frequent patterns in the training dataset, thereby exhibiting overconfidence on
these targets given poisoned inputs. To address these limitations, we introduce
TokenSwap, a more evasive and stealthy backdoor attack that focuses on the
compositional understanding capabilities of LVLMs. Instead of enforcing a fixed
targeted content, TokenSwap subtly disrupts the understanding of object
relationships in text. Specifically, it causes the backdoored model to generate
outputs that mention the correct objects in the image but misrepresent their
relationships (i.e., bags-of-words behavior). During training, TokenSwap
injects a visual trigger into selected samples and simultaneously swaps the
grammatical roles of key tokens in the corresponding textual answers. However,
the poisoned samples exhibit only subtle differences from the original ones,
making it challenging for the model to learn the backdoor behavior. To address
this, TokenSwap employs an adaptive token-weighted loss that explicitly
emphasizes the learning of swapped tokens, such that the visual triggers and
bags-of-words behavior are associated. Extensive experiments demonstrate that
TokenSwap achieves high attack success rates while maintaining superior
evasiveness and stealthiness across multiple benchmarks and various LVLM
architectures.

</details>


### [280] [BFSM: 3D Bidirectional Face-Skull Morphable Model](https://arxiv.org/abs/2509.24577)
*Zidu Wang,Meng Xu,Miao Xu,Hengyuan Ma,Jiankuo Zhao,Xutao Li,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本论文提出了一种3D双向人脸-颅骨可变形模型（BFSM），可实现人脸与颅骨形状之间的高精度推断和重建，并展示了其在医疗等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸与颅骨联合建模面临数据稀缺、配准精度不足、临床应用探索有限、以及对颅面畸形病人的包容性不高等问题，亟需提升数据全面性与建模方法。

Method: 1）构建包含正常与罕见颅面畸形案例的200+样本新数据集，每例含CT颅骨、CT人脸及高精度纹理人脸扫描；2）提出基于密集射线匹配的配准方法，实现面-颅骨-组织拓扑一致配准；3）基于此提出BFSM模型，实现面与颅骨间的共享系数空间推断，并能表达组织厚度变化，支持同一颅骨的多种面貌重建。

Result: BFSM模型在单图像3D重建、人脸-颅骨高精度互推及手术规划等任务中表现出强大的鲁棒性和精度，实验结果全面验证了方法有效性。

Conclusion: 本文开创性地实现了高精度、包容性强的人脸-颅骨联合建模，为临床、教育、远程医疗等应用提供了可靠工具和新思路。

Abstract: Building a joint face-skull morphable model holds great potential for
applications such as remote diagnostics, surgical planning, medical education,
and physically based facial simulation. However, realizing this vision is
constrained by the scarcity of paired face-skull data, insufficient
registration accuracy, and limited exploration of reconstruction and clinical
applications. Moreover, individuals with craniofacial deformities are often
overlooked, resulting in underrepresentation and limited inclusivity. To
address these challenges, we first construct a dataset comprising over 200
samples, including both normal cases and rare craniofacial conditions. Each
case contains a CT-based skull, a CT-based face, and a high-fidelity textured
face scan. Secondly, we propose a novel dense ray matching registration method
that ensures topological consistency across face, skull, and their tissue
correspondences. Based on this, we introduce the 3D Bidirectional Face-Skull
Morphable Model (BFSM), which enables shape inference between the face and
skull through a shared coefficient space, while also modeling tissue thickness
variation to support one-to-many facial reconstructions from the same skull,
reflecting individual changes such as fat over time. Finally, we demonstrate
the potential of BFSM in medical applications, including 3D face-skull
reconstruction from a single image and surgical planning prediction. Extensive
experiments confirm the robustness and accuracy of our method. BFSM is
available at https://github.com/wang-zidu/BFSM

</details>


### [281] [Comprehensive Benchmarking of YOLOv11 Architectures for Scalable and Granular Peripheral Blood Cell Detection](https://arxiv.org/abs/2509.24595)
*Mohamad Abou Ali,Mariam Abdulfattah,Baraah Al Hussein,Fadi Dornaika,Ali Cherry,Mohamad Hajj-Hassan,Lara Hamawy*

Main category: cs.CV

TL;DR: 本文构建了一个大规模的血细胞检测与分类的标注数据集，并系统评估了五种YOLOv11模型在外周血涂片细粒度检测任务中的表现，发现YOLOv11 Medium模型在准确率与效率间表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动分析外周血涂片（PBS）不仅劳动强度大，还容易受主观因素影响。尽管深度学习为自动化血细胞检测提供了新途径，但当前针对最先进检测模型（如YOLOv11）在此类细粒度任务的系统评估还很缺乏，因此需要探究这些方法在血细胞检测和分类中的实际效能。

Method: 作者首先整理并重新标注了一个包含16,891张图片、12种白细胞及红细胞类别的大型PBS数据集，总共298,850个细胞标注。随后，基于该数据集，对五种不同体量的YOLOv11模型（Nano至XLarge）进行了全面性能评测，采用两种常见数据划分策略（70:20:10和80:10:10），并结合mAP、精度、召回率、F1分数以及计算效率等多项指标系统比较。

Result: YOLOv11 Medium模型以mAP@0.5=0.934（8:1:1数据集切分）在检测精度与推理效率间取得了最优权衡。大规模模型（Large和XLarge）虽然准确率略有提升，但计算成本显著提高，性价比不高。8:1:1的数据切分在各模型中表现均优于7:2:1。

Conclusion: YOLOv11模型，尤其是Medium变体，是外周血涂片自动化、细粒度检测的高效框架。作者公开了高质量数据集，为血细胞检测与分类领域的后续研究提供了宝贵资源。

Abstract: Manual peripheral blood smear (PBS) analysis is labor intensive and
subjective. While deep learning offers a promising alternative, a systematic
evaluation of state of the art models such as YOLOv11 for fine grained PBS
detection is still lacking. In this work, we make two key contributions. First,
we curate a large scale annotated dataset for blood cell detection and
classification, comprising 16,891 images across 12 peripheral blood cell (PBC)
classes, along with the red blood cell class, all carefully re annotated for
object detection tasks. In total, the dataset contains 298,850 annotated cells.
Second, we leverage this dataset to conduct a comprehensive evaluation of five
YOLOv11 variants (ranging from Nano to XLarge). These models are rigorously
benchmarked under two data splitting strategies (70:20:10 and 80:10:10) and
systematically assessed using multiple performance criteria, including mean
Average Precision (mAP), precision, recall, F1 score, and computational
efficiency. Our experiments show that the YOLOv11 Medium variant achieves the
best trade off, reaching a mAP@0.5 of 0.934 under the 8:1:1 split. Larger
models (Large and XLarge) provide only marginal accuracy gains at substantially
higher computational cost. Moreover, the 8:1:1 split consistently outperforms
the 7:2:1 split across all models. These findings highlight YOLOv11,
particularly the Medium variant, as a highly effective framework for automated,
fine grained PBS detection. Beyond benchmarking, our publicly released dataset
(github.com/Mohamad-AbouAli/OI-PBC-Dataset) offers a valuable resource to
advance research on blood cell detection and classification in hematology.

</details>


### [282] [Biomechanical-phase based Temporal Segmentation in Sports Videos: a Demonstration on Javelin-Throw](https://arxiv.org/abs/2509.24606)
*Bikash Kumar Badatya,Vipul Baghel,Jyotirmoy Amin,Ravi Hegde*

Main category: cs.CV

TL;DR: 本论文提出了一种新的无监督方法，用于精确、高效地对标枪投掷等精细运动分段，无需人工标注，且在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 运动分析需要对关键动作阶段进行精确分割，传统方法依靠人工标注或实验室设备，效率低、成本高，难以大规模应用。

Method: 提出将结构化最优传输（SOT）与时空图卷积网络（ASTGCN）结合，形成新的无监督分段框架，无需人工标签即可识别动作阶段变化。

Result: 新方法在标枪投掷视频分段任务上取得了71.02%的mAP和74.61%的F1分数，显著超过其他无监督方法。同时，论文还发布了211个专业标枪投掷框架级标注数据集。

Conclusion: 所提出的无监督方法能有效提升运动时序分段的准确性与自动化，有望推广到其它体育运动分析场景中。

Abstract: Precise analysis of athletic motion is central to sports analytics,
particularly in disciplines where nuanced biomechanical phases directly impact
performance outcomes. Traditional analytics techniques rely on manual
annotation or laboratory-based instrumentation, which are time-consuming,
costly, and lack scalability. Automatic extraction of relevant kinetic
variables requires a robust and contextually appropriate temporal segmentation.
Considering the specific case of elite javelin-throw, we present a novel
unsupervised framework for such a contextually aware segmentation, which
applies the structured optimal transport (SOT) concept to augment the
well-known Attention-based Spatio-Temporal Graph Convolutional Network
(ASTGCN). This enables the identification of motion phase transitions without
requiring expensive manual labeling. Extensive experiments demonstrate that our
approach outperforms state-of-the-art unsupervised methods, achieving 71.02%
mean average precision (mAP) and 74.61% F1-score on test data, substantially
higher than competing baselines. We also release a new dataset of 211 manually
annotated professional javelin-throw videos with frame-level annotations,
covering key biomechanical phases: approach steps, drive, throw, and recovery.

</details>


### [283] [FreeRet: MLLMs as Training-Free Retrievers](https://arxiv.org/abs/2509.24621)
*Yuhan Zhu,Xiangyu Zeng,Chenting Wang,Xinhao Li,Yicheng Xu,Ziang Yan,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练即可将主流多模态大语言模型（MLLM）转变为高效检索器的框架——FreeRet，并在大规模基准测试中大幅领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM虽然具备强大的生成和理解能⼒，但用于跨模态检索时需繁重的额外训练（如对比学习），极大限制了通用性和应用效率。因此，作者探索：能否直接利用现有模型能力实现强检索？

Method: 提出FreeRet框架：1）利用现成MLLM直接提取具有语义信息的嵌入用于快速初筛；2）利用其推理能力对候选集进行更精细的重排序。技术细节包括绕过词汇对齐层获取更贴近语义的表示、引入显式先验条件提升表征质量，以及通过中性选项减少重排序时的框架效应。

Result: 在MMEB及MMEB-V2的46个数据集上，FreeRet表现超过经过数百万样本训练的模型；且可适用于各类和不同规模的MLLM，并支持任意模态组合。

Conclusion: 经过巧妙方法，无需微调训练的多模态大语言模型即可成为强大的通用检索引擎，可一体化实现检索、重排序和生成，极大拓展了其通用性和实际应用价值。

Abstract: Multimodal large language models (MLLMs) are emerging as versatile
foundations for mixed-modality retrieval. Yet, they often require heavy
post-hoc training to convert them into contrastive encoders for retrieval. This
work asks: Can off-the-shelf MLLMs serve as powerful retrievers without
additional training? We present FreeRet, a plug-and-play framework that turns
any MLLM into a two-stage retriever. FreeRet first derives semantically
grounded embeddings directly from the model for fast candidate search, and then
exploits its reasoning ability for precise reranking. The framework contributes
three advances: bypassing lexical alignment layers to obtain semantically
faithful embeddings, conditioning representation generation with explicit
priors, and mitigating framing effect in reranking via neutral choice framing.
On the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially
outperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is
model-agnostic and scales seamlessly across MLLM families and sizes, preserves
their generative abilities, supports arbitrary modality combinations, and
unifies retrieval, reranking, and generation into end-to-end RAG within a
single model. Our findings demonstrate that pretrained MLLMs, when carefully
harnessed, can serve as strong retrieval engines without training, closing a
critical gap in their role as generalists.

</details>


### [284] [Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs](https://arxiv.org/abs/2509.24640)
*Mohamad Ballout,Okajevo Wilfred,Seyedalireza Yaghoubi,Nohayr Muhammad Abdelmoneim,Julius Mayer,Elia Bruni*

Main category: cs.CV

TL;DR: 本论文提出了SPLICE基准集，专注于多维度事件推理，评估人类和多模态模型在视频片段排序上的表现，并发现现有视觉-语言模型与人类间存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型缺乏系统的事件推理评测方法，尤其是在涉及时间、因果、空间、上下文和常识推理时。为填补这个空白，作者希望构建一个能够多维度衡量事件推理能力的新基准。

Method: 作者基于COIN教学视频数据集，人工筛选和分段得到3,381个视频和11,423个事件片段，覆盖12大类180小类任务。通过让人类和前沿视觉-语言模型完成“事件片段重排序”任务，并引入文本描述，系统比较二者在各个维度推理能力上的表现。

Result: 结果显示，VLMs在视觉推理任务上显著落后于人类。虽然人为文本注释可提升模型准确率，但对人类无影响，显示模型严重依赖语言先验而非视觉理解。细分分析发现，模型在时间和因果推理要求高或日常任务上表现较好，但在需要上下文或空间推理及专业领域任务上表现较弱。

Conclusion: SPLICE展示了当前视觉-语言模型在多维事件推理方面的不足，特别强调了模型对语言信息的依赖以及视觉推理的持续挑战。该基准可促进未来相关模型的研究与改进。

Abstract: In this work, we introduce SPLICE, a human-curated benchmark derived from the
COIN instructional video dataset, designed to probe event-based reasoning
across multiple dimensions: temporal, causal, spatial, contextual, and general
knowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories
and 180 sub-categories, such as sports, engineering, and housework. These
videos are segmented into a total of 11,423 event clips. We evaluate both human
participants and state-of-the-art vision-language models (VLMs) on the task of
rearranging these clips into coherent event sequences to assess visual
reasoning capabilities. Results reveal a significant gap: VLMs struggle to
match human performance. While human-annotated textual descriptions improve
model accuracy, they do not affect human performance, suggesting that models
rely more on language priors than on visual understanding. Even with
annotations, VLMs fall short of human-level reasoning, underscoring persistent
challenges in visual reasoning. A deeper analysis across sub-categories shows
that VLMs perform relatively better on videos where temporal and causal
reasoning are dominant, compared to those where contextual and spatial
reasoning are dominant. They also perform better on everyday tasks than on
specialized ones.

</details>


### [285] [RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement](https://arxiv.org/abs/2509.24644)
*Zhu,Libo,Zhou,Zihan,Liu,Xiaoyang,Zhang,Weihang,Shi,Keyu,Fu,Yifan,Zhang,Yulun*

Main category: cs.CV

TL;DR: 本文针对数码设备屏幕拍摄中常见的闪烁条带（flicker-banding, FB）现象，提出了首个基于扩散模型的去除方法，并构建了真实对齐数据集。


<details>
  <summary>Details</summary>
Motivation: 随着拍摄屏幕内容逐渐普及，因相机滚动快门与显示器亮度调制时序失配而产生的条带影响画面质量，但相关研究极少，且与更常见的摩尔纹有本质区别。需要专门方法解决该影响图像可读性和美观度的问题。

Method: 1. 将FB去除建模为成像复原任务，提出基于扩散模型的RIFLE框架；2. 引入闪烁条带先验估计器（FPE），预测条带属性并引入复原神经网络；3. 设计了Masked Loss只监督带条带的图像区域；4. 构建合成管线，随机参数生成更真实的FB，并增加羽化边界和传感器噪声模拟；5. 建立实拍（长曝光对齐）带FB和无FB参考数据集。

Result: RIFLE在自建真实条带数据集上，对比最新图像复原基线方法，在轻微到重度条带问题下都取得了更好的指标和主观效果。

Conclusion: 本工作首次系统性提出FB的模拟及去除问题，为相关数据集和模型设计打下基础，对图像还原及屏幕内容拍摄相关研究具有启发意义，后续将开放数据集和代码以促进领域发展。

Abstract: Capturing screens is now routine in our everyday lives. But the photographs
of emissive displays are often influenced by the flicker-banding (FB), which is
alternating bright%u2013dark stripes that arise from temporal aliasing between
a camera's rolling-shutter readout and the display's brightness modulation.
Unlike moire degradation, which has been extensively studied, the FB remains
underexplored despite its frequent and severe impact on readability and
perceived quality. We formulate FB removal as a dedicated restoration task and
introduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement,
RIFLE, a diffusion-based framework designed to remove FB while preserving fine
details. We propose the flicker-banding prior estimator (FPE) that predicts key
banding attributes and injects it into the restoration network. Additionally,
Masked Loss (ML) is proposed to concentrate supervision on banded regions
without sacrificing global fidelity. To overcome data scarcity, we provide a
simulation pipeline that synthesizes FB in the luminance domain with stochastic
jitter in banding angle, banding spacing, and banding width. Feathered
boundaries and sensor noise are also applied for a more realistic simulation.
For evaluation, we collect a paired real-world FB dataset with pixel-aligned
banding-free references captured via long exposure. Across quantitative metrics
and visual comparisons on our real-world dataset, RIFLE consistently
outperforms recent image reconstruction baselines from mild to severe
flicker-banding. To the best of our knowledge, it is the first work to research
the simulation and removal of FB. Our work establishes a great foundation for
subsequent research in both the dataset construction and the removal model
design. Our dataset and code will be released soon.

</details>


### [286] [Learning Object-Centric Representations Based on Slots in Real World Scenarios](https://arxiv.org/abs/2509.24652)
*Adil Kaan Akan*

Main category: cs.CV

TL;DR: 本文提出了一种将预训练扩散模型适配为面向对象合成的新框架，实现了对图像和视频中离散对象的精细可控生成。该方法通过轻量级slot机制平衡全局场景一致性与对象级控制，显著提升了对象发现、分割、编辑与生成性能，并扩展至视频中的一致性对象建模和复杂编辑任务。


<details>
  <summary>Details</summary>
Motivation: 现有主流的扩散生成模型对图像整体处理，依赖文本条件，难以实现对象级别的可控生成和编辑。而实现对场景中离散对象的灵活组合与控制是AI在生成领域的重要目标，对提升创造性和交互性极为关键。

Method: 提出SlotAdapt框架，在预训练扩散模型中引入slot机制：用register token编码背景/风格，同时为对象分配独立slot条件模块，从而降低对文本条件的依赖，实现对象级可控生成。视频方面，结合Invariant Slot Attention分离对象标识与姿态，并用Transformer聚合时间信息，保持对象在多帧中的一致表示与运动建模。

Result: 在图像任务中实现了对象发现、分割、组合编辑和可控生成的最新效果。在视频任务中，无监督的视频对象分割与重建取得新基准，还能在无监督下实现对象移除、替换与插入等复杂编辑。

Conclusion: 本方法为图像与视频的面向对象生成建模提供了通用且可扩展的路径，有助于拉近AI与人的对象式认知，为创意、科学与实际应用领域的结构化、交互式生成工具扩展了设计空间。

Abstract: A central goal in AI is to represent scenes as compositions of discrete
objects, enabling fine-grained, controllable image and video generation. Yet
leading diffusion models treat images holistically and rely on text
conditioning, creating a mismatch for object-level editing. This thesis
introduces a framework that adapts powerful pretrained diffusion models for
object-centric synthesis while retaining their generative capacity.
  We identify a core challenge: balancing global scene coherence with
disentangled object control. Our method integrates lightweight, slot-based
conditioning into pretrained models, preserving their visual priors while
providing object-specific manipulation. For images, SlotAdapt augments
diffusion models with a register token for background/style and
slot-conditioned modules for objects, reducing text-conditioning bias and
achieving state-of-the-art results in object discovery, segmentation,
compositional editing, and controllable image generation.
  We further extend the framework to video. Using Invariant Slot Attention
(ISA) to separate object identity from pose and a Transformer-based temporal
aggregator, our approach maintains consistent object representations and
dynamics across frames. This yields new benchmarks in unsupervised video object
segmentation and reconstruction, and supports advanced editing tasks such as
object removal, replacement, and insertion without explicit supervision.
  Overall, this work establishes a general and scalable approach to
object-centric generative modeling for images and videos. By bridging human
object-based perception and machine learning, it expands the design space for
interactive, structured, and user-driven generative tools in creative,
scientific, and practical domains.

</details>


### [287] [VNODE: A Piecewise Continuous Volterra Neural Network](https://arxiv.org/abs/2509.24659)
*Siddharth Roheda,Aniruddha Bala,Rohit Chowdhury,Rohan Jaiswal*

Main category: cs.CV

TL;DR: 该论文提出了VNODE，一种结合了非线性Volterra滤波与神经常微分方程的混合神经网络模型，在图像分类任务上具有更优性能和较低的参数要求。


<details>
  <summary>Details</summary>
Motivation: 受到视觉皮层中离散事件处理和连续积分交替机制的启发，旨在设计一种既能高效提取复杂特征又能降低模型复杂度的新型神经网络结构。

Method: 提出了Volterra Neural ODE（VNODE）架构，模型通过在离散的Volterra特征提取和ODE驱动的连续状态演化之间交替，有效结合非线性卷积和常微分方程处理手段。

Result: 在CIFAR10与Imagenet1K等主流基准数据集上，VNODE表现优于当前主流模型，同时使用了更少的参数，计算复杂度更低。

Conclusion: VNODE实现了复杂模式的准确捕捉，提高了分类效果，证实了融合Volterra滤波和ODE的新架构在性能和效率上的优越性。

Abstract: This paper introduces Volterra Neural Ordinary Differential Equations
(VNODE), a piecewise continuous Volterra Neural Network that integrates
nonlinear Volterra filtering with continuous time neural ordinary differential
equations for image classification. Drawing inspiration from the visual cortex,
where discrete event processing is interleaved with continuous integration,
VNODE alternates between discrete Volterra feature extraction and ODE driven
state evolution. This hybrid formulation captures complex patterns while
requiring substantially fewer parameters than conventional deep architectures.
VNODE consistently outperforms state of the art models with improved
computational complexity as exemplified on benchmark datasets like CIFAR10 and
Imagenet1K.

</details>


### [288] [Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation](https://arxiv.org/abs/2509.24681)
*Hanyu Zhang,Yiming Zhou,Jinxia Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对开放词汇伪装物体分割（OVCamo）任务的分类器中心自适应框架，通过增强分类器有效提升分割性能，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 开放词汇伪装物体分割需要模型分割出训练时未见过类别的伪装物体，对模型的泛化能力提出了极高要求。然而现有方法中，分类器部分对分割性能影响较大且存在提升空间。

Method: 作者提出了一个以分类器为核心的自适应框架。通过引入一个轻量级文本适配器以及新颖的分层非对称初始化方法，强化了分类组件，从而提升了整体分割性能。

Result: 在OVCamo基准测试上，相比OVCoser基线，本文方法显著提升了各项分割指标：cIoU从0.443提升到0.493，cSm从0.579提升到0.658，cMAE从0.336降低到0.239。

Conclusion: 有针对性地提升分类器能力，可以有效推动开放词汇伪装物体分割性能，是改善相关任务的有效途径。

Abstract: Open-vocabulary camouflaged object segmentation requires models to segment
camouflaged objects of arbitrary categories unseen during training, placing
extremely high demands on generalization capabilities. Through analysis of
existing methods, it is observed that the classification component
significantly affects overall segmentation performance. Accordingly, a
classifier-centric adaptive framework is proposed to enhance segmentation
performance by improving the classification component via a lightweight text
adapter with a novel layered asymmetric initialization. Through the
classification enhancement, the proposed method achieves substantial
improvements in segmentation metrics compared to the OVCoser baseline on the
OVCamo benchmark: cIoU increases from 0.443 to 0.493, cSm from 0.579 to 0.658,
and cMAE reduces from 0.336 to 0.239. These results demonstrate that targeted
classification enhancement provides an effective approach for advancing
camouflaged object segmentation performance.

</details>


### [289] [Traumatic Brain Injury Segmentation using an Ensemble of Encoder-decoder Models](https://arxiv.org/abs/2509.24684)
*Ghanshyam Dhamat,Vaanathi Sundaresan*

Main category: cs.CV

TL;DR: 本论文提出了一种自动化流程，用于在T1加权MRI扫描中识别和分割中重度创伤性脑损伤（TBI）病变，获得了较高的准确率，并在AIMS-TBI 2025竞赛中取得前六名。


<details>
  <summary>Details</summary>
Motivation: TBI病变在形态和分布上具有高度异质性，增加了神经影像处理的难度，影响分析的准确性。因此，亟需开发高精度、自动化的病变分割方法。

Method: 作者设计了一个自动分割流程，以nnUNet架构为核心进行初步分割，并结合后处理策略提升结果。对多种方法进行了效果评估，并使用Python实现并开源该流程。

Result: 流程在挑战赛中整体Dice分数为0.5973，准确率为0.8451，分别在有病灶和无病灶影像中的Dice分数为0.4711和0.8514，排名前六。

Conclusion: 该自动化流程能有效提升TBI病变分割的准确性，可为神经影像分析提供有力工具，相关实现已开放获取。

Abstract: The identification and segmentation of moderate-severe traumatic brain injury
(TBI) lesions pose a significant challenge in neuroimaging. This difficulty
arises from the extreme heterogeneity of these lesions, which vary in size,
number, and laterality, thereby complicating downstream image processing tasks
such as image registration and brain parcellation, reducing the analytical
accuracy. Thus, developing methods for highly accurate segmentation of TBI
lesions is essential for reliable neuroimaging analysis. This study aims to
develop an effective automated segmentation pipeline to automatically detect
and segment TBI lesions in T1-weighted MRI scans. We evaluate multiple
approaches to achieve accurate segmentation of the TBI lesions. The core of our
pipeline leverages various architectures within the nnUNet framework for
initial segmentation, complemented by post-processing strategies to enhance
evaluation metrics. Our final submission to the challenge achieved an accuracy
of 0.8451, Dice score values of 0.4711 and 0.8514 for images with and without
visible lesions, respectively, with an overall Dice score of 0.5973, ranking
among the top-6 methods in the AIMS-TBI 2025 challenge. The Python
implementation of our pipeline is publicly available.

</details>


### [290] [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://arxiv.org/abs/2509.24695)
*Junsong Chen,Yuyang Zhao,Jincheng Yu,Ruihang Chu,Junyu Chen,Shuai Yang,Xianbang Wang,Yicheng Pan,Daquan Zhou,Huan Ling,Haozhe Liu,Hongwei Yi,Hao Zhang,Muyang Li,Yukang Chen,Han Cai,Sanja Fidler,Ping Luo,Song Han,Enze Xie*

Main category: cs.CV

TL;DR: SANA-Video是一种高效的小型扩散模型，可在低成本和高速度下生成高分辨率、长时长、高质量的视频，实现了优越的文本-视频对齐和极快的生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型往往成本高、速度慢、内存消耗大，难以实现高质量、长时长和高分辨率的视频生成。因此，亟需一种既高效又能保证生成质量的视频生成方法。

Method: 提出SANA-Video，利用线性注意力机制替换传统注意力，提升处理大量视频token的效率，并设计基于Block的常量内存KV缓存，保证长视频生成时的全局上下文，显著降低内存消耗，同时结合高效数据筛选和训练策略，缩短训练时间与成本。

Result: SANA-Video在仅1% MovieGen训练成本下实现性能接近当前SOTA的小型扩散模型，生成速度比同类先进模型快16倍。在RTX 5090 GPU上，将5秒720p视频生成时间从71秒缩短到29秒（2.4倍加速）。

Conclusion: SANA-Video在保持高分辨率与高质量的同时，大幅降低了视频生成的时间和计算成本，为低成本高效视频生成技术提供了切实可部署的解决方案。

Abstract: We introduce SANA-Video, a small diffusion model that can efficiently
generate videos up to 720x1280 resolution and minute-length duration.
SANA-Video synthesizes high-resolution, high-quality and long videos with
strong text-video alignment at a remarkably fast speed, deployable on RTX 5090
GPU. Two core designs ensure our efficient, effective and long video
generation: (1) Linear DiT: We leverage linear attention as the core operation,
which is more efficient than vanilla attention given the large number of tokens
processed in video generation. (2) Constant-Memory KV cache for Block Linear
Attention: we design block-wise autoregressive approach for long video
generation by employing a constant-memory state, derived from the cumulative
properties of linear attention. This KV cache provides the Linear DiT with
global context at a fixed memory cost, eliminating the need for a traditional
KV cache and enabling efficient, minute-long video generation. In addition, we
explore effective data filters and model training strategies, narrowing the
training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of
MovieGen. Given its low cost, SANA-Video achieves competitive performance
compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B
and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,
SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating
the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x
speedup). In summary, SANA-Video enables low-cost, high-quality video
generation.

</details>


### [291] [Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility](https://arxiv.org/abs/2509.24702)
*Yutong Hao,Chen Chen,Ajmal Saeed Mian,Chang Xu,Daochang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新框架，通过物理感知推理，有效提高扩散模型生成视频时的物理合理性，并提出了新颖的SDG指导方法，有效抑制物理解违的内容。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成视频时，往往隐式地从大规模文本-视频数据中学习物理规律，这种方法成本高、扩展性差且易生成违反物理规律的不合理运动。作者希望提高生成视频的物理合理性、降低成本、并避免额外训练。

Method: 1. 设计训练外（inference-time）的物理推理流程，用于识别和推理物理违背内容。2. 利用该流程生成反事实提示（counterfactual prompts），明确引导模型规避物理违背行为。3. 提出同步解耦指导（Synchronized Decoupled Guidance，SDG）策略，包括同步方向归一化和轨迹解耦去噪，保证去噪过程中始终抑制不合物理的生成内容。

Result: 在不同物理场景下进行实验，结果显示新方法显著提升物理逼真度，且保持照片真实感，无需额外训练。消融实验验证了物理推理组件和SDG方法协同下的有效性，SDG两个关键设计被证明对抑制不合理内容和提升物理合理性起到重要作用。

Conclusion: 本文框架无需训练，方法简单、效果显著，为未来物理感知视频生成提供了可直接应用的解决方案。

Abstract: Diffusion models can generate realistic videos, but existing methods rely on
implicitly learning physical reasoning from large-scale text-video datasets,
which is costly, difficult to scale, and still prone to producing implausible
motions that violate fundamental physical laws. We introduce a training-free
framework that improves physical plausibility at inference time by explicitly
reasoning about implausibility and guiding the generation away from it.
Specifically, we employ a lightweight physics-aware reasoning pipeline to
construct counterfactual prompts that deliberately encode physics-violating
behaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG)
strategy, which leverages these prompts through synchronized directional
normalization to counteract lagged suppression and trajectory-decoupled
denoising to mitigate cumulative trajectory bias, ensuring that implausible
content is suppressed immediately and consistently throughout denoising.
Experiments across different physical domains show that our approach
substantially enhances physical fidelity while maintaining photorealism,
despite requiring no additional training. Ablation studies confirm the
complementary effectiveness of both the physics-aware reasoning component and
SDG. In particular, the aforementioned two designs of SDG are also individually
validated to contribute critically to the suppression of implausible content
and the overall gains in physical plausibility. This establishes a new and
plug-and-play physics-aware paradigm for video generation.

</details>


### [292] [IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?](https://arxiv.org/abs/2509.24709)
*Yang Chen,Minghao Liu,Yufan Shen,Yunwen Li,Tianyuan Huang,Xinyu Fang,Tianyu Zheng,Wenxuan Huang,Cheng Yang,Daocheng Fu,Jianbiao Mei,Rong Wu,Licheng Wen,Xuemeng Yang,Song Mao,Qunshu Lin,Zhi Yu,Yongliang Shen,Yu Qiao,Botian Shi*

Main category: cs.CV

TL;DR: 该论文提出了IWR-Bench，这是一个用于评估大规模视觉语言模型（LVLMs）网页交互视频到代码生成能力的新基准。通过实验发现，现有LVLM在理解交互逻辑和生成功能性代码方面还有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前网页到代码的自动生成任务多基于静态截图，忽视了真实网页动态交互的重要性。对此，作者希望推动模型朝更真实、更复杂的网页交互理解和代码生成方向发展。

Method: 作者构建了IWR-Bench基准，包含来自100个真实网站的113个任务与1001个交互动作，涵盖不同交互复杂度、视觉风格与领域。每个任务包含用户交互视频与所有静态资产。评测要求模型从视频和素材中推理交互逻辑，并生成对应功能性代码。评测采用自动化agent-as-a-judge框架，综合评估模型的网页功能正确性和视觉还原度。

Result: 在对28个主流LVLM的测试中，功能正确性最高得分仅24.39%，而视觉还原度最高为64.25%，总体最高分也只有36.35%。功能正确性明显落后于视觉还原，暴露了现有模型在处理时序动态与事件驱动逻辑方面的显著短板。

Conclusion: IWR-Bench为视觉语言模型领域树立了新的、更高难度的基准，强调了在理解和生成网页交互逻辑方面的差距。论文呼吁未来研究关注提升LVLM对动态交互和功能性代码生成的能力。

Abstract: The webpage-to-code task requires models to understand visual representations
of webpages and generate corresponding code. However, existing benchmarks
primarily focus on static screenshot-to-code tasks, thereby overlooking the
dynamic interactions fundamental to real-world web applications. To address
this limitation, this paper introduces IWR-Bench, a novel benchmark for
evaluating the capabilities of Large Vision-Language Models (LVLMs) in
interactive webpage reconstruction from video. IWR-Bench comprises 113
meticulously curated tasks from 100 real-world websites, with 1,001 actions and
featuring diverse interaction complexities (e.g., web games), visual styles,
and domains. Aligning with standard web development practices, each task
includes not only user interaction videos but also all crawled static assets
(e.g., images, videos). This benchmark evaluates models on two fundamental
challenges: comprehensive multi-modal reasoning to infer interaction logic from
video and assets, and advanced code generation to translate this logic into
functional code. An agent-as-a-judge framework with a comprehensive metric
system automatically assesses the functional correctness and visual fidelity of
generated webpages. Extensive experiments on 28 LVLMs reveal a significant
challenge: the best model achieves an overall score of only 36.35%, as
functional correctness (24.39% IFS) lags significantly behind visual fidelity
(64.25% VFS). These results highlight critical limitations in current models'
ability to reason about temporal dynamics and synthesize event-driven logic,
establishing IWR-Bench as a challenging frontier for vision-language research.
The benchmark and evaluation code will be made publicly available. Code is
available at https://github.com/L-O-I/IWR-Bench.

</details>


### [293] [Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation](https://arxiv.org/abs/2509.24739)
*Huu Tien Nguyen,Dac Thai Nguyen,The Minh Duc Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Huy Hieu Pham,Johan Barthelemy,Minh Quan Tran,Thanh Tam Nguyen,Quoc Viet Hung Nguyen,Quynh Anh Chau,Hong Son Mai,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一个全新的越南语多模态医学影像数据集，包含156万对CT-PET图像和2757份完整临床报告，用于提升医疗视觉-语言基础模型（VLMs）在功能性影像任务和低资源语言下的表现。实验结果表明该数据集显著改善了VLMs在医学报告生成和视觉问答等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言基础模型在医学领域的应用受限，主要由于（1）现有训练数据缺乏涉及PET/CT等功能影像的多样化样本；（2）低资源语言（如越南语）临床数据极度稀缺，导致模型在这些场景下泛化能力差。本文旨在填补上述空白，提高AI医疗模型在越南等低资源地区的临床价值。

Method: 作者构建了一个包含1,567,062对CT-PET图像和2,757份越南语完整临床报告的大型数据集。随后提出了针对视觉-语言模型的训练与数据增强框架，并设立专家验证的测试集，系统评估主流VLMs在数据集上的表现，涵盖医学报告生成与视觉问答等下游任务。

Result: 实验表明，加入该越南语PET/CT多模态数据集后，主流VLMs在医学报告生成和视觉问答任务上的性能均明显提升。新构建的数据和训练框架有效增强了模型在低资源语言与功能影像处理方面的能力。

Conclusion: 本研究首次提供了覆盖越南语的PET/CT-报告配对医学数据，为医学影像AI模型发展和在低资源语言环境中的应用带来突破。该数据集和基准测试有望推动更强健的医疗VLMs问世，提高其在越南及类似环境中实际应用的临床相关性。

Abstract: Vision-Language Foundation Models (VLMs), trained on large-scale multimodal
datasets, have driven significant advances in Artificial Intelligence by
enabling rich cross-modal reasoning. Despite their success in general domains,
applying these models to medical imaging remains challenging due to the limited
availability of diverse imaging modalities and multilingual clinical data. Most
existing medical VLMs are trained on a subset of imaging modalities and focus
primarily on high-resource languages, thus limiting their generalizability and
clinical utility. To address these limitations, we introduce a novel
Vietnamese-language multimodal medical dataset comprising 1,567,062 paired
CT-PET images and corresponding 2,757 full-length clinical reports. This
dataset is designed to fill two pressing gaps in medical AI development: (1)
the lack of PET/CT imaging data in existing VLMs training corpora, which
hinders the development of models capable of handling functional imaging tasks;
and (2) the underrepresentation of low-resource languages, particularly the
Vietnamese language, in medical vision-language research. To the best of our
knowledge, this is the first dataset to provide comprehensive PET/CT-report
pairs in Vietnamese. We further introduce a training framework to enhance VLMs'
learning, including data augmentation and expert-validated test sets. We
conduct comprehensive experiments benchmarking state-of-the-art VLMs on
downstream tasks, including medical report generation and visual question
answering. The experimental results show that incorporating our dataset
significantly improves the performance of existing VLMs. We believe this
dataset and benchmark will serve as a pivotal step in advancing the development
of more robust VLMs for medical imaging, particularly in low-resource
languages, and improving their clinical relevance in Vietnamese healthcare.

</details>


### [294] [Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm](https://arxiv.org/abs/2509.24741)
*Xue-Feng Zhu,Tianyang Xu,Yifan Pan,Jinjie Gu,Xi Li,Jiwen Lu,Xiao-Jun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 该论文提出了一个新的三模态（RGB、深度、热红外）目标跟踪任务，推出了RGBDT500数据集和RDTTrack三模态跟踪器，在复杂场景下显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态跟踪方法大多仅限于双模态，面对复杂场景表现有限。为提升跟踪的鲁棒性，论文尝试将更多互补模态引入目标跟踪。

Method: 作者构建了包含500段视频、三模态对齐帧和精准边界框标注的RGBDT500数据集。提出了RDTTrack算法：用正交投影约束融合热红外和深度模态，作为提示输入给训练好的RGB跟踪模型，通过提示学习实现三模态融合强化跟踪。

Result: 实验结果显示，RDTTrack在跟踪精度和鲁棒性上明显优于以往的双模态方法，尤其是在复杂场景下优势明显。

Conclusion: 引入三模态信息和新数据集，有效提升了目标跟踪性能，为多模态跟踪研究开拓了新方向，方法具备较强应用潜力。

Abstract: Existing multi-modal object tracking approaches primarily focus on dual-modal
paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex
scenarios due to limited input modalities. To address this gap, this work
introduces a novel multi-modal tracking task that leverages three complementary
modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR),
aiming to enhance robustness in complex scenarios. To support this task, we
construct a new multi-modal tracking dataset, coined RGBDT500, which consists
of 500 videos with synchronised frames across the three modalities. Each frame
provides spatially aligned RGB, depth, and thermal infrared images with precise
object bounding box annotations. Furthermore, we propose a novel multi-modal
tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust
tracking by leveraging a pretrained RGB-only tracking model and prompt learning
techniques. In specific, RDTTrack fuses thermal infrared and depth modalities
under a proposed orthogonal projection constraint, then integrates them with
RGB signals as prompts for the pre-trained foundation tracking model,
effectively harmonising tri-modal complementary cues. The experimental results
demonstrate the effectiveness and advantages of the proposed method, showing
significant improvements over existing dual-modal approaches in terms of
tracking accuracy and robustness in complex scenarios.

</details>


### [295] [ExGS: Extreme 3D Gaussian Compression with Diffusion Priors](https://arxiv.org/abs/2509.24758)
*Jiaqi Chen,Xinhao Ji,Yuanyuan Gao,Hao Li,Yuning Gong,Yifei Liu,Dan Xu,Zhihang Zhong,Dingwen Zhang,Xiao Sun*

Main category: cs.CV

TL;DR: ExGS是一种极高效3D高斯Splatting模型压缩与还原方案，能将模型体积压缩百倍以上并保留高画质渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splating的神经场表示虽然呈现效果好，但存储和传输成本很高；传统压缩要么优化成本高且慢，要么无监督方法压缩比高则画质劣化，亟需新的高效压缩与高保真还原方法。

Method: ExGS提出两大模块：Universal Gaussian Compression（UGC）采用训练自由优化的裁剪策略，大幅减少高斯基元数量仅保留必要信息；GaussPainter利用轻量型VAE和一步扩散模型，通过掩码及扩散先验，快速重建和增强被裁剪后的低质图像，包括填充丢失区域并提升显著像素质量，实现实时高画质恢复。

Result: ExGS能将3DGS模型压缩上百倍（如354.77MB变为3.31MB），同时在高压缩率下仍大幅提升压缩后渲染画面质量，效果超越以往训练型或无训练方法。

Conclusion: ExGS展现了扩散先验在极限压缩与高质量神经渲染之间的桥梁作用，为3D场景神经渲染提供了高效、实用的新范式。

Abstract: Neural scene representations, such as 3D Gaussian Splatting (3DGS), have
enabled high-quality neural rendering; however, their large storage and
transmission costs hinder deployment in resource-constrained environments.
Existing compression methods either rely on costly optimization, which is slow
and scene-specific, or adopt training-free pruning and quantization, which
degrade rendering quality under high compression ratios. In contrast, recent
data-driven approaches provide a promising direction to overcome this
trade-off, enabling efficient compression while preserving high rendering
quality. We introduce \textbf{ExGS}, a novel feed-forward framework that
unifies \textbf{Universal Gaussian Compression} (UGC) with
\textbf{GaussPainter} for \textbf{Ex}treme 3D\textbf{GS} compression.
\textbf{UGC} performs re-optimization-free pruning to aggressively reduce
Gaussian primitives while retaining only essential information, whereas
\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided
refinement to restore high-quality renderings from heavily pruned Gaussian
scenes. Unlike conventional inpainting, GaussPainter not only fills in missing
regions but also enhances visible pixels, yielding substantial improvements in
degraded renderings. To ensure practicality, it adopts a lightweight VAE and a
one-step diffusion design, enabling real-time restoration. Our framework can
even achieve over $100\times$ compression (reducing a typical 354.77 MB model
to about 3.31 MB) while preserving fidelity and significantly improving image
quality under challenging conditions. These results highlight the central role
of diffusion priors in bridging the gap between extreme compression and
high-quality neural rendering. Our code repository will be released at
\href{https://github.com/chenttt2001/ExGS}{here}.

</details>


### [296] [VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding](https://arxiv.org/abs/2509.24776)
*Yizhuo Ding,Mingkang Chen,Zhibang Feng,Tong Xiao,Wanying Qu,Wenqi Shao,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文系统性分析了多模态大模型（MLLMs）在感知与推理结合方面的策略，并提出了一种双阶段的感知-推理分离框架VTPerception-R1，显著提升了多模态推理表现。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在推理时常常缺乏对感知证据的有效利用，限制了其推理准确性和鲁棒性。因而，作者亟需分析各种感知方式对推理影响，并设计更有效的感知与推理协同方法。

Method: 作者对四个多模态基准任务和两个MLLM进行了显式/隐式、视觉/文本四类感知策略的系统评测。基于发现，提出了VTPerception-R1：第一阶段引入感知增强微调，第二阶段结合视觉、文本与一致性奖励进行知觉感知的强化学习，以实现感知与推理的解耦和协同优化。

Result: 显式感知方式（特别是结合文本线索）能最显著提升模型表现，尤其对小模型提升更明显。所提出的VTPerception-R1在多类任务中有效提升了推理准确率与鲁棒性。

Conclusion: 将感知与推理解耦、显式引入感知过程，可大幅提升多模态推理能力和系统的可扩展性、可审计性，为后续多模态推理任务提供了方法论基础。

Abstract: Multimodal large language models (MLLMs) often struggle to ground reasoning
in perceptual evidence. We present a systematic study of perception
strategies-explicit, implicit, visual, and textual-across four multimodal
benchmarks and two MLLMs. Our findings show that explicit perception,
especially when paired with textual cues, consistently yields the best
improvements, particularly for smaller models. Based on this insight, we
propose VTPerception-R1, a unified two-stage framework that decouples
perception from reasoning. Stage 1 introduces perception-augmented fine-tuning,
and Stage 2 applies perception-aware reinforcement learning with novel visual,
textual, and consistency rewards. Experiments demonstrate that VTPerception-R1
significantly improves reasoning accuracy and robustness across diverse tasks,
offering a scalable and auditable solution for perception-grounded multimodal
reasoning. Our code is available at:
https://github.com/yizhuoDi/VTPerceprion-R1.

</details>


### [297] [SkyLink: Unifying Street-Satellite Geo-Localization via UAV-Mediated 3D Scene Alignment](https://arxiv.org/abs/2509.24783)
*Hongyang Zhang,Yinhao Liu,Zhenyu Kuang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SkyLink的新方法，通过多种技术提升跨视角地理定位的鲁棒性和准确率，显著提升了极端视角差异下的检索能力，并在挑战赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角地理定位方法在极端视角变化下，由于语义退化，难以实现准确的特征匹配。该研究旨在解决由视角巨大差异导致的语义信息损失，以提升城市环境中不同视角之间的鲁棒检索效果。

Method: 1）利用Google检索增强模块对街景图像进行数据增强，缓解街景视角受限导致的目标遮挡问题；2）提出Patch-Aware特征聚合模块，强化多局部特征聚集，实现跨视角下更一致的特征提取；3）结合多尺度无人机图像构建的3D场景信息作为街景与卫星图像之间的桥梁，通过自监督和跨视角对比学习进行特征对齐。

Result: 在University-1652数据集UAVM2025挑战赛中，SkyLink方法实现了25.75%的Recall@1准确率，展示了在多样城市场景下良好的鲁棒性和泛化能力。

Conclusion: SkyLink有效缓解了极端视角差异下的语义退化问题，通过多模块协同作用提升了跨视角地理定位的性能，在实际城市应用中具有较好的推广价值。

Abstract: Cross-view geo-localization aims at establishing location correspondences
between different viewpoints. Existing approaches typically learn cross-view
correlations through direct feature similarity matching, often overlooking
semantic degradation caused by extreme viewpoint disparities. To address this
unique problem, we focus on robust feature retrieval under viewpoint variation
and propose the novel SkyLink method. We firstly utilize the Google Retrieval
Enhancement Module to perform data enhancement on street images, which
mitigates the occlusion of the key target due to restricted street viewpoints.
The Patch-Aware Feature Aggregation module is further adopted to emphasize
multiple local feature aggregations to ensure the consistent feature extraction
across viewpoints. Meanwhile, we integrate the 3D scene information constructed
from multi-scale UAV images as a bridge between street and satellite
viewpoints, and perform feature alignment through self-supervised and
cross-view contrastive learning. Experimental results demonstrate robustness
and generalization across diverse urban scenarios, which achieve 25.75$\%$
Recall@1 accuracy on University-1652 in the UAVM2025 Challenge. Code will be
released at https://github.com/HRT00/CVGL-3D.

</details>


### [298] [LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning](https://arxiv.org/abs/2509.24786)
*Shenghao Fu,Qize Yang,Yuan-Ming Li,Xihan Wei,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的长视频理解模型LOVE-R1，通过自适应变焦机制解决了传统统一采样在时间长和空间细节间的权衡问题，显著提升了长视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型视频-语言模型在处理长视频时常常面临要么损失时序信息、要么丧失空间细节的问题，难以兼顾。作者希望通过新的采样和推理机制来提高长视频的理解效果，实现效率和精度之间的最佳平衡。

Method: LOVE-R1采用自适应“变焦”机制：首先以低分辨率密集采样所有帧，如果视频某一部分需要细节，模型可自主决策将其高分辨率重新采样，从而获得更多细节。这一过程采用多步推理，训练时利用38k高质量思维链(CoT)数据和分步奖励强化学习，将多步推理分解成多个单步训练，显性地训练‘变焦’能力。

Result: 在四个主流长视频理解基准上，LOVE-R1通过慢-快自适应帧采样机制表现突出。与Qwen2.5-VL基线相比，其综合性能平均提升了3.1百分点。

Conclusion: LOVE-R1模型能有效平衡采样密度和帧分辨率，极大提升长视频的理解效果，在现有LVLMs面临的长视频时空冲突难题上给出了创新性解决方案。

Abstract: Long video understanding is still challenging for recent Large Video-Language
Models (LVLMs) due to the conflict between long-form temporal understanding and
detailed spatial perception. LVLMs with a uniform frame sampling mechanism,
which samples frames with an equal frame size and fixed sampling rate,
inevitably sacrifice either temporal clues or spatial details, resulting in
suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model
that can adaptively zoom in on a video clip. The model is first provided with
densely sampled frames but in a small resolution. If some spatial details are
needed, the model can zoom in on a clip of interest with a large frame
resolution based on its reasoning until key visual information is obtained. The
whole process is implemented as a multi-step reasoning process. To train the
reasoning ability, we first finetune the model on our collected 38k
high-quality CoT data and enhance it with decoupled reinforcement finetuning.
As outcome rewards can not provide fine-grained process supervision, we
decouple multi-step reasoning into multiple single-step reasoning and optimize
the internal zoom-in ability explicitly. Experiments on long video
understanding benchmarks show that our model with the slow-fast adaptive frame
sampling mechanism achieves a great trade-off between sampling density and
frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an
average of 3.1% points across 4 common long video understanding benchmarks.

</details>


### [299] [Vision Function Layer in Multimodal LLMs](https://arxiv.org/abs/2509.24791)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文发现多模态大语言模型（MLLMs）中的视觉相关功能在不同的解码层中分布，并提出了一种新的分析方法（Visual Token Swapping）来精确定位各层的功能。此外，基于这些发现，提出的VFL-LoRA和VFL-select方法能在下游任务中提升模型效率和表现。


<details>
  <summary>Details</summary>
Motivation: 深入理解MLLMs的视觉处理机制，并提升模型在实际任务中的效率、可解释性和健壮性。通过剖析视觉功能在解码层的分布，希望指导更高效的模型训练与数据选择。

Method: 提出了Visual Token Swapping分析框架，通过定向修改KV cache项，揭示特定功能在模型层中的分布和作用。同时，基于视觉功能层（VFLs），在对应功能层进行选择性LoRA训练（VFL-LoRA），以及VFL-select自动筛选功能相关数据。

Result: 各类视觉功能如计数、定位、OCR识别等主要集中于模型特定2-3层中（VFL），且这些层在多种模型中顺序和深度表现出一致的人类类比模式。VFL-LoRA方法优于全文本LoRA训练，防止了越域功能遗忘。VFL-select数据筛选仅用20%的数据即可达到98%的效果，并超越人工筛选。

Conclusion: VFL分析为MLLMs视觉处理机制提供了更深层次的理解，并能指导更高效且可解释的模型优化和数据利用，推动实际应用性能提升。

Abstract: This study identifies that visual-related functional decoding is distributed
across different decoder layers in Multimodal Large Language Models (MLLMs).
Typically, each function, such as counting, grounding, or OCR recognition,
narrows down to two or three layers, which we define as Vision Function Layers
(VFL). Additionally, the depth and its order of different VFLs exhibits a
consistent pattern across different MLLMs, which is well-aligned with human
behaviors (e.g., recognition occurs first, followed by counting, and then
grounding). These findings are derived from Visual Token Swapping, our novel
analytical framework that modifies targeted KV cache entries to precisely
elucidate layer-specific functions during decoding. Furthermore, these insights
offer substantial utility in tailoring MLLMs for real-world downstream
applications. For instance, when LoRA training is selectively applied to VFLs
whose functions align with the training data, VFL-LoRA not only outperform
full-LoRA but also prevent out-of-domain function forgetting. Moreover, by
analyzing the performance differential on training data when particular VFLs
are ablated, VFL-select automatically classifies data by function, enabling
highly efficient data selection to directly bolster corresponding capabilities.
Consequently, VFL-select surpasses human experts in data selection, and
achieves 98% of full-data performance with only 20% of the original dataset.
This study delivers deeper comprehension of MLLM visual processing, fostering
the creation of more efficient, interpretable, and robust models.

</details>


### [300] [Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation](https://arxiv.org/abs/2509.24798)
*Lei Tong,Zhihua Liu,Chaochao Lu,Dino Oglic,Tom Diethe,Philip Teare,Sotirios A. Tsaftaris,Chen Jin*

Main category: cs.CV

TL;DR: Causal-Adapter 是用于定制文本到图像扩散模型，实现反事实图像生成的新型模块化框架，能在不改变图像主体身份的情况下，精确控制和干预图像属性。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过提示工程进行图像生成，缺乏显式的因果结构，不易实现对属性的精准和持续干预，因此需要一种结合因果建模的方法以提升反事实编辑的可控性和泛化性。

Method: 该方法引入结构化因果建模，结合两种属性正则化策略：prompt-aligned injection（对齐属性和文本嵌入，实现精确控制）与条件token对比损失（解耦属性并减少虚假相关性），以适配冻结的文本到图像扩散模型，用于反事实编辑。

Result: Causal-Adapter 在合成和真实数据集上取得了最优结果，包括Pendulum数据集上属性精度控制MAE降低91%，ADNI数据集上高保真MRI生成FID降低87%。

Conclusion: 该方法能够实现稳健且具泛化能力的反事实图像编辑，在准确修改属性的同时，出色地保持了图像主体身份的一致性。

Abstract: We present Causal-Adapter, a modular framework that adapts frozen
text-to-image diffusion backbones for counterfactual image generation. Our
method enables causal interventions on target attributes, consistently
propagating their effects to causal dependents without altering the core
identity of the image. In contrast to prior approaches that rely on prompt
engineering without explicit causal structure, Causal-Adapter leverages
structural causal modeling augmented with two attribute regularization
strategies: prompt-aligned injection, which aligns causal attributes with
textual embeddings for precise semantic control, and a conditioned token
contrastive loss to disentangle attribute factors and reduce spurious
correlations. Causal-Adapter achieves state-of-the-art performance on both
synthetic and real-world datasets, with up to 91\% MAE reduction on Pendulum
for accurate attribute control and 87\% FID reduction on ADNI for high-fidelity
MRI image generation. These results show that our approach enables robust,
generalizable counterfactual editing with faithful attribute modification and
strong identity preservation.

</details>


### [301] [TACO-Net: Topological Signatures Triumph in 3D Object Classification](https://arxiv.org/abs/2509.24802)
*Anirban Ghosh,Ayan Dutta*

Main category: cs.CV

TL;DR: 本文提出了一种新型的3D目标分类方法，将拓扑数据分析与多种图像滤波技术结合，通过体素化点云并提取拓扑特征，再用轻量级1D CNN分类，实现了在主流数据集上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来基于深度学习的点云3D物体分类取得进展，但由于点云的无序、不规则与噪声等特性，要实现高分类精度依然富有挑战。因此需要更有效、鲁棒的特征提取和分类新方法。

Method: 首先将每个点云转化为体素化的三维二值图像，然后利用拓扑数据分析结合多种图像滤波方法，提取区分物体的拓扑特征，最后用轻量级一维卷积神经网络（1D CNN）在特征集上进行分类。该方法被称为TACO-Net。

Result: TACO-Net在ModelNet40和ModelNet10这两个合成数据集上获得了99.05%和99.52%的准确率，在大型实际数据集OmniObject3D上也显示出较强鲁棒性。对十种不同扰动的ModelNet40进行测试，TACO-Net整体保持了很强的鲁棒性。

Conclusion: TACO-Net结合了拓扑分析与卷积神经网络，能够在点云分类任务中实现新的SOTA性能，并且在不同数据分布和干扰下表现出优异的鲁棒性。

Abstract: 3D object classification is a crucial problem due to its significant
practical relevance in many fields, including computer vision, robotics, and
autonomous driving. Although deep learning methods applied to point clouds
sampled on CAD models of the objects and/or captured by LiDAR or RGBD cameras
have achieved remarkable success in recent years, achieving high classification
accuracy remains a challenging problem due to the unordered point clouds and
their irregularity and noise. To this end, we propose a novel state-of-the-art
(SOTA) 3D object classification technique that combines topological data
analysis with various image filtration techniques to classify objects when they
are represented using point clouds. We transform every point cloud into a
voxelized binary 3D image to extract distinguishing topological features. Next,
we train a lightweight one-dimensional Convolutional Neural Network (1D CNN)
using the extracted feature set from the training dataset. Our framework,
TACO-Net, sets a new state-of-the-art by achieving $99.05\%$ and $99.52\%$
accuracy on the widely used synthetic benchmarks ModelNet40 and ModelNet10, and
further demonstrates its robustness on the large-scale real-world OmniObject3D
dataset. When tested with ten different kinds of corrupted ModelNet40 inputs,
the proposed TACO-Net demonstrates strong resiliency overall.

</details>


### [302] [UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections](https://arxiv.org/abs/2509.24817)
*Zeyu Cai,Ziyang Li,Xiaoben Li,Boqian Li,Zeyu Wang,Zhenyu Zhang,Yuliang Xiu*

Main category: cs.CV

TL;DR: UP2You是一种无需微调的高保真3D人物重建方法，可直接从极为自由、不规整的2D野外照片中恢复三维着装肖像，支持任意姿态、遮挡、裁剪条件下的输入，效率高、准确度强。


<details>
  <summary>Details</summary>
Motivation: 现有3D人物重建方法对输入要求高（如需全身、无遮挡、标准拍摄等），难以应用于真实场景的不规整照片。需要一种同时具备宽输入适应性的重建方法。

Method: 提出数据整流器框架，将混乱的原始输入单次推理快速转化为整齐多视图，简化3D重建流程。核心为姿态相关特征聚合模块（PCFA），可多参考图像融合实现身份保持、内存占用几乎不变。创新性地采用perceiver多参考形状预测器，无需模板人体。

Result: 在4D-Dress、PuzzleIOI等数据集，以及真实野外照片上大量实验证明，几何精度与纹理保真度均显著优于现有方法（如Chamfer距离提升15%，P2S提升18%，PSNR提升21%，LPIPS提升46%）。每个人只需1.5分钟重建速度。

Conclusion: UP2You开创了无需调参、支持复杂输入、可训练多个着装的高效3D人物重建新范式，兼具实际可用性与科研推动意义。代码及模型将开源，推动领域进展。

Abstract: We present UP2You, the first tuning-free solution for reconstructing
high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D
photos. Unlike previous approaches that require "clean" inputs (e.g., full-body
images with minimal occlusions, or well-calibrated cross-view captures), UP2You
directly processes raw, unstructured photographs, which may vary significantly
in pose, viewpoint, cropping, and occlusion. Instead of compressing data into
tokens for slow online text-to-3D optimization, we introduce a data rectifier
paradigm that efficiently converts unconstrained inputs into clean, orthogonal
multi-view images in a single forward pass within seconds, simplifying the 3D
reconstruction. Central to UP2You is a pose-correlated feature aggregation
module (PCFA), that selectively fuses information from multiple reference
images w.r.t. target poses, enabling better identity preservation and nearly
constant memory footprint, with more observations. We also introduce a
perceiver-based multi-reference shape predictor, removing the need for
pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and
in-the-wild captures demonstrate that UP2You consistently surpasses previous
methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and
texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5
minutes per person), and versatile (supports arbitrary pose control, and
training-free multi-garment 3D virtual try-on), making it practical for
real-world scenarios where humans are casually captured. Both models and code
will be released to facilitate future research on this underexplored task.
Project Page: https://zcai0612.github.io/UP2You

</details>


### [303] [Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models](https://arxiv.org/abs/2509.24837)
*Youngeun Kim,Youjia Zhang,Huiling Liu,Aecheon Jung,Sunwoo Lee,Sungeun Hong*

Main category: cs.CV

TL;DR: 本文提出了一种训练自由、计算高效的视觉-语言模型（VLM）Token剪枝方法，通过对投影层进行轻量级扰动，估算Token对模型输出的敏感度，能大幅减少推理时冗余Token，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM推理代价高，主要因为冗余视觉Token。Attention-或Diversity-based方法剪枝各有缺陷：前者依赖不稳定注意力分布，后者容易丢失重要区域。

Method: 方法基于直觉：敏感度高的Token对输出影响大，且应捕获互补信息。具体做法是不经训练而在投影层加小扰动，通过前向传播观察输出变化，估算每个Token的重要性，从而选择并保留最关键的部分。无须反向传播且计算高效。

Result: 在多个VLM和基准测试上，方法在保留准确率的同时，剪除了高达94.4%的Token，推理速度最高提升2.3倍，表现优于现有方法。

Conclusion: 该方法在效率和准确性之间取得很好的平衡，为视觉-语言模型的轻量级推理提供了新的解决思路。

Abstract: Large Vision-Language Models (VLMs) enable strong multimodal reasoning but
incur heavy inference costs from redundant visual tokens. Token pruning
alleviates this issue, yet existing approaches face limitations.
Attention-based methods rely on raw attention scores, which are often unstable
across layers and heads and can lead to redundant selections. Diversity-based
methods improve robustness by selecting tokens far apart in feature space but
risk dropping regions needed for accurate prediction. We propose \ours, a
training-free framework built on a simple intuition: tokens with higher
sensitivity are more likely to influence the model's output, and they should
also capture complementary visual cues rather than overlapping information. To
achieve this, we estimate token sensitivity using zeroth-order perturbations at
the projection layer, a shallow and computationally light component of the
model. This approach measures how small random perturbations affect the
projection outputs, allowing us to approximate each token's influence through
lightweight forward passes without backpropagation. Extensive experiments
across multiple VLMs and benchmarks show that \ours consistently outperforms
prior methods, pruning up to 94.4\% of tokens while maintaining accuracy and
significantly improving efficiency, achieving up to 2.30x faster end-to-end
inference over the baseline.

</details>


### [304] [PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement](https://arxiv.org/abs/2509.24850)
*Bo Zhao,Dan Guo,Junzhe Cao,Yong Xu,Tao Tan,Yue Sun,Bochao Zou,Jie Zhang,Zitong Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于物理理论的新型无接触式心率测量方法（rPPG），通过引入新网络PHASE-Net，有效在头部运动及光照变化下提升了测量的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前多数rPPG深度学习方法缺乏理论基础，且在头部运动和光照变化下准确性下降，导致鲁棒性和可解释性不足。因此，急需一种理论支持下的高鲁棒性rPPG方案。

Method: 作者基于血流动力学的Navier-Stokes方程，推导出脉搏信号是二阶动态系统，可以用因果卷积（如TCN）建模。在此基础上设计了PHASE-Net，包括：（1）零运算量的轴向通道交换模块用于混合面部远距离区域特征；（2）自适应空间滤波器按帧提取信号丰富区域；（3）门控TCN捕获长时间脉搏动态。

Result: 大量实验显示，PHASE-Net在准确性和效率上均优于现有方案，达到了当前最佳效果。

Conclusion: PHASE-Net基于物理理论，能高效、准确地进行rPPG测量，并已达可部署应用水平。

Abstract: Remote photoplethysmography (rPPG) measurement enables non-contact
physiological monitoring but suffers from accuracy degradation under head
motion and illumination changes. Existing deep learning methods are mostly
heuristic and lack theoretical grounding, which limits robustness and
interpretability. In this work, we propose a physics-informed rPPG paradigm
derived from the Navier-Stokes equations of hemodynamics, showing that the
pulse signal follows a second-order dynamical system whose discrete solution
naturally leads to a causal convolution. This provides a theoretical
justification for using a Temporal Convolutional Network (TCN). Based on this
principle, we design PHASE-Net, a lightweight model with three key components:
(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial
channels to mix distant facial regions and enhance cross-region feature
interaction without breaking temporal order; (2) Adaptive Spatial Filter, which
learns a soft spatial mask per frame to highlight signal-rich areas and
suppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models
long-range temporal dynamics for accurate pulse recovery. Extensive experiments
demonstrate that PHASE-Net achieves state-of-the-art performance with strong
efficiency, offering a theoretically grounded and deployment-ready rPPG
solution.

</details>


### [305] [ELPG-DTFS: Prior-Guided Adaptive Time-Frequency Graph Neural Network for EEG Depression Diagnosis](https://arxiv.org/abs/2509.24860)
*Jingru Qiu,Jiale Liang,Xuanhan Fan,Mingda Zhang,Zhenli He*

Main category: cs.CV

TL;DR: 作者提出了一种新型EEG分析深度模型ELPG-DTFS，用于更准确、可解释地筛查重度抑郁症（MDD），在主流数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 重度抑郁症（MDD）普遍且严重，准确及时诊断至关重要。目前诊断依赖主观量表，易受影响且主观性强。EEG作为廉价可得的生物标志物受到关注，但现有深度模型多视其为静态图像，固定通道关系且忽略先验知识，导致准确性和可解释性受限。

Method: 提出ELPG-DTFS模型：引入（1）通道-频带注意力机制结合跨频带互信息；（2）可学习的邻接矩阵实现动态脑区功能联系；（3）残差式神经科学先验图通路，提高解读性。利用128通道MODMA数据集（53例）进行实验。

Result: ELPG-DTFS在MODMA数据集上达到97.63%准确率和97.33% F1分数，优于当前最先进方法ACM-GNN。消融实验显示移除任一模块，都使F1最高下降4.35个百分点，表明各模块互为补充且必要。

Conclusion: ELPG-DTFS为基于EEG的MDD筛查提供了更高精度和更好可解释性的模型框架，具备实际临床应用潜力。

Abstract: Timely and objective screening of major depressive disorder (MDD) is vital,
yet diagnosis still relies on subjective scales. Electroencephalography (EEG)
provides a low-cost biomarker, but existing deep models treat spectra as static
images, fix inter-channel graphs, and ignore prior knowledge, limiting accuracy
and interpretability. We propose ELPG-DTFS, a prior-guided adaptive
time-frequency graph neural network that introduces: (1) channel-band attention
with cross-band mutual information, (2) a learnable adjacency matrix for
dynamic functional links, and (3) a residual knowledge-graph pathway injecting
neuroscience priors. On the 128-channel MODMA dataset (53 subjects), ELPG-DTFS
achieves 97.63% accuracy and 97.33% F1, surpassing the 2025 state-of-the-art
ACM-GNN. Ablation shows that removing any module lowers F1 by up to 4.35,
confirming their complementary value. ELPG-DTFS thus offers a robust and
interpretable framework for next-generation EEG-based MDD diagnostics.

</details>


### [306] [Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations](https://arxiv.org/abs/2509.24863)
*Lorena Stracke,Lia Nimmermann,Shashank Agnihotri,Margret Keuper,Volker Blanz*

Main category: cs.CV

TL;DR: 本文提出一种受人眼视觉机制启发的预处理方法，将Difference-of-Gaussians（DoG）滤波应用于输入图像，以增强局部对比度，提升语义分割模型在恶劣环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人视觉系统通过对比增强和颜色对抗机制来适应多样视觉环境。随着视觉模型应用在自动驾驶等安全关键场景，对模型在夜间、雾、雪等复杂环境下的鲁棒性提出更高要求。文中欲探索无须更改模型结构，仅通过输入预处理提升模型鲁棒性的可能性。

Method: 在输入图像的RGB、灰度和对抗性色彩通道上应用Difference-of-Gaussians（DoG）滤波进行对比度增强，然后直接输入现有语义分割模型，无需模型结构或训练的任何改动。

Result: 在Cityscapes、ACDC和Dark Zurich等数据集上的实验证明，该预处理方法在保持常规条件性能的同时，提升了模型在夜间、雾天、下雪等恶劣条件下的鲁棒性。

Conclusion: 该基于生物启发、与模型无关、计算开销小的预处理方法能集成进视觉系统成像流程，为自动驾驶等安全关键场景提供更可靠的输入，提高下游视觉模型的实际应用能力。

Abstract: Inspired by the human visual system's mechanisms for contrast enhancement and
color-opponency, we explore biologically motivated input preprocessing for
robust semantic segmentation. By applying Difference-of-Gaussians (DoG)
filtering to RGB, grayscale, and opponent-color channels, we enhance local
contrast without modifying model architecture or training. Evaluations on
Cityscapes, ACDC, and Dark Zurich show that such preprocessing maintains
in-distribution performance while improving robustness to adverse conditions
like night, fog, and snow. As this processing is model-agnostic and
lightweight, it holds potential for integration into imaging pipelines,
enabling imaging systems to deliver task-ready, robust inputs for downstream
vision models in safety-critical environments.

</details>


### [307] [StreamForest: Efficient Online Video Understanding with Persistent Event Memory](https://arxiv.org/abs/2509.24871)
*Xiangyu Zeng,Kefan Qiu,Qingyu Zhang,Xinhao Li,Jing Wang,Jiaxin Li,Ziang Yan,Kun Tian,Meng Tian,Xinhai Zhao,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: StreamForest提出了一种面向流式视频理解的多模态大语言模型架构，通过创新记忆机制和特定调优数据集，在多项基准任务上取得了SOTA表现，特别是在实时和高压缩率环境下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在处理流式视频时，因历史视觉信息存储受限、空间-时间推理能力不足，难以高效理解持续实时视频，尤其是在资源受限的实际应用场景下，存在显著瓶颈。

Method: StreamForest采用核心创新的Persistent Event Memory Forest机制，把视频帧以事件为单位自适应地组织为多棵树结构，通过时间距离、内容相似度与合并频率的惩罚函数优化长期记忆管理；配合细粒度时空窗口提升短时流的感知。团队还提出了针对流式视频的OnlineIT调优数据集，并构建了自动驾驶场景专用的ODV-Bench评测基准。

Result: 实验显示，StreamForest在StreamingBench、OVBench、OVO-Bench等标准基准上分别取得77.3%、60.5%、55.6%的准确率。即使视觉token极度压缩（仅1024），平均准确率也能保留96.8%。

Conclusion: StreamForest在流式视频理解领域表现出高度的鲁棒性、效率与泛化性，尤其适用于资源受限下的实时视频应用，如自动驾驶等场景。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable
progress in video understanding. However, their effectiveness in real-time
streaming scenarios remains limited due to storage constraints of historical
visual features and insufficient real-time spatiotemporal reasoning. To address
these challenges, we propose StreamForest, a novel architecture specifically
designed for streaming video understanding. Central to StreamForest is the
Persistent Event Memory Forest, a memory mechanism that adaptively organizes
video frames into multiple event-level tree structures. This process is guided
by penalty functions based on temporal distance, content similarity, and merge
frequency, enabling efficient long-term memory retention under limited
computational resources. To enhance real-time perception, we introduce a
Fine-grained Spatiotemporal Window, which captures detailed short-term visual
cues to improve current scene perception. Additionally, we present OnlineIT, an
instruction-tuning dataset tailored for streaming video tasks. OnlineIT
significantly boosts MLLM performance in both real-time perception and future
prediction. To evaluate generalization in practical applications, we introduce
ODV-Bench, a new benchmark focused on real-time streaming video understanding
in autonomous driving scenarios. Experimental results demonstrate that
StreamForest achieves the state-of-the-art performance, with accuracies of
77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In
particular, even under extreme visual token compression (limited to 1024
tokens), the model retains 96.8% of its average accuracy in eight benchmarks
relative to the default setting. These results underscore the robustness,
efficiency, and generalizability of StreamForest for streaming video
understanding.

</details>


### [308] [Environment-Aware Satellite Image Generation with Diffusion Models](https://arxiv.org/abs/2509.24875)
*Nikos Kostagiolas,Pantelis Georgiades,Yannis Panagakis,Mihalis A. Nicolaou*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于扩散模型的大模型方法，能够融合文本、元数据、和视觉信息等多种控制信号进行遥感卫星图像合成，尤其首次将动态环境上下文引入控制流程，并且支持对缺失或损坏的数据具有更强的鲁棒性与响应性，实验在图像质量、准确性等多个指标上全面超越以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的生成模型已在影像生成领域表现优异，但在遥感领域的应用还面临三个问题：1）对环境上下文信息利用有限；2）对缺失或损坏数据不够鲁棒；3）难以准确体现用户意图。为充分挖掘多模态公开数据的价值并提升生成质量，亟需突破上述局限。

Method: 本文提出一种能够同时结合文本、元数据、视觉数据三类控制信号的新型遥感影像扩散生成方法。方法创新点包括：（1）首次将动态环境上下文作为控制信号引入卫星图像生成扩散模型；（2）设计了元数据融合策略，对特征嵌入进行交互建模，从而有效处理部分损坏或缺失观测。

Result: 本方法在单幅图像生成和时序生成的实验场景中，定性评测表现出对缺失元数据的鲁棒性提升、对控制信号响应增强，定量评测在六个图像质量与准确性指标上均显著优于过往方法。

Conclusion: 实验表明，将环境上下文纳入控制信号显著提升了卫星遥感扩散大模型的表现，所提出模型在下游任务中具有较强应用潜力。所构建的三模态（文本、元数据、视觉）公开数据集为该方向首创，为后续相关研究提供了基础。

Abstract: Diffusion-based foundation models have recently garnered much attention in
the field of generative modeling due to their ability to generate images of
high quality and fidelity. Although not straightforward, their recent
application to the field of remote sensing signaled the first successful trials
towards harnessing the large volume of publicly available datasets containing
multimodal information. Despite their success, existing methods face
considerable limitations: they rely on limited environmental context, struggle
with missing or corrupted data, and often fail to reliably reflect user
intentions in generated outputs. In this work, we propose a novel diffusion
model conditioned on environmental context, that is able to generate satellite
images by conditioning from any combination of three different control signals:
a) text, b) metadata, and c) visual data. In contrast to previous works, the
proposed method is i) to our knowledge, the first of its kind to condition
satellite image generation on dynamic environmental conditions as part of its
control signals, and ii) incorporating a metadata fusion strategy that models
attribute embedding interactions to account for partially corrupt and/or
missing observations. Our method outperforms previous methods both
qualitatively (robustness to missing metadata, higher responsiveness to control
inputs) and quantitatively (higher fidelity, accuracy, and quality of
generations measured using 6 different metrics) in the trials of single-image
and temporal generation. The reported results support our hypothesis that
conditioning on environmental context can improve the performance of foundation
models for satellite imagery, and render our model a promising candidate for
usage in downstream tasks. The collected 3-modal dataset is to our knowledge,
the first publicly-available dataset to combine data from these three different
mediums.

</details>


### [309] [Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs](https://arxiv.org/abs/2509.24880)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文针对在交通工具识别任务中类别极度不平衡的问题，提出通过数据增强和平衡处理提升对少数类的识别，综合实验对比传统集成方法与深度学习模型，深度模型表现更优，但极少类仍难以识别。


<details>
  <summary>Details</summary>
Motivation: 车辆类型识别在智能交通和物流领域中非常重要。然而，公开数据集中类别不均衡，尤其是稀有类别样本过少，严重影响整体识别性能。作者希望通过改进数据集和方法提升少数类别的识别效果。

Method: 作者收集并整合了Kaggle、ImageNet及网络爬取数据，构建了16类、约4.7万张图片的数据集，并利用SMOTE过采样和有针对性的欠采样获得了6个平衡版本。对比了以MobileNet-V2特征为基础的轻量集成学习（如RF、AdaBoost和Soft Voting）与经数据增强和标签平滑训练的可配置ResNet型CNN。

Result: SMOTE下的最优集成模型在测试集上达到74.8%准确率，深度CNN模型在完整测试集和新推理数据上分别获得79.19%和81.25%的准确率，显示出深度模型对于多类别交通工具识别的优势。但“Barge”类（极少数类）识别仍表现较差。

Conclusion: 仅靠数据重采样并不能完全解决少数类难识别的问题，应加强少数类样本收集、采用如focal loss等损失函数、并探索结合集成学习与CNN的混合方法，以兼顾模型可解释性与强大特征表达能力。

Abstract: Accurate vehicle type recognition underpins intelligent transportation and
logistics, but severe class imbalance in public datasets suppresses performance
on rare categories. We curate a 16-class corpus (~47k images) by merging
Kaggle, ImageNet, and web-crawled data, and create six balanced variants via
SMOTE oversampling and targeted undersampling. Lightweight ensembles, such as
Random Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2
features are benchmarked against a configurable ResNet-style CNN trained with
strong augmentation and label smoothing. The best ensemble (SMOTE-combined)
attains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set
and 81.25% on an unseen inference batch, confirming the advantage of deep
models. Nonetheless, the most under-represented class (Barge) remains a failure
mode, highlighting the limits of rebalancing alone. Results suggest
prioritizing additional minority-class collection and cost-sensitive objectives
(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine
interpretability with representational power.

</details>


### [310] [VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines](https://arxiv.org/abs/2509.24891)
*Mostafa Mohaimen Akand Faisal,Rabeya Amin Jhuma*

Main category: cs.CV

TL;DR: 本文提出了一种针对生成模型（如GANs和扩散模型）的全新攻击方法，可通过对输入添加极其隐蔽的扰动实现对输出图像的精准操控，同时保持甚至提升生成结果的视觉质量，揭示了现有像素级防御机制的盲点。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本攻击多针对判别模型，对生成模型（如GAN和扩散模型）的攻击鲜有涉及，尤其是如何通过微小扰动隐蔽地操控生成输出尚属研究空白。这一领域的安全性和可信度需进一步探讨。

Method: 作者提出VagueGAN攻击流程，结合同模块化扰动网络PoisonerNet与生成-判别器的联合架构，设计用于制造对输入的隐蔽扰动（trigger），实现对生成图像的目标化操控。通过自定义代理指标评估攻击效果，并结合视觉与频域分析测试扰动的隐蔽性。还将方法迁移到基于ControlNet的扩散模型验证泛化性。

Result: 实验证明，该方法能在保持高度隐蔽的前提下，实现对GAN和扩散模型输出的目标化操控，甚至部分中毒输出在视觉上优于正常生成结果。此外，扰动在GAN和扩散流程中的迁移表现良好，验证了方法的广泛适用性。

Conclusion: 生成模型存在通过对输入施加隐蔽扰动实现输出精准操控的安全隐患，且此类‘潜空间’投毒方法能够保持甚至提升生成质量，对以像素为防御基准的传统机制构成威胁。该工作强调需关注生成模型的新型攻击手段与防御方案的研究。

Abstract: Generative models such as GANs and diffusion models are widely used to
synthesize photorealistic images and to support downstream creative and editing
tasks. While adversarial attacks on discriminative models are well studied,
attacks targeting generative pipelines where small, stealthy perturbations in
inputs lead to controlled changes in outputs are less explored. This study
introduces VagueGAN, an attack pipeline combining a modular perturbation
network PoisonerNet with a Generator Discriminator pair to craft stealthy
triggers that cause targeted changes in generated images. Attack efficacy is
evaluated using a custom proxy metric, while stealth is analyzed through
perceptual and frequency domain measures. The transferability of the method to
a modern diffusion based pipeline is further examined through ControlNet guided
editing. Interestingly, the experiments show that poisoned outputs can display
higher visual quality compared to clean counterparts, challenging the
assumption that poisoning necessarily reduces fidelity. Unlike conventional
pixel level perturbations, latent space poisoning in GANs and diffusion
pipelines can retain or even enhance output aesthetics, exposing a blind spot
in pixel level defenses. Moreover, carefully optimized perturbations can
produce consistent, stealthy effects on generator outputs while remaining
visually inconspicuous, raising concerns for the integrity of image generation
pipelines.

</details>


### [311] [DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping](https://arxiv.org/abs/2509.24893)
*Yu Ma,Guoliang Wei,Yue Cheng*

Main category: cs.CV

TL;DR: 本文提出DWGS方法，通过引入深度估计、虚拟视角合成和遮挡恢复等模块，使3D高斯Splatting在稀疏视角下合成新视图时，效果更高质量、更真实，并能保持实时渲染能力。实验结果在多个标准数据集上实现了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏视角的新视角合成方法，容易出现过拟合、几何失真、场景不完整等问题。3D高斯Splatting方法虽能高保真/实时渲染，但在稀疏输入下易产生伪影和结构不一致。这些问题限制了其实际应用。

Method: DWGS整体框架包括三大创新：(1) 深度估计模块结合多种损失增强多视角一致性；(2) 双向变形的虚拟视角生成，生成更多训练视角加强约束；(3) 基于遮挡感知与修复模型，恢复被遮挡区域。各模块协同提升在稀疏输入下的渲染质量。

Result: 在LLFF、Blender、DTU等常用数据集上DWGS取得了SOTA表现，PSNR高达21.13 dB，LPIPS为0.189，并且保持实时推理效率。

Conclusion: DWGS显著提升了3D高斯Splatting在稀疏视角下的新视角合成质量，消除浮动伪影/结构不一致等问题，能高效实时合成高质量新视图，为3D重建与虚拟现实等应用拓展了适用性。

Abstract: Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D
reconstruction, typically suffering from overfitting, geometric distortion, and
incomplete scene recovery due to limited multi-view constraints. Although 3D
Gaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it
suffers from floating artifacts and structural inconsistencies under
sparse-input settings. To address these issues, we propose DWGS, a novel
unified framework that enhances 3DGS for sparse-view synthesis by integrating
robust structural cues, virtual view constraints, and occluded region
completion. Our approach introduces three principal contributions: a
Hybrid-Loss Depth Estimation module that leverages dense matching priors with
reprojection, point propagation, and smoothness constraints to enforce
multi-view consistency; a Bidirectional Warping Virtual View Synthesis method
generates virtual training views to impose stronger geometric and photometric
constraints; and an Occlusion-Aware Reconstruction component that utilizes
depth-difference mask and a learning-based inpainting model to recover obscured
regions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)
show that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR
and 0.189 LPIPS, while retaining real-time inference capabilities.

</details>


### [312] [DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation](https://arxiv.org/abs/2509.24896)
*Xi Chen,Hongxun Yao,Zhaopan Xu,Kui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无源主动域适应（SFADA）方法——DAM框架，通过集成视觉-语言多模态模型的监督信号与有限人工标注，显著提升了模型的跨域迁移性能，在多个基准上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前SFADA方法在利用视觉-语言（ViL）模型提升伪标签或特征对齐时，往往只将ViL模型监督和人类标注分别处理，未有效融合两类监督。为充分利用多模态知识与有限人工信息，实现更强跨域适应性，需开发新的融合策略。

Method: 提出Dual Active learning with Multimodal (DAM)框架，将ViL大模型生成的多模态监督与稀疏人工标注信息融合，形成双重监督信号。通过启用ViL引导的目标并采用双向蒸馏机制，实现目标模型与双重监督信号的双向知识传递和迭代自适应。

Result: 在多个SFADA基准数据集和多种主动学习策略下，DAM框架表现优异，性能显著超过现有方法，取得了最新的最优结果。

Conclusion: DAM方法有效融合多模态大模型与人工标注的监督信号，提升了无源主动域适应任务的迁移性能，为该领域提供了一种具有广泛适用性的高效框架。

Abstract: Source-free active domain adaptation (SFADA) enhances knowledge transfer from
a source model to an unlabeled target domain using limited manual labels
selected via active learning. While recent domain adaptation studies have
introduced Vision-and-Language (ViL) models to improve pseudo-label quality or
feature alignment, they often treat ViL-based and data supervision as separate
sources, lacking effective fusion. To overcome this limitation, we propose Dual
Active learning with Multimodal (DAM) foundation model, a novel framework that
integrates multimodal supervision from a ViL model to complement sparse human
annotations, thereby forming a dual supervisory signal. DAM initializes stable
ViL-guided targets and employs a bidirectional distillation mechanism to foster
mutual knowledge exchange between the target model and the dual supervisions
during iterative adaptation. Extensive experiments demonstrate that DAM
consistently outperforms existing methods and sets a new state-of-the-art
across multiple SFADA benchmarks and active learning strategies.

</details>


### [313] [Accurate Cobb Angle Estimation via SVD-Based Curve Detection and Vertebral Wedging Quantification](https://arxiv.org/abs/2509.24898)
*Chang Shi,Nan Meng,Yipeng Zhuang,Moxin Zhao,Jason Pui Yin Cheung,Hua Huang,Xiuyuan Chen,Cong Nie,Wenting Zhong,Guiqiang Jiang,Yuxin Wei,Jacob Hong Man Yu,Si Chen,Xiaowen Ou,Teng Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新型深度学习框架用于青少年特发性脊柱侧弯（AIS）评估，不仅准确预测椎体端板角度，还引入了新的椎体变形指标VWI，有效提升了诊断与进展监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Cobb角AIS评估手工测量主观性强，自动化方法又过度简化脊柱结构，难以应对复杂临床情况。作者意在开发更精确、能适应多样病理模式的自动化评估工具。

Method: 采用融合HRNet主干网络与Swin-Transformer模块的深度模型，结合生物力学约束，从630例AIS全脊柱X射线正位片双标注中训练。利用SVD直接分析椎体角度，对脊柱模式无预设假设。并提出椎体变形新指标VWI。

Result: 该深度学习方法诊断准确率达83.45%，角度平均绝对误差仅2.55度，对分布外数据亦表现出良好泛化能力。VWI与侧弯进展显著相关，而传统Cobb角无此相关性。

Conclusion: 该方法能更准确自动地评估AIS严重程度及预后，VWI指标为早期检测与个体化治疗提供新参考，可望用作AIS监测与分型新标准。

Abstract: Adolescent idiopathic scoliosis (AIS) is a common spinal deformity affecting
approximately 2.2% of boys and 4.8% of girls worldwide. The Cobb angle serves
as the gold standard for AIS severity assessment, yet traditional manual
measurements suffer from significant observer variability, compromising
diagnostic accuracy. Despite prior automation attempts, existing methods use
simplified spinal models and predetermined curve patterns that fail to address
clinical complexity. We present a novel deep learning framework for AIS
assessment that simultaneously predicts both superior and inferior endplate
angles with corresponding midpoint coordinates for each vertebra, preserving
the anatomical reality of vertebral wedging in progressive AIS. Our approach
combines an HRNet backbone with Swin-Transformer modules and biomechanically
informed constraints for enhanced feature extraction. We employ Singular Value
Decomposition (SVD) to analyze angle predictions directly from vertebral
morphology, enabling flexible detection of diverse scoliosis patterns without
predefined curve assumptions. Using 630 full-spine anteroposterior radiographs
from patients aged 10-18 years with rigorous dual-rater annotation, our method
achieved 83.45% diagnostic accuracy and 2.55{\deg} mean absolute error. The
framework demonstrates exceptional generalization capability on
out-of-distribution cases. Additionally, we introduce the Vertebral Wedging
Index (VWI), a novel metric quantifying vertebral deformation. Longitudinal
analysis revealed VWI's significant prognostic correlation with curve
progression while traditional Cobb angles showed no correlation, providing
robust support for early AIS detection, personalized treatment planning, and
progression monitoring.

</details>


### [314] [Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer](https://arxiv.org/abs/2509.24899)
*Mohsen Ghafoorian,Denis Korzhenkov,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了Attention Surgery方法，通过对已有的视频扩散模型（VDM）进行注意力机制线性化与混合，显著降低了计算代价，并有效保持了高生成质量。


<details>
  <summary>Details</summary>
Motivation: Transformer型视频扩散模型在生成高质量视频方面表现优异，但自注意力机制的二次复杂度导致长序列和高分辨率视频的计算成本极高，亟需更高效的注意力方法。

Method: 提出Attention Surgery框架，结合创新的混合（softmax与linear token）注意力机制，并引入轻量级的蒸馏和微调流程，对未经重新训练的预训练模型高效施加线性或混合注意力。还采用了一个成本感知的block-rate策略，层间灵活地平衡表达能力与效率。

Result: 在最先进的DiT基础VDM Wan2.1 1.3B上应用所提方法，实现了首个具有竞争力的子二次复杂度视频扩散模型，FLOPs降低至原本的60%，同时在VBench及VBench-2.0基准上保持了较高的生成质量。

Conclusion: Attention Surgery为现有高性能视频扩散模型显著减负，兼顾效率和表达能力，为高效视频生成提供了新思路，无需完全重训即可轻松落地并推广。

Abstract: Transformer-based video diffusion models (VDMs) deliver state-of-the-art
video generation quality but are constrained by the quadratic cost of
self-attention, making long sequences and high resolutions computationally
expensive. While linear attention offers sub-quadratic complexity, prior
attempts fail to match the expressiveness of softmax attention without costly
retraining. We introduce \textit{Attention Surgery}, an efficient framework for
\textit{linearizing} or \textit{hybridizing} attention in pretrained VDMs
without training from scratch. Inspired by recent advances in language models,
our method combines a novel hybrid attention mechanism-mixing softmax and
linear tokens-with a lightweight distillation and fine-tuning pipeline
requiring only a few GPU-days. Additionally, we incorporate a cost-aware
block-rate strategy to balance expressiveness and efficiency across layers.
Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery
achieves the first competitive sub-quadratic attention video diffusion models,
reducing attention cost by up to 40\% in terms of FLOPs, while maintaining
generation quality as measured on the standard VBench and VBench-2.0
benchmarks.

</details>


### [315] [OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing](https://arxiv.org/abs/2509.24900)
*Zhihong Chen,Xuehai Bai,Yang Shi,Chaoyou Fu,Huanyu Zhang,Haotian Wang,Xiaoyan Sun,Zhang Zhang,Liang Wang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang*

Main category: cs.CV

TL;DR: 本文提出了OpenGPT-4o-Image，一个大规模、多样化的用于多模态图像生成与编辑的数据集，为模型提升实际应用能力提供了关键支撑。通过系统化的数据设计和自动化生成，显著提升了主流模型在多个基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图像生成与编辑模型受限于训练数据的质量和全面性，现有数据集缺乏系统性和复杂场景，难以满足实际应用需求。

Method: 提出一种结合分层任务分类和自动化数据生成的新方法，构建涵盖11大领域、51项子任务的高质量数据集，通过自动化流程和GPT-4o生成8万组指令图像对，确保数据多样性和挑战性。

Result: 在多个权威基准上微调主流模型，图像编辑任务性能提升高达18%，生成任务提升达13%。

Conclusion: 系统化、结构化的数据集构建方法可显著推动多模态AI模型能力提升，被验证为推动实际场景应用的关键。

Abstract: The performance of unified multimodal models for image generation and editing
is fundamentally constrained by the quality and comprehensiveness of their
training data. While existing datasets have covered basic tasks like style
transfer and simple object manipulation, they often lack the systematic
structure and challenging scenarios required for real-world applications. To
address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset
constructed using a novel methodology that combines hierarchical task taxonomy
with automated data generation. Our taxonomy not only includes fundamental
capabilities such as text rendering and style control but also introduces
highly practical yet challenging categories like scientific imagery for
chemistry illustrations and complex instruction editing requiring simultaneous
execution of multiple operations. Through an automated pipeline leveraging
structured resource pools and GPT-4o, we generate 80k high-quality
instruction-image pairs with controlled diversity, covering 11 major domains
and 51 subtasks. Extensive experiments show that fine-tuning leading models on
our dataset achieves significant performance gains across multiple benchmarks,
with improvements of up to 18\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)
and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that
systematic data construction is key to advancing multimodal AI capabilities.

</details>


### [316] [Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale](https://arxiv.org/abs/2509.24910)
*Songze Li,Zun Wang,Gengze Zhou,Jialu Li,Xiangyu Zeng,Limin Wang,Yu Qiao,Qi Wu,Mohit Bansal,Yi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种通过自我改进演示（SID）的目标导向语言导航方法，有效提升了导航智能体在未知环境中的探索能力，并取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的导航方法主要依赖最短路径轨迹，缺乏有效的探索先验，导致导航智能体在新环境下泛化能力和探索能力受限。

Method: SID方法首先在最短路径演示数据上对智能体进行初始训练，然后利用该智能体在环境中生成新的探索轨迹。这些新轨迹被采集为更具探索策略的演示，用于训练更强的智能体，并不断迭代，形成自我提升的训练流程。

Result: SID方法可扩展到新环境，并能将获得的演示迁移到不同的语言导航任务。实验表明，SID显著提升了智能体的探索能力和泛化能力，并在REVERIE和SOON等任务上取得了新SOTA性能，在SOON's unseen验证集上的成功率达到50.9%，比以往领先方法高出13.9%。

Conclusion: SID通过自我改进和探索轨迹的持续提升，显著提高了基于语言的目标导航任务中的智能体性能，突破了现有方法的性能上限，具备良好的泛化和迁移能力。

Abstract: Goal-oriented language-guided navigation requires robust exploration
capabilities for agents to navigate to specified goals in unknown environments
without step-by-step instructions. Existing methods tend to exclusively utilize
shortest-path trajectories, lacking effective exploration priors for training
navigation agents. To address the above challenges, we present SID, a
goal-oriented language-guided navigation learning approach with Self-Improving
Demonstrations. Specifically, SID learns an initial agent on the shortest-path
data sampled from environments and then leverages this agent to generate novel
exploration trajectories. The novel rollouts provide demonstrations with
stronger exploration strategies to train a better agent, which in turn produces
higher-quality agent demonstrations for the next round of training. We show
that this iterative self-improving pipeline readily scales to new environments,
and the resulting demonstrations can be transferred across a variety of
language-guided navigation tasks, elevating the performance ceiling in diverse
goal-oriented navigation tasks. Extensive experiments demonstrate that SID
significantly boosts the exploration capabilities and generalization of
navigation agents. The resulting agent achieves new state-of-the-art
performance on goal-oriented language-guided navigation tasks, including
REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation
splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.

</details>


### [317] [Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis](https://arxiv.org/abs/2509.24913)
*Tian Xia,Matthew Sinclair,Andreas Schuh,Fabio De Sousa Ribeiro,Raghav Mehta,Rajat Rasal,Esther Puyol-Antón,Samuel Gerber,Kersten Petersen,Michiel Schaap,Ben Glocker*

Main category: cs.CV

TL;DR: 本论文提出了一种新的反事实图像生成方法Seg-CFT，通过结构特异性变量便捷地进行局部结构干预，并有效生成高质量的胸片等医学图像，用于数据增强和疾病建模。


<details>
  <summary>Details</summary>
Motivation: 现有反事实图像生成主要依赖外部分类器或回归器，这种方法在需要针对特定结构（如肺部区域）干预时容易引入其他不想要的全局变化；而像素级引导又对用户要求过高，需提供精细分割，操作繁琐。

Method: 提出了Segmentor-guided Counterfactual Fine-Tuning（Seg-CFT）方法，仅需对结构特异性标量变量进行调整，通过分割器指导，使生成的反事实图像在局部结构干预下具备局部一致性而无全局干扰。

Result: 成功生成了真实感强的胸部X光片，在冠状动脉疾病建模任务中取得了令人鼓舞的表现。

Conclusion: Seg-CFT方法在保证用户操作简便性的同时，实现了高质量的局部结构反事实生成，适用于数据增强、数据去偏和临床疾病建模等任务，具有广阔应用前景。

Abstract: Counterfactual image generation is a powerful tool for augmenting training
data, de-biasing datasets, and modeling disease. Current approaches rely on
external classifiers or regressors to increase the effectiveness of
subject-level interventions (e.g., changing the patient's age). For
structure-specific interventions (e.g., changing the area of the left lung in a
chest radiograph), we show that this is insufficient, and can result in
undesirable global effects across the image domain. Previous work used
pixel-level label maps as guidance, requiring a user to provide hypothetical
segmentations which are tedious and difficult to obtain. We propose
Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the
simplicity of intervening on scalar-valued, structure-specific variables while
producing locally coherent and effective counterfactuals. We demonstrate the
capability of generating realistic chest radiographs, and we show promising
results for modeling coronary artery disease. Code:
https://github.com/biomedia-mira/seg-cft.

</details>


### [318] [Scalable GANs with Transformers](https://arxiv.org/abs/2509.24935)
*Sangeek Hyun,MinKyu Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的生成对抗网络（GAN）方法，将变分自编码器（VAE）紧凑潜空间与纯Transformer结构结合，显著提升了大规模生成模型的稳定性和效率。新方法GAT在ImageNet-256上以更少训练周期达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 目前生成模型领域的进步主要依赖于扩展网络规模，但GAN在扩展过程中存在优化不稳定、生成器早期层利用率低等问题。本文旨在探索解决GAN扩展性问题的方法。

Method: 作者尝试了两大策略：1) 在紧凑的VAE潜空间中训练GAN以获得高效且高保真度的生成；2) 采用纯Transformer结构的生成器和判别器。他们分析在简单地扩展网络时出现的失效模式，并提出两种改进：中间轻量级监督与宽度自适应学习率调整。

Result: 实验表明基于上述方法构建的GAT网络（纯Transformer和潜空间GAN）在各种规模下均可稳定训练，其中GAT-XL/2在ImageNet-256数据集上用更少的epoch（40个，远少于对比方法）获得了SOTA单步类别条件生成性能（FID=2.96）。

Conclusion: 通过结合潜空间训练和Transformer结构，并针对性地提出扩展友好的训练改进，GAN的扩展性和生成性能都得到了显著提升，推动了生成对抗学习在大规模任务中的应用。

Abstract: Scalability has driven recent advances in generative modeling, yet its
principles remain underexplored for adversarial learning. We investigate the
scalability of Generative Adversarial Networks (GANs) through two design
choices that have proven to be effective in other types of generative models:
training in a compact Variational Autoencoder latent space and adopting purely
transformer-based generators and discriminators. Training in latent space
enables efficient computation while preserving perceptual fidelity, and this
efficiency pairs naturally with plain transformers, whose performance scales
with computational budget. Building on these choices, we analyze failure modes
that emerge when naively scaling GANs. Specifically, we find issues as
underutilization of early layers in the generator and optimization instability
as the network scales. Accordingly, we provide simple and scale-friendly
solutions as lightweight intermediate supervision and width-aware learning-rate
adjustment. Our experiments show that GAT, a purely transformer-based and
latent-space GANs, can be easily trained reliably across a wide range of
capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art
single-step, class-conditional generation performance (FID of 2.96) on
ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.

</details>


### [319] [Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents](https://arxiv.org/abs/2509.24943)
*Jiahua Li,Kun Wei,Zhe Xu,Zibo Su,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了CogniGPT框架，通过多粒度感知与反思验证循环，有效提升AI系统对长视频的理解能力，并在多个数据集取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 长视频信息稀疏且结构复杂，现有大模型难以兼顾信息获取的完整性与效率，需要新的机制提升其对长视频的推理和理解能力。

Method: 提出CogniGPT，其中包含多粒度感知代理（MGPA）模拟人类视觉注意机制，聚焦关键信息；反思验证代理（VERA）对感知到的关键信息进行验证，减少幻觉并优化感知策略。两者通过互动循环，提取最有价值的线索。

Result: CogniGPT在EgoSchema、Video-MME、NExT-QA和MovieChat等数据集上表现出色。在EgoSchema测试中，仅用11.2帧就超越了现有的免训练方法，并达到与Gemini 1.5-Pro相当的水平。

Conclusion: CogniGPT通过模拟人类的渐进视觉认知，有效提升了长视频理解的准确率和效率，为AI长视频分析提供了新思路。

Abstract: Long videos, characterized by temporal complexity and sparse task-relevant
information, pose significant reasoning challenges for AI systems. Although
various Large Language Model (LLM)-based approaches have advanced long video
understanding, they still struggle to achieve both completeness and efficiency
in capturing task-critical information. Inspired by human progressive visual
cognition, we propose CogniGPT, a framework that leverages an interactive loop
between Multi-Granular Perception Agent (MGPA) and Verification-Enhanced
Reflection Agent (VERA) for efficient and reliable long video understanding.
Specifically, MGPA mimics human visual divergent and focused attention to
capture task-related information, while VERA verifies perceived key clues to
mitigate hallucination and optimize subsequent perception strategies. Through
this interactive process, CogniGPT explores a minimal set of informative and
reliable task-related clues. Extensive experiments on EgoSchema, Video-MME,
NExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both
accuracy and efficiency. Notably, on EgoSchema, it surpasses existing
training-free methods using only 11.2 frames and achieves performance
comparable to Gemini 1.5-Pro.

</details>


### [320] [Evaluating Temperature Scaling Calibration Effectiveness for CNNs under Varying Noise Levels in Brain Tumour Detection](https://arxiv.org/abs/2509.24951)
*Ankur Chanda,Kushan Choudhury,Shubhrodeep Roy,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: 本文探讨了温度缩放（Temperature Scaling, TS）这种后处理校准方法在大脑肿瘤图像分类中提升深度学习模型置信度估计可靠性的效果。


<details>
  <summary>Details</summary>
Motivation: 医学影像中涉及高风险场景，分类模型过度自信易导致严重后果，因此需要准确的置信度估计。

Method: 自定义卷积神经网络（CNN），在合并的脑部MRI数据集上训练，并引入五种图像噪声模仿现实不确定性。使用多项性能指标评估TS方法校准前后模型在不同噪声条件下的表现。

Result: 在所有噪声条件下，TS均明显降低了期望校准误差（ECE）和负对数似然（NLL），且未影响分类准确率。

Conclusion: TS作为一种有效、计算高效的后处理校准方法，可提升医疗AI模型在有噪声或不确定环境下的决策置信度和结果可靠性。

Abstract: Precise confidence estimation in deep learning is vital for high-stakes
fields like medical imaging, where overconfident misclassifications can have
serious consequences. This work evaluates the effectiveness of Temperature
Scaling (TS), a post-hoc calibration technique, in improving the reliability of
convolutional neural networks (CNNs) for brain tumor classification. We develop
a custom CNN and train it on a merged brain MRI dataset. To simulate real-world
uncertainty, five types of image noise are introduced: Gaussian, Poisson, Salt
& Pepper, Speckle, and Uniform. Model performance is evaluated using precision,
recall, F1-score, accuracy, negative log-likelihood (NLL), and expected
calibration error (ECE), both before and after calibration. Results demonstrate
that TS significantly reduces ECE and NLL under all noise conditions without
degrading classification accuracy. This underscores TS as an effective and
computationally efficient approach to enhance decision confidence of medical AI
systems, hence making model outputs more reliable in noisy or uncertain
settings.

</details>


### [321] [Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots](https://arxiv.org/abs/2509.24966)
*Ermanno Bartoli,Dennis Rotondi,Buwei He,Patric Jensfelt,Kai O. Arras,Iolanda Leite*

Main category: cs.CV

TL;DR: 文章提出了Social 3D Scene Graphs，一种能够描述场景中人与环境及相互关系的增强型3D场景图，并构建了新的评测基准。实验结果显示该方法提升了人在场景中活动与关系的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景图在理解场景时，往往忽视了人及其与环境的关系，主要原因在于缺乏标注好的数据集。此外，现有方法大多只基于单帧图片，无法捕捉更为复杂、长距离的人物互动关系。这些限制影响了机器人社会化与场景感知的智能化。

Method: 本文提出Social 3D Scene Graphs，将人、人的属性、活动及其与环境的关系纳入3D场景图中，并采用开放词汇框架表述各种关系。此外，作者还建立了一个包含丰富人物-场景关系标注的合成环境基准数据集，用于检验新的3D社交场景理解能力。

Result: 实验表明，Social 3D Scene Graphs能够提升机器人在3D场景条件下的人类活动预测与人-环境关系推理的准确性，在特定任务和多元查询下表现优秀。

Conclusion: Social 3D Scene Graphs为3D场景理解提供了一种更全面、更具社会意识的表征方法，有助于推动机器人在复杂社会环境中的智能与适应能力发展。

Abstract: Understanding how people interact with their surroundings and each other is
essential for enabling robots to act in socially compliant and context-aware
ways. While 3D Scene Graphs have emerged as a powerful semantic representation
for scene understanding, existing approaches largely ignore humans in the
scene, also due to the lack of annotated human-environment relationships.
Moreover, existing methods typically capture only open-vocabulary relations
from single image frames, which limits their ability to model long-range
interactions beyond the observed content. We introduce Social 3D Scene Graphs,
an augmented 3D Scene Graph representation that captures humans, their
attributes, activities and relationships in the environment, both local and
remote, using an open-vocabulary framework. Furthermore, we introduce a new
benchmark consisting of synthetic environments with comprehensive human-scene
relationship annotations and diverse types of queries for evaluating social
scene understanding in 3D. The experiments demonstrate that our representation
improves human activity prediction and reasoning about human-environment
relations, paving the way toward socially intelligent robots.

</details>


### [322] [Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning](https://arxiv.org/abs/2509.24968)
*Donghwa Kang,Junho Kim,Dongwoo Kang*

Main category: cs.CV

TL;DR: 本文介绍了一种结合跨模态融合注意力（CMFA）和自监督多事件表示学习（SSMER）的新方法，实现了事件相机下的人脸关键点对齐，在真实和合成数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机在弱光和快速运动等极端环境下有优势，但现有基于RGB的方法难以直接迁移，且事件数据空间信息有限，缺乏大规模标注数据，导致性能受限。

Method: 提出了CMFA模块，将RGB和事件数据特征融合，指导模型从事件数据中提取鲁棒的人脸表征；同时设计了SSMER模块，利用无标签事件数据进行自监督特征学习，缓解空间信息不足的问题。

Result: 在自建的E-SIE真实事件数据集和公开WFLW-V合成事件数据集上进行实验，本文方法在多个评价指标上均超过最新主流方法。

Conclusion: 该方法有效融合了RGB与事件数据优势，即便事件数据标签有限，也能实现高精度的人脸关键点对齐，对事件驱动视觉任务具有广泛应用前景。

Abstract: Event cameras offer unique advantages for facial keypoint alignment under
challenging conditions, such as low light and rapid motion, due to their high
temporal resolution and robustness to varying illumination. However, existing
RGB facial keypoint alignment methods do not perform well on event data, and
training solely on event data often leads to suboptimal performance because of
its limited spatial information. Moreover, the lack of comprehensive labeled
event datasets further hinders progress in this area. To address these issues,
we propose a novel framework based on cross-modal fusion attention (CMFA) and
self-supervised multi-event representation learning (SSMER) for event-based
facial keypoint alignment. Our framework employs CMFA to integrate
corresponding RGB data, guiding the model to extract robust facial features
from event input images. In parallel, SSMER enables effective feature learning
from unlabeled event data, overcoming spatial limitations. Extensive
experiments on our real-event E-SIE dataset and a synthetic-event version of
the public WFLW-V benchmark show that our approach consistently surpasses
state-of-the-art methods across multiple evaluation metrics.

</details>


### [323] [On-the-Fly Data Augmentation for Brain Tumor Segmentation](https://arxiv.org/abs/2509.24973)
*Ishika Jain,Siri Willems,Steven Latre,Tom De Schepper*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练生成对抗网络（GliGANs）动态合成肿瘤的实时增强方法，解决了训练期存储大量增强3D数据的难题，在BraTS 2025挑战中取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 传统肿瘤分割模型难以跨不同时期的治疗前后MRI扫描泛化；与此同时，有限的高质量标注数据和庞大的3D增强数据存储需求成为发展的挑战。

Method: 采用基于nnU-Net的三个模型，结合预训练GliGANs在训练过程中实时插入合成肿瘤进行数据增强。分别验证：1）无外部增强；2）常规实时增强；3）定制实时增强三种方式，并结合模型集成方法提升效果。

Result: 在BraTS 2025在线验证平台上，集成模型在多种病灶区域分割中取得了Dice分数ET 0.79、NETC 0.749、RC 0.872、SNFH 0.825、TC 0.79、WT 0.88，并在挑战赛中排名第一。

Conclusion: 基于预训练生成对抗网络的动态合成肿瘤增强能够有效提升跨治疗周期的胶质瘤分割模型的泛化能力，同时避免了传统增强存储的计算资源瓶颈。

Abstract: Robust segmentation across both pre-treatment and post-treatment glioma scans
can be helpful for consistent tumor monitoring and treatment planning. BraTS
2025 Task 1 addresses this by challenging models to generalize across varying
tumor appearances throughout the treatment timeline. However, training such
generalized models requires access to diverse, high-quality annotated data,
which is often limited. While data augmentation can alleviate this, storing
large volumes of augmented 3D data is computationally expensive. To address
these challenges, we propose an on-the-fly augmentation strategy that
dynamically inserts synthetic tumors using pretrained generative adversarial
networks (GliGANs) during training. We evaluate three nnU-Net-based models and
their ensembles: (1) a baseline without external augmentation, (2) a regular
on-the-fly augmented model, and (3) a model with customized on-the-fly
augmentation. Built upon the nnU-Net framework, our pipeline leverages
pretrained GliGAN weights and tumor insertion methods from prior
challenge-winning solutions. An ensemble of the three models achieves
lesion-wise Dice scores of 0.79 (ET), 0.749 (NETC), 0.872 (RC), 0.825 (SNFH),
0.79 (TC), and 0.88 (WT) on the online BraTS 2025 validation platform. This
work ranked first in the BraTS Lighthouse Challenge 2025 Task 1- Adult Glioma
Segmentation.

</details>


### [324] [Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel](https://arxiv.org/abs/2509.24979)
*Haotian Dong,Wenjing Wang,Chen Li,Di Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Wan-Alpha的新型透明RGBA视频生成框架，在视觉质量、运动真实感以及透明度渲染方面优于现有方法，同时支持生成丰富细节和半透明特效。


<details>
  <summary>Details</summary>
Motivation: 目前RGBA视频的生成在实际应用中越来越重要，但现有方法往往忽视了视觉质量，导致难以实际应用。因此需要开发一种能够共同学习RGB和Alpha通道、提升生成视频视觉效果的方法。

Method: 作者设计了一种高效的变分自编码器（VAE），能将Alpha通道编码到RGB潜在空间中，同时构建了一个高质量和多样化的RGBA视频数据集，用以训练扩散Transformer。

Result: 与现有最先进的方法相比，该模型在视觉质量、运动真实感与透明度渲染多方面均表现更优，能够生成多样化的半透明物体、光效和细致结构如发丝等。

Conclusion: Wan-Alpha框架改善了RGBA视频的生成效果，有望拓展其在实用领域的应用，并为后续相关研究提供新基线和实验工具。

Abstract: RGBA video generation, which includes an alpha channel to represent
transparency, is gaining increasing attention across a wide range of
applications. However, existing methods often neglect visual quality, limiting
their practical usability. In this paper, we propose \textit{Wan-Alpha}, a new
framework that generates transparent videos by learning both RGB and alpha
channels jointly. We design an effective variational autoencoder (VAE) that
encodes the alpha channel into the RGB latent space. Then, to support the
training of our diffusion transformer, we construct a high-quality and diverse
RGBA video dataset. Compared with state-of-the-art methods, our model
demonstrates superior performance in visual quality, motion realism, and
transparency rendering. Notably, our model can generate a wide variety of
semi-transparent objects, glowing effects, and fine-grained details such as
hair strands. The released model is available on our website:
\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.

</details>


### [325] [SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation](https://arxiv.org/abs/2509.24980)
*Shuang Liang,Jing He,Chuanmeizhi Wang,Lejun Liao,Guo Zhang,Yingcong Chen,Yuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了SDPose，一种基于Stable Diffusion微调的人体姿态估计新框架，实现了跨域鲁棒性提升并达到最新的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有方法将扩散模型的先验用于密集预测任务并展现出较强的泛化能力，但它们在结构化输出场景（如人体姿态估计）上的潜力尚未充分挖掘。因此，作者希望利用稳定扩散模型（Stable Diffusion）的强大生成能力和多尺度潜在特征，提升人体姿态估计的表现和跨域鲁棒性。

Method: 1. 在Stable Diffusion的U-Net潜在空间内直接预测关键点热力图，最大化保留扩散生成先验；2. 通过轻量级卷积姿态分支（pose head）将潜在特征映射为关键点热力图，避免干扰预训练主干网；3. 引入辅助RGB重建分支，防止过拟合并增强跨域泛化能力；4. 构建了COCO-OOD数据集用于评估模型在风格迁移和域偏移情况下的鲁棒性。

Result: SDPose在COCO验证集上仅需Sapiens五分之一的训练预算，性能与Sapiens-1B/2B持平；在HumanArt和COCO-OOD等跨域基准上取得了新的SOTA。此外，SDPose还展示了作为零-shot姿态标注器在可控生成（如ControlNet图像合成和视频生成）中的优越性。

Conclusion: 本文首次将Stable Diffusion微调到结构化预测的人体姿态估计任务，通过设计解耦机制和辅助任务，显著提升了性能和跨域鲁棒性，同时为高质量的下游可控生成任务提供了有力的姿态信息指导。

Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are
emerging as powerful vision backbones. While recent works such as
Marigold~\citep{ke2024repurposing} and Lotus~\citep{he2024lotus} adapt
diffusion priors for dense prediction with strong cross-domain generalization,
their potential for structured outputs (e.g., human pose estimation) remains
underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning
framework built upon Stable Diffusion to fully exploit pre-trained diffusion
priors for human pose estimation. First, rather than modifying cross-attention
modules or introducing learnable embeddings, we directly predict keypoint
heatmaps in the SD U-Net's image latent space to preserve the original
generative priors. Second, we map these latent features into keypoint heatmaps
through a lightweight convolutional pose head, which avoids disrupting the
pre-trained backbone. Finally, to prevent overfitting and enhance
out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction
branch that preserves domain-transferable generative semantics. To evaluate
robustness under domain shift, we further construct \textbf{COCO-OOD}, a
style-transferred variant of COCO with preserved annotations. With just
one-fifth of the training schedule used by Sapiens on COCO, SDPose attains
parity with Sapiens-1B/2B on the COCO validation set and establishes a new
state of the art on the cross-domain benchmarks HumanArt and COCO-OOD.
Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream
controllable generation tasks, including ControlNet-based image synthesis and
video generation, where it delivers qualitatively superior pose guidance.

</details>


### [326] [PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion](https://arxiv.org/abs/2509.24997)
*Yuyang Yin,HaoXiang Guo,Fangfu Liu,Mengyu Wang,Hanwen Liang,Eric Li,Yikai Wang,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种高保真、可控的全景视频生成框架PanoWorld-X，可实现多样化相机运动轨迹和高质量场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有全景视觉世界生成方法在视场范围或相机控制方面存在不足，难以实现连续、可控和高质量的场景生成。用户或代理的自由探索受限。

Method: 1）构建了包含全景视频与探索路径对的大规模数据集；2）在虚拟3D环境（基于Unreal Engine）中模拟相机轨迹生成数据；3）设计了Sphere-Aware Diffusion Transformer，通过将等距矩形特征重投影到球面，有效建模几何邻接性，提升生成视觉质量和空间连续性。

Result: 实验表明，PanoWorld-X在运动范围、控制精度和视觉质量等多个维度均优于现有方法。

Conclusion: PanoWorld-X具备高保真、强可控性和更佳体验，展现出在实际应用领域的巨大潜力。

Abstract: Generating a complete and explorable 360-degree visual world enables a wide
range of downstream applications. While prior works have advanced the field,
they remain constrained by either narrow field-of-view limitations, which
hinder the synthesis of continuous and holistic scenes, or insufficient camera
controllability that restricts free exploration by users or autonomous agents.
To address this, we propose PanoWorld-X, a novel framework for high-fidelity
and controllable panoramic video generation with diverse camera trajectories.
Specifically, we first construct a large-scale dataset of panoramic
video-exploration route pairs by simulating camera trajectories in virtual 3D
environments via Unreal Engine. As the spherical geometry of panoramic data
misaligns with the inductive priors from conventional video diffusion, we then
introduce a Sphere-Aware Diffusion Transformer architecture that reprojects
equirectangular features onto the spherical surface to model geometric
adjacency in latent space, significantly enhancing visual fidelity and
spatiotemporal continuity. Extensive experiments demonstrate that our
PanoWorld-X achieves superior performance in various aspects, including motion
range, control precision, and visual quality, underscoring its potential for
real-world applications.

</details>


### [327] [LVT: Large-Scale Scene Reconstruction via Local View Transformers](https://arxiv.org/abs/2509.25001)
*Tooba Imtiaz,Lucy Chai,Kathryn Heal,Xuan Luo,Jungyeon Park,Jennifer Dy,John Flynn*

Main category: cs.CV

TL;DR: 本文提出了一种名为Local View Transformer (LVT)的新型网络架构，能高效处理和重建大规模3D场景，并支持新视角合成，突破了传统Transformer在大场景视觉任务中计算复杂度过高的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在3D视觉和新视角生成任务中效果突出，但其随输入规模增长呈二次复杂度，难以直接扩展到大场景重建。为此，作者希望绕开该复杂度瓶颈，实现大规模、高分辨率3D场景的高效合成。

Method: 提出Local View Transformer (LVT)架构。模型仅处理每个视角周围的局部邻域信息，通过设计新的相对几何变换位置编码，使模型能关注相邻视图中的token，高效利用局部关联。最后将网络输出解码为3D Gaussian Splat表示，编码了颜色和不透明度的视角依赖。

Result: LVT模型可在单次前向传播下重建任意大、任意高分辨率的场景，并取得良好效果。项目页面展示了实验结果和互动演示。

Conclusion: LVT有效突破了传统Transformer二次复杂度的限制，为大场景3D重建和新视角合成提供了高效、高质量的新方案。

Abstract: Large transformer models are proving to be a powerful tool for 3D vision and
novel view synthesis. However, the standard Transformer's well-known quadratic
complexity makes it difficult to scale these methods to large scenes. To
address this challenge, we propose the Local View Transformer (LVT), a
large-scale scene reconstruction and novel view synthesis architecture that
circumvents the need for the quadratic attention operation. Motivated by the
insight that spatially nearby views provide more useful signal about the local
scene composition than distant views, our model processes all information in a
local neighborhood around each view. To attend to tokens in nearby views, we
leverage a novel positional encoding that conditions on the relative geometric
transformation between the query and nearby views. We decode the output of our
model into a 3D Gaussian Splat scene representation that includes both color
and opacity view-dependence. Taken together, the Local View Transformer enables
reconstruction of arbitrarily large, high-resolution scenes in a single forward
pass. See our project page for results and interactive demos
https://toobaimt.github.io/lvt/.

</details>


### [328] [CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation](https://arxiv.org/abs/2509.25016)
*Max Curie,Paulo da Costa*

Main category: cs.CV

TL;DR: 论文提出了CLASP，一种无需训练和标注数据的无监督图像分割方法，表现与最新基线接近。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督图像分割方法通常依赖于大量标注数据或复杂的模型微调过程，这在大规模、未注释的数据集（如广告营销领域）中难以实现，因此亟需一种无需标注、无需训练且效果稳定的方法。

Method: CLASP流程包括：1）利用DINO自监督ViT编码器提取图像小块特征；2）构建特征亲和矩阵，进行谱聚类分割；3）采用eigengap silhouette搜索自动确定分割数量；4）使用DenseCRF优化分割边界。整个流程无训练过程且参数调优最小化。

Result: 在COCO Stuff和ADE20K两个数据集上，CLASP实现了与最新无监督分割方法相当的mIoU与像素准确度。

Conclusion: CLASP因其简洁、完全无训练的特点成为大规模未标注数据集分割的强大且易复现的基线，特别适用于品牌审核、广告内容管理等实际应用场景。

Abstract: We introduce CLASP (Clustering via Adaptive Spectral Processing), a
lightweight framework for unsupervised image segmentation that operates without
any labeled data or finetuning. CLASP first extracts per patch features using a
self supervised ViT encoder (DINO); then, it builds an affinity matrix and
applies spectral clustering. To avoid manual tuning, we select the segment
count automatically with a eigengap silhouette search, and we sharpen the
boundaries with a fully connected DenseCRF. Despite its simplicity and training
free nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff
and ADE20K, matching recent unsupervised baselines. The zero training design
makes CLASP a strong, easily reproducible baseline for large unannotated
corpora especially common in digital advertising and marketing workflows such
as brand safety screening, creative asset curation, and social media content
moderation

</details>


### [329] [GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning](https://arxiv.org/abs/2509.25026)
*Mustansar Fiaz,Hiyam Debary,Paolo Fraccaro,Danda Paudel,Luc Van Gool,Fahad Khan,Salman Khan*

Main category: cs.CV

TL;DR: 作者提出了一种新颖的基于强化学习的后训练框架，通过引入任务感知奖励，提升了遥感影像任务中推理能力与鲁棒性，实验证明其优于现有视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在自然图像领域表现优异，但其在遥感影像等地球观测领域应用较少，而此类任务对推理能力和任务适应性要求更高，因此需开发新方法提升遥感推理效果。

Method: 提出一种新的后训练策略，在已有推理型RL模型基础上，加入任务感知奖励机制，实现模型对多种地球观测任务（如目标检测、时序分析等）的有效适配和强化训练。

Result: 在多个遥感领域基准测试上，这一方法优于当前领先的通用及专用视觉语言模型，在推理能力、优化稳定性和鲁棒性等方面表现出持续提升。

Conclusion: 所提框架能有效促进遥感影像相关任务的推理性能，为后续地球观测研究和实际应用提供了一种通用且高效的解决方案。

Abstract: Recent advances in reinforcement learning (RL) have delivered strong
reasoning capabilities in natural image domains, yet their potential for Earth
Observation (EO) remains largely unexplored. EO tasks introduce unique
challenges, spanning referred object detection, image or region captioning,
change detection, grounding, and temporal analysis, that demand task aware
reasoning. We propose a novel post training framework that incorporates task
aware rewards to enable effective adaptation of reasoning based RL models to
diverse EO tasks. This training strategy enhances reasoning capabilities for
remote sensing images, stabilizes optimization, and improves robustness.
Extensive experiments across multiple EO benchmarks show consistent performance
gains over state of the art generic and specialized vision language models.
Code and models will be released publicly at
https://mustansarfiaz.github.io/GeoVLM-R1/ .

</details>


### [330] [STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation](https://arxiv.org/abs/2509.25027)
*Xiaoxiao Ma,Haibo Qiu,Guohui Zhang,Zhixiong Zeng,Siqi Yang,Lin Ma,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的稳定且泛化能力强的GRPO框架(STAGE)用于自回归文本生成图像模型，大幅提升了视觉质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO（基于强化学习的梯度策略优化）方法应用于自回归图像生成模型时，存在训练不稳定、预训练模型能力受损以及泛化差等问题，迫切需要新的方法来解决这些负面影响。

Method: 作者分析了自回归模型采用GRPO训练时的两大问题：1) 不必要token导致梯度矛盾，2) 策略熵动态不稳定。为此提出STAGE框架，包含两项策略：（1）优势/KL加权与相似性感知再加权以缓解梯度冲突；（2）基于引用模型熵的奖励项以稳定训练过程。

Result: 实验证明，STAGE在多个基准下均超过了传统GRPO方法，在提升视觉质量、训练稳定性以及跨任务泛化能力方面表现出优势。

Conclusion: STAGE极大缓解了自回归图像生成中GRPO训练带来的模型能力受损，并显著提高了生成质量及泛化性能，为后续相关任务提供了有效思路。

Abstract: Reinforcement learning has recently been explored to improve text-to-image
generation, yet applying existing GRPO algorithms to autoregressive (AR) image
models remains challenging. The instability of the training process easily
disrupts the pretrained model capability during long runs, resulting in
marginal gains, degraded image quality, and poor generalization. In this work,
we revisit GRPO for AR image generation and identify two key issues:
contradictory gradients from unnecessary tokens and unstable policy entropy
dynamics. To address these, we introduce STAGE, a stable and generalizable
framework that leverages two targeted solutions: 1) Advantage/KL reweighting.
Similarity-aware reweighting to alleviate conflicting updates; and 2) Entropy
reward. An entropy-based reward corresponding to reference model to stabilize
learning. With the help of alleviating conflicts between tokens and an entropy
reward for stabilizing training, we reduce disruption of the pretrained
distribution and mitigate reward hacking, which in turn improves generalization
and transfer better to other benchmarks. Experiments across multiple benchmarks
show that STAGE consistently improves visual quality, stability, and cross-task
generalization compared to baseline GRPO.

</details>


### [331] [VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning](https://arxiv.org/abs/2509.25033)
*Wenhao Li,Qiangchang Wang,Xianjing Meng,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跨模态大模型引导的小样本学习方法（VT-FSL），显著提升了小样本识别能力，超越多项基准。


<details>
  <summary>Details</summary>
Motivation: 尽管已有基于语义增强和语义融合模块的方法能提升小样本学习，但依赖的语义信息未能与真实视觉实例结合，导致语义与视觉证据不符、引入噪声和修正成本高。因此，提出方法旨在克服语义与视觉证据融合不足的问题。

Method: 新方法VT-FSL包含两大关键模块：1）跨模态迭代提示（CIP），即通过条件LLM输入类别和样本图像，生成准确的类别描述，用于丰富语义和合成新图片；2）跨模态几何对齐（CGA），通过最小化三维平行体积核将文本、支持和合成视觉表征做全局且非线性对齐，实现多模态信息整合。

Result: VT-FSL在十个多样化的小样本学习基准测试（含标准、跨域与细粒度）上取得了新的最优结果。

Conclusion: 通过跨模态几何对齐地整合大模型生成的描述与视觉信息，VT-FSL极大提升了小样本学习的表现，并为多模态学习提供了新思路。

Abstract: Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples. Recent studies enhance support features by
incorporating additional semantic information or designing complex semantic
fusion modules. However, they still suffer from hallucinating semantics that
contradict the visual evidence due to the lack of grounding in actual
instances, resulting in noisy guidance and costly corrections. To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment. It mainly consists of
Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment
(CGA). Specifically, the CIP conditions an LLM on both class names and support
images to generate precise class descriptions iteratively in a single
structured reasoning pass. These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images. The descriptions and synthetic images act
respectively as complementary textual and visual prompts, providing high-level
class semantics and low-level intra-class diversity to compensate for limited
support data. Furthermore, the CGA jointly aligns the fused textual, support,
and synthetic visual representations by minimizing the kernelized volume of the
3-dimensional parallelotope they span. It captures global and nonlinear
relationships among all representations, enabling structured and consistent
multimodal integration. The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios. Code is available
at https://github.com/peacelwh/VT-FSL.

</details>


### [332] [Fast Real-Time Pipeline for Robust Arm Gesture Recognition](https://arxiv.org/abs/2509.25042)
*Milán Zsolt Bagladi,László Gulyás,Gergő Szalay*

Main category: cs.CV

TL;DR: 本文提出了一种基于OpenPose关键点估计、关键点归一化和循环神经网络分类器的动态手势识别实时流程。实验结果在交通指挥手势数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动态手势识别常因人体角度、速度变化导致识别准确率下降，尤其在实际应用（如交通指挥）中亟需兼具高效与鲁棒的解决方案。

Method: 作者提出了以OpenPose提取关键点后，采用1×1归一化和两种特征表达（基于坐标和基于角度），最后输入循环神经网络进行分类。同时引入人工旋转训练数据，提高对摄像头角度变化的鲁棒性，并提供了手势速度计算方法。

Result: 在自建交通指挥手势数据集上，模型在不同视角和不同动作速度下均实现了高准确率，验证了流程的鲁棒性和有效性。

Conclusion: 提出的手势识别流程能实时识别变化手势，具有很强的鲁棒性和泛化能力，适合需要精准动态手势分析的实际场景。

Abstract: This paper presents a real-time pipeline for dynamic arm gesture recognition
based on OpenPose keypoint estimation, keypoint normalization, and a recurrent
neural network classifier. The 1 x 1 normalization scheme and two feature
representations (coordinate- and angle-based) are presented for the pipeline.
In addition, an efficient method to improve robustness against camera angle
variations is also introduced by using artificially rotated training data.
Experiments on a custom traffic-control gesture dataset demonstrate high
accuracy across varying viewing angles and speeds. Finally, an approach to
calculate the speed of the arm signal (if necessary) is also presented.

</details>


### [333] [A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration](https://arxiv.org/abs/2509.25044)
*Rohit Jena,Vedant Zope,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: 本文提出了FFDP，这是一组针对非GEMM（通用矩阵乘法）操作的IO感知融合核，并配合分布式框架，大幅提升超大规模医学图像配准的速度与效率。


<details>
  <summary>Details</summary>
Motivation: 随着生物医学和生命科学中成像技术的进步，图像获取能力大幅提升，但图像配准算法的可扩展性却未能同步发展，难以满足超大规模数据处理需求。

Method: FFDP通过针对非GEMM瓶颈进行优化，支持卷积感知的张量分片，并补充现有大规模transformer训练中的模型并行方法，为大规模图像配准任务提供分布式计算支持。

Result: FFDP实现了原生分辨率下的100微米体素级人脑MRI多模态配准任务，比标准临床数据量大570倍，仅用8块A6000 GPU约一分钟完成。对比当前最优方法，FFDP加速6-7倍，峰值内存消耗降低20-59%。在250微米数据集上，单GPU可处理的问题规模比现有最佳方法大64倍。

Conclusion: FFDP显著突破传统医学图像配准的可扩展性与效率极限，为大规模和高分辨率医学影像处理提供强有力的技术支撑。

Abstract: In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels
supplemented with a distributed framework for image registration at
unprecedented scales. Image registration is an inverse problem fundamental to
biomedical and life sciences, but algorithms have not scaled in tandem with
image acquisition capabilities. Our framework complements existing model
parallelism techniques proposed for large-scale transformer training by
optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding.
We demonstrate unprecedented capabilities by performing multimodal registration
of a 100 micron ex-vivo human brain MRI volume at native resolution - an
inverse problem more than 570x larger than a standard clinical datum in about a
minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art
optimization and deep learning registration pipelines by upto 6 - 7x while
reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250
micron dataset shows that FFDP can fit upto 64x larger problems than existing
SOTA on a single GPU, and highlights both the performance and efficiency gains
of FFDP compared to SOTA image registration methods.

</details>


### [334] [GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction](https://arxiv.org/abs/2509.25075)
*Huaizhi Qu,Xiao Wang,Gengwei Zhang,Jie Peng,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的冷冻电镜三维重建方法GEM，基于3D高斯Splatting，兼顾了效率和高分辨率，显著减少了算力和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 随着冷冻电镜数据集规模迅速增长（超过10万粒子图像），现有的三维重建方法在计算和内存方面面临重大挑战。傅里叶空间方法虽然高效，但精度不足；基于NeRF的实空间方法精度好，但对算力和内存开销极大。因此急需一种兼顾高效和高分辨率的三维重建新框架。

Method: 提出GEM算法，基于3D高斯Splatting，将蛋白质表示为紧凑的3D高斯，每个高斯仅需11个参数，并设计了高效的梯度计算方法，只对实际贡献的体素计算梯度，显著降低了内存和计算需求。

Result: 在标准冷冻电镜重建基准上，GEM相比最新方法训练速度提升最高48%，内存占用减少12%，局部分辨率提升可达38.8%。

Conclusion: GEM在速度、内存效率和高分辨率精度上均取得突破，成为冷冻电镜三维重建的实用性、可扩展性新范式。

Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for
high-resolution structural biology, yet the massive scale of datasets (often
exceeding 100k particle images) renders 3D reconstruction both computationally
expensive and memory intensive. Traditional Fourier-space methods are efficient
but lose fidelity due to repeated transforms, while recent real-space
approaches based on neural radiance fields (NeRFs) improve accuracy but incur
cubic memory and computation overhead. Therefore, we introduce GEM, a novel
cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that
operates directly in real-space while maintaining high efficiency. Instead of
modeling the entire density volume, GEM represents proteins with compact 3D
Gaussians, each parameterized by only 11 values. To further improve the
training efficiency, we designed a novel gradient computation to 3D Gaussians
that contribute to each voxel. This design substantially reduced both memory
footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to
48% faster training and 12% lower memory usage compared to state-of-the-art
methods, while improving local resolution by as much as 38.8%. These results
establish GEM as a practical and scalable paradigm for cryo-EM reconstruction,
unifying speed, efficiency, and high-resolution accuracy. Our code is available
at https://github.com/UNITES-Lab/GEM.

</details>


### [335] [BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation](https://arxiv.org/abs/2509.25077)
*Dingning Liu,Haoyu Guo,Jingyi Zhou,Tong He*

Main category: cs.CV

TL;DR: 本文提出了一种新方法BRIDGE，通过深度到图像的生成方式，大规模合成真实且几何准确的RGB图像-深度对，并结合混合监督以提升单目深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 传统单目深度估计方法由于缺乏高质量和多样化的数据，导致模型鲁棒性有限。解决数据稀缺和提升深度估计泛化能力是本研究动机。

Method: 提出BRIDGE框架，采用RL优化的深度到图像（D2I）生成模型，从多样化深度图合成高质量的RGB-深度对（超2000万对）。随后利用生成的数据，通过结合教师伪标签和真实深度标签的混合监督策略进行模型训练。

Result: BRIDGE在数据规模和领域多样性上取得突破。实验显示其在量化评测和复杂场景细节恢复方面，均优于现有最先进方法。

Conclusion: 创新性数据生成和训练范式显著提升了单目深度估计的泛化和鲁棒性，为领域发展带来新方向。

Abstract: Monocular Depth Estimation (MDE) is a foundational task for computer vision.
Traditional methods are limited by data scarcity and quality, hindering their
robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps. Then we train our depth estimation
model on this dataset, employing a hybrid supervision strategy that integrates
teacher pseudo-labels with ground truth depth for comprehensive and robust
training. This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features. Code and models are available at
https://dingning-liu.github.io/bridge.github.io/.

</details>


### [336] [UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation](https://arxiv.org/abs/2509.25079)
*Guanjun Wu,Jiemin Fang,Chen Yang,Sikuang Li,Taoran Yi,Jia Lu,Zanwei Zhou,Jiazhong Cen,Lingxi Xie,Xiaopeng Zhang,Wei Wei,Wenyu Liu,Xinggang Wang,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出UniLat3D框架，实现了几何与外观统一的单阶段高保真3D资产生成，显著提升了速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有主流3D生成方法大多采用扩散模型的两阶段流程，先生成几何后合成外观，导致几何与纹理易出现对齐问题，并且计算代价较高。

Method: 设计统一的几何-外观VAE，将高分辨率稀疏特征压缩编码为一个低分辨率致密潜变量UniLat，将结构和外观信息统一，并可高效解码为不同3D格式（如高斯点、网格等）。同时训练单一流匹配模型实现高斯噪声到UniLat的直接映射，精简生成流程。

Result: 在仅用公开数据集训练的条件下，UniLat3D能够从单张图片在数秒内生成高质量、几何与外观更精确对齐的3D资产，且外观和几何质量均优于现有方法。

Conclusion: UniLat3D显著提升了3D资产生成的效率与保真度，在实际应用中潜力巨大，突破了两阶段模型的限制，未来有望推动3D内容自动生成的发展。

Abstract: High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&
code are available at https://unilat3d.github.io/

</details>


### [337] [MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification](https://arxiv.org/abs/2509.25082)
*Xiaoyi Huang,Junwei Wu,Kejia Zhang,Carl Yang,Zhiming Luo*

Main category: cs.CV

TL;DR: 该论文提出了针对对抗样本的普适性净化方法MANI-Pure，通过频率自适应方式提升防御效果，实验证明其在主流数据集上性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对抗净化方法采用均匀噪声注入，导致对所有频率一视同仁地扰动，损害了语义结构，影响鲁棒性。而实际对抗扰动并非均匀分布，主要集中在高频区域。论文基于这一发现，旨在设计更精细化的净化方法，以提升图像分类器对抗鲁棒性。

Method: 作者提出MANI-Pure，一种基于输入幅度谱的净化框架。与均匀噪声不同，MANI-Pure根据不同频段扰动的强度，自适应地在高频带和低频带注入具有异质性的噪声，有效抑制高频低幅度区域的对抗扰动，同时保护低频语义信息。

Result: 在CIFAR-10和ImageNet-1K上的大量实验表明，MANI-Pure能显著提升鲁棒准确率，干净样本准确率与原始分类器差距很小（仅0.59），对抗准确率提升2.15，在RobustBench排行榜上取得top-1的鲁棒准确率，超越现有SOTA方法。

Conclusion: MANI-Pure通过幅度自适应、异频注噪方式，有效改善扩散模型对抗净化的鲁棒性和判别性能，为图像分类领域提供了更强的对抗防御新策略。

Abstract: Adversarial purification with diffusion models has emerged as a promising
defense strategy, but existing methods typically rely on uniform noise
injection, which indiscriminately perturbs all frequencies, corrupting semantic
structures and undermining robustness. Our empirical study reveals that
adversarial perturbations are not uniformly distributed: they are predominantly
concentrated in high-frequency regions, with heterogeneous magnitude intensity
patterns that vary across frequencies and attack types. Motivated by this
observation, we introduce MANI-Pure, a magnitude-adaptive purification
framework that leverages the magnitude spectrum of inputs to guide the
purification process. Instead of injecting homogeneous noise, MANI-Pure
adaptively applies heterogeneous, frequency-targeted noise, effectively
suppressing adversarial perturbations in fragile high-frequency, low-magnitude
bands while preserving semantically critical low-frequency content. Extensive
experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of
MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original
classifier, while boosting robust accuracy by 2.15, and achieves the top-1
robust accuracy on the RobustBench leaderboard, surpassing the previous
state-of-the-art method.

</details>


### [338] [Triangle Splatting+: Differentiable Rendering with Opaque Triangles](https://arxiv.org/abs/2509.25122)
*Jan Held,Renaud Vandeghen,Sanghyun Son,Daniel Rebain,Matheus Gadelha,Yi Zhou,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出了Triangle Splatting+方法，直接优化三角形以实现高效、高质量的3D场景重建和新视角合成，并生成可直接用于主流图形引擎的网格。


<details>
  <summary>Details</summary>
Motivation: 神经辐射场（NeRF）虽然生成质量高，但训练和渲染速度慢；3D高斯Splatting提升了效率，但高斯原语无法直接兼容VR等主流图形系统的网格流程。目前试图将高斯转换为网格的方法繁琐且降低画质，需要一种既高效、又与现有图形管线兼容的方法。

Method: 提出Triangle Splatting+，在可微分splatting框架下，直接以三角形作为优化原语。通过三角形参数化实现顶点共享，提高网格连通性，并设计训练策略确保三角形不透明，输出的网格无需额外后处理，可直接用于标准图形引擎。

Result: 在Mip-NeRF360和Tanks & Temples数据集上，Triangle Splatting+在网格新视角合成任务中达到了当前最优表现。对比此前splatting方法，该方法在效率和画质上均有提升。输出的半连通网格支持后续物理仿真和交互漫游等应用。

Conclusion: Triangle Splatting+方法在保证训练和推理速度的同时，提升了视觉质量且兼容主流图形系统，为3D场景重建和新视角合成提供了更实用、高效的解决方案，具有广泛的应用前景。

Abstract: Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years. Neural Radiance Fields demonstrated that continuous volumetric
radiance fields can achieve high-quality image synthesis, but their long
training and rendering times limit practicality. 3D Gaussian Splatting (3DGS)
addressed these issues by representing scenes with millions of Gaussians,
enabling real-time rendering and fast optimization. However, Gaussian
primitives are not natively compatible with the mesh-based pipelines used in VR
headsets, and real-time graphics applications. Existing solutions attempt to
convert Gaussians into meshes through post-processing or two-stage pipelines,
which increases complexity and degrades visual quality. In this work, we
introduce Triangle Splatting+, which directly optimizes triangles, the
fundamental primitive of computer graphics, within a differentiable splatting
framework. We formulate triangle parametrization to enable connectivity through
shared vertices, and we design a training strategy that enforces opaque
triangles. The final output is immediately usable in standard graphics engines
without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis. Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.
Moreover, the resulting semi-connected meshes support downstream applications
such as physics-based simulation or interactive walkthroughs. The project page
is https://trianglesplatting2.github.io/trianglesplatting2/.

</details>


### [339] [Score Distillation of Flow Matching Models](https://arxiv.org/abs/2509.25127)
*Mingyuan Zhou,Yi Gu,Huangjie Zheng,Liangchen Song,Guande He,Yizhe Zhang,Wenze Hu,Yinfei Yang*

Main category: cs.CV

TL;DR: 本文提出了一个基于Bayes定理和条件期望的统一推导，将高斯扩散模型与流匹配模型联系在一起，不依赖常见的ODE/SDE框架。在此基础上，作者将Score identity Distillation（SiD）方法推广到了多种主流文本到图像的流匹配模型，实验证明该方法无需教师微调和架构更改即可直接应用。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成图像质量高，但采样速度慢。流匹配虽被认为是不同框架，但理论上与扩散在高斯假设下等价。此前，不清楚相关的蒸馏技术能否直接迁移应用。因此，作者希望从理论和实证两方面统一二者，并验证扩散领域的蒸馏手段对流匹配模型是否有效。

Method: 作者用Bayes定理和条件期望，从概率角度推导，统一高斯扩散和流匹配理论。随后，将Score identity Distillation（SiD）蒸馏方法拓展到多种主流预训练文本生成图像流匹配模型（如SANA、SD3、FLUX等），只针对流匹配和DiT做了些微调，且适用于无数据和部分数据设置。

Result: 实验证明，SiD在多种流匹配模型和不同训练设置下均能直接发挥作用，无需教师/架构调整，稳定性和有效性良好。首次系统性证明了得分蒸馏可泛化至主流文本到图像流匹配模型，并解决了以往对于稳定性/合理性的疑虑。

Conclusion: 该工作理论上和实证上统一了现有扩散和流匹配模型的蒸馏加速路径，为两类主流生成模型的加速提供了通用可移植的技术方案，并承诺开源相关代码，有望推动生成建模领域进展。

Abstract: Diffusion models achieve high-quality image generation but are limited by
slow iterative sampling. Distillation methods alleviate this by enabling one-
or few-step generation. Flow matching, originally introduced as a distinct
framework, has since been shown to be theoretically equivalent to diffusion
under Gaussian assumptions, raising the question of whether distillation
techniques such as score distillation transfer directly. We provide a simple
derivation -- based on Bayes' rule and conditional expectations -- that unifies
Gaussian diffusion and flow matching without relying on ODE/SDE formulations.
Building on this view, we extend Score identity Distillation (SiD) to
pretrained text-to-image flow-matching models, including SANA, SD3-Medium,
SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show
that, with only modest flow-matching- and DiT-specific adjustments, SiD works
out of the box across these models, in both data-free and data-aided settings,
without requiring teacher finetuning or architectural changes. This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators. We will make the PyTorch implementation publicly
available.

</details>


### [340] [VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning](https://arxiv.org/abs/2509.25151)
*Zhaozhi Wang,Tong Zhang,Mingyue Guo,Yaowei Wang,Qixiang Ye*

Main category: cs.CV

TL;DR: 提出了一种名为VideoAnchor的新模块，用于提升多模态大语言模型在视觉-空间推理任务中的表现，通过强化视觉线索，实现了无需重新训练即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型虽然在视觉-语言对齐上取得了较大进展，但在视觉-空间推理能力上表现不足。其主要原因是现有的注意力机制中，视觉token容易被语言token压制，导致模型难以在不同帧间持续识别一致的视觉线索。

Method: 作者将稀疏子空间聚类中的自表达性质与Transformer中的注意力机制联系起来，提出VideoAnchor模块。该模块利用子空间亲和性，在无需重新训练的前提下，加强不同帧间的视觉结构一致性，实现视觉线索的有效锚定。

Result: 在多个基准任务和基础模型上，VideoAnchor都带来了性能提升。例如在VSI-Bench和Video-MME（空间相关任务）上，采用InternVL2-8B和Qwen2.5VL-72B模型分别提升了3.2%和4.6%。此外，定性分析也显示该方法能获得更清晰的子空间划分及更强的视觉锚定能力。

Conclusion: VideoAnchor模块能够作为即插即用的组件，显著提升MLLMs在视觉空间推理任务中的能力，无需模型重新训练，具备很好的通用性和实际应用价值。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in
vision-language alignment, yet they remain limited in visual-spatial reasoning.
We first identify that this limitation arises from the attention mechanism:
visual tokens are overshadowed by language tokens, preventing the model from
consistently recognizing the same visual cues across frames. To address this
challenge, we draw a novel connection between the self-expressiveness property
in sparse subspace clustering and the attention mechanism in Transformers.
Building on this insight, we propose VideoAnchor, a plug-and-play module that
leverages subspace affinities to reinforce visual cues across frames without
retraining, effectively anchoring attention to shared visual structures.
Extensive experiments across benchmarks and backbone models show consistent
performance gains -- $e.g.$, 3.2% and 4.6% improvements on VSI-Bench and
Video-MME (spatial-related tasks) with InternVL2-8B and Qwen2.5VL-72B -- while
qualitative analyses demonstrate more coherent subspace partitions and stronger
visual grounding. Our codes will be made public available at
https://github.com/feufhd/VideoAnchor.

</details>


### [341] [Rolling Forcing: Autoregressive Long Video Diffusion in Real Time](https://arxiv.org/abs/2509.25161)
*Kunhao Liu,Wenbo Hu,Jiale Xu,Ying Shan,Shijian Lu*

Main category: cs.CV

TL;DR: 提出了一种新的流式视频生成技术Rolling Forcing，可以长时间生成高质量、实时且连贯的视频流，并有效减少误差累积。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成方法在长时间生成时容易出现严重的误差累积，导致视频质量随着生成过程显著下降，亟需设计能够抑制误差增长的生成方法。

Method: Rolling Forcing 包含三项创新设计：（1）联合去噪机制：同时对多个帧进行不同噪声强度的去噪，降低相邻帧之间的严格因果约束，抑制误差传播；（2）attention sink机制：在生成过程中保留初始帧的关键状态作为全局锚点，增强视频流的长期一致性；（3）高效训练算法：在大幅扩展的去噪窗口上进行少步知识蒸馏，使用非重叠窗口缓解由自回归历史导致的暴露偏差。

Result: 广泛实验表明，Rolling Forcing 可在单块GPU上实时流式生成数分钟的视频，且误差累积大幅降低，生成效果优于现有方法。

Conclusion: Rolling Forcing 实现了高质量、低延迟、连贯且长时长的视频流生成，有效解决了流式生成的误差累积问题，具有重要的实际应用价值。

Abstract: Streaming video generation, as one fundamental component in interactive world
models and neural game engines, aims to generate high-quality, low-latency, and
temporally coherent long video streams. However, most existing work suffers
from severe error accumulation that often significantly degrades the generated
stream videos over long horizons. We design Rolling Forcing, a novel video
generation technique that enables streaming long videos with minimal error
accumulation. Rolling Forcing comes with three novel designs. First, instead of
iteratively sampling individual frames, which accelerates error propagation, we
design a joint denoising scheme that simultaneously denoises multiple frames
with progressively increasing noise levels. This design relaxes the strict
causality across adjacent frames, effectively suppressing error growth. Second,
we introduce the attention sink mechanism into the long-horizon stream video
generation task, which allows the model to keep key value states of initial
frames as a global context anchor and thereby enhances long-term global
consistency. Third, we design an efficient training algorithm that enables
few-step distillation over largely extended denoising windows. This algorithm
operates on non-overlapping windows and mitigates exposure bias conditioned on
self-generated histories. Extensive experiments show that Rolling Forcing
enables real-time streaming generation of multi-minute videos on a single GPU,
with substantially reduced error accumulation.

</details>


### [342] [Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models](https://arxiv.org/abs/2509.25162)
*Bowei Chen,Sai Bi,Hao Tan,He Zhang,Tianyuan Zhang,Zhengqi Li,Yuanjun Xiong,Jianming Zhang,Kai Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种将预训练视觉编码器对齐为潜变量扩散模型的图像生成分词器的新方法，显著提升了模型的收敛速度及生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型中的分词器通常通过从零训练变分自编码器（VAE），但VAE更强调细节，缺乏对高层语义结构的建模能力。作者希望利用基础视觉编码器的丰富语义特征，提高潜变量图像分词器的能力。

Method: 论文提出三阶段对齐策略：1）冻结预训练视觉编码器，训练适配器和解码器，建立语义潜在空间；2）联合优化所有组件，加入语义保持损失，使编码器在捕获感知细节时也保留高层语义信息；3）优化解码器以提升重建质量。

Result: 在ImageNet 256×256数据集上，新分词器加速了扩散模型的收敛（仅64个epoch即达gFID 1.90），并在有无分类器自由引导情况下均提升生成质量。在LAION大规模数据上，基于该分词器的20亿参数文本到图像模型一致优于同等训练步长的FLUX VAE。

Conclusion: 本方法简单、具备扩展性，并首次提出了有语义基础的连续分词器设计范式，有效促进了扩散模型在图像生成领域的表现。

Abstract: In this work, we propose aligning pretrained visual encoders to serve as
tokenizers for latent diffusion models in image generation. Unlike training a
variational autoencoder (VAE) from scratch, which primarily emphasizes
low-level details, our approach leverages the rich semantic structure of
foundation encoders. We introduce a three-stage alignment strategy: (1) freeze
the encoder and train an adapter and a decoder to establish a semantic latent
space; (2) jointly optimize all components with an additional semantic
preservation loss, enabling the encoder to capture perceptual details while
retaining high-level semantics; and (3) refine the decoder for improved
reconstruction quality. This alignment yields semantically rich image
tokenizers that benefit diffusion models. On ImageNet 256$\times$256, our
tokenizer accelerates the convergence of diffusion models, reaching a gFID of
1.90 within just 64 epochs, and improves generation both with and without
classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model
trained with our tokenizer consistently outperforms FLUX VAE under the same
training steps. Overall, our method is simple, scalable, and establishes a
semantically grounded paradigm for continuous tokenizer design.

</details>


### [343] [YOLO26: Key Architectural Enhancements and Performance Benchmarking for Real-Time Object Detection](https://arxiv.org/abs/2509.25164)
*Ranjan Sapkota,Rahul Harsha Cheppally,Ajay Sharda,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文介绍了Ultralytics最新发布的YOLO26在实时边缘端目标检测中的关键架构创新与性能表现。通过与前几代YOLO系列的对比，展示了其在效率、精度和部署灵活性上的突破。


<details>
  <summary>Details</summary>
Motivation: YOLO26旨在突破YOLO系列在低功耗设备上的效率与精度瓶颈，满足边缘计算和实时物体检测对轻量化和高性能的双重需求。

Method: YOLO26采用多项架构创新，如端到端无NMS推理、移除DFL简化模型导出、引入ProgLoss与STAL提升稳定性与小目标检测能力，并借鉴大模型训练思路引入MuSGD优化器。同时，在NVIDIA Orin Jetson等边缘设备上进行性能基准测试，并与YOLOv8、YOLO11、YOLOv12、YOLOv13进行全面对比。

Result: YOLO26在边缘设备上展现出显著优于前代模型的效率、准确率和部署适应性，特别在小目标检测和推理速度等方面有明显提升。

Conclusion: YOLO26作为YOLO系列的重要里程碑，在边缘端目标检测领域带来了架构层面的创新和应用层面的突破，为未来相关研究和工业部署奠定了坚实基础。

Abstract: This study presents Key Architectural Enhancements and Performance
Benchmarking of Ultralytics YOLO26 for real-time edge object detection,
providing a comprehensive overview of the design principles of YOLO26,
technological advances, and deployment readiness. YOLO26, released in September
2025 by Ultralytics, represents the newest and most cutting-edge member of the
You Only Look Once (YOLO) family, engineered to push the boundaries of
efficiency and accuracy on edge and low-power devices. This paper highlights
architectural innovations in YOLO26, including end-to-end NMS-free inference,
removal of Distribution Focal Loss (DFL) for streamlined exports, introduction
of ProgLoss and Small-Target-Aware Label Assignment (STAL) for improved
stability and small-object detection, and the adoption of the MuSGD optimizer
inspired by large language model training. In addition, we report performance
benchmarks for YOLO26 across edge devices, specifically NVIDIA Orin Jetson
platforms, and compare results against YOLOv8 and YOLO11 (previous Ultralytics
releases) as well as YOLOv12 and YOLOv13, which bridged the lineage between
YOLO11 and YOLO26. Our comparative analysis highlights superior efficiency of
YOLO26, accuracy, and deployment versatility, establishing it as a pivotal
milestone in the YOLO evolution.

</details>


### [344] [Personalized Vision via Visual In-Context Learning](https://arxiv.org/abs/2509.25172)
*Yuxin Jiang,Yuchao Gu,Yiren Song,Ivor Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了一种名为PICO（Personalized In-Context Operator）的新型视觉个性化方法，能够在无需再次训练的情况下，仅基于单样本示例实现对新任务的泛化和迁移。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型虽在大数据集上表现优异，但很难对测试时用户自定义的目标或新颖任务进行个性化。现有个性化方法依赖昂贵的微调和合成数据，灵活性不足，任务类型有限，限制了实用性。因此，亟需一种高效且通用的视觉个性化方法。

Method: 提出PICO框架，将扩散Transformer转化为视觉上下文学习器。只需一个带注释的示例，PICO即可推断底层转化规则，并应用到新输入，无需再训练。为提升泛化能力，作者构建了多样化的小型数据集VisRel，并通过引入注意力引导的seed scorer优化推理效率与可靠性。

Result: 在大量实验中，PICO在识别和生成任务中均超越了微调和合成数据等主流基线，在适应新颖、用户定义任务方面表现出极强的灵活性和泛化能力。

Conclusion: PICO实现了视觉个性化场景下更高效、更灵活的上下文学能力，有效突破了传统个性化方法的局限，并展示了强大的任务适应与迁移表现。

Abstract: Modern vision models, trained on large-scale annotated datasets, excel at
predefined tasks but struggle with personalized vision -- tasks defined at test
time by users with customized objects or novel objectives. Existing
personalization approaches rely on costly fine-tuning or synthetic data
pipelines, which are inflexible and restricted to fixed task formats. Visual
in-context learning (ICL) offers a promising alternative, yet prior methods
confine to narrow, in-domain tasks and fail to generalize to open-ended
personalization. We introduce Personalized In-Context Operator (PICO), a simple
four-panel framework that repurposes diffusion transformers as visual
in-context learners. Given a single annotated exemplar, PICO infers the
underlying transformation and applies it to new inputs without retraining. To
enable this, we construct VisRel, a compact yet diverse tuning dataset, showing
that task diversity, rather than scale, drives robust generalization. We
further propose an attention-guided seed scorer that improves reliability via
efficient inference scaling. Extensive experiments demonstrate that PICO (i)
surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to
novel user-defined tasks, and (iii) generalizes across both recognition and
generation.

</details>


### [345] [Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding](https://arxiv.org/abs/2509.25177)
*Bingkui Tong,Jiaer Xia,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为Layer Contrastive Decoding（LayerCD）的方法，用于减少多模态大语言模型（MLLMs）在图像理解任务中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在理解和推理图像内容时，常常会出现幻觉现象，即生成的结果虽然语言流畅，但与输入图像内容不符，具体表现为对象、属性和关系上的错误。需要一种简单有效的方法来缓解幻觉现象。

Method: 作者提出了Layer Contrastive Decoding（LayerCD）方法。该方法基于观察：浅层视觉特征更易导致幻觉，因为其只捕获了偏见和低级信息。LayerCD通过对比视觉编码器浅层和深层提取的特征分别生成的输出分布，来筛除更可能带来幻觉的结果。

Result: 在两个主流幻觉基准上进行了大量实验，结果显示LayerCD方法明显优于当前最先进的技术。

Conclusion: LayerCD是一种简单有效的解决幻觉问题的方法，能够提升多模态大语言模型在视觉理解任务中的准确性和可靠性。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive perception and
reasoning capabilities, yet they often suffer from hallucinations -- generating
outputs that are linguistically coherent but inconsistent with the context of
the input image, including inaccuracies in objects, attributes, and relations.
To address this challenge, we propose a simple approach called Layer
Contrastive Decoding (LayerCD). Our design is motivated by the observation that
shallow visual features are much more likely than deep visual features to cause
an MLLM to hallucinate as they only capture biased, low-level information that
is insufficient for high-level reasoning. Therefore, LayerCD aims to filter out
hallucinations by contrasting the output distributions generated from visual
features of different levels, specifically those from the shallow and deep
layers of the vision encoder, respectively. We conduct extensive experiments on
two hallucination benchmarks and show that LayerCD significantly outperforms
current state-of-the-art. The code for LayerCD is available at
https://github.com/maifoundations/LayerCD .

</details>


### [346] [GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs](https://arxiv.org/abs/2509.25178)
*Aryan Yazdan Parast,Parsa Hosseini,Hesam Asadollahzadeh,Arshia Soltani Moakhar,Basim Azam,Soheil Feizi,Naveed Akhtar*

Main category: cs.CV

TL;DR: 本文提出了一种全自动的模型GHOST，通过主动生成诱发幻觉的图像，系统性测试多模态大语言模型（MLLMs）的对象幻觉问题。实验结果显示，GHOST能显著提高幻觉发现率并揭示在其他模型上的迁移性漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型常常错误地感知图像中不存在的对象（对象幻觉），而现有的评估方法基于静态、固定的视觉场景，难以全面发现模型的特定或未预料到的幻觉弱点。缺乏灵活、自动化的压力测试机制限制了对模型可靠性的深入研究。

Method: 提出GHOST方法：在无监督和无需先验知识的设定下，直接在图像嵌入空间优化，诱使模型产生对象幻觉，同时利用扩散模型生成外观自然、接近原图、但模型易被误导的新图像。该方法完全自动，无需人工干预。

Result: GHOST在多个MLLM及推理模型（如GLM-4.1V-Thinking）上测试，达到超过28%的对象幻觉率，相较传统数据驱动方法（约1%）显著提升。实验还发现，针对一个模型优化的图像对其它模型（如GPT-4o）也能诱发高达66.5%的幻觉率。生成图像通过定量指标和人工评测均为高质量且无目标对象。

Conclusion: GHOST不仅是一种有效的诊断工具，能够系统挖掘并量化多模态模型的对象幻觉问题，还可用于图像微调，显著减弱幻觉发生率，有助于提升多模态系统实际应用中的可靠性。

Abstract: Object hallucination in Multimodal Large Language Models (MLLMs) is a
persistent failure mode that causes the model to perceive objects absent in the
image. This weakness of MLLMs is currently studied using static benchmarks with
fixed visual scenarios, which preempts the possibility of uncovering
model-specific or unanticipated hallucination vulnerabilities. We introduce
GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method
designed to stress-test MLLMs by actively generating images that induce
hallucination. GHOST is fully automatic and requires no human supervision or
prior knowledge. It operates by optimizing in the image embedding space to
mislead the model while keeping the target object absent, and then guiding a
diffusion model conditioned on the embedding to generate natural-looking
images. The resulting images remain visually natural and close to the original
input, yet introduce subtle misleading cues that cause the model to
hallucinate. We evaluate our method across a range of models, including
reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success
rate exceeding 28%, compared to around 1% in prior data-driven discovery
methods. We confirm that the generated images are both high-quality and
object-free through quantitative metrics and human evaluation. Also, GHOST
uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce
hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on
our images mitigates hallucination, positioning GHOST as both a diagnostic and
corrective tool for building more reliable multimodal systems.

</details>


### [347] [DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space](https://arxiv.org/abs/2509.25180)
*Wenkun He,Yuchao Gu,Junyu Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Haocheng Xi,Muyang Li,Ligeng Zhu,Jincheng Yu,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 本文提出了一种名为DC-Gen的通用框架，通过在潜在空间进行深度压缩，有效加速文本到图像的扩散模型，尤其适用于高分辨率（如4K）图像生成，显著减少生成延迟。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型虽能生成高质量图片，但在高分辨率任务尤其是4K图像生成时效率低下。以往工作虽加速了部分流程，但很少解决潜在空间冗余导致的效率问题。为此，作者提出要高效压缩潜在空间，从根源提升推理速度。

Method: DC-Gen采用高效的后训练流程，不从头训练，而是在预训练模型上，用轻量的嵌入对齐训练消除基础模型潜在空间与压缩空间之间的表示差距，然后仅需极少量LoRA微调即可恢复原始生成质量。

Result: 经实验证明，DC-Gen在SANA及FLUX.1-Krea两大数据集上达到与基础模型相当的生成质量，同时加速效果显著：在NVIDIA H100 GPU上，DC-Gen-FLUX生成4K图像延迟降低53倍，联合NVFP4 SVDQuant后，在NVIDIA 5090 GPU上仅需3.5秒生成，延迟总降至基础模型的1/138。

Conclusion: DC-Gen在不损失生成质量的前提下，实现了高分辨率文本到图像模型的大幅加速，为相关应用提供了更高效的技术路径。

Abstract: Existing text-to-image diffusion models excel at generating high-quality
images, but face significant efficiency challenges when scaled to high
resolutions, like 4K image generation. While previous research accelerates
diffusion models in various aspects, it seldom handles the inherent redundancy
within the latent space. To bridge this gap, this paper introduces DC-Gen, a
general framework that accelerates text-to-image diffusion models by leveraging
a deeply compressed latent space. Rather than a costly training-from-scratch
approach, DC-Gen uses an efficient post-training pipeline to preserve the
quality of the base model. A key challenge in this paradigm is the
representation gap between the base model's latent space and a deeply
compressed latent space, which can lead to instability during direct
fine-tuning. To overcome this, DC-Gen first bridges the representation gap with
a lightweight embedding alignment training. Once the latent embeddings are
aligned, only a small amount of LoRA fine-tuning is needed to unlock the base
model's inherent generation quality. We verify DC-Gen's effectiveness on SANA
and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve
quality comparable to their base models but with a significant speedup.
Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on
the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a
4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total
latency reduction of 138x compared to the base FLUX.1-Krea model. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [348] [DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder](https://arxiv.org/abs/2509.25182)
*Junyu Chen,Wenkun He,Yuchao Gu,Yuyang Zhao,Jincheng Yu,Junsong Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Muyang Li,Haocheng Xi,Ligeng Zhu,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 本论文提出了DC-VideoGen，这是一个可用于任何预训练视频扩散模型的后训练加速框架，能够大幅提高视频生成的效率，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 目前，视频扩散模型在生成高质量视频方面表现优异，但推理速度慢、计算资源消耗大，尤其是高清视频生成限制较多，亟需高效、通用的加速与压缩方法。

Method: 作者提出了两大创新：（1）基于深度压缩的视频自编码器，采用新颖的分块因果时序结构，实现32x/64x空间、4x时间压缩；（2）AE-Adapt-V自适应策略，实现预训练模型高效、稳定地迁移到新潜空间。该方法通过轻量级微调完成。

Result: 用DC-VideoGen对预训练Wan-2.1-14B模型适配，仅需10天H100 GPU计算。加速后模型推理延迟最高降低14.8倍，生成质量无明显损失，可在单GPU上生成2160x3840分辨率的视频。

Conclusion: DC-VideoGen可高效适配多种已存在的视频生成模型，大幅提升推理效率与分辨率，具有实际应用价值。

Abstract: We introduce DC-VideoGen, a post-training acceleration framework for
efficient video generation. DC-VideoGen can be applied to any pre-trained video
diffusion model, improving efficiency by adapting it to a deep compression
latent space with lightweight fine-tuning. The framework builds on two key
innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal
temporal design that achieves 32x/64x spatial and 4x temporal compression while
preserving reconstruction quality and generalization to longer videos; and (ii)
AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer
of pre-trained models into the new latent space. Adapting the pre-trained
Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100
GPU. The accelerated models achieve up to 14.8x lower inference latency than
their base counterparts without compromising quality, and further enable
2160x3840 video generation on a single GPU. Code:
https://github.com/dc-ai-projects/DC-VideoGen.

</details>


### [349] [PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos](https://arxiv.org/abs/2509.25183)
*Ting-Hsuan Liao,Haowen Liu,Yiran Xu,Songwei Ge,Gengshan Yang,Jia-Bin Huang*

Main category: cs.CV

TL;DR: PAD3R 提出了一种能从常规单目视频重建可变形3D物体的方法，能够应对剧烈变形和大幅相机运动等高难场景，重建效果优异，适用范围广。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频3D重建方法在处理长视频、大变形、相机广泛移动和视角受限时，效果不理想。作者希望突破这些限制，实现更通用、鲁棒的3D物体重建。

Method: PAD3R 通过训练针对特定对象的姿态估计器（由预训练图像到3D模型监督），优化可变形的3D高斯表达。并结合了全局2D点追踪，生成式先验和可微渲染技术实现端到端重建。

Result: 在大量定性和定量实验中，PAD3R 在对复杂场景（大变形、长序列等）和不同类别对象上都表现出很强的鲁棒性与泛化性，显著优于现有方法。

Conclusion: PAD3R 能实现高保真、泛类别的动态3D物体重建，在动态场景理解和3D内容创作方面展现了强大潜力。

Abstract: We present PAD3R, a method for reconstructing deformable 3D objects from
casually captured, unposed monocular videos. Unlike existing approaches, PAD3R
handles long video sequences featuring substantial object deformation,
large-scale camera movement, and limited view coverage that typically challenge
conventional systems. At its core, our approach trains a personalized,
object-centric pose estimator, supervised by a pre-trained image-to-3D model.
This guides the optimization of deformable 3D Gaussian representation. The
optimization is further regularized by long-term 2D point tracking over the
entire input video. By combining generative priors and differentiable
rendering, PAD3R reconstructs high-fidelity, articulated 3D representations of
objects in a category-agnostic way. Extensive qualitative and quantitative
results show that PAD3R is robust and generalizes well across challenging
scenarios, highlighting its potential for dynamic scene understanding and 3D
content creation.

</details>


### [350] [PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images](https://arxiv.org/abs/2509.25185)
*Shuoshuo Zhang,Zijian Li,Yizhen Zhang,Jingjing Fu,Lei Song,Jiang Bian,Jun Zhang,Yujiu Yang,Rui Wang*

Main category: cs.CV

TL;DR: PixelCraft 提出了一种多智能体系统，结合高保真图像处理与灵活推理流程，显著提升多模态大模型对结构化图片（如图表、几何图）的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在处理结构化图片时，经常会因感知细节疏漏导致推理错误。现有方法因图像处理精度低且推理流程僵化，难以应对复杂的结构化图片任务。

Method: PixelCraft 设计了一套包含派发器、规划者、推理者、批判者和视觉工具代理的多智能体系统。为提升图像处理精度，构建高质量语料库并微调 MLLM 作为定位模型，将像素级定位与传统计算机视觉算法结合。创新地引入三阶段工作流（工具选择、智能体讨论、自我批判），并通过图像记忆机制实现灵活非线性推理。

Result: 在复杂的图表和几何图基准测试上，PixelCraft 显著提升了先进多模态大模型的视觉推理表现，优于现有方法。

Conclusion: PixelCraft 为结构化图片的多模态推理树立了新标准，解决了传统方法在图像处理精度与推理灵活性上的瓶颈。

Abstract: Structured images (e.g., charts and geometric diagrams) remain challenging
for multimodal large language models (MLLMs), as perceptual slips can cascade
into erroneous conclusions. Intermediate visual cues can steer reasoning;
however, existing cue-based methods are constrained with low-fidelity image
processing and linear, rigid reasoning patterns, limiting their effectiveness
on complex structured-image tasks. In this paper, we propose PixelCraft, a
novel multi-agent system for high-fidelity image processing and flexible visual
reasoning on structured images. The system comprises a dispatcher, a planner, a
reasoner, critics, and a set of visual tool agents. To achieve high-fidelity
processing, we construct a high-quality corpus and fine-tune an MLLM into a
grounding model, whose pixel-level localizations are integrated with
traditional computer vision (CV) algorithms in tool agents. Building on this
foundation, PixelCraft facilitates flexible visual reasoning through a dynamic
three-stage workflow of tool selection, agent discussion, and self-criticism.
Moreover, unlike prior linear reasoning patterns that simply append historical
images, PixelCraft maintains an image memory to allow the planner to adaptively
revisit earlier visual steps, explore alternative reasoning branches, and
dynamically adjust the reasoning trajectory during discussion. Extensive
experiments on challenging chart and geometry benchmarks demonstrate that
PixelCraft significantly improves visual reasoning performance for advanced
MLLMs, setting a new standard for structured image reasoning. Our code will be
available at https://github.com/microsoft/PixelCraft.

</details>


### [351] [FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation](https://arxiv.org/abs/2509.25187)
*Yunyang Ge,Xinhua Cheng,Chengshu Zhao,Xianyi He,Shenghai Yuan,Bin Lin,Bin Zhu,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像到视频（I2V）生成方法FlashI2V，通过创新性地减少条件图像信息泄露问题，使生成视频在一致性与泛化能力上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法中，条件图像信息容易被模型短路，导致慢动作、色彩不一致等问题，且对域外数据泛化能力差。因此需要新的方法解决条件图像信息泄露导致的性能下降。

Method: 提出了FlashI2V，包括两个关键点：（1）Latent Shifting，利用隐空间信息调整，减弱条件图像的直接影响；（2）Fourier Guidance，用傅里叶变换获得高频特征，加速模型收敛并可控细节生成。

Result: 实验证明，该方法能有效克服条件图像信息泄露，特别是在域外数据上的生成效果优于现有各类I2V方法，动态评分超过主流大模型，同时参数量更少。

Conclusion: FlashI2V为图像到视频生成任务提供了更有效的解决方案，在提升泛化性能和视频细节表现方面显示出优越性，具有良好的实际应用前景。

Abstract: In Image-to-Video (I2V) generation, a video is created using an input image
as the first-frame condition. Existing I2V methods concatenate the full
information of the conditional image with noisy latents to achieve high
fidelity. However, the denoisers in these methods tend to shortcut the
conditional image, which is known as conditional image leakage, leading to
performance degradation issues such as slow motion and color inconsistency. In
this work, we further clarify that conditional image leakage leads to
overfitting to in-domain data and decreases the performance in out-of-domain
scenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named
FlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists
of: (1) Latent Shifting. We modify the source and target distributions of flow
matching by subtracting the conditional image information from the noisy
latents, thereby incorporating the condition implicitly. (2) Fourier Guidance.
We use high-frequency magnitude features obtained by the Fourier Transform to
accelerate convergence and enable the adjustment of detail levels in the
generated video. Experimental results show that our method effectively
overcomes conditional image leakage and achieves the best generalization and
performance on out-of-domain data among various I2V paradigms. With only 1.3B
parameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V,
surpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Github page:
https://pku-yuangroup.github.io/FlashI2V/

</details>


### [352] [Visual Jigsaw Post-Training Improves MLLMs](https://arxiv.org/abs/2509.25190)
*Penghao Wu,Yushan Zhang,Haiwen Diao,Bo Li,Lewei Lu,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个名为Visual Jigsaw的自监督视觉训练框架，通过视觉拼图任务强化多模态大语言模型（MLLMs）的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MMMLs训练过于依赖文本，视觉内容常被简化为获取零散线索，缺乏对于视觉本身的深入理解。文中指出以视觉为中心的后训练对于提升模型视觉感知极为关键。

Method: 提出Visual Jigsaw框架，将视觉输入划分并打乱，要求模型以自然语言的方式还原正确顺序。这一任务可自动生成监督信号，无需额外标注和生成组件，并通过可验证奖励强化学习进行优化。该方法适用于图像、视频和3D数据。

Result: 实验表明，采用Visual Jigsaw训练后，MLLMs在精细感知、时序推理和三维空间理解方面均获得了显著提升。

Conclusion: 自监督视觉任务对提升多模态大语言模型的视觉理解能力前景广阔，有望激发更多以视觉为中心的训练策略研究。

Abstract: Reinforcement learning based post-training has recently emerged as a powerful
paradigm for enhancing the alignment and reasoning capabilities of multimodal
large language models (MLLMs). While vision-centric post-training is crucial
for enhancing MLLMs' intrinsic understanding of visual signals, current
post-training paradigms are predominantly text-centric, where dense visual
inputs are only leveraged to extract sparse cues for text-based reasoning.
There exist a few approaches in this direction, however, they often still rely
on text as an intermediate mediator or introduce additional visual generative
designs. In this work, we introduce Visual Jigsaw, a generic self-supervised
post-training framework designed to strengthen visual understanding in MLLMs.
Visual Jigsaw is formulated as a general ordering task: visual inputs are
partitioned, shuffled, and the model must reconstruct the visual information by
producing the correct permutation in natural language. This naturally aligns
with reinforcement learning from verifiable rewards (RLVR), requires no
additional visual generative components, and derives its supervisory signal
automatically without any annotations. We instantiate Visual Jigsaw across
three visual modalities, including images, videos, and 3D data. Extensive
experiments demonstrate substantial improvements in fine-grained perception,
temporal reasoning, and 3D spatial understanding. Our findings highlight the
potential of self-supervised vision-centric tasks in post-training MLLMs and
aim to inspire further research on vision-centric pretext designs. Project
Page: https://penghao-wu.github.io/visual_jigsaw/

</details>


### [353] [VGGT-X: When VGGT Meets Dense Novel View Synthesis](https://arxiv.org/abs/2509.25191)
*Yang Liu,Chuanchen Luo,Zimo Tang,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本论文关注于3D基础模型（3DFMs）在稠密新视角合成（NVS）中的应用，提出了VGGT-X框架来克服当前模型在VRAM消耗和初始化敏感性训练中的难题，实现无需COLMAP初始化下的高保真NVS与位姿估计，并对剩余差距进行了分析。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3DGS等技术在新视角合成方面取得了显著进展，但依赖传统Structure-from-Motion（SfM）的准确3D属性获得，既慢又容易受限于场景特性。因此，如何实现高效、鲁棒且无需传统SfM初始化的稠密NVS，成为亟需解决的问题。

Method: 作者提出VGGT-X，采用内存高效的VGGT实现以支持千张以上图片，结合自适应全局对齐来提升VGGT输出质量，并设计鲁棒的3DGS训练策略，以应对大规模数据下的VRAM压力和初始化不足导致的训练退化。

Result: 实验显示该方案在无需COLMAP初始化情境下，NVS与位姿估计均达到当前最佳水平，并大大缩小了与COLMAP初始化流水线在合成保真度上的差距。

Conclusion: 本文证实，通过VGGT-X框架可在完全COLMAP-free的条件下实现高质量稠密新视角合成，并为未来3D基础模型和新视角合成的发展提供了方法论支持及研究线索。

Abstract: We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel
View Synthesis (NVS). Despite significant progress in Novel View Synthesis
powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D
attributes (e.g., camera poses and point clouds) acquired from
Structure-from-Motion (SfM), which is often slow and fragile in low-texture or
low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over
the traditional pipeline and great potential for online NVS. But most of the
validation and conclusions are confined to sparse-view settings. Our study
reveals that naively scaling 3DFMs to dense views encounters two fundamental
barriers: dramatically increasing VRAM burden and imperfect outputs that
degrade initialization-sensitive 3D training. To address these barriers, we
introduce VGGT-X, incorporating a memory-efficient VGGT implementation that
scales to 1,000+ images, an adaptive global alignment for VGGT output
enhancement, and robust 3DGS training practices. Extensive experiments show
that these measures substantially close the fidelity gap with
COLMAP-initialized pipelines, achieving state-of-the-art results in dense
COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of
remaining gaps with COLMAP-initialized rendering, providing insights for the
future development of 3D foundation models and dense NVS. Our project page is
available at https://dekuliutesla.github.io/vggt-x.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [354] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 本文提出了一种不依赖人工监督的方法，通过模型在对弱势群体标注信息时的分类不确定性来衡量内容审核模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的内容审核系统在社交媒体中被广泛应用，但它们容易固化种族和社会偏见。虽已有评测资源和基准语料，但如何对内容审核模型的公平性进行量化仍存在难题。

Method: 作者提出一种无监督方法，基于conformal prediction技术，利用模型在对女性和非白人群体标注信息时的预测不确定性作为偏见的代理指标，并对比这一指标与传统性能指标（如F1分数）的差异，分析了11个模型。

Result: 实验发现，一些预训练模型虽然能高准确率预测少数群体标注的标签，但其预测置信度较低。

Conclusion: 通过衡量模型置信度，可以揭示弱势群体在预训练模型中的代表性，并为后续模型去偏提供有力依据，有助于提升自动内容审核的公平性。

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [355] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 该论文提出了AccessEval基准，系统性地评估不同大模型对残障相关查询的表现，发现存在显著偏见和事实错误，对现实决策与残障用户有实际负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于现实生活，但它们在处理涉及残障的查询上存在表现差异与潜在偏见。论文动机在于系统性地揭示这些模型对于不同障碍类型在各领域具体存在哪些歧视与错误，从而推动技术公平性。

Method: 作者构建了AccessEval基准，涵盖6个现实领域和9种残障类型，设计成对中性与残障意识查询；共评测21种开源和闭源大模型，采用情感、社会感知与事实准确性指标对输出结果进行分析。

Result: 模型在应对“残障意识”查询时，通常表现出更负面的语气、更多刻板印象及更高事实错误率。表现差异因领域和障碍类型而异，听力、言语和行动障碍用户受影响更大。

Conclusion: 大模型存在根深蒂固的“能力主义”偏见，这些偏见在现实决策场景中可对残障用户造成实际伤害。论文强调在日常应用中应重视偏见缓解，将技术评估与用户真实利益紧密结合。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [356] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为RAR^2的新方法，通过联合增强推理与检索，有效提升了大语言模型在医学问答领域的表现。该方法在多个医学领域基准数据集上超越了传统RAG方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的基于检索增强生成（RAG）方法能够缓解大语言模型的知识盲区和幻觉问题，但它们在复杂医学问题上仍显不足，原因在于只关注表层输入，未能有效建模推理过程，导致相关医学知识的检索和整合能力有限。

Method: RAR^2采用了联合增强的方式：一方面通过构建推理过程揭示隐式知识需求，引导检索与答案生成；另一方面借助混合偏好数据对训练集，并采用直接偏好优化（DPO）进行训练。此外，设计了两种推理与检索的测试扩展策略，进一步探索方法的上限。

Result: RAR^2在多个生物医学问答数据集上的实验结果表明，其表现优于现有的RAG方法，包括未经过微调以及经过微调的基线。

Conclusion: RAR^2通过有效结合推理与检索环节，能够更好地获取和整合临床相关知识，显著提升医学问答任务中的表现，对未来临床辅助决策的智能化具有积极意义。

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [357] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: 本文提出了一个新基准TRUEBench，用于更真实地评估大语言模型（LLMs）作为生产力助手时的指令遵循能力。该基准多语言支持，能反映更复杂、真实的用户需求和对话情况。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评测基准在多语言、多轮对话、隐式约束等方面不足，无法准确衡量LLM在真实生产力场景中的表现。因此需要一个更贴合实际需求的基准。

Method: 研究者构建了TRUEBench，覆盖12种语言，包括多轮多语言指令、显性和隐性约束的严格评判标准，以及复杂的多轮对话场景；对约束进行LLM自动化校验，保证评价标准可靠。

Result: 实验结果显示TRUEBench比现有基准难度更高。例如，OpenAI o1模型整体通过率仅为69.07%。

Conclusion: TRUEBench能更全面地揭示LLM作为生产力助手的能力和局限，对后续研究和模型改进具有现实指导意义。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [358] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 本文提出了一种动态注意力融合（DAF）框架，将文本和语音两种模态的信息通过自适应注意力机制融合，以提升多模态情感分析的准确性，无需对底层编码器微调即可超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析大多只利用文本，忽视了语音中的非语言线索（如语调、韵律），限制了对真实情感表达的捕捉。本文旨在充分整合文本和声学特征，改善情感分析的表现。

Method: 作者设计了DAF框架，联合使用预训练语言模型输出的文本嵌入和语音编码器的声学特征，并引入动态注意力机制，使模型能针对每一句话自适应地分配不同模态的权重。所有编码器均保持冻结，无需微调。

Result: DAF模型在大规模多模态数据集上，F1分数显著提升，预测误差减少，优于静态融合和单一模态基线。消融实验表明，动态权重策略对处理复杂情感输入至关重要。

Conclusion: DAF有效整合了文本和非文本信息，为情感分析和情感计算应用（如情绪识别、心理健康评估、人机交互等）提供了更稳健的基础。

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [359] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 本文提出了一种提升掩码扩散语言模型（diffusion LM）生成效率的方法，在每次模型前向传播时能更高效地并行解码多个token, 并保持采样质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型可以通过并行解码提升生成速度，但过多并行会导致生成结果偏离真实联合分布，造成准确率下降。因此需要一种方法，在加速生成的同时尽可能保持与联合分布的一致性。

Method: 作者提出在已有的大型扩散语言模型之上，新增一个轻量级的单层采样器（sampler）。先进行一次全模型前向传播，然后通过多次只对采样器的前向传播，实现并行解码多个token。训练方法是让采样器模仿全模型的联合采样结果。

Result: 在Dream-7B-Base和Dream-7B-Instruct两个模型上，作者的方法能在每次全模型去噪时并行生成4个token，MAUVE分数达到0.87（传统边缘采样基线仅有0.31），在语言建模及数学和代码任务上效果良好。

Conclusion: 所提出的采样方法能在加速大模型生成的同时，较好地保持输出文本的概率分布质量，有助于扩散语言模型的高效文本生成。

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [360] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 本文提出了一种全自动化的激活引导方法（PAS），能够高效、可控地对语言模型进行后训练，无需人工构建提示或特征标注。在18项任务与多个模型上的实验表明，PAS在行为类任务中有效，但对智能类任务提升有限。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型后训练方法主要分为基于权重和基于提示两类，但权重法成本高、提示法可控性差。激活引导方法（AS）虽具备高效、可控优势，但现有AS往往需手工提示对或特征标注，实用性不足。亟需一种自动化、免人工干预的AS方法提升便捷性与实用性。

Method: 提出Painless Activation Steering（PAS），一套全自动化激活引导方法，无需人工参与，直接基于标注数据集获得激活向量，适用于多种开源大模型。评估了三种模型和18项任务，分析了原始PAS和内省变体iPAS的效果，并考察了其与ICL、SFT等方法的叠加效果。

Result: PAS在行为类任务上表现出可靠的性能提升，但对智能类任务帮助有限。iPAS变体在偏见、道德和对齐任务上分别提升10.1%、5.2%和34.8%。与ICL、SFT等经典方法结合，PAS依然带来额外收益。

Conclusion: PAS为AS方法提供实用化自动化方案，具备速度快、训练存储成本低和应用灵活等优点。实验描绘了AS的适用和局限场景，为实际部署提供了指导。

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [361] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文关注真实世界中多跳问答任务中的多层次歧义问题，发现现有大模型难以有效处理此类问题，并提出了MIRAGE基准与CLARION多智能体方法以推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多跳问题常伴随着复杂的歧义，现有模型在此场景下表现不佳，缺乏针对歧义与多步推理交织情况下的专门评测和算法。

Method: 作者构建了MIRAGE基准集，收集了1142个高质量、涵盖三类歧义（句法、泛指、语义）的多跳问答样本，并用多大模型交叉验证确保数据精确。同时，提出CLARION多智能体推理框架来更好地澄清和解决推理过程中的歧义。

Result: 实验证明，即使是最先进的大语言模型，在MIRAGE上的表现也较差。所提出的CLARION方法在该基准集上的表现显著优于现有方法。

Conclusion: 歧义与多跳推理的结合仍然是大模型的重大挑战，MIRAGE以及CLARION为相关研究提供了重要工具和进展，未来需关注模型在真实复杂推理场景下的适应性和健壮性。

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [362] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 本论文提出了ML2B，这是首个用于多语言机器学习代码生成的基准数据集，通过Kaggle比赛题目多语言翻译，评估大模型应对多语环境下的代码生成能力。实验表明，非英文任务性能降低明显。


<details>
  <summary>Details</summary>
Motivation: 目前ML代码生成领域的基准几乎只关注英文，忽视了机器学习全球化、多语种的实际需求。因此需要一种多语言下统一评测的基准，以推动相关研究。

Method: 作者构建了ML2B基准：选取30个Kaggle比赛，将任务指令翻译成13种语言，涵盖表格、文本和图像任务，并人工校验。采用AIDE自动化框架对生成的端到端数据科学流程进行评估，比较跨语言性能。

Result: 对比实验显示，在非英语任务上，主流大语言模型的代码生成性能下降明显，降幅达15%-45%。

Conclusion: 多语言代码生成仍面临较大挑战，现有大模型在多语言理解与代码生成方面能力不足。ML2B和AIDE等工具可支持更多研究者深入该领域创新和改进。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [363] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: 本文提出了首个多方言阿拉伯语语音仿冒数据集，并系统评估了不同生成模型的欺骗难度与检测方式。


<details>
  <summary>Details</summary>
Motivation: 随着生成式文本转语音技术的发展，分辨真实语音与合成语音日益困难，尤其在研究较少的阿拉伯语及其方言环境中更为突出。现有仿冒检测大多聚焦英语，阿拉伯语领域存在明显研究空白。

Method: 作者构建了多方言阿拉伯语仿冒语音数据集，并设计了一套评估流程：包含使用现代嵌入方法与分类器头、将经典机器学习算法应用于MFCC特征、以及RawNet2架构，结合人工评分的主观质量均值（Mean Opinion Score, MOS）与自动语音识别计算的字错误率（WER），对多种TTS模型的样本进行全面评价。

Result: 实验表明，FishSpeech模型在Casablanca语料上的阿拉伯语语音克隆性能优于其他TTS模型，生成的合成语音样本更具迷惑性和真实性。

Conclusion: FishSpeech作为单一TTS模型能生成最具挑战性的阿拉伯语合成语音，但数据集只依赖单一TTS模型可能降低泛化能力，多模型融合更有助于构建高质量检测基线数据集。

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [364] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 本文提出了一种针对放射报告生成任务的混合策略强化学习算法EditGRPO，通过引入临床驱动的奖励和句级细粒度纠正，有效提升了多模态大模型在该任务下的临床表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然在放射报告生成方面有提升，但其监督微调目标与实际临床有效性不完全对齐，难以充分激发模型的临床推理与文本生成能力。

Method: 提出EditGRPO算法，结合了强化学习中的on-policy探索与off-policy引导，通过在训练过程中加入句级纠正，优化模型学习效率和生成质量。具体在Qwen2.5-VL-3B模型基础上，采用此算法并与传统基线进行比较。

Result: EditGRPO在四个胸片报告生成数据集上的主要指标（CheXbert、GREEN、Radgraph和RATEScore）均超越SFT和原始GRPO方法，平均提升3.4%。在未见过的数据集上表现也更优，平均提升5.9%。

Conclusion: EditGRPO通过设计合理的临床奖励和混合策略，有效解决了强化学习中的探索与采样效率难题，显著提升了放射报告生成的效果及跨领域泛化能力。

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [365] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 提出了一种新的训练方法Critique Reinforcement Learning（CRL），通过引导模型生成批判性评价提升大型语言模型的推理能力，并实验证明其在代码和推理任务上优于传统RL方法。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习（RL）训练范式虽然有效，但主要关注生成回答而缺乏明确培养模型批判与反思能力的机制。近期研究显示显式教会大模型进行批判性评价有益，因此作者希望结合该方向改进RL。

Method: 提出CRL方法，让模型针对（问题，解决方案）对生成评价，并仅依据评价的最终判断（True/False）与真实标签是否一致给予奖励。此外，提出Critique-Coder，将20%的标准RL数据替换为CRL数据进行混合训练，并在多个基准测试上验证。

Result: Critique-Coder在全部评测基准上都优于仅用RL训练的基线模型。特别地，Critique-Coder-8B在LiveCodeBench数据集上超过60%，优于其他推理模型（如DeepCoder-14B和GPT-o1）；在BBEH数据集上的逻辑推理任务亦表现更佳，展现出更好的泛化推理能力。

Conclusion: CRL不仅提升了模型的代码生成能力，同时增强了其批判与推理能力，且这些能力可以迁移到多样任务中。因此，CRL可作为标准RL的有力补充用于提升大模型的推理表现。

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [366] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文发现并研究了一种新型的针对大型语言模型（LLMs）代理的间接提示注入攻击（ChatInject），该攻击通过仿造聊天模板并结合多轮对话，能大幅提升攻击成功率，并能迁移至多种模型，目前的防御措施对此无效。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能代理广泛应用于与外部环境交互，其对外部信息的依赖带来了新的安全威胁。现有研究主要关注明文提示注入，但很少关注更“原生”的聊天模板和多轮对话语境下的注入方式。作者希望揭示和量化这种被忽视的安全风险。

Method: 作者提出ChatInject攻击，将恶意载荷包装成仿真“原生”聊天模板的格式，并进一步设计成多轮劝说性对话，使LLM代理在多轮对话后更易接受和执行恶意操作。通过AgentDojo和InjecAgent实验平台，对不同前沿LLM进行了系统攻击测试。

Result: （1）ChatInject攻击相较传统提示注入大幅提升了攻击成功率（AgentDojo从5.18%提升到32.05%，InjecAgent从15.13%提升到45.90%，多轮对话在InjecAgent上能达到52.33%）；（2）该方法可在不同模型间迁移，并适用于闭源模型；（3）现有基于prompt的防御方案对ChatInject（尤其是多轮变体）基本无效。

Conclusion: LLM代理系统对基于模板和多轮对话的间接注入攻击极为脆弱，当前防御措施未能有效应对，急需新的防护思路以提升安全性。

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [367] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: 本论文提出了一种新型的多轮对话响应选择模型，能够检测并利用与上下文及知识相关的重要信息，从而提高知识驱动对话系统中响应匹配的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的对话系统通常无差别地利用全部上下文和知识内容进行响应匹配，但实际上，很多对话上下文因话题漂移而存在大量无用信息，这些赘余信息会弱化匹配效果，导致性能下降。

Method: 提出RSM-DCK模型：首先用最新的对话上下文作为查询，从词级和话语级预选相关的上下文与知识；其次，让响应候选分别与被筛选的上下文和知识交互；最后联合融合的上下文及响应表示进一步筛选知识，提升最终匹配的相关性。

Result: 在两个基准数据集上的实验表明，该方法优于现有的响应选择模型，能有效检测和利用高相关性的上下文和知识。

Conclusion: 针对多轮知识驱动对话系统，精筛相关内容显著提升响应选择能力，减少无效信息干扰，为对话系统准确性提供了有效的新思路。

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [368] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出了一种用于大语言模型的全新隐式上下文学习方法In-Context Routing (ICR)，可大幅提升零样本和小样本任务表现，并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有隐式上下文学习方法（ICL）大多依赖于从有标签样本或任务定制向量中注入shift vectors，但这种方式没能充分利用ICL的结构机制，且泛化能力弱。作者希望提出一种更具结构性和通用性的隐式ICL方案。

Method: 作者提出In-Context Routing（ICR）方法：不是在残差流中注入shift vector，而是在注意力logits层面抽取和利用可泛化的结构方向，通过一个可学习、输入条件化的router实现对注意力logits的调节，实现一次训练、多次复用。

Result: 在12个不同领域的真实数据集和多个大语言模型上评估，ICR在不依赖任务特定检索或训练的情况下，始终优于以往隐式ICL方法，并在跨领域任务中表现出强健的泛化性。

Conclusion: ICR方法不仅提升了隐式ICL的性能，更推动了ICL方法实际应用的边界，为低成本高表现的Few-shot学习带来了新可能。

Abstract: Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [369] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

TL;DR: 本文对45个大语言模型（LLMs）在8类认知偏见上的表现进行了大规模评估，结果显示LLMs在一定比例上会显示认知偏见，而模型规模和提示细节会影响偏见程度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实世界的决策过程中应用日益广泛，了解其是否以及在多大程度上表现出类似人类的认知偏见变得非常重要。认知偏见会导致判断系统性偏差，若LLM继承了这些偏差，可能影响决策的公正性与可靠性。

Method: 作者提出了一种新评价框架，基于多项选择题并手工整理了220个涉及8种基本认知偏见的决策情景，与心理学专家协作完成数据集。同时，通过人类撰写模板生成多样化提示，系统性评估45个LLM（涉及280万余条模型回复）在不同提示和任务下的偏见表现。

Result: LLMs在各类认知及决策任务中，表现出17.8%至57.3%的与偏见一致行为。模型规模越大（>32B），39.5%的情况下偏见会降低。提示内容越具体，大多数偏见下降高达14.9%。但是在过度归因偏见下，提示细节丰富反而提升8.8%的偏见表现。

Conclusion: LLMs与人类一样会受到认知偏见影响，但合理调整模型规模和提示设计有助于减轻大部分偏见。提示工程和模型规模调整是提升大语言模型决策中公正性的关键方向。

Abstract: As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [370] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

TL;DR: 本文提出了一种结合词典资源的图神经网络模型，用于预测阿拉伯语文档难易度，在共享任务中取得较好表现。


<details>
  <summary>Details</summary>
Motivation: 现有的文档可读性预测在阿拉伯语领域表现有限，且融合细粒度句子关系和丰富词汇资源的方法较少，亟需更有效的解决方案。

Method: 将每篇文档建模为以句子和词元为节点的图结构，通过边表示词汇共现和类别关系。句子节点利用SAMER词典特征和Arabic transformer上下文嵌入进行增强。图神经网络（GNN）和transformer编码器分别训练，推理时通过后期融合结合两者预测。最终通过max pooling聚合句子级结果反映文档最难部分。

Result: 实验表明，融合GNN与transformer的混合方法在多项文档可读性指标上优于单独分支，GNN在句子级可读性预测中依旧更强。

Conclusion: 融合方法在文档级预测中展现优势，GNN适合精细的句子级难度预测。为阿拉伯语可读性建模提供了新的思路和实用工具。

Abstract: We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [371] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言模型推理增强方法HEART，通过情感驱动的迭代自我纠正提示提升模型在复杂推理任务中的表现。HEART利用基于六大基础情感的情感化反馈引导模型探索更优解法，实验验证提升明显，尤其在有权威验证者协助下效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型推理增强方法主要侧重于逻辑或结构改进，忽略了情感反馈对认知表现的调节作用。心理学研究表明情绪能显著影响个体推理与决策能力，因此论文探索在模型推理自我纠正过程中引入情感因素的效果。

Method: 提出HEART框架：利用Dr. Paul Ekman提出的六种基本情感，对模型错误回答给予不同情感色彩的简明反馈。在推理迭代过程中系统性地调整反馈情绪，引导模型规避僵化或错误的推理路径，探索更优答案。并在多个高难度推理基准（如OlympiadBench、Humanity's Last Exam、SimpleQA）上评估。

Result: 实验证明，在有权威验证者介入的情况下，HEART框架大幅提升推理准确率，并显著优于当前主流的自我反思方法。但在无验证者场景下效果不稳定，成为实际应用的瓶颈。

Conclusion: HEART展示了情感驱动提示对语言模型深度推理能力的提升潜力，表明未来机推理研究应不仅关注逻辑优化，也需探索情感机制的融合。但目前需解决无验证者场景下性能难以稳定发挥的问题。

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [372] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: 本文提出将“心智理论”（ToM）融入大语言模型（LLM）对话系统，以提升其社交智能，在对话目标实现上明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型作为社交智能体，尚未充分整合对他人心理状态的理解（心智理论），而这是人类社交智能的核心。本研究希望证明，显性引入心智理论有助于提升对话系统的表现。

Method: 首先，实验证实仅通过引导模型在对话过程中生成对方心理状态，即可提升对话质量。随后，作者提出了ToMAgent（ToMA）系统，将心智理论与对话前瞻结合训练，生成对实现对话目标最有帮助的心理状态。

Result: 在Sotopia社交评测基准下，与多种基线方法相比，ToMA的对话表现更优，能展现更具策略性和目标导向的推理行为，并在适应多回合、长期对话中表现更佳，同时也能与对话伙伴维持较好关系。

Conclusion: 将心智理论显性整合到LLM社交智能体中是一项有效提升其对话能力和社会智能的手段，为构建更强社交能力的LLM奠定了基础。

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [373] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 本文提出了Extract-0，一种专为文档信息抽取优化、拥有70亿参数的语言模型，其性能超越了大量更大参数规模的主流模型。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）在通用任务上表现良好，但针对特定任务如文档信息抽取时，往往需要更高效和资源友好的方法，尤其是在参数量和计算资源受限的情况下，开发小参数、高性能模型尤为关键。

Method: 作者综合采用了合成数据生成、基于LoRA的高效监督微调，以及基于GRPO的强化学习方法进行训练。合成数据流程生成了28万余份训练样本，微调时只更新了模型0.53%的参数，强化阶段则设计了新的语义相似性奖励函数，用于应对抽取任务中的歧义问题。

Result: Extract-0在1000个文档信息抽取基准任务上取得了0.573的平均奖励，明显优于GPT-4.1、o3等更大模型。

Conclusion: 专为特定任务优化的小参数模型在表现上可超越通用大模型，同时极大降低了计算成本，展示了任务专用优化的巨大潜力。

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [374] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: 本文评估了GPT-4o在药物相关任务中的表现，发现其准确性和一致性都较差，尤其是在药物剂型匹配和药物相互作用识别方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在医疗诊断领域表现突出，但其在临床用药推荐和药物管理中的可靠性与一致性尚未被充分评估。本文旨在通过具体基准测试检验GPT-4o在用药推荐场景下的表现。

Method: 选取三项测试：1）药物名称与对应正确剂型的匹配；2）凭内知识与结合网页检索两种方式识别药物间相互作用；3）依据给定药名生成规范的用药医嘱语句。采用TF-IDF余弦相似度、Levenshtein相似度、ROUGE分数及临床医生人工审核评价其准确性。

Result: GPT-4o在药物剂型匹配上表现较差，常漏报和虚构剂型，仅49%的药物能完全正确匹配；剂型越多准确率越低。在药物相互作用识别上，结合搜索时表现优于自身知识库，但无相互作用时反而准确率下降。药品医嘱生成仅65.8%的医嘱无错误。总的来看，三项测试表现均不能令人满意。

Conclusion: GPT-4o尚不适合直接用于临床药物管理，急需依赖临床数据集进一步训练，并建立完善的医学领域评测标准。

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [375] [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
*Melissa Roemmele,John Joon Young Chung,Taewook Kim,Yuqian Sun,Alex Calderwood,Max Kreminski*

Main category: cs.CL

TL;DR: 本论文提出了一种利用生成式AI，将文本故事自动转化为视觉插画的方法，并构建了新的数据集SceneIllustrations，以推动故事插画生成与评估研究。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如文本到图像模型）能够有效地将一种媒介的内容转换为另一种，这对故事讲述特别有意义，因为视觉插画可以增强文本故事的表现力。作者希望探索如何自动化地为故事场景生成插画，以及如何量化和提升图文转换的质量。

Method: 本文提出了一个基于大语言模型（LLM）和文本到图像模型的管道。首先用LLM从原始故事文本中抽取和重构场景描述，然后用这些描述去驱动文本到图像模型生成场景插画。随后，作者在著名故事语料库上应用了多种变体并生成配图，并通过人工标注，获得不同插画之间的质量对比评选。最终整理出SceneIllustrations数据集。

Result: 构建了SceneIllustrations数据集，包含故事文本、基于不同方法生成的插画及相应的人类质量评估。实验结果和数据分析表明，LLM能够从故事文本中有效提取和转述场景知识，提高插画生成和评估的效果。

Conclusion: 本文验证了LLM可作为连接故事文本与插画生成模型的有效接口。所提出的数据集和分析实验为未来跨模态叙事转换（文字转图像）提供了坚实的数据基础和研究工具。

Abstract: Generative AI has established the opportunity to readily transform content
from one medium to another. This capability is especially powerful for
storytelling, where visual illustrations can illuminate a story originally
expressed in text. In this paper, we focus on the task of narrative scene
illustration, which involves automatically generating an image depicting a
scene in a story. Motivated by recent progress on text-to-image models, we
consider a pipeline that uses LLMs as an interface for prompting text-to-image
models to generate scene illustrations given raw story text. We apply
variations of this pipeline to a prominent story corpus in order to synthesize
illustrations for scenes in these stories. We conduct a human annotation task
to obtain pairwise quality judgments for these illustrations. The outcome of
this process is the SceneIllustrations dataset, which we release as a new
resource for future work on cross-modal narrative transformation. Through our
analysis of this dataset and experiments modeling illustration quality, we
demonstrate that LLMs can effectively verbalize scene knowledge implicitly
evoked by story text. Moreover, this capability is impactful for generating and
evaluating illustrations.

</details>


### [376] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: 本文通过引入Bi-Induct课程，以平衡的方式在预训练阶段添加合成诱导（Induction）与反诱导（Anti）数据，系统评估其对大模型上下文学习（ICL）与归纳电路激活的影响。实验发现，虽然诱导数据加速了诱导头的出现，但并不总能带来更好的泛化能力，实际性能与自然数据训练持平甚至略逊。


<details>
  <summary>Details</summary>
Motivation: 当前学界普遍认为通过显式练习诱导电路可提升模型的上下文学习能力，但究竟这种方法是否优于仅采用自然文本进行预训练尚无定论。本文意在探究在相同算力预算下（等FLOPs），合成诱导数据是否能加速模型对ICL能力的习得，以及对归纳电路激活的作用。

Method: 作者提出Bi-Induct课程，将正向复制（诱导）、反向复制（反诱导）及其混合的合成数据注入预训练流，并在0.13B~1B参数规模下的模型上开展等FLOPs训练。性能评价包括few-shot ICL基准、头部激活监测，以及语言建模困惑度。

Result: 实验表明，Bi-Induct确实能在小模型阶段加速诱导头出现，但这一现象未必提升ICL泛化能力或下游任务表现。在LM和ICL基准上，完全自然数据训练的1B模型表现最佳。合成数据带来的困惑度损失随模型规模增加而减小。反诱导数据并未产生有意义的归纳激活，而自然数据训练的模型出现更多负载更集中的归纳头。

Conclusion: 仅加速诱导激活不足以提升ICL能力，关键在于这些电路是否成为模型结构中负责任务的'负载结构'。有效提升ICL需关照电路在预训练中的实际功能负载，而不仅仅是激活的出现。研究强调以机制为导向的数据与诊断方案以培育更有效的模型结构。

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [377] [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
*Jon Gauthier,Canaan Breiss,Matthew Leonard,Edward F. Chang*

Main category: cs.CL

TL;DR: 该论文研究了自监督语音模型在识别自然噪声环境下口语单词时，其内部表征对英语常见词形变化（名词、动词的词形变化）现象的处理方式。发现这些模型的表征在词与规则派生词之间呈现出全局线性几何结构。


<details>
  <summary>Details</summary>
Motivation: 虽然自监督语音模型在复杂环境下表现优秀，但我们仍不了解这些模型内部建构了何种语言学表征来实现任务。论文目的是揭示这类模型在识别单词、处理词形变化（尤其是英语常见的名词和动词屈折变化）时，内部表征的语言学结构。

Method: 分析多种针对单词识别优化的S3M自监督语音模型，研究这些模型对英语高频名词、动词屈折形态（如复数、过去式等）所抽取出的表征向量之间的几何结构。考查其与语音音系单元和词法结构的对应关系。

Result: 模型表征展示出一种可以将名词和动词与其规则词形变化（如复数、过去式）关联起来的全局线性几何关系。然而，这种结构并不直接映射到传统的音位或形态学单元，而是体现了大量词对之间的统计分布关系，这些关系源自但不限于词形变化。

Conclusion: 自监督语音模型可通过捕捉词对间的全局几何关系来支持口语识别，而不必依赖明确分离的音系和形态学表征。这一发现对现有“音系-形态分离处理”的理论提出挑战，也提示人类口语识别或许也依赖类似的分布式表征。

Abstract: Self-supervised speech models can be trained to efficiently recognize spoken
words in naturalistic, noisy environments. However, we do not understand the
types of linguistic representations these models use to accomplish this task.
To address this question, we study how S3M variants optimized for word
recognition represent phonological and morphological phenomena in frequent
English noun and verb inflections. We find that their representations exhibit a
global linear geometry which can be used to link English nouns and verbs to
their regular inflected forms.
  This geometric structure does not directly track phonological or
morphological units. Instead, it tracks the regular distributional
relationships linking many word pairs in the English lexicon -- often, but not
always, due to morphological inflection. These findings point to candidate
representational strategies that may support human spoken word recognition,
challenging the presumed necessity of distinct linguistic representations of
phonology and morphology.

</details>


### [378] [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
*Yue Zhang,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 论文分析了表格问答任务在不同表格结构下的表现，揭示了表格表示方式对模型效果的关键影响，并提出了诊断性评测方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答评测主要针对单一格式的数据，缺乏对多种真实数据结构中模型表现的系统性分析，因此亟需探究表格表示本身对表格问答任务性能的影响。

Method: 提出通过保持表格内容一致，仅改变表格结构的方式，利用口语化处理生成结构化与半结构化表格对，构建新的诊断型基准数据集，并设计多种实验分割（如表格大小、查询复杂度等）以系统测试不同模型在多样表格下的表现。

Result: 实验发现：SQL类方法在结构化表格性能高但在半结构化表格明显下降，大型语言模型更灵活但精准度较低，混合方法在噪声较多的表格下表现最佳。表格规模越大、查询越复杂，这些现象越明显。

Conclusion: 表格表示方式对表格问答模型性能具有决定性影响，目前无单一方法能在所有场景下表现最好，建议采用更健壮的混合方法，为模型选择和设计提供关键参考。

Abstract: Table Question Answering (Table QA) in real-world settings must operate over
both structured databases and semi-structured tables containing textual fields.
However, existing benchmarks are tied to fixed data formats and have not
systematically examined how representation itself affects model performance. We
present the first controlled study that isolates the role of table
representation by holding content constant while varying structure. Using a
verbalization pipeline, we generate paired structured and semi-structured
tables, enabling direct comparisons across modeling paradigms. To support
detailed analysis, we introduce a diagnostic benchmark with splits along table
size, join requirements, query complexity, and schema quality. Our experiments
reveal consistent trade-offs: SQL-based methods achieve high accuracy on
structured inputs but degrade on semi-structured data, LLMs exhibit flexibility
but reduced precision, and hybrid approaches strike a balance, particularly
under noisy schemas. These effects intensify with larger tables and more
complex queries. Ultimately, no single method excels across all conditions, and
we highlight the central role of representation in shaping Table QA
performance. Our findings provide actionable insights for model selection and
design, paving the way for more robust hybrid approaches suited for diverse
real-world data formats.

</details>


### [379] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文提出ADAM，一个评估和提升多模态大语言模型（MLLM）在传记推理方面表现的框架，包含多语种多模态大规模数据和认知层次评测，并引入检索增强方法，显著提升了相关模型的准确性和抗幻觉能力。


<details>
  <summary>Details</summary>
Motivation: 传记知识是事实性知识的重要组成部分，但长期以来缺乏系统性的LLM评测与数据集。尤其是对于知名度较低人物，现有模型常常产生幻觉性错误。因此，需构建针对传记推理的系统化评测与改进方法。

Method: 1. 构建AdamDB：多语种多模态大型数据集，涵盖全球400多万人物及其图像、信息。2. 提出AdamBench：依据Bloom认知分类法设计多层次推理测试，覆盖英语及本地语言。3. 创新AdamRAG（检索增强生成）系统，专为传记场景减缓幻觉。4. 实验对比开源与闭源模型表现、检索增强与图像输入影响。

Result: AdamRAG显著提升开源模型和一定程度提升闭源模型在传记推理任务中的表现，尤其在低阶推理上效果显著；知名度越高，准确率越高；图像信息的加成有限且表现不稳定，检索增强效果更佳。

Conclusion: ADAM作为第一个认知、多模态、跨文化的传记知识评测与提升框架，极大丰富了多语种多模态大模型的传记推理能力，有效降低了幻觉风险，对相关模型的研究和实用化有重要意义。

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [380] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本文介绍了基于大语言模型（LLMs）自动生成的英语和捷克语多体裁平行语料库，便于比对人类写作与LLM文本，所有数据公开可用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为语言学研究提供一个资源，可以系统地对比分析人类写作和大语言模型生成文本的语言特征，并保证语料在体裁、主题、作者和类型上的多样性，实现与现有的人类语料库的可比性。

Method: 作者以BE21和Koditex两个基于Brown Corpus传统的人类参考语料为蓝本，采用OpenAI、Anthropic、Alphabet、Meta和DeepSeek等公司的LLM（从GPT-3到GPT-4.5等），自动生成结构相同的多体裁大规模语料。同时，所有语料均按照Universal Dependencies标准进行分词、词形还原、形态及句法标注，以保证分析的一致性和科学性。

Result: 生成的英语与捷克语语料库分别包含由不同模型生成的子语料，每个模型平均贡献英语864k、捷克语768k词符，总量分别为2700万和2150万词符。所有语料均公开下载，并可通过捷克国家语料库检索使用。

Conclusion: 该工作生成了高质量、多体裁、结构化的英语与捷克语LLM文本语料，可与公认的人类文本语料进行系统比较，将为语言学、AI文本检测等领域研究提供重要数据支持。

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [381] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新型内存增强型智能体ReMemR1，能够高效解决大语言模型在长上下文问答中的信息遗失与推理难题，并通过多层次奖励强化学习方法显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本问答时，关键信息常散布在百万级别token中，现有“边读边记忆”方法易导致信息覆盖、无法回溯、奖励信号稀疏，严重限制了模型的推理与记忆能力。

Method: 作者提出ReMemR1智能体，利用带有回调机制的增强记忆系统，可全历史选择性回溯和非线性推理。同时，提出多层次奖励强化学习（RLMLR），结合最终答案奖励与每步细粒度指导，有效优化记忆使用。

Result: 在长文档问答任务上，ReMemR1及其训练方法比现有记忆增强方法取得了显著性能提升。

Conclusion: 引入回调增强记忆与多层次奖励信号，有效缓解信息退化、加强监督，促进了长上下文推理能力的发展，验证了ReMemR1的有效性。

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [382] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: 本文关注大型语言模型（LLM）在多智能体辩论系统（MADS）中表现出的“拍马屁”（sycophancy）问题。作者提出了sycophancy在MADS场景下的定义与评测指标，分析了不同拍马屁程度对辩论效果的影响，并提出了改善建议。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论系统依赖有建设性的分歧来提升论证质量，但LLM容易表现得过于迎合（即拍马屁），导致辩论过早达成共识，影响系统效果。目前对LLM之间内部拍马屁的研究较少，亟需深入分析。

Method: 作者首先为MADS场景下的拍马屁现象给出正式定义，并设计了新的评测指标。通过对不同拍马屁水平（涉及辩手与评委）在分散与集中式辩论框架下的系统性实验，评估拍马屁对信息交换与最终结论准确性的影响。

Result: 研究发现，拍马屁是多智能体辩论中的核心失败模式，会加速分歧消解、错误达成共识，准确率低于单智能体基线，并揭示了辩手与评委主导的不同失败模式。

Conclusion: 作者据实验提出了一系列可操作的设计原则，以帮助MADS在智能体间互动时实现有效的建设性分歧与合作平衡，减少拍马屁带来的负面影响。

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [383] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 该论文提出了一种无需自评机制、面向不可验证任务的大型语言模型自我提升新方法——语义投票，可以有效降低算力消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 监督数据获取成本不断增加，推动了大型语言模型自我提升技术的发展。传统的多数组投票方法虽然在可验证任务上有效，但难以应用于如翻译等不可验证任务。因此，需要新的方法来减少对高算力且有偏见的自评机制的依赖。

Method: 作者提出语义投票（semantic voting）机制，将可验证任务中的严格匹配原则放宽为语义相似度的软匹配，通过使用轻量级句子嵌入模型评估候选输出的语义相似性，从而生成更可靠的伪标签，提高不可验证任务的自我提升效率。

Result: 实验表明，语义投票方法在多种模型结构和任务上，相较于自评依赖型方法显著提高了计算效率，同时获得了更优的性能。

Conclusion: 语义投票机制为LLMs在不可验证任务上的自我提升提供了一种高效且效果更优的新路径，摆脱了自评过程带来的高昂成本与内在偏差。

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [384] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了一种名为EviPath的证据驱动推理路径合成范式，用于提升RAG（检索增强生成）代理的能力，通过合成高质量推理路径数据，显著提升了大规模语言模型在复杂任务分解和工具使用方面的表现。


<details>
  <summary>Details</summary>
Motivation: 目前RAG代理的发展受限于缺乏流程级监督，无法有效指导任务分解、检索调用和逐步决策。强化学习方案受限于回报稀疏和LLM推理能力有限，现有数据合成方法也无法建模环境互动。

Method: EviPath包含三个核心模块：（1）诱导性子任务规划，将问题分解为子问题并依据依赖关系迭代规划最优解路径；（2）可信子问题解答，利用证据构建代理环境，为每个子问题生成推理和答案；（3）对话式微调，将完整交互轨迹格式化为适合监督微调的对话数据。通过该合成数据训练LLM。

Result: 在主流问答任务上，EviPath合成数据训练的8B参数模型大幅领先SOTA基线，公开领域问答准确率（EM）提升14.7%。

Conclusion: EviPath能系统性增强LLM的复杂推理和工具使用能力，是解决RAG代理流程监督稀缺的有效路径。

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [385] [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
*Esteban Garces Arias,Julian Rodemann,Christian Heumann*

Main category: cs.CL

TL;DR: 该论文提出了利用几何框架（credal sets）分析和量化大语言模型在创造性文本生成任务中的不确定性，发现模型与人类创造力差距较大，且生成策略选择对不确定性影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在创造性任务中存在多种有效输出，但如何理解和量化模型输出的不确定性仍是难题，尤其是在需要对人类创造性变异校准的场景下。

Method: 作者利用credal sets（概率分布的凸包）从几何角度描述和分解神经网络文本生成中的不确定性。在WritingPrompts数据集上，对500个创作提示的每个收集10个人类续写，并用4种模型和5种生成策略生成故事，系统性分析模型与人类之间的创造性差距和不确定性来源。

Result: credal set分析显示，当前最佳模型与人类在创造性输出方面的校准度仅达0.434（最佳为Gemma-2B温度0.7）。模型生成策略对总的不确定性（尤其是epistemic不确定性）的贡献高达39.4%-72.0%。模型规模与校准质量相关性弱，且base与instruction-tuned模型无显著差异。

Conclusion: 提出的几何分析框架能帮助深入理解语言模型在创造性任务中的不确定性，并对未来提升人机创造性对齐提供参考。作者已开放完整实验框架，便于后续研究。

Abstract: Understanding uncertainty in large language models remains a fundamental
challenge, particularly in creative tasks where multiple valid outputs exist.
We present a geometric framework using credal sets - convex hulls of
probability distributions - to quantify and decompose uncertainty in neural
text generation, calibrated against human creative variation. Analyzing 500
creative writing prompts from the WritingPrompts dataset with 10 unique human
continuations each, we evaluate four language models across five decoding
strategies, generating 100,000 stories. Our credal set analysis reveals
substantial gaps in capturing human creative variation, with the best
model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We
decompose total uncertainty into epistemic and aleatoric components, finding
that the choice of decoding strategy contributes 39.4% to 72.0% of total
epistemic uncertainty. Model scale shows weak correlation with calibration
quality and no significant difference exists between base and instruction-tuned
models in calibration quality. Our geometric framework provides actionable
insights for improving generation systems for human-AI creative alignment. We
release our complete experimental framework.

</details>


### [386] [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
*Yuchu Jiang,Yue Cai,Xiangzhong Luo,Jiale Fu,Jiarui Wang,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual aDaptive Cache（d^2Cache）的新方法，以提升扩散式大语言模型（dLLMs）推理效率。该方法无需重新训练，通过近似KV缓存机制显著加速模型推理，并提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型推理效率较低，主要由于其双向注意力机制，无法像自回归模型那样直接利用标准的KV缓存机制，因此需要一种新的加速方法，同时不损失生成质量。

Method: 作者提出了Dual aDaptive Cache（d^2Cache）框架，该方法无需训练，通过两阶段细粒度选择策略，在每一步生成时自适应地更新部分token的KV状态，剩余token的KV状态则被缓存以供复用。此外，该机制支持准从左到右的生成方式，提高序列末尾token的可靠性。

Result: 在LLaDA与Dream这两个代表性扩散大模型上的实验结果显示，d^2Cache可以大幅提升推理速度，并且在生成质量上也能带来一致的提升。

Conclusion: d^2Cache作为一种训练无关、易于应用的加速机制，为扩散式大模型推理瓶颈提供了有效解决方案，兼顾效率与生成质量，对实际部署和应用具有重要意义。

Abstract: Diffusion-based large language models (dLLMs), despite their promising
performance, still suffer from inferior inference efficiency. This is because
dLLMs rely on bidirectional attention and cannot directly benefit from the
standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle
this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a
training-free approximate KV cache framework for accelerating dLLM inference.
d$^2$Cache features a two-stage fine-grained selection strategy to identify
tokens and adaptively update their KV states at each decoding step, while
caching the KV states of the remaining tokens for reuse. Furthermore,
d$^2$Cache naturally offers a more reliable decoding alternative, which can
enable quasi left-to-right generation and mitigate premature overconfidence in
tokens at the end of the sequence. Extensive experimental results on two
representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not
only achieves substantial inference speedups, but also yields consistent
improvements in generation quality. The code is available at
https://github.com/Kamichanw/d2Cache.

</details>


### [387] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出SmiSelf框架，通过将无效的SMILES分子结构字符串转换为SELFIES，再修正确保分子有效性，实现100%生成有效分子的目标，解决了LLM在分子生成中的有效性挑战。


<details>
  <summary>Details</summary>
Motivation: 生成具有特定性质的新分子在药物发现和材料科学中至关重要。虽然LLM有强大学习能力，但在仅凭少量例子生成合法分子时困难重重，尤其是以SMILES为表示形式时，容易产出无效分子。因此，作者希望探索保障生成分子的有效性方法，提升LLM在化学和生物医药实际中的实用性。

Method: 作者首先比较LLM用SELFIES（任意字符串都为有效分子的表示方法）与SMILES生成分子的有效率，发现SELFIES反而效果更差。随后测试了LLM修正无效SMILES的能力，结果有限。最终，提出SmiSelf框架，基于语法规则把无效SMILES自动转换成SELFIES，利用SELFIES的机制实现修正和有效性保障。

Result: 实验表明，SmiSelf不仅能确保百分百生成有效分子，同时能保持分子性质特征、并在其他性能指标上维持甚至提升表现。此外，SmiSelf可与所有基于SMILES的生成模型兼容。

Conclusion: SmiSelf为LLM在分子生成任务中带来100%有效的解决方案，扩展了其在生物医药等领域的实际应用前景，是现有生成工具的有力补充。

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [388] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了一种用于工具型智能体的非合作型用户模拟器，能更真实地反映真实用户的不合作行为，并对当前主流智能体进行测试和分析。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器主要表现为合作性，对于现实中用户的非合作行为模拟不足，导致工具型智能体训练和评测时缺乏对复杂、真实场景的适应能力，需要更具挑战性的用户模拟机制。

Method: 设计了一种新颖的用户模拟器框架，能够模拟四种主要非合作行为：请求不可用服务、跑题、表现出不耐烦、提供不完整信息。在MultiWOZ和τ-bench数据集上测试评估工具型智能体在面对这些非合作行为时的表现。

Result: 在非合作用户模拟下，主流工具型智能体的性能明显下降，出现如幻觉增加和对话崩溃等具体弱点。对每类非合作行为下智能体表现进行细致分析。

Conclusion: 所提非合作用户模拟器能够逼真地再现现实用户挑战，为工具型智能体的开发与测试提供了更为严苛和实用的工具框架，有助于研究社区提前发现并优化智能体在真实服务中的表现。

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [389] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 本文提出TagPR框架，通过“tagging the thought”（思维打标签）方法，显著提升大语言模型（LLM）个性化推理能力，在公开和自制数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然通用推理能力很强，但在理解用户历史、推断个性化偏好、生成定制回复方面存在明显不足。提升LLM个性化推理能力对于实际应用至关重要。

Method: 1. 首先提出自动生成推理链并语义标注的管道，得到结构化、易于解释的数据集。
2. 使用带标签的数据实体进行有监督微调（SFT），确立基础推理模式。
3. 采用多阶段强化学习（RL），结合标签约束与基于用户嵌入的个性化奖励模型，精细对齐用户特定思维逻辑。

Result: 在LaMP公开基准和自建数据集上，TagPR方法相较基础模型平均提升32.65%，达到了新的最优效果。

Conclusion: 通过结构化、可解释的推理链对LLM进行训练，是实现模型个性化推理能力提升的有效途径。TagPR能够显著增强LLM的个性化响应能力。

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [390] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: 本文提出了TReASURe方法，通过改进树搜索算法，有效提升了基于Diffusion语言模型在生成任务中的对齐性能，在多个自然语言生成评测中取得了新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统的树搜索方法在针对生成模型（如Masked Diffusion Language Models）进行测试时，面临分支高度相关导致探索能力受限，以及奖励评估方差过大导致剪枝效果不稳定的难题。为提升模型在特定任务上的表现，亟需有效解决上述挑战。

Method: 提出了TReASURe方法，包括两大创新：一是UnmaskBranch分支策略，采用first-hitting unmasking，可在每个父节点仅调用一次模型的情况下实现内容和揭示顺序的全面多样化；二是ResubstituteScore剪枝规则，通过确定性重替代对子序列进行低方差评分，提升剪枝稳定性。论文还从理论上分析了NFE（函数调用数）效率与评分误差界限，并证明了更宽树搜索可带来性能提升。

Result: 实验结果显示，TReASURe在perplexity、语言可接受性、情感与毒性控制等任务中刷新了最新表现，在计算资源相同设置下显著超越现有方法，尤其在低NFE场景提升更为显著。

Conclusion: TReASURe方法有效克服了树搜索在Diffusion语言模型生成中的探索与剪枝不稳定问题，理论和实验均验证了新方法在对齐特定任务奖励时的优越性和实用性。

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [391] [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
*Chenxing Wei,Hong Wang,Ying He,Fei Yu,Yao Shu*

Main category: cs.CL

TL;DR: 本文提出了一种面向多轮对话的测试时策略自适应新范式，通过利用用户反馈，实时调整LLM在多轮互动中的表现，并引入轻量级的单步更新算法ROSA，提升对话任务效果与效率。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂任务中需依赖多轮互动，但传统训练数据多为静态单轮，导致模型在长对话中难以根据实时反馈自我纠正，影响用户体验和结果质量。因此，作者试图使LLM能实时利用用户反馈自适应优化。

Method: 作者提出T2PAM范式，将用户在多轮对话中的反馈视为奖励信号，借此估计最符合用户偏好的最优策略，并通过更新少量参数实现模型自我纠正。同时提出ROSA算法，将模型参数在一轮高效更新后引导至理论最优策略，避免多步梯度计算，降低算力消耗。

Result: 理论分析保证ROSA策略随着交互次数增多趋于用户偏好。大量基准实验显示，ROSA在任务有效性和计算效率上均有显著提升。

Conclusion: 通过结合用户反馈和高效单步适应，ROSA能持续提升LLM多轮对话表现，为泛化、自适应多轮互动贡献轻量级高效方案。

Abstract: Large Language Models (LLMs) employ multi-turn interaction as a fundamental
paradigm for completing complex tasks. However, their performance often
degrades in extended interactions, as they are typically trained on static,
single-turn data, which hinders their ability to adapt to real-time user
feedback. To address this limitation, we first propose a new paradigm:
Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes
user feedback from the ongoing interaction as a reward signal to estimate a
latent optimal policy aligned with user preferences, then updates a small
subset of parameters to steer the model toward this policy, ultimately enabling
efficient in-conversation self-correction. We then introduce Optimum-Referenced
One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.
ROSA guides the model parameters toward a theoretical optimal policy in a
single, efficient update step, avoiding costly iterative gradient-based
optimization and minimizing computational overhead. We provide a rigorous
theoretical analysis guaranteeing that the policy of ROSA converges to the
preference of user as the number of interactions increases. Extensive
experiments on challenging benchmark demonstrate that ROSA achieves significant
improvements in both task effectiveness and efficiency.

</details>


### [392] [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
*Boyi Zeng,He Li,Shixiang Song,Yixuan Wang,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言模型预训练方法，通过在生成每个真实token之前，先生成一个中间“潜在思维”状态显著提升模型性能。实验表明该方法能在相同推理成本下超越更大规模参数的基线模型。


<details>
  <summary>Details</summary>
Motivation: 受Chain-of-Thought推理过程的启发，作者思考能否在预训练阶段增加计算步骤，在生成每个token时提升模型能力。目标是在不增加参数规模或推理成本的情况下提升模型性能。

Method: 提出让语言模型在每次生成token前，先产生一个“潜在思维”（即当前位置的隐藏状态），再利用该潜在思维来预测下一个token。每次token生成过程引入一个额外的计算步骤。同时还实验了生成多个潜在思维链式推理的效果。

Result: 实验显示，在相同的推理成本下，采用该方法的1.4B参数模型优于标准2.8B参数模型（同数据、架构下），在语言建模及多种下游任务上都表现更佳。随着每个token前潜在思维数的增加，模型表现持续提升。

Conclusion: 在预训练阶段引入潜在思维生成的额外步骤，能有效提升语言模型的下游表现，并为进一步探索计算步骤扩展带来新启示。

Abstract: The remarkable success of Chain-of-Thought (CoT), which enhances performance
by scaling generation steps at test-time, inspires us to ask: can we leverage a
similar scaling of computational steps during pretraining to improve the
generation of each individual token? To address this, we propose a novel
pre-training methodology: Pretraining Language Models with Latent Thoughts. Our
approach pretrains a language model (LM) to first generate an intermediate
latent thought-the last hidden state of the current position-which is then used
as input to predict the actual subsequent token. This additional computational
step enables the LM to refine its prediction within unconstrained continuous
space. Our experiments demonstrate that, at an identical inference cost, a LM
that generates one additional latent thought per token outperforms a standard
model with double the parameters. For instance, ours-1.4B (Pythia Arch),
pretrained on 300B tokens from the Pile, significantly surpasses the vanilla
Pythia-2.8B trained on the same data on both language modeling and a range of
general downstream tasks. Furthermore, increasing the number of latent thoughts
generated before each actual token-forming a chain analogous to
CoT-consistently improves the model's performance.

</details>


### [393] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种提升大型语言模型驱动的多智能体系统在存在指令冲突时遵循层级指令的能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的多智能体系统在任务协作、工具使用等方面取得了显著进展，但在部署到需要高度可靠性的场景时会因指令冲突（如系统指令与用户指令、智能体间指令）而导致合规性问题，且传统评测指标无法有效识别和指导改进这一失效模式。

Method: 作者提出一个三阶段的全栈框架：（1）诊断，提出情境化角色遵循评分（CRAS），细化并量化合规性；（2）定位，通过注意力漂移分析，发现决策集中在中间层注意力头；（3）对齐，提出外科式指令层对齐（SAIL），仅在关键层使用LoRA微调，并采用关注贡献加权的DPO目标优化输出。

Result: 在多个标准基准测试和主流多智能体系统框架上，提出的方法可在无需全模型微调的情况下，有效提升指令层级遵循（例如AutoGen在MedQA任务上合规性提升5.60%）。

Conclusion: 该方法为多智能体系统中指令冲突下的合规性提升提供了可操作的指标和针对性优化手段，有助于其更可靠地部署于对合规性要求高的复杂应用中。

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [394] [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
*Nan Wang,Jiaxuan Li*

Main category: cs.CL

TL;DR: 该研究探讨了句法结构在句子加工中的时间顺序，发现短语结构的构建可能早于句法类别检测，并优先组织阅读视线轨迹。


<details>
  <summary>Details</summary>
Motivation: 心理语言学长期关注在句子理解过程中，句法结构构建和句法类别检测究竟孰先孰后。现有大多基于违规范式，无法区分这两个过程。本研究希望在自然语言阅读环境下理清它们的时间关系。

Method: 研究采用了来自ZuCo语料库的同步EEG与眼动数据，分析了阅读时的注视转移（视线轨迹）、贝叶斯网络建模影响因素，并评估了与注视相关的脑电信号，以揭示句法处理时序。

Result: 实验发现：1）阅读时视线优先转向句法中心词而非逐词顺序；2）结构深度比词汇熟悉度和意外性更能驱动非线性阅读偏离；3）句法意外性对神经活动的影响在词呈现前就已开始，并持续至整合初期。

Conclusion: 短语结构构建可在句法类别检测之前发生，并在阅读中占主导作用，支持“树支架”式预测性理解模型，对句法时序理论提供了新的实证支持。

Abstract: A central question in psycholinguistics is the timing of syntax in sentence
processing. Much of the existing evidence comes from violation paradigms, which
conflate two separable processes - syntactic category detection and phrase
structure construction - and implicitly assume that phrase structure follows
category detection. In this study, we use co-registered EEG and eye-tracking
data from the ZuCo corpus to disentangle these processes and test their
temporal order under naturalistic reading conditions. Analyses of gaze
transitions showed that readers preferentially moved between syntactic heads,
suggesting that phrase structures, rather than serial word order, organize
scanpaths. Bayesian network modeling further revealed that structural depth was
the strongest driver of deviations from linear reading, outweighing lexical
familiarity and surprisal. Finally, fixation-related potentials demonstrated
that syntactic surprisal influences neural activity before word onset (-184 to
-10 ms) and during early integration (48 to 300 ms). These findings extend
current models of syntactic timing by showing that phrase structure
construction can precede category detection and dominate lexical influences,
supporting a predictive "tree-scaffolding" account of comprehension.

</details>


### [395] [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
*Haonan Wang,Weida Liang,Zihang Fu,Nie Zheng,Yifan Zhang,Yao Tong,Tongyao Zhu,Hao Jiang,Chuang Li,Jiaying Wu,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: 使用DeepSeek-R1生成的优质推理链作为示例时，增加示例数会导致推理大模型表现下降。论文提出了Insight-to-Solve (I2S)流程，有效提升了模型的推理能力，并在多个基准测试中显著优于常规方法。


<details>
  <summary>Details</summary>
Motivation: 现代推理大模型在少样本链式推理（few-shot CoT）时，准确率反而低于直接给出答案的方式，这是一个反直觉现象。作者希望理解和解决这一问题，提升模型利用演示推理示例时的效果。

Method: 作者用DeepSeek-R1生成高质量推理示例，将其用于few-shot CoT，发现增加优秀演示反而降低模型表现。他们通过分析，提出了两个机制：语义误导和策略迁移失败，并基于此设计了Insight-to-Solve (I2S)方法：将推理示例转化为显式、可复用的洞见，并生成针对目标问题的推理链，最后可以进一步自我优化（I2S+）。

Result: I2S及I2S+在多个开放与闭源模型与数据集上，均表现出优于直接回答和传统few-shot增益的能力。在AIME'25测试中，GPT-4.1提升14%，o1-mini在AIME与GPQA中分别提升2.7%和1.7%。

Conclusion: 常规few-shot推理演示存在陷阱，提升推理效果需要借助结构化与可复用的洞见。Insight-to-Solve方法为大模型推理能力的发挥提供了更优框架。

Abstract: Recent reasoning LLMs (RLMs), especially those trained with verifier-based
reinforcement learning, often perform worse with few-shot CoT than with direct
answering. We revisit this paradox using high-quality reasoning traces from
DeepSeek-R1 as demonstrations and find that adding more exemplars consistently
degrades accuracy, even when demonstrations are optimal. A detailed analysis
reveals two mechanisms behind this decline: (i) semantic misguidance, where
high textual similarity leads the model to treat the target as the same as the
exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer
failure, where the model struggles to extract useful reasoning strategies and
apply them to target questions. Guided by these, we introduce Insight-to-Solve
(I2S), a sequential test-time procedure that turns demonstrations into
explicit, reusable insights and derives a target-specific reasoning trace;
optionally, the reasoning is self-refined for coherence and correctness (I2S+).
Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently
outperform both direct answering and test-time scaling baselines across open-
and closed-source models. Even for GPT models, our method helps: on AIME'25,
GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on
GPQA, indicating that in-context demonstrations can be harnessed effectively
via insight-refine-solve framework.

</details>


### [396] [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
*Aditya Narayan Sankaran,Reza Farahbakhsh,Noel Crespi*

Main category: cs.CL

TL;DR: 本论文研究了在全球榜单上成功的K-pop歌曲中的中英混用情况，发现英文比例甚高，且无显著性别差异。


<details>
  <summary>Details</summary>
Motivation: 随着K-pop全球影响力提升，韩语与英语的切换成为其标志性特征。论文旨在探究歌词语言选择背后的策略与对国际市场成功的影响。

Method: 作者构建了2017-2025年Billboard Hot 100及Global 200榜单中14个组合与8位solo歌手的K-pop歌曲数据集，统计并分析了英语与韩语歌词比例、代码切换频率及风格特征，并利用统计检验与歌词特征进行性别分类实验。

Result: 分析表明，全球榜单K-pop歌曲以英文主导，男女艺人均有高比例代码切换；性别间无显著差异，女solo有更高英文倾向。基于多语嵌入与人工特征的性别分类F1最高0.76。Hot 100榜单歌曲需更高英文比例。

Conclusion: K-pop歌词中语言选择受全球市场驱动，英文的高度使用映射了艺人身份和榜单定位需求。

Abstract: Code switching, particularly between Korean and English, has become a
defining feature of modern K-pop, reflecting both aesthetic choices and global
market strategies. This paper is a primary investigation into the linguistic
strategies employed in K-pop songs that achieve global chart success, with a
focus on the role of code-switching and English lyric usage. A dataset of K-pop
songs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to
2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,
the proportion of English and Korean lyrics, the frequency of code-switching,
and other stylistic features were analysed. It was found that English dominates
the linguistic landscape of globally charting K-pop songs, with both male and
female performers exhibiting high degrees of code-switching and English usage.
Statistical tests indicated no significant gender-based differences, although
female solo artists tend to favour English more consistently. A classification
task was also performed to predict performer gender from lyrics, achieving
macro F1 scores up to 0.76 using multilingual embeddings and handcrafted
features. Finally, differences between songs charting on the Hot 100 versus the
Global 200 were examined, suggesting that, while there is no significant gender
difference in English, higher English usage may be more critical for success in
the US-focused Hot 100. The findings highlight how linguistic choices in K-pop
lyrics are shaped by global market pressures and reveal stylistic patterns that
reflect performer identity and chart context.

</details>


### [397] [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
*Stefan Arnold,René Gröbner*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（Gemma-2）在生成带有with介词短语时，如何在工具性补语（修饰动词）和属性性补语（修饰名词）之间进行决策，并展示了可以通过调整单个注意力头的激活来有效控制模型的输出偏好。


<details>
  <summary>Details</summary>
Motivation: 当生成介词短语时，模型需判断补语作为工具性补语还是属性性补语。然而，这一决策的内部机制尚不明确。为理解和控制这种机制，作者有针对性地实验Gemma-2模型。

Method: 作者设计了一组包含带有with头介词短语的提示句选集，这些上下文环境对工具性和属性性解释均适用。通过分析Gemma-2在生成时的倾向，并将注意力头的激活投影到词汇空间，进一步通过调整某个注意力头的value向量，分析其对补语功能角色分布的影响。

Result: Gemma-2模型展现出对工具性解读（修饰动词）的偏好，比例为3:4。调整特定注意力头的value向量后，工具性解读降低到33%，而属性性解读（修饰名词）上升到36%。

Conclusion: 该研究揭示了具体注意力头对介词补语功能角色的倾向性控制能力，为理解语言模型内部机制和生成解释控制提供了证据。

Abstract: Language Models, when generating prepositional phrases, must often decide for
whether their complements functions as an instrumental adjunct (describing the
verb adverbially) or an attributive modifier (enriching the noun adjectivally),
yet the internal mechanisms that resolve this split decision remain poorly
understood. In this study, we conduct a targeted investigation into Gemma-2 to
uncover and control the generation of prepositional complements. We assemble a
prompt suite containing with-headed prepositional phrases whose contexts
equally accommodate either an instrumental or attributive continuation,
revealing a strong preference for an instrumental reading at a ratio of 3:4. To
pinpoint individual attention heads that favor instrumental over attributive
complements, we project activations into the vocabulary space. By scaling the
value vector of a single attention head, we can shift the distribution of
functional roles of complements, attenuating instruments to 33% while elevating
attributes to 36%.

</details>


### [398] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 本文提出了一种针对多轮函数调用任务的LLM训练新框架PARL-MT，通过进展感知提升模型在实际任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮对话中的函数调用能力有限，难以有效管理任务进展与整体规划，影响真实场景应用需求，如旅行规划、数据分析等长程任务。

Method: 作者提出了PARL-MT框架，包含：1) 进展感知生成（PAG）流程，自动构建包含对话摘要和未来规划的数据集；2) 进展感知引导的强化学习算法（PAG-RL），将进展感知融入强化学习，减少上下文冗余，提升局部与整体任务完成的协同。

Result: 在两个公开多轮函数调用基准测试上，PARL-MT明显优于传统方法，无论在准确性还是整体效率上都有显著提升。

Conclusion: 通过显式引入进展感知，LLM能更好地支持多轮、长程任务的函数调用，展现出更强大的鲁棒性和任务完成能力。

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [399] [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
*Haorui Yu,Ramon Ruiz-Dolz,Qiufeng Yi*

Main category: cs.CL

TL;DR: 本研究评估了主流视觉语言模型（VLMs）在生成中国画评论方面的能力，通过构建定量评测框架和多角度评论任务，揭示了其表现、优势和不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在中国画等传统艺术领域的评论能力尚未得到系统性评估。传统中国画的鉴赏和评论具有高度的专业性和多维度特征，因此需要针对该领域设计专门的评价方法，推动AI模型在复杂语义和内容生成任务中的应用。

Method: 作者首先从专家评论中总结出多维度评分特征（评述立场、关注要素、评论质量），利用零样本分类模型构建中国画评论的定量分析框架，并定义和量化了几种典型评论者角色。随后，采用persona-guided prompt（角色引导的提示），评估Llama、Qwen、Gemini等VLM在生成不同风格评论时的表现。

Result: 实验结果揭示了当前VLM在中国画评论任务上的表现水平，能够从部分角度进行合理评论，但在某些复杂语义理解和细致审美评价上尚有不足。针对评论维度和评论者角色的引导也显著影响了评论内容和质量。

Conclusion: 主流VLM在艺术评论等复杂语义生成任务上展现出初步潜力，但其在精细理解和富有深度的内容生成方面仍有改进空间。该研究为AI系统在传统艺术领域的应用和评价提供了标准和思路。

Abstract: This study aims to test and evaluate the capabilities and characteristics of
current mainstream Visual Language Models (VLMs) in generating critiques for
traditional Chinese painting. To achieve this, we first developed a
quantitative framework for Chinese painting critique. This framework was
constructed by extracting multi-dimensional evaluative features covering
evaluative stance, feature focus, and commentary quality from human expert
critiques using a zero-shot classification model. Based on these features,
several representative critic personas were defined and quantified. This
framework was then employed to evaluate selected VLMs such as Llama, Qwen, or
Gemini. The experimental design involved persona-guided prompting to assess the
VLM's ability to generate critiques from diverse perspectives. Our findings
reveal the current performance levels, strengths, and areas for improvement of
VLMs in the domain of art critique, offering insights into their potential and
limitations in complex semantic understanding and content generation tasks. The
code used for our experiments can be publicly accessed at:
https://github.com/yha9806/VULCA-EMNLP2025.

</details>


### [400] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: 该论文聚焦于Wikipedia事实不一致问题，提出了CLAIRE系统用于检测不一致内容，并贡献了首个Wikipedia不一致性数据集WIKICOLLIDE，以及相关实证评估。


<details>
  <summary>Details</summary>
Motivation: Wikipedia被广泛用作知识库和人工智能训练资源，其准确性至关重要，但其中存在事实不一致等问题。该论文旨在评估和提升Wikipedia的知识一致性。

Method: 作者提出了一个名为CLAIRE的系统，它结合了大型语言模型推理和检索能力，自动发现Wikipedia中的不一致性，并为人工审核提供证据。通过CLAIRE与人工标注结合，构建了真实维基百科不一致性的基准（WIKICOLLIDE），并对现有自动化检测方法进行了性能测试。

Result: 在用户实验中，大部分资深编辑表示CLAIRE提升了他们的信心，且检测出的不一致数目提升了64.7%。数据分析发现，随机抽样下，至少3.3%的英文Wikipedia事实相互矛盾。当前最优自动系统在Benchmark上的AUROC仅为75.1%。

Conclusion: 事实矛盾是Wikipedia中可量化且普遍存在的问题。基于LLM的系统如CLAIRE可有效辅助编辑者发现和改正知识不一致，为大规模保持Wikipedia质量提供了有力工具。

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [401] [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
*Soumick Sarker,Abhijit Kumar Rai*

Main category: cs.CL

TL;DR: 提出了一种高效的金融对话意图句提取框架Fin-ExBERT，通过优化的BERT结构和灵活训练设计，能精准自动地从对话中提取用户意图相关句子。


<details>
  <summary>Details</summary>
Motivation: 金融服务电话对话文本存在非正式结构、专业术语多和意图密度不均等特点，导致基于句子的关键信息提取难度大。现有方法常需大量标注数据，泛化性和可解释性有限，因此亟需高效、低资源且可解释的解决方案。

Method: 提出了Fin-ExBERT框架：1）利用领域适应后的BERT作为主干结构，引入低秩适配器（LoRA）实现少量数据的高效微调；2）采用两阶段渐进式解冻训练策略—先只训练分类头，后分阶段微调整个模型并采用差异化学习率；3）结合基于概率曲率（折点检测）的动态阈值策略，替代传统固定阈值。兼容批量评估、结果可视化与标准化导出。

Result: 在真实金融服务对话数据上，Fin-ExBERT展现出强劲的精准率和F1表现，输出结果具有较好可解释性，适合用于后续审核和问答流程。

Conclusion: Fin-ExBERT在金融对话意图句抽取任务中准确、高效、可部署，解决了领域数据稀缺与自动化需求，能大幅助力金融领域的文本挖掘应用。

Abstract: Financial dialogue transcripts pose a unique challenge for sentence-level
information extraction due to their informal structure, domain-specific
vocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight
and modular framework for extracting user intent-relevant sentences from
annotated financial service calls. Our approach builds on a domain-adapted BERT
(Bidirectional Encoder Representations from Transformers) backbone enhanced
with LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using
limited labeled data. We propose a two-stage training strategy with progressive
unfreezing: initially training a classifier head while freezing the backbone,
followed by gradual fine-tuning of the entire model with differential learning
rates. To ensure robust extraction under uncertainty, we adopt a dynamic
thresholding strategy based on probability curvature (elbow detection),
avoiding fixed cutoff heuristics. Empirical results show strong precision and
F1 performance on real-world transcripts, with interpretable output suitable
for downstream auditing and question-answering workflows. The full framework
supports batched evaluation, visualization, and calibrated export, offering a
deployable solution for financial dialogue mining.

</details>


### [402] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: 该论文提出A2D方法，通过在dLLM生成过程中，在检测到有害内容时立即生成“[EOS]”信号以终止回复，有效防止任意步骤、任意顺序下的有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 因扩散大语言模型(dLLM)具有任意顺序生成的灵活性，但也带来了安全威胁，使攻击者可利用模板预填等绕过拒答，甚至在任意位置插入有害内容，因此亟需细粒度、实时的防护方法。

Method: 提出A2D算法，在Token级别对dLLM进行对齐。采用随机掩码训练，使模型在生成过程中一旦检测到有害内容，就输出特定的“[EOS]”终止标志，实现对任意顺序和步骤的 prefilling 攻击的鲁棒防御。

Result: A2D显著降低了DIJA等template-based攻击成功率（在LLaDA-8B-Instruct上降至1.3%，Dream-v0-Instruct-7B上降至0%），同时通过动态判定增长终止，有害内容生成时的提前停止速度提高了最多19.3倍。

Conclusion: A2D提升了扩散类大语言模型的安全性，能在任意内容生成顺序和步骤下及时拒绝有害内容输出，为dLLM的实际安全应用提供了有效的技术支撑。

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [403] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文提出一种称为Policy Reasoning Traces（PRT，政策推理轨迹）的方法，通过生成特定的推理链，提升大语言模型（LLM）在政策合规检测任务中的表现。实验证明该方法在HIPAA与GDPR等政策评估任务上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 人工专家通常通过逐步推理过程来判断一个案例是否符合特定政策条款，但获得专家级推理过程的文档成本较高。随着大语言模型在复杂推理任务中的应用，如何低成本地提升模型的政策合规评估能力成为重要问题。

Method: 作者提出PRT，即专门为政策合规评估任务自动生成的推理链。这些推理链作为连接，能够增强大语言模型在推理和合规性判断时的准确性。作者在推理阶段和训练阶段都应用了PRT，并在多个开源和商业模型上进行了实验对比。

Result: 引入PRT后，无论是在推理时使用还是用于模型训练，所测模型在政策（如HIPAA和GDPR）合规判断上的准确性均显著提升，达到了最优结果。同时，模型更能准确引用政策条款，并能通过链式思维提高决策的合规性。

Conclusion: PRT为提升大语言模型在政策合规评估任务中的表现提供了一条有效且切实可行的路径。该方法不仅提升了准确率，还增强了模型解读政策条款和链式推理的能力，有望推广至更多政策与规则推理相关的应用场景。

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [404] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: 本文提出了Structured In-context Environment（SIE）框架，自动构建推理环境，提升大模型在结构化数据上的泛化与可验证推理能力，并显著增强了域内外任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型通过强化学习提升推理能力，但环境的局限性——如数学和编程环境需专家标注且难以扩展，游戏环境又技能专用缺乏泛化——影响了模型推理学习的广度和深度。因此需要一种同时具备可扩展性、可泛化推理和可验证性的推理环境。

Method: 作者提出SIE框架，通过自动化方式从大规模结构化数据中构建推理环境，使得推理环境可以自然支持组合式的推理。同时结构化数据的显式模式和推理链接可以实现基于规则的结果验证。作者还探索了信息有限的SIE环境下的学习能力。

Result: 实验结果显示，SIE框架不仅能大幅提升模型在结构化数据任务上的推理能力，还能将组合推理技能有效迁移到数学与逻辑等新领域。同时，在信息受限的SIE环境下，模型能够通过环境探索推断缺失信息，进一步提升推理鲁棒性与泛化。

Conclusion: SIE框架为训练和评估大语言模型推理能力提供了一条新路径，解决了环境扩展性与推理泛化难题，促进了大模型在多领域推理能力的提升。

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [405] [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
*Tiancheng Li,Yuhang Wang,Zhiyang Chen,Zijun Wang,Liyuan Ma,Guo-jun Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Consensus-Evolve（C-Evolve）的演化算法，通过多个提示（prompt）聚合结果并投票选出最佳答案来提升大模型任务表现，显著优于现有方法，在开放和封闭任务上均取得了最优成绩。


<details>
  <summary>Details</summary>
Motivation: 虽然对闭源模型优化的提示演化已有研究，但鲜少关注多提示聚合与共识机制是否能进一步突破AI系统能力边界。本文旨在探索通过投票聚合多提示结果的策略，是否能带来性能提升。

Method: 提出C-Evolve算法，利用岛屿式演化算法保持提示多样性，从不同岛屿选择提示组成组，通过多数投票聚合各自输出。创新点在于用投票贡献分（而非个体表现）作为进化适应度，优先保留对群体提升有潜力的提示。

Result: C-Evolve在多个任务上取得新最优：如Qwen3-8B模型在HotpotQA上得分70.67%、IFBench上43.88%，较GEPA分别提升4.95%、2.73%；GPT-4.1-mini在IFBench和MATH基准测试上分别达到47.96%和95.33%。

Conclusion: C-Evolve通过群体投票共识机制，有效提升大模型的任务表现，优于单一提示和现有演化算法，适用于多种任务，表现出极强的竞争力。

Abstract: Prompt evolution algorithms offer a powerful paradigm for enhancing AI
systems based on closed-source models, while few work explores whether
aggregating results from multiple prompts to reach a consensus can further
advance the system capability boundary. In this paper, we introduce
Consensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group
of prompts whose aggregated outputs after majority voting achieve optimal
performance. More specifically, C-Evolve employs an island-based evolutionary
algorithm to maintain population diversity, and prompts from distinct islands
are selected to form groups to aggregate their outputs. The key difference from
single individual evolution is a voting score, which evaluates each individual
prompt's contribution within groups. We take this as the fitness score for
evolution instead of individual performance. Consequently, C-Evolve is more
likely to produce and maintain prompts with higher potential to form a
high-performing group and eliminate low-performing ones, gradually improving
the group performance after reaching consensus. Our method achieves
state-of-the-art performance across a wide range of tasks, including both
open-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,
C-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and
2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench
is further improved to 47.96% and reaches 95.33% in the MATH benchmark. These
results demonstrate the C-Evolve's competitive performance.

</details>


### [406] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: 针对大模型中的机器忘却（Machine Unlearning）问题，提出了PRISM框架，通过在表示空间和参数空间中实现双重平滑性，提升对攻击的鲁棒性，并在忘却各项指标之间取得更佳平衡。


<details>
  <summary>Details</summary>
Motivation: 目前主流的机器忘却方法存在灾难性遗忘和指标失衡的问题，如过度关注某一目标（例如忘却效果、效用保持或隐私保护）而牺牲其他目标。同时，模型在表示或参数空间中的微小扰动易被再学习和越狱攻击利用。

Method: 提出PRISM统一框架，通过两个阶段实现平滑性优化：第一阶段在表示空间中使用经过鲁棒训练的探测器抵御越狱攻击；第二阶段在参数空间中解耦保留与忘却梯度冲突、减少指标失衡，并平滑参数空间以缓解再学习攻击。

Result: 在WMDP和MUSE等多数据集、对话和连续文本场景下，PRISM在多种攻击下均优于现有方法，并在关键指标之间取得更优平衡。

Conclusion: PRISM有效提升了大模型机器忘却的安全性和鲁棒性，能够在各种场景下实现指标平衡和更强的攻击防御能力。

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [407] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedCritical的两阶段自我学习框架，大幅提升了小型医学语言模型的推理能力，并以较低成本达到了领先水平。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在医学复杂推理任务中表现不及大模型，现有基于知识蒸馏的方法依赖大模型评判，成本高且效率低。作者希望提升小模型医学推理能力，同时降低成本和提高效率。

Method: 提出MedCritical，两阶段：1）利用大模型提取长链推理模板，指导小模型生成更复杂推理；2）采用直接偏好优化（DPO）让小模型自我迭代，通过对自己改正过程对抗训练，强化推理能力，实现自我驱动改错和知识巩固。

Result: MedCritical 7B模型在CMExam医学基准测试上表现优于当前主流同级模型Taiyi和Huatuo-o1-7B，分别提升3.04%和10.12%，达到7B级小模型最新SOTA水平。

Conclusion: MedCritical框架使小语言模型通过自我学习在医学推理任务上以更低成本达到传统知识蒸馏相当乃至更优效果，推动了小模型在医学领域的应用能力。

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [408] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的偏好优化框架MetaAPO，通过动态结合数据生成与模型训练，显著提升了大语言模型的对齐效果，同时大幅降低线上标注成本。


<details>
  <summary>Details</summary>
Motivation: 偏好优化是实现大语言模型与人类价值观一致的关键，但存在离线数据分布与模型动态策略不匹配的问题，现有方法在自适应性和数据利用方面存在不足。

Method: 作者提出MetaAPO框架，引入轻量级元学习器作为“对齐差距评估器”，动态评估在策略数据（on-policy）和离线数据上的采样收益，基于评估结果有针对性地生成在线数据，并为每个样本分配元权重，以优化线上线下数据的质量与分布。

Result: 在AlpacaEval 2、Arena-Hard和MT-Bench等多个基准测试中，MetaAPO在不同设置下均表现出优于现有偏好优化方法的效果，同时可节省约42%的在线数据标注成本。

Conclusion: MetaAPO实现了动态高效的数据生成与利用，使得偏好优化更加贴近模型实际学习状态，不仅提升了模型性能，也有效降低了数据标注成本。

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [409] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: 本文提出了一种名为Clinical Contrastive Decoding（CCD）的无训练、无检索推理框架，以减少放射学多模态大模型（MLLMs）中的医学幻觉问题，实现更为准确且受图像支持的自动报告生成。


<details>
  <summary>Details</summary>
Motivation: 放射学MLLMs在医学影像解读领域取得显著进展，但存在生成与临床实际不符的内容（医学幻觉），这在医疗场景中极具风险。为此，急需提升生成描述的临床准确性和图像依赖性。

Method: 提出CCD推理方法：无需重新训练和外部检索地，在报告生成过程中，利用行业专家模型输出的结构化临床信号，通过两阶段对比机制细致调整每个token的生成概率，从而提升输出的临床真实度。

Result: 在三个数据集和多个模型上的实验表明，CCD可持续提升自动放射学报告生成的整体表现。在知名的MIMIC-CXR数据集上，应用CCD后，RadGraph-F1指标提升高达17%。

Conclusion: CCD为放射学MLLMs减少医学幻觉提供了一种轻量、通用且有效的手段，实现了专家模型与通用大模型的有机结合，可大幅提升医学文本生成的可靠性。

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [410] [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
*Wonhyuk Lee,Youngchol Kim,Yunjin Park,Junhyung Moon,Dongyoung Jeong,Wanjin Park*

Main category: cs.CL

TL;DR: 本文提出了一种名为Guard Vector的新方法，通过计算守护模型和同结构预训练语言模型的参数差，生成安全任务向量，在不同语言模型之间迁移安全能力。方法无需额外训练和标注即可扩展到多种语言，并提高流式处理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型（LLM）在安全性上存在挑战，尤其是在多语言环境和流式应用场景下。此外，现有安全守护机制往往需要大量数据和计算资源进行迁移和适配，不利于高效部署和推广。

Method: 提出Guard Vector，通过守护模型和基础预训练模型的参数差构建安全向量，再将其与目标模型“组合”，无须再训练即可获得目标守护模型（TGM），并结合基于前缀的训练和评测策略，实现流式适应。同时采用单token输出分类器，提升推理效率。

Result: TGM在多个标准安全评测中，相较传统守护模型表现更优，且方法可直接迁移至如汉语、日语、韩语等新语言，并在Llama和Gemma两大主流模型骨干间实现了跨模型迁移。采用prefix SFT有效保证流式场景下的稳定性和分类准确性。单token输出显著提升模型吞吐和降低延迟。

Conclusion: Guard Vector方法有效降低了多语言安全守护模型的开发和部署门槛，减少数据/计算需求，提升了跨场景、跨模型和多语言下的安全性迁移和流式适应能力，对推动更负责任的AI系统建设具有积极意义。

Abstract: We introduce Guard Vector, a safety task vector computed as the parameter
difference between a guardrail model (Guard Model) and a same-architecture
pretrained language model. Composing this vector with a target language model
yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware
approach that combines prefix-based training and evaluation with a classifier
that produces a single-token output. With this composition alone, TGM improves
classification quality over established Guard Models across standard safety
suites and enables language extensibility to Chinese, Japanese, and Korean,
requiring neither additional training nor target language labels. It also
demonstrates model portability across two widely used public guardrail
backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM
preserves classification quality under streaming by aligning the behavior
between prefix inputs and full-text inputs. The single-token output design
increases throughput and reduces latency. Together, these components reduce
data and compute requirements while promoting streaming-aware evaluation
practices, thereby contributing to a more responsible AI ecosystem.

</details>


### [411] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: 作者提出了一种在单次训练过程中同时进行多个预训练实验的方法，大幅提升了大型语言模型实验的计算效率，且对模型性能影响很小。


<details>
  <summary>Details</summary>
Motivation: 以往通过受控预训练实验来研究大型语言模型的学习与推理，但因计算资源消耗大限制了实验规模，急需提升实验效率以推动更多深入研究。

Method: 在一次1.5B参数、210B tokens的模型训练中，嵌入了10组不同的预训练实验，并评估各组实验对数据污染、攻击、记忆、知识获取等问题的影响，还设计了检测实验间干扰的测试。

Result: 证明了在单次训练中融入多个实验是可行的，模型表现和训练动态几乎未受影响，实验间的相互作用也可以忽略，成功复现多项先前研究结果，并进行了新的探索。

Conclusion: 单次训练融合多预训练实验使得低成本下能严谨且高效地对大型语言模型进行科学实验，为未来大模型研究提供了实用方案。

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [412] [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
*Wenhang Shi,Yiren Chen,Shuqing Bian,Xinyi Zhang,Kai Tang,Pengfei Hu,Zhe Zhao,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 该论文提出了一种高效的自动化prompt优化框架GRACE，结合门控精炼和自适应压缩两大策略，在多个任务和领域实现了比现有方法更优的表现和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化prompt优化方法难以稳定提升prompt性能，且容易陷入局部最优，导致效率低下。手工设计成本高，难以扩展，亟需一种更高效、稳定的自动优化方法。

Method: GRACE框架结合了两个创新策略：(1)门控精炼，包括反馈调节门和更新拒绝门，用于筛选和优化prompt更新信号，实现更稳定有效的提升；(2)自适应压缩，当优化过程停滞时提取并重组prompt核心信息，帮助摆脱局部最优。两者通过有策略的信息“丢失”促进优化空间探索。

Result: 在11个任务和三个实际领域的实验中，GRACE分别在BIG-Bench Hard、领域特定NLP、通用NLP任务上，平均相对性能提升4.7%、4.4%、2.7%。且仅使用传统方法25%的计算预算，显示出高效和低算力消耗。

Conclusion: GRACE框架有效提升了自动化prompt优化的性能与效率，减少了计算资源需求，并在多种任务上取得领先效果，在实际应用中具有较高的推广和应用价值。

Abstract: Prompt engineering is crucial for leveraging the full potential of large
language models (LLMs). While automatic prompt optimization offers a scalable
alternative to costly manual design, generating effective prompts remains
challenging. Existing methods often struggle to stably generate improved
prompts, leading to low efficiency, and overlook that prompt optimization
easily gets trapped in local optima. Addressing this, we propose GRACE, a
framework that integrates two synergistic strategies: Gated Refinement and
Adaptive Compression, achieving Efficient prompt optimization. The gated
refinement strategy introduces a feedback regulation gate and an update
rejection gate, which refine update signals to produce stable and effective
prompt improvements. When optimization stagnates, the adaptive compression
strategy distills the prompt's core concepts, restructuring the optimization
trace and opening new paths. By strategically introducing information loss
through refinement and compression, GRACE delivers substantial gains in
performance and efficiency. In extensive experiments on 11 tasks across three
practical domains, including BIG-Bench Hard (BBH), domain-specific, and general
NLP tasks, GRACE achieves significant average relative performance improvements
of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further
analysis shows that GRACE achieves these gains using only 25% of the prompt
generation budget required by prior methods, highlighting its high optimization
efficiency and low computational overhead. Our code is available at
https://github.com/Eric8932/GRACE.

</details>


### [413] [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
*Sherrie Shen,Weixuan Wang,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出将“脚注、尾注等副文本说明”引入机器翻译，提升对文化相关词的解释效果，并构建了数据集和评测方法。实验证明LLM生成的副文本有助于理解，但仍逊于人工专家。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译难以准确传达具有文化色彩词汇的深层含义。人类译者通常借助脚注等副文本形式进行说明，而当前自动翻译系统忽视了这种方式。

Method: 1）借鉴文学/翻译理论中Genette的“副文本”理论，首次提出在机器翻译中对文化绑定词汇采用副文本说明；2）构建以《聊斋》英译本为例的专家副文本数据集；3）评估大语言模型（LLMs）自动生成副文本说明的选择及内容，比较人工与机器生成的效果。

Result: 人类评估显示，虽然LLM生成的副文本提升了读者的理解力，但效果仍明显不及专业译者；统计分析还发现，不同专业译者对副文本使用存在较大分歧，反映出文化调适的多样性和开放性。

Conclusion: 副文本说明有助于机器翻译克服仅靠文本直译的局限性，为文化词汇解释、单语扩展说明与个性化适应提供新方向。

Abstract: The faithful transfer of contextually-embedded meaning continues to challenge
contemporary machine translation (MT), particularly in the rendering of
culture-bound terms--expressions or concepts rooted in specific languages or
cultures, resisting direct linguistic transfer. Existing computational
approaches to explicitating these terms have focused exclusively on in-text
solutions, overlooking paratextual apparatus in the footnotes and endnotes
employed by professional translators. In this paper, we formalize Genette's
(1987) theory of paratexts from literary and translation studies to introduce
the task of paratextual explicitation for MT. We construct a dataset of 560
expert-aligned paratexts from four English translations of the classical
Chinese short story collection Liaozhai and evaluate LLMs with and without
reasoning traces on choice and content of explicitation. Experiments across
intrinsic prompting and agentic retrieval methods establish the difficulty of
this task, with human evaluation showing that LLM-generated paratexts improve
audience comprehension, though remain considerably less effective than
translator-authored ones. Beyond model performance, statistical analysis
reveals that even professional translators vary widely in their use of
paratexts, suggesting that cultural mediation is inherently open-ended rather
than prescriptive. Our findings demonstrate the potential of paratextual
explicitation in advancing MT beyond linguistic equivalence, with promising
extensions to monolingual explanation and personalized adaptation.

</details>


### [414] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 本研究比较了人类评分员和大型语言模型（LLM，如GPT-4o、Gemini等）在作文自动评分任务中的评分准确性与评分理由。结果揭示了两者评分和推理过程的异同。


<details>
  <summary>Details</summary>
Motivation: 自动评分系统依赖于机器学习和自然语言处理技术，随着大型语言模型（LLM）的进展，探讨这些模型在自动评分任务中的表现及其给出的评分理由有助于理解决策过程、提升评分一致性。

Method: 研究基于大规模考试作文，分析GPT-4o、Gemini等LLM评分的准确度（通过加权kappa和归一化互信息度量），并用余弦相似度和主成分分析分别衡量评分理由的相似性与聚类特征。

Result: LLM能生成与人类评分员类似甚至更一致的评分结果，并能提供理由支持其打分，实验量化了不同模型间评分和理由的一致性和差异性。

Conclusion: LLM在自动评分中的表现令人鼓舞，其推理过程有助于揭示自动评分和人工评分背后的思考方式，对提升自动评分系统的透明度和公正性具有重要意义。

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [415] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码方法（RCD），指出传统评测方法低估了大语言模型的知识能力，通过RCD能更真实反映模型水平。


<details>
  <summary>Details</summary>
Motivation: 许多语言模型生成的答案被错误判定为不正确，仅因其表达方式不同，这导致模型知识能力被系统性低估。

Method: 作者提出了RCD（Retrieval-Constrained Decoding）解码策略，限制模型输出为唯一表述形式，并构建了YAGO-QA数据集用于评测不同规模开源模型。

Result: 评测显示，传统解码下Llama-3.1-70B的F1得分为32.3%，但用RCD提升到46.0%；Llama-3.1-8B用RCD也优于更大的模型用传统解码。

Conclusion: 标准解码低估了模型能力，RCD更公平可靠地反映知识水平，新评测方法有望更好促进模型发展。

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [416] [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码时对齐框架Cognition-of-Thought（CooT），通过在生成文本时引入认知自我监控机制，提高大语言模型（LLMs）的安全性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然具备复杂推理能力，但仍可能表现出有害行为，且现有对齐方法嵌入在模型权重中，难以修改和灵活应对。因此需要一种能在推理阶段动态控制和监督模型行为的手段。

Method: CooT框架由一个标准文生成器和一个认知感知器（Perceiver）组成，感知器实时监控生成序列，基于一套优先级分明的原则（如安全优先于服从）检测潜在的对齐偏差。一旦识别到违规行为，系统会回滚到出错点，结合通用社会规范和上下文警告，进行引导后重新生成。

Result: 通过多项基准测试和多种模型实验，CooT方法显著提升了模型在安全性和社会推理能力方面的表现。

Conclusion: CooT框架将模型对齐从固定属性转变为显式、动态、可审计的流程，能够在无需重新训练的前提下灵活调整政策，有效提升LLMs输出的安全性和适应性。

Abstract: Large language models (LLMs) excel at complex reasoning but can still exhibit
harmful behaviors. Current alignment strategies typically embed safety into
model weights, making these controls implicit, static, and difficult to modify.
This paper introduces Cognition-of-Thought (CooT), a novel decoding-time
framework that equips LLMs with an explicit cognitive self-monitoring loop.
CooT couples a standard text Generator with a cognitive Perceiver that
continuously monitors the unfolding sequence. The Perceiver uses a structured,
precedence-based hierarchy of principles (e.g., safety over obedience) to
detect potential misalignments as they arise. When violations are flagged, CooT
intervenes by rolling back the generation to the point of error and
regenerating under injected guidance that combines universal social priors with
context-specific warnings. CooT thus transforms alignment from a fixed property
into an explicit, dynamic, and auditable process active during inference,
allowing for flexible policy updates without retraining the model. Extensive
experiments across multiple benchmarks and model families confirm that CooT
consistently improves safety and social reasoning performance.

</details>


### [417] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: 本文综述并分析了大规模评估中自动化试题难度预测的研究进展，比较了经典机器学习与最新语言模型的效果，总结了相关模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统的试题难度建模依赖于现场测试、经典测验理论(CTT)或项目反应理论(IRT)，过程耗时且成本高昂。为解决这些问题，近年来，基于文本和机器学习的方法成为替代方案，本文旨在系统回顾该领域的相关研究，并评估自动化方法的有效性。

Method: 作者回顾了截至2025年5月发表的37篇相关文献，总结每篇论文的数据集、难度参数、学科领域、题型、样本数量、训练和测试分割、输入特征、模型类型、评价标准及模型性能，比较了传统机器学习与最新语言模型（如transformer架构）在难度预测上的表现。

Result: 研究显示，虽然经典机器学习模型仍具可解释性，但基于Transformer的语言模型在无需人工特征工程的情况下也能很好地捕捉句法和语义特征。在实际应用中，文本方法在难度预测的均方根误差（RMSE）最低可达0.165，Pearson相关系数最高0.87，准确率最高0.806。

Conclusion: 基于文本的自动化试题难度预测方法在效率和性能上展现出巨大潜力，可为后续研究提供基准。文中还讨论了实际应用影响，并提出了未来研究的方向。

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [418] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 本论文探讨了在零样本与小样本学习中，大语言模型（LLM）提示中角色设计的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程（prompt engineering）被广泛研究，但目前尚未深入探讨在提示中不同角色配置对模型表现的影响。因此，论文旨在探索角色设计在提示中的作用。

Method: 作者针对GPT-3.5、GPT-4o、Llama2-7b及Llama2-13b，测试了在零样本和小样本场景下，不同角色配置的效果。使用的任务涵盖情感分析、文本分类、问答和数学推理，并进行模型表现评估。

Result: 研究发现，通过基于角色的提示设计，能够提升LLM在多种任务上的表现。

Conclusion: 角色配置是提升大语言模型任务表现的一个有效方向，未来值得深入探索与应用基于角色的提示结构。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [419] [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
*Bassam Matar,Mohamed Fayed,Ayman Khalafallah*

Main category: cs.CL

TL;DR: 本文提出了一种名为AraS2P的语音转音素系统，通过两阶段训练、创新性地结合了音素感知预训练与增强策略，在Iqra'Eval 2025比赛中获得第一。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语语音转音素及音素级发音错误检测尚缺高效模型，需应对人类朗读的多样性和错误类型，推动相关资源发展。

Method: 基于Wav2Vec2-BERT，通过两阶段训练：（1）在大规模阿拉伯语音音素数据上进行任务自适应连续预训练（数据由MSA Phonetiser转换获得）；（2）在官方竞赛数据上微调，并利用XTTS-v2合成多样化诵读增强数据，模拟各种人类发音错误。

Result: AraS2P系统在该竞赛官方排行榜中排名第一，表现优异。

Conclusion: 音素感知预训练结合目标化增强方法能显著提升音素级错误检测性能，为语音转音素系统提供有效方向。

Abstract: This paper describes AraS2P, our speech-to-phonemes system submitted to the
Iqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training
strategy. In the first stage, task-adaptive continue pretraining was performed
on large-scale Arabic speech-phonemes datasets, which were generated by
converting the Arabic text using the MSA Phonetiser. In the second stage, the
model was fine-tuned on the official shared task data, with additional
augmentation from XTTS-v2-synthesized recitations featuring varied Ayat
segments, speaker embeddings, and textual perturbations to simulate possible
human errors. The system ranked first on the official leaderboard,
demonstrating that phoneme-aware pretraining combined with targeted
augmentation yields strong performance in phoneme-level mispronunciation
detection.

</details>


### [420] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一种主动学习框架，通过结合人类和大语言模型（LLMs）的标注方式，降低阿拉伯语情感分析的数据标注成本，并在多个深度学习模型和数据集上验证了方法的高效性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语情感分析因高质量标注数据短缺而进展缓慢，手工标注成本高昂。目前尚缺乏基于主动学习和大语言模型辅助标注的阿拉伯语情感分析研究。本文旨在探索如何结合主动学习和LLMs，减少标注工作量并提升分析效果。

Method: 作者提出一种新的主动学习框架，实验中采用LSTM、GRU和RNN等神经网络，对三大阿拉伯语主流数据集（Hunger Station、AJGT、MASAC）进行实证。通过对人类标注和多种大语言模型辅助标注进行比较，评估了GPT-4o、Claude 3 Sonnet、Gemini 2.5 Pro、DeepSeek Chat和LLaMA 3 70B Instruct等LLMs在不同任务上的表现。

Result: 结果显示，使用LLM辅助主动学习，模型表现可媲美或优于人类标注。例如用GPT-4o标注的Hunger Station数据集，仅需450条标注样本，LSTM模型准确率达93%；DeepSeek Chat在MASAC上仅用650条标注样本，准确率达82%，与人工标注相当。

Conclusion: LLM辅助主动学习方法能显著降低阿拉伯语情感分析所需的人工标注量，同时保持甚至提升模型性能。此方法为低资源语言的情感分析任务提供了可行且高效的路径。

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [421] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文关注于大语言模型作为评判者（LLM-as-a-judge）在真实部署场景下面临的可用性与泛化问题，系统分析了未来适应性、兼容性和问题泛化三个维度，并提出了实验和改进建议。


<details>
  <summary>Details</summary>
Motivation: 虽然经过微调的评判模型（judges）效果和效率优于直接使用大型预训练模型作为评判者，但在实际应用中，其面对未来模型、历史模型以及新的问答场景时的表现尚不清楚，因此需要系统评估其实际可用性。

Method: 在数学领域，作者设置了变化的训练和测试分布，选用三种微调算法（包括SFT、DPO）和三种基础模型，对未来适应性、兼容性、问题泛化等方面进行统一实验和比较，并分析了持续学习（continual learning）与单一训练策略的差异。

Result: 实验显示，评判模型在适应未来生成模型响应表现不佳（未来适应性弱），但对历史模型的表现较好（兼容性强）。DPO训练方法在这两个方面都有明显提升。持续学习能够更好地平衡对新旧生成模型的适应性。此外，所有模型在面对未见过的新问题时表现下降，说明泛化能力有限。

Conclusion: 当前微调评判模型尚未能很好应对未来生成模型或新场景，泛化性也不强。论文的结果为评判模型在实际复杂环境中的开发和部署提供了新的视角和实证参考。

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [422] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: 本文提出了一个针对希腊语医学语音转录的系统，通过结合自动语音识别和文本纠正模型，有效提升了专业术语和语言变体的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前医疗文档录入工作量大，传统的语音转写系统难以适应希腊语医学领域中的专业术语和语言多样性，严重影响医疗工作效率。

Method: 作者将自动语音识别（ASR）技术与文本纠正模型结合应用，并对现有语音和语言技术进行希腊语医学领域的定向微调，提升系统对复杂医学术语与语言不一致性的适应能力。

Result: 通过领域特定的数据微调，系统在希腊语医学语音转录中取得了更高的准确率和连贯性，较好应对了医学术语和语言变体。

Conclusion: 本系统显著提升了希腊语医疗行业语音转录的准确率和实用性，有助于减少医生书写负担，提升医疗流程效率。

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [423] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 该论文提出MoRSD方法，通过高质量推理链选择提升小型语言模型的推理能力，比以往只注重数据量的方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有链式思考（CoT）蒸馏更多关注数据量，忽视了推理链质量，导致将噪声或错误信息传递给学生模型，从而影响推理能力提升。

Method: 提出了模型导向推理链选择蒸馏（MoRSD）方法，通过度量并筛选高质量推理链用于模型蒸馏，具体提出了推理链难度（RD）指标，衡量学生模型在给定推理链情况下的答题能力，并结合准确性、多样性和难度筛选推理链。

Result: 在七个数据集、三个任务上，MoRSD方法相较于基线平均提升4.6%，并且用更少但质量更高的推理链来训练学生模型。

Conclusion: 高质量推理链比全部数据更能提升学生模型的推理能力，MoRSD为高效CoT蒸馏提供了新的解决方案。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [424] [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
*Kevin Frank,Anmol Gulati,Elias Lumer,Sindy Campagna,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了Jackal，这是一个规模大、针对Jira查询语言（JQL）的自然语言到JQL映射基准数据集，并用于评估当前大型语言模型生成JQL的能力。实验显示即便是最先进的模型在不同类型的请求上表现仍有很大差异，存在显著改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管Jira的JQL查询在企业中广泛使用，但社区缺乏可以评估自然语言到JQL映射性能的真实、可执行基准数据集，这制约了相关技术的发展。

Method: 作者构建了一个包含10万个自然语言请求与JQL对的数据集Jackal，包括长短请求、语义相似与完全一致四类问法，并基于Jira实例的执行结果进行验证。他们还开发了执行型评测工具，并系统性评测了23种LLM在不同维度上的表现。

Result: 在Jackal-5K（5000组数据子集）上的基准测试表明，当前最优模型（Gemini 2.5 Pro）的总体执行准确率仅为60.3%，不同请求类型表现差异大，如长文本准确率为86.0%，短文本仅35.7%，语义相似更低至22.7%，语义完全一致则能达99.3%。

Conclusion: Jackal数据集和评测体系揭示了当前LLM在企业实际JQL生成中的不足，也为将来基于执行的NL-to-JQL研究提供了标准与挑战，推动领域发展。

Abstract: Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter
issues from Jira. Yet, to our knowledge, there is no open, real-world,
execution-based benchmark for mapping natural language queries to JQL. We
introduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000
natural language (NL) requests paired with validated JQL queries and
execution-based results on a live Jira instance with over 200,000 issues. To
reflect real-world usage, each JQL query is associated with four types of user
requests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)
Semantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,
together with an execution-based scoring toolkit, and a static snapshot of the
evaluated Jira instance for reproducibility. We report text-to-JQL results on
23 Large Language Models (LLMs) spanning parameter sizes, open and closed
source models, across execution accuracy, exact match, and canonical exact
match. In this paper, we report results on Jackal-5K, a 5,000-pair subset of
Jackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only
60.3% execution accuracy averaged equally across four user request types.
Performance varies significantly across user request types: (i) Long NL
(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)
Semantically Exact (99.3%). By benchmarking LLMs on their ability to produce
correct and executable JQL queries, Jackal exposes the limitations of current
state-of-the-art LLMs and sets a new, execution-based challenge for future
research in Jira enterprise data.

</details>


### [425] [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
*JinXin Li,Gang Tu,JunJie Hu*

Main category: cs.CL

TL;DR: 提出了一种基于频域分析的LLM幻觉检测新方法HSAD，通过对隐藏层时序信号做傅里叶变换来捕捉推理过程中的异常，实现更高效的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法要么依赖事实一致性，受知识覆盖面制约，要么依赖静态特征，难捕捉推理偏差，无法有效发现LLM生成内容中的幻觉。

Method: 受认知神经科学信号分析方法启发，将LLM推理比作一个时序认知历程，捕捉其隐藏层时序信号动态。采用快速傅里叶变换(FFT)将信号映射到频域，提取频谱特征，基于这些特征设计检测算法识别幻觉。

Result: 频谱特征分析实验证明该方法有效，基于频域特征的HSAD检测算法能更好识别生成内容中的幻觉。

Conclusion: HSAD方法有效结合推理过程建模与频域特征提取，克服现有方法知识覆盖与推理偏差检测的局限，检测准确性和鲁棒性更高。

Abstract: Although Large Language Models have demonstrated powerful capabilities in a
wide range of tasks such as language understanding and code generation, the
frequent occurrence of hallucinations during the generation process has become
a significant impediment to their deployment in critical application scenarios.
Current mainstream hallucination detection methods rely on factual consistency
verification or static hidden layer features. The former is constrained by the
scope of knowledge coverage, while the latter struggles to capture reasoning
biases during the inference process. To address these issues, and inspired by
signal analysis methods in cognitive neuroscience, this paper proposes a
hallucination detection method based on the frequency-domain analysis of hidden
layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal
\textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's
reasoning process as a cognitive journey that unfolds over time, we propose
modeling and simulating the human process of signal perception and
discrimination in a deception-detection scenario through hidden layer temporal
signals. Next, The Fast Fourier Transform is applied to map these temporal
signals into the frequency domain to construct spectral features, which are
used to capture anomalies that arise during the reasoning process; analysis
experiments on these spectral features have proven the effectiveness of this
approach. Finally, a hallucination detection algorithm is designed based on
these spectral features to identify hallucinations in the generated content. By
effectively combining the modeling of the reasoning process with
frequency-domain feature extraction, the HSAD method overcomes the limitations
of existing approaches in terms of knowledge coverage and the detection of
reasoning biases, demonstrating higher detection accuracy and robustness.

</details>


### [426] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: 本文发现Instruct模型后训练(post-training)对模型权值改动极小，针对由此带来的探索能力减弱问题，提出一种无需再训练即可改进Instruct模型“Timber”方法。该方法通过对权重变动进行有针对性的细致调整，提升模型的探索能力，在Llama和Qwen系列实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的后训练(post-training)方法主要让基础模型(Base Model)转化为指令模型(Instruct Model)，但这种转化常被认为只带来表面上的改动，实际上可能限制了模型的探索能力。因此作者想量化这一“表浅”改动，并寻找无需再训练即可提升模型探索能力的新方法。

Method: 作者首先从权重层面量化Instruct模型和基础模型的权重有效秩(eRank)变化，证明后训练改动极小。进而，作者提出Timber方法，通过对Instruct和Base模型权重增量进行细致、定向回退调整，改善模型探索能力，无需对模型做额外训练。

Result: 在Llama和Qwen两个知名开源大模型体系中，Timber方法显著提升了Instruct模型的Pass@k等指标，优于仅做后训练的Instruct模型。

Conclusion: 本文证实了后训练Instruct模型时权重改动很有限，并发现这种“表浅”改动带来探索/利用能力的权衡。提出的Timber方法无需训练即可改进Instruct模型，在实际模型改造和理解中具有重要意义。

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [427] [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
*Haoyu Zheng,Zhuonan Wang,Yuqian Yuan,Tianwei Lin,Wenqiao Zhang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang,Hongyang He*

Main category: cs.CL

TL;DR: 本文提出了一种高效推理的大语言模型推理框架，通过训练时使用简明的思维链（CoT）草图，学习一套离散策略先验码本，推理时无需生成冗长推理过程，仅用少量思维向量即可实现策略级引导，并结合动态路由机制降低算力消耗。


<details>
  <summary>Details</summary>
Motivation: 当前多步推理大模型普遍依赖冗长的显式推理（Chain-of-Thought），导致推理效率低、成本高，因此亟需提升推理速度、降低资源消耗，同时保持乃至提升准确性。

Method: 作者提出Latent Codebooks for Fast Thinking框架：训练阶段用简明CoT草图学习离散策略码本，推理时用连续思维向量（蒸馏自码本）做策略级引导，不再生成全部推理token。此外，还提出GainRouter路由机制，可按需在快/慢推理路径间切换，自动抑制过度推理。

Result: 在多个推理基准测试中，该方法在实现与现有技术相媲美或更优准确度的同时，大幅降低模型推理时的延迟及token用量。

Conclusion: 本工作为大语言模型复杂推理提供了一种效率高、可控性强的新方向，其策略可显著降低推理成本并提高实用性。

Abstract: Reasoning-oriented Large Language Models (LLMs) often rely on generating
explicit tokens step by step, and their effectiveness typically hinges on
large-scale supervised fine-tuning or reinforcement learning. While
Chain-of-Thought (CoT) techniques substantially enhance performance on complex
reasoning tasks, they remain inefficient, requiring long reasoning traces that
increase latency and token usage. In this work, we introduce Latent Codebooks
for Fast Thinking, a framework that uses concise CoT sketches only during
training to learn a codebook of discrete strategy priors. At inference, the
model conditions on a handful of continuous thinking vectors distilled from the
codebook in a single pass, enabling strategy-level guidance without producing
explicit reasoning tokens. To complement this design, we propose GainRouter, a
lightweight routing mechanism that adaptively switches between fast codebook
guided inference and slow explicit reasoning, thereby suppressing overthinking
and reducing unnecessary token generation. Experiments across multiple
reasoning benchmarks show that our approach achieves competitive or superior
accuracy while substantially lowering inference cost, offering a practical path
toward efficient and controllable reasoning in large language models.

</details>


### [428] [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
*Zemin Huang,Yuhang Wang,Zhiyang Chen,Guo-Jun Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为RemeDi的Remasking机制，显著提升了基于Diffusion的语言模型在文本生成过程中的纠错与灵活性能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Diffusion的掩码语言模型一旦生成token后通常不会校正错误，难以对已生成的低质量token进行修改。作者希望通过新机制提升模型发现和修正错误token的能力。

Method: RemeDi在每个生成步骤中同时预测token分布和每个token的置信度分数，通过置信度决定哪些token需要重新掩码后再生成。训练中，用有监督微调增强模型对错误token的检测和重新掩码能力，并结合强化学习优化完整生成轨迹。

Result: RemeDi在多个数据集上取得了开源Diffusion语言模型中的最新最优表现。

Conclusion: 引入Remasking机制能有效提升Diffusion语言模型的文本精炼、纠错和整体生成质量，推动了该领域的进步。

Abstract: Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect
tokens: once a token is generated, it typically remains fixed. The key
challenge is to identify potential errors in the inputs. In this paper, we
propose \emph{\underline{Rem}asking-\underline{e}nabled \underline{Di}ffusion
Language Model (RemeDi}, a mask-based DLM that introduces \emph{remasking} as
another fundamental mechanism, enabling more flexible text refinement in
diffusion-based text generation. To achieve this, RemeDi jointly predicts token
distributions and per-token confidence scores at each step. The confidence
scores determine which tokens to be unmasked after the current step, allowing
the model to identify tokens with low quality and remask them. These remasked
tokens can be resampled with richer context in subsequent steps. We design a
remask-aware pipeline to train this ability, including supervised fine-tuning
which teaches the model to detect and remask incorrect tokens in addition to
predict mask tokens, and reinforcement learning which optimizes full generation
trajectories toward higher rewards. Experiments show that RemeDi achieves the
state-of-the-art results among open-source DLMs on multiple datasets.

</details>


### [429] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: 本文系统地比较了强化学习（RL）和有监督微调（SFT）在大型语言模型复杂跨语言推理能力提升方面的效果，发现RL在跨语言泛化与推理策略上优于SFT。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLMs）在复杂推理和跨语言泛化能力方面的表现，特别是在使用RL和SFT两种常见微调策略下，其对多语言推理任务的影响。

Method: 基于Qwen2.5-3B-Base模型，在数学推理、常识推理和科学推理等多语言基准数据集上，分别采用RL和SFT进行微调，并对比其推理准确性和跨语言泛化能力。同时通过机制分析探究RL优越性的内在原因。

Result: （1）RL微调比SFT在准确率和跨语言泛化能力上表现更好；（2）RL在非英语数据上训练优于英语数据训练，而SFT则无此现象。

Conclusion: RL能够赋予模型更强大和稳健的跨语言推理策略，为今后公平高效的多语言推理提供了重要参考。

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [430] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 大语言模型在多语种环境下性能表现不一致，尤其非英语表现较差。作者提出一种批量对齐微调方法，显著提升了非英语语言的模型准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型主要以英语为预训练核心，导致在其他语种（中低资源语种）上的表现明显落后，这严重影响了全球企业在客户支持、内容审核等多语种场景下的服务一致性和可靠性。即使是采用了最新的RAG技术，非英语语言的准确率依然大幅下滑。

Method: 作者提出了一种实用的批量对齐（batch-wise alignment）微调策略。具体做法是在每个训练批次中引入语义等价的多语种数据，以直接对齐模型在不同语种下的输出。该方法易于实现、可扩展，并能无缝集成进已有的LLM训练与部署流程。

Result: 采用该对齐方法后，非英语语言的模型准确率最高提升了23.9%，且不会损害英语的性能、模型推理能力或信息检索质量。

Conclusion: 该方法能够有效提升LLMs在多语种场景下的表现，对行业实现更强健、更公平的多语种AI方案提供了简单且高效的技术路径。

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [431] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: 论文提出了专门用于评估大语言模型（LLM）在代码语义推理能力方面的新基准——TF-Bench，并揭示了现有LLM在真正理解程序语义方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于软件工程领域，仅凭现有评测方式无法有效区分模型是否真正理解程序语义。因此需要一个新框架来更准确地评估其推理能力。

Method: 作者设计了TF-Bench，通过类型推断（System F）任务衡量模型的程序语义推理能力，并且通过移除无关自然语言构建了TF-Bench_pure，以聚焦于语义而非语言表层。同时引入了两个新指标衡量模型的鲁棒性及推理效果。

Result: 实验显示即使最强的模型（Claude-3.7-sonnet）在TF-Bench_pure上的表现也仅有55.85%的准确率，暴露了主流LLM在深层语义推理上的局限性。

Conclusion: 当前LLM对代码语义理解远未达令人满意的水平，现有评测和能力都有明显短板。论文为未来研究提供了新的基准工具与评估方向。

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [432] [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
*Zhe Hu,Yixiao Ren,Guanzhong Liu,Jing Li,Yu Yin*

Main category: cs.CL

TL;DR: VIVA+是一个针对多模态大语言模型（MLLMs）决策与推理能力的新评测基准，涵盖了1,317个实际场景及6,373个多选题，旨在系统地评价模型在人类中心环境中的社会化理解与推理能力。实验揭示了模型表现的差异性、存在的挑战，并提出多步推理等能有效提升模型表现的方法。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型（MLLMs）在人类中心复杂环境下进行细致、人类般推理和决策的能力，目前缺乏系统、认知基础强的真实环境评测基准。

Method: 提出VIVA+基准，包含1,317个真实情景和6,373个多选题，针对'情境理解'、'基于上下文行为解释'和'反思性推理'三大决策核心能力进行系统评测。并对最新商用和开源MLLMs进行对比实验，还探索了针对性训练和多步推理提升方法。

Result: 评测显示不同MLLMs在各能力维度上的表现差异较大，当前模型在人类中心环境决策仍面临显著挑战。采用多步推理和针对性训练可带来稳定的性能提升。

Conclusion: VIVA+为MLLMs的社会化推理和决策能力评测提供了系统工具，揭示了现有模型的不足，并为未来提升模型的情境感知、决策能力提供了有价值的方向。

Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied
agents in operating meaningfully in complex, human-centered environments. Yet,
evaluating their capacity for nuanced, human-like reasoning and decision-making
remains challenging. In this work, we introduce VIVA+, a cognitively grounded
benchmark for evaluating the reasoning and decision-making of MLLMs in
human-centered situations. VIVA+ consists of 1,317 real-world situations paired
with 6,373 multiple-choice questions, targeting three core abilities for
decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven
Action Justification, and (3) Reflective Reasoning. Together, these dimensions
provide a systematic framework for assessing a model's ability to perceive,
reason, and act in socially meaningful ways. We evaluate the latest commercial
and open-source models on VIVA+, where we reveal distinct performance patterns
and highlight significant challenges. We further explore targeted training and
multi-step reasoning strategies, which yield consistent performance
improvements. Finally, our in-depth analysis highlights current model
limitations and provides actionable insights for advancing MLLMs toward more
robust, context-aware, and socially adept decision-making in real-world
settings.

</details>


### [433] [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
*Zhiqiang Liu,Yichi Zhang,Mengshu Sun,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多模态知识图谱补全方法M-Hyper，结合了融合和集成两类主流范式，利用双四元数数学工具实现更高效的信息表示与交互，并在多项实验中取得了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态知识图谱补全方法存在两大局限：1）融合式方法丧失模态专属信息且适应性有限，2）集成式方法则难以建模跨模态的细粒度语义联系。

Method: M-Hyper方法创新性地借鉴四元数代数，将三个独立模态和一个融合模态分别映射到双四元数的四个正交基，并通过Hamilton积有效建模模态间的两两关系。具体包含细粒度实体分解（FERF）和鲁棒关系感知模态融合（R2MF）两个模块，以获得各模态鲁棒表示。

Result: 在多项基准数据集上的实验显示，M-Hyper方法在性能、鲁棒性及计算效率方面均优于现有同类方法，取得了新的最优实验结果。

Conclusion: M-Hyper有效结合了融合与集成范式的优点，能够兼顾模态独立性与复杂的跨模态交互，为多模态知识图谱补全任务提供了高效而稳定的解决方案。

Abstract: Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts
in multi-modal knowledge graphs (MMKGs) by leveraging both structural
relationships and diverse modality information of entities. Existing MMKGC
methods follow two multi-modal paradigms: fusion-based and ensemble-based.
Fusion-based methods employ fixed fusion strategies, which inevitably leads to
the loss of modality-specific information and a lack of flexibility to adapt to
varying modality relevance across contexts. In contrast, ensemble-based methods
retain modality independence through dedicated sub-models but struggle to
capture the nuanced, context-dependent semantic interplay between modalities.
To overcome these dual limitations, we propose a novel MMKGC method M-Hyper,
which achieves the coexistence and collaboration of fused and independent
modality representations. Our method integrates the strengths of both
paradigms, enabling effective cross-modal interactions while maintaining
modality-specific information. Inspired by ``quaternion'' algebra, we utilize
its four orthogonal bases to represent multiple independent modalities and
employ the Hamilton product to efficiently model pair-wise interactions among
them. Specifically, we introduce a Fine-grained Entity Representation
Factorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)
module to obtain robust representations for three independent modalities and
one fused modality. The resulting four modality representations are then mapped
to the four orthogonal bases of a biquaternion (a hypercomplex extension of
quaternion) for comprehensive modality interaction. Extensive experiments
indicate its state-of-the-art performance, robustness, and computational
efficiency.

</details>


### [434] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）在解答和解释罗马尼亚交通法规问答任务中的表现，发布了1,208道题目数据集，并分析不同模型及微调方式的效果，提出了文本描述比直接图像输入效果更好等发现，对低资源语言可解释问答有启示。


<details>
  <summary>Details</summary>
Motivation: 确保新手和有经验的驾驶员都能掌握最新交通规则对道路安全至关重要，但现有针对罗马尼亚语等低资源语言的交通法规问答和解释系统有限。研究旨在推动解释型问答系统在低资源语言中的发展，并评估LLM在此类任务中的能力。

Method: 作者发布了包含1,208道问题（其中387为多模态题）及其解释的数据集，系统比较了当前文本和多模态SOTA模型表现，并对Llama 3.1-8B-Instruct和RoLlama 3.1-8B-Instruct进行了领域特定微调。同时，比较了图片文本描述和直接图像输入的效果，并用LLM担任解释质量评审以揭示偏好。

Result: 主流SOTA模型整体表现良好，经微调后的8B参数模型也具有竞争力。将图像内容转换为文本描述的方式优于直接输入图片。通过让LLM评审解释质量，发现其倾向于偏好自身生成的答案。

Conclusion: SOTA和微调小参数模型均适合罗马尼亚交通法规问答与解释任务。文本化图像信息效果突出。该研究为低资源语言下的可解释问答系统研究带来新思路，强调了数据和评测方式选择的重要性。

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [435] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出了一种基于逻辑的评估框架，系统分析多模态大语言模型（MLLMs）在多模态推理中的优势与瓶颈，发现模型的多模态集成能力是提升推理表现的主要障碍。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs能够整合多种模态（文本、视觉、音频）以期提升推理能力，但目前对多模态互动对推理的正负影响缺乏系统分析和严格评估，特别是缺乏对内部机制的深入剖析，这导致结论存在矛盾和不一致。

Method: 作者提出一个逻辑基础的评估框架，将多模态推理划分为六种互动模式，考察事实在不同模态的分布及其逻辑组合，进一步通过实验考察不同模态组合对推理的具体影响，并分析模型注意力模式与融合机制。

Result: 实验发现，只有在不同模态提供独立且充分的信息时，多模态推理性能才提升；冗余信息或链式支持反而削弱推理能力。此外，推理性能下降主要表现在：弱模态拖累整体效果、模态冲突带来偏向、以及不同信号融合失败。作者揭示了任务组合瓶颈和融合瓶颈两大核心问题，并通过两步提示和早期注意力软化手段部分恢复推理表现。

Conclusion: 多模态推理的主要障碍在于信息融合而非感知，建议采用组合意识训练和早期融合控制作为改进方向，以提升模型的多模态推理能力。

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [436] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: 本文研究了语音集成到大语言模型中导致文本能力下降的问题，揭示了原因，并提出两种缓解方法，有效提升了文本与语音任务的整体表现。


<details>
  <summary>Details</summary>
Motivation: 将语音能力集成至LLM虽然增强了模型的多模态能力，但会破坏原有的文本能力，限制了模型对文本知识的利用，亟需分析原因和解决办法。

Method: 作者通过对主流encoder-adaptor范式的参数重要性分布进行分析，提出参数重要性估计框架，并基于发现，测试了分层学习率调度和LoRA（低秩适配）两种缓解策略以保持参数分布。

Result: 实验表明，两种缓解策略在不损失或更好地保持文本能力的同时，提高了语音问答任务的表现。

Conclusion: 论文系统揭示了语音微调带来文本能力下降的机理，所提方法有效克服这一现象，并为后续模型多模态集成提供了理论和实证支持。

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [437] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种新型的一致性强化学习框架（KLCF），在无需外部知识检索或繁重验证的情况下，有效缓解了大模型生成长文本时的幻觉和事实性不足问题，显著提升了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 长文本生成中大语言模型容易产生“幻觉”（即生成与事实不符的信息），而现有RLHF方法主要依赖偏好反馈，忽视了模型自身知识边界，导致事实性问题难以根本解决。作者希望设计一种更关注知识边界与一致性的强化学习机制。

Method: 提出知识层一致性强化学习框架（KLCF），通过知识一致性和双重事实对齐机制，分别提升事实回忆和精准性：一方面用预训练知识边界构建事实列表，引导RL提升事实覆盖和回忆率；另一方面用自我评估模块提升生成时的事实精度。该方法奖励设计完全不依赖外部知识，计算开销较低，易于大规模训练。

Result: 实验显示，KLCF在多个长文本生成基准上大幅提升了事实性指标，有效减少了幻觉现象。

Conclusion: KLCF框架能够高效且扩展性强地提升大模型长文本生成的事实性，是缓解LLM幻觉问题的有力方法。

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [438] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: 该论文提出了一种结合本地和全局记忆的LoGo框架，用于提升大语言模型（LLM）在个性化中的表现，有效解决冷启动和偏置问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化存在冷启动（用户历史数据少无法有效定制）和偏置（用户数据多但单一导致过拟合）两大难题，而这些问题本质上源于模型无法利用用户群体的集体知识。

Method: 作者提出了本地-全局记忆框架（LoGo），将个体化的本地记忆与反映用户群体兴趣的全局记忆相结合，并引入调解器模块来处理本地与全局信号间的冲突。

Result: 实验证明，LoGo框架在多个基准数据集上显著提升了LLM的个性化水平，既改善了冷启动用户的表现，也减少了偏置预测。

Conclusion: 融合集体知识对于提升LLM的个性化至关重要，LoGo框架有效解决了传统个性化方法在冷启动与偏置上的不足。

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [439] [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
*Yoonah Park,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 本文发现大语言模型在多项选择题上的错误预测，源自模型隐藏状态中知识与预测的错位。为此，提出了一种基于投影的KAPPA方法，通过对隐藏状态进行参数无关的调整，有效提升了模型准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在生成任务中显示出正确知识，但在多项选择题（MCQ）上仍表现不佳，存在明显的知识-预测鸿沟。作者旨在揭示这一差距的机制并提出解决方法。

Method: 设计探针分析隐藏状态，识别出包含知识基和预测基的子空间。提出KAPPA，一种无需更改参数、利用投影调整隐藏状态的方法，使预测和知识坐标对齐。

Result: 在Big-Bench-Hard与ARC-Challenge的二选一任务上，KAPPA显著提升模型准确率，较现有方法表现更优。此外，KAPPA在不同任务间具有一定泛化能力，并可扩展至自由问答任务。

Conclusion: KAPPA在几何上解释了知识-预测不一致现象，并为提升大模型推理表现提供了结构化、实用的调优策略。

Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)
despite demonstrating correct knowledge in other contexts, such as free-form
generation. To investigate the mechanism underlying this knowledge-prediction
gap on MCQs and alleviate it, we conduct a probing analysis and find that
residual streams in certain layers contain a subspace spanned by two important
bases: a \emph{knowledge basis} that encodes the probability of the
ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes
the probability of the answer choice predicted by the model. We observe that
incorrect predictions arise from a misalignment of the model's hidden states
along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned
Prediction through Projection-based Adjustment), a parameter-free intervention
that transforms the hidden states to align the prediction coordinate with the
knowledge coordinate within this subspace. Experiments on binary-choice
reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA
substantially improves accuracy and consistently outperforms baselines. While
optimal subspaces differ across tasks, subspaces generalize to some extent, as
supported by cross-dataset experiments. Moreover, KAPPA extends its
effectiveness to free-form questions beyond MCQs. Our work provides a new
geometric understanding of the knowledge-prediction gap and offers a practical
method for better aligning model behavior with its latent knowledge.

</details>


### [440] [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
*Muhammad Abu Ahmad,Mohamad Ballout,Raia Abu Ahmad,Elia Bruni*

Main category: cs.CL

TL;DR: 本文提出了一种混合检索增强生成（RAG）系统，结合稀疏和稠密检索技术以及交叉编码器重排序，显著提升了大语言模型在伊斯兰知识理解与推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 针对伊斯兰知识理解和推理这一挑战，现有LLM表现有限，亟需提升信息检索与生成能力，推动知识密集型任务的应用。

Method: 采用三阶段流程：1）使用BM25算法进行初步文本检索；2）稠密嵌入检索模型实现语义匹配；3）交叉编码器进行内容精细重排序。系统最终由LLM（Fanar或Mistral）结合检索到的文档生成回答。

Result: 该系统在两个子任务上均带来性能提升，准确率最高提升达25%。最佳配置（使用Fanar模型）在子任务1和2中分别获得45%和80%的准确率。

Conclusion: 混合RAG系统显著提高了LLM在伊斯兰知识推理领域的准确性，可为后续相关研究与实际应用提供有力工具。

Abstract: This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.

</details>


### [441] [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
*Arti Rani,Shweta Singh,Nihar Ranjan Sahoo,Gaurav Kumar Nayak*

Main category: cs.CL

TL;DR: 本文提出了OpenBiasBench基准和Open-DeBias方法，实现了大语言模型（LLM）在更广泛开放集偏见检测与消除上的突破，提升了常见与新型偏见场景下的公平性表现。新方法在多任务和多语言环境下均表现出强健的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在问答任务中取得了巨大成功，但仍存在潜在的社会和刻板印象偏见，并且多数偏见消除方法仅能覆盖预定义类别，难以应对新出现或者更细粒度的隐性偏见。本文旨在开发可应对开放集偏见检测和消除的新范式，提高模型在实际应用中的公平性和可信度。

Method: 作者提出了一个新的基准集OpenBiasBench，涵盖更广泛的偏见类型用于全面评测；并提出了一种高效的数据和参数分离的去偏见方法Open-DeBias，通过引入adapter模块，有效削弱模型的社会与刻板印象偏见，同时具备对未知偏见的泛化能力。该方法仅需极少量训练数据即可迁移并适用于多语言环境。

Result: Open-DeBias方法在BBQ数据集的歧义子集准确率提升了近48%，在去歧义子集提升6%；以极少训练数据fine-tune的adapter在韩语数据上zero-shot转移取得了84%的准确率。此外，在StereoSet和CrowS-Pairs等偏见评测任务上也显示出了多任务、多语言下的高效去偏见效果和泛化能力。

Conclusion: 本文工作突破了偏见消除在类别和场景上的局限，证明了Open-DeBias可广泛适用于多任务、多语言和开放领域的大语言模型去偏见问题，支持其作为通用公平性提升模块的应用价值。

Abstract: Large Language Models (LLMs) have achieved remarkable success on question
answering (QA) tasks, yet they often encode harmful biases that compromise
fairness and trustworthiness. Most existing bias mitigation approaches are
restricted to predefined categories, limiting their ability to address novel or
context-specific emergent biases. To bridge this gap, we tackle the novel
problem of open-set bias detection and mitigation in text-based QA. We
introduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases
across a wide range of categories and subgroups, encompassing both known and
previously unseen biases. Additionally, we propose Open-DeBias, a novel,
data-efficient, and parameter-efficient debiasing method that leverages adapter
modules to mitigate existing social and stereotypical biases while generalizing
to unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias
improves QA accuracy on BBQ dataset by nearly $48\%$ on ambiguous subsets and
$6\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction
of the training data. Remarkably, the same adapters, in a zero-shot transfer to
Korean BBQ, achieve $84\%$ accuracy, demonstrating robust language-agnostic
generalization. Through extensive evaluation, we also validate the
effectiveness of Open-DeBias across a broad range of NLP tasks, including
StereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,
and suitability for general-purpose, open-domain bias mitigation. The project
page is available at: https://sites.google.com/view/open-debias25

</details>


### [442] [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
*Ziyi Yang,Weizhou Shen,Ruijun Chen,Chenliang Li,Fanqi Wan,Ming Yan,Xiaojun Quan,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出了SPELL框架，实现长文本推理场景下的大语言模型无标签自我优化。通过集成提问者、回答者和验证者三个角色，SPELL能持续自我提升，在六项长上下文任务中显著提升表现，并超越同规模有标注微调模型。


<details>
  <summary>Details</summary>
Motivation: 长文本推理是大语言模型的一个重要挑战，但由于缺乏高质量标注数据和自动评价信号，进展缓慢。该工作旨在解决上述难题，实现无需人工标注即可提升长文本推理能力。

Method: 提出SPELL多角色自博弈强化学习框架，在一个模型中集成提问者、回答者与验证者三环节。提问者从原始文档生成问题，回答者基于文档作答，验证者判定答复与标准答案的语义一致性并产生奖励。在自适应课程学习与奖励机制下，模型自动提升处理更长文本和更复杂问题的能力。

Result: 在六个长上下文推理基准测试上，SPELL能提升多种LLM的表现，优于同规模、用大规模有标注数据微调的模型。在Qwen3-30B-A3B-Thinking等强推理能力模型上平均pass@8提升7.6个百分点。

Conclusion: SPELL实现了可扩展、无标签的长文本推理能力提升，有望推动更强大模型的发展。其三角色自博弈和智能课程机制为LLM优化提供了新路径。

Abstract: Progress in long-context reasoning for large language models (LLMs) has
lagged behind other recent advances. This gap arises not only from the
intrinsic difficulty of processing long texts, but also from the scarcity of
reliable human annotations and programmatically verifiable reward signals. In
this paper, we propose SPELL, a multi-role self-play reinforcement learning
framework that enables scalable, label-free optimization for long-context
reasoning. SPELL integrates three cyclical roles-questioner, responder, and
verifier-within a single model to enable continual self-improvement. The
questioner generates questions from raw documents paired with reference
answers; the responder learns to solve these questions based on the documents;
and the verifier evaluates semantic equivalence between the responder's output
and the questioner's reference answer, producing reward signals to guide
continual training. To stabilize training, we introduce an automated curriculum
that gradually increases document length and a reward function that adapts
question difficulty to the model's evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves
performance across diverse LLMs and outperforms equally sized models fine-tuned
on large-scale annotated data. Notably, SPELL achieves an average 7.6-point
gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising
its performance ceiling and showing promise for scaling to even more capable
models.

</details>


### [443] [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
*Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种高效利用训练数据的方法Q-Tuning，可在大规模语言模型监督微调过程中，显著减少所需数据量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的监督微调（SFT）阶段变得愈发耗费计算，数据效率变得至关重要。现有的数据剪枝方法要么只关注样本、要么只关注token，没法联合优化，导致数据利用效率低下。

Method: 提出Error-Uncertainty (EU) Plane框架，对样本及token两维度的数据效用进行联合诊断，并基于此提出Quadrant-based Tuning (Q-Tuning)方法，分两步进行：先筛选高质量样本，再按样本类型，对不同token施加差异化的剪枝策略，实现动态且细粒度的数据剪枝。

Result: 在五个基准测试上创下新纪录，在SmolLM2-1.7B模型上，Q-Tuning用12.5%的数据实现了比全量训练高38%的平均性能提升。

Conclusion: Q-Tuning首次实现了动态数据剪枝能力，并且在所有实验中都优于全量训练，为预算有限下的LLM监督微调提供了实用且可扩展的核心思路。

Abstract: As supervised fine-tuning (SFT) evolves from a lightweight post-training step
into a compute-intensive phase rivaling mid-training in scale, data efficiency
has become critical for aligning large language models (LLMs) under tight
budgets. Existing data pruning methods suffer from a fragmented design: they
operate either at the sample level or the token level in isolation, failing to
jointly optimize both dimensions. This disconnect leads to significant
inefficiencies--high-value samples may still contain redundant tokens, while
token-level pruning often discards crucial instructional or corrective signals
embedded in individual examples. To address this bottleneck, we introduce the
Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes
the heterogeneous utility of training data across samples and tokens. Guided by
this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework
that strategically coordinates sample pruning and token pruning. Q-Tuning
employs a two-stage strategy: first, it performs sample-level triage to retain
examples rich in informative misconceptions or calibration signals; second, it
applies an asymmetric token-pruning policy, using a context-aware scoring
mechanism to trim less salient tokens exclusively from misconception samples
while preserving calibration samples in their entirety. Our method sets a new
state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,
Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline
using only 12.5\% of the original training data. As the first dynamic pruning
approach to consistently outperform full-data training, Q-Tuning provides a
practical and scalable blueprint for maximizing data utilization in
budget-constrained LLM SFT.

</details>


### [444] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 该论文提出DocPruner，一种用于文档视觉检索（VDR）的自适应嵌入剪枝框架，能大幅减少存储开销，同时几乎不影响检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前多向量VDR方法虽然检索效果提升显著，但每页需存储数百个向量，导致大规模实际部署存储成本过高，限制了应用场景。

Method: DocPruner利用文档内部的patch注意力分布，自适应地识别和删除冗余的patch级嵌入，从而减少存储量。该方法动态调整每个文档的嵌入保留数量，实现高效空间利用。

Result: 在主流多向量VDR模型上，DocPruner能减少50%-60%的存储空间，占用的空间大幅缩减，且检索性能损失可以忽略不计。

Conclusion: DocPruner为VDR系统提供了一种高效、灵活且效果稳健的存储优化方案，支持大规模实际部署，并在十余个代表性数据集上验证了其实用性与有效性。

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [445] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了解决Masked Diffusion Language Models（MDLMs）在解码和强化学习训练上的一致性与效率问题的新方法，并通过实验验证这些改进策略能有效提升MDLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MDLMs虽然具备并行解码和灵活生成等优点，但直接借用自回归（AR）模型的技术会带来训练与推理不一致等问题，影响性能。因此亟需为MDLMs设计专用的解码和训练方法。

Method: 提出EOS Early Rejection（EOSER）和Ascending Step-Size（ASS）两种新型解码机制，使MDLMs可高效执行全扩散式解码，并引入Consistency Trajectory Group Relative Policy Optimization（CJ-GRPO）优化算法，专门处理MDLMs中的训练-推理一致性问题。

Result: 在数学和规划任务基准上的实验表明，EOSER与ASS的解码策略配合CJ-GRPO优化器，可以用更少推理步骤，实现与NLU大型模型接近的或有竞争力的性能。

Conclusion: 上述新方法能够有效释放MDLMs的潜力，提升推理效率与一致性，对提升扩散语言模型的实际应用具有重要意义。

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [446] [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本论文提出了评估大语言模型在面对新信息时，预测和信心修正能力的新框架EVOLVECAST，发现现有模型在信心调整和预测更新方面仍存不足。


<details>
  <summary>Details</summary>
Motivation: 过去的事件预测研究多为静态任务，未系统研究模型在获得新证据后如何更新预测及信心水平。因此，作者希望弥补语言模型在动态预测中的研究空白，提出更合理的评估方法。

Method: 作者提出了EVOLVECAST评测框架，专门用以测试大语言模型在得到训练截止后才发布的新信息时，是否能适时并适度修正其预测。研究通过引入人类预测者作为对照，分析模型在不同上下文下预测变动和信心校准的表现，评估了文本表达和logits两种置信度估算法。

Result: 实验发现，语言模型虽然对新证据有一定反应，但预测调整不一致且普遍保守。无论是文本化还是logits置信估算均未优于对方，并明显低于人类表现。总体上，模型表现出保守偏见，信仰更新能力有限。

Conclusion: 当前大语言模型在动态预测任务中对新信息的适应仍有很大改进空间，未来需要开发更鲁棒的信仰更新和置信度校准机制，以缩小与人类预测者间的差距。

Abstract: Prior work has largely treated future event prediction as a static task,
failing to consider how forecasts and the confidence in them should evolve as
new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework
for evaluating whether large language models appropriately revise their
predictions in response to new information. In particular, EVOLVECAST assesses
whether LLMs adjust their forecasts when presented with information released
after their training cutoff. We use human forecasters as a comparative
reference to analyze prediction shifts and confidence calibration under updated
contexts. While LLMs demonstrate some responsiveness to new information, their
updates are often inconsistent or overly conservative. We further find that
neither verbalized nor logits-based confidence estimates consistently
outperform the other, and both remain far from the human reference standard.
Across settings, models tend to express conservative bias, underscoring the
need for more robust approaches to belief updating.

</details>


### [447] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: 本文提出了一种名为Easy Turn的开源双模（声学+语言）轮次检测模型，并发布配套数据集，提升了人机自然交互中的轮次检测性能。该方法在现有开源模型基础上实现了更高精度。


<details>
  <summary>Details</summary>
Motivation: 人机自然交互要求系统具备流畅的全双工对话能力，而实现该目标的关键——轮次检测，现有方法存在模型不开源、模型体积大或仅支持单一模态等问题，且缺乏高质量的数据资源。为此，作者希望设计一种开源、灵活、同时融合多模态信息的轮次检测模型，并发布相关数据集。

Method: 作者提出了一种模块化的开源轮次检测模型Easy Turn。该模型融合了声学和语言两种模态信息，能够预测四种对话轮次状态（完整、未完成、附和、等待）。同时，作者还发布了Easy Turn训练数据集（1145小时语音数据），用于模型训练。

Result: 与目前主流的开源轮次检测模型（如TEN Turn Detection和Smart Turn V2）相比，Easy Turn在自身的公开测试集（Easy Turn testset）上达到了最优的检测准确率。

Conclusion: Easy Turn模型及配套大规模训练数据集的开源，有望促进全双工人机交互系统的发展，弥补了当前由于模型不开源和数据资源稀缺带来的研究和应用障碍。

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [448] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: 现有的机器口译系统主要依赖单一的语音输入，忽略了视觉等其他信息，导致在需要多模态信息时表现受限。本文提出并实现了一种结合视觉输入的机器口译新方法，并在特定语料上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 单一语音信号输入的口译系统在歧义消解和语义充分性方面受限，现实环境下常需视觉或情境等额外信息，因此需要探索多模态方法提升译文质量。

Method: 提出Vision-Grounded Interpreting (VGI)方法，通过集成视觉-语言模型，利用摄像头实时采集视觉信息，与语音输入一起辅助机器翻译，并构建针对三类歧义的手工语料库进行评测。

Result: 实验结果显示，视觉输入显著提升了词汇层面的歧义消解能力，对性别消解有小幅且不稳定的提升，对句法歧义则无明显帮助。

Conclusion: 多模态输入是提升机器口译质量的必经之路，视觉辅助特别有助于词汇歧义消解，未来机器口译系统应进一步融入多源信息。

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [449] [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
*Ken Deng,Zizheng Zhan,Wen Xiang,Wenqiang Zhu,Tianhao Peng,Xinping Lei,Weihao Li,Jingxuan Xu,Kun Wu,Yifan Yao,Haoyang Huang,Huaixi Tang,Kepeng Lei,Zhiyi Lai,Songwei Yu,Zongxian Feng,Zuchen Gao,Weihao Xie,Chenchen Zhang,Yanan Wu,Yuanxing Zhang,Lecheng Huang,Yuqun Zhang,Jie Liu,Zhaoxiang Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.CL

TL;DR: 本文提出了HiPO框架，能让大语言模型在需要时选择详细推理或直接作答，从而在保持甚至提升准确率的同时，显著减少推理产生的token数量，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多采用链式推理（CoT）提升复杂任务准确率，但始终输出长推理链既低效又成本高昂，因此需要更高效的推理控制机制。

Method: 提出混合策略优化（HiPO）：一方面通过混合数据管道，提供“详细推理”和“直接作答”配对数据；另一方面结合强化学习奖励体系，同时平衡准确率与效率，抑制对冗长推理的过度依赖。

Result: 在数学及代码类任务基准测试中，HiPO大幅减少了推理token数量，并能维持甚至提升任务准确率。

Conclusion: HiPO框架可实现高效且自适应的推理控制，为实际资源敏感场景下推理型大模型的部署提供了可行方法。

Abstract: Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)
reasoning to improve accuracy on complex tasks. However, always generating
lengthy reasoning traces is inefficient, leading to excessive token usage and
higher inference costs. This paper introduces the Hybrid Policy Optimization
(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to
selectively decide when to engage in detailed reasoning (Think-on) and when to
respond directly (Think-off). Specifically, HiPO combines a hybrid data
pipelineproviding paired Think-on and Think-off responseswith a hybrid
reinforcement learning reward system that balances accuracy and efficiency
while avoiding over-reliance on detailed reasoning. Experiments across
mathematics and coding benchmarks demonstrate that HiPO can substantially
reduce token length while maintaining or improving accuracy. Finally, we hope
HiPO a can be a principled approach for efficient adaptive reasoning, advancing
the deployment of reasoning-oriented LLMs in real-world, resource-sensitive
settings.

</details>


### [450] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: 本文提出了ByteSized32Refactored，一个模块化、可扩展的文集和框架，可高效支持文本游戏的生成和扩展，并验证了其对于GPT-4o生成游戏时的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在交互式世界建模模拟（如文本游戏生成）中仍面临核心挑战，原有的ByteSized32语料库在扩展和代码维护上存在困难。研究旨在优化、重构该体系以提升扩展性和复用性。

Method: 研究对ByteSized32语料及其代码进行模块化重构，设计了GameBasic.py基础库，将32款游戏的共用逻辑抽象为7个基础类，并减少了整体代码量。大量实验基于GPT-4o，分析了新结构对游戏生成效果的影响。

Result: 新设计下的框架在32款文本游戏的开发效率提升明显，代码量从2万行降至1万行。GPT-4o在两个评测维度上表现提升，在另外两个维度有所下降，说明分层结构带来新挑战。

Conclusion: 论文认为，模块化与基础库为核心的重构设计既支持未来环境扩展，又便利了大模型针对环境定制，是面向可扩展文本游戏生成的重要基础。

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [451] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练即可实现大型语言模型（LLM）偏好对齐的新方法 —— PaLRS。该方法利用LLM残差流中的偏好信号，只需少量偏好数据即可通过推理时插入轻量级向量，引导模型生成更偏好的人类响应，相比传统方法更高效灵活。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型偏好对齐方法（如RLHF和DPO）需高质量数据和庞大的计算，效率低且模型专用性强。期望找到一种无需重新训练、低成本又具通用性的对齐方法，以提升实际应用中大模型与人类偏好的契合度。

Method: 提出PaLRS（Residual Steering）方法，核心是：从百级偏好对比样本中，提取残差流中的偏好信号，形成轻量、可插拔的“steering vectors”，在推理阶段直接插入该向量，无需对模型参数进行大规模优化或训练，实现输出行为对齐人类偏好。

Result: 在多个小至中型开源LLM上的数学推理和代码生成任务中，PaLRS均较基线模型有提升，对比DPO优化后的模型，PaLRS达成更佳效果且耗时极低，同时对整体性能影响很小。

Conclusion: PaLRS是一种高效灵活、完全免训练的LLM偏好对齐方式，用极少数据即可实现plug-and-play式对齐，优于现有主流偏好优化管道，具有广泛应用潜力。

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [452] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: 本文比较了完整、大规模语言模型(LLM)、蒸馏模型和量化模型在机器翻译任务中的翻译效果与效率，发现压缩模型可以显著降低计算和碳排放成本，同时维持优异翻译质量。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型应用的扩展，其计算和环境成本日益引发关注。研究动机是探索在不显著损失翻译质量的前提下，是否能通过模型压缩降低环境与资源消耗。

Method: 作者以机器翻译为案例，比较了全规模(3.3B参数, fp32)、蒸馏及量化(INT4)模型。通过Flores+基准和法语、印地语、卡纳达语的人类评价，量化了不同模型在翻译精度、推理速度、碳排放等方面的表现。

Result: 完整大模型BLEU分数最高，但碳排放最大(0.007—0.008kg CO2/次)；蒸馏模型推理速度提升4.5倍，BLEU分数损失极小；INT4量化模型依然保持高准确度和流畅度。不同模型在人类主观评价上差异较小，但在低资源语种上权衡更为明显。

Conclusion: 模型压缩（如蒸馏和量化）可在保证翻译质量的同时，大幅降低计算资源消耗和环境影响。作者呼吁NLP领域应将效率和可持续性纳入评价标准，而不应只关注性能指标。

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [453] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 提出了一种用大语言模型（LLM）把非结构化设计文档转化为可验证、可实时应用的AI安全管理制度（guardrails）的方法，并通过实际案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统在工业中的广泛应用，其安全性和可管理性成为亟需解决的问题。现有的安全策略往往很难从自然语言文档高效转换为可自动检查和执行的机制。

Method: 提出“Policy as Prompt”框架，利用LLM解释和执行自然语言政策，通过采集技术文档、构建可验证的策略树，并将其编译为高效的、提示词驱动的分类器，实现对AI代理行为的实时审查和管理。

Result: 在多个应用领域中验证了该方法的可扩展性和可审计性，展示了从政策到实践的高效转化效果。

Conclusion: 该框架能够弥合政策到实践的关键鸿沟，为AI安全合规提供了可验证且可监管的解决方案。

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [454] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: 本文提出了MCPMark基准，旨在更全面真实地评估大模型通过MCP协议与外部系统的交互能力。该基准包含127项由专家和AI协作创建的复杂任务，并给出了最新大模型的基准测试成绩。


<details>
  <summary>Details</summary>
Motivation: 现有MCP基准测试大多局限于简单、读操作为主的任务，未能覆盖真实场景中复杂多样的交互需求。因此，需要一个更现实、交互更复杂的评测标准来推动模型进步。

Method: 作者设计了包含127个高质量任务的新基准，每项任务有精心设置的初始状态和自动验收脚本，任务涉及创建、读取、更新、删除等多种操作。使用极简Agent框架循环调用工具，对主流LLM进行了系统评测。

Result: 实验证明，目前性能最佳的gpt-5-medium模型在该基准下pass@1仅为52.56%，pass^4为33.86%。其他如claude-sonnet-4和o3在pass@1甚至低于30%。平均每个任务需要16.2次执行和17.4次工具调用，远高于以往基准测试，体现了MCPMark的挑战性。

Conclusion: MCPMark展示了现有大模型尚难以胜任涉及复杂CRUD操作的真实工作流任务，现有模型仍有很大提升空间。该基准为未来智能体与系统交互能力的研究提供了更严格的评测平台。

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [455] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: 本文提出了NSP（Next Sequence Prediction）和SDLM（Sequential Diffusion Language Model），可高效、动态地生成文本，并改善现有Diffusion Language Model的不足。SDLM在速度和推理效率上优于主流模型，并且可兼容KV-cache，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（Diffusion Language Models, DLMs）理论效率高，但存在解码长度固定和无法兼容KV-cache（即中间结果缓存，提升生成效率的关键技术）的问题，限制了其实际应用。现有采用block diffusion的改进方案虽然缓解了部分问题，但仍然存在固定块大小和训练代价高的问题。因此，研究者希望设计一种能自适应生成长度、成本低、且与KV-cache兼容的高效语言建模方法。

Method: 提出Next Sequence Prediction（NSP）机制，将下一个token预测和下一个block预测统一在一起，使每步生成的文本长度可以动态调整（而非固定）。在NSP机制基础上，提出Sequential Diffusion Language Model（SDLM）：该模型对已有的自回归大模型只需小幅修改，即可利用NSP和扩散推理框架，在固定mask块中利用模型置信度动态生成连续子序列，从而既兼容KV-cache，也提高了对文本不确定性/语义变化的适应性。

Result: 实验证明，SDLM只需用350万训练样本即可达到甚至超过强基线（主流自回归生成模型）的效果，并且生成速度比Qwen-2.5快2.1倍。更大规模的SDLM-32B模型在效率和吞吐量方面提升更明显，显示了该范式的良好可扩展性。

Conclusion: SDLM提供了一种能够动态生成、推理高效、训练成本低并兼容KV-cache的语言建模新思路，有望推动大规模预训练语言模型的发展和实际部署。

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [456] [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
*Zeqing Wang,Gongfan Fang,Xinyin Ma,Xingyi Yang,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出了SparseD，一种专为扩散语言模型（DLMs）设计的稀疏注意力方法，在保证生成质量的前提下，显著降低了推理延时，提高了模型在长上下文下的推理效率。


<details>
  <summary>Details</summary>
Motivation: 开源的扩散语言模型在推理时由于注意力机制在长上下文下的二次复杂度导致推理延时过高，亟需高效的稀疏注意力机制以应对长文本应用场景。

Method: 作者深入分析DLMs的注意力稀疏行为，发现各个头的注意力模式差异大、且每个头在不同去噪步骤中变化小，早期去噪步骤对生成质量影响大。基于此，提出SparseD：仅需预计算一次头特定的稀疏模式，并在所有步骤重用，且在早期使用全注意力，后期切换到稀疏注意力，从而节省计算量且不损失质量。

Result: 实验表明，SparseD在64k上下文、1024去噪步骤下，相比于FlashAttention可实现至多1.5倍的无损加速效果。

Conclusion: SparseD作为一种针对DLMs定制的稀疏注意力方案，有效降低了推理延迟，是长上下文应用中可行且高效的解决方案。

Abstract: While diffusion language models (DLMs) offer a promising alternative to
autoregressive models (ARs), existing open-source DLMs suffer from high
inference latency. This bottleneck is mainly due to the attention's quadratic
complexity with respect to context length in computing all query-key pairs.
Intuitively, to reduce this complexity, a natural strategy is to restrict
attention to sparse patterns that retain only the most relevant connections.
Such approaches are well-established in ARs, where attention follows fixed and
clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity
behaviors: (1) attention patterns vary across heads, (2) attention patterns in
each head remain highly similar across denoising steps, and (3) early denoising
steps are critical for generation. These findings render sparse attention
methods designed for ARs largely incompatible with DLMs, as they fail to
capture head-specific structures and risk degrading generation when applied in
early denoising steps. To address these challenges, we propose SparseD, a novel
sparse attention method for DLMs. Leveraging the observations, SparseD only
requires pre-computing head-specific sparse patterns one time, and reuses them
across all steps. This prevents recomputing sparse patterns at each denoising
step. Meanwhile, SparseD uses full attention in the early steps, then switches
to sparse attention later to maintain generation quality. Together, these
establish SparseD as a practical and efficient solution for deploying DLMs in
long-context applications. Experimental results demonstrate that SparseD
achieves lossless acceleration, delivering up to $1.50\times$ speedup over
FlashAttention at a 64k context length with 1,024 denoising steps.

</details>


### [457] [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
*Hongbo Liu,Jia Xu*

Main category: cs.CL

TL;DR: 提出了一种新的神经网络架构ResFormer，专为高效处理不同长度的上下文序列设计，在多个数据集上显著优于现有模型，并降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型处理长序列时受限于二次复杂度，导致难以高效利用长上下文信息，影响诸如情感分析、意图识别等序列分类任务的表现。

Method: 提出ResFormer架构：通过级联方法，将reservoir computing网络（具备非线性读出，线性时间内建模长依赖）与传统Transformer（建模句内短期依赖，输入长度固定）结合，实现对长、短依赖的高效建模。

Result: ResFormer在EmoryNLP、MultiWOZ、MELD和IEMOCAP等数据集上表现优异，对比DeepSeek-Qwen与ModernBERT，准确率提升最高可达22.3%；同时显著降低了内存使用。

Conclusion: ResFormer在长上下文建模效率和效果上均优于主流方法，是处理大规模、长序列自然语言任务的有效工具。

Abstract: Sequence classification is essential in NLP for understanding and
categorizing language patterns in tasks like sentiment analysis, intent
detection, and topic classification. Transformer-based models, despite
achieving state-of-the-art performance, have inherent limitations due to
quadratic time and memory complexity, restricting their input length. Although
extensive efforts have aimed at reducing computational demands, processing
extensive contexts remains challenging.
  To overcome these limitations, we propose ResFormer, a novel neural network
architecture designed to model varying context lengths efficiently through a
cascaded methodology. ResFormer integrates an reservoir computing network
featuring a nonlinear readout to effectively capture long-term contextual
dependencies in linear time. Concurrently, short-term dependencies within
sentences are modeled using a conventional Transformer architecture with
fixed-length inputs.
  Experiments demonstrate that ResFormer significantly outperforms baseline
models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of
up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,
and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,
underscoring its effectiveness and efficiency in modeling extensive contextual
information.

</details>


### [458] [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 本文提出结合多种预训练变换器模型和大语言模型（LLM）的集合方法，对多语言情感分析任务进行优化，在无标签数据和多语言数据环境下取得了超过86%的准确率。


<details>
  <summary>Details</summary>
Motivation: 情感分析在社交媒体、市场营销等领域应用广泛，但在外语、尤其缺乏标注数据时效果较差，因此需要突破多语言情感分析中的瓶颈。

Method: 提出采用bert-base-multilingual-uncased-sentiment和XLM-R等多种多语言预训练模型组成集合模型，通过集成学习方式对无标签多语种数据进行情感分析。

Result: 实验表明，提出的集合方法在多语言情感分析任务中的准确率超过86%，效果优于单一模型方案。

Conclusion: 集合预训练变换器和大语言模型能够有效提升多语言背景下的情感分析性能，为无标签、多语言情感理解提供了可行的解决思路。

Abstract: Sentiment analysis is a very important natural language processing activity
in which one identifies the polarity of a text, whether it conveys positive,
negative, or neutral sentiment. Along with the growth of social media and the
Internet, the significance of sentiment analysis has grown across numerous
industries such as marketing, politics, and customer service. Sentiment
analysis is flawed, however, when applied to foreign languages, particularly
when there is no labelled data to train models upon. In this study, we present
a transformer ensemble model and a large language model (LLM) that employs
sentiment analysis of other languages. We used multi languages dataset.
Sentiment was then assessed for sentences using an ensemble of pre-trained
sentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.
Our experimental results indicated that sentiment analysis performance was more
than 86% using the proposed method.

</details>


### [459] [Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出并研究了大语言模型（LLMs）在面对大量细致约束时的生成能力，发现其性能随约束数量增加显著下降，并提出了FoCusNet模型作为辅助，显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注LLM对少量明确任务约束的响应能力，但现实应用中往往需要面对规模更大、粒度更细的约束，亟需系统地评估LLM在此类场景下的表现并寻求有效改进方案。

Method: 作者提出了“大规模约束生成”(LSCG)的新任务，并基于该任务设计了Words Checker实例，分别考察了模型规模、家族和提示策略（如简单提示、Chain of Thought、Best of N）对效果的影响。同时提出并引入FoCusNet模型，用于先从原始约束集合中筛选出与任务相关的更小子集，再引导LLM更高效地处理。

Result: 实验发现，现有LLM方案在约束数量增加时性能大幅下降；引入FoCusNet后准确率提升了8-13%。

Conclusion: LLM在处理大规模复杂约束时存在局限性，但通过加设前置筛选模型（FoCusNet）能够有效缓解准确率衰减，为后续相关应用和模型设计提供了新思路。

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [460] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 该论文提出了GEAR评测方法，用于大语言模型在溯因推理能力上的自动评估，并通过实验体现了其有效性及提升作用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究多关注于指令跟随和演绎推理，尚不清楚模型是否具备发现新知识的能力，且缺乏评价该能力的方法。因此，研究动机是探索和量化LLM进行新知识发现（尤其是溯因推理）的能力，并建立通用的、无需人工标注的评测手段。

Method: 作者提出了GEAR（General Evaluation for Abductive Reasoning），通过一致性、泛化性和多样性三项指标，对模型生成的假设集合进行评分。GEAR完全自动化、无需标签、可扩展，并为训练提供动态“学习进度”调整策略。此外，通过动量式课程学习调整数据难度，提升模型多样性与能力。

Result: 在1500个问题、9个大模型与4个基准数据集上，GEAR自动生成5万多候选假设，有效揭示了不同模型在溯因推理上的表现差异。同时，提出的课程学习策略，在无人工标签的前提下改善了所有GEAR指标，并能迁移至现有基准测试。

Conclusion: GEAR为LLMs溯因推理评测提供了系统化、自动化和无标签的新途径，不仅能更好地衡量模型真正的创新能力，也能高效指导更优秀模型的训练。

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [461] [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
*Zsolt T. Kardkovács,Lynda Djennane,Anna Field,Boualem Benatallah,Yacine Gaci,Fabio Casati,Walid Gaaloul*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的偏见测试框架BTC-SAM，利用大语言模型高效生成多样化测试句子，提升了情感分析模型偏见检测的覆盖率和质量。


<details>
  <summary>Details</summary>
Motivation: 情感分析模型普遍存在社会偏见，这些偏见会带来现实世界的危害。而现有检测模型偏见的方法在构建质量高、语法丰富、具有代表性的测试句子时成本高昂，需要领域专家或众包，难以覆盖所有潜在偏见。

Method: 提出BTC-SAM框架，基于大语言模型（LLMs），通过可控句子生成，自动化地生成多样化且高质量的测试样本，以检测情感分析模型中的各种社会偏见。该方法可以以极少需求输入情况下产生丰富的测试用例。

Result: 实验证明，BTC-SAM生成的测试句子在语言多样性和覆盖面上优于传统的提示法，尤其能发现尚未被关注的新型或未知类型的偏见。

Conclusion: 利用大语言模型自动生成测试句，可以高效且经济地提升偏见检测的全面性和深度，是情感分析模型偏见测试的有力工具。

Abstract: Sentiment Analysis (SA) models harbor inherent social biases that can be
harmful in real-world applications. These biases are identified by examining
the output of SA models for sentences that only vary in the identity groups of
the subjects. Constructing natural, linguistically rich, relevant, and diverse
sets of sentences that provide sufficient coverage over the domain is
expensive, especially when addressing a wide range of biases: it requires
domain experts and/or crowd-sourcing. In this paper, we present a novel bias
testing framework, BTC-SAM, which generates high-quality test cases for bias
testing in SA models with minimal specification using Large Language Models
(LLMs) for the controllable generation of test sentences. Our experiments show
that relying on LLMs can provide high linguistic variation and diversity in the
test sentences, thereby offering better test coverage compared to base
prompting methods even for previously unseen biases.

</details>


### [462] [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
*Guangliang Liu,Xi Chen,Bocheng Chen,Xitong Zhang,Kristen Johnson*

Main category: cs.CL

TL;DR: 本文提出了一种基于道德基础理论的语用推理方法，以提升大模型在道德推理任务中的泛化能力，实验结果显示该方法效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在道德推理研究中展现前景，但其擅长分布式语义，难以泛化至依赖语用层面的道德推理任务，因此需要探索其泛化能力提升之道。

Method: 作者基于道德基础理论，提出语用推理方法，利用每一步的上下文信息，弥补模型语用能力不足，引导模型在道德基础和道德推理目标之间建立联系。

Result: 实验结果表明，该基于语用推理的方法能够显著增强大型语言模型在道德推理任务中的泛化表现。

Conclusion: 本文工作为基于道德基础理论的未来大模型道德推理泛化研究奠定了方法论基础。

Abstract: Moral reasoning has emerged as a promising research direction for Large
Language Models (LLMs), yet achieving generalization remains a central
challenge. From a linguistic standpoint, this difficulty arises because LLMs
are adept at capturing distributional semantics, which fundamentally differs
from the morals which operate at the pragmatic level. This paper investigates
how LLMs can achieve generalized moral reasoning despite their reliance on
distributional semantics. We propose pragmatic inference methods grounded in
moral foundations theory, which leverage contextual information at each step to
bridge the pragmatic gap and guide LLMs in connecting moral foundations with
moral reasoning objectives. Experimental results demonstrate that our approach
significantly enhances LLMs' generalization in moral reasoning, providing a
foundation for future research grounded in moral foundations theory.

</details>


### [463] [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: GLoW是一种改进大模型（LLM）智能体硬探索能力的新方法，在文本游戏基准Jericho上取得了新的最佳成绩，并大幅减少了环境交互次数。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在需要自主探索、学习新知识的“硬探索”任务上表现有限，为提升其能力，研究者希望开发更有效的探索机制。

Method: 提出GLoW方法，结合了双尺度模型：一方面通过维护高价值轨迹前沿进行全局引导，另一方面通过“多路径优势反思”机制学习局部试错，以推断并利用奖励优势信号指导智能体探索。

Result: 在Jericho文本游戏基准测试中，GLoW方法取得了LLM智能体的新最高性能。此外，与RL领域内最优方法相比，GLoW达到了相当性能，但环境交互次数减少了100-800倍。

Conclusion: GLoW克服了当前LLM方法在硬探索任务中的不足，通过高效的探索机制显著提升了性能，同时大幅降低了算力消耗和探索成本，对于复杂任务具备广泛应用前景。

Abstract: LLM-based agents have seen promising advances, yet they are still limited in
"hard-exploration" tasks requiring learning new knowledge through exploration.
We present GLoW, a novel approach leveraging dual-scale world models,
maintaining a trajectory frontier of high-value discoveries at the global
scale, while learning from local trial-and-error in exploration through a
Multi-path Advantage Reflection mechanism which infers advantage-based progress
signals to guide exploration. To evaluate our framework for hard-exploration,
we tackle the Jericho benchmark suite of text-based games, where GLoW achieves
a new state-of-theart performance for LLM-based approaches. Compared to
state-of-the-art RLbased methods, our approach achieves comparable performance
while requiring 100-800x fewer environment interactions.

</details>


### [464] [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
*Sourjyadip Ray,Shubham Sharma,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文研究了如何利用多模态大语言模型（MLLMs）自动回答学生关于在线课程的问题，并提出了新的EduVidQA数据集及该领域的基准。


<details>
  <summary>Details</summary>
Motivation: 随着数字化平台改变教育模式，提高课堂互动对学习效果至关重要。当前缺乏能自动理解和回答学生针对视频讲座提问的有效系统，有必要推动智能问答在实际教育中的应用。

Method: 作者构建了EduVidQA数据集，包括296个计算机科学视频中的5252对真实和合成问题-答案样本，并收集了学生的质量偏好作为评价参考。以该数据集为基准，系统评测了6个主流MLLMs在合成数据微调和任务挑战性上的表现，采用文本和主观评价两类指标综合分析模型效能。

Result: 实验结果证明该任务难度较高，合成数据对模型微调有效。同时，不同模型在客观与主观评价下的性能存在差异，提供了对多模态问答系统表现的更细致见解。

Conclusion: 本研究建立了教育视频问答的新基准数据集和评测体系，对教育NLP领域有重要推动作用，也为今后的相关研究指明了发展方向。

Abstract: As digital platforms redefine educational paradigms, ensuring interactivity
remains vital for effective learning. This paper explores using Multimodal
Large Language Models (MLLMs) to automatically respond to student questions
from online lectures - a novel question answering task of real world
significance. We introduce the EduVidQA Dataset with 5252 question-answer pairs
(both synthetic and real-world) from 296 computer science videos covering
diverse topics and difficulty levels. To understand the needs of the dataset
and task evaluation, we empirically study the qualitative preferences of
students, which we provide as an important contribution to this line of work.
Our benchmarking experiments consist of 6 state-of-the-art MLLMs, through which
we study the effectiveness of our synthetic data for finetuning, as well as
showing the challenging nature of the task. We evaluate the models using both
text-based and qualitative metrics, thus showing a nuanced perspective of the
models' performance, which is paramount to future work. This work not only sets
a benchmark for this important problem, but also opens exciting avenues for
future research in the field of Natural Language Processing for Education.

</details>


### [465] [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
*Guancheng Wan,Lucheng Fu,Haoxin Liu,Yiqiao Jin,Hui Yi Leong,Eric Hanchen Jiang,Hejia Geng,Jinhe Bi,Yunpu Ma,Xiangru Tang,B. Aditya Prakash,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TARE的优化方法，增强了大语言模型（LLMs）针对不同语义等价表达（paraphrase）时的稳健性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt优化方法主要关注点对点的准确率，忽视了语义等价表达（paraphrase）引起的不稳定性，导致模型对小的语义变化表现脆弱，此问题被称为“textual sharpness”。

Method: 作者在prompt语义空间中形式化了textual sharpness指标，并提出了一种基于黑盒（API）操作的鲁棒性准则。方法TARE为无梯度的框架，采用内外两层对抗搜索：内部利用采样寻找最具挑战性的等价表述，外部选择在邻域内性能稳定的优化候选。此外，作者还提出ATARE，通过自适应调整语义邻域的形状和半径，平衡探索与模型忠实度。

Result: 实验在多种任务上验证了方法的有效性。与仅关注准确率的搜索方法相比，TARE/ATARE生成的prompts在保证准确率的同时，对语义等价表达更具鲁棒性，且计算效率较高。

Conclusion: 提出的方法成功缓解了prompt优化的“textual sharpness”问题，实现了对副本表达鲁棒且高性能的prompt搜索，对实际大语言模型应用具有重要意义。

Abstract: The performance of Large Language Models (LLMs) hinges on carefully
engineered prompts. However, prevailing prompt optimization methods, ranging
from heuristic edits and reinforcement learning to evolutionary search,
primarily target point-wise accuracy. They seldom enforce paraphrase invariance
or searching stability, and therefore cannot remedy this brittleness in
practice. Automated prompt search remains brittle: small, semantically
preserving paraphrases often cause large performance swings. We identify this
brittleness as the textual sharpness of the prompt landscape. In this work, we
provide the first formal treatment of textual sharpness in the discrete,
semantic space of prompts, together with an operational robustness criterion
over a semantic neighborhood; the design is black-box or API-only, requiring no
gradients to update the model's parameters. Then we introduce TARE (Textual
Sharpness-Aware Evolving), a derivative-free framework that alternates between
an inner, sampling-based adversarial search that stresses a prompt with hard
paraphrases and an outer, robust selection that prefers candidates whose
neighborhoods remain strong. We further propose ATARE, which learns anisotropic
weights to shape the semantic neighborhood and adapts its radius over time to
balance exploration and fidelity. Diverse tasks evaluate our methods, whose
design for minimizing textual sharpness gap leads to prompts that preserve
accuracy under paraphrasing, outperforming accuracy-only prompt search while
remaining computationally practical.

</details>


### [466] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: 本文提出了一种名为LLM-proposed Open Taxonomy (LOT)的新方法，通过让生成式大模型自动对比和归纳大型推理模型（LRMs）在推理过程中的差异，以自然语言形式生成分类体系，分析和区分不同模型的推理方式。方法验证了LOT工具在多个开源模型和多项任务上的有效性，并发现调整小模型推理风格可提升其性能。


<details>
  <summary>Details</summary>
Motivation: 目前对大型推理模型（LRMs）的比较大多停留在整体准确率或推理长度等宏观层面，难以解释不同模型在推理内在过程上的差异。如何揭示模型推理路径的异同与其背后的思维风格，是实现更细致模型评价和改进的重要课题。

Method: 作者提出LOT方法，将生成式大语言模型用于对比两组LRM的推理轨迹，并用自然语言提炼每组的特征，基于这些特征及其统计分布实现模型来源预测。通过多轮迭代，LOT自动构建指出各模型推理差异的人类可读分类体系，并以此对模型推理风格进行量化和描述。

Result: LOT被应用于12个开源LRM在数学、科学和编程任务上的推理，能以80-100%的准确率区分不同规模、基座模型家族或目标领域的LRM推理轨迹。同时，LOT提供的自然语言分类解释也为模型推理过程差异提供了定性说明。

Conclusion: LOT不仅提升了对大型推理模型内部推理流程差异的可解释性，并支持通过调整较小模型的推理风格以显著提升其下游任务表现（GPQA准确率提升3.3-5.7%），为模型比较与优化带来新的工具和视角。

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [467] [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
*Haolin Yang,Hakaze Cho,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种新的分析框架，揭示大语言模型上下文学习（ICL）的机制，将注意力头的细粒度分析与ICL的任务识别（TR）和任务学习（TL）两大组成部分结合起来。


<details>
  <summary>Details</summary>
Motivation: 当前对ICL机制的解释存在两个主流视角，分别为对注意力头的分析和将ICL分解为TR和TL两部分，但这两者之间关系尚不明确，因此需要一种方法来统一和解析二者。

Method: 提出Task Subspace Logit Attribution（TSLA）框架，通过相关性分析、消融实验和输入扰动，识别出专门负责TR与TL的注意力头，并利用几何分析和steering实验揭示其具体功能。

Result: 实验证实TR头主要通过对齐隐藏状态与任务子空间促进任务识别，TL头则主要通过在子空间内旋转隐藏状态以帮助正确预测；两者功能独立且互补。同时，该框架还能统一此前关于ICL机制（如induction heads、task vectors）的发现。

Conclusion: 所提出的分析框架为理解大语言模型如何跨任务执行ICL提供了统一、可解释的机制视角，有助于未来对模型工作原理的进一步研究。

Abstract: We investigate the mechanistic underpinnings of in-context learning (ICL) in
large language models by reconciling two dominant perspectives: the
component-level analysis of attention heads and the holistic decomposition of
ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel
framework based on Task Subspace Logit Attribution (TSLA) to identify attention
heads specialized in TR and TL, and demonstrate their distinct yet
complementary roles. Through correlation analysis, ablation studies, and input
perturbations, we show that the identified TR and TL heads independently and
effectively capture the TR and TL components of ICL. Using steering experiments
with geometric analysis of hidden states, we reveal that TR heads promote task
recognition by aligning hidden states with the task subspace, while TL heads
rotate hidden states within the subspace toward the correct label to facilitate
prediction. We further show how previous findings on ICL mechanisms, including
induction heads and task vectors, can be reconciled with our
attention-head-level analysis of the TR-TL decomposition. Our framework thus
provides a unified and interpretable account of how large language models
execute ICL across diverse tasks and settings.

</details>


### [468] [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
*Haolin Yang,Hakaze Cho,Kaize Ding,Naoya Inoue*

Main category: cs.CL

TL;DR: 该论文提出直接训练任务向量（LTVs），改进了以往从LLM输出或隐藏状态中提取任务向量（TVs）的方法，提升了任务表示的准确性和灵活性，并系统性地分析了TV在模型预测中的作用机制。


<details>
  <summary>Details</summary>
Motivation: 现有工作对LLM中任务向量的获取方法过于复杂且不透明，且缺乏对TV在模型推理中实际作用的深入机制阐释，因此本研究希望提出更高效直接的TV获取方式，并揭示其内部机制。

Method: 作者提出了直接训练得到的 Learned Task Vectors（LTVs），不依赖于复杂的提取方法，并对TVs在模型不同层与不同位置的作用效果进行了系统对比分析。此外，通过分析LTV在注意力头各部件中的传播过程，明确其对最终预测的影响路径。

Result: LTV在准确度上超越了以往提取的TVs，且具备更高的灵活性。机制分析显示，TV的预测导向主要由注意力头中的OV电路驱动，极少数关键注意力头作用最为显著；TV在传播过程中呈现出较强的线性性质。

Conclusion: 直接训练的LTV不仅具备高效和实用的优势，还为理解LLM中ICL的机制提供了新视角，有助于未来相关任务和模型解释性的深入研究。

Abstract: Large Language Models (LLMs) can perform new tasks from in-context
demonstrations, a phenomenon known as in-context learning (ICL). Recent work
suggests that these demonstrations are compressed into task vectors (TVs),
compact task representations that LLMs exploit for predictions. However, prior
studies typically extract TVs from model outputs or hidden states using
cumbersome and opaque methods, and they rarely elucidate the mechanisms by
which TVs influence computation. In this work, we address both limitations.
First, we propose directly training Learned Task Vectors (LTVs), which surpass
extracted TVs in accuracy and exhibit superior flexibility-acting effectively
at arbitrary layers, positions, and even with ICL prompts. Second, through
systematic analysis, we investigate the mechanistic role of TVs, showing that
at the low level they steer predictions primarily through attention-head OV
circuits, with a small subset of "key heads" most decisive. At a higher level,
we find that despite Transformer nonlinearities, TV propagation is largely
linear: early TVs are rotated toward task-relevant subspaces to improve logits
of relevant labels, while later TVs are predominantly scaled in magnitude.
Taken together, LTVs not only provide a practical approach for obtaining
effective TVs but also offer a principled lens into the mechanistic foundations
of ICL.

</details>


### [469] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种面向GUI任务的轻量级视图-语言模型（VLM）插件RAG-GUI，通过推理时利用网页教程，提升VLM agent在复杂数字任务中的泛化与实用能力，实验证明扩展性强并优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的GUI agent在真实数字任务中受限于训练数据稀缺和任务复杂性，难以应对长尾、罕见场景，亟需提升实用性和泛化能力。

Method: 提出RAG-GUI，不依赖特定模型，通过两步学习：一是通过有监督微调（SFT）热启动；二是通过自引导拒绝采样微调（RSF）进一步优化。推理阶段动态利用网络教程知识，可作为任何VLM agent的即插即用插件扩展。

Result: 在三项不同任务上评估RAG-GUI，针对两种模型规模，对比基线模型和其他推理方法，RAG-GUI的表现提升了2.6%到13.3%，表现出优异的泛化能力和实用性。

Conclusion: RAG-GUI能有效提升VLM agent在实际数字任务中的性能，具备高扩展性，能增强现有模型对稀有场景的处理能力，满足真实场景应用需求。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [470] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出了一种基于项目反应理论（IRT）的医学大模型评估框架MedIRT，通过对80个LLM在1100道美国医师资格考试对齐题库上的测试，提供比传统准确率更细致和可靠的模型表现评估。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型逐步应用于高风险的医疗领域，当前常用的评估指标（如准确率）不足以反映题目特征及各主题下的具体表现，因此医疗领域急需一种更精细、更具解释力的评估方法。

Method: 作者创新性地引入高风险教育测评中成熟的IRT理论，基于新采集的80个模型对1100道USMLE题目的作答情况，分主题采用双参数逻辑IRT模型，联合估计模型能力、题目难度和区分度，并分析各模型在不同细分领域的表现。

Result: 实验结果显示，传统总排名可能掩盖模型在特定领域的突出能力。例如GPT-5整体表现最优，但在某些社科和沟通能力领域被总体排名较低的Claude-3-opus超越。IRT还能检测到基准测试本身的瑕疵题。

Conclusion: MedIRT为医疗领域大模型评估提供了权威、稳健的心理测量学工具，可有效提升模型选型和应用安全性，是未来可信医疗AI落地不可或缺的方法学基础。

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [471] [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
*Luyang Zhang,Siyuan Peng,Jialu Wang,Shichao Zhu,Beibei Li,Zhongcun Wang,Guangmou Pan,Yan Li,Song Yang*

Main category: cs.CL

TL;DR: 本文提出了PET方法，通过将用户偏好建模为分布式概率而非直接生成物品列表，实现了更透明和多样化的个性化推荐，并在公开数据集和真实大规模场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然可处理丰富用户行为语义，但其直接生成推荐列表的方法存在个性化受限、决策过程不透明和流行性偏差问题，难以进行全面的用户建模。

Method: 提出了PET（Preference Evolution Tracking）框架，将用户偏好推断任务转化为在解释性良好的偏好簇格子上的概率分布建模。具体采用logit-probing和生成式分类技术，对用户偏好建模为动态概率分布，而非静态排序列表。

Result: 在Yelp和MovieLens等公开基准数据集上，PET在NDCG指标上比直接生成的方法提升最高达40%。在某短视频平台真实大规模数据集上，对长尾内容的排序能力远超SOTA生产模型，NDCG分数提升7倍。

Conclusion: PET方法用概率分布式的透明用户偏好表示，替代了传统黑箱式的列表生成方式，使个性化推荐系统更加可解释、公平和多样化。

Abstract: Understanding how user preference evolves over time is a fundamental
challenge central to modern digital ecosystems, for which Large Language Models
(LLMs) are an increasingly prominent and popular approach due to their ability
to comprehend the rich semantic context within behavioral data. A common
practice is to use LLMs to predict a user's next action by directly generating
a ranked list of preferred items. Although effective for short-term prediction,
the end-to-end generation paradigm inherently limits personalization. Its
opaque decision-making process obscures holistic user profiling and exacerbates
popularity bias. To address these limitations, we propose Preference Evolution
Tracking (PET), a framework that reframes the task as inferring a dynamic
probability distribution over a stable and interpretable lattice of preference
clusters. By applying logit-probing and generative classification techniques,
PET infers a user's preference as a probability distribution, enabling
transparent preference learning. On public benchmarks (Yelp, MovieLens), PET
improves ranking quality by up to 40% in NDCG over direct generation baselines.
On a large-scale, real-world dataset from a short-video platform, it excels at
ranking long-tail contents, significantly outperforming a SOTA production model
by 7 times in the NDCG score. Ultimately, PET transforms the user profile model
from direct preference list generation to a transparent distributional
preference mapping, paving the way for more explainable, fair, and diverse
personalization systems.

</details>


### [472] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher提出了一种自我博弈框架，有效提升搜索增强大模型（LLM）在复杂推理任务中的表现，各项测试均优于现有强基线，并显著提高了参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强的LLM在多跳检索和复杂推理能力方面存在不足，难以应对高推理复杂度任务，因此需要新的方法提升其推理与检索协同能力。

Method: 设计了AceSearcher自我博弈框架，让同一个LLM交替扮演分解者和解答者的角色，通过监督微调与强化微调结合，提升模型在检索、问题分解和推理各环节性能，无需中间注释。

Result: 在10个数据集、3类高推理性任务上，AceSearcher平均精确匹配度比分强基线高7.6%；在文档级财经推理上，AceSearcher-32B以仅5%参数量达到DeepSeek-V3性能；小模型（1.5B/8B）也可超越大参数量搜索增强基线。

Conclusion: AceSearcher框架能大幅提升LLM高复杂度推理任务性能，并在参数效率上极具优势，为高效搜索增强推理提供新范式。

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [473] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言表达（如'可能'、'大概'）的信心估计方法，用以提升大语言模型（LLMs）回答时的不确定性表达，并证明该方法高效、与人类沟通方式一致，且有助于信任建立。


<details>
  <summary>Details</summary>
Motivation: 当前大模型广泛应用于高风险场景，传统的信心分数计算方法存在logits不可获取、计算资源消耗大、以及数字表述不符合自然沟通等实际问题。为此，研究动机在于寻找一种高效、易用且人性化的信心表达方式。

Method: 作者提出了语言信心（Linguistic Confidence, LC），即通过模型在输出中使用含糊修饰词（如‘可能’‘大概’）表达不确定性。具体措施包括：1）发布了首个包含人类标注信心分数的大型多样化“hedge”语言数据集；2）提出将修饰语映射为信心分的新方法（几乎无额外计算成本）；3）系统性评测主流LLM的LC表现，并通过prompt设计优化信心表述；4）进一步提出微调框架提升LC效果。

Result: 实验表明，大多数LLMs当前难以自如表达LC，但通过精心设计的prompt可以显著提升信心校准和辨别力；而采用作者提出的微调框架后，LC的可靠性进一步增强。

Conclusion: 基于语言信心的方法在大模型不确定性估计上具备可扩展性、高效性及人性化优点，值得更深入探索和推广。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [474] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: 本文提出了BeyondBench，一个避免训练数据污染、用于公平评估大语言模型推理能力的新基准，它通过算法生成新颖、未见过的题目，覆盖44类算法任务，并在多个语言模型上的实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准容易被互联网训练数据污染，使评估结果失真，难以判断模型是推理还是记忆，因此需要一种生成新颖无污染测试题的评测框架。

Method: 采用算法自动生成具数学基础的问题，涵盖44项任务，共117种变体，分为三类难度，并确保每个测试实例都是新的且用数学证明验证答案正确。对101个不同体量与类型的开源和闭源大模型进行全面评测并量化表现。

Result: 发现各模型在高复杂度任务上的推理能力普遍下降，如Gemini-2.5-pro、Llama-3.3-70B等在最难任务集的准确率仅有56.38%、26.91%等；不借助工具情况下，GPT-5及其变体准确率降幅高达47.59%。

Conclusion: 通过动态、无污染的评测，初步揭示现有主流语言模型在高复杂推理任务上的弱点，并为社区提供公正、可复现的开放排行榜支持后续研究。

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [475] [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: ScenarioBench是一个专为政策合规场景设计、可追溯的Text-to-SQL/检索增强生成基准，侧重于决策结果及其理由的可验证性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL或KILT/RAG基准主要关注系统给出答案的准确性，而没有严格要求其理由（explanation）可验证和可审计，难以满足政策合规等高敏感领域的需求。因此，急需一个能够严格关联决策与政策条款，并能全方位评估系统推理与检索能力的新基准。

Method: 每个场景以YAML格式详细设计，包括金标准决策、最简追溯路径（witness trace）、相关政策条款集合和标准SQL。系统需基于同一政策规范，通过条款ID进行决策理由解释，从而保证理由可查可证。评估维度涵盖决策准确率、追溯路径（完整性、正确性、顺序）、检索效果、SQL正确性（结果集等价）、政策覆盖度、延迟和解释幻觉率。综合性指标Scenario Difficulty Index (SDI)及耗时限制变体（SDI-R）进一步量化系统表现。

Result: 相比已有Text-to-SQL和KILT/RAG基准，ScenarioBench实现在决策与政策条款逐一、严格绑定的评测框架。同时评测不仅看输出结果，还重视决策背后的理由质量和合规性，在检索难度和时间限制下也能衡量系统综合能力。

Conclusion: ScenarioBench推动Text-to-SQL及相关系统向合规环境适用方向发展，提高了解释的可验证性和可审计性，有助于实际应用中更可信的系统构建。

Abstract: ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.

</details>


### [476] [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
*Ziyu Chen,Junfei Sun,Chenxi Li,Tuan Dung Nguyen,Jing Yao,Xiaoyuan Yi,Xing Xie,Chenhao Tan,Lexing Xie*

Main category: cs.CL

TL;DR: 论文介绍了MoVa，这是一个为了促进人类道德和价值观自动识别而构建的资源工具包，包括多样化标注数据集、有效的大模型提示策略以及实用的评测应用。


<details>
  <summary>Details</summary>
Motivation: 人类交流中的道德与价值观分析十分重要，但现有理论和数据框架多样且难以统一，为此研究者亟需通用且可推广的工具资源。

Method: MoVa包含：(1) 16个按照四种理论框架标注的数据集与基准测试；(2) 一种轻量级大语言模型（LLM）提示策略，能优于传统微调模型；(3) 一项新的心理调查评测应用。作者还提出了all@once多标签分类策略，能同时对相关概念打分。

Result: MoVa内的LLM提示法在多个领域和理论框架下的分类效果，超过了现有微调模型。同时all@once策略提升了多标签分类准确度。

Conclusion: MoVa工具包及其相关方法，为细致解读人类与机器交流中的道德和价值观提供了有效手段，对推动机器行为与人类价值观对齐等任务具有积极意义。

Abstract: Identifying human morals and values embedded in language is essential to
empirical studies of communication. However, researchers often face substantial
difficulty navigating the diversity of theoretical frameworks and data
available for their analysis. Here, we contribute MoVa, a well-documented suite
of resources for generalizable classification of human morals and values,
consisting of (1) 16 labeled datasets and benchmarking results from four
theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy
that outperforms fine-tuned models across multiple domains and frameworks; and
(3) a new application that helps evaluate psychological surveys. In practice,
we specifically recommend a classification strategy, all@once, that scores all
related concepts simultaneously, resembling the well-known multi-label
classifier chain. The data and methods in MoVa can facilitate many fine-grained
interpretations of human and machine communication, with potential implications
for the alignment of machine behavior.

</details>


### [477] [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
*Kangxu Wang,Ze Chen,Chengcheng Wei,Jiewen Zheng,Jiarong He,Max Gao*

Main category: cs.CL

TL;DR: 本文介绍了opdainlp团队在CPDC 2025 GPU赛道挑战中的解决方案，通过集成Qwen3-14B大模型、LoRA微调及多Adapter融合，在有效性和推理资源受限下取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 该比赛要求构建遵循不同角色设定、符合游戏世界观、并支持功能调用的对话式AI系统，同时兼顾推理时的效率和资源限制，具有实际落地意义和挑战性。

Method: 团队基于Qwen3-14B模型，结合了LoRA微调与模型融合。比赛中，分别针对工具调用、工具调用结果回复、无工具调用结果回复三个子任务设计了不同的LoRA Adapter，并在推理时通过vLLM实现MultiLoRA并用，以提升模型适应不同任务场景的能力。同时还基于官方数据进行部分任务的数据合成，增强训练样本的多样性。

Result: 该方案在GPU赛道的Task 1和Task 3中获得第一名，在Task 2中获得第二名，验证了其方法的有效性和实用性。

Conclusion: 多LoRA Adapter集成和模型融合在有限的资源和实际的游戏场景下，能够有效提升对话系统的任务表现，方法值得相关领域进一步研究和应用。

Abstract: This paper presents the opdainlp team's solution for the GPU track of the
CPDC 2025 challenge. The challenge consists of three tasks, aiming to build an
in-game conversational AI that adheres to character personas, aligns with the
game's worldview, and supports function calling. Considering both effectiveness
and resource/time constraints during inference, we synthesized data for some of
the tasks based on the datasets provided by the competition organizers. We
employed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base
model integrated with multiple LoRA adapters during inference. Specifically, in
the competition, we used three distinct LoRA adapters to handle tool calling,
response generation with tool call results, and response generation without
tool call results, respectively. MultiLoRA inference was implemented using
vLLM. Our solution achieved the first place in Task 1 and Task 3, and the
second place in Task 2 of the GPU track.

</details>


### [478] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出了MetaTuner框架，首次将Prompt优化与微调两种大型语言模型（LLM）提升方法有效结合，实现了性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM性能的方法主要有Prompt优化（通过设计更好的自然语言提示）和微调（对模型参数进行调整），二者各有优点但以往多独立研究，未探索其协同效果。作者希望结合两者，在知识共享的基础上挖掘协同潜力，进一步提升LLM表现。

Method: 提出MetaTuner框架，分别用两个神经网络负责生成Prompt和参数，并共享底层编码层实现知识共享。在最终监督信号的指导下，联合优化Prompt与参数。针对Prompt学习的离散特性和微调的连续特性，设计了监督正则化损失函数来有效训练框架。

Result: 在多个不同基准测试上，MetaTuner方法在性能上始终超过现有基线方法。

Conclusion: MetaTuner有效融合了Prompt优化与微调，展现了二者协同带来的性能提升，对提升LLM能力具有实际价值。

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [479] [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
*Yuelyu Ji*

Main category: cs.CL

TL;DR: 论文提出了MRAG-Suite评测平台，系统性分析视觉增强检索生成（Visual RAG）系统在处理不同难度和歧义查询时的表现，并引入了诊断工具MM-RAGChecker。


<details>
  <summary>Details</summary>
Motivation: 虽然Visual RAG技术让多模态问答取得了重要进展，但现有评估方法未能科学区分查询的难度和歧义性，因此不能全面反映系统实际能力。

Method: 作者整合了多个多模态基准数据集构建MRAG-Suite评测平台，并提出基于难度与歧义性的过滤策略。同时，提出了MM-RAGChecker作为细粒度诊断工具，对检索生成系统的回答进行主张级的诊断分析。

Result: 在引入难度和歧义性区分后，发现各种Visual RAG系统的准确率显著下降，并普遍存在幻觉现象。MM-RAGChecker可以有效定位这些问题。

Conclusion: MRAG-Suite平台和MM-RAGChecker能够系统评估并诊断Visual RAG系统在复杂、多义查询下的失效问题，为未来多模态检索生成系统的优化指明了方向。

Abstract: Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.

</details>


### [480] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出并开源了名为SimuHome的高仿真智能家居模拟环境及其配套评测基准，用于推动大语言模型智能体（LLM agents）在智能家居场景下的能力提升。实验表明，当前主流LLM agent在复杂任务，特别是隐含意图推断和时序调度等方面表现不理想。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM agents在多步工具增强任务上表现强劲，但在智能家居环境下面临如用户隐含意图、设备约束、时序关联等独特挑战。当前缺乏现实且支持交互的模拟环境与统一评测体系，这极大限制了智能体算法的开发和比对。

Method: 作者开发了SimuHome环境，具备智能家居设备仿真、API 接口，可实时反映环境变量变化，基于Matter协议，并设计了涵盖12类用户查询、600个任务实例的系统性基准。通过基于ReAct框架评估了11种智能体（包括GPT-4.1等）的性能表现。

Result: 结果显示，在简单任务下模型表现良好，但在隐含意图推断、状态核查和尤其是时间调度方面普遍吃力；即使GPT-4.1等顶尖模型，成功率也仅为54%。

Conclusion: 当前LLM agent在复杂智能家居任务上有明显短板，尤其是在状态验证和时间相关行为协调上。今后需开发更可靠的工具使用与状态核查方法，以提升智能家居场景下的智能体表现。

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [481] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的生成式句子嵌入框架GIRCSE，能够通过自回归生成和对比研磨目标，提升大型语言模型嵌入的表达能力，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的嵌入方法主要采用仅编码的方式，忽略了LLM强大的生成能力，导致对隐含语义和概念的表达受限。作者希望通过利用生成和迭代优化提升句子嵌入的质量。

Method: 提出GIRCSE框架，通过生成一串经过对比损失优化的软Token，实现语义表达的逐步完善，并设计了迭代对比研磨（ICR）目标，推动每步生成嵌入表现提升。

Result: GIRCSE在MTEB基准和指令跟随任务中性能明显优于主流LLM嵌入方法；并展示了在推理时生成更多Token能持续提升嵌入质量的属性。

Conclusion: GIRCSE和生成式迭代研磨机制为表征学习提供了新思路，显示出优于编码器方法的潜力，并引入了可持续增强的推理能力。

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [482] [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
*Xinyu Pi,Qisen Yang,Chuong Nguyen*

Main category: cs.CL

TL;DR: 本文提出了LOGOS，这是一种实现完全自动化扎根理论分析流程的新框架，显著提升了定性研究的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统扎根理论依赖于专家手动编码，流程繁琐且难以扩展，现有的计算工具也无法实现真正自动化，因此迫切需要一种能够自动化、标准化且高效的解决方案。

Method: 作者设计了LOGOS框架，结合了大语言模型(LLM)驱动的自动编码、语义聚类、图推理以及新颖的迭代细化流程，实现了原始文本到层次理论结构的全自动转化。同时，提出5维评价指标及训练-测试划分协议，以实现公正、标准化评估。

Result: LOGOS在五个不同领域数据集上都优于现有强基线模型，在复杂数据集上与专家编码方案达到了88.2%的高度一致性。

Conclusion: LOGOS成功实现了定性研究流程的全自动化，为扎根理论研究的普及与规模化提供了强有力的新途径，且不损失理论的细致性和深度。

Abstract: Grounded theory offers deep insights from qualitative data, but its reliance
on expert-intensive manual coding presents a major scalability bottleneck.
Current computational tools stop short of true automation, keeping researchers
firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that
fully automates the grounded theory workflow, transforming raw text into a
structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic
clustering, graph reasoning, and a novel iterative refinement process to build
highly reusable codebooks. To ensure fair comparison, we also introduce a
principled 5-dimensional metric and a train-test split protocol for
standardized, unbiased evaluation. Across five diverse corpora, LOGOS
consistently outperforms strong baselines and achieves a remarkable $88.2\%$
alignment with an expert-developed schema on a complex dataset. LOGOS
demonstrates a powerful new path to democratize and scale qualitative research
without sacrificing theoretical nuance.

</details>


### [483] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 本文系统分析了扩散大语言模型（dLLMs）在越狱攻击下的独特脆弱性，并提出了全新防御框架DiffuGuard，在大幅降低越狱成功率的同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着扩散大语言模型（dLLMs）的迅速发展，其生成机制与自回归LLM截然不同，带来了前所未有的新型安全脆弱性。当前尚无针对dLLMs安全性的充分分析或专用防御措施。

Method: 作者提出从两大层面（步内与步间）系统剖析dLLM在越狱攻击中的脆弱性，揭示了标准贪婪再掩码机制的有害偏差，并首次发现了去噪路径依赖现象。为此，提出DiffuGuard防御框架：一方面通过随机退火再掩码在生成过程中引入受控随机性；另一方面引入块级审核修复机制，利用模型内部表征实现越狱风险自主检测和有指导的修正。该方法无需额外训练。

Result: 在四种主流dLLMs和六类越狱攻击下实验表明，DiffuGuard将越狱攻击成功率由47.9%降至14.7%，且基本无损模型效能和效率。

Conclusion: 当前dLLM的标准解码机制带来显著安全风险，但dLLM本体具有可挖掘的安全潜力。DiffuGuard作为首个专为dLLM设计的免训练防御方法，有效提升了dLLM的实用安全性。

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [484] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种自动将文本问答对（TQA）转化为高质量多模态问答对（MMQA）的方法，并开发了相关评测体系和智能系统，实现了大规模科学推理基准的自动构建。


<details>
  <summary>Details</summary>
Motivation: 手工创建高质量、多模态科学推理基准成本高且难以扩展，为推动相关领域进步，需探索自动化、高效的基准生成方法。

Method: 1. 提出TQA到MMQA的转化框架，并建立多维度的MMQA质量评测标准；2. 构建两个大规模基准，评估最先进模型在MMQA生成和评价任务上的表现；3. 开发Q-Mirror智能系统，将MMQA的生成与质量评价闭环整合，实现自动迭代优化。

Result: 实验证明，现有最优模型虽能生成MMQA，但输出质量仍存在明显不足。此外，顶级理解模型在MMQA质量评价上与人工判断高度一致。Q-Mirror系统通过自动化迭代将基准问答的平均分数由78.90提升至85.22，通过率由72%提升至95%。

Conclusion: Q-Mirror系统为大规模高质量多模态科学推理基准的自动构建提供了有效、现实的解决方案，有助于推动相关领域的发展。

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [485] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）表达价值观的两种机制：固有机制与提示机制，并揭示二者既有重叠也存在独特成分，对不同任务表现出不同的可引导性和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在价值观对齐和角色设定等任务中的广泛应用，清楚理解其表达价值观的内在机制（模型固有学习的价值）与外部提示机制（通过提示引导表达）非常重要，但对此的机理研究尚不充分。作者希望明确两者之间的关系及影响。

Method: 作者采用两种机制分析方法：（1）价值向量分析，从残差流中提取特征方向，表示价值观机制；（2）价值神经元分析，识别MLP中的特定神经元对价值表达的贡献。通过这些方法比较固有机制和提示机制的异同。

Result: 结果表明，固有机制与提示机制在实现价值观表达时具有部分共享的关键成分，也各自拥有独特组成。二者带来了不同的结果：提示机制可引导性更强，固有机制则带来更高的响应多样性。

Conclusion: 这项研究揭示了LLMs在表达价值观时固有机制与提示机制既有交集又各具特色。固有机制独特成分促进了词汇多样性，而提示机制独特成分则强化了指令执行能力，对“越狱”等任务也有影响。

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [486] [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
*Yuntao Shou,Tao Meng,Wei Ai,Keqin Li*

Main category: cs.CL

TL;DR: 本文系统性综述了当前基于大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在情感识别与推理中的研究进展，内容涵盖模型、数据集与性能评测，并提出了未来挑战及研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着对更高级语义与多模态融合的需求增加，LLMs与MLLMs在情感识别与推理领域表现突出，但缺乏系统性综述。该论文旨在填补这一空白，为学界和工业界提供指导与参考。

Method: 作者对现有LLMs及MLLMs在多模态情感识别与推理的架构、数据集、基准测试进行了全面梳理与比较，并提出该领域面临的核心挑战及未来方向。

Result: 论文系统总结了当前主流模型及其性能表现，梳理了公开数据集，展示了多模态情感识别与推理的当前水平，并列出核心难题。

Conclusion: 该综述为多模态情感识别与推理领域提供了详尽的知识体系与参考，推动未来研究发展。也是首次全面系统梳理该交叉领域的工作。

Abstract: In recent years, large language models (LLMs) have driven major advances in
language understanding, marking a significant step toward artificial general
intelligence (AGI). With increasing demands for higher-level semantics and
cross-modal fusion, multimodal large language models (MLLMs) have emerged,
integrating diverse information sources (e.g., text, vision, and audio) to
enhance modeling and reasoning in complex scenarios. In AI for Science,
multimodal emotion recognition and reasoning has become a rapidly growing
frontier. While LLMs and MLLMs have achieved notable progress in this area, the
field still lacks a systematic review that consolidates recent developments. To
address this gap, this paper provides a comprehensive survey of LLMs and MLLMs
for emotion recognition and reasoning, covering model architectures, datasets,
and performance benchmarks. We further highlight key challenges and outline
future research directions, aiming to offer researchers both an authoritative
reference and practical insights for advancing this domain. To the best of our
knowledge, this paper is the first attempt to comprehensively survey the
intersection of MLLMs with multimodal emotion recognition and reasoning. The
summary of existing methods mentioned is in our Github:
\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

</details>


### [487] [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
*Sungkyun Kim,Jaemin Kim,Dogyung Yoon,Jiho Shin,Junyeol Lee,Jiwon Seo*

Main category: cs.CL

TL;DR: 本论文提出了一种用于提升大型语言模型（LLM）推理效率的新方法——Speculative Verification (SV)，通过引入辅助模型动态优化验证过程，大幅减少无效计算，并在多个任务和模型组合上取得了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的Speculative Decoding（SD）方法虽然能加速LLM推理，但在草稿模型预测精度较低时，错误预测带来的回退验证开销会抵消加速效果，尤其是在大批量（batch）推理场景。此问题限制了SD的有效性和实际应用价值。

Method: 作者提出了Speculative Verification（SV）方法。SV在SD基础上新增一个小型辅助模型，用于预测草稿模型与目标模型分布的对齐程度，动态调整需验证token的长度，从而优化验证流程，最大化吞吐量。该方法无需修改原有草稿或目标模型，并能兼容不同SD变体。

Result: 在公开的大型语言模型上，作者在三个NLP任务、九组模型组合（包括13B到72B规模的模型、基线/指令调优/任务调优版本）上，进行了大批量（batch size 4-80）的实验。SV在所有实验和批量大小设定中均优于SD及直接使用目标模型推理。特别是在大批量场景下，平均加速可达1.4倍，最高达2倍。

Conclusion: SV方法显著提升了LLM推理效率，且具备良好的鲁棒性、可扩展性和通用性，为实际应用中高效的大模型生成推理提供了有效的解决方案。

Abstract: LLMs have low GPU efficiency and high latency due to autoregressive decoding.
Speculative decoding (SD) mitigates this using a small draft model to
speculatively generate multiple tokens, which are then verified in parallel by
a target model. However, when speculation accuracy is low, the overhead from
rejected tokens can offset the benefits, limiting SD's effectiveness,
especially at large batch sizes. To address this, we propose Speculative
Verification (SV), an efficient augmentation to SD that dynamically predicts
speculation accuracy and adapts the verification length to maximize throughput.
SV introduces a companion model - a small auxiliary model similar in size to
the draft model - to estimate the alignment between draft and target model
distributions. By maximizing the information gain from quantifying this
alignment, SV refines verification decisions, reducing wasted computation on
rejected tokens and improving decoding efficiency. Moreover, SV requires no
modifications to the draft or target models and is compatible with existing SD
variants. We extensively evaluated SV on publicly available LLMs across three
NLP tasks using nine combinations of draft, companion, and target models,
including 13B-72B target models and three types of variations: base (no
finetuning), instruction-tuned, and task fine-tuned. Across all experiments and
batch sizes (4-80), SV consistently outperforms both SD and standard decoding
with the target model. It improves SD performance by up to 2$\times$, with an
average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80).
These results demonstrate SV's robustness, scalability, and practical utility
for efficient LLM inference.

</details>


### [488] [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
*Mengyu Bu,Shaolei Zhang,Zhongjun He,Hua Wu,Yang Feng*

Main category: cs.CL

TL;DR: 本文提出AlignX方法，通过两阶段表征对齐框架提升大型语言模型（LLMs）在多语言场景下的表现，尤其是对弱势语言的支持。实验表明，该方法有效增强了模型的多语言泛化及跨语种生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语言大模型在主流语言上表现较好，但在非主流语言（弱势语言）上性能欠佳，跨语种表示对齐不足。传统训练方法难以实现高效知识迁移，因此需要新的框架来缩小多语言性能差距。

Method: 作者提出AlignX，两阶段框架：第一阶段进行多语语义对齐和语言特征融合对LLM表征进行约束；第二阶段通过多语指令微调进一步激发LLM的多语能力。

Result: 在多种已有LLM上的实验表明，AlignX方法可提升模型的多语泛化能力和跨语种生成效果，尤其改善了多语言表征的聚合性和对齐度。

Conclusion: AlignX能有效拉近不同语言的表征距离，提升多语言、跨语种的对齐及生成能力，是改进多语LLM性能的可行方法。

Abstract: Multilingual large language models (LLMs) possess impressive multilingual
understanding and generation capabilities. However, their performance and
cross-lingual alignment often lag for non-dominant languages. A common solution
is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but
such approaches often lead to imprecise alignment and suboptimal knowledge
transfer, struggling with limited improvements across languages. In this paper,
we propose AlignX to bridge the multilingual performance gap, which is a
two-stage representation-level framework for enhancing multilingual performance
of pre-trained LLMs. In the first stage, we align multilingual representations
with multilingual semantic alignment and language feature integration. In the
second stage, we stimulate the multilingual capability of LLMs via multilingual
instruction fine-tuning. Experimental results on several pre-trained LLMs
demonstrate that our approach enhances LLMs' multilingual general and
cross-lingual generation capability. Further analysis indicates that AlignX
brings the multilingual representations closer and improves the cross-lingual
alignment.

</details>


### [489] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: 本文研究了在数据有限的情况下，预训练语言模型时数据组织顺序和文本简化（数据增强）对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 目前关于语言模型预训练的大多数研究均着重于大规模数据集，对于在数据受限条件下的优化方式关注较少，特别是数据顺序及不同版本文本的作用尚不明确。该研究旨在通过课程学习（curriculum learning）来探究文本顺序和简化版本对预训练效果的影响。

Method: 作者基于一对人类撰写段落与大模型简化段落的平行语料，设计了四种数据训练顺序：重复暴露、难到易、易到难和交错顺序。通过微调和零样本评估模型在语言知识、实体追踪、常识推理等方面的表现，并从样本效率角度进行分析。

Result: 添加简化文本的数据增强方法比传统的重复使用原文更能提升微调和零样本表现。对于小模型，采用由易到难顺序收益更高；而大模型则在交错顺序下效果最佳。

Conclusion: 在数据有限的情况下，数据顺序和数据增强对语言模型预训练效果有显著影响。模型规模不同，最优数据组织方式也不同，合理设计课程学习策略可提升模型性能。

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [490] [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
*Yijun Tian,Shaoyu Chen,Zhichao Xu,Yawei Wang,Jinhe Bi,Peng Han,Wei Wang*

Main category: cs.CL

TL;DR: 该论文提出将大型语言模型的训练过程细分为预训练、中间阶段（强化中训练）和后训练，并引入RMT框架，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前主流的大型语言模型训练流程仅有预训练和后训练，忽略了中间可能需要的优化步骤。模型在推理和训练过程中存在效率低下、信息分布不均和信息利用不足的问题。

Method: 提出RMT（Reinforcement Mid-Training）框架，通过动态token预算机制、基于课程的自适应采样方法，以及结合强化学习与下一个token预测的双重训练策略，提高训练效率和模型效果。

Result: RMT在语言建模任务中，以仅21%的推理长度实现了高达64.91%的性能提升。在数学领域，RMT中训练模型的checkpoint还能进一步提高后续训练的效果，提升率达18.76%。

Conclusion: RMT作为训练流程中新引入的中间强化训练步骤，能够显著提高大语言模型的性能与训练效率，对后续训练也有促进作用。

Abstract: The development of state-of-the-art large language models is commonly
understood as a two-stage process involving pre-training and post-training. We
point out the need for an additional intermediate stage called reinforcement
mid-training with potential for strong performance gains. In this paper, we
formally define the problem and identify three key challenges: (1) inefficient
training due to excessive reasoning steps, (2) disregard of the imbalanced
token entropy distribution, and (3) underutilization of token information. To
address these challenges, we propose RMT, a framework for efficient, adaptive,
and unified reinforcement mid-training with various innovative components. In
particular, we first introduce a dynamic token budget mechanism that constrains
unnecessary reasoning steps and mitigates model overthinking. Next, we design a
curriculum-based adaptive sampling method that fosters a progressive learning
trajectory from easy to hard tokens. Finally, we present a dual training
strategy that combines reinforcement learning with next-token prediction,
ensuring targeted learning on key tokens and full exploitation of all token
information. Extensive experiments demonstrate the superiority of RMT over
state-of-the-art methods, achieving up to +64.91% performance improvement with
only 21% of the reasoning length in language modeling. We also show that
checkpoints obtained after reinforcement mid-training can benefit the
subsequent post-training, yielding up to +18.76% improvement in the
mathematical domain.

</details>


### [491] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLMs）在应对用户攻击（jailbreak attacks）时有害内容的检测，提出了 HarmMetric Eval 这一全面评测基准，并发现传统文本相似度指标比当前流行的LLM评审更有效。


<details>
  <summary>Details</summary>
Motivation: 当前对于LLMs遭受的jailbreak攻击及其有害输出，虽然有多种衡量和评审方法，但缺乏公认、系统化的评测基准，导致各种方法和效果难以对齐和比较，风险评估也不够可靠。

Method: 作者构建了一个包含高质量有害提示和多种模型响应（包括有害与无害回复）的数据集，以及一个适配多种指标和评委的灵活评分体系，可进行全局及细粒度的评价对比。

Result: 实验证明，传统文本相似度度量标准（如METEOR和ROUGE-1）在判定模型回复有害性时优于基于LLM的自动化评审，颠覆了当前有关LLM自动评审优越性的普遍观点。

Conclusion: HarmMetric Eval为有害内容评测领域提供了一个权威的基准和工具，有助于更准确地量化和比较各种检测方法的成效，推动安全性评测和模型防护方法的改进。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [492] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: LLaDA-MoE 提出了一种结合 Mixture-of-Experts（MoE）架构的大型语言扩散模型，7B 参数但推理仅激活 1.4B 参数，性能领先且推理计算消耗低。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型虽性能可观，但参数量大且推理资源消耗高；作者希望降低计算负担，同时不牺牲性能。

Method: 提出 LLaDA-MoE，将稀疏 MoE 架构整合进遮蔽扩散语言模型的训练目标。模型具备 7B 总参数，但推理时每次只激活 1.4B 参数，从零开始用约 20T tokens 训练。

Result: 在多项基准测试中，LLaDA-MoE 性能优于同类更大参数扩散语言模型（如 LLaDA、LLaDA 1.5 和 Dream）；指令微调版本在知识理解、代码生成、数学推理等任务逼近甚至超越更小模型如 Qwen2.5-3B-Instruct。

Conclusion: 稀疏 MoE 架构在扩散语言模型下也能展现其推理高效、参数利用高的优势，并为进一步探究提供了空间。LLaDA-MoE 已开源。

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [493] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: 本文提出了Agentar-Scale-SQL框架，通过多层次、协同的测试时扩展策略，有效提升了Text-to-SQL系统在BIRD等挑战性基准上的表现，达成了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在应对高难度基准（如BIRD）时，表现明显落后于人类专家。现有测试时扩展方法缺乏整体协同策略，且未充分挖掘模型内部推理能力。

Method: 提出Agentar-Scale-SQL框架，结合三种扩展策略：（1）基于强化学习（RL）的内部推理增强（Internal Scaling）；（2）通过迭代精炼的顺序扩展（Sequential Scaling）；（3）通过多样化生成及锦标赛筛选的并行扩展（Parallel Scaling）。该框架高度通用，易于适配新数据库与更强大模型。

Result: 在BIRD基准上的测试集获得81.67%的执行准确率，在官方排行榜中排名第一，达到了目前最佳水平。

Conclusion: Agentar-Scale-SQL框架显著提升了Text-to-SQL任务中的性能，为追求接近人类专家表现提供了有效路径。

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [494] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: 本文提出了多语言版本的Text-to-SQL基准MultiSpider 2.0，覆盖8种语言，并在结构复杂性和语言多样性上进行扩展。实验表明，当前主流大模型在该任务上的表现极差，仅提升至15%准确率，揭示了多语言处理的巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL任务主要基准只支持英语，严重限制了多语言、实际应用环境下模型的发展和评测。因此，作者希望推动多语言场景下的Text-to-SQL能力提升和评价。

Method: 作者以Spider 2.0为基础，扩展为8种语言，保证了原有的结构复杂性，并引入不同语言及方言的多样性。还提出了基于协作式语言智能体的baseline，用于逐步优化查询。

Result: 最先进的大模型（如DeepSeek-R1和OpenAI o1）在MultiSpider 2.0上仅有4%的执行准确率，远低于在以往多语言基准（MultiSpider 1.0）上的60%。提出的协作智能体Baseline可提升至15%。

Conclusion: 多语言Text-to-SQL任务对现有大模型提出极大挑战。论文所建基准能更真实地反映复杂多语言环境下的能力缺口，未来应针对多语言和多结构复杂性发展更健壮的方法，以支持企业级实际应用。

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [495] [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
*Haosi Mo,Xinyu Ma,Xuebo Liu,Derek F. Wong,Yu Li,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了CDT（Cognition-Domain-Task）评测框架，用于全方位测量大语言模型（LLMs）的能力，从认知理论出发，弥补了现有评测侧重单一能力的局限。实验证明，CDT评测指标与实际下游任务表现高度相关，并能有效指导数据集分析与选择，性能显著超过基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）能力增强，但主流评测标准往往只聚焦于某一单一能力，缺乏对模型整体能力的全方位评估方法。这种局限阻碍了对模型真实水平的全面认知和科学比较，因此亟需一种新的、系统性的评测框架。

Method: 作者基于Cattell-Horn-Carroll认知理论，提出了CDT（Cognition-Domain-Task）三维度框架，从认知、领域、任务三个层面重新定义并细化模型能力。该框架可用于模型能力评估和数据选择，通过具体实验对能力指标和数据选择效果进行了验证。

Result: 实验结果显示，基于CDT的能力衡量指标与下游任务表现高度相关。同时，CDT用于数据选择在多个评测基准上提升显著，分别比基线提高1.6和2.2分，证明了CDT的有效性和实用性。

Conclusion: CDT框架为大模型能力评估和数据集构建提供了科学、全面的新方法，有助于推动LLMs实际应用和后续研究，具备很强的实践价值和推广前景。

Abstract: Recent advances in Large Language Models (LLMs) have significantly enhanced
their capabilities, highlighting the need for comprehensive evaluation
frameworks that extend beyond task-specific benchmarks. However, existing
benchmarks often focus on isolated abilities, lacking a holistic framework for
assessing LLM capabilities. To address this gap, we propose the
Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's
capabilities across three dimensions. We expand the scope of model capability
definitions at the cognitive level by incorporating the Cattell-Horn-Carroll
cognitive theory, refining the categorization of model capabilities. We apply
CDT in two directions: dataset capability evaluation and data selection.
Experiments show that our capability metrics correlate well with downstream
performance and can support effective dataset analysis and construction. The
experiments on data selection also show significant improvements in both
general and specific benchmarks, achieving scores of 44.3 and 45.4, with an
increase of 1.6 and 2.2 points over the baselines, respectively. These results
validate the effectiveness and practicality of CDT. Source code and models are
available at https://github.com/Alessa-mo/CDT.

</details>


### [496] [Alternatives To Next Token Prediction In Text Generation - A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 本文综述了作为大模型主流范式的下一词预测（NTP）的局限，并系统梳理了五类NTP替代方案，为研究社区指明了未来改进及创新的方向。


<details>
  <summary>Details</summary>
Motivation: 尽管NTP（下一词预测范式）推动了大语言模型的崛起，但也带来了诸如长期推理差、错误积累和计算低效等问题。为克服这些不足，研究者正在探索NTP以外的生成方法。本文旨在梳理和分类这些新兴替代方案，帮助理论和应用层面的进步。

Method: 本文采用文献调研与技术归纳的方法，将NTP的主要替代方法归为五大类：1）多词预测（一次预测多个词块）；2）先规划后生成（先生成高层次计划再细化生成）；3）隐空间推理（在连续隐空间内自回归生成）；4）连续生成（利用扩散、能量驱动等方法并行迭代生成）；5）非Transformer架构（通过不同结构本身规避NTP）。

Result: 通过对不同替代范式的系统比较和归纳，本文提出一套详细的分类体系，不仅总结了当前的研究成果，也指出了各范式各自的优势和适用场景。

Conclusion: 该综述文为理解和突破NTP带来的局限性提供了理论基础和研究指南，呼吁学界继续探索更高效、更具推理能力的新一代生成模型架构。

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [497] [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文提出了SOBACO基准，用于统一评测LLM中的社会偏见与文化常识，并发现现有去偏方法会严重削弱模型的文化常识能力。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法可能削弱大模型能力，但以往研究主要用与社会偏见无关的任务评估影响，忽略了与社会偏见关系紧密的文化常识。

Method: 作者提出SOBACO（日语）基准，从社会偏见和文化常识两个维度统一评测，并用多个大模型测试不同去偏方法对文化常识的影响。

Result: 实验发现去偏方法会导致LLM在文化常识任务上的准确率显著下降，最高降幅可达75%。

Conclusion: 去偏方法在消除偏见的同时，可能牺牲文化常识表现，因此优化去偏方法时需权衡公平性与实用性。

Abstract: Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.

</details>


### [498] [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
*Lasse Borgholt,Jakob Havtorn,Christian Igel,Lars Maaløe,Zheng-Hua Tan*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的对齐算法，通过动态规划结合beam search评分，提升了语音识别模型输出和参考文本之间的对齐精度，从而支持更细粒度的误差分析。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络在语音识别方面取得重大进展，但主要提升体现在高频、语义权重有限的词语上，不常见词、专有名词和领域词汇的错误不易被标准的整体指标发现，严重影响应用场景。这需要更细粒度的误差分析，但前提是具备高精度的文本对齐方法，而传统对齐方法难以达到此要求。

Method: 作者提出了一种结合动态规划和beam search评分的新型对齐算法，与传统方法相比，让每个错误的对齐更加精确。

Result: 新方法在个体错误的对齐精度上优于现有常规文本对齐方法，为后续更可靠的误差分析提供了基础。该算法还通过PyPI公开发布。

Conclusion: 所提出的对齐算法能更准确地识别关键错误类型，为语音识别误差分析和模型提升提供了有力支持，具有实际应用价值。

Abstract: Modern neural networks have greatly improved performance across speech
recognition benchmarks. However, gains are often driven by frequent words with
limited semantic weight, which can obscure meaningful differences in word error
rate, the primary evaluation metric. Errors in rare terms, named entities, and
domain-specific vocabulary are more consequential, but remain hidden by
aggregate metrics. This highlights the need for finer-grained error analysis,
which depends on accurate alignment between reference and model transcripts.
However, conventional alignment methods are not designed for such precision. We
propose a novel alignment algorithm that couples dynamic programming with beam
search scoring. Compared to traditional text alignment methods, our approach
provides more accurate alignment of individual errors, enabling reliable error
analysis. The algorithm is made available via PyPI.

</details>


### [499] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM安全防护框架Self-Sanitize，能在无需额外延迟和资源开销的情况下，实时发现并修正有害内容，显著优于现有过滤方法，提升隐私防护及模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类场景的广泛应用，其生成有害内容或泄漏隐私的风险引发关注。现有的后置内容过滤方法带来延迟高、资源占用大及无法流式生成的问题，亟需高效、低延迟的新型防护方案。

Method: 作者提出Self-Sanitize框架，受认知心理学启发，引入轻量级Self-Monitor模块对LLM的生成意图进行持续检测，并通过Self-Repair模块在检测到有害内容时即刻原地修正，实现对话流中的实时监控与修复，无需跳出当前对话。该方案通过表征工程在token级别操作，适合流式生成场景。

Result: 在四种主流LLM和三类隐私泄漏场景下进行大量实验，Self-Sanitize在降低有害内容泄露方面效果显著，且对系统延迟和计算资源影响极小，模型正常效用未受到影响。

Conclusion: Self-Sanitize为构建更安全、实用的大语言模型提供了有效、低成本的解决方案，尤其在隐私保护方面表现突出，有望广泛应用于实际部署中。

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [500] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: 本文提出了GRPO-MA方法，通过多答案生成改进了LLMs和VLMs的链式思维推理训练，显著提升了模型性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO算法在大模型链式推理训练中已表现有效，但存在梯度耦合、奖励稀疏和优势估计不稳定等问题，限制了方法的进一步提升。

Method: 作者提出了GRPO-MA方法，在每一步思维过程中生成多个答案，通过增加样本数量降低梯度方差，改进优化过程。方法上具备理论依据，并通过梯度分析和实验验证了效果。

Result: 实验结果表明，GRPO-MA在数学、代码及多模态任务上均大幅提升模型性能和训练效率。梯度分析显示，相比原方法，GRPO-MA显著降低了梯度尖峰。此外，消融实验验证生成更多答案能够持续增强模型表现。

Conclusion: GRPO-MA成功克服了GRPO算法的关键挑战，为链式推理训练提供了更鲁棒和高效的方法，对语言与视觉-语言大模型训练具有理论与实际意义。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [501] [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
*Haewon Park,Sangwoo Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种名为SUIT的知识编辑方法，通过仅修改关键子空间的特征，有效减少了对大模型整体行为的干扰，同时确保高效的知识编辑。该方法在多个主流模型上的实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型知识编辑方法容易引入不必要的全局扰动，破坏模型的已有知识。作者希望找到一种更精准、局部的方法，仅针对需要纠正的知识进行编辑，减少副作用。

Method: 提出了Subspace Knowledge Edit (SUIT) 方法。该方法首先定位和识别出与待编辑知识高度相关的子空间，然后只在这个关键子空间上进行参数修改，避免不必要的全局干预。

Result: 在LLaMA-3-8B、GPT-J-6B和Qwen2.5-7B等主流大语言模型上，SUIT显著优于对比基线方法，既提升了编辑准确性，又更好地保留了其他原有知识。

Conclusion: SUIT方法能有效定位和编辑知识相关子空间，在减少全局知识干扰的同时，实现高效编辑。实验验证其有效性和通用性，为今后更安全、精准的模型知识编辑提供了新思路。

Abstract: Knowledge editing aims to efficiently correct factual errors in Language
Models (LMs). The popular locate-then-edit approach modifies an MLP layer by
finding an optimal mapping between its input vector (key) and output vector
(value) that leads to the expression of the edited knowledge. However, existing
methods without any constraints on the key and value vectors cause significant
perturbations to the edited model. To address this, we propose Subspace
Knowledge Edit (SUIT), a method that identifies and modifies only the subspace
of critical features relevant to the edit. Our empirical results on LLaMA-3-8B,
GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge
preservation over strong baselines while maintaining high edit efficacy. This
effectiveness confirms that SUIT successfully identifies the critical subspace
for the edit. Further analyses provide additional validation for our approach.
The source code and data will be released to the public upon publication of the
paper.

</details>


### [502] [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
*Hamna,Gayatri Bhat,Sourabrata Mukherjee,Faisal Lalani,Evan Hadfield,Divya Siddarth,Kalika Bali,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 本文提出了一种由社区主导的评估流程Samiksha，使大语言模型（LLMs）在现实社区场景下进行自动化、文化敏感的效果评测。该方法在印度健康领域进行了验证，强调了包容性和场景适应性的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估多依赖通用或领域基准，缺乏对终端用户实际需求和文化背景的考虑，尤其在如医疗等关键领域中表现不足。作者希望通过更贴近社区实际的评测方式，实现对模型更具现实指导意义的评估。

Method: 提出了Samiksha评估流程，该流程通过与民间社会组织和社区成员协作，收集社区反馈，来确定评测重点、流程和评分标准，实现社区共创。整个流程是自动化、可扩展且兼具文化敏感性的。

Result: 在印度健康领域的实证中，验证了Samiksha框架的有效性。分析了当前多语言LLM应对细致健康问题的表现，展示了社区主导评估流程的成功应用。

Conclusion: 社区驱动、情境化的LLM评估方法更能反映实际需求和文化差异，为关键领域（如医疗）人工智能应用奠定了更具包容性和现实基础的评测路径。

Abstract: Large Language Models (LLMs) are typically evaluated through general or
domain-specific benchmarks testing capabilities that often lack grounding in
the lived realities of end users. Critical domains such as healthcare require
evaluations that extend beyond artificial or simulated tasks to reflect the
everyday needs, cultural practices, and nuanced contexts of communities. We
propose Samiksha, a community-driven evaluation pipeline co-created with
civil-society organizations (CSOs) and community members. Our approach enables
scalable, automated benchmarking through a culturally aware, community-driven
pipeline in which community feedback informs what to evaluate, how the
benchmark is built, and how outputs are scored. We demonstrate this approach in
the health domain in India. Our analysis highlights how current multilingual
LLMs address nuanced community health queries, while also offering a scalable
pathway for contextually grounded and inclusive LLM evaluation.

</details>


### [503] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的端到端方法AdaThink-Med，用于提升医学大语言模型（LLM）的自适应思考能力，使其能根据问题难度动态调整推理长度，显著降低推理成本且保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型即便面对简单问题也会执行冗长的推理，导致推理成本居高不下，难以高效应用于实际医疗场景。因此亟需让模型针对不同难度的问题做出不同的推理长度，实现更高效的“自适应思考”。

Method: AdaThink-Med 首先为每个问题生成多个答案候选者，评估每个候选的正确性和不确定性。模型通过不确定性引导的长度校准模块估算问题难度。对于简单且答案正确的问题，系统惩罚过长推理；相反，对于难题且答错，则鼓励模型延长推理路径/探索更多解答。

Result: 在六个公开医学问答基准上，AdaThink-Med 平均实现了高达6.4倍的推理长度压缩，性能几乎无损失，仅有极小降幅。实验还发现模型自发出现了“非思考”和“思考”两种推理模式，能动态抑制冗余推理过程。

Conclusion: AdaThink-Med有效提升了医疗LLM的自适应推理能力，兼顾高推理效率与性能，为医学领域实际部署大模型提供新方向。

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [504] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: 本研究利用大规模视觉-语言模型（VLMs）模拟阅读障碍（dyslexia），揭示与视觉词形区功能障碍相关的机制。通过有针对性地移除模型中类比于字词加工的单元，复现实验中阅读障碍的关键特征。


<details>
  <summary>Details</summary>
Motivation: 阅读障碍与大脑视觉词形区活性降低密切相关，但现有行为和神经影像方法难以明确推断致因机制，因此需要新的手段来探索其底层机制。

Method: 通过VLMs识别模型中专门处理视觉词形的单元，并通过有针对性地“消融”这些单元，测试该操作对模拟阅读任务和其他认知任务的影响，比较与随机单元消融的异同。

Result: 有针对性地移除视觉词形单元后，模型在阅读任务上出现选择性障碍，而综合视觉或语言能力保持不变，且出现类似人类阅读障碍者的语音学困难，却不影响正字法加工。

Conclusion: 研究不仅重现了阅读障碍的关键特征，还提出了一种全新的计算性框架，为探究阅读障碍等读写障碍性疾病的机制提供了新工具。

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [505] [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
*Gio Paik,Yongbeom Kim,Soungmin Lee,Sangmin Ahn,Chanwoo Kim*

Main category: cs.CL

TL;DR: 该论文提出了HiKE，这是第一个全球可访问的韩英混合语（code-switching，CS）评测基准，专门用于评估多语种语音识别（ASR）系统在韩英混合语场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多语种ASR技术进展显著，但在实际生活中普遍存在的“混合语”现象（code-switching，即在一句话中混合编用两种语言）却研究不足，尤其是韩英混合语缺乏标准化评测工具。

Method: 作者开发了HiKE评测框架，包含高质量、自然的韩英混合语语音数据，涉及多个话题，并精确标注了借词（loanword）以及分层的混合语级别（单词、短语、句子级）。此外，评估了多种多语种ASR模型并进行针对HiKE的微调实验。

Result: 实验表明，现有多语种ASR模型普遍难以直接应对韩英混合语识别任务，但通过使用HiKE数据进行微调，可显著提升模型对混合语的识别能力。

Conclusion: HiKE为多语种ASR模型在韩英混合语场景提供了系统、细致的评估依据。该基准可促进后续混合语语音识别领域的深入研究和技术提升。

Abstract: Despite advances in multilingual automatic speech recognition (ASR),
code-switching (CS), the mixing of languages within an utterance common in
daily speech, remains a severely underexplored challenge. In this paper, we
introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the
first globally accessible evaluation framework for Korean-English CS, aiming to
provide a means for the precise evaluation of multilingual ASR models and to
foster research in the field. The proposed framework not only consists of
high-quality, natural CS data across various topics, but also provides
meticulous loanword labels and a hierarchical CS-level labeling scheme (word,
phrase, and sentence) that together enable a systematic evaluation of a model's
ability to handle each distinct level of code-switching. Through evaluations of
diverse multilingual ASR models and fine-tuning experiments, this paper
demonstrates that while most multilingual ASR models initially struggle with
CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE
will be available at https://github.com/ThetaOne-AI/HiKE.

</details>


### [506] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: 本文提出并实现了科学论文和资助申请中“夸大宣传性语言”（hype）的自动检测任务，制定注释标准，构建数据集，并验证了机器学习模型在该任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 近年来科学文献中的宣传性、主观性用语（hype）日益增多，这种现象可能影响客观证据评价、阻碍科学发展并损害公众信任，因此亟需自动化手段检测和抑制这种不规范用语。

Method: 作者首先界定了hype的定义，并制定了具体的语言注释指南，随后在NIH资助申请文本中标注hype语言，构建数据集。然后，作者用传统文本分类器和语言模型在该任务上进行实验，并与人工基线做性能对比。

Result: 实验表明，规范化注释标准能帮助人类更一致地识别出hype形容词；基于该数据集训练的机器学习模型取得了有前景的检测效果。实验显示该任务具有较高的语言复杂性，可能需要领域知识和时效性识别能力。

Conclusion: 本文首次将“夸大宣传性语言检测”系统化为自然语言处理任务，验证了自动检测的可行性，并为未来相关研究提供了数据集和方法论基础。

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [507] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了InfLLM-V2框架，实现了稠密-稀疏可切换注意力机制，极大提升Transformer在处理长序列任务时的效率与性能。


<details>
  <summary>Details</summary>
Motivation: Transformer结构在处理长序列时，标准自注意力计算和内存消耗急剧上升，为大模型的实际应用设下瓶颈。虽然已有稀疏注意力方法可以缓解，但它们通常引入了过多额外参数并打破了"短序列预训练-长序列微调"的工作流，导致收敛变慢且难以加速。

Method: 提出InfLLM-V2，将稠密注意力参数在无额外参数改动下复用到稀疏注意力，通过架构调整实现短序列稠密注意力与长序列稀疏注意力之间的无缝切换，兼顾一致性和高效性；同时设计了极为高效的实现方式，减少计算开销。

Result: 实验结果表明，在长上下文理解和链式推理任务中，InfLLM-V2相较于稠密注意力实现了4倍提速，同时分别保留了98.1%和99.7%的性能。此外，作者基于该框架训练并开源了MiniCPM4.1大模型。

Conclusion: InfLLM-V2框架为长序列处理提供了兼顾效率和效果的新方案，有利于大模型在更广阔应用场景的落地，同时其开源实现便于社区复现与后续研究。

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [508] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 该论文评估了大语言模型（LLMs）中“去学习”技术的有效性，发现目前主流方法要么不能有效移除敏感知识，要么导致模型通用能力崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着大模型应用的广泛，如何安全和彻底地移除模型中某一类知识（比如敏感信息）成为重要需求；但去学习方法的成效和副作用一直存疑，具体细节缺乏可解释分析。

Method: 提出unPact框架，通过分析prompt各token对模型输出的贡献，对去学习过程中知识变化进行可解释性追踪，并比较去学习前后模型的表现。实验涵盖六种去学习方法、三种主流模型及三套评测集。

Result: （1）现有去学习法主要通过扰乱关键字在prompt中的作用来实现表面上的知识移除；（2）被“遗忘”的知识可通过加强关键字提示轻松恢复；（3）灾难性遗忘源于对所有prompt token的无差别惩罚。

Conclusion: 当前主流去学习方法存在两难：要么知识未真正移除，容易被找回；要么模型整体能力大幅下降。可靠的去学习方法仍需进一步探索。

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [509] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: 该论文探讨了在没有参考答案的情况下，利用单次响应的LLM（大语言模型）对答案进行评分的可靠性，并提出了更细粒度且确定性的评分方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流做法是让大语言模型作为裁判，对文本答案打分（如Likert量表），但这种做法存在结果不稳定且分数校准差的问题，导致分数压缩和分数并列现象，因此需要探索更可靠的评分方式。

Method: 作者提出了"Latent Judges"方法，从模型内部信号中提取分数，包括：1) 基于概率加权的整数评分；2) 类似验证者的“是/否”概率输出；3) 对模型激活位置进行线性探测。并在多个单项和成对评价基准上进行测试对比。

Result: Latent Judges方法在多项指标上表现优异，常规基于提示的方法。尤其概率加权分数单项相关性最强，当输出logits校准不佳时，探针还能挖掘有用信号。

Conclusion: 模型内部隐含信息能够产生更确定、更具区分力的评分信号，提升了无参考评价的表现，有助于后续最佳答案选择、模型蒸馏和分流等实际应用。

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [510] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文提出了一种名为MemGen的动态生成式记忆框架，使大模型驱动的智能体在推理过程中能够更加人性化地利用和扩展记忆，并显著优于以往的外部记忆系统。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体记忆方法要么直接调整模型参数（参数记忆），要么将经验外化为结构化数据库（检索记忆），都无法模拟人类推理与记忆高度交织的认知特性。该研究旨在填补这一认知空白。

Method: 提出MemGen框架，包括两大模块：1）memory trigger，监控推理状态以决定何时调用记忆；2）memory weaver，以当前推理状态生成潜在的token序列作为“机器原生”的记忆，用以丰富推理。整体使记忆与推理过程深度交互。

Result: 在8个基准任务上，MemGen最大可提升38.22%的性能（优于ExpeL、AWM），对比GRPO最大提升13.44%，且展现出强跨领域泛化能力。

Conclusion: MemGen无需显式监督即可自发进化出规划记忆、程序性记忆与工作记忆等人与类类似的记忆能力，开启了更加自然的机器认知新方向。

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [511] [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
*Shaobo Wang,Zhengbo Jiao,Zifan Zhang,Yilang Peng,Xu Ze,Boyu Yang,Wei Wang,Hu Wei,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了Socratic-Zero，一个无需人工标签、完全自主生成高质量推理训练数据的闭环框架，仅从极少种子问题出发，实现了大模型在数学推理任务上的显著进步。


<details>
  <summary>Details</summary>
Motivation: 目前LLM推理能力的提升高度依赖大量人工标注高质量数据，难以扩展；数据合成/蒸馏技术则面临数据质量不一致和难以随模型能力自适应的问题。

Method: Socratic-Zero利用教师（Teacher）、解答者（Solver）、生成器（Generator）三智能体协同进化。Solver基于偏好反馈自我改进推理能力；Teacher根据Solver弱点自适应生成更具挑战性的问题；Generator从Teacher中学习高效的题目生成策略，实现大规模高质量课程自动生成。该系统无需预设任务与标签，通过闭环交互自我改进。

Result: 只需100道种子题，Socratic-Solver-8B在七个数学推理基准上平均提升20.2个百分点，且对多个主流大模型均有一致效果。其生成的数据更让学生模型超越了现有SOTA商业大模型（如GPT-5、Gemini等）。

Conclusion: Socratic-Zero可极大降低高质量推理数据生成的人工成本，自动实现数据/模型共进化，有望推动LLM推理能力持续突破。

Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely
heavily on massive, high-quality datasets-typically human-annotated and thus
difficult to scale. While data synthesis or distillation offers a promising
alternative, existing methods struggle with inconsistent data quality and an
inability to dynamically adapt to the evolving capabilities of the model,
leading to suboptimal training signals. To address these limitations, we
introduce Socratic-Zero, a fully autonomous framework that generates
high-quality training data from minimal seed examples through the co-evolution
of three agents: the Teacher, the Solver, and the Generator. The Solver
continuously refines its reasoning by learning from preference feedback on both
successful and failed trajectories; the Teacher adaptively crafts increasingly
challenging questions based on the Solver's weaknesses; and the Generator
distills the Teacher's question-design strategy to enable scalable,
high-fidelity curriculum generation. This closed-loop system produces a
self-improving curriculum-requiring no pre-existing tasks or labels.
Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B
achieves an average gain of +20.2 percentage points over prior data synthesis
methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,
Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3
and GLM4 series models. Even more surprisingly, synthetic data from
Socratic-Generator-32B enables student LLMs to achieve superior performance
compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,
including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,
and Claude-4.1-Opus.

</details>


### [512] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种新的无训练稀疏注意力算法ProxyAttn，在降低计算复杂度的同时，能更精确地估算注意力块的重要性，从而提升了大模型处理长文本时的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的二次复杂度限制了大语言模型在长文本任务中的效率，现有块稀疏注意力方法虽然加速明显，但粗粒度估算导致高稀疏率下性能下降。因此需要更精细、高效的方法提升长文本处理性能并减少性能损失。

Method: 作者提出ProxyAttn，通过压缩注意力头的维度，利用代表性头部的合并分数近似所有头部分数，同时引入块感知的动态预算估计。该方法无需训练，结合代表性分数与多头动态预算，实现细粒度的块重要性评估，计算成本低。

Result: 在各种主流模型和基准测试上实验发现，不同注意力头之间确有高度相似性。基于更细致的估算，ProxyAttn实现了最高10.3倍注意力加速、2.4倍预填加速，且基本无性能损失。

Conclusion: ProxyAttn方法充分利用了注意力头内的冗余，相比现有方法兼顾效率与精度，为长文本任务提供了一种高效、简单的解决方案。

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [513] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出LatentEvolve框架，通过模拟人脑昼夜学习机制，使大语言模型在推理阶段自我进化地提升推理能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Test-time Scaling（TTS）方法虽然能提升大语言模型在推理阶段的能力，但它们是独立的，缺乏持续学习如何更有效缩放的能力。因此，亟需一种让模型能逐步进化学习TTS的方法。

Method: 作者提出LatentEvolve框架，受人脑互补学习系统理论启发，包含昼间快速检索历史潜在表示（daytime scaling）和夜间整合优化（nighttime scaling），交替进行，增强模型的推理能力，实现无监督自进化。

Result: 在八个基准和五种模型上测试，LatentEvolve在性能上优于现有TTS方法，如LatentSeek和TTRL，最大提升达到13.33%，并展现出强大的跨领域、跨模型泛化能力。

Conclusion: LatentEvolve有效促进大语言模型在测试时推理能力的自我进化，显著优于现有TTS技术，具有良好泛化性。

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [514] [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
*Jun Rao,Yunjie Liao,Xuebo Liu,Zepeng Lin,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的偏好优化方法SeaPO，通过有策略地增加错误类型，使负样本与正样本区别更明显，提高大模型的偏好学习效果。实验证明这种方式能显著提升模型在多个能力维度的表现，尤其在真实性提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏好优化依赖正负样本，但在训练过程中两者质量趋于接近，导致优化难度加大，难以进一步提升模型性能。

Method: 提出了SeaPO方法，通过引入模型常见的三类错误，系统性地加大负样本的错误性。在偏好训练中，通过让模型更容易区分正负样本，从而更有效地优化模型表现。对1.5B至14B规模的模型，在五个能力维度上进行评测。

Result: 使用这种数据增强与优化策略，模型整体能力明显提升，尤其在输出真实性上提高了5-10个百分点。不同错误类型对相关任务的提升效果有差异，混合多种错误类型能带来更广泛的提升。

Conclusion: 有策略地放大负样本错误性是一种有效的偏好优化方法，实现了更优的模型表现，特别适用于提升输出真实性及多个任务表现，建议后续偏好优化可采用此类策略。

Abstract: Existing alignment methods for preference optimization of large language
models (LLMs) aim to enhance model performance by utilizing pairs of positive
and negative samples. However, due to the limited capacity of models in scoring
or generating responses, the quality of positive and negative samples may
become similar during training, which complicates optimization for preference
learning. To address this issue, we introduce SeaPO, a Strategic Error
Amplification method that leverages three error types commonly occurring in
LLMs to introduce specific error patterns into the model Preference
Optimization. This strategy ensures that negative samples are more erroneous
than positive samples and preference-based training is employed to mitigate the
occurrence of these errors, thereby enhancing model performance. Evaluations
across five capability dimensions and different model scales (1.5B to 14B)
demonstrate that the generated data significantly improved overall model
performance, particularly in terms of truthfulness, with improvements of 5-10
percentage points observed. Further analysis reveals that task performance
varies depending on the error types introduced. Injecting the most common error
types improves performance in related tasks, while a mix of error types leads
to a broader performance enhancement: most tasks show stable improvements,
while a few tasks exhibit significant gains.

</details>


### [515] [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
*Luisa Geiger,Mareike Hartmann,Michael Sullivan,Alexander Koller*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的、自动化的树结构评价指标，用于更准确地评估大语言模型生成的逐步装配说明，尤其关注空间和时间要素。实验证明该指标在缝纫领域优于传统文本相似性指标。


<details>
  <summary>Details</summary>
Motivation: 传统用于评估说明生成质量的指标（如BLEU、BERT相似度）主要基于文本表面相似性，难以反映装配说明中空间与时序的正确性。在实际任务中，说明的空间和时间合理性对于装配结果至关重要。因此，需要一种能够反映这类复杂性的自动评估指标。

Method: 作者设计了一种树结构为基础的自动化评价方法，将逐步说明映射为树形结构，针对空间与时间信息进行分析。并将这种新指标应用于缝纫说明生成任务，对比传统BLEU、BERT等指标，实验中同时利用人工评价和错误注释验证该方法有效性。

Result: 实验结果显示，该方法与人工标注错误数及人类质量评分的相关性更高，优于传统指标。此外，该指标在应对恶意构造的迷惑性反例时，表现出更强的鲁棒性。

Conclusion: 树结构评价指标能更准确反映装配说明的空间与时间有效性，在缝纫指令等需要时序空间合理性的任务中，比传统文本相似度指标更有优势。

Abstract: In this paper, we propose a novel, automatic tree-based evaluation metric for
LLM-generated step-by-step assembly instructions, that more accurately reflects
spatiotemporal aspects of construction than traditional metrics such as BLEU
and BERT similarity scores. We apply our proposed metric to the domain of
sewing instructions, and show that our metric better correlates with
manually-annotated error counts as well as human quality ratings, demonstrating
our metric's superiority for evaluating the spatiotemporal soundness of sewing
instructions. Further experiments show that our metric is more robust than
traditional approaches against artificially-constructed counterfactual examples
that are specifically constructed to confound metrics that rely on textual
similarity.

</details>


### [516] [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
*Xilin Dang,Kexin Chen,Xiaorui Su,Ayush Noori,Iñaki Arango,Lucas Vittor,Xinyi Long,Yuyang Du,Marinka Zitnik,Pheng Ann Heng*

Main category: cs.CL

TL;DR: 本文提出了一种新方法KnowGuard，通过在临床诊断任务中引入知识图谱探索，提升大语言模型（LLM）在面对信息不足时合理选择“弃权”而非过度自信给出不可靠答案的能力。实验证明该方法在诊断准确率和交互效率上均优于已有技术。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医学场景下往往在信息不足时仍给出自信答案，缺乏识别自身知识边界和合理弃权的机制，这在临床实践中可能导致误诊，存在安全隐患。

Method: 提出KnowGuard框架，分为证据发现和证据评估两个阶段，基于共享证据池，首先通过知识图谱扩展和检索系统性发现医学证据，然后通过多因素对证据进行评估排序，根据病人语境和对话历史自适应调整探索，判断是否有足够证据支持给出诊断，否则选择弃权。

Result: 在开放式多轮临床对话基准上评测，与现有弃权方法相比，KnowGuard将诊断准确率提高了3.93%，同时平均减少了7.27轮不必要的交互。

Conclusion: KnowGuard通过整合知识图谱和结构化证据探索，显著提升了大语言模型在临床场景下安全弃权的能力，提高了诊断性能，对医疗AI安全应用具有积极推动作用。

Abstract: In clinical practice, physicians refrain from making decisions when patient
information is insufficient. This behavior, known as abstention, is a critical
safety mechanism preventing potentially harmful misdiagnoses. Recent
investigations have reported the application of large language models (LLMs) in
medical scenarios. However, existing LLMs struggle with the abstentions,
frequently providing overconfident responses despite incomplete information.
This limitation stems from conventional abstention methods relying solely on
model self-assessments, which lack systematic strategies to identify knowledge
boundaries with external medical evidences. To address this, we propose
\textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that
integrates systematic knowledge graph exploration for clinical decision-making.
Our approach consists of two key stages operating on a shared contextualized
evidence pool: 1) an evidence discovery stage that systematically explores the
medical knowledge space through graph expansion and direct retrieval, and 2) an
evidence evaluation stage that ranks evidence using multiple factors to adapt
exploration based on patient context and conversation history. This two-stage
approach enables systematic knowledge graph exploration, allowing models to
trace structured reasoning paths and recognize insufficient medical evidence.
We evaluate our abstention approach using open-ended multi-round clinical
benchmarks that mimic realistic diagnostic scenarios, assessing abstention
quality through accuracy-efficiency trade-offs beyond existing closed-form
evaluations. Experimental evidences clearly demonstrate that KnowGuard
outperforms state-of-the-art abstention approaches, improving diagnostic
accuracy by 3.93\% while reducing unnecessary interaction by 7.27 turns on
average.

</details>


### [517] [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
*Rui Jia,Yuang Wei,Ruijia Li,Yuang-Hao Jiang,Xinyu Xie,Yaomin Shen,Min Zhang,Bo Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的认知诊断模型DiaCDM，能够在教师-学生真实对话场景下更准确地评估学生知识掌握情况。


<details>
  <summary>Details</summary>
Motivation: 传统的认知诊断(CD)模型主要针对结构化测试数据，难以适应现实中的动态、非结构化师生对话。而现有模型欠缺处理非结构化对话的理论框架，也难以从冗长对话中准确提取诊断语义。

Method: 作者提出DiaCDM模型，引入教育理论中的“IRE（提问-回应-评价）”框架以适应对话诊断，同时开发了基于图的编码方法，将教师提问与相应知识点关联，更精确地捕捉关键信息。

Result: 在三个真实对话数据集上的实验显示，DiaCDM模型相较现有方法显著提升了认知诊断的准确性，并增强了结果的可解释性。

Conclusion: DiaCDM为师生对话中的认知诊断开辟了新路径，不仅提升了诊断效果，还为教师提供了有效的学生认知状态评估工具。

Abstract: While cognitive diagnosis (CD) effectively assesses students' knowledge
mastery from structured test data, applying it to real-world teacher-student
dialogues presents two fundamental challenges. Traditional CD models lack a
suitable framework for handling dynamic, unstructured dialogues, and it's
difficult to accurately extract diagnostic semantics from lengthy dialogues. To
overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted
the initiation-response-evaluation (IRE) framework from educational theory to
design a diagnostic framework tailored for dialogue. We also developed a unique
graph-based encoding method that integrates teacher questions with relevant
knowledge components to capture key information more precisely. To our
knowledge, this is the first exploration of cognitive diagnosis in a dialogue
setting. Experiments on three real-world dialogue datasets confirm that DiaCDM
not only significantly improves diagnostic accuracy but also enhances the
results' interpretability, providing teachers with a powerful tool for
assessing students' cognitive states. The code is available at
https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.

</details>


### [518] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: SemShareKV通过语义相似性共享和压缩KV缓存，有效提升大模型推理效率，显著加快速度并降低GPU内存占用，且输出质量基本无损。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的规模扩大，推理时用于存储历史信息的KV缓存占用越来越多的显存资源，成为推理加速的瓶颈。现有压缩或复用KV缓存的方法多依赖于词面完全匹配，但面对语义相似却词面不同的输入时效果有限，尤其在多文档摘要和对话场景经常遇到这种情况。

Method: 提出SemShareKV框架，基于语义相似性来共享和压缩KV缓存。具体方法利用LSH（局部敏感哈希）在token嵌入上做模糊匹配，同时结合RoPE以保留位置信息，从参考缓存中选择性重用相关的KV对，从而减少冗余计算。

Result: 在多种摘要数据集上，SemShareKV实现了最高6.25倍的推理加速和42%的显存节省（5k tokens输入），并且输出质量几乎没有明显下降。

Conclusion: 证明了基于语义感知的缓存共享在提升大模型推理效率上的巨大潜力，为未来相关加速方法提供了有益思路。

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [519] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 本文提出了一个分层错误纠正（HEC）框架，通过系统性误差分析和分层干预，改善大型语言模型（LLM）在医疗、法律等专门领域的表现。框架在多个领域验证有效，提升平均准确率11.2个百分点，但在高基线任务中有限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定专业领域任务（如医疗编码）表现不佳，准确率低，表现出严重局限性，急需系统性提升方法。现有模型在专门领域面对复杂、结构化的知识和推理要求时易出错，因此作者寻求通过系统性误差分析和干预提升AI表现。

Method: 作者首先在四个专门领域分析LLM的错误模式，将其分为知识层、推理层与复杂性层。基于这种分层结构，设计了三阶段分层纠错框架，根据误差层次的重要性逐层修正模型输出。方法在跨领域和跨模型实验中进行了验证。

Result: 框架在医疗转录、法律文档分类、政治偏见检测和法律推理等四个任务上实现了显著且一致的性能提升，平均提升11.2个百分比点；在高基线（>75%准确率）任务中增益有限甚至可能干扰模型原有推理。

Conclusion: 系统性分层错误分析和纠错可有效提升LLM在特定领域中等基线任务的表现，但在高水平任务中应谨慎使用。理解框架适用边界对于实际落地和最优部署至关重要。

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [520] [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
*Adrian Arnaiz-Rodriguez,Miguel Baidal,Erik Derner,Jenn Layton Annable,Mark Ball,Mark Ince,Elvira Perez Vallejos,Nuria Oliver*

Main category: cs.CL

TL;DR: 本文研究了当前主流大语言模型在应对心理健康危机相关情境下的能力和安全性，发现虽然模型整体表现一致且可靠，但在处理模糊或间接风险时存在显著隐患。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT和Llama等大语言模型被广泛用于心理健康等高风险情境，其在危机识别和应对方面的实际能力和安全性尚不清楚，且缺乏统一的危机分类体系和评估标准。

Method: 作者构建了六类经临床验证的心理健康危机统一分类法，整理了多样化的测试数据集，并设计了专家评估协议。随后，系统性测试了三种先进LLM在危机类型分类及生成安全、适当回复方面的表现。

Result: 结果表明，LLM对明确危机场景表现稳定，但仍存在一定比例的不当甚至有害回复，且开源模型的失败率高于商业模型。系统性弱点包括难以识别间接或模糊风险，以及倾向于机械、不贴切的默认回复。

Conclusion: 需提升LLM的危机识别和情境感知能力，加强安全防护，推动相关负责任研究。作者提出的分类法、数据集和评估体系为后续AI心理健康应用研究和安全性改进奠定了基础。

Abstract: The widespread use of chatbots powered by large language models (LLMs) such
as ChatGPT and Llama has fundamentally reshaped how people seek information and
advice across domains. Increasingly, these chatbots are being used in
high-stakes contexts, including emotional support and mental health concerns.
While LLMs can offer scalable support, their ability to safely detect and
respond to acute mental health crises remains poorly understood. Progress is
hampered by the absence of unified crisis taxonomies, robust annotated
benchmarks, and empirical evaluations grounded in clinical best practices. In
this work, we address these gaps by introducing a unified taxonomy of six
clinically-informed mental health crisis categories, curating a diverse
evaluation dataset, and establishing an expert-designed protocol for assessing
response appropriateness. We systematically benchmark three state-of-the-art
LLMs for their ability to classify crisis types and generate safe, appropriate
responses. The results reveal that while LLMs are highly consistent and
generally reliable in addressing explicit crisis disclosures, significant risks
remain. A non-negligible proportion of responses are rated as inappropriate or
harmful, with responses generated by an open-weight model exhibiting higher
failure rates than those generated by the commercial ones. We also identify
systemic weaknesses in handling indirect or ambiguous risk signals, a reliance
on formulaic and inauthentic default replies, and frequent misalignment with
user context. These findings underscore the urgent need for enhanced
safeguards, improved crisis detection, and context-aware interventions in LLM
deployments. Our taxonomy, datasets, and evaluation framework lay the
groundwork for ongoing research and responsible innovation in AI-driven mental
health support, helping to minimize harm and better protect vulnerable users.

</details>


### [521] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: 本研究评估大语言模型（LLMs）在自动化隐喻识别中的表现，并探讨不同方法的有效性。结果显示，LLMs能高效且准确地辅助隐喻分析，降低人工标注的需求。


<details>
  <summary>Details</summary>
Motivation: 隐喻广泛存在于话语中，是研究认知、情感和意识形态的重要工具。但由于隐喻的语境敏感性，大规模分析需大量人工标注，制约了相关研究的进展。研究者希望借助LLMs自动化隐喻标注，提升效率。

Method: 比较了三种基于LLM的隐喻识别方法：1）RAG，提供规则和范例让模型进行标注；2）提示工程，包括零样本、少量样本和链式思考策略；3）微调，即用人工编码文本训练模型优化表现。

Result: 最先进的封闭源LLM通过微调可达到F1中位分0.79的高准确率。与人工结果相比，大部分分歧反映已知的理论灰区和概念挑战。

Conclusion: LLMs可在一定程度上自动化隐喻识别，有助于发展和完善隐喻识别协议及理论。

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [522] [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
*Yoonna Jang,Kisu Yang,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本研究探讨了在推理阶段向大语言模型输入中人工插入无意义填充token，模型是否能利用这些token作为额外算力空间，从而提升推理表现。实验证明，尤其对小模型效果显著。


<details>
  <summary>Details</summary>
Motivation: 既往研究认为长文本输入能为模型提供更大的计算空间，但传统方法通常需要对token进行专门训练。本研究希望验证：仅在推理时插入填充token、不需额外训练，是否同样能提升模型推理能力。

Method: 1. 设计实验，系统研究不同类型、数量及位置的填充token插入策略。
2. 在训练不同规模（1.7B至32B参数）的模型上，于open-domain QA和数学任务进行测试。
3. 通过attention map分析模型是否对插入空间进行有效利用。

Result: 对于不同模型，最优的填充token类型与数目不同。将填充token插入在最终“Answer:”token前最有效。小模型获益最大，例如SmolLM2-1.7B-Instruct性能提升高达12.372百分点。attention分析显示，这些token被用于问题或者答案的进一步计算，具有实际推理意义。

Conclusion: 推理阶段简单有效地插入填充token，无需额外训练，即可大幅提升小型语言模型的表现。这种扩展计算空间的方式为模型推理提供了更大潜力。

Abstract: Chain-of-thought (CoT) rationale enables language models to use additional
task-related text for problem-solving, benefiting not only from detailed
reasoning steps but also from the expanded computational space of longer
inputs. Prior work has trained filler or special tokens to serve as additional
computation spaces. In this study, we investigate whether language models can
leverage artificially inserted sequences of filler tokens solely at inference.
We first identify effective token types, numbers, and insertion locations, then
examine at what stage of training models begin to exploit the expanded
computation space, and finally analyze dynamics within these spaces via
attention maps. Experiments on models ranging from 1.7B to 32B across
open-domain QA and math tasks show that appropriate token types and counts
vary, but placing filler tokens directly before the final 'Answer:' token is
most effective. Smaller models benefit most, up to 12.372 percentage points in
SmolLM2-1.7B-Instruct, indicating that these spaces act as additional
computational capacity rather than redundant input. Attention maps reveal that
expanded spaces often continue the original attention mechanism and sometimes
focus on questions or answer options, suggesting meaningful computation for
problem-solving.

</details>


### [523] [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
*Andrés Fernández García,Javier de la Rosa,Julio Gonzalo,Roser Morante,Enrique Amigó,Alejandro Benito-Santos,Jorge Carrillo-de-Albornoz,Víctor Fresno,Adrian Ghajari,Guillermo Marco,Laura Plaza,Eva Sánchez Salido*

Main category: cs.CL

TL;DR: 本文介绍了BOE-XSUM西班牙官方公报简明摘要数据集，并证明了在该数据集上微调的中型大语言模型在摘要任务上优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 随着信息过载问题加剧，能够对长文档进行简明摘要变得至关重要。然而，特别是在西班牙语及其法律领域，高质量简明摘要稀缺。作者希望填补这一空白。

Method: 作者建立了BOE-XSUM，一个包含3,648个西班牙官方公报文档简明摘要的数据集。每条数据都包含简洁摘要、原文及文档类型标签。随后，作者对中型LLMs进行数据集微调，并与零样本（zero-shot）通用生成模型做性能对比。

Result: 微调后的模型在生成简明摘要上的准确率远超通用未微调模型。最佳模型BERTIN GPT-J 6B（32位）准确率为41.6%，相比零样本情况下最好的DeepSeek-R1（33.5%）高出24%。

Conclusion: 为西班牙官方公报文档建立的BOE-XSUM数据集有效提升了基于大模型的摘要能力，数据集驱动的微调显著优于直接使用通用大模型。

Abstract: The ability to summarize long documents succinctly is increasingly important
in daily life due to information overload, yet there is a notable lack of such
summaries for Spanish documents in general, and in the legal domain in
particular. In this work, we present BOE-XSUM, a curated dataset comprising
3,648 concise, plain-language summaries of documents sourced from Spain's
``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each
entry in the dataset includes a short summary, the original text, and its
document type label. We evaluate the performance of medium-sized large language
models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose
generative models in a zero-shot setting. Results show that fine-tuned models
significantly outperform their non-specialized counterparts. Notably, the
best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\%
performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of
41.6\% vs.\ 33.5\%).

</details>


### [524] [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
*Rebira Jemama,Rajesh Kumar*

Main category: cs.CL

TL;DR: 本论文提出了一种快速、无需训练的作者身份验证与文体模仿分析方法，结合TF-IDF n-gram与Transformer嵌入，不依赖监督训练即可实现高精度文体匹配。基于该框架，分析了不同LLM模型及提示策略下的风格模仿能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）能够生成流畅文本，但其对特定作者独有文风的模仿能力尚不明确，因此需要一种有效评估与分析文体模仿与作者验证的方法。

Method: 作者提出了一种结合TF-IDF字元n-gram和Transformer嵌入的无监督文本对比方法，通过实证距离分布实现作者身份验证和文体模仿分析，无需参数化训练和阈值调优。

Result: 该方法在学术论文数据集上准确率达97.5%，跨领域评测中为94.5%，训练时长和内存消耗相比现有方法分别降低91.8%和59%。对于不同LLM及提示方法，few-shot比zero-shot风格匹配准确率高23.5倍，补全文本策略风格一致性高达99.9%。但高风格一致性并不等于更具人类不可预测性（困惑度远低于真人）。

Conclusion: 论文证明了文体模仿的准确性与统计可检测性是可分离的，为未来作者建模、风格检测和身份相关生成提供了新方法论和评估基线。

Abstract: Large language models (LLMs) can generate fluent text, but their ability to
replicate the distinctive style of a specific human author remains unclear. We
present a fast, training-free framework for authorship verification and style
imitation analysis. The method integrates TF-IDF character n-grams with
transformer embeddings and classifies text pairs through empirical distance
distributions, eliminating the need for supervised training or threshold
tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in
cross-domain evaluation, while reducing training time by 91.8\% and memory
usage by 59\% relative to parameter-based baselines. Using this framework, we
evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across
four prompting strategies - zero-shot, one-shot, few-shot, and text completion.
Results show that the prompting strategy has a more substantial influence on
style fidelity than model size: few-shot prompting yields up to 23.5x higher
style-matching accuracy than zero-shot, and completion prompting reaches 99.9\%
agreement with the original author's style. Crucially, high-fidelity imitation
does not imply human-like unpredictability - human essays average a perplexity
of 29.5, whereas matched LLM outputs average only 15.2. These findings
demonstrate that stylistic fidelity and statistical detectability are
separable, establishing a reproducible basis for future work in authorship
modeling, detection, and identity-conditioned generation.

</details>


### [525] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 本论文提出了MobileLLM-R1系列小参数量推理模型，证明只需高质量且较少的数据也能实现强大的推理能力，大幅降低训练需求。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为强推理能力需要极大模型和超大规模数据集。本研究挑战第二点，探索较小的数据规模下的推理能力涌现。

Method: 通过精心筛选与重采样高质量开源数据集（约2T tokens），并在此基础上训练和微调模型，设计新的数据混合比例和预训练流程，最后进行评测。

Result: MobileLLM-R1-950M（9.5亿参数）在AIME等推理基准上大幅超越同类开源模型，甚至在11.7%训练数据量下达到或超过Qwen3-0.6B。

Conclusion: 小参数量模型只要高质量数据精加工，不必使用超大语料库，也能实现卓越推理能力，为轻量推理模型的发展和应用提供了新依据。

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [526] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本论文提出了MAQuE，这是目前最大的医学问诊AI多轮问答能力评测基准，通过3000个高仿真患者模拟，系统评估AI医生的临床对话多维表现。结果表明，即使最先进的大模型在多方面仍有显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前AI医生主要提升了诊断和信息主动获取能力，但缺乏对优秀医生应具备的共情、耐心和沟通等素质的系统评价，现实患者的多样性也未被充分考虑。作者为此提出更全面公正的评测方法。

Method: 作者构建了包含3000位模拟患者（涵盖语言、认知、情绪、披露倾向多样性）的评测基准，并设计了涵盖任务成功率、问询能力、对话能力、效率及患者体验的多维评估框架，对不同大语言模型进行了系统测试与对比分析。

Result: 实验显示，主流和先进大模型在问询能力、对话表现、适应复杂患者行为等方面依然存在较大短板，诊断效果受患者行为变化显著影响，不同评价维度间也存在权衡关系。

Conclusion: AI医生虽具备一定专业能力，但在多维度综合素质提升、真实患者适应能力等方面还有长足进步空间。平衡实用性和综合表现将是AI医生走向临床应用的挑战。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [527] [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
*Kaihong Li,Huichi Zhou,Bin Ma,Fangjun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型的两阶段检测框架，有效识别推荐系统中的shilling攻击，并展示出在已知和未知攻击策略下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制主要关注用户行为，忽视了可揭示恶意意图的物品侧特征（如标题、描述），导致shilling攻击难以有效识别。

Method: 提出两阶段检测框架：第一阶段利用低成本的行为特征筛查可疑用户，第二阶段采用大语言模型审计物品侧语义一致性。同时，对轻量LLM进行强化微调，设计奖励函数，形成专用检测器SemanticShield。

Result: 在六种代表性攻击策略下，SemanticShield表现出色。在应对未见过的新型攻击方式时也具有很强的泛化能力，显著优于传统方法。

Conclusion: 结合物品侧语义特征与大语言模型，可以有效提升推荐系统抗shilling攻击能力。SemanticShield为推荐系统防御提供了一种泛用且高效的解决方案。

Abstract: Recommender systems (RS) are widely used in e-commerce for personalized
suggestions, yet their openness makes them susceptible to shilling attacks,
where adversaries inject fake behaviors to manipulate recommendations. Most
existing defenses emphasize user-side behaviors while overlooking item-side
features such as titles and descriptions that can expose malicious intent. To
address this gap, we propose a two-stage detection framework that integrates
item-side semantics via large language models (LLMs). The first stage
pre-screens suspicious users using low-cost behavioral criteria, and the second
stage employs LLM-based auditing to evaluate semantic consistency. Furthermore,
we enhance the auditing model through reinforcement fine-tuning on a
lightweight LLM with carefully designed reward functions, yielding a
specialized detector called SemanticShield. Experiments on six representative
attack strategies demonstrate the effectiveness of SemanticShield against
shilling attacks, and further evaluation on previously unseen attack methods
shows its strong generalization capability. Code is available at
https://github.com/FrankenstLee/SemanticShield.

</details>


### [528] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出一种通用的准确性预测模型（GCM），通过系统性地注入历史预测正确性信息，提升大模型置信度估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大模型在高风险或用户交互场景中，需要准确并经过校准的置信度估计。然而，目前依赖模型自身“自知力”判断答案正误的方法效果有限，难以保证预测可靠性。作者希望通过分析并改进这一过程，使置信度估计更加稳健。

Method: 作者提出将多个模型的历史预测正确性作为训练数据，训练通用准确性模型（GCM），并尝试不同的历史信息注入方式：如利用模型自我标注历史、在上下文中引入示例，以及后验校准结果。同时，系统分析哪些因素最能预测答案正确性.

Result: 实验表明，GCM模型不依赖于特定LLM自身“自省”，只需历史预测数据就能泛化多个数据集与模型族，在MMLU、TriviaQA和选择性预测任务中均有可靠表现。结果还发现，答案表述方式强烈影响预测正确性。

Conclusion: 文章证明，通过注入历史正确性信息，置信度估计成为一种易推广、与模型无关的通用技能，而非依赖某模型自知力的特定能力。这为后续提升大模型可用性和安全性提供了思路。

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [529] [Circuit Distillation](https://arxiv.org/abs/2509.25002)
*Somin Wadhwa,Silvio Amir,Byron C. Wallace*

Main category: cs.CL

TL;DR: 该论文提出了一种新的模型蒸馏方法，不仅模仿教师模型输出，还对齐师生模型内部的功能结构，实现更有效的能力迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的蒸馏方法只关注模型输出的行为模仿，忽视了模型内部计算机制的迁移，这可能限制了蒸馏效果。作者希望探索能否通过对齐内部计算机制，实现更有效的算法能力转移。

Method: 提出了一种“电路蒸馏（circuit distillation）”方法，设计了基于师生模型内部各功能部件表示对齐的损失函数，采用特定方式找到功能等效的结构，针对Llama3家族模型在实体追踪和理论心智任务上进行了实证。

Result: 实验表明，电路蒸馏方法相比传统蒸馏更有效，仅需微调模型部分参数就可以成功转移关键算法能力。

Conclusion: 电路蒸馏不仅可行，还提供了一种高效、可解释、可控地迁移教师模型能力的新途径，有望推动蒸馏技术的发展。

Abstract: Model distillation typically focuses on behavioral mimicry, where a student
model is trained to replicate a teacher's output while treating its internal
computations as a black box. In this work we propose an alternative approach:
Distilling the underlying computational mechanisms implemented by a teacher
model. Specifically, we propose circuit distillation, which introduces an
objective to align internal representations between analogous circuit
components in teacher and student models. We propose a method to match
``functionally correspondent'' circuit components and introduce a loss
reflecting similarities between the representations that these induce. We
evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks
using models from the Llama3 family. Our results demonstrate that circuit
distillation outperforms standard distillation, successfully transferring
algorithmic capabilities by adjusting only a small, targeted subset of student
model parameters. This work establishes the feasibility of transferring
mechanisms, which may in turn allow for efficient distillation of targeted
teacher capabilities via interpretable and controllable internal student
mechanisms.

</details>


### [530] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: DiDi-Instruct是一种基于离散扩散语言模型（dLLM）预训练的加速文本生成方法，在加速的同时显著提升了生成性能。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，快速高效地生成自然语言文本一直是核心目标之一。现有扩散模型虽然生成质量高，但推理速度慢，不能满足实际需求，因此论文提出新的训练方法来解决该难题。

Method: 提出DiDi-Instruct方法：从预训练的（掩码）离散扩散语言模型（dLLM）初始化，结合积分KL散度最小化，采用分组奖励归一化、中间状态匹配和奖励引导的祖先采样（RGAS）等技术，从而提升训练稳定性和推理效率。

Result: 在OpenWebText等基准上，DiDi-Instruct全面超过主流加速生成模型、GPT-2基线和标准dLLM，在推理速度提升64倍的情况下还能显著降低困惑度（最低18.4），且只损失了1%的熵，训练时间增加极少。

Conclusion: DiDi-Instruct是一种高效且有效的蒸馏式文本生成方案，实现了“眨眼之间”级别的推理速度，将公开代码和模型，推动快速高质量文本生成的发展。

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [531] [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
*Adamu Lawan,Haruna Yunusa*

Main category: cs.CL

TL;DR: 本文提出了一种面向多模态方面情感分析（MABSA）的新模型GateMABSA，通过引入门控机制和多种mLSTM结构，实现了对文本和图像信号的更有效对齐和融合，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析模型难以有效筛选图像噪音信息，且在不同模态间进行方面与观点内容对齐存在挑战，因此需要改进模型结构以提升性能。

Method: 作者提出GateMABSA模型，设计了三种不同的门控mLSTM结构：Syn-mLSTM专注于引入句法结构，Sem-mLSTM关注于方面-语义相关性，Fuse-mLSTM实现了选择性多模态融合。该方法通过合理结合上述信息，有效提升多模态情感分析的准确性和鲁棒性。

Result: 在两个Twitter基准数据集上的大量实验表明，GateMABSA在效果上明显优于多种已有的基线模型。

Conclusion: GateMABSA能够更好地应对多模态数据中的噪音和对齐问题，为多模态方面情感分析提供了一种有效的新方案。

Abstract: Aspect-based Sentiment Analysis (ABSA) has recently advanced into the
multimodal domain, where user-generated content often combines text and images.
However, existing multimodal ABSA (MABSA) models struggle to filter noisy
visual signals, and effectively align aspects with opinion-bearing content
across modalities. To address these challenges, we propose GateMABSA, a novel
gated multimodal architecture that integrates syntactic, semantic, and
fusion-aware mLSTM. Specifically, GateMABSA introduces three specialized
mLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize
aspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal
fusion. Extensive experiments on two benchmark Twitter datasets demonstrate
that GateMABSA outperforms several baselines.

</details>


### [532] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 本文提出了一种新的解释大型语言模型（LLMs）内部表示的方法，称为“高维探针（Hyperdimensional Probe）”，能够以更结构化和可解释的方式解码模型向量空间中的信息。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs具备强大能力，但其内部表征机制仍然不透明，现有的解释方法如DLA和SAE存在输出词表受限、特征难以解释等不足。亟需更有效的方法解读模型内部向量所蕴含的信息。

Method: 作者受符号表征与神经探针技术启发，提出结合Vector Symbolic Architectures（VSA）的高维探针方法，将LLM的残差流投影到可解释的概念上。通过不同于传统稀疏自编码器和传统探针的方法，该方法在输入-生成任务、关键值关联、抽象推理、问答等多场景下对模型状态进行解码评估。

Result: 实验表明，该高维探针能够跨越不同模型、嵌入维度和输入任务，稳定地提取出有意义的概念，并帮助定位LLM输出问题。

Conclusion: 本研究推进了LLM向量空间的信息解码技术，使得研究者可以从神经表征中提取更具结构性、信息性、可解释性的特征，为LLM可解释性与调试带来新的工具。

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [533] [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
*Abner Hernandez,Tomás Arias Vergara,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.CL

TL;DR: 本论文提出利用大型语言模型（LLM）作为自动语音识别（ASR）的后处理模块，采用信心度感知的提示方法，有效提升对紊乱或障碍性语音的纠错能力。


<details>
  <summary>Details</summary>
Motivation: ASR系统在处理有语言障碍或非典型语音时常出现识别错误，现有LLM后处理方法容易过度纠错或泛化性不足，因此需要更有效且稳健的纠错方法。

Method: 提出了“信心度感知提示”（confidence-informed prompting）方法，将ASR输出的逐词不确定度直接嵌入到LLM训练过程中，指导模型重点关注高不确定性区域，并与单纯转录微调和基于置信度后过滤进行对比。实验在LLaMA 3.1模型上进行微调。

Result: 在Speech Accessibility Project自发语音集上，所提方法相比LLM直接纠错使相对字错误率（WER）下降10%；在TORGO障碍性语音集上下降47%。均明显优于对照方法。

Conclusion: 信心度感知的微调方法显著提升LLM对障碍性语音ASR输出的纠错能力，减少过度纠错，增强泛化性，适合实际残障用户的语音识别应用。

Abstract: We investigate the use of large language models (LLMs) as post-processing
modules for automatic speech recognition (ASR), focusing on their ability to
perform error correction for disordered speech. In particular, we propose
confidence-informed prompting, where word-level uncertainty estimates are
embedded directly into LLM training to improve robustness and generalization
across speakers and datasets. This approach directs the model to uncertain ASR
regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare
our approach to both transcript-only fine-tuning and post hoc confidence-based
filtering. Evaluations show that our method achieves a 10% relative WER
reduction compared to naive LLM correction on the Speech Accessibility Project
spontaneous speech and a 47% reduction on TORGO, demonstrating the
effectiveness of confidence-aware fine-tuning for impaired speech.

</details>


### [534] [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
*Simeng Sun*

Main category: cs.CL

TL;DR: 本文研究了在程序轨迹生成任务中的变换器模型（Transformer），分析了其在泛化能力上的局限性以及不同修改对提升泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型在算法任务中表现优异，但其推理能力和泛化性能尚未被彻底检验，特别是在每一步推理都很简单、但需要外显表达完整推理过程（如PTG任务）时。该研究希望通过该任务来揭示Transformer的泛化特性及其背后的机制。

Method: 作者设计了程序轨迹生成这一任务，要求模型逐步输出程序执行轨迹。实验中对小型Transformer进行了多项结构变异，包括更换位置编码、替换softmax、引入混合模型和短卷积模块等，然后评估其在分布内（in-distribution）与泛化场景（各种长度与步数）下的表现。

Result: 所有模型在分布内的准确率均很高，但在面对任务复杂度变化时普遍出现系统性失败。不同结构的改进对提升泛化有一定帮助，其中部分设计显著提升了模型泛化能力。

Conclusion: Transformer及其常规变种在程序轨迹生成任务中容易过拟合，泛化能力有限；但通过设计上的调整（如位置编码、softmax等）可以部分缓解这一问题，提示未来模型设计需更关注泛化机制。

Abstract: We study Transformers on the task \emph{program trace generation} (PTG),
where models produce step-by-step execution traces for synthetic programs.
Unlike existing algorithmic problems, PTG externalizes reasoning through long
traces where each step is trivial. We train small Transformers with diverse
modifications, including alternative position encodings, softmax replacements,
hybrid model, and short convolutions. While these models achieve strong
in-distribution accuracy, they exhibit systematic failures when generalizing to
various factors (e.g., program length, trace steps), though some designs
significantly improve generalization.

</details>


### [535] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: 本论文提出了DataMind，一个可扩展的数据合成与智能体训练框架，用于构建通用型数据分析智能体，并在多个数据分析基准测试中取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的数据分析智能体依赖闭源模型与提示工程，且开源模型无法处理多格式、大规模及需长程多步推理的数据分析任务，限制了其实际应用与发展。

Method: DataMind通过细粒度任务分类与自适应递归生成多种复杂查询、知识增强式轨迹采样、结合SFT与RL损失的自适应训练目标，以及高效稳定的代码多轮推理机制，构建多样高质量数据分析智能体训练数据集（DataMind-12K），并基于此训练大模型（DataMind-7B/14B）。

Result: DataMind-14B模型在主流数据分析基准上平均得分71.16%，超过主流闭源模型DeepSeek-V3.1与GPT-5。DataMind-7B也以68.10%的成绩领先所有开源同类模型。

Conclusion: DataMind有效提升了数据分析智能体的泛化与推理能力，推动开源基础模型在知识密集型数据分析领域的进展。相关数据与模型将开放，促进社区进一步研究。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [536] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina-reranker-v3 是一款参数量为0.6B的多语言文档重排序模型，采用新的“last but not late interaction”机制，实现小模型下的高性能文档重排序。


<details>
  <summary>Details</summary>
Motivation: 现有的late interaction方法（如ColBERT）在文档重排序时需要分别编码，再进行匹配，多向量交互有限；而生成式重排序器虽然表现好但模型体积大，计算资源消耗高。因此亟需一种既高效又高性能的文档重排序新方法。

Method: 本方法提出了一种“last but not late interaction”的新机制，在同一上下文窗口中通过因果自注意力让查询与文档产生充分交互，然后再提取每篇文档最后一个token的上下文嵌入，既保留了深度交互也减小了模型规模。

Result: 该方法在BEIR基准上获得了61.94的nDCG@10，达到当前最优水平，而且模型参数量仅为同类生成式re-ranker的十分之一。

Conclusion: jina-reranker-v3在小模型结构下，通过创新交互机制实现了高效且高性能的文档重排序，为多语言信息检索系统提供了优质、低成本的解决方案。

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [537] [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
*Akio Hayakawa,Stefan Bott,Horacio Saggion*

Main category: cs.CL

TL;DR: 本文提出了一种高效的词汇简化（LS）系统框架，采用可本地部署的小型大语言模型，兼顾性能、安全性与资源约束，并提出了识别并过滤有害简化的新方法。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，传统大语言模型用于词汇简化面临隐私与资源限制，且需保障弱势群体（如残障人士）安全正确地使用LS系统。因此需要在本地可执行、资源消耗低且输出安全的简化系统。

Method: 提出一个基于小型LLM的LS框架，探索知识蒸馏（用合成数据）与in-context learning两种方法，在五种语言上自动与人工评估输出质量，以及分析输出概率与有害简化关系，并基于此设计过滤有害简化的策略。

Result: 知识蒸馏提高了自动化评测分数，但带来了更多有害简化；模型输出概率能有效指示有害简化；应用基于概率的过滤策略后，能显著减少有害简化，且保留大部分有益简化。

Conclusion: 本研究建立了面向小型LLM的高效安全LS基线，强调了性能、效率与安全的权衡，并为LS系统安全实际部署提供了新可行路径。

Abstract: Despite their strong performance, large language models (LLMs) face
challenges in real-world application of lexical simplification (LS),
particularly in privacy-sensitive and resource-constrained environments.
Moreover, since vulnerable user groups (e.g., people with disabilities) are one
of the key target groups of this technology, it is crucial to ensure the safety
and correctness of the output of LS systems. To address these issues, we
propose an efficient framework for LS systems that utilizes small LLMs
deployable in local environments. Within this framework, we explore knowledge
distillation with synthesized data and in-context learning as baselines. Our
experiments in five languages evaluate model outputs both automatically and
manually. Our manual analysis reveals that while knowledge distillation boosts
automatic metric scores, it also introduces a safety trade-off by increasing
harmful simplifications. Importantly, we find that the model's output
probability is a useful signal for detecting harmful simplifications.
Leveraging this, we propose a filtering strategy that suppresses harmful
simplifications while largely preserving beneficial ones. This work establishes
a benchmark for efficient and safe LS with small LLMs. It highlights the key
trade-offs between performance, efficiency, and safety, and demonstrates a
promising approach for safe real-world deployment.

</details>


### [538] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了首个针对深度研究智能体个性化能力的评测基准和系统评价框架，旨在促进AI个性化研究助理的发展。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体的评测多为封闭式，缺乏开放式且注重个性化的基准，难以反映实际应用中用户多样化和动态需求。

Method: 设计了Personalized Deep Research Bench，涵盖50个任务（10领域）和25种真实用户画像，形成250个真实查询；同时提出PQR评测框架，从个性化匹配、内容质量、事实可靠性三维评价系统性能。

Result: 通过在不同系统上实验证明，当前深度研究智能体在个性化深度研究方面仍有明显不足；同时PQR评测指标也有效监测了系统各方面能力。

Conclusion: 本工作为开发和评估具备真实个性化能力的下一代AI研究助手奠定了坚实基础。

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [539] [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
*Kai Sun,Yin Huang,Srishti Mehra,Mohammad Kachuee,Xilun Chen,Renjie Tao,Zhaojiang Lin,Andrea Jessee,Nirav Shah,Alex Betty,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本论文探讨了在大语言模型快速发展的背景下，知识三元组抽取在Web问答系统中的作用及价值。研究表明，尽管大模型QA表现优异，但通过抽取知识三元组并进行多任务学习，可以进一步提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在Web问答中的成功，传统的知识抽取手段（如三元组抽取）是否仍然有意义成为科研关注点。作者希望研究知识三元组抽取在新范式下的价值。

Method: 作者通过为现有Web问答基准数据集新增知识抽取标注，并对不同规模的商用及开源LLM进行评测，考察三元组抽取任务的难度及其对实际QA性能的影响。此外，探讨了将三元组抽取结果用于问答增强和多任务学习的效果。

Result: 实验结果显示，知识三元组抽取对现有LLM来说仍然具有挑战性。但在问答任务已表现较好情况下，通过知识三元组增强或多任务学习，依然可以进一步提升模型的效果。

Conclusion: 知识三元组抽取在Web问答场景中依旧有价值。针对不同规模和资源的LLM，可以通过知识增强、任务融合等方式提高模型普适性和准确性，对未来QA系统的设计具有启示意义。

Abstract: The advent of Large Language Models (LLMs) has significantly advanced
web-based Question Answering (QA) systems over semi-structured content, raising
questions about the continued utility of knowledge extraction for question
answering. This paper investigates the value of triple extraction in this new
paradigm by extending an existing benchmark with knowledge extraction
annotations and evaluating commercial and open-source LLMs of varying sizes.
Our results show that web-scale knowledge extraction remains a challenging task
for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from
knowledge extraction, through augmentation with extracted triples and
multi-task learning. These findings provide insights into the evolving role of
knowledge triple extraction in web-based QA and highlight strategies for
maximizing LLM effectiveness across different model sizes and resource
settings.

</details>


### [540] [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
*Ivan Vykopal,Antonia Karamolegkou,Jaroslav Kopčan,Qiwei Peng,Tomáš Javůrek,Michal Gregor,Marián Šimko*

Main category: cs.CL

TL;DR: 本文分析了多语种大模型在跨语种事实核查中的偏见问题，包括语言偏见和信息检索偏见，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型在高资源语言（如英语）和低资源语言之间表现差异明显，存在不公平问题。同时，信息检索系统也可能偏好某些信息，影响检索结果的均衡性。

Method: 选取六个开源多语种大模型，采用全多语种提示策略，对20种语言进行Previously Fact-Checked Claim Detection (PFCD) 任务的测试，使用AMC-16K数据集，对任务提示进行翻译，并分析模型家族、规模和提示策略对偏见的影响。针对检索偏见，使用多语种嵌入模型，统计被检索声明的频率。

Result: 发现无论是模型家族、规模还是提示策略，多语种大模型在单语和跨语任务中都存在显著的语言偏见和检索偏见，常见声明被过度检索，冷门声明被忽略，检索性能对热门声明表现“虚高”。

Conclusion: 多语种大模型在事实核查应用中存在持续性偏见，需要从模型训练、提示设计和检索策略等方面改进，以提升跨语种事实核查的公平性和准确性。

Abstract: Multilingual Large Language Models (LLMs) offer powerful capabilities for
cross-lingual fact-checking. However, these models often exhibit language bias,
performing disproportionately better on high-resource languages such as English
than on low-resource counterparts. We also present and inspect a novel concept
- retrieval bias, when information retrieval systems tend to favor certain
information over others, leaving the retrieval process skewed. In this paper,
we study language and retrieval bias in the context of Previously Fact-Checked
Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20
languages using a fully multilingual prompting strategy, leveraging the AMC-16K
dataset. By translating task prompts into each language, we uncover disparities
in monolingual and cross-lingual performance and identify key trends based on
model family, size, and prompting strategy. Our findings highlight persistent
bias in LLM behavior and offer recommendations for improving equity in
multilingual fact-checking. To investigate retrieval bias, we employed
multilingual embedding models and look into the frequency of retrieved claims.
Our analysis reveals that certain claims are retrieved disproportionately
across different posts, leading to inflated retrieval performance for popular
claims while under-representing less common ones.

</details>


### [541] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注或平行数据的输入输出配对数据自动生成方法PbT（Paired by the Teacher），可提升低资源NLG任务的数据质量与模型效果。


<details>
  <summary>Details</summary>
Motivation: 在低资源自然语言生成任务中，往往只拥有原始输入或输出，缺少成对数据，导致训练数据不足或依赖代价高且泛化弱的LLM合成样本。

Method: 提出两阶段教师-学生流程：首先用教师LLM将单边样例压缩为中间表示（IR），再训练学生模型从IR重构输入，从而可将学生生成的输入与原始输出配对，形成优质合成数据。

Result: 在五个基准任务和开集测试中，一个仅用PbT数据训练的8B学生模型优于用70B教师模型直接合成数据的模型，ROUGE-L得分仅比人工标注差1.2，且Annotation成本仅为直接合成的三分之一。人类评测也确认PbT能生成风格、信息高度匹配的配对数据。

Conclusion: PbT能高效生成配对合成数据，极大缩小训练数据差距，提升低资源NLG下小模型的效果，并解决了直接大模型合成方法的领域不匹配等问题。

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [542] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于NVFP4格式的大语言模型4比特精度训练方法，使训练效率显著提升且精度接近现有8比特方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，训练消耗的算力和能量越来越高。进一步提升训练效率（如使用更低精度的数值格式）能极大节约资源，但4比特量化会导致训练不稳定、难以收敛等问题，因此亟须解决低精度带来的挑战。

Method: 1. 采用NVFP4（4比特浮点）格式训练大模型；2. 引入随机Hadamard变换（RHT）抑制区块级异值，保证数值稳定；3. 采用二维量化方案，在前向和反向传播中均保证表达一致性；4. 使用随机舍入获得无偏梯度估计；5. 针对个别关键层采用高精度处理以提升最终模型性能。

Result: 在12B参数模型、10万亿token的数据集上完成了史上最长的4比特精度公开预训练实验，结果显示，所提出方法下训练损失和各项下游任务准确性与FP8基线相当。

Conclusion: 本文证明结合NVFP4精度和创新训练技术，可有效保障大模型极低精度训练的稳定性和性能，为推动高效节能的大模型预训练开辟了新路径。

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [543] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: EasySteer 是一个高效、可扩展的LLM行为调控框架，支持多方法融合和快速推理，推动研究成果落地。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM steering方法（推理时通过隐藏状态操控模型行为）存在计算效率低、扩展性差、功能有限的问题，影响研究和实际部署。

Method: EasySteer框架基于vLLM构建，采用模块化架构和可插拔接口，支持分析法与学习法混用，细粒度参数控制，预计算多领域调控向量，并提供交互式演示系统。通过与vLLM深度集成，实现了显著的推理速度提升。

Result: 相比现有同类框架，EasySteer在推理速度上提升5.5-11.4倍，多项实验验证其在减少过度思考、降低幻觉等实际应用中的有效性。

Conclusion: EasySteer将steering方法由研究技术转变为可生产部署能力，成为可控LLM落地的重要基础设施。

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [544] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: 该论文提出了NAIPv2，一个去偏高效的科学论文质量估算框架，利用分组对比学习和新颖信号整合方法，提升推断效率和一致性，并在大规模数据集和跨会议测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的论文质量估算方法推理成本高，而直接分数回归法存在尺度不一致的问题，限制了自动化科学评价的效率与准确性。

Method: NAIPv2利用领域-年份分组内的成对学习，减少审稿人评分不一致，通过新设计的Review Tendency Signal (RTS) 将评分与信心度概率整合。模型以分组对比训练，推理阶段却实现高效点预测。作者还构建了包含2.4万余条ICLR投稿数据、具备元数据和结构化信息的大规模NAIDv2数据集。

Result: NAIPv2在ICLR数据测试中达到78.2% AUC和0.432 Spearman，在NeurIPS未见论文上预测分数随最终决策类别递增，显示出强泛化能力。整体推理效率为线性规模，适用于大规模自动评估场景。

Conclusion: NAIPv2实现了去偏、高效和可扩展的论文质量估算，促进了未来面向科学智能的自动化评审工具发展。

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [545] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: 本文提出了Truthful Text Summarization (TTS) 框架，提升大型语言模型在摘要合成中的事实准确性与鲁棒性，无需依赖人工标注。核心方法包含拆解声明、源判断立场、基于多任务对等预测的打分和过滤机制。实验验证TTS能提升摘要真实度，同时抑制恶意数据源的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在整合多源信息生成摘要时，原有流程未能有效激励信息源提供真实内容，容易受到恶意内容影响，因而需要一个无需标注、能自我激励、提升整体事实准确性的新机制。

Method: TTS方法包括：1）将自动合成的草稿拆分为原子命题；2）收集各数据源对每个命题的立场表态；3）用适配的多任务同伴预测机制，为源打分并激励准确、信息丰富的内容；4）过滤不可靠源，最后再整合生成摘要，并通过理论保证，确保信息源的最优策略是诚实汇报。

Result: 实验结果显示，TTS显著提高了LLM生成摘要的事实准确性和鲁棒性（抗恶意内容能力），且保证语言流畅性。同时，有效平衡了信息暴露和证据支持，降低了操控风险。

Conclusion: TTS能在无须事实标注的情况下提升摘要系统的事实一致性和健壮性，是应对恶意信息干扰、提升LLM答案可信度的有效新方法。

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [546] [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
*Wenrui Bao,Zhiben Chen,Dan Xu,Yuzhang Shang*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的大语言模型并行解码方法Learn2PD，通过训练轻量级自适应过滤器模型，实现各token位置是否可并行输出的动态判别，显著提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码速度慢需逐token预测，限制推理效率。即便基于扩散的并行解码方法出现，但现有策略采用静态阈值无法根据输入灵活调整，难以兼顾不同NLP任务下的速度—质量平衡。

Method: 方法核心为Learn2PD框架——训练一个轻量级自适应过滤器模型，预测每个token预测结果是否已与最终输出一致，从而在并行解码时动态决定是否锁定该token。该过滤器通过后训练方式高效优化。同时，加入了一种End-of-Text Prediction(EoTP)机制，能自动判别解码结束以减少无谓填充解码。

Result: 在LLaDA基准测试上，Learn2PD实现了最高22.58倍的速度提升且性能无损，结合KV-Cache时更可达57.51倍提速。

Conclusion: Learn2PD为大语言模型并行解码带来了自适应、高效的新范式，既大幅提升了解码效率，也保证了输出质量，展示了在实际NLP任务中的强大应用前景。

Abstract: Autoregressive decoding in large language models (LLMs) requires
$\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting
inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token
generation through iterative denoising. However, current parallel decoding
strategies rely on fixed, input-agnostic heuristics (e.g., confidence
thresholds), which fail to adapt to input-specific characteristics, resulting
in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,
we explore a more flexible and dynamic approach to parallel decoding. We
propose Learning to Parallel Decode (Learn2PD), a framework that trains a
lightweight and adaptive filter model to predict, for each token position,
whether the current prediction matches the final output. This learned filter
approximates an oracle parallel decoding strategy that unmasks tokens only when
correctly predicted. Importantly, the filter model is learned in a
post-training manner, requiring only a small amount of computation to optimize
it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction
(EoTP) to detect decoding completion at the end of sequence, avoiding redundant
decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that
our method achieves up to 22.58$\times$ speedup without any performance drop,
and up to 57.51$\times$ when combined with KV-Cache.

</details>


### [547] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文提出了InfoAgent，一种通过自研数据合成流程和自主检索工具强化的大型语言模型智能体，并在多个基准任务上超越了现有开源深度研究智能体。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型通过外部工具扩展能力成为AI研究和应用的前沿方向，但现有方法过度依赖商业检索工具，缺乏透明和可控的环境，且难以生成足够复杂的任务评测智能体能力。因此，亟需一种开放、可扩展且能够提升智能体复杂推理能力的系统与评测方法。

Method: 本工作自研并部署了一套专属的检索基础设施，结合创新的数据合成流水线——基于实体树结构，通过子树采样和实体模糊化，系统性增加问题难度。InfoAgent模型在Qwen3-14B基础上，采用分两阶段的后训练策略：先进行冷启动下的有监督微调学习长链路搜索行为，再用强化学习提升推理和工具调用能力。

Result: InfoAgent在标准评测集BrowseComp上达到15.3%准确率，在BrowseComp-ZH上29.2%，在Xbench-DS上40.4%，明显超越了WebSailor-72B和DeepDive-32B等开源基线智能体。同时，其工具系统表现为更高效的问题求解能力（平均工具调用次数减少）。

Conclusion: InfoAgent通过创新的数据合成和自主检索工具管线，提升了深度研究智能体在复杂问题上的推理与检索能力，为开放环境和智能体能力提升提供了新途径。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


### [548] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文提出了一种名为Q2E的新方法，通过自动分解查询并利用大模型知识，提升复杂事件相关视频的检索效果，能支持多语言、跨模态输入并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和VLM在提取知识上的表现优秀，但复杂现实事件的视频检索，特别是零样本、多语种和多模态场景下，仍面临查询理解简单化、跨领域泛化弱等问题。作者希望通过深入挖掘和分解查询所隐含的事件知识，改进检索表现。

Method: 提出了Query-to-Event(Q2E)分解方法，利用LLM和VLM内嵌知识，将人类自然语言查询分解为更具辨识度的事件特征，适用于多语言、多模态（视觉和语音）输入。最终多模态知识通过基于熵的融合打分实现零样本融合检索。方法适配性强，可横跨数据集、领域和模型。

Result: 在两个多样化数据集及多项检索指标上评测，Q2E在零样本文本-视频检索任务上显著优于多种现有先进方法。融入音频信息后，检索性能进一步大幅提升。

Conclusion: Q2E提供了强大的查询理解与复杂事件识别能力，为多语言、多模态零样本文本到视频检索任务树立了新基准。音频和跨模态融合是未来提升检索效果的重要方向。已开源代码与数据以促进后续研究。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [549] [Mobile Robot Localization via Indoor Positioning System and Odometry Fusion](https://arxiv.org/abs/2509.22693)
*Muhammad Hafil Nugraha,Fauzi Abdul,Lastiko Bramantyo,Estiko Rijanto,Roni Permana Saputra,Oka Mahendra*

Main category: cs.RO

TL;DR: 本文提出了通过超声波室内定位系统（IPS）与轮式里程计数据融合，实现室内移动机器人高精度定位的方法。融合方式采用扩展卡尔曼滤波（EKF），显著提升了定位准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在室内环境下，准确定位对于移动机器人至关重要。常用的轮式里程计和超声波IPS各自存在局限性，单独使用无法满足高精度稳健定位需求，因此有必要研究两者的有效融合。

Method: 采用扩展卡尔曼滤波（EKF）将超声波IPS与轮式里程计数据进行融合，充分利用两种传感器的互补性，减小各自的系统误差。

Result: 在受控室内环境中进行大量实验，融合系统在定位精度和可靠性方面均优于单一定位系统，有效改善轨迹跟踪性能，减少了由车轮打滑和传感器噪声带来的定位误差。

Conclusion: 基于EKF的多传感器融合方法显著提升了室内移动机器人定位的准确性和鲁棒性，为未来室内机器人导航应用提供了可行的技术路径。

Abstract: Accurate localization is crucial for effectively operating mobile robots in
indoor environments. This paper presents a comprehensive approach to mobile
robot localization by integrating an ultrasound-based indoor positioning system
(IPS) with wheel odometry data via sensor fusion techniques. The fusion
methodology leverages the strengths of both IPS and wheel odometry,
compensating for the individual limitations of each method. The Extended Kalman
Filter (EKF) fusion method combines the data from the IPS sensors and the
robot's wheel odometry, providing a robust and reliable localization solution.
Extensive experiments in a controlled indoor environment reveal that the
fusion-based localization system significantly enhances accuracy and precision
compared to standalone systems. The results demonstrate significant
improvements in trajectory tracking, with the EKF-based approach reducing
errors associated with wheel slippage and sensor noise.

</details>


### [550] [Nonlinear Model Predictive Control with Single-Shooting Method for Autonomous Personal Mobility Vehicle](https://arxiv.org/abs/2509.22694)
*Rakha Rahmadani Pratama,Catur Hilman A. H. B. Baskoro,Joga Dharma Setiawan,Dyah Kusuma Dewi,P Paryanto,Mochammad Ariyanto,Roni Permana Saputra*

Main category: cs.RO

TL;DR: 该论文提出了一种基于非线性模型预测控制（NMPC）的自动驾驶单人电动车（SEATER）控制方法，通过仿真实验证明该方法能有效实现目标跟踪和障碍物避让。


<details>
  <summary>Details</summary>
Motivation: 随着个人自动驾驶交通工具的需求增长，提高其控制精准性与安全性成为亟需解决的问题。现有控制方法面对非完整约束与实时多目标约束难以兼顾，因此需开发更高效、可靠的方法。

Method: 采用非线性模型预测控制（NMPC），结合single-shooting算法，将最优控制问题转化为非线性规划问题。以差速驱动、非完整约束车辆为对象，利用里程计反馈进行自主导航，并在ROS与Gazebo仿真环境中进行算法实现与验证。

Result: 仿真数据显示，在无障碍和静态障碍场景下，所提NMPC法能引导车辆精准到达目标点且满足约束条件，如避障等。

Conclusion: 基于single-shooting的NMPC方法在自动驾驶个人交通工具控制中具有出色的鲁棒性与实时性，能有效控制车辆实现任务目标，验证了其在该领域的实用价值。

Abstract: This paper introduces a proposed control method for autonomous personal
mobility vehicles, specifically the Single-passenger Electric Autonomous
Transporter (SEATER), using Nonlinear Model Predictive Control (NMPC). The
proposed method leverages a single-shooting approach to solve the optimal
control problem (OCP) via non-linear programming (NLP). The proposed NMPC is
implemented to a non-holonomic vehicle with a differential drive system, using
odometry data as localization feedback to guide the vehicle towards its target
pose while achieving objectives and adhering to constraints, such as obstacle
avoidance. To evaluate the performance of the proposed method, a number of
simulations have been conducted in both obstacle-free and static obstacle
environments. The SEATER model and testing environment have been developed in
the Gazebo Simulation and the NMPC are implemented within the Robot Operating
System (ROS) framework. The simulation results demonstrate that the NMPC-based
approach successfully controls the vehicle to reach the desired target location
while satisfying the imposed constraints. Furthermore, this study highlights
the robustness and real-time effectiveness of NMPC with a single-shooting
approach for autonomous vehicle control in the evaluated scenarios.

</details>


### [551] [ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows](https://arxiv.org/abs/2509.22695)
*Zhitao Wang,Yanke Wang,Jiangtao Wen,Roberto Horowitz,Yuxing Han*

Main category: cs.RO

TL;DR: 提出了一种高效、只需一步推理即可完成任务的SE(3)等变扩散模型（ReSeFlow），用于提升机器人在非结构化环境下的操作策略生成效果，同时大幅减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)等变扩散模型能支持高效的数据学习和环境泛化，但在实际推理时计算成本很高，难以满足机器人对实时性的需求。因此，如何既保证等变性又加快推理速度是一大难题。

Method: 受rectified flows推理高效性的启发，将其思想引入SE(3)等变扩散模型，设计出ReSeFlow模型，兼顾轨迹级策略学习的旋转与平移对称性。新方法采用SE(3)等变网络，确保模型对刚体运动的泛化能力，同时实现一步高效推理，保持速度和准确性的平衡。

Result: 在仿真基准实验中，ReSeFlow在只需一步推理的情况下，即可获得比传统100步推理方法更低的测地距离和更高的任务完成准确率。比如在painting task上减少了48.5%的错误率，在rotating triangle task上减少了21.9%。

Conclusion: ReSeFlow结合了SE(3)等变性和rectified flow的优势，大幅提升了生成式策略学习模型的数据利用率和推理效率，为机器人实际应用（尤其是需要高效实时性的场景）提供了有效的新工具。

Abstract: Robotic manipulation in unstructured environments requires the generation of
robust and long-horizon trajectory-level policy with conditions of perceptual
observations and benefits from the advantages of SE(3)-equivariant diffusion
models that are data-efficient. However, these models suffer from the inference
time costs. Inspired by the inference efficiency of rectified flows, we
introduce the rectification to the SE(3)-diffusion models and propose the
ReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing
fast, geodesic-consistent, least-computational policy generation. Crucially,
both components employ SE(3)-equivariant networks to preserve rotational and
translational symmetry, enabling robust generalization under rigid-body
motions. With the verification on the simulated benchmarks, we find that the
proposed ReSeFlow with only one inference step can achieve better performance
with lower geodesic distance than the baseline methods, achieving up to a 48.5%
error reduction on the painting task and a 21.9% reduction on the rotating
triangle task compared to the baseline's 100-step inference. This method takes
advantages of both SE(3) equivariance and rectified flow and puts it forward
for the real-world application of generative policy learning models with the
data and inference efficiency.

</details>


### [552] [Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments](https://arxiv.org/abs/2509.22698)
*Hailong Zhang,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.RO

TL;DR: 提出了一个名为MASTAVN的多智能体音视频导航框架，实现了两个智能体在真实感3D环境中协作定位和导航至音频目标，并在多个仿真环境中显著优于单智能体与非协作基线。


<details>
  <summary>Details</summary>
Motivation: 现有音视频导航研究多聚焦单智能体，难以应对需要快速多智能体协作的复杂3D和时间敏感场景（如紧急救援），因此需开发能有效协作的多智能体导航方法。

Method: MASTAVN框架通过引入跨智能体通信协议与联合音视频融合机制，实现多智能体在空间推理与时间同步上的提升。系统在高真实感的Replica和Matterport3D仿真器中进行测试评估。

Result: MASTAVN在多项评估指标上优于单智能体及无协作基线，特别是在任务完成时间和导航成功率上取得显著提升。

Conclusion: 空间—时间协作对多智能体系统至关重要。MASTAVN在紧急场景下的有效性为可扩展多智能体智能系统在复杂3D环境中的发展奠定了基础。

Abstract: Intelligent agents often require collaborative strategies to achieve complex
tasks beyond individual capabilities in real-world scenarios. While existing
audio-visual navigation (AVN) research mainly focuses on single-agent systems,
their limitations emerge in dynamic 3D environments where rapid multi-agent
coordination is critical, especially for time-sensitive applications like
emergency response. This paper introduces MASTAVN (Multi-Agent Scalable
Transformer Audio-Visual Navigation), a scalable framework enabling two agents
to collaboratively localize and navigate toward an audio target in shared 3D
environments. By integrating cross-agent communication protocols and joint
audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal
synchronization. Through rigorous evaluation in photorealistic 3D simulators
(Replica and Matterport3D), MASTAVN achieves significant reductions in task
completion time and notable improvements in navigation success rates compared
to single-agent and non-collaborative baselines. This highlights the essential
role of spatiotemporal coordination in multi-agent systems. Our findings
validate MASTAVN's effectiveness in time-sensitive emergency scenarios and
establish a paradigm for advancing scalable multi-agent embodied intelligence
in complex 3D environments.

</details>


### [553] [Large Language Models for 3D IC Space Planning](https://arxiv.org/abs/2509.22716)
*Hung-Ying Chu,Guan-Wei Chen,Shao-Yu Wei,Yu-Cheng Lin*

Main category: cs.RO

TL;DR: 本研究提出利用大型语言模型（LLM）进行3D集成电路空间规划，通过后序切片树表示实现高效且合法的布局，显著减少死区并具备良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着3D IC设计复杂度提升，传统空间规划难以兼顾布局质量和空间利用率。因此，探索AI方法辅助空间规划成为亟需突破的方向。

Method: 采用开源LLM，基于大规模合成数据集微调模型，并用MCNC参考基准进行测试。利用后序切片树表征空间规划，确保布局合法性和高效性。

Result: 在多项测试中，该框架实现了高效运行和死区最小化，部分实例甚至实现零死区。对ami33和ami49等实际案例展现良好泛化，对大型和不规则实例仍具挑战。方法对物流、3D物体摆放等领域也有潜力。

Conclusion: 基于LLM的空间规划可作为传统EDA方法的数据驱动补充，为可扩展的3D布局生成带来新见解，在实际应用和跨领域任务中具有广阔前景。

Abstract: Three-dimensional integrated circuits (3D ICs) have emerged as a promising
solution to the scaling limits of two-dimensional designs, offering higher
integration density, shorter interconnects, and improved performance. As design
complexity increases, effective space planning becomes essential to reduce dead
space and ensure layout quality. This study investigates the use of large
language models (LLMs) for 3D IC space planning through a post-order slicing
tree representation, which guarantees legal space plans while aiming to
minimize dead space. Open-source LLMs were fine-tuned on large-scale synthetic
datasets and further evaluated on MCNC-derived 3D benchmarks. Experimental
results indicate that the proposed framework achieves a favorable balance
between runtime efficiency, legality, and dead-space reduction, with
zero-dead-space layouts obtained in a significant portion of test cases under
practical runtime budgets. Beyond synthetic benchmarks, the method generalizes
to MCNC cases such as ami33 and ami49, though larger and irregular instances
remain challenging. The approach also shows potential for cross-domain
applications, including logistics and 3D object placement, where spatial
efficiency is critical. Overall, the results suggest that LLM-based space
planning can serve as a data-driven complement to traditional electronic design
automation (EDA) methods, providing new insights for scalable 3D layout
generation.

</details>


### [554] [Self-driving cars: Are we there yet?](https://arxiv.org/abs/2509.22754)
*Merve Atasever,Zhuochen Liu,Qingpei Li,Akshay Hitendra Shah,Hans Walker,Jyotirmoy V. Deshmukh,Rahul Jain*

Main category: cs.RO

TL;DR: 本论文对CARLA、nuPlan和Waymo这三大自动驾驶运动规划算法排行榜上的方法进行了全面对比分析，找出当前主流方法的优劣和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要车辆在复杂环境下感知、预测和规划安全高效的行为。为推动该领域发展，各机构建立了标准化数据集和评测平台，但不同平台间方法表现缺乏统一对比。因此，亟需系统性分析主要榜单上的规划方法。

Method: 采用CARLA leaderboard v2.0作为统一评测平台，将三大榜单（CARLA、nuPlan、Waymo）的代表性运动规划模型进行兼容性改写和统一测试，系统对比各方法在不同驾驶场景下的表现。

Result: 对三大平台主流运动规划方法的优缺点、当前发展趋势和存在的挑战进行了梳理和总结。通过统一评测标准，对比方法间的表现差异，并指出了具体改进空间。

Conclusion: 本研究明确了当前运动规划研究的主流方向和挑战，为未来方法改进和领域发展提出了宝贵建议和潜在研究方向。

Abstract: Autonomous driving remains a highly active research domain that seeks to
enable vehicles to perceive dynamic environments, predict the future
trajectories of traffic agents such as vehicles, pedestrians, and cyclists and
plan safe and efficient future motions. To advance the field, several
competitive platforms and benchmarks have been established to provide
standardized datasets and evaluation protocols. Among these, leaderboards by
the CARLA organization and nuPlan and the Waymo Open Dataset have become
leading benchmarks for assessing motion planning algorithms. Each offers a
unique dataset and challenging planning problems spanning a wide range of
driving scenarios and conditions. In this study, we present a comprehensive
comparative analysis of the motion planning methods featured on these three
leaderboards. To ensure a fair and unified evaluation, we adopt CARLA
leaderboard v2.0 as our common evaluation platform and modify the selected
models for compatibility. By highlighting the strengths and weaknesses of
current approaches, we identify prevailing trends, common challenges, and
suggest potential directions for advancing motion planning research.

</details>


### [555] [Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving](https://arxiv.org/abs/2509.22756)
*Shiyi Liang,Xinyuan Chang,Changjie Wu,Huiyuan Yan,Yifan Bai,Xinran Liu,Hang Zhang,Yujian Yuan,Shuang Zeng,Mu Xu,Xing Wei*

Main category: cs.RO

TL;DR: PAMR提出了一种新框架，实现了对车道向量与交通规则的联合自回归建图，并首次关注交通规则在长时驾驶过程中的持续有效性。实验显示，其在持续生成高质量地图和规则表达方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶地图建模方法要么只关注几何元素，要么把交通规则当做临时属性，难以保证规则在长时段驾驶中的持续有效性。因此亟需一种能持续表达交通规则且高效联合构建地图与规则的方法。

Method: 提出PAMR框架，通过自回归方式联合构建车道与交通规则。具体引入“地图-规则协同构建”(Map-Rule Co-Construction)来分段处理驾驶时序数据，以及“地图-规则缓存”(Map-Rule Cache)来保证规则在连续片段间的一致性。并设计了新的评测基准MapDRv2。

Result: 大量实验表明，PAMR在联合向量-规则映射任务中优于现有方法，且在长时驾驶中能维持流畅、一致的规则表达。

Conclusion: PAMR显著提升了地图构建及交通规则表达的持续性和准确性，为安全自动驾驶提供了更强基础。

Abstract: Safe autonomous driving requires both accurate HD map construction and
persistent awareness of traffic rules, even when their associated signs are no
longer visible. However, existing methods either focus solely on geometric
elements or treat rules as temporary classifications, failing to capture their
persistent effectiveness across extended driving sequences. In this paper, we
present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel
framework that performs autoregressive co-construction of lane vectors and
traffic rules from visual observations. Our approach introduces two key
mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal
segments, and Map-Rule Cache for maintaining rule consistency across these
segments. To properly evaluate continuous and consistent map generation, we
develop MapDRv2, featuring improved lane geometry annotations. Extensive
experiments demonstrate that PAMR achieves superior performance in joint
vector-rule mapping tasks, while maintaining persistent rule effectiveness
throughout extended driving sequences.

</details>


### [556] [Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem](https://arxiv.org/abs/2509.22801)
*Huajing Zhao,Brian Flynn,Adam Norton,Holly Yanco*

Main category: cs.RO

TL;DR: COMPARE 生态系统旨在通过制定组件级和流水线级的标准与指南，提升开源机器人操作软件的兼容性与基准测试能力。


<details>
  <summary>Details</summary>
Motivation: 目前开源机器人操作产品存在组件间兼容性不足、基准测试缺乏统一标准等问题。为此，COMPARE 计划通过标准化，促进组件化开发和系统集成。

Method: （1）建立开源产品库，梳理各模块的共同特征；（2）调研现有模块化流水线，提炼最佳实践；（3）开发符合新标准和指南的新型模块化流水线。

Result: 已建立初步的开源产品库并整理了基本模块特征，总结了部分最佳实践，通过实验开发推进新的模块化流水线。

Conclusion: 标准化和指南的制定，有助于推动机器人操作领域的模块与系统高度兼容、易于集成和优化评测，促进领域可持续发展。

Abstract: The COMPARE Ecosystem aims to improve the compatibility and benchmarking of
open-source products for robot manipulation through a series of activities. One
such activity is the development of standards and guidelines to specify
modularization practices at the component-level for individual modules (e.g.,
perception, grasp planning, motion planning) and integrations of components
that form robot manipulation capabilities at the pipeline-level. This paper
briefly reviews our work-in-progress to date to (1) build repositories of
open-source products to identify common characteristics of each component in
the pipeline, (2) investigate existing modular pipelines to glean best
practices, and (3) develop new modular pipelines that advance prior work while
abiding by the proposed standards and guidelines.

</details>


### [557] [Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots](https://arxiv.org/abs/2509.22815)
*Ruturaj Sambhus,Muneeb Ahmad,Basit Muhammad Imran,Sujith Vijayan,Dylan P. Losey,Kaveh Akbari Hamed*

Main category: cs.RO

TL;DR: 本文提出了一种自适应非线性模型预测控制(ANMPC)框架，使四足机器人在遥操作和复杂环境下实现安全高效的障碍物规避和人机协作。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，四足机器人需要与人类远程操作者协同工作，穿越复杂障碍，常规的共享控制方法因未考虑四足步态动力学和人类行为不确定性而难以保障安全性。因此，需要能动态自适应并确保安全的人机协作控制框架。

Method: 该方法采用包含固定权重的人机控制融合，同时通过噪声理性Boltzmann模型对遥控器输入进行建模，并通过投影梯度下降(PGD)实时学习人类意图参数。安全性通过嵌入控制障碍函数(CBF)到高效NMPC内保障。整体架构为分层结构：高层CBF-ANMPC（10Hz）融合人机速度指令，中层NMPC（60Hz）追踪约简动力学，低层非线性全身控制器（500Hz）跟踪实际运动，全流程确保安全与实时性。

Result: 在Unitree Go2四足机器人上，通过大量仿真、实物实验和用户调查，验证了所提框架能实时实现障碍物规避、人类意图在线学习及安全高效人机协作。

Conclusion: 该方法有效提升了遥操作下四足机器人及人类的协作安全与效率，为未来复杂环境下的自主与远程机器人协作提供可靠技术基础。

Abstract: Ensuring safe and effective collaboration between humans and autonomous
legged robots is a fundamental challenge in shared autonomy, particularly for
teleoperated systems navigating cluttered environments. Conventional
shared-control approaches often rely on fixed blending strategies that fail to
capture the dynamics of legged locomotion and may compromise safety. This paper
presents a teleoperator-aware, safety-critical, adaptive nonlinear model
predictive control (ANMPC) framework for shared autonomy of quadrupedal robots
in obstacle-avoidance tasks. The framework employs a fixed arbitration weight
between human and robot actions but enhances this scheme by modeling the human
input with a noisily rational Boltzmann model, whose parameters are adapted
online using a projected gradient descent (PGD) law from observed joystick
commands. Safety is enforced through control barrier function (CBF) constraints
integrated into a computationally efficient NMPC, ensuring forward invariance
of safe sets despite uncertainty in human behavior. The control architecture is
hierarchical: a high-level CBF-based ANMPC (10 Hz) generates blended
human-robot velocity references, a mid-level dynamics-aware NMPC (60 Hz)
enforces reduced-order single rigid body (SRB) dynamics to track these
references, and a low-level nonlinear whole-body controller (500 Hz) imposes
the full-order dynamics via quadratic programming to track the mid-level
trajectories. Extensive numerical and hardware experiments, together with a
user study, on a Unitree Go2 quadrupedal robot validate the framework,
demonstrating real-time obstacle avoidance, online learning of human intent
parameters, and safe teleoperator collaboration.

</details>


### [558] [Parameter Identification of a Differentiable Human Arm Musculoskeletal Model without Deep Muscle EMG Reconstruction](https://arxiv.org/abs/2509.22825)
*Philip Sanderink,Yingfan Zhou,Shuzhen Luo,Cheng Fang*

Main category: cs.RO

TL;DR: 本论文提出了一种无需重建深层肌肉EMG的参数识别新方法，实现了对个体人类手臂骨骼肌肉模型参数的高精度识别。


<details>
  <summary>Details</summary>
Motivation: 目前基于肌电图（EMG）的个性化肌骨模型参数识别方法因难以侵入性获取深层肌肉EMG而受限，现有的深层肌肉EMG重建策略也因依赖模型假设而可靠性和鲁棒性不足。因此亟需一种无需获取深层EMG就能实现参数识别的方法。

Method: 作者提出了一种在可微优化框架中，通过仅利用深层肌肉力的最小二乘解，直接对骨骼与浅表肌肉参数进行识别的新方法，无需重建深层EMG信号，仅用浅表EMG数据即可。

Result: 大量仿真对比实验结果显示，该方法在不用全部肌肉EMG的情况下，识别精度可与现有全部依赖EMG信号的类似方法相当。

Conclusion: 该工作证明了仅用浅表EMG数据、结合最小二乘与可微优化，也能高效准确地实现个体化肌骨模型参数识别，有助于提升物理人机协作等领域个性化设备的安全性与可靠性。

Abstract: Accurate parameter identification of a subject-specific human musculoskeletal
model is crucial to the development of safe and reliable physically
collaborative robotic systems, for instance, assistive exoskeletons.
Electromyography (EMG)-based parameter identification methods have demonstrated
promising performance for personalized musculoskeletal modeling, whereas their
applicability is limited by the difficulty of measuring deep muscle EMGs
invasively. Although several strategies have been proposed to reconstruct deep
muscle EMGs or activations for parameter identification, their reliability and
robustness are limited by assumptions about the deep muscle behavior. In this
work, we proposed an approach to simultaneously identify the bone and
superficial muscle parameters of a human arm musculoskeletal model without
reconstructing the deep muscle EMGs. This is achieved by only using the
least-squares solution of the deep muscle forces to calculate a loss gradient
with respect to the model parameters for identifying them in a framework of
differentiable optimization. The results of extensive comparative simulations
manifested that our proposed method can achieve comparable estimation accuracy
compared to a similar method, but with all the muscle EMGs available.

</details>


### [559] [Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking](https://arxiv.org/abs/2509.22828)
*Arman Barghi,Hamed Hosseini,Seraj Ghasemi,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 该论文针对机器人在复杂桌面环境下物品重排列问题，引入了一种新的动态缓冲区规划方法，有效提升了搬运效率和空间利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的规划方法在处理密集物品时效率低，尤其是在缓冲区有限的情况下容易因物品位置限制增加复杂度。静态堆叠虽然扩展了缓冲能力，但堆底一旦支撑起其他物品便无法移动，进一步限制了优化空间。

Method: 提出了动态缓冲区（Dynamic Buffer）这一新型规划单元，机器人可临时构建可移动堆，将多个物品作为整体搬运，灵感来自人类分组搬运策略。与只用空桌面或静态堆叠相比，动态缓冲能灵活组合，减少搬运次数和路径长度。

Result: 实验表明，在密集桌面环境中引入动态缓冲，机器人搬运路径减少了11.89%；在空间充裕的大场景下，移动型机械臂的搬运距离也减少了5.69%。方案在Delta并联机器人及双指夹持器上验证了实用性。

Conclusion: 动态缓冲区被确立为一种有前景的、可提升机器人重排列效率和鲁棒性的核心规划单元。

Abstract: Rearranging objects in cluttered tabletop environments remains a
long-standing challenge in robotics. Classical planners often generate
inefficient, high-cost plans by shuffling objects individually and using fixed
buffers--temporary spaces such as empty table regions or static stacks--to
resolve conflicts. When only free table locations are used as buffers, dense
scenes become inefficient, since placing an object can restrict others from
reaching their goals and complicate planning. Allowing stacking provides extra
buffer capacity, but conventional stacking is static: once an object supports
another, the base cannot be moved, which limits efficiency. To overcome these
issues, a novel planning primitive called the Dynamic Buffer is introduced.
Inspired by human grouping strategies, it enables robots to form temporary,
movable stacks that can be transported as a unit. This improves both
feasibility and efficiency in dense layouts, and it also reduces travel in
large-scale settings where space is abundant. Compared with a state-of-the-art
rearrangement planner, the approach reduces manipulator travel cost by 11.89%
in dense scenarios with a stationary robot and by 5.69% in large, low-density
settings with a mobile manipulator. Practicality is validated through
experiments on a Delta parallel robot with a two-finger gripper. These findings
establish dynamic buffering as a key primitive for cost-efficient and robust
rearrangement planning.

</details>


### [560] [Empart: Interactive Convex Decomposition for Converting Meshes to Parts](https://arxiv.org/abs/2509.22847)
*Brandon Vu,Shameek Ganguly,Pushkar Joshi*

Main category: cs.RO

TL;DR: 本文提出了一种允许用户指定三维网格不同区域简化容差的交互式工具Empart，用于在不影响关键区域精度的前提下显著提升物理仿真性能。


<details>
  <summary>Details</summary>
Motivation: 传统的网格简化方法对整个模型采用统一的误差容差，导致非关键区域有过多细节或关键区域简化过度，无法兼顾仿真效率和关键交互部位的精度。尤其在机器人抓取等任务中，对接触表面的几何保真有较高需求，而非关键部分可容忍较粗的简化。

Method: Empart工具允许用户选择网格的不同区域并设定不同的简化容差。其核心为基于现有凸分解算法、结合新提出的并行化框架，有效支持区域约束，同时提供可视化反馈，帮助用户迭代优化网格分解。

Result: 在固定误差阈值下，与现有方法V-HACD相比，Empart明显减少了所需凸块数量，显著加快了碰撞仿真速度。在典型机器人搬运任务中，基于Empart生成的碰撞网格使整体仿真时间缩短了69%。

Conclusion: 交互式、区域自适应的网格简化工具能有效提升机器人仿真性能，并改善精度与效率的平衡，对于需求差异化区域精度的应用（如机器人抓取）尤为有价值。

Abstract: Simplifying complex 3D meshes is a crucial step in robotics applications to
enable efficient motion planning and physics simulation. Common methods, such
as approximate convex decomposition, represent a mesh as a collection of simple
parts, which are computationally inexpensive to simulate. However, existing
approaches apply a uniform error tolerance across the entire mesh, which can
result in a sub-optimal trade-off between accuracy and performance. For
instance, a robot grasping an object needs high-fidelity geometry in the
vicinity of the contact surfaces but can tolerate a coarser simplification
elsewhere. A uniform tolerance can lead to excessive detail in non-critical
areas or insufficient detail where it's needed most.
  To address this limitation, we introduce Empart, an interactive tool that
allows users to specify different simplification tolerances for selected
regions of a mesh. Our method leverages existing convex decomposition
algorithms as a sub-routine but uses a novel, parallelized framework to handle
region-specific constraints efficiently. Empart provides a user-friendly
interface with visual feedback on approximation error and simulation
performance, enabling designers to iteratively refine their decomposition. We
demonstrate that our approach significantly reduces the number of convex parts
compared to a state-of-the-art method (V-HACD) at a fixed error threshold,
leading to substantial speedups in simulation performance. For a robotic
pick-and-place task, Empart-generated collision meshes reduced the overall
simulation time by 69% compared to a uniform decomposition, highlighting the
value of interactive, region-specific simplification for performant robotics
applications.

</details>


### [561] [Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments](https://arxiv.org/abs/2509.22883)
*Kaleb Ben Naveed,Haejoon Lee,Dimitra Panagou*

Main category: cs.RO

TL;DR: 本论文提出了一种两阶段多机器人场估计框架，针对时空动态各异的非均匀环境，改进了高斯过程方法对区域和时间变化的适应性，并通过实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 目前高斯过程在估计空间-时间场时常假设全局一致的空间和时间相关尺度，忽略了不同区域和时间段的变异性，导致不确定性估计失真。因此，需要能自动适应区域和时间变化的建模和采样方法。

Method: 提出了两阶段方法：第一阶段用变异函数(VARIogram)驱动的路径规划器学习每个区域的空间相关尺度；第二阶段基于当前的不确定性合理分配多机器人采样，动态更新时间相关尺度，并用自定义信息量度衡量不确定性。

Result: 方法在多种环境下进行实验，验证空间相关尺度估计收敛性，分析动态分配策略的表现，并给出相较理想分配（oracle）的动态遗憾（dynamic regret）界限。

Conclusion: 两阶段框架有效提升了多机器人在非均匀时空环境中的场估计精度和不确定性评估能力，优于只用全局尺度或静态分配的传统方法。

Abstract: Autonomous robots are increasingly deployed to estimate spatiotemporal fields
(e.g., wind, temperature, gas concentration) that vary across space and time.
We consider environments divided into non-overlapping regions with distinct
spatial and temporal dynamics, termed non-uniform spatiotemporal environments.
Gaussian Processes (GPs) can be used to estimate these fields. The GP model
depends on a kernel that encodes how the field co-varies in space and time,
with its spatial and temporal lengthscales defining the correlation. Hence,
when these lengthscales are incorrect or do not correspond to the actual field,
the estimates of uncertainty can be highly inaccurate. Existing GP methods
often assume one global lengthscale or update only periodically; some allow
spatial variation but ignore temporal changes. To address these limitations, we
propose a two-phase framework for multi-robot field estimation. Phase 1 uses a
variogram-driven planner to learn region-specific spatial lengthscales. Phase 2
employs an allocation strategy that reassigns robots based on the current
uncertainty, and updates sampling as temporal lengthscales are refined. For
encoding uncertainty, we utilize clarity, an information metric from our
earlier work. We evaluate the proposed method across diverse environments and
provide convergence analysis for spatial lengthscale estimation, along with
dynamic regret bounds quantifying the gap to the oracle's allocation sequence.

</details>


### [562] [Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM](https://arxiv.org/abs/2509.22910)
*Yanwei Du,Jing-Chen Peng,Patricio A. Vela*

Main category: cs.RO

TL;DR: 论文提出了一种名为Good Weights（GW）的算法，通过自适应权重融合死算（DR）和视觉SLAM，实现更稳定、准确的位姿估计，在视觉信息弱时提升SLAM鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉SLAM在纹理稀少或光照差的环境下容易定位失败，而机器人一般还具备短时准确但长时漂移明显的死算（DR）传感器。利用两者互补特性，是提升SLAM鲁棒性的动力。

Method: 文章提出Good Weights算法，在综合SLAM系统内自适应调整视觉信息与死算信息的权重：当视觉跟踪不可靠时提升死算影响力，视觉特征强时则降低死算作用。该方法要求对SLAM系统所有模块适配以融合DR信息。

Result: 在采集的数据集和实地部署环境中进行实验，验证了Good Weights方法显著提升了视觉SLAM的准确性和鲁棒性。

Conclusion: Good Weights算法为基于视觉的移动机器人导航提供了一种实用且高效的解决方案，增强了视觉SLAM在真实环境下的表现和稳定性。

Abstract: Given that Visual SLAM relies on appearance cues for localization and scene
understanding, texture-less or visually degraded environments (e.g., plain
walls or low lighting) lead to poor pose estimation and track loss. However,
robots are typically equipped with sensors that provide some form of dead
reckoning odometry with reasonable short-time performance but unreliable
long-time performance. The Good Weights (GW) algorithm described here provides
a framework to adaptively integrate dead reckoning (DR) with passive visual
SLAM for continuous and accurate frame-level pose estimation. Importantly, it
describes how all modules in a comprehensive SLAM system must be modified to
incorporate DR into its design. Adaptive weighting increases DR influence when
visual tracking is unreliable and reduces when visual feature information is
strong, maintaining pose track without overreliance on DR. Good Weights yields
a practical solution for mobile navigation that improves visual SLAM
performance and robustness. Experiments on collected datasets and in real-world
deployment demonstrate the benefits of Good Weights.

</details>


### [563] [ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality](https://arxiv.org/abs/2509.22914)
*Rohan Walia,Yusheng Wang,Ralf Römer,Masahiro Nishio,Angela P. Schoellig,Jun Ota*

Main category: cs.RO

TL;DR: 本论文提出了ARMimic，一种通过消费者级XR头显和固定工作台摄像头即可进行无机器人、可扩展的数据采集的新框架，简化了机器人模仿学习演示的流程，并提升了演示效率和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人模仿学习演示方法如动力教学和远程操作非常繁琐，对硬件依赖高且打断日常工作流程。近期利用XR头显进行被动观测有一定前景，但相关方法通常需要额外硬件、复杂的标定或受限的录制环境，极大限制了可扩展性与易用性。

Method: 提出ARMimic框架，仅用消费级XR头显和静态摄像头采集演示，集成了第一视角手部跟踪、AR机器人虚拟叠加及实时深度感知，确保演示过程安全且具运动学可行性。在模仿学习管线中，将人与机器人（虚拟）轨迹统一处理，使策略能跨不同主体与环境泛化。

Result: 在两个机械臂操作任务（包括难度较高的长时序叠碗任务）中，ARMimic将演示时间减少50%，任务成功率比最先进的ACT方法提升11%。

Conclusion: ARMimic实现了安全、无缝并适用于实际环境的数据采集，降低了门槛，提升了效率和任务表现，为机器人大规模学习在真实场景下的应用带来了重要推动。

Abstract: Imitation learning is a powerful paradigm for robot skill acquisition, yet
conventional demonstration methods--such as kinesthetic teaching and
teleoperation--are cumbersome, hardware-heavy, and disruptive to workflows.
Recently, passive observation using extended reality (XR) headsets has shown
promise for egocentric demonstration collection, yet current approaches require
additional hardware, complex calibration, or constrained recording conditions
that limit scalability and usability. We present ARMimic, a novel framework
that overcomes these limitations with a lightweight and hardware-minimal setup
for scalable, robot-free data collection using only a consumer XR headset and a
stationary workplace camera. ARMimic integrates egocentric hand tracking,
augmented reality (AR) robot overlays, and real-time depth sensing to ensure
collision-aware, kinematically feasible demonstrations. A unified imitation
learning pipeline is at the core of our method, treating both human and virtual
robot trajectories as interchangeable, which enables policies that generalize
across different embodiments and environments. We validate ARMimic on two
manipulation tasks, including challenging long-horizon bowl stacking. In our
experiments, ARMimic reduces demonstration time by 50% compared to
teleoperation and improves task success by 11% over ACT, a state-of-the-art
baseline trained on teleoperated data. Our results demonstrate that ARMimic
enables safe, seamless, and in-the-wild data collection, offering great
potential for scalable robot learning in diverse real-world settings.

</details>


### [564] [DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent Autonomous Racing Overtakes](https://arxiv.org/abs/2509.22937)
*Trent Weiss,Amar Kulkarni,Madhur Behl*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩展型微分贝叶斯滤波（DBF-MA）方法的轨迹合成方式，用于自动驾驶赛车中的安全超车。方法以复合贝塞尔曲线为基础，通过贝叶斯推断优化轨迹，无需常见的简化假设，实验证明超车成功率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车中，如何在复杂赛道环境下进行安全有效的超车是一大技术难题。现有优化和图搜索方法多依赖过度简化的碰撞规避与动力学约束假设，难以应对真实环境的复杂情况，因此需要更准确且高效的轨迹规划方法。

Method: 作者将轨迹合成问题建模为对复合贝塞尔曲线空间的贝叶斯推断，通过非导数优化流程，避免了对车辆外形的球形近似、约束线性化和碰撞规避上限的简化设定。这一新方法基于扩展微分贝叶斯滤波（DBF-MA）。

Result: 该方法在闭环仿真测试中实现了87%的超车成功率，优于现有自动超车方案。

Conclusion: DBF-MA方法能更好平衡精度与现实约束，显著提高了自动赛车在复杂场景下的安全超车表现，具有推广应用价值。

Abstract: A significant challenge in autonomous racing is to generate overtaking
maneuvers. Racing agents must execute these maneuvers on complex racetracks
with little room for error. Optimization techniques and graph-based methods
have been proposed, but these methods often rely on oversimplified assumptions
for collision-avoidance and dynamic constraints. In this work, we present an
approach to trajectory synthesis based on an extension of the Differential
Bayesian Filtering framework. Our approach for collision-free trajectory
synthesis frames the problem as one of Bayesian Inference over the space of
Composite Bezier Curves. Our method is derivative-free, does not require a
spherical approximation of the vehicle footprint, linearization of constraints,
or simplifying upper bounds on collision avoidance. We conduct a closed-loop
analysis of DBF-MA and find it successfully overtakes an opponent in 87% of
tested scenarios, outperforming existing methods in autonomous overtaking.

</details>


### [565] [Hierarchical Control Design for Space Robots with Application to In-Orbit Servicing Missions](https://arxiv.org/abs/2509.22955)
*Pietro Bruschi*

Main category: cs.RO

TL;DR: 本文提出了一种分层控制框架，实现了对太空中翻滚目标的自主捕捉，并模拟了追捕器内部液体晃动效应，展示了在鲁棒性和适应性上的提升。


<details>
  <summary>Details</summary>
Motivation: 随着轨道空间作业和太空碎片清除任务的增多，需要更高阶的机器人能力来抓取甚至稳定非合作的旋转目标。

Method: 提出了分层控制架构。内环使用基于Lyapunov的鲁棒控制来处理多体动力学，外环则解决扩展的逆运动学问题，同时在仿真器中引入了追捕器内部液体晃动效应进行测试。

Result: 仿真结果表明，该控制方案在鲁棒性和适应性方面优于现有方法。

Conclusion: 该分层控制框架能有效提升空间机器人对翻滚目标的自主捕捉能力，尤其在存在内部扰动（如液体晃动）情况下表现更优。

Abstract: In-Orbit Servicing and Active Debris Removal require advanced robotic
capabilities for capturing and detumbling uncooperative targets. This work
presents a hierarchical control framework for autonomous robotic capture of
tumbling objects in space. A simulation environment is developed, incorporating
sloshing dynamics of the chaser, a rarely studied effect in space robotics. The
proposed controller combines an inner Lyapunov-based robust control loop for
multi-body dynamics with an outer loop addressing an extended inverse
kinematics problem. Simulation results show improved robustness and
adaptability compared to existing control schemes.

</details>


### [566] [Robot Learning from Any Images](https://arxiv.org/abs/2509.22970)
*Siheng Zhao,Jiageng Mao,Wei Chow,Zeyu Shangguan,Tianheng Shi,Rong Xue,Yuxi Zheng,Yijia Weng,Yang You,Daniel Seita,Leonidas Guibas,Sergey Zakharov,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了RoLA框架，可以将任意图片转化为具有交互性和物理属性的机器人环境，无需额外硬件或数字资源，快速生成大规模机器人视觉运动数据。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人训练数据获取依赖昂贵的硬件设备或制作数字资产，获取门槛高且成本大，难以满足机器人智能训练对大规模、丰富数据的需求。

Method: RoLA直接作用于单张图片，不需要额外的硬件或数字资产。其核心方法结合了单视图物理场景复原和高效的视觉融合策略，实现了逼真的数据采集过程。

Result: RoLA可以在几分钟内从各种图片源（如相机拍摄、机器人数据集、互联网上的图片）生成大规模的机器人操作视运动示范数据，并展示了应用于多类场景，如互联网图片机器人学习、单图“现实-仿真-现实”流程等。

Conclusion: RoLA极大降低了机器人学习数据生成的门槛，并因其广泛适用性和高效率，有望推动机器人学科在数据生成、算法训练等多个方向的发展。

Abstract: We introduce RoLA, a framework that transforms any in-the-wild image into an
interactive, physics-enabled robotic environment. Unlike previous methods, RoLA
operates directly on a single image without requiring additional hardware or
digital assets. Our framework democratizes robotic data generation by producing
massive visuomotor robotic demonstrations within minutes from a wide range of
image sources, including camera captures, robotic datasets, and Internet
images. At its core, our approach combines a novel method for single-view
physical scene recovery with an efficient visual blending strategy for
photorealistic data collection. We demonstrate RoLA's versatility across
applications like scalable robotic data generation and augmentation, robot
learning from Internet images, and single-image real-to-sim-to-real systems for
manipulators and humanoids. Video results are available at
https://sihengz02.github.io/RoLA .

</details>


### [567] [Safe Task Space Synchronization with Time-Delayed Information](https://arxiv.org/abs/2509.22976)
*Rounak Bhattacharya,Vrithik R. Guthikonda,Ashwin P. Dani*

Main category: cs.RO

TL;DR: 本文提出了一种自适应控制器，实现了具有未知运动学和动力学的机器人在存在通信时延条件下与人类轨迹的同步，并通过多种自适应机制保证系统安全与收敛。


<details>
  <summary>Details</summary>
Motivation: 在实际人机协作中，通信延迟、传感器融合、网络延迟等因素会导致机器人难以准确、及时地同步人类轨迹，且机器人常常存在未知的运动学与动力学参数，不利于任务有效和安全地完成。

Method: 设计了一种自适应同步控制器：1) 采用Barrier Lyapunov Function（BLF）对机器人笛卡尔空间坐标加以约束，保障安全；2) 使用基于ICL（Immersion and Invariance Control Law）的方法自适应估计未知运动学参数；3) 采用梯度型自适应律估计动力学参数。利用Barrier Lyapunov-Krasovskii（LK）泛函分析系统在时延下的稳定性。

Result: 通过仿真实验，在具有人-机器人同步任务和存在时延的情况下，所设计的同步控制器能够有效实现轨迹同步，且保证了安全性约束，同时同步和参数估计误差实现了半全局一致终极有界性（SGUUB）。

Conclusion: 该方法能够使机器人在存在未知参数与时延的复杂实际环境下，安全高效地同步人类任务轨迹，验证了所提出自适应控制框架的有效性和实用性。

Abstract: In this paper, an adaptive controller is designed for the synchronization of
the trajectory of a robot with unknown kinematics and dynamics to that of the
current human trajectory in the task space using the delayed human trajectory
information. The communication time delay may be a result of various factors
that arise in human-robot collaboration tasks, such as sensor processing or
fusion to estimate trajectory/intent, network delays, or computational
limitations. The developed adaptive controller uses Barrier Lyapunov Function
(BLF) to constrain the Cartesian coordinates of the robot to ensure safety, an
ICL-based adaptive law to account for the unknown kinematics, and a
gradient-based adaptive law to estimate unknown dynamics. Barrier
Lyapunov-Krasovskii (LK) functionals are used for the stability analysis to
show that the synchronization and parameter estimation errors remain
semi-globally uniformly ultimately bounded (SGUUB). The simulation results
based on a human-robot synchronization scenario with time delay are provided to
demonstrate the effectiveness of the designed synchronization controller with
safety constraints.

</details>


### [568] [UniPrototype: Humn-Robot Skill Learning with Uniform Prototypes](https://arxiv.org/abs/2509.23021)
*Xiao Hu,Qi Yin,Yangming Shi,Yang Ye*

Main category: cs.RO

TL;DR: 本文提出了UniPrototype框架，通过共享运动原型实现从人类到机器人领域的高效知识迁移，解决机器人学习中的数据稀缺问题，实现了更高效的学习和任务表现。


<details>
  <summary>Details</summary>
Motivation: 人类示范依赖充足的运动捕捉和互联网资源，而机器人操作任务数据稀缺，导致学习受限，该研究旨在弥合人类与机器人操作能力的差距。

Method: 提出UniPrototype框架，包括三个核心创新：1）基于软分配的组合型运动原型发现机制，可捕捉多技能的混合及层次结构；2）自适应原型选择策略，根据任务复杂性自动调整原型数量，实现可扩展和高效的表示；3）通过在仿真和现实机器人系统上的大量实验，验证了方法的有效性。

Result: 实验结果表明，UniPrototype能有效将人类操作知识迁移到机器人，提高了学习效率和任务表现，效果优于现有方法。

Conclusion: UniPrototype框架为人机操作知识迁移提供了新思路，显著缓解了数据稀缺问题，具有较强的通用性和实用价值。代码和数据集将在论文接收后公开。

Abstract: Data scarcity remains a fundamental challenge in robot learning. While human
demonstrations benefit from abundant motion capture data and vast internet
resources, robotic manipulation suffers from limited training examples. To
bridge this gap between human and robot manipulation capabilities, we propose
UniPrototype, a novel framework that enables effective knowledge transfer from
human to robot domains via shared motion primitives. ur approach makes three
key contributions: (1) We introduce a compositional prototype discovery
mechanism with soft assignments, enabling multiple primitives to co-activate
and thus capture blended and hierarchical skills; (2) We propose an adaptive
prototype selection strategy that automatically adjusts the number of
prototypes to match task complexity, ensuring scalable and efficient
representation; (3) We demonstrate the effectiveness of our method through
extensive experiments in both simulation environments and real-world robotic
systems. Our results show that UniPrototype successfully transfers human
manipulation knowledge to robots, significantly improving learning efficiency
and task performance compared to existing approaches.The code and dataset will
be released upon acceptance at an anonymous repository.

</details>


### [569] [RAISE: A Robot-Assisted Selective Disassembly and Sorting System for End-of-Life Phones](https://arxiv.org/abs/2509.23048)
*Chang Liu,Badrinath Balasubramaniam,Neal Yancey,Michael Severson,Adam Shine,Philip Bove,Beiwen Li,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文针对EoL（生命周期终结）手机拆解过程劳动密集、低效等问题，提出并实现了一套低成本、易部署且自动化的选择性拆解与分选系统，显著提升了拆解效率和收益。


<details>
  <summary>Details</summary>
Motivation: EoL手机生产量大、生命周期短，回收拆解过程中人工作业繁重、效率低，难以应对电子垃圾危机。亟需开发高效、经济的自动化拆解解决方案。

Method: 系统包含三大子系统：自适应切割系统、基于视觉的机器人分选系统和电池拆卸系统。综合实现手机的自动化、选择性拆解和高价值部件回收。

Result: 该系统每小时可处理120部手机，平均拆解成功率达98.9%，能高效将高价值部件输送至下游处理环节。

Conclusion: 本方案在拆解效率、经济效益和可扩展性方面均显著优于传统人工方法，为EoL手机拆解与电子垃圾处理提供可靠的自动化解决途径。

Abstract: End-of-Life (EoL) phones significantly exacerbate global e-waste challenges
due to their high production volumes and short lifecycles. Disassembly is among
the most critical processes in EoL phone recycling. However, it relies heavily
on human labor due to product variability. Consequently, the manual process is
both labor-intensive and time-consuming. In this paper, we propose a low-cost,
easily deployable automated and selective disassembly and sorting system for
EoL phones, consisting of three subsystems: an adaptive cutting system, a
vision-based robotic sorting system, and a battery removal system. The system
can process over 120 phones per hour with an average disassembly success rate
of 98.9%, efficiently delivering selected high-value components to downstream
processing. It provides a reliable and scalable automated solution to the
pressing challenge of EoL phone disassembly. Additionally, the automated system
can enhance disassembly economics, converting a previously unprofitable process
into one that yields a net profit per unit weight of EoL phones.

</details>


### [570] [In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands with Sim-to-Real Transfer](https://arxiv.org/abs/2509.23075)
*Soofiyan Atar,Daniel Huang,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 本文提出了一种结合仿真训练与真实硬件演示的多模态控制方法，实现了机器人手对铰接工具（如剪刀、钳子、手术工具、订书机等）的灵巧操作，显著提升了从模拟到现实的迁移能力与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习及模拟到现实的迁移在操作刚性物体领域已有进展，但在铰接机制（存在摩擦、间隙等复杂接触特性的物体）上的策略泛化能力较弱，难以应对现实中的动态干扰和个体差异。本文旨在提升机器人对这类复杂物体的操作稳定性和适应性。

Method: 方法结合了基于仿真的初始控制策略和基于硬件演示学习的传感器驱动细化，通过融合机器人自身体感、目标关节状态、全手力觉和触觉反馈，并利用跨注意力机制，将这些信息整合进控制策略，实现了在线适应、稳态接触、力调节及耦合关节协调。

Result: 在剪刀、钳子、微创外科工具、订书机等多种现实例子上验证了该方法，表现出优秀的仿真到现实迁移能力、抗干扰性能和对新型铰接工具的泛化能力。

Conclusion: 所提方法减少了对精确物理建模的依赖，提高了机器人在复杂接触场景下的操作鲁棒性，有助于推动机器人灵巧操作现实世界中多样化的铰接工具。

Abstract: Reinforcement learning (RL) and sim-to-real transfer have advanced robotic
manipulation of rigid objects. Yet, policies remain brittle when applied to
articulated mechanisms due to contact-rich dynamics and under-modeled joint
phenomena such as friction, stiction, backlash, and clearances. We address this
challenge through dexterous in-hand manipulation of articulated tools using a
robotic hand with reduced articulation and kinematic redundancy relative to the
human hand. Our controller augments a simulation-trained base policy with a
sensor-driven refinement learned from hardware demonstrations, conditioning on
proprioception and target articulation states while fusing whole-hand tactile
and force feedback with the policy's internal action intent via
cross-attention-based integration. This design enables online adaptation to
instance-specific articulation properties, stabilizes contact interactions,
regulates internal forces, and coordinates coupled-link motion under
perturbations. We validate our approach across a diversity of real-world
examples, including scissors, pliers, minimally invasive surgical tools, and
staplers. We achieve robust transfer from simulation to hardware, improved
disturbance resilience, and generalization to previously unseen articulated
tools, thereby reducing reliance on precise physical modeling in contact-rich
settings.

</details>


### [571] [Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning](https://arxiv.org/abs/2509.23107)
*Yi Wang,Zeyu Xue,Mujie Liu,Tongqin Zhang,Yan Hu,Zhou Zhao,Chenguang Yang,Zhenyu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种新的场景表示方法ST-OVSG，用于降低远程操控中由于通信延迟引发的状态错配与指令误解，并提高机器人或系统的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 在远程或高风险场景中，自然语言遥操作可以减轻操作员负担并提升安全性，但由于动态环境和通信延迟，导致本地和远程场景状态不同步，进而出现指令执行错误。本研究旨在解决延迟导致的指令误解和执行不准确问题。

Method: 引入时空开放词汇场景图（ST-OVSG），结合LVLMs（大型视觉语言模型）生成开放词汇的3D物体表征，并通过匈牙利匹配以及自定义时序匹配成本，实现跨时间的对象跟踪。场景图嵌入延迟标签，使LVLM规划器能够回溯查询历史场景状态，解决因传输延迟造成的状态错配；还提出任务导向的子图筛选策略，从而给规划器提供更紧凑、相关性更高的输入。

Result: 在Replica基准上，该方法达到74%的节点准确率，优于现有的ConceptGraph。在延迟鲁棒性实验中，辅以ST-OVSG的LVLM规划器，规划成功率达到70.5%。

Conclusion: ST-OVSG无需微调即可推广到新类别，有效增强了远程操控的规划鲁棒性，尤其在出现通信延迟时表现优异，为动态遥操作场景中的安全与效率带来显著提升。

Abstract: Teleoperation via natural-language reduces operator workload and enhances
safety in high-risk or remote settings. However, in dynamic remote scenes,
transmission latency during bidirectional communication creates gaps between
remote perceived states and operator intent, leading to command
misunderstanding and incorrect execution. To mitigate this, we introduce the
Spatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that
enriches open-vocabulary perception with temporal dynamics and lightweight
latency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D
object representations, and extends them into the temporal domain via Hungarian
assignment with our temporal matching cost, yielding a unified spatio-temporal
scene graph. A latency tag is embedded to enable LVLM planners to
retrospectively query past scene states, thereby resolving local-remote state
mismatches caused by transmission delays. To further reduce redundancy and
highlight task-relevant cues, we propose a task-oriented subgraph filtering
strategy that produces compact inputs for the planner. ST-OVSG generalizes to
novel categories and enhances planning robustness against transmission latency
without requiring fine-tuning. Experiments show that our method achieves 74
percent node accuracy on the Replica benchmark, outperforming ConceptGraph.
Notably, in the latency-robustness experiment, the LVLM planner assisted by
ST-OVSG achieved a planning success rate of 70.5 percent.

</details>


### [572] [Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications](https://arxiv.org/abs/2509.23111)
*Chen Yizhe,Wang Qi,Hu Dongxiao,Jingzhe Fang,Liu Sichao,Zixin An,Hongliang Niu,Haoran Liu,Li Dong,Chuanfen Feng,Lan Dapeng,Liu Yu,Zhibo Pang*

Main category: cs.RO

TL;DR: 本文提出了一个工业级多模态干扰数据集，用于提升机器人在复杂环境下的感知与控制能力。数据集整合了多种维度的干扰特征，并采用高精度传感器同步采集多模态数据。实验证明该数据集有助于提升模型验证的稳健性和机器人的运行稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前机器人数据集在多模态传感器数据融合以表征复杂动态环境状态方面存在挑战。工业4.0应用中，环境干扰引起的强非线性与耦合现象增强了该问题的紧迫性。研究急需可以真实反映工业复杂场景下机器人行为与环境状态互动的高质量数据集。

Method: 作者构建了一个包含尺寸、颜色、光照变化等多维度干扰因素的工业级多模态数据集。采用高精度视觉、力矩、关节状态等多种传感器同步采集数据，通过高时间精度同步与抗振动的数据采集流程，保障数据准确性。使用ROS平台标准化数据采集流程和时序。

Result: 实验证明所提出的数据集在模型验证过程中提升了鲁棒性，有效增强了机器人在动态与高干扰环境下的运行稳定性。

Conclusion: 该数据集能够真实再现和表征工业场景下复杂环境与机器人行为的动态耦合，提高了机器人算法与模型对真实干扰的适应性和稳定性。数据集已公开，可为相关领域研究者提供支持。

Abstract: In Industry 4.0 applications, dynamic environmental interference induces
highly nonlinear and strongly coupled interactions between the environmental
state and robotic behavior. Effectively representing dynamic environmental
states through multimodal sensor data fusion remains a critical challenge in
current robotic datasets. To address this, an industrial-grade multimodal
interference dataset is presented, designed for robotic perception and control
under complex conditions. The dataset integrates multi-dimensional interference
features including size, color, and lighting variations, and employs
high-precision sensors to synchronously collect visual, torque, and joint-state
measurements. Scenarios with geometric similarity exceeding 85\% and
standardized lighting gradients are included to ensure real-world
representativeness. Microsecond-level time-synchronization and
vibration-resistant data acquisition protocols, implemented via the Robot
Operating System (ROS), guarantee temporal and operational fidelity.
Experimental results demonstrate that the dataset enhances model validation
robustness and improves robotic operational stability in dynamic,
interference-rich environments. The dataset is publicly available
at:https://modelscope.cn/datasets/Liaoh_LAB/Liaohe-CobotMagic-PnP.

</details>


### [573] [FTACT: Force Torque aware Action Chunking Transformer for Pick-and-Reorient Bottle Task](https://arxiv.org/abs/2509.23112)
*Ryo Watanabe,Maxime Alvarez,Pablo Ferreiro,Pavel Savkin,Genki Sano*

Main category: cs.RO

TL;DR: 本论文提出了一种结合力量与力矩传感的新型多模态模仿学习方法，有效提升了零售环境中机械臂在操纵复杂接触物体任务（如瓶子重定向）中的成功率。


<details>
  <summary>Details</summary>
Motivation: 在零售环境中，机械臂操作常因富含接触的极端案例（如瓶子摆正）导致操作失败，需要昂贵的人类远程操控；单纯依赖视觉线索难以准确把握物体接触细节，亟需多模态信息提升操作精度。

Method: 作者提出将力量与力矩传感信息融入Action Chunking Transformer（ACT）架构，形成端到端的多模态模仿学习算法。该方法输入包括图像、机器人关节状态及实时获取的力与力矩数据，利用这些多源信息更好地学习复杂接触操作。并在Telexistence Inc公司的Ghost单臂平台上进行了实际部署与对比实验。

Result: 与仅使用视觉和关节状态的对照组方法相比，本文提出的多模态方法在瓶子抓取与重定向任务的实验中取得更高的成功率。实验还显示力量与扭矩信号在压、放两个物体视觉不可见的阶段尤其有效。

Conclusion: 结合模仿学习与轻量级力/力矩传感，可有效提高机械臂在接触丰富任务中的表现，为零售自动化中机械臂大规模实用化提供可行路径。

Abstract: Manipulator robots are increasingly being deployed in retail environments,
yet contact rich edge cases still trigger costly human teleoperation. A
prominent example is upright lying beverage bottles, where purely visual cues
are often insufficient to resolve subtle contact events required for precise
manipulation. We present a multimodal Imitation Learning policy that augments
the Action Chunking Transformer with force and torque sensing, enabling
end-to-end learning over images, joint states, and forces and torques. Deployed
on Ghost, single-arm platform by Telexistence Inc, our approach improves
Pick-and-Reorient bottle task by detecting and exploiting contact transitions
during pressing and placement. Hardware experiments demonstrate greater task
success compared to baseline matching the observation space of ACT as an
ablation and experiments indicate that force and torque signals are beneficial
in the press and place phases where visual observability is limited, supporting
the use of interaction forces as a complementary modality for contact rich
skills. The results suggest a practical path to scaling retail manipulation by
combining modern imitation learning architectures with lightweight force and
torque sensing.

</details>


### [574] [EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation](https://arxiv.org/abs/2509.23118)
*Zeyi Li,Zhe Tang,Kyeong Soo Kim,Sihao Li,Jeremy S. Smith*

Main category: cs.RO

TL;DR: 传统Wi-Fi RSSI指纹法因精度不足难以胜任高精度室内定位，LiDAR虽准但成本高。该文提出融合Wi-Fi RSSI指纹、LiDAR-SLAM与IMU的多传感器室内定位框架，用扩展卡尔曼滤波（EKF）融合多源数据，有效抑制各自噪声。实验结果显示，新方案比单一技术精度高、鲁棒性强，误差显著减少至0.24-0.37米。


<details>
  <summary>Details</summary>
Motivation: 室内定位对精度与稳定性需求提高，而现有Wi-Fi RSSI指纹技术精度有限，LiDAR定位成本高、部署复杂。因此需要一种综合多种传感器优势、成本合理且精度更高的室内定位解决方案。

Method: 提出融合Wi-Fi RSSI指纹（粗定位、DNN优化）、LiDAR-SLAM（建图、定位）、IMU导航（姿态估计）及EKF（多传感器信息融合与漂移抑制）的多模态定位框架。分别提取每种传感器的优点，通过EKF加权融合，弥补噪声干扰与漂移问题。

Result: 在西交利物浦大学真实环境下测试，融合方法的2D定位误差仅0.2449至0.3781米，优于单独Wi-Fi RSSI指纹（干扰时达1.34米）与LiDAR/IMU（因漂移达0.62至2.88米），稳健性大幅提升。

Conclusion: 通过多传感器融合、深度学习与EKF协同，显著提升室内定位精度和鲁棒性，为实际部署提供更优选择。

Abstract: Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting
cannot meet the growing demand for accurate indoor localization and navigation
due to its lower accuracy, while solutions based on light detection and ranging
(LiDAR) can provide better localization performance but is limited by their
higher deployment cost and complexity. To address these issues, we propose a
novel indoor localization and navigation framework integrating Wi-Fi RSSI
fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and
inertial measurement unit (IMU) navigation based on an extended Kalman filter
(EKF). Specifically, coarse localization by deep neural network (DNN)-based
Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a
Gmapping-based SLAM to generate an occupancy grid map and output high-frequency
attitude estimates, which is followed by EKF prediction-update integrating
sensor information while effectively suppressing Wi-Fi-induced noise and IMU
drift errors. Multi-group real-world experiments conducted on the IR building
at Xi'an Jiaotong-Liverpool University demonstrates that the proposed
multi-sensor fusion framework suppresses the instability caused by individual
approaches and thereby provides stable accuracy across all path configurations
with mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In
contrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m
in areas with severe signal interference, and those of LiDAR/IMU localization
are between 0.6233 m and 2.8803 m due to cumulative drift.

</details>


### [575] [LAGEA: Language Guided Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.23155)
*Abdul Monaf Chowdhury,Akm Moshiur Rahman Mazumder,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.RO

TL;DR: 本文提出了LAGEA框架，利用语言模型为机器人操作提供基于自然语言的即时反馈和自我反思能力，有效提升了任务成功率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管当前机器人操作可由基础模型描述目标，但现有智能体仍缺乏利用自身错误经验进行学习的机制。作者希望探索自然语言作为智能体错误诊断与自我纠正反馈信号的可行性。

Method: 提出LAGEA框架，将视觉语言模型（VLM）生成的语言型总结转化为强化学习中的时序指导信号。具体做法包括：1）用自然语言简洁总结每次尝试；2）定位关键的决定性时刻；3）将语言反馈与视觉状态在共享空间对齐；4）根据目标进展和反馈一致性，设计分阶段调整奖励系数。反馈信号初期密集，能力提升后自适应递减。

Result: 在Meta-World MT10机器人操作基准测试中，LAGEA在随机目标上比SOTA平均提升9.0%，在固定目标上提升5.3%，同时收敛更快。

Conclusion: 结构化、时序化的自然语言反馈能有效帮助机器人反思错误、做出更优决策，是提升机器人自我学习能力的有效机制。

Abstract: Robotic manipulation benefits from foundation models that describe goals, but
today's agents still lack a principled way to learn from their own mistakes. We
ask whether natural language can serve as feedback, an error reasoning signal
that helps embodied agents diagnose what went wrong and correct course. We
introduce LAGEA (Language Guided Embodied Agents), a framework that turns
episodic, schema-constrained reflections from a vision language model (VLM)
into temporally grounded guidance for reinforcement learning. LAGEA summarizes
each attempt in concise language, localizes the decisive moments in the
trajectory, aligns feedback with visual state in a shared representation, and
converts goal progress and feedback agreement into bounded, step-wise shaping
rewardswhose influence is modulated by an adaptive, failure-aware coefficient.
This design yields dense signals early when exploration needs direction and
gracefully recedes as competence grows. On the Meta-World MT10 embodied
manipulation benchmark, LAGEA improves average success over the
state-of-the-art (SOTA) methods by 9.0% on random goals and 5.3% on fixed
goals, while converging faster. These results support our hypothesis: language,
when structured and grounded in time, is an effective mechanism for teaching
robots to self-reflect on mistakes and make better choices. Code will be
released soon.

</details>


### [576] [Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion](https://arxiv.org/abs/2509.23185)
*Ziyi Zhou,Qian Meng,Hadas Kress-Gazit,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种集成规划框架，使四足机器人能够应对动态变化且不可预见的地形。该方法结合符号级控制器的反应式合成与MICP（混合整数凸规划）实现动态、可行的步态规划，并具备故障修正和实时重规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人步态规划方法过度依赖启发式策略，导致鲁棒性和适应性有限；或者依赖于计算密集的轨迹优化，难以实时应对复杂、变化的地形，因此亟需一种高效且动态适应性强的规划方法。

Method: 本框架将“反应式符号控制器合成”和“MICP路径规划”相结合，在每一步符号转换中动态规划步伐，并通过符号修正机制减少计算负担。在运行时结合实地地形数据进行MICP实时重规划，同时采用符号修正和延迟感知协调，实现从离线合成到在线操作的无缝衔接。

Result: 基于大规模仿真和硬件实验，验证了该方法能够发现所需但缺失的运动技能，并在安全关键场景（如散布的跳石、钢筋地形）下有效响应，提高了鲁棒性和自适应能力。

Conclusion: 提出的集成规划框架提升了四足机器人面对动态复杂地形时的适应性和安全性，兼顾了实时性与可行性，为实际高风险环境中四足机器人运动规划提供了有效方案。

Abstract: We present an integrated planning framework for quadrupedal locomotion over
dynamically changing, unforeseen terrains. Existing methods often depend on
heuristics for real-time foothold selection-limiting robustness and
adaptability-or rely on computationally intensive trajectory optimization
across complex terrains and long horizons. In contrast, our approach combines
reactive synthesis for generating correct-by-construction symbolic-level
controllers with mixed-integer convex programming (MICP) for dynamic and
physically feasible footstep planning during each symbolic transition. To
reduce the reliance on costly MICP solves and accommodate specifications that
may be violated due to physical infeasibility, we adopt a symbolic repair
mechanism that selectively generates only the required symbolic transitions.
During execution, real-time MICP replanning based on actual terrain data,
combined with runtime symbolic repair and delay-aware coordination, enables
seamless bridging between offline synthesis and online operation. Through
extensive simulation and hardware experiments, we validate the framework's
ability to identify missing locomotion skills and respond effectively in
safety-critical environments, including scattered stepping stones and rebar
scenarios.

</details>


### [577] [CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation](https://arxiv.org/abs/2509.23203)
*Kai Yang,Tianlin Zhang,Zhengbo Wang,Zedong Chu,Xiaolong Wu,Yang Cai,Mu Xu*

Main category: cs.RO

TL;DR: 本文提出了CE-Nav框架，实现了机器人在多种形态下的通用导航，显著降低了适应新平台的成本并取得了业界领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航政策难以跨不同机体泛化，主要受限于高昂且机体相关的数据需求、规划与控制间耦合，以及多模态决策下确定性模型平均失效的问题。该研究旨在解决这些瓶颈，实现导航策略的广泛兼容。

Method: 提出了两阶段（IL-then-RL）CE-Nav框架：首先，利用模仿学习离线训练通用专家VelFlow模型，其为条件归一化流模型，学习经典规划器生成的大规模数据以掌握多样的运动决策分布，无需真实机器人数据。其次，将该专家模型作为先验，通过在线强化学习训练动态自适应精化器，实现针对新机器人动力特性和控制器偏差的快速调整。

Result: 在四足、两足以及四旋翼平台上，CE-Nav均取得了业界领先的导航表现，并显著降低了适应新硬件所需的交互成本。

Conclusion: CE-Nav提供了一种高效且可扩展的机器人导航系统建设方案，能够广泛适用于多种机器人，实现通用性、低成本与高性能的有机统一。

Abstract: Generalizing local navigation policies across diverse robot morphologies is a
critical challenge. Progress is often hindered by the need for costly and
embodiment-specific data, the tight coupling of planning and control, and the
"disastrous averaging" problem where deterministic models fail to capture
multi-modal decisions (e.g., turning left or right). We introduce CE-Nav, a
novel two-stage (IL-then-RL) framework that systematically decouples universal
geometric reasoning from embodiment-specific dynamic adaptation. First, we
train an embodiment-agnostic General Expert offline using imitation learning.
This expert, a conditional normalizing flow model named VelFlow, learns the
full distribution of kinematically-sound actions from a large-scale dataset
generated by a classical planner, completely avoiding real robot data and
resolving the multi-modality issue. Second, for a new robot, we freeze the
expert and use it as a guiding prior to train a lightweight, Dynamics-Aware
Refiner via online reinforcement learning. This refiner rapidly learns to
compensate for the target robot's specific dynamics and controller
imperfections with minimal environmental interaction. Extensive experiments on
quadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art
performance while drastically reducing adaptation cost. Successful real-world
deployments further validate our approach as an efficient and scalable solution
for building generalizable navigation systems.

</details>


### [578] [Simulated Annealing for Multi-Robot Ergodic Information Acquisition Using Graph-Based Discretization](https://arxiv.org/abs/2509.23214)
*Benjamin Wong,Aaron Weber,Mohamed M. Safwat,Santosh Devasia,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: 本文提出了一种利用多机器人团队主动信息获取的改进算法，通过模拟退火机制自适应分配采样任务，显著提升了观测信息熵表现，并通过TurtleBot群验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 在多机器人信息采集中，为了保证所有区域观测质量一致，需要合理分配各区域的采样量。然而，区域噪声水平未知，初步估计不可靠，导致采样分布难以最优分配。如何在不确定噪声条件下优化采样分布，是实际应用中的难题。

Method: 提出用模拟退火（通过调节Boltzmann分布冷却参数，依据采样熵逐步调整分布）方法来动态生成采样分布。从初始均匀分布逐步收敛到估算出的最优分布，结合熵估计引导采样策略。采用仿真对比了均匀、直接遍历与所提方法，并最终在TurtleBot机器人群上实物验证。

Result: 仿真结果显示，无论是过渡阶段还是最终表现，所提方法在信息熵指标上均优于均匀采样和直接遍历分布。实际系统（TurtleBot群）实验进一步验证了算法的可用性和有效性。

Conclusion: 使用模拟退火调节采样分布的策略能够有效提升多机器人系统下的信息采集均衡性与效率，适用于实际物理机器人平台，有较好的实用推广价值。

Abstract: One of the goals of active information acquisition using multi-robot teams is
to keep the relative uncertainty in each region at the same level to maintain
identical acquisition quality (e.g., consistent target detection) in all the
regions. To achieve this goal, ergodic coverage can be used to assign the
number of samples according to the quality of observation, i.e., sampling noise
levels. However, the noise levels are unknown to the robots. Although this
noise can be estimated from samples, the estimates are unreliable at first and
can generate fluctuating values. The main contribution of this paper is to use
simulated annealing to generate the target sampling distribution, starting from
uniform and gradually shifting to an estimated optimal distribution, by varying
the coldness parameter of a Boltzmann distribution with the estimated sampling
entropy as energy. Simulation results show a substantial improvement of both
transient and asymptotic entropy compared to both uniform and direct-ergodic
searches. Finally, a demonstration is performed with a TurtleBot swarm system
to validate the physical applicability of the algorithm.

</details>


### [579] [GLUE: Global-Local Unified Encoding for Imitation Learning via Key-Patch Tracking](https://arxiv.org/abs/2509.23220)
*Ye Chen,Zichen Zhou,Jianyu Dou,Te Cui,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: 本文提出了GLUE，一种结合全局与局部特征编码的模仿学习框架，在复杂的视觉环境下提升了机器人任务表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂的机器人模仿学习任务中，环境常常存在杂乱和遮挡，导致传统全局视觉特征容易受干扰，影响策略效果。利用对任务相关物体的不变局部特征，有助于缓解训练与测试分布间的不一致（covariate shift）问题。

Method: 作者提出了GLUE框架，通过text-guided机制选取并跟踪关键局部patch，设计了新型全局-局部特征融合方法，实现低异质性的精细局部特征，使训练和测试数据在特征空间中更靠近，提升了策略的鲁棒性。

Result: GLUE在仿真和真实任务中均取得优异结果：在仿真环境中提升17.6%，真实环境提升36.3%，在真实泛化场景中提升58.3%，均超过最强基线。

Conclusion: 通过全局-局部统一编码，GLUE有效对齐了异分布环境下的特征空间，提高了模仿学习策略的泛化性和鲁棒性。

Abstract: In recent years, visual representation learning has gained widespread
attention in robotic imitation learning. However, in complex
Out-of-Distribution(OOD) settings characterized by clutter and occlusion, the
attention of global visual representations can be diluted or interfered,
leading to degraded policy performance. The invariance of local representations
for task-relevant objects offers a solution. By efficiently utilizing these
local representations, training and testing data can be mapped to a more
similar feature space, thereby mitigating the covariate shift problem.
Accordingly, we propose GLUE, a global-local unified encoding framework for
imitation learning based on key-patch tracking. GLUE selects and tracks
key-patches as critical local representations by employing a text-guided
mechanism. It features a novel fusion framework where global patch features
query local patches to distill essential information, yielding fine-grained
local features with low heterogeneity relative to the global context. This
fused representation steers the robot's visual attention toward task-relevant
objects and preserves precise global context, which together align the training
and testing distributions into a similar and task-informative feature space,
ultimately enhancing the robustness of the imitation learning policy.
Experiments demonstrate that GLUE achieves strong performance across diverse
tasks in both simulation and real-world settings, outperforming the strongest
baseline by 17.6% in simulation, 36.3% in real-world environments, and 58.3% on
real-world generalization settings. The project website of GLUE is available at
https://GLUE666.github.io/.

</details>


### [580] [SAC-Loco: Safe and Adjustable Compliant Quadrupedal Locomotion](https://arxiv.org/abs/2509.23223)
*Aoqian Zhang,Zixuan Zhuang,Chunzheng Wang,Shuzhi Sam Ge,Fan Shi,Cheng Xiang*

Main category: cs.RO

TL;DR: 本论文提出了一种新的切换式控制框架，使四足机器人能够在遭遇外部干扰时保持弹性与安全，提高了稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人控制方法难以像动物一样根据外部干扰实现自适应、可调节的顺应性，且缺少弹性调节能力，容易在遭遇大扰动时失效。

Method: 首先，利用师生强化学习框架训练具有可调节顺应性的力顺应策略，无需显式力传感；其次，基于捕获点理论设计了安全策略，用于当顺应策略失效时稳定机器人；最后，提出了一种可恢复性网络，根据失败概率实现顺应策略和安全策略间的切换。

Result: 该框架使得四足机器人不仅可以实现力顺应，还能在遭受剧烈外部干扰时保证鲁棒性和安全性。

Conclusion: 结果表明，提出的切换策略和可恢复性评估相结合的方法，有效提高了机器人在复杂环境下的顺应性与安全水平，为四足机器人的实用化提供了支持。

Abstract: Quadruped robots are designed to achieve agile locomotion by mimicking legged
animals. However, existing control methods for quadrupeds often lack one of the
key capabilities observed in animals: adaptive and adjustable compliance in
response to external disturbances. Most locomotion controllers do not provide
tunable compliance and tend to fail under large perturbations. In this work, we
propose a switched policy framework for compliant and safe quadruped
locomotion. First, we train a force compliant policy with adjustable compliance
levels using a teacher student reinforcement learning framework, eliminating
the need for explicit force sensing. Next, we develop a safe policy based on
the capture point concept to stabilize the robot when the compliant policy
fails. Finally, we introduce a recoverability network that predicts the
likelihood of failure and switches between the compliant and safe policies.
Together, this framework enables quadruped robots to achieve both force
compliance and robust safety when subjected to severe external disturbances.

</details>


### [581] [Leave No Observation Behind: Real-time Correction for VLA Action Chunks](https://arxiv.org/abs/2509.23224)
*Kohei Sendai,Maxime Alvarez,Tatsuya Matsushima,Yutaka Matsuo,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 提出了A2C2方法，能够为现有视觉-语言-行动（VLA）模型的动作分块机制带来轻量、实时、可扩展的动作矫正，从而提升系统的反应性与性能，无需重新训练原有策略。实验证明在不同任务和挑战场景下能显著提升成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型通过动作分块提升效率和时序一致性，但动作分块策略在面对推理延迟和长时任务时，反应性变差，闭环性能下降。为兼顾高效性与实时反应，亟需能修正延迟影响且无需大幅更改原模型的方法。

Method: 提出异步动作分块矫正模块A2C2，在每个控制步骤轻量运行。该模块输入最新观测、原VLA输出的分块动作、动作在分块中的位置编码，以及部分原策略特征，输出逐步的动作校正。无需对原有策略重新训练，任意VLA模型均可插入，且与现有异步执行机制（如RTC）兼容。

Result: 在Kinetix 12个动态任务和LIBERO Spatial任务上，A2C2在提升执行延迟和长时任务下的成功率方面优于RTC，分别平均提升23%和7%。即便无额外延迟，也能提升长时任务鲁棒性。矫正头很小，计算开销极低。

Conclusion: A2C2是一种有效且易部署的增强机制，能够让基于动作分块的高容量VLA模型实现更高效、响应更灵敏的实时控制，适应更苛刻或易变的现实环境。

Abstract: To improve efficiency and temporal coherence, Vision-Language-Action (VLA)
models often predict action chunks; however, this action chunking harms
reactivity under inference delay and long horizons. We introduce Asynchronous
Action Chunk Correction (A2C2), which is a lightweight real-time chunk
correction head that runs every control step and adds a time-aware correction
to any off-the-shelf VLA's action chunk. The module combines the latest
observation, the predicted action from VLA (base action), a positional feature
that encodes the index of the base action within the chunk, and some features
from the base policy, then outputs a per-step correction. This preserves the
base model's competence while restoring closed-loop responsiveness. The
approach requires no retraining of the base policy and is orthogonal to
asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic
Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent
success rate improvements across increasing delays and execution horizons (+23%
point and +7% point respectively, compared to RTC), and also improves
robustness for long horizons even with zero injected delay. Since the
correction head is small and fast, there is minimal overhead compared to the
inference of large VLA models. These results indicate that A2C2 is an
effective, plug-in mechanism for deploying high-capacity chunking policies in
real-time control.

</details>


### [582] [Online Dynamic Goal Recognition in Gym Environments](https://arxiv.org/abs/2509.23244)
*Shamir Matan,Elhadad Osher,Nageris Ben,Mirsky Reuth*

Main category: cs.RO

TL;DR: 本文介绍了两个开源库gr-libs和gr-envs，为目标识别（Goal Recognition, GR）研究提供标准化和可复现的平台，以促进各种GR算法的开发、评估与比较。


<details>
  <summary>Details</summary>
Motivation: 当前目标识别领域在基准、测试环境和评估协议方面存在碎片化和不一致问题，这阻碍了这一领域的进一步发展和研究成果的横向对比。

Method: 作者开发了两个开源工具包：gr-libs实现了基于MDP的GR基线算法、诊断工具与评估工具，gr-envs则提供了为动态和目标导向行为设计的环境以及与主流强化学习套件兼容的接口封装。这两个库均与Gym环境兼容，使算法能方便地进行测试与对比。

Result: 两个工具包均已开源发布，能让研究者更便捷地开发、评估和比较各类GR算法，推动标准化和可重复研究的进展。

Conclusion: gr-libs和gr-envs为目标识别研究领域提供了标准化、可扩展且可复现的基础研究平台，有助于统一GR算法的评估体系，推动整个领域的发展。

Abstract: Goal Recognition (GR) is the task of inferring an agent's intended goal from
partial observations of its behavior, typically in an online and one-shot
setting. Despite recent advances in model-free GR, particularly in applications
such as human-robot interaction, surveillance, and assistive systems, the field
remains fragmented due to inconsistencies in benchmarks, domains, and
evaluation protocols.
  To address this, we introduce gr-libs
(https://github.com/MatanShamir1/gr_libs) and gr-envs
(https://github.com/MatanShamir1/gr_envs), two complementary open-source
frameworks that support the development, evaluation, and comparison of GR
algorithms in Gym-compatible environments. gr-libs includes modular
implementations of MDP-based GR baselines, diagnostic tools, and evaluation
utilities. gr-envs provides a curated suite of environments adapted for dynamic
and goal-directed behavior, along with wrappers that ensure compatibility with
standard reinforcement learning toolkits. Together, these libraries offer a
standardized, extensible, and reproducible platform for advancing GR research.
Both packages are open-source and available on GitHub and PyPI.

</details>


### [583] [Preventing Robotic Jailbreaking via Multimodal Domain Adaptation](https://arxiv.org/abs/2509.23281)
*Francesco Marchiori,Rohan Sinha,Christopher Agia,Alexander Robey,George J. Pappas,Mauro Conti,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了J-DAPT框架，用于机器人领域中的多模态越狱检测，显著提升检测准确率，增强系统安全。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和视觉-语言模型在机器人中应用广泛，但易受越狱攻击，现有的数据驱动防御方法在缺乏专用数据集时效果有限，急需更通用高效的检测机制。

Method: J-DAPT利用基于注意力机制的融合方法，将文本与视觉嵌入结合，提取语义意图及环境信息，并通过领域自适应，将通用越狱数据集对齐到特定应用领域。

Result: 在自动驾驶、海事机器人和四足机器人导航三个场景下，J-DAPT大幅提高越狱检测准确率（接近100%），并且开销很小。

Conclusion: J-DAPT能够以低成本为机器人中的视觉-语言模型提供实用、高效的越狱检测防御，对保障机器人应用安全具有现实意义。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) are
increasingly deployed in robotic environments but remain vulnerable to
jailbreaking attacks that bypass safety mechanisms and drive unsafe or
physically harmful behaviors in the real world. Data-driven defenses such as
jailbreak classifiers show promise, yet they struggle to generalize in domains
where specialized datasets are scarce, limiting their effectiveness in robotics
and other safety-critical contexts. To address this gap, we introduce J-DAPT, a
lightweight framework for multimodal jailbreak detection through
attention-based fusion and domain adaptation. J-DAPT integrates textual and
visual embeddings to capture both semantic intent and environmental grounding,
while aligning general-purpose jailbreak datasets with domain-specific
reference data. Evaluations across autonomous driving, maritime robotics, and
quadruped navigation show that J-DAPT boosts detection accuracy to nearly 100%
with minimal overhead. These results demonstrate that J-DAPT provides a
practical defense for securing VLMs in robotic applications. Additional
materials are made available at: https://j-dapt.github.io.

</details>


### [584] [A Novel Narrow Region Detector for Sampling-Based Planners' Efficiency: Match Based Passage Identifier](https://arxiv.org/abs/2509.23288)
*Yafes Enes Şahiner,Esat Yusuf Gündoğdu,Volkan Sezer*

Main category: cs.RO

TL;DR: 本文提出了一种改进采样器，通过确定性地识别窄通道区域并增加在这些区域的采样，提高了移动机器人等自主系统路径规划的效率和效果。实验涵盖仿真与现实环境，对比主流方法表现更佳。


<details>
  <summary>Details</summary>
Motivation: 目前主流的路径规划采样方法在遇到狭窄通道时普遍存在性能下降，导致路径不可达或效率低下。因此，亟需提升在这类环境下路径规划算法的表现。

Method: 作者提出了一种新型采样器，利用占据栅格地图（occupancy grid maps）确定性地检测狭窄通道，对这些区域增加采样点密度。同时，算法源码开源。

Result: 在三类基准测试环境（特定模拟、随机模拟、现实环境）中，所提方法在路径规划时间和里程碑数量等指标上，相较传统采样方法有明显提升。

Conclusion: 该采样器显著提升了窄通道稀疏采样带来的性能瓶颈，在实际自主系统路径规划中具有较强实用价值。

Abstract: Autonomous technology, which has become widespread today, appears in many
different configurations such as mobile robots, manipulators, and drones. One
of the most important tasks of these vehicles during autonomous operations is
path planning. In the literature, path planners are generally divided into two
categories: probabilistic and deterministic methods. In the analysis of
probabilistic methods, the common problem of almost all methods is observed in
narrow passage environments. In this paper, a novel sampler is proposed that
deterministically identifies narrow passage environments using occupancy grid
maps and accordingly increases the amount of sampling in these regions. The
codes of the algorithm is provided as open source. To evaluate the performance
of the algorithm, benchmark studies are conducted in three distinct categories:
specific and random simulation environments, and a real-world environment. As a
result, it is observed that our algorithm provides higher performance in
planning time and number of milestones compared to the baseline samplers.

</details>


### [585] [Distributed Multi-Robot Multi-Target Simultaneous Search and Tracking in an Unknown Non-convex Environment](https://arxiv.org/abs/2509.23308)
*Jun Chen,Jiaqing Ma,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了一种结合探索、覆盖和多目标跟踪的全新多机器人运动规划框架，并通过仿真验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 复杂未知（如非凸）环境中的多机器人探索与高精度多目标跟踪任务同时优化，是应急救援、环境监测等场景中的关键难题。现有研究未能建立综合优化这两类任务的统一框架，难以平衡数据采集的精度和空间覆盖效率。

Method: 本文提出一个全新运动规划框架，将基于前沿（frontier-based）的探索策略、基于Lloyd算法的覆盖策略、及基于传感器的多目标跟踪策略三者有机结合，使机器人群能够兼顾环境探索与高精度目标跟踪。实现上，设计了相应的算法流程，并在MATLAB仿真环境下对系统进行评测。

Result: 通过一系列MATLAB仿真实验，本文方法在探索效率、多目标跟踪精度等方面优于传统单一策略，表现出良好的兼容性和协作能力。

Conclusion: 综合三种策略的运动规划算法不仅能提升探索和覆盖效率，同时保障目标的高精度跟踪，为多机器人在复杂环境下的多任务协同提供了有效方案。

Abstract: In unknown non-convex environments, such as indoor and underground spaces,
deploying a fleet of robots to explore the surroundings while simultaneously
searching for and tracking targets of interest to maintain high-precision data
collection represents a fundamental challenge that urgently requires resolution
in applications such as environmental monitoring and rescue operations. Current
research has made significant progress in addressing environmental exploration,
information search, and target tracking problems, but has yet to establish a
framework for simultaneously optimizing these tasks in complex environments. In
this paper, we propose a novel motion planning algorithm framework that
integrates three control strategies: a frontier-based exploration strategy, a
guaranteed coverage strategy based on Lloyd's algorithm, and a sensor-based
multi-target tracking strategy. By incorporating these three strategies, the
proposed algorithm balances coverage search and high-precision active tracking
during exploration. Our approach is validated through a series of MATLAB
simulations, demonstrating validity and superiority over standard approaches.

</details>


### [586] [GUARD: Toward a Compromise between Traditional Control and Learning for Safe Robot Systems](https://arxiv.org/abs/2509.23312)
*Johannes A. Gaus,Junheon Yoon,Woo-Jeong Baek,Seungwon Choi,Suhan Park,Jaeheung Park*

Main category: cs.RO

TL;DR: 本文提出了GUARD框架，将传统控制与基于主动学习的不确定性感知技术结合，实现了具备实时能力的安全机器人避碰。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术中，如何在传统方法与学习算法之间找到安全、高效且灵活的折中方案，是实现可靠应用的核心挑战。

Method: 将基于模型的预测轮廓控制（RMPCC）与迭代最近点（ICP）算法相结合，通过主动学习和概率核优化，能够实时归因不确定性来源。这个方法同时兼顾了传统控制的稳定性和机器学习的不确定性处理能力。

Result: 实验显示，GUARD在安全避碰任务中表现出色。

Conclusion: GUARD为机器人安全应用提供了有效新途径，有必要在未来进一步拓展其应用范围。

Abstract: This paper presents the framework \textbf{GUARD} (\textbf{G}uided robot
control via \textbf{U}ncertainty attribution and prob\textbf{A}bilistic kernel
optimization for \textbf{R}isk-aware \textbf{D}ecision making) that combines
traditional control with an uncertainty-aware perception technique using active
learning with real-time capability for safe robot collision avoidance. By doing
so, this manuscript addresses the central challenge in robotics of finding a
reasonable compromise between traditional methods and learning algorithms to
foster the development of safe, yet efficient and flexible applications. By
unifying a reactive model predictive countouring control (RMPCC) with an
Iterative Closest Point (ICP) algorithm that enables the attribution of
uncertainty sources online using active learning with real-time capability via
a probabilistic kernel optimization technique, \emph{GUARD} inherently handles
the existing ambiguity of the term \textit{safety} that exists in robotics
literature. Experimental studies indicate the high performance of \emph{GUARD},
thereby highlighting the relevance and need to broaden its applicability in
future.

</details>


### [587] [Space Robotics Bench: Robot Learning Beyond Earth](https://arxiv.org/abs/2509.23328)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 本文提出了Space Robotics Bench，一个专为太空环境设计的开源机器人学习仿真框架，帮助研究人员开发和测试鲁棒的自主系统。


<details>
  <summary>Details</summary>
Motivation: 太空探索对在极端和未经结构化环境中运行的自主机器人系统提出了更高要求。但受限于高昂的技术演示成本及数据稀缺，当前机器人学习在太空领域的发展受阻。

Method: 设计并实现了Space Robotics Bench仿真平台，采用模块化架构，结合按需生成的多样环境与大规模并行仿真。此外，包含多种与实际任务相关的基准测试，方便研究与对比。利用标准强化学习算法建立性能基线，并进行案例研究，分析泛化、端到端学习、自适应控制及仿真到实际的迁移等挑战。

Result: 实验发现，当前方法在泛化、迁移等关键问题上存在限制，但通过该框架能训练出具备现实应用能力的策略。

Conclusion: Space Robotics Bench为太空机器人领域的鲁棒自主系统开发、基准测试和部署提供了宝贵平台，有望推动该领域的进步。

Abstract: The growing ambition for space exploration demands robust autonomous systems
that can operate in unstructured environments under extreme extraterrestrial
conditions. The adoption of robot learning in this domain is severely hindered
by the prohibitive cost of technology demonstrations and the limited
availability of data. To bridge this gap, we introduce the Space Robotics
Bench, an open-source simulation framework for robot learning in space. It
offers a modular architecture that integrates on-demand procedural generation
with massively parallel simulation environments to support the creation of vast
and diverse training distributions for learning-based agents. To ground
research and enable direct comparison, the framework includes a comprehensive
suite of benchmark tasks that span a wide range of mission-relevant scenarios.
We establish performance baselines using standard reinforcement learning
algorithms and present a series of experimental case studies that investigate
key challenges in generalization, end-to-end learning, adaptive control, and
sim-to-real transfer. Our results reveal insights into the limitations of
current methods and demonstrate the utility of the framework in producing
policies capable of real-world operation. These contributions establish the
Space Robotics Bench as a valuable resource for developing, benchmarking, and
deploying the robust autonomous systems required for the final frontier.

</details>


### [588] [Robust Orientation Estimation with TRIAD-aided Manifold EKF](https://arxiv.org/abs/2509.23456)
*Arjun Sadananda,Ravi Banavar,Kavi Arya*

Main category: cs.RO

TL;DR: 本论文提出了一种改进的Manifold EKF方法，通过结合TRIAD算法减弱磁传感器在俯仰和横滚轴姿态估计中的干扰影响，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 使用磁力计进行姿态确定时，容易受到外部磁场和校准误差干扰，降低了姿态估计精度。因此需要寻找方法减小这些干扰对EKF滤波器的影响。

Method: 将TRIAD算法的子最优特性结合到Manifold EKF中，专门用来减弱磁力计读数在俯仰和横滚轴姿态确定上的误差影响。

Result: 实验结果表明，改进后的Manifold EKF能有效降低磁力计干扰对俯仰和横滚姿态估计的影响。

Conclusion: 结合TRIAD的Manifold EKF算法在抗磁扰能力和姿态估计精度上优于传统方法，具有较强的实用性。

Abstract: The manifold extended Kalman filter (Manifold EKF) has found extensive
application for attitude determination. Magnetometers employed as sensors for
such attitude determination are easily prone to disturbances by their
sensitivity to calibration and external magnetic fields. The TRIAD (Tri-Axial
Attitude Determination) algorithm is well known as a sub-optimal attitude
estimator. In this article, we incorporate this sub-optimal feature of the
TRIAD in mitigating the influence of the magnetometer reading in the pitch and
roll axis determination in the Manifold EKF algorithm. We substantiate our
results with experiments.

</details>


### [589] [Multi-Modal Manipulation via Multi-Modal Policy Consensus](https://arxiv.org/abs/2509.23468)
*Haonan Chen,Jiaming Xu,Hongyu Chen,Kaiwen Hong,Binghao Huang,Chaoqi Liu,Jiayuan Mao,Yunzhu Li,Yilun Du,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 该论文提出了一种新的多模态融合方法，显著提升了机器人操作任务中多感知模态信息利用的效果，优于传统特征拼接方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法，尤其是特征拼接，经常被主导模态（如视觉）压制，导致稀疏但关键的触觉信号无法有效利用。同时，架构缺乏灵活性，难以应对新模态或模态缺失的问题。作者希望建立一种能灵活融合不同模态，并提升机器人多模态感知表现的方法。

Method: 作者将策略结构分解为多个专门处理单一感知模态（如视觉或触觉）的扩散模型，并设计了一个路由网络学习权重以自适应整合这些专用模型的输出。这种方式支持新模态的增量加入，无需整体重训练。

Result: 在模拟平台（RLBench）和现实世界场景（如遮挡物体抓取、手内勺子重新定向、拼图插入）上，该方法在需要多模态推理的任务中明显优于传统特征拼接基线。同时，在物理扰动和传感器干扰下展现出更强的鲁棒性。通过扰动分析，展示了不同情境下对各模态的适应性调整。

Conclusion: 该方法有效提升了多感知模态的整合效率和策略鲁棒性，实现了灵活扩展和更优性能，是多模态机器人感知与操作领域的重要进展。

Abstract: Effectively integrating diverse sensory modalities is crucial for robotic
manipulation. However, the typical approach of feature concatenation is often
suboptimal: dominant modalities such as vision can overwhelm sparse but
critical signals like touch in contact-rich tasks, and monolithic architectures
cannot flexibly incorporate new or missing modalities without retraining. Our
method factorizes the policy into a set of diffusion models, each specialized
for a single representation (e.g., vision or touch), and employs a router
network that learns consensus weights to adaptively combine their
contributions, enabling incremental of new representations. We evaluate our
approach on simulated manipulation tasks in {RLBench}, as well as real-world
tasks such as occluded object picking, in-hand spoon reorientation, and puzzle
insertion, where it significantly outperforms feature-concatenation baselines
on scenarios requiring multimodal reasoning. Our policy further demonstrates
robustness to physical perturbations and sensor corruption. We further conduct
perturbation-based importance analysis, which reveals adaptive shifts between
modalities.

</details>


### [590] [Ask, Reason, Assist: Decentralized Robot Collaboration via Language and Logic](https://arxiv.org/abs/2509.23506)
*Dan BW Choe,Sundhar Vinodh Sangeetha,Steven Emanuel,Chih-Yuan Chiu,Samuel Coogan,Shreyas Kousik*

Main category: cs.RO

TL;DR: 该论文提出了一种去中心化框架，使仓储等领域的异构机器人团队可以通过自然语言互相请求和提供帮助，从而有效解决突发冲突，提高协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器人部署数量的增加（如在仓储中），异构机器人团队协作时未知冲突频发。现有方法在机器人自主协作与效率优化方面存在局限，尤其在缺乏集中指挥情况下。

Method: 当机器人检测到冲突后，会利用大语言模型（LLM）决策是否需要外部协助，并用自然语言生成并广播求助信息。潜在助手机器人借助基于BNF语法的LLM将请求转化为信号时序逻辑（STL），以确保语法正确，并通过混合整数线性规划（MILP）来决定自己的协助响应，并报告对自身任务的影响。发起求助的机器人则根据对系统级任务完成总时长预期影响来选择最优助手。

Result: 通过实验对比不同的协助者选择策略，发现综合考虑多个候选者报价的策略能够最小化整体任务延时。该方法显著优于单纯选择最近可用机器人的启发式方法，并在不需要中心化全局信息的情况下，达到接近中央“Oracle”基线的性能。

Conclusion: 提出的去中心化机器人团队求助与协调框架，在任务效率和灵活性方面具备突出优势，可广泛用于实际多机器人系统协作冲突场景。

Abstract: Increased robot deployment, such as in warehousing, has revealed a need for
seamless collaboration among heterogeneous robot teams to resolve unforeseen
conflicts. To address this challenge, we propose a novel decentralized
framework that enables robots to request and provide help. The process begins
when a robot detects a conflict and uses a Large Language Model (LLM) to decide
whether external assistance is required. If so, it crafts and broadcasts a
natural language (NL) help request. Potential helper robots reason over the
request and respond with offers of assistance, including information about the
effect on their ongoing tasks. Helper reasoning is implemented via an LLM
grounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar,
ensuring syntactically valid NL-to-STL translations, which are then solved as a
Mixed Integer Linear Program (MILP). Finally, the requester robot selects a
helper by reasoning over the expected increase in system-level total task
completion time. We evaluated our framework through experiments comparing
different helper-selection strategies and found that considering multiple
offers allows the requester to minimize added makespan. Our approach
significantly outperforms heuristics such as selecting the nearest available
candidate helper robot, and achieves performance comparable to a centralized
"Oracle" baseline but without heavy information demands.

</details>


### [591] [Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso via Guided Reinforcement Learning](https://arxiv.org/abs/2509.23556)
*Curtis C. Johnson,Carlo Alessi,Egidio Falotico,Marc D. Killpack*

Main category: cs.RO

TL;DR: 本文提出了一种高效仿真方法，实现了软体机器人在全身操纵任务中的零样本策略迁移，并在真实机器人平台上取得88%的成功率。


<details>
  <summary>Details</summary>
Motivation: 全身操纵可让机器人更有效地处理大型、笨重或复杂物体，但软体机器人的动力学和运动学不确定性给仿真与控制带来挑战。现有方法难以实现准确、快速且可迁移的控制策略。

Method: 开发了一套在MuJoCo中可以高达实时350倍速度运行的高效仿真框架，深入分析了仿真速度和准确性的权衡。利用该仿真环境，通过引导强化学习（RL）以简单运动基元，而非传统奖励塑造，训练全身操纵策略，并实现零样本仿真转现实。

Result: 所学策略在真实软体机器人平台（Baloo，10公斤负载）上取得88%的成功率。分析发现引导RL的运动基元提升了性能，策略展现了反应性（如再抓取、扰动恢复），对比开环基线可主动过度修正。

Conclusion: 本研究首次展示了基于两只连续型软臂的大型平台上，能够6自由度全身操纵、单次学习零样本迁移的成功实现，为软体机器人在复杂操作中的前景提供了新突破。

Abstract: Whole-body manipulation is a powerful yet underexplored approach that enables
robots to interact with large, heavy, or awkward objects using more than just
their end-effectors. Soft robots, with their inherent passive compliance, are
particularly well-suited for such contact-rich manipulation tasks, but their
uncertainties in kinematics and dynamics pose significant challenges for
simulation and control. In this work, we address this challenge with a
simulation that can run up to 350x real time on a single thread in MuJoCo and
provide a detailed analysis of the critical tradeoffs between speed and
accuracy for this simulation. Using this framework, we demonstrate a successful
zero-shot sim-to-real transfer of a learned whole-body manipulation policy,
achieving an 88% success rate on the Baloo hardware platform. We show that
guiding RL with a simple motion primitive is critical to this success where
standard reward shaping methods struggled to produce a stable and successful
policy for whole-body manipulation. Furthermore, our analysis reveals that the
learned policy does not simply mimic the motion primitive. It exhibits
beneficial reactive behavior, such as re-grasping and perturbation recovery. We
analyze and contrast this learned policy against an open-loop baseline to show
that the policy can also exhibit aggressive over-corrections under
perturbation. To our knowledge, this is the first demonstration of forceful,
six-DoF whole-body manipulation using two continuum soft arms on a large-scale
platform (10 kg payloads), with zero-shot policy transfer.

</details>


### [592] [High Torque Density PCB Axial Flux Permanent Magnet Motor for Micro Robots](https://arxiv.org/abs/2509.23561)
*Jianren Wang,Jie Han,Abhinav Gupta,Deepak Pathak,Yang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于HDI技术的微型轴向磁通永磁（AFPM）电机，实现了高铜填充率和小尺寸，突破了传统小型AFPM电机铜填充率低导致的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 受限于传统线圈绕组制造工艺，小于20mm外径的AFPM电机存在铜填充率低、电阻高、连续输出扭矩受限等问题，难以满足QDD机器人关节高扭矩低速小型化需求。

Method: 采用先进集成电路（IC）基板的高密度互连（HDI）技术，通过叠加多个12层HDI模块，制造出48层PCB绕组定子，提高铜填充率，并结合电磁与热分析指导电机结构设计，最终完成并测试了原型。

Result: 最终实现了一种外径19mm、厚度5mm、铜填充率高达45%的AFPM电机原型，并通过实验验证其优异的性能指标。

Conclusion: 所提出的HDI-PCB绕组方法显著提升了微型AFPM电机的铜填充率和性能，为QDD机器人小型高性能关节电机提供了新方案，具有重要应用前景。

Abstract: Quasi-direct-drive (QDD) actuation is transforming legged and manipulator
robots by eliminating high-ratio gearboxes, yet it demands motors that deliver
very high torque at low speed within a thin, disc-shaped joint envelope.
Axial-flux permanent-magnet (AFPM) machines meet these geometric and torque
requirements, but scaling them below a 20mm outer diameter is hampered by poor
copper fill in conventional wound stators, inflating resistance and throttling
continuous torque. This paper introduces a micro-scale AFPM motor that
overcomes these limitations through printed-circuit-board (PCB) windings
fabricated with advanced IC-substrate high-density interconnect (HDI)
technology. The resulting 48-layer stator-formed by stacking four 12-layer HDI
modules-achieves a record 45\% copper fill in a package only 5mm thick and 19mm
in diameter. We perform comprehensive electromagnetic and thermal analyses to
inform the motor design, then fabricate a prototype whose performance
characteristics are experimentally verified.

</details>


### [593] [RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation](https://arxiv.org/abs/2509.23563)
*Seungchan Kim,Omar Alama,Dmytro Kurdydyk,John Keller,Nikhil Keetha,Wenshan Wang,Yonatan Bisk,Sebastian Scherer*

Main category: cs.RO

TL;DR: 本文提出了RAVEN，一个基于3D记忆和行为树框架的户外空中语义导航系统，有效提升了无人机在广阔、非结构化环境中搜寻目标物体的能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的语义导航方法多专注于室内环境，受空间范围和结构化限制，难以应用于户外广阔环境。现有户外方法要么仅依赖观察下的反应式策略（导致短视行为），要么需离线计算场景图（影响在线适应），因而缺乏灵活和长效的解决方案。

Method: 提出RAVEN：1）采用空间一致的语义体素射线地图作为长期记忆，实现长距离规划并避免仅反应式行为；2）结合短距离体素搜索和长距离射线搜索，实现大环境适应性；3）借助大型视觉-语言模型提供辅助线索，减少户外目标稀疏带来的影响。所有组件通过行为树协调，动态切换行为以增强鲁棒性。

Result: 在10个高度逼真的户外仿真环境下完成100个语义任务（包括单目标、多类别、多实例和顺序任务），RAVEN在仿真中超越基线法85.25%，并在真实无人机户外测试中展示了实践能力。

Conclusion: RAVEN显著提升了户外空中语义导航性能，兼具远距离规划与适应性，满足实际应用需求，优于现有方法，在仿真和实测中均证明其有效性和实用性。

Abstract: Aerial outdoor semantic navigation requires robots to explore large,
unstructured environments to locate target objects. Recent advances in semantic
navigation have demonstrated open-set object-goal navigation in indoor
settings, but these methods remain limited by constrained spatial ranges and
structured layouts, making them unsuitable for long-range outdoor search. While
outdoor semantic navigation approaches exist, they either rely on reactive
policies based on current observations, which tend to produce short-sighted
behaviors, or precompute scene graphs offline for navigation, limiting
adaptability to online deployment. We present RAVEN, a 3D memory-based,
behavior tree framework for aerial semantic navigation in unstructured outdoor
environments. It (1) uses a spatially consistent semantic voxel-ray map as
persistent memory, enabling long-horizon planning and avoiding purely reactive
behaviors, (2) combines short-range voxel search and long-range ray search to
scale to large environments, (3) leverages a large vision-language model to
suggest auxiliary cues, mitigating sparsity of outdoor targets. These
components are coordinated by a behavior tree, which adaptively switches
behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor
simulation environments over 100 semantic tasks, encompassing single-object
search, multi-class, multi-instance navigation and sequential task changes.
Results show RAVEN outperforms baselines by 85.25% in simulation and
demonstrate its real-world applicability through deployment on an aerial robot
in outdoor field tests.

</details>


### [594] [GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With Geometry-Based Expert Selection](https://arxiv.org/abs/2509.23567)
*Fangting Xu,Jilin Zhu,Xiaoming Gu,Jianzhong Tang*

Main category: cs.RO

TL;DR: 本文提出了ContactGrasp数据集和基于几何体聚类的把握专家选择（GES）方法，显著提升了多样物体形状下类人灵巧抓取的自然性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习抓取方法虽然可以指导机器人把控先验，但常出现不自然的抓握动作，难以实现类人的灵巧操控，限制了实际应用。

Method: 作者提出ContactGrasp数据集，涵盖773个物体、82个类别，明确考虑任务相关的手腕姿态和拇食指配合。随后，利用几何聚类，将物体按形状分组，并提出两阶段的几何体专家选择（GES）架构，根据物体形状选取最适合的专家策略，提升适应性和泛化能力。

Result: 该方法在自然抓取姿态和成功率上表现出色，抓取成功率在训练集上达到99.4%，在测试集上达到96.3%，体现了良好的泛化能力和高质量的抓取性能。

Conclusion: ContactGrasp数据集和GES方法有效提升了机器人对多样未知物体的类人灵巧抓取能力，为智能机器人实际应用打下了坚实基础。

Abstract: Robust and human-like dexterous grasping of general objects is a critical
capability for advancing intelligent robotic manipulation in real-world
scenarios. However, existing reinforcement learning methods guided by grasp
priors often result in unnatural behaviors. In this work, we present
\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that
explicitly accounts for task-relevant wrist orientation and thumb-index
pinching coordination. The dataset covers 773 objects in 82 categories,
providing a rich foundation for training human-like grasp strategies. Building
upon this dataset, we perform geometry-based clustering to group objects by
shape, enabling a two-stage Geometry-based Expert Selection (GES) framework
that selects among specialized experts for grasping diverse object geometries,
thereby enhancing adaptability to diverse shapes and generalization across
categories. Our approach demonstrates natural grasp postures and achieves high
success rates of 99.4\% and 96.3\% on the train and test sets, respectively,
showcasing strong generalization and high-quality grasp execution.

</details>


### [595] [Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints](https://arxiv.org/abs/2509.23575)
*Jianshu Hu,Lidi Wang,Shujia Li,Yunpeng Jiang,Xiao Li,Paul Weng,Yutong Ban*

Main category: cs.RO

TL;DR: 本文提出了一种名为CLAP（Coarse-to-fine Language-Aligned manipulation Policy）的新型机器人层次化粗到细策略，大幅提升机器人3D操作的泛化能力，在仿真和真实机器人上均验证了其高效性和普适性。


<details>
  <summary>Details</summary>
Motivation: 传统的粗到细分层策略虽然提升了样本效率和操作精度，但在泛化到新任务和新环境时依旧存在明显不足，尤其是在复杂多变的实际应用下。作者希望通过增强策略泛化能力，使机器人能适应更多不同的操作指令和环境变化。

Method: 作者提出CLAP框架，融合三大核心：1）利用任务分解将复杂任务拆解为更易于学习的子任务；2）通过对视觉语言模型（VLM）进行微调，实现3D关键点的精准预测；3）引入包含3D空间信息的表征，提升策略对空间变化的适应性。

Result: 在GemBench基准测试集上，CLAP方法平均成功率较最新方法提高了12%，且训练所需样本数仅为其1/5。在实际机器人实验中，CLAP仅凭10次示范即可泛化到全新指令和环境，展现出极强的泛化能力和样本效率。

Conclusion: CLAP极大地提升了机器人操作在3D任务中的泛化能力和数据利用率，可有效学习到对新任务和新环境有较好适应性的操作策略，对实际场景下的机器人应用具有很大潜力。

Abstract: Hierarchical coarse-to-fine policy, where a coarse branch predicts a region
of interest to guide a fine-grained action predictor, has demonstrated
significant potential in robotic 3D manipulation tasks by especially enhancing
sample efficiency and enabling more precise manipulation. However, even
augmented with pre-trained models, these hierarchical policies still suffer
from generalization issues. To enhance generalization to novel instructions and
environment variations, we propose Coarse-to-fine Language-Aligned manipulation
Policy (CLAP), a framework that integrates three key components: 1) task
decomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware
representation. Through comprehensive experiments in simulation and on a real
robot, we demonstrate its superior generalization capability. Specifically, on
GemBench, a benchmark designed for evaluating generalization, our approach
achieves a 12\% higher average success rate than the SOTA method while using
only 1/5 of the training trajectories. In real-world experiments, our policy,
trained on only 10 demonstrations, successfully generalizes to novel
instructions and environments.

</details>


### [596] [Encoding Material Safety using Control Barrier Functions for Soft Actuator Control](https://arxiv.org/abs/2509.23623)
*Nicholas Pagliocca,Behrad Koohbor,Mitja Trkov*

Main category: cs.RO

TL;DR: 该论文提出了软体机器人中的材料安全的正式定义，并运用高阶控制屏障函数（HOCBF）与二次规划反馈控制实现对软体机器人的材料安全约束。通过仿真验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着软体机器人应用的增加，安全性变得尤为重要。但目前软体机器人安全的定义不明确，且材料失效和模型误差带来的安全风险尚未被有效解决。

Method: 作者基于应变能函数，给出了软体机器人材料安全的形式化定义。随后利用高阶控制屏障函数（HOCBF）与二次规划反馈控制器，针对不可压缩超弹性材料，划分出安全/不安全状态集合，并在有惯性与黏性效应的充气超弹性管道上进行建模与控制。

Result: 通过仿真实验，验证了所提出控制方法能够满足形式化的材料安全规范，能有效防止材料失效并确保系统按要求稳定运行。

Conclusion: 本文首次在软体机器人领域形式化地定义了材料安全，并提出了可以强制满足该安全规范的控制方法，对软体机器人安全研究具有重要意义。

Abstract: Until recently, the concept of soft robot safety was an informal notion,
often attributed solely to the fact that soft robots are less likely to damage
their operating environment than rigid robots. As the field moves toward
feedback control for practical applications, it becomes increasingly important
to define what safety means and to characterize how soft robots can become
unsafe. The unifying theme of soft robotics is to achieve useful functionality
through deformation. Consequently, limitations in constitutive model accuracy
and risks of material failure are inherent to all soft robots and pose a key
challenge in designing provably safe controllers. This work introduces a formal
definition of material safety based on strain energy functions and provides a
controller that enforces it. We characterize safe and unsafe sets of an
incompressible hyperelastic material and demonstrate that safety can be
enforced using a high-order control barrier function (HOCBF) with quadratic
program-based feedback control. As a case study, we consider a pressurized
hyperelastic tube with inertial effects, first-order viscous effects, and
full-state feedback. Simulation results verify that the proposed methodology
can enforce the material safety specification.

</details>


### [597] [KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe Egocentric Legged Locomotion](https://arxiv.org/abs/2509.23650)
*Peizhuo Li,Hongyi Li,Yuxuan Ma,Linnan Chang,Xinrong Yang,Ruiqi Yu,Yifeng Zhang,Yuhong Cao,Qiuguo Zhu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本文提出了一种名为KiVi的运动-视觉一体化框架，使四足机器人能够稳定地应对复杂、多变的地形环境，同时对视觉噪声和遮挡具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的步态控制方法虽然能够让机器人自主适应复杂环境，但视觉信息易受遮挡、反光和光照变化影响，导致机器人运动不稳定。受动物本体觉与视觉整合机制的启发，作者提出结合本体感觉与视觉的框架来提升鲁棒性。

Method: 提出KiVi框架，将本体感觉（kinesthetics）和视觉空间感知（visuospatial reasoning）信号分离处理。本体感觉为稳定主干，视觉只在需要感知地形及避障时介入。框架还利用带有记忆能力的注意力机制，实现多模态数据集成。

Result: 通过大量实验验证，KiVi能够让四足机器人在不同地形和非结构化室外环境下稳定运动，对视觉数据中的噪声和训练时未见过的遮挡保持鲁棒性。

Conclusion: KiVi框架提升了机器人步态对视觉脆弱性的鲁棒性，有效结合了多模态感知，具备较强的实际应用和扩展价值。

Abstract: Vision-based locomotion has shown great promise in enabling legged robots to
perceive and adapt to complex environments. However, visual information is
inherently fragile, being vulnerable to occlusions, reflections, and lighting
changes, which often cause instability in locomotion. Inspired by animal
sensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial
integration framework, where kinesthetics encodes proprioceptive sensing of
body motion and visuospatial reasoning captures visual perception of
surrounding terrain. Specifically, KiVi separates these pathways, leveraging
proprioception as a stable backbone while selectively incorporating vision for
terrain awareness and obstacle avoidance. This modality-balanced, yet
integrative design, combined with memory-enhanced attention, allows the robot
to robustly interpret visual cues while maintaining fallback stability through
proprioception. Extensive experiments show that our method enables quadruped
robots to stably traverse diverse terrains and operate reliably in unstructured
outdoor environments, remaining robust to out-of-distribution (OOD) visual
noise and occlusion unseen during training, thereby highlighting its
effectiveness and applicability to real-world legged locomotion.

</details>


### [598] [HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod Robot](https://arxiv.org/abs/2509.23651)
*Xinrong Yang,Peizhuo Li,Hongyi Li,Junkai Lu,Linnan Chang,Yuhong Cao,Yifeng Zhang,Ge Sun,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的分层全身操控框架HeLoM，使六足机器人能够稳定有效地推重物，在仿真和现实环境中均取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统抓取搬运重物对机器人抓手设计复杂且能效较低，而推运作为一种更直接有效的非抓取式操控策略，尤其在处理重物或形状不规则物体时更具优势，但推重物时需要保证机器人的推动力和整体稳定性，挑战较大。

Method: 提出了一种层次化全身操控方法（HeLoM），让六足机器人通过多足协作实现末端动作和稳定性的动态协调。高层规划器负责制定推运策略和物体目标位姿，低层控制器保障运动稳定、输出一致的关节动作。所有策略先经仿真训练后，无需微调直接应用于真实机器人。

Result: 仿真和实际实验表明，HeLoM能够让六足机器人在真实环境下稳定地将不同尺寸、未知物理属性的箱体推送到目标位姿。

Conclusion: HeLoM框架有效提升了六足机器人在现实复杂环境下全身稳定推重物的能力，表现出优良的泛化性和实用性。

Abstract: Robots in real-world environments are often required to move/manipulate
objects comparable in weight to their own bodies. Compared to grasping and
carrying, pushing provides a more straightforward and efficient non-prehensile
manipulation strategy, avoiding complex grasp design while leveraging direct
contact to regulate an object's pose. Achieving effective pushing, however,
demands both sufficient manipulation forces and the ability to maintain
stability, which is particularly challenging when dealing with heavy or
irregular objects. To address these challenges, we propose HeLoM, a
learning-based hierarchical whole-body manipulation framework for a hexapod
robot that exploits coordinated multi-limb control. Inspired by the cooperative
strategies of multi-legged insects, our framework leverages redundant contact
points and high degrees of freedom to enable dynamic redistribution of contact
forces. HeLoM's high-level planner plans pushing behaviors and target object
poses, while its low-level controller maintains locomotion stability and
generates dynamically consistent joint actions. Our policies trained in
simulation are directly deployed on real robots without additional fine-tuning.
This design allows the robot to maintain balance while exerting continuous and
controllable pushing forces through coordinated foreleg interaction and
supportive hind-leg propulsion. We validate the effectiveness of HeLoM through
both simulation and real-world experiments. Results show that our framework can
stably push boxes of varying sizes and unknown physical properties to
designated goal poses in the real world.

</details>


### [599] [Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models](https://arxiv.org/abs/2509.23655)
*Rokas Bendikas,Daniel Dijkman,Markus Peschl,Sanjay Haresh,Pietro Mazzaglia*

Main category: cs.RO

TL;DR: 本文提出了一种面向对象和机器人主体的视觉-语言-动作模型令牌化方法（Oat-VLA），大幅降低视觉输入的计算代价，并在效率和表现上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型（VLM）进行机器人操作学习计算开销过高，主要原因是视觉输入的令牌化方式效率低下。作者希望通过改进令牌化方案，提升训练效率与效果。

Method: 提出了Oat-VLA方法，基于面向对象的表示学习，为视觉输入引入关于场景中的对象与机器人自身信息的归纳偏置，显著减少了视觉令牌的数量。

Result: Oat-VLA在LIBERO套件中训练收敛速度是OpenVLA的两倍以上，并且在现实中多种搬运任务上表现优于OpenVLA，同时只需要极少的视觉令牌数。

Conclusion: Oat-VLA原理简单高效，显著提升了视觉-语言-动作模型在机器人任务上的训练效率，同时保持甚至提升了性能，具有很高的实际应用价值。

Abstract: Vision-Language-Action (VLA) models offer a pivotal approach to learning
robotic manipulation at scale by repurposing large pre-trained
Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs
for robotic domains comes with an unnecessarily high computational cost, which
we attribute to the tokenization scheme of visual inputs. In this work, we aim
to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric
Tokenization for VLAs. Building on the insights of object-centric
representation learning, our method introduces an inductive bias towards scene
objects and the agent's own visual information. As a result, we find that
Oat-VLA can drastically reduce the number of visual tokens to just a few tokens
without sacrificing performance. We reveal that Oat-VLA converges at least
twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in
diverse real-world pick and place tasks.

</details>


### [600] [Certifiably Optimal State Estimation and Robot Calibration Using Trace-Constrained SDP](https://arxiv.org/abs/2509.23656)
*Liangting Wu,Roberto Tron*

Main category: cs.RO

TL;DR: 论文利用轨迹约束的半正定规划（SDP）方法，将机器人中的非凸问题松弛为易于求解的凸问题，通过特定的约束及梯度投影方法更高效地获得全局最优解，并展示了在机器人估计和标定中的实际有效性。


<details>
  <summary>Details</summary>
Motivation: 许多机器人领域的非凸优化问题虽然能转化为SDP形式获得更好（全球最优）的理论解，但在实际应用中常因求解矩阵不是秩1导致效果不佳，且缺乏便捷通用的建模工具，限制了这些方法的推广和实用性。

Method: 1. 使用带固定轨迹约束的PSD矩阵提升松弛SDP的表达与解的质量。
2. 针对旋转、平移等常见机器人工程参数设计定制化变量和约束。
3. 提出基于梯度的精化过程，将松弛SDP解进一步投影到秩1低代价解空间，并通过对偶问题认证全局最优性。
4. 创设虚拟机器人抽象，方便不同场景下统一建模。

Result: 利用该方法，作者在PnP估计、手眼标定、双机械臂系统等多个机器人任务仿真实验中，成功获得更多秩1、低代价、可认证全局最优的解，且方法在建模与解算效率上优于以往做法。

Conclusion: 带轨迹约束的SDP及其配套的梯度精化和建模框架，为各类机器人非凸优化问题提供了高效且效果优良的求解方法，有望在实际工程中大范围推广。

Abstract: Many nonconvex problems in robotics can be relaxed into convex formulations
via semidefinite programming (SDP), which offers the advantage of global
optimality. The practical quality of these solutions, however, critically
depends on achieving rank-1 matrices, a condition that typically requires
additional tightening. In this work, we focus on trace-constrained SDPs, where
the decision variables are positive semidefinite (PSD) matrices with fixed
trace values. These additional constraints not only capture important
structural properties but also facilitate first-order methods for recovering
rank-1 solutions. We introduce customized fixed-trace variables and constraints
to represent common robotic quantities such as rotations and translations,
which can be exactly recovered when the corresponding variables are rank-1. To
further improve practical performance, we develop a gradient-based refinement
procedure that projects relaxed SDP solutions toward rank-1, low-cost
candidates, which can then be certified for global optimality via the dual
problem. We demonstrate that many robotics tasks can be expressed within this
trace-constrained SDP framework, and showcase its effectiveness through
simulations in perspective-n-point (PnP) estimation, hand-eye calibration, and
dual-robot system calibration. To support broader use, we also introduce a
modular ``virtual robot'' abstraction that simplifies modeling across different
problem settings.

</details>


### [601] [MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation](https://arxiv.org/abs/2509.23705)
*Jun Chen,Mingjia Chen,Shinkyu Park*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多机器人动态覆盖路径规划（MDCPP）算法，能够提高多机器人在二维环境下的覆盖效率，尤其适用于机器人速度需根据任务动态调整的现实应用场景。实验表明，该算法优于传统方法，并对通信范围对覆盖效率的影响进行了定量分析。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人覆盖路径规划假设机器人速度固定，但实际上机器人需依据具体任务动态调整速度，导致任务负载分配不均和总覆盖时间增加，因此急需解决负载均衡与效率提升问题。

Method: 作者提出MDCPP算法：利用高斯混合模型动态估算每个机器人剩余工作量，并用容量约束的Voronoi图分配覆盖区域。此外，算法还设计了分布式实现，适用于通信范围有限的机器人网络。

Result: 仿真结果显示，与现有清扫算法相比，MDCPP在覆盖效率和负载平衡方面表现更优。同时，论文对通信范围对系统性能的量化影响进行了分析。

Conclusion: MDCPP能够有效提升多机器人在二维环境下的覆盖效率，并具有更好的负载均衡能力，适用于实际中速度需动态调整的场景。分布式实现能力进一步拓宽了其实际应用场景。

Abstract: Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing
paths for multiple robots to effectively cover a large area of interest.
Conventional approaches to MCPP typically assume that robots move at fixed
velocities, which is often unrealistic in real-world applications where robots
must adapt their speeds based on the specific coverage tasks assigned to
them.Consequently, conventional approaches often lead to imbalanced workload
distribution among robots and increased completion time for coverage tasks. To
address this, we introduce a novel Multi-robot Dynamic Coverage Path Planning
(MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP
dynamically estimates each robot's remaining workload by approximating the
target distribution with Gaussian mixture models, and assigns coverage regions
using a capacity-constrained Voronoi diagram. We further develop a distributed
implementation of MDCPP for range-constrained robotic networks. Simulation
results validate the efficacy of MDCPP, showing qualitative improvements and
superior performance compared to an existing sweeping algorithm, and a
quantifiable impact of communication range on coverage efficiency.

</details>


### [602] [DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware Motion Manifold Primitives](https://arxiv.org/abs/2509.23721)
*Chi Chu,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种新的运动生成框架DA-MMP，有效提升了机器人在动态操控任务（如掷环）中的表现，生成更协调平滑的动作轨迹，且实际表现超越人类专家并具备较强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人动态操控方法大多依赖人工设计的动作参数化方式，难以实现复杂任务中所需的高协调动作；而运动规划虽能生成可行轨迹，但由于动力学差距，计划与实际执行结果常有偏差，因此亟需一种能兼顾动作协调性与动力学适应性的运动生成方法。

Method: 作者提出Dynamics-Aware Motion Manifold Primitives（DA-MMP）框架，通过紧凑参数化将动作基元拓展到可变长轨迹，并利用大量计划轨迹数据学习高质量动作流形；随后在潜在空间中结合少量真实试验以条件流匹配模型进行训练，从而生成能适应真实动力学的投掷轨迹。

Result: 本方法在现实世界掷环任务中，生成了协调平滑的动作轨迹；实验表明，实际表现获得了较高成功率，甚至优于训练有素的人类专家，并且可推广到训练范围外的新目标。

Conclusion: DA-MMP方法能够有效桥接规划与动力学差距，提升动态操控任务中的机器人表现，对于目标条件变化也具有良好泛化能力，推进了机器人高难度运动技能的发展。

Abstract: Dynamic manipulation is a key capability for advancing robot performance,
enabling skills such as tossing. While recent learning-based approaches have
pushed the field forward, most methods still rely on manually designed action
parameterizations, limiting their ability to produce the highly coordinated
motions required in complex tasks. Motion planning can generate feasible
trajectories, but the dynamics gap-stemming from control inaccuracies, contact
uncertainties, and aerodynamic effects-often causes large deviations between
planned and executed trajectories. In this work, we propose Dynamics-Aware
Motion Manifold Primitives (DA-MMP), a motion generation framework for
goal-conditioned dynamic manipulation, and instantiate it on a challenging
real-world ring-tossing task. Our approach extends motion manifold primitives
to variable-length trajectories through a compact parametrization and learns a
high-quality manifold from a large-scale dataset of planned motions. Building
on this manifold, a conditional flow matching model is trained in the latent
space with a small set of real-world trials, enabling the generation of
throwing trajectories that account for execution dynamics. Experiments show
that our method can generate coordinated and smooth motion trajectories for the
ring-tossing task. In real-world evaluations, it achieves high success rates
and even surpasses the performance of trained human experts. Moreover, it
generalizes to novel targets beyond the training range, indicating that it
successfully learns the underlying trajectory-dynamics mapping.

</details>


### [603] [LocoFormer: Generalist Locomotion via Long-context Adaptation](https://arxiv.org/abs/2509.23745)
*Min Liu,Deepak Pathak,Ananye Agarwal*

Main category: cs.RO

TL;DR: LocoFormer 是一种通用型的全身态运动模型，无需精确运动学信息就能控制各种腿式与轮式机器人，并能适应形态和动力学的变化。其核心通过大规模强化学习和更长的上下文扩展，实现了对不同机器人的稳健控制及跨回合的自适应能力。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人运动控制器需要为特定机器人手动调参，缺乏通用性，遇到新型机器人或形态变化时适应能力弱。本文旨在研究一种能泛化到不同机器人类型、自动适应形态动力学变化的全能运动模型。

Method: 提出 LocoFormer，采用大规模强化学习，在程序生成的多样机器人和强随机化环境下训练。同时，将策略模型的上下文窗口大幅扩展至跨越回合，以吸收更多经验。不同于以往方法，模型可利用更长历史来习得适应能力。

Result: LocoFormer 可在未见过的腿式和轮式机器人之间迁移控制能力，即使对机器人动力学和形态缺乏精确信息也能稳定运行。面对体重变化、电机故障等极端干扰依然表现出鲁棒性。更重要的是，模型能在多回合中从失败中自适应学习改进。

Conclusion: 大规模 RL、激进域随机化与长上下文窗口三者结合，使 LocoFormer 能广泛适应不同形态与动态的机器人控制。该通用方法有望推广至其他机器人技能的基础模型训练。

Abstract: Modern locomotion controllers are manually tuned for specific embodiments. We
present LocoFormer, a generalist omni-bodied locomotion model that can control
previously unseen legged and wheeled robots, even without precise knowledge of
their kinematics. LocoFormer is able to adapt to changes in morphology and
dynamics at test time. We find that two key choices enable adaptation. First,
we train massive scale RL on procedurally generated robots with aggressive
domain randomization. Second, in contrast to previous policies that are myopic
with short context lengths, we extend context by orders of magnitude to span
episode boundaries. We deploy the same LocoFormer to varied robots and show
robust control even with large disturbances such as weight change and motor
failures. In extreme scenarios, we see emergent adaptation across episodes,
LocoFormer learns from falls in early episodes to improve control strategies in
later ones. We believe that this simple, yet general recipe can be used to
train foundation models for other robotic skills in the future. Videos at
generalist-locomotion.github.io.

</details>


### [604] [Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse](https://arxiv.org/abs/2509.23778)
*Zeyuan Zhang,Chaoran Li,Shao Zhang,Ying Wen*

Main category: cs.RO

TL;DR: 针对多智能体拾取与投递（MAPD）任务，提出了一种基于Transformer的信息隐式交换网络SePar，大幅提升分布式决策效率与全局感知能力，并在多种任务和场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的方法在类仓库环境（如狭窄通道、长走廊）下，由于信息局限性，决策性能较差。基于通信的改进虽有助于补足全局信息，但点对点通信引入了高计算复杂度。

Method: 把多智能体路径规划（MAPF）问题建模为序列建模问题，并证明其序列模型下的决策政策具备顺序无关的最优性。基于此，设计了SePar模型，利用Transformer机制实现隐式信息交换，将决策复杂度从指数级降为线性。

Result: 实验结果显示，SePar始终优于现有的学习方法，且能很好地泛化到未见过的新环境。同时，作者强调在复杂仓库地图下融合模仿学习的重要性。

Conclusion: SePar有效提升了多智能体拾取与投递任务的分布式决策效率和全局感知能力，是在实际复杂环境下更具竞争力的解决方案。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of
Multi-Agent Path Finding (MAPF), where agents are required to sequentially
complete tasks with fixed-location pickup and delivery demands. Although
learning-based methods have made progress in MAPD, they often perform poorly in
warehouse-like environments with narrow pathways and long corridors when
relying only on local observations for distributed decision-making.
Communication learning can alleviate the lack of global information but
introduce high computational complexity due to point-to-point communication. To
address this challenge, we formulate MAPF as a sequence modeling problem and
prove that path-finding policies under sequence modeling possess
order-invariant optimality, ensuring its effectiveness in MAPD. Building on
this, we propose the Sequential Pathfinder (SePar), which leverages the
Transformer paradigm to achieve implicit information exchange, reducing
decision-making complexity from exponential to linear while maintaining
efficiency and global awareness. Experiments demonstrate that SePar
consistently outperforms existing learning-based methods across various MAPF
tasks and their variants, and generalizes well to unseen environments.
Furthermore, we highlight the necessity of integrating imitation learning in
complex maps like warehouses.

</details>


### [605] [High-Precision Climbing Robot Localization Using Planar Array UWB/GPS/IMU/Barometer Integration](https://arxiv.org/abs/2509.23801)
*Shuning Zhang,Renjing Xu,Zhanchen Zhu,Xiangyu Chen,Yunheng Wang,Xu Jiang,Peibo Duan*

Main category: cs.RO

TL;DR: 本文提出了一种基于多传感器融合（包括UWB、GPS、IMU和气压计）的高精度定位系统，应用于复杂高空环境下攀爬机器人，显著提升了定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单一传感器在复杂高空环境下（如GPS遮挡、UWB非视距等）定位困难，无法满足攀爬机器人对高精度定位的需求。

Method: 分析典型定位场景，设计Attention机制融合算法（AMFA）结构，将UWB、GPS、IMU和气压计集成。为UWB和气压计开发端到端神经网络推理模型，并引入多模态注意力机制自适应融合多源数据，联合无迹卡尔曼滤波（UKF）优化轨迹。

Result: 实际环境实验表明，该方法定位精度达0.48米，最大误差1.50米，显著优于GPS/INS-EKF等传统基线算法。

Conclusion: 所提多传感器融合系统能有效应对传感器不足和环境挑战，为高空复杂环境下攀爬机器人应用提供了高精度并具鲁棒性的定位能力。

Abstract: To address the need for high-precision localization of climbing robots in
complex high-altitude environments, this paper proposes a multi-sensor fusion
system that overcomes the limitations of single-sensor approaches. Firstly, the
localization scenarios and the problem model are analyzed. An integrated
architecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating
planar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and
barometer is designed to handle challenges such as GPS occlusion and UWB
Non-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference
models for UWB and barometer are developed, along with a multimodal attention
mechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied
to refine the trajectory, improving accuracy and robustness. Finally,
real-world experiments show that the method achieves 0.48 m localization
accuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such
as GPS/INS-EKF and demonstrating stronger robustness.

</details>


### [606] [Fostering Robots: A Governance-First Conceptual Framework for Domestic, Curriculum-Based Trajectory Collection](https://arxiv.org/abs/2509.23821)
*Federico Pablo-Marti,Carlos Mir Fernandez*

Main category: cs.RO

TL;DR: 本文提出了一种以治理优先、课程驱动的家用机器人部署框架，强调长期的、经策划的交互过程，并给出量化评估标准和实验路线图。


<details>
  <summary>Details</summary>
Motivation: 当前家用机器人部署面临治理和长期交互管理不足的问题，缺乏系统性和可验证的框架来保障其社会和道德合规。

Method: 作者提出以“机器人养育”为核心理念，制定课程驱动、治理优先的部署方案，并通过可量化的轨迹质量评估指标和协议，结合欧盟治理标准，设计低资源但有效的实证验证路径。

Result: 明确了机器人交互轨迹的量化评估指标和验证流程，为未来开展试点研究和实际部署提供标准化、可执行的实验蓝图。

Conclusion: 该框架有望提升家用机器人治理水平，促进其在日常环境中的合规、安全和可持续发展，同时为后续实证研究奠定基础。

Abstract: We propose a conceptual, empirically testable framework for Robot Fostering,
-a curriculum-driven, governance-first approach to domestic robot deployments,
emphasizing long-term, curated interaction trajectories. We formalize
trajectory quality with quantifiable metrics and evaluation protocols aligned
with EU-grade governance standards, delineating a low-resource empirical
roadmap to enable rigorous validation through future pilot studies.

</details>


### [607] [Control Your Robot: A Unified System for Robot Control and Policy Deployment](https://arxiv.org/abs/2509.23823)
*Tian Nian,Weijie Ke,Yao Mu,Tianxing Chen,Shaolong Zhu,Bingshan Hu*

Main category: cs.RO

TL;DR: 本文提出了一个名为Control Your Robot的通用化模块化机器人控制框架，能够简化不同平台间的数据采集和控制策略部署，实现高效的跨平台机器人学习。


<details>
  <summary>Details</summary>
Motivation: 不同机器人平台间的软件接口、数据格式和控制方式差异很大，导致工具链分裂，部署效率低。缺乏一个能够统一标准化流程和简化工作的通用方案。

Method: 设计了模块化、统一API的闭环机器人控制系统，包括灵活的机器人注册、遥操作和轨迹回放两种控制模式，从多模态数据采集到推断全流程无缝集成，适配多种平台。

Result: 在单臂和双臂机器人系统上进行实验，结果显示系统能实现高效低延迟的数据采集，并支持模仿学习和视觉-语言-动作模型的策略学习。所采集数据训练出的策略能很好复现专家演示。

Conclusion: Control Your Robot框架有效促进了横跨多种平台的可扩展、可复现的机器人学习，降低了跨平台开发和部署的难度。

Abstract: Cross-platform robot control remains difficult because hardware interfaces,
data formats, and control paradigms vary widely, which fragments toolchains and
slows deployment. To address this, we present Control Your Robot, a modular,
general-purpose framework that unifies data collection and policy deployment
across diverse platforms. The system reduces fragmentation through a
standardized workflow with modular design, unified APIs, and a closed-loop
architecture. It supports flexible robot registration, dual-mode control with
teleoperation and trajectory playback, and seamless integration from multimodal
data acquisition to inference. Experiments on single-arm and dual-arm systems
show efficient, low-latency data collection and effective support for policy
learning with imitation learning and vision-language-action models. Policies
trained on data gathered by Control Your Robot match expert demonstrations
closely, indicating that the framework enables scalable and reproducible robot
learning across platforms.

</details>


### [608] [DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation](https://arxiv.org/abs/2509.23829)
*Kefei Zhu,Fengshuo Bai,YuanHao Xiang,Yishuai Cai,Xinglin Chen,Ruochong Li,Xingtao Wang,Hao Dong,Yaodong Yang,Xiaopeng Fan,Yuanpei Chen*

Main category: cs.RO

TL;DR: 提出了DexFlyWheel框架，实现灵巧操作机器人数据的高效、持续、多样化生成，提升机器人通用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前用于灵巧操作机器人的高质量、多样化数据稀缺，现有方法依赖人工远程操作或大量工程人力，扩展性和多样性有限，无法满足大规模和实际应用需求。

Method: DexFlyWheel采用自我提升的循环流程数据生成框架：初始用高效示范数据热身，然后通过模仿学习（IL）提取类似人类的行为，基于残差强化学习（RL）增强策略泛化能力。学到的策略放入仿真环境生成新轨迹，通过数据增强进一步丰富多样性，这些数据再反馈进入下一迭代，实现封闭循环，实现“飞轮效应”。

Result: DexFlyWheel在四项具有挑战性的任务上生成了超过2000条多样化示范。基于这些数据训练的策略在测试集上平均成功率达到81.9%，并可通过数字孪生成功迁移到现实世界，双臂举升任务实际成功率达78.3%。

Conclusion: DexFlyWheel能有效提升机器人灵巧操作任务的数据规模和多样性，增强策略泛化能力和现实可用性，是推进真实环境中机器人灵巧操作能力的重要进展。

Abstract: Dexterous manipulation is critical for advancing robot capabilities in
real-world applications, yet diverse and high-quality datasets remain scarce.
Existing data collection methods either rely on human teleoperation or require
significant human engineering, or generate data with limited diversity, which
restricts their scalability and generalization. In this paper, we introduce
DexFlyWheel, a scalable data generation framework that employs a self-improving
cycle to continuously enrich data diversity. Starting from efficient seed
demonstrations warmup, DexFlyWheel expands the dataset through iterative
cycles. Each cycle follows a closed-loop pipeline that integrates Imitation
Learning (IL), residual Reinforcement Learning (RL), rollout trajectory
collection, and data augmentation. Specifically, IL extracts human-like
behaviors from demonstrations, and residual RL enhances policy generalization.
The learned policy is then used to generate trajectories in simulation, which
are further augmented across diverse environments and spatial configurations
before being fed back into the next cycle. Over successive iterations, a
self-improving data flywheel effect emerges, producing datasets that cover
diverse scenarios and thereby scaling policy performance. Experimental results
demonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across
four challenging tasks. Policies trained on our dataset achieve an average
success rate of 81.9\% on the challenge test sets and successfully transfer to
the real world through digital twin, achieving a 78.3\% success rate on
dual-arm lift tasks.

</details>


### [609] [MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control](https://arxiv.org/abs/2509.23960)
*Manan Tayal,Aditya Singh,Shishir Kolathaya,Somil Bansal*

Main category: cs.RO

TL;DR: 该论文提出MAD-PINN框架，通过物理引导神经网络在多智能体系统中同时优化安全与性能，兼顾可扩展性，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多智能体系统中，安全与性能的协同优化极具挑战。现有MARL、安全过滤、MPC等方法难以兼顾严格安全性、保守性或可扩展性，因此需要新的解决方案。

Method: 提出MAD-PINN框架，将状态约束最优控制问题通过epigraph重构，利用物理引导神经网络近似解。通过在小规模系统中训练值函数后以去中心化方式部署，每个智能体仅基于邻居本地观测决策。同时，采用HJ可达性方法优选邻居并引入滑动时域决策策略，提升安全性及计算效率。

Result: 在多智能体导航实验中，MAD-PINN较现有先进方法表现出更优的安全-性能权衡，并且在智能体数量增长时仍保持良好可扩展性。

Conclusion: MAD-PINN不仅能实现更好的安全与性能协同，还能扩展至大规模智能体系统应用，为多智能体系统的安全最优控制提供了有效新思路。

Abstract: Co-optimizing safety and performance in large-scale multi-agent systems
remains a fundamental challenge. Existing approaches based on multi-agent
reinforcement learning (MARL), safety filtering, or Model Predictive Control
(MPC) either lack strict safety guarantees, suffer from conservatism, or fail
to scale effectively. We propose MAD-PINN, a decentralized physics-informed
machine learning framework for solving the multi-agent state-constrained
optimal control problem (MASC-OCP). Our method leverages an epigraph-based
reformulation of SC-OCP to simultaneously capture performance and safety, and
approximates its solution via a physics-informed neural network. Scalability is
achieved by training the SC-OCP value function on reduced-agent systems and
deploying them in a decentralized fashion, where each agent relies only on
local observations of its neighbours for decision-making. To further enhance
safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based
neighbour selection strategy to prioritize safety-critical interactions, and a
receding-horizon policy execution scheme that adapts to dynamic interactions
while reducing computational burden. Experiments on multi-agent navigation
tasks demonstrate that MAD-PINN achieves superior safety-performance
trade-offs, maintains scalability as the number of agents grows, and
consistently outperforms state-of-the-art baselines.

</details>


### [610] [Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras](https://arxiv.org/abs/2509.24094)
*Vignesh Ramanathan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Flash的轻量级视觉地点识别（VPR）系统，实现了基于事件相机的亚毫秒级地点识别，显著提升了识别速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 传统事件相机VPR方法虽然具有高时间分辨率，但往往需依赖于稠密化稀疏事件数据并需要较长时间窗口才能实现识别，限制了机器人在实时和高动态场景下的定位响应速度。为此，亟需更高效、快速的解决方案。

Method: 作者发现事件相机‘活跃像素位置’本身具有很强的判别力，并基于此提出以二值帧高效编码活跃像素，通过快速位运算计算帧间相似性，同时根据查询帧与参考帧间的事件活跃度进行归一化，最终实现高效亚毫秒级VPR。

Result: Flash在QCR-Event-Dataset和Brisbane-Event-VPR数据集上，分别将Recall@1提升11.33倍和5.92倍，并有效降低机器人定位失配持续时间（即Time to Correct Match，TCM）。

Conclusion: Flash首次实现了基于事件相机的亚毫秒级VPR，大幅提升了地点识别的速度和鲁棒性，为自主导航系统带来了更实时、更高效的地标识别能力。

Abstract: Visual Place Recognition (VPR) enables systems to identify previously visited
locations within a map, a fundamental task for autonomous navigation. Prior
works have developed VPR solutions using event cameras, which asynchronously
measure per-pixel brightness changes with microsecond temporal resolution.
However, these approaches rely on dense representations of the inherently
sparse camera output and require tens to hundreds of milliseconds of event data
to predict a place. Here, we break this paradigm with Flash, a lightweight VPR
system that predicts places using sub-millisecond slices of event data. Our
method is based on the observation that active pixel locations provide strong
discriminative features for VPR. Flash encodes these active pixel locations
using efficient binary frames and computes similarities via fast bitwise
operations, which are then normalized based on the relative event activity in
the query and reference frames. Flash improves Recall@1 for sub-millisecond VPR
over existing baselines by 11.33x on the indoor QCR-Event-Dataset and 5.92x on
the 8 km Brisbane-Event-VPR dataset. Moreover, our approach reduces the
duration for which the robot must operate without awareness of its position, as
evidenced by a localization latency metric we term Time to Correct Match (TCM).
To the best of our knowledge, this is the first work to demonstrate
sub-millisecond VPR using event cameras.

</details>


### [611] [Ancestry Tree Clustering for Particle Filter Diversity Maintenance](https://arxiv.org/abs/2509.24124)
*Ilari Vallivaara,Bingnan Duan,Yinhuan Dong,Tughrul Arslan*

Main category: cs.RO

TL;DR: 本文提出了一种基于谱系树结构的线性时间粒子滤波多样性维护方法，通过将粒子按谱系树拓扑聚类，有效防止算法过早收敛，并在多峰环境中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波在多模态（多峰值）环境下容易发生早收敛，导致表现不佳，现有的多样性维护策略要么计算复杂要么效果有限。

Method: 通过分析粒子在谱系树上的关系，将有密切谱系关系且足够大的子树内的粒子聚类，不依赖空间或领域特定的度量。对每个簇内进行适应度共享，并保护未被聚类的粒子，从而促进样本多样性。

Result: 在多模态机器人仿真和实际室内环境实验中，本方法与现有的多样性维护算法（如确定性重采样和粒子高斯混合）对比，表现出高成功率且几乎没有负面影响于估计紧致性，在不同领域和初始条件下均显示出较强鲁棒性。

Conclusion: 所提出的算法可有效防止粒子滤波中的早收敛，且方法通用性强、实现高效，对实际多模态问题具有更强适应性和可靠性。

Abstract: We propose a method for linear-time diversity maintenance in particle
filtering. It clusters particles based on ancestry tree topology: closely
related particles in sufficiently large subtrees are grouped together. The main
idea is that the tree structure implicitly encodes similarity without the need
for spatial or other domain-specific metrics. This approach, when combined with
intra-cluster fitness sharing and the protection of particles not included in a
cluster, effectively prevents premature convergence in multimodal environments
while maintaining estimate compactness. We validate our approach in a
multimodal robotics simulation and a real-world multimodal indoor environment.
We compare the performance to several diversity maintenance algorithms from the
literature, including Deterministic Resampling and Particle Gaussian Mixtures.
Our algorithm achieves high success rates with little to no negative effect on
compactness, showing particular robustness to different domains and challenging
initial conditions.

</details>


### [612] [BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes](https://arxiv.org/abs/2509.24126)
*Athanasios Bacharis,Konstantinos D. Polyzos,Georgios B. Giannakis,Nikolaos Papanikolopoulos*

Main category: cs.RO

TL;DR: 本论文提出了一种基于贝叶斯优化的视角规划（VP）新框架，在少量高质量的2D图像下实现高效农业环境的3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量随意采集的2D图片用于3D重建，操作繁琐、效率低。且在实际应用时，相机位置噪声和环境的变化性使得优化变得复杂。作者希望提出一种高效、鲁棒、可推广的视角规划方法。

Method: 采用基于重建质量的优化目标，将“结构光流（structure-from-motion）”作为3D重建算法，结合贝叶斯优化在函数未知、评估高成本下高效搜索最优相机布置，并处理不同噪声情形。

Result: 在仿真和真实农业场景下进行了数值测试，结果显示该VP方法能高效估算最优相机布局，实现高精度3D重建，且在未知但类似的环境中具有良好泛化能力。

Conclusion: 所提基于贝叶斯优化的VP框架能在环境复杂、数据有限情况下高效完成3D重建，具有实际农业应用前景，并能适应不同但相似的新环境，无需重新训练或优化。

Abstract: Active vision (AV) has been in the spotlight of robotics research due to its
emergence in numerous applications including agricultural tasks such as
precision crop monitoring and autonomous harvesting to list a few. A major AV
problem that gained popularity is the 3D reconstruction of targeted
environments using 2D images from diverse viewpoints. While collecting and
processing a large number of arbitrarily captured 2D images can be arduous in
many practical scenarios, a more efficient solution involves optimizing the
placement of available cameras in 3D space to capture fewer, yet more
informative, images that provide sufficient visual information for effective
reconstruction of the environment of interest. This process termed as view
planning (VP), can be markedly challenged (i) by noise emerging in the location
of the cameras and/or in the extracted images, and (ii) by the need to
generalize well in other unknown similar agricultural environments without need
for re-optimizing or re-training. To cope with these challenges, the present
work presents a novel VP framework that considers a reconstruction
quality-based optimization formulation that relies on the notion of
`structure-from-motion' to reconstruct the 3D structure of the sought
environment from the selected 2D images. With no analytic expression of the
optimization function and with costly function evaluations, a Bayesian
optimization approach is proposed to efficiently carry out the VP process using
only a few function evaluations, while accounting for different noise cases.
Numerical tests on both simulated and real agricultural settings signify the
benefits of the advocated VP approach in efficiently estimating the optimal
camera placement to accurately reconstruct 3D environments of interest, and
generalize well on similar unknown environments.

</details>


### [613] [Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress](https://arxiv.org/abs/2509.24129)
*Priyanka Mandikal,Jiaheng Hu,Shivin Dass,Sagnik Majumder,Roberto Martín-Martín,Kristen Grauman*

Main category: cs.RO

TL;DR: 本文提出了SPARTA框架，用于实现机器人对物体状态逐步变化的操作任务，比如捣碎、撒布和切片，显著提升了训练效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作主要针对物体的位置或姿态改变，但现实中如捣碎、撒布等任务涉及物体逐步变化，需要新的方法来感知和掌控这类状态变化。

Method: 提出SPARTA框架，将任务建模为空间逐步推进的状态变化，采用基于视觉的分割图来区分物体的“可操作”区域与“已变化”区域，并结合稠密奖励信号，同时采用强化学习和贪婪控制两种策略完成任务。

Result: 在真实机器人和10种多样化物体上进行3类复杂任务的实证，SPARTA在训练时间和操作精度上均大幅超越传统稀疏奖励和视觉目标基准方法。

Conclusion: 基于状态变化感知的视觉表示为复杂物体操作提供了有效手段，SPARTA为处理各类物体状态逐步变化的任务奠定了坚实基础。

Abstract: Most robot manipulation focuses on changing the kinematic state of objects:
picking, placing, opening, or rotating them. However, a wide range of
real-world manipulation tasks involve a different class of object state
change--such as mashing, spreading, or slicing--where the object's physical and
visual state evolve progressively without necessarily changing its position. We
present SPARTA, the first unified framework for the family of object state
change manipulation tasks. Our key insight is that these tasks share a common
structural pattern: they involve spatially-progressing, object-centric changes
that can be represented as regions transitioning from an actionable to a
transformed state. Building on this insight, SPARTA integrates spatially
progressing object change segmentation maps, a visual skill to perceive
actionable vs. transformed regions for specific object state change tasks, to
generate a) structured policy observations that strip away appearance
variability, and b) dense rewards that capture incremental progress over time.
These are leveraged in two SPARTA policy variants: reinforcement learning for
fine-grained control without demonstrations or simulation; and greedy control
for fast, lightweight deployment. We validate SPARTA on a real robot for three
challenging tasks across 10 diverse real-world objects, achieving significant
improvements in training time and accuracy over sparse rewards and visual
goal-conditioned baselines. Our results highlight progress-aware visual
representations as a versatile foundation for the broader family of object
state manipulation tasks. Project website:
https://vision.cs.utexas.edu/projects/sparta-robot

</details>


### [614] [A Novel Model for 3D Motion Planning for a Generalized Dubins Vehicle with Pitch and Yaw Rate Constraints](https://arxiv.org/abs/2509.24143)
*Deepak Prakash Kumar,Swaroop Darbha,Satyanarayana Gupta Manyam,David Casbeer*

Main category: cs.RO

TL;DR: 本文提出了一种新颖且高效的3D运动规划建模方法和快速算法，主要针对固定翼无人机，实现满足运动约束条件下的最短路径规划。相比主流方法，突出考虑了完整的飞行器姿态和双输入控制，利用旋转最小化框架与拼接Dubins最优路径，有效提升路径可行性与最优性。


<details>
  <summary>Details</summary>
Motivation: 现有3D路径规划方法大多只考虑飞行器的俯仰或航向角，无法唯一确定完整的空间姿态，且通常采用单一输入模型（如路径曲率），不适合准确模拟固定翼无人机在三维空间中的真实运动。本文旨在解决这些建模不足，提高路径的准确性与可行性。

Method: 作者基于与飞行器刚体绑定的坐标系，将姿态的滚转、俯仰和偏航角全部纳入状态变量。采用两个独立控制输入来表述受限于物理约束的俯仰和偏航速率。方法上利用旋转最小化框架刻画飞行器姿态演变，通过在球面、柱面和平面上拼接Dubins最优路径进行最短路径规划。

Result: 仿真实验表明，该方法能在平均10秒内生成可行路径，并且多数情况下生成的路径比现有主流方法更短。

Conclusion: 本文方法能够更准确模拟固定翼无人机的真实运动，提升了三维路径规划的效率和路径质量，对实际无人机路径规划有重要意义。

Abstract: In this paper, we propose a new modeling approach and a fast algorithm for 3D
motion planning, applicable for fixed-wing unmanned aerial vehicles. The goal
is to construct the shortest path connecting given initial and final
configurations subject to motion constraints. Our work differs from existing
literature in two ways. First, we consider full vehicle orientation using a
body-attached frame, which includes roll, pitch, and yaw angles. However,
existing work uses only pitch and/or heading angle, which is insufficient to
uniquely determine orientation. Second, we use two control inputs to represent
bounded pitch and yaw rates, reflecting control by two separate actuators. In
contrast, most previous methods rely on a single input, such as path curvature,
which is insufficient for accurately modeling the vehicle's kinematics in 3D.
We use a rotation minimizing frame to describe the vehicle's configuration and
its evolution, and construct paths by concatenating optimal Dubins paths on
spherical, cylindrical, or planar surfaces. Numerical simulations show our
approach generates feasible paths within 10 seconds on average and yields
shorter paths than existing methods in most cases.

</details>


### [615] [Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation](https://arxiv.org/abs/2509.24160)
*Tomoyuki Kagaya,Subramanian Lakshmi,Yuxuan Lou,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: 提出了一种名为Memory Transfer Planning (MTP)的新框架，通过利用以往环境中成功的控制代码案例，提升大语言模型（LLM）在机器人操作任务中的适应性和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在机器人操作中难以适应新环境，需要专门训练或频繁调参，缺乏通用性和可迁移性。作者旨在解决这些痛点。

Method: MTP框架分为三步：(1) 用LLM自动生成初始计划和代码；(2) 从存储库中检索与当前任务相关的成功代码案例；(3) 在不更新模型参数情况下，将检索到的代码进行情境适配，以改善后续规划。

Result: 实验在RLBench、CALVIN以及真实机器人平台上进行，结果显示MTP显著提升了成功率和环境适应性，优于固定prompt、简单检索和无记忆的重规划方法。

Conclusion: MTP有效利用程序性知识，提升了LLM驱动的机器人规划在不同环境中的适用性和稳健性，为仿真到现实的落地提供了有力方案。

Abstract: Large language models (LLMs) are increasingly explored in robot manipulation,
but many existing methods struggle to adapt to new environments. Many systems
require either environment-specific policy training or depend on fixed prompts
and single-shot code generation, leading to limited transferability and manual
re-tuning. We introduce Memory Transfer Planning (MTP), a framework that
leverages successful control-code examples from different environments as
procedural knowledge, using them as in-context guidance for LLM-driven
planning. Specifically, MTP (i) generates an initial plan and code using LLMs,
(ii) retrieves relevant successful examples from a code memory, and (iii)
contextually adapts the retrieved code to the target setting for re-planning
without updating model parameters. We evaluate MTP on RLBench, CALVIN, and a
physical robot, demonstrating effectiveness beyond simulation. Across these
settings, MTP consistently improved success rate and adaptability compared with
fixed-prompt code generation, naive retrieval, and memory-free re-planning.
Furthermore, in hardware experiments, leveraging a memory constructed in
simulation proved effective. MTP provides a practical approach that exploits
procedural knowledge to realize robust LLM-based planning across diverse
robotic manipulation scenarios, enhancing adaptability to novel environments
and bridging simulation and real-world deployment.

</details>


### [616] [Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models](https://arxiv.org/abs/2509.24163)
*Wanming Yu,Adrian Röfer,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出使用多模态大型语言模型（LLMs）作为机器人长时序堆叠任务的高层规划器，通过多模态输入和自定义数据集微调，显著提升对物体物理属性的推理与任务完成度。


<details>
  <summary>Details</summary>
Motivation: 传统预训练LLM在复杂机器人操控任务（如容器堆叠涉及隐含物理属性时）知晓性和推理能力有限，需要更好融入物理知识以指导实际操作。

Method: 将多模态LLM作为高层规划器，输入关于待堆叠物体的多模态信息；设计并使用兼顾重量、稳定性、尺寸、支撑面积等物理偏好的自定义数据集微调LLM，提高其无明确指令下多偏好推理能力。

Result: 在大规模仿真实验中，相比仅有提示词调优的LLM，微调后模型堆叠完成率明显提升。在真实类人机器人上的在线实验也验证了方法有效性。

Conclusion: 通过结合多模态输入和自定义微调，LLM能够更好地作为复杂长时序机器操控任务的高层规划器，具备对重要物理属性的推理能力和实际应用潜力。

Abstract: Pretrained large language models (LLMs) can work as high-level robotic
planners by reasoning over abstract task descriptions and natural language
instructions, etc. However, they have shown a lack of knowledge and
effectiveness in planning long-horizon robotic manipulation tasks where the
physical properties of the objects are essential. An example is the stacking of
containers with hidden objects inside, which involves reasoning over hidden
physics properties such as weight and stability. To this end, this paper
proposes to use multimodal LLMs as high-level planners for such long-horizon
robotic stacking tasks. The LLM takes multimodal inputs for each object to
stack and infers the current best stacking sequence by reasoning over stacking
preferences. Furthermore, in order to enable the LLM to reason over multiple
preferences at the same time without giving explicit instructions, we propose
to create a custom dataset considering stacking preferences including weight,
stability, size, and footprint, to fine-tune the LLM. Compared to the
pretrained LLM with prompt tuning, we demonstrate the improved stacking
completion of the LLM fine-tuned with our custom dataset via large-scale
simulation evaluation. Furthermore, we showcase the effectiveness of the
proposed framework for the long-horizon stacking task on a real humanoid robot
in an online manner.

</details>


### [617] [Very High Frequency Interpolation for Direct Torque Control](https://arxiv.org/abs/2509.24175)
*Rafael Kourdis,Maciej Stępień,Jérôme Manhes,Nicolas Mansard,Steve Tonneau,Philippe Souères,Thomas Flayols*

Main category: cs.RO

TL;DR: 本文提出了一种在开源硬件上以高达40 kHz的频率稳定执行全身线性反馈的新方法，有效提升了力矩控制机器人的运动稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 力矩控制虽然可带来灵活、鲁棒的机器人运动，但实际部署经常受制于系统不稳定和硬件性能限制。作者希望能找到兼具高性能与稳定性的控制方案，突破现有力矩控制机器人发展的瓶颈。

Method: 在开源硬件上实现了一套可高频（40 kHz）执行的全身线性反馈方案。其核心思路是用高频线性反馈稳住系统，同时在实际运行过程中对非线性控制（如逆动力学和学习到的力矩策略）进行插值融合。

Result: 实验证明，通过提升线性反馈控制频率，可以显著提升力矩控制器的稳定性，使原本不易部署的先进力矩控制策略得以实际应用。

Conclusion: 高频率的线性反馈不仅能增强机器人力矩控制的稳定性，还可以作为实现高效、高性能力矩控制系统的有效途径。

Abstract: Torque control enables agile and robust robot motion, but deployment is often
hindered by instability and hardware limits. Here, we present a novel solution
to execute whole-body linear feedback at up to 40 kHz on open-source hardware.
We use this to interpolate non-linear schemes during real-world execution, such
as inverse dynamics and learned torque policies. Our results show that by
stabilizing torque controllers, high-frequency linear feedback could be an
effective route towards unlocking the potential of torque-controlled robotics.

</details>


### [618] [ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning](https://arxiv.org/abs/2509.24219)
*Tomoyuki Kagaya,Subramanian Lakshmi,Anbang Ye,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: ViReSkill提出了一种结合视觉感知和技能记忆的机器人动作规划框架，实现了稳定、高效的自主运动规划和技能迁移。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习或模仿学习的机器人适应新任务效率低，而LLM/VLM虽然数据需求低，但面临符号规划与实际场景脱节以及输出稳定性差的问题。

Method: ViReSkill通过视觉感知辅助的重规划机制与技能记忆库结合：在任务失败时根据当前场景生成新的动作序列，在成功后将动作序列存储为技能并复用，无需再次查询LLM/VLM，实现了持续自我学习。

Result: 在LIBERO、RLBench等仿真环境及真实机器人上测试，ViReSkill在任务成功率上稳定优于传统基线，展现出更强的模拟到现实泛化能力。

Conclusion: ViReSkill为机器人任务规划带来更快的适应、可靠执行和持续技能积累，推动了LLM/VLM在机器人领域的实际应用。

Abstract: Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL)
often adapt slowly to new tasks, whereas recent Large Language Models (LLMs)
and Vision-Language Models (VLMs) promise knowledge-rich planning from minimal
data. Deploying LLMs/VLMs for motion planning, however, faces two key
obstacles: (i) symbolic plans are rarely grounded in scene geometry and object
physics, and (ii) model outputs can vary for identical prompts, undermining
execution reliability. We propose ViReSkill, a framework that pairs
vision-grounded replanning with a skill memory for accumulation and reuse. When
a failure occurs, the replanner generates a new action sequence conditioned on
the current scene, tailored to the observed state. On success, the executed
plan is stored as a reusable skill and replayed in future encounters without
additional calls to LLMs/VLMs. This feedback loop enables autonomous continual
learning: each attempt immediately expands the skill set and stabilizes
subsequent executions. We evaluate ViReSkill on simulators such as LIBERO and
RLBench as well as on a physical robot. Across all settings, it consistently
outperforms conventional baselines in task success rate, demonstrating robust
sim-to-real generalization.

</details>


### [619] [Towards Tighter Convex Relaxation of Mixed-integer Programs: Leveraging Logic Network Flow for Task and Motion Planning](https://arxiv.org/abs/2509.24235)
*Xuan Lin,Jiming Ren,Yandong Luo,Weijun Xie,Ye Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种面向机器人任务与动作规划的“逻辑网络流”优化框架，将时序逻辑引入混合整数规划，通过网络流建模显著提升了规划效率，对于多机器人、复杂车辆路径等场景在理论和算力上获得突破。


<details>
  <summary>Details</summary>
Motivation: 传统基于“逻辑树”方法在时序逻辑任务—动作规划中约束数量多、计算缓慢，尤其在实际动态系统及多机器人协同任务中效率受限。作者希望寻求提升规划效率与解空间松弛性的创新方法。

Method: 受“凸集图（Graph-of-Convex-Sets）”方法启发，本文将时序谓词作为多面体约束编码到网络流模型的每条边，而非传统逻辑树节点间约束。同时提出基于网络流的Fourier-Motzkin消元方法，消除连续流变量并保持凸松弛的紧性，从而降低约束数量，提高求解效率。

Result: 方法在车辆路径、多机器人协同、动态系统时序任务等多类实验中，较传统逻辑树规划获得数量级的计算加速，充分证明了在数学松弛性和约束规模上的优势。四足机器人实际硬件实验证实其可在动态环境下实现实时重规划。

Conclusion: “逻辑网络流”框架有效集成时序逻辑至优化任务动作规划，大幅提升求解效率与可扩展性，为机器人复杂任务的实时决策与控制提供了理论与工程新突破。

Abstract: This paper proposes an optimization-based task and motion planning framework,
named "Logic Network Flow", that integrates temporal logic specifications into
mixed-integer programs for efficient robot planning. Inspired by the
Graph-of-Convex-Sets formulation, temporal predicates are encoded as polyhedron
constraints on each edge of a network flow model, instead of as constraints
between nodes in traditional Logic Tree formulations. We further propose a
network-flow-based Fourier-Motzkin elimination procedure that removes
continuous flow variables while preserving convex relaxation tightness, leading
to provably tighter convex relaxations and fewer constraints than Logic Tree
formulations. For temporal logic motion planning with piecewise-affine dynamic
systems, comprehensive experiments across vehicle routing, multi-robot
coordination, and temporal logic control on dynamical systems using point mass
and linear inverted pendulum models demonstrate computational speedups of up to
several orders of magnitude. Hardware demonstrations with quadrupedal robots
validate real-time replanning capabilities under dynamically changing
environmental conditions. The project website is at
https://logicnetworkflow.github.io/.

</details>


### [620] [PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization](https://arxiv.org/abs/2509.24236)
*Siyan Dong,Zijun Wang,Lulu Cai,Yi Ma,Yanchao Yang*

Main category: cs.RO

TL;DR: 本文提出了一种结合学习与优化的RGB-D SLAM新方法，实现了在不稳定运动下实时高精度稠密重建，优于当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人领域中，进行实时场景稠密重建非常关键，但现有的RGB-D SLAM系统在摄像头剧烈运动、视角大变化或抖动时常失效。传统基于优化的方法精度高但对初始值敏感，基于学习的方法鲁棒性强但不够精确。所以需要一种兼具鲁棒性与精度的新方案。

Method: 方法结合了深度学习和优化两种思路。首先利用相机位姿回归网络预测连续RGB-D帧之间的相对位姿，提高初始化的鲁棒性；然后采用随机优化算法进一步细化，优化深度图与场景几何的对齐效果。

Result: 实验显示，在有挑战性的测试集上，该方法表现优于现有最佳对手，在稳定运动下的重建精度也相当。能实时运行，兼具速度和精度。

Conclusion: 通过融合简单的学习与优化技术，该方法在应对不稳定运动时具备更强鲁棒性，同时保证稠密重建的高精度，实用性强。

Abstract: Real-time dense scene reconstruction during unstable camera motions is
crucial for robotics, yet current RGB-D SLAM systems fail when cameras
experience large viewpoint changes, fast motions, or sudden shaking. Classical
optimization-based methods deliver high accuracy but fail with poor
initialization during large motions, while learning-based approaches provide
robustness but lack sufficient accuracy for dense reconstruction. We address
this challenge through a combination of learning-based initialization with
optimization-based refinement. Our method employs a camera pose regression
network to predict metric-aware relative poses from consecutive RGB-D frames,
which serve as reliable starting points for a randomized optimization algorithm
that further aligns depth images with the scene geometry. Extensive experiments
demonstrate promising results: our approach outperforms the best competitor on
challenging benchmarks, while maintaining comparable accuracy on stable motion
sequences. The system operates in real-time, showcasing that combining simple
and principled techniques can achieve both robustness for unstable motions and
accuracy for dense reconstruction. Project page:
https://github.com/siyandong/PROFusion.

</details>


### [621] [SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions](https://arxiv.org/abs/2509.24243)
*Jeongyong Yang,Seunghwan Jang,Soojean Han*

Main category: cs.RO

TL;DR: SafeFlowMatcher结合了flow matching算法与控制障碍函数，实现了高效且有理论安全保证的路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有flow matching（FM）类生成式规划器可快速生成路径，但缺乏对路径安全的理论保证，且在靠近约束时路径容易不完整，因此需要新的方法实现路径的同时保证安全性。

Method: 提出SafeFlowMatcher，采用两阶段预测-校正（prediction-correction, PC）积分器：第一阶段预测用学习得来的FM快速生成备选路径；第二阶段用随时间消失的调整向量场及控制障碍函数（CBF）构建的二次规划，对路径进行最小扰动校正。通过理论证明为该系统提供了barrier certificate，确保生成路径的安全集正向不变性及有限时间收敛性。

Result: 在迷宫导航和运动控制等基准测试上，SafeFlowMatcher生成的路径相比现有扩散法和普通FM法更快、更平滑且更安全。消融实验验证了PC积分器和barrier certificate的重要贡献。

Conclusion: SafeFlowMatcher有效融合了FM和CBF，兼顾路径规划效率与可证明的安全性，是生成式路径规划的新进展。

Abstract: Generative planners based on flow matching (FM) can produce high-quality
paths in one or a few ODE steps, but their sampling dynamics offer no formal
safety guarantees and can yield incomplete paths near constraints. We present
SafeFlowMatcher, a planning framework that couples FM with control barrier
functions (CBFs) to achieve both real-time efficiency and certified safety.
SafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a
prediction phase integrates the learned FM once (or a few steps) to obtain a
candidate path without intervention; (ii) a correction phase refines this path
with a vanishing time-scaled vector field and a CBF-based quadratic program
that minimally perturbs the vector field. We prove a barrier certificate for
the resulting flow system, establishing forward invariance of a robust safe set
and finite-time convergence to the safe set. By enforcing safety only on the
executed path (rather than on all intermediate latent paths), SafeFlowMatcher
avoids distributional drift and mitigates local trap problems. Across maze
navigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,
and safer paths than diffusion- and FM-based baselines. Extensive ablations
corroborate the contributions of the PC integrator and the barrier certificate.

</details>


### [622] [Contextual Neural Moving Horizon Estimation for Robust Quadrotor Control in Varying Conditions](https://arxiv.org/abs/2509.24281)
*Kasra Torshizi,Chak Lam Shek,Khuzema Habib,Guangyao Shi,Pratap Tokekar,Troi Williams*

Main category: cs.RO

TL;DR: 本文提出了一种用于四旋翼的自适应控制方法，通过贝叶斯优化和高斯过程选择性地收集环境上下文数据，以提升轨迹跟踪的鲁棒性和泛化能力，无需在所有环境中进行穷举训练。实验证明方法显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有自适应控制器需对环境干扰进行估计以保证鲁棒性，但这种估计需要在特定场景下大量调参，适应性差，且机器学习方法收集所有场景数据既不切实际也低效，因此需要一种能有效取样以提升泛化能力的新方法。

Method: 采用贝叶斯优化结合高斯过程，智能地选择哪些环境上下文进行数据收集，训练神经网络动态调整估算器参数，实现Contextual NeuroMHE方法，避免在所有环境上下文下穷举训练。

Result: 在多种真实场景实验中，该方法在最大绝对位置误差上比现有方法提升20.3%，并能通过少量精心选择的上下文捕捉环境变化。

Conclusion: 所提方法在无需大规模数据收集的情况下，实现了在不同环境下的高效和鲁棒轨迹跟踪，具有较强的泛化能力和实用价值。

Abstract: Adaptive controllers on quadrotors typically rely on estimation of
disturbances to ensure robust trajectory tracking. Estimating disturbances
across diverse environmental contexts is challenging due to the inherent
variability and uncertainty in the real world. Such estimators require
extensive fine-tuning for a specific scenario, which makes them inflexible and
brittle to changing conditions. Machine-learning approaches, such as training a
neural network to tune the estimator's parameters, are promising. However,
collecting data across all possible environmental contexts is impossible. It is
also inefficient as the same estimator parameters could work for "nearby"
contexts. In this paper, we present a sequential decision making strategy that
decides which environmental contexts, using Bayesian Optimization with a
Gaussian Process, to collect data from in order to ensure robust performance
across a wide range of contexts. Our method, Contextual NeuroMHE, eliminates
the need for exhaustive training across all environments while maintaining
robust performance under different conditions. By enabling the neural network
to adapt its parameters dynamically, our method improves both efficiency and
generalization. Experimental results in various real-world settings demonstrate
that our approach outperforms the prior work by 20.3\% in terms of maximum
absolute position error and can capture the variations in the environment with
a few carefully chosen contexts.

</details>


### [623] [Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning](https://arxiv.org/abs/2509.24313)
*Korbinian Moller,Roland Stroop,Mattia Piccinini,Alexander Langmann,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习和解析方法的混合采样运动规划框架，大幅减少了采样数量和计算时间，同时保证了自动驾驶在城市环境中的安全与高效。


<details>
  <summary>Details</summary>
Motivation: 传统采样型运动规划在复杂城市场景下，常因均匀或启发式采样产生大量无效或不可行轨迹，影响效率和实际应用效果。作者希望提升采样效率和可行性，以应对城市自动驾驶对决策速度和可靠性的高要求。

Method: 方法将强化学习（RL）与解析化轨迹生成/评估相结合：RL智能体指导采样关注潜在可行区域，轨迹生成和筛选仍由可验证的解析方法完成。RL采样器结合基于可译码深度集合编码器的世界模型，实现对不同交通要素的灵活建模。最终在CommonRoad仿真环境中验证其有效性。

Result: 相比传统方法，混合采样方案使所需样本数最多减少99%，运行时间降低最多84%，并能保持原有的成功率和避碰率等运动规划质量。

Conclusion: 该方法有效提升了城市自动驾驶运动规划的效率和可靠性，为真实场景下更安全、响应更快的决策提供了有力支撑，具有重要应用前景。源码与模型已开放。

Abstract: Sampling-based motion planning is a well-established approach in autonomous
driving, valued for its modularity and analytical tractability. In complex
urban scenarios, however, uniform or heuristic sampling often produces many
infeasible or irrelevant trajectories. We address this limitation with a hybrid
framework that learns where to sample while keeping trajectory generation and
evaluation fully analytical and verifiable. A reinforcement learning (RL) agent
guides the sampling process toward regions of the action space likely to yield
feasible trajectories, while evaluation and final selection remains governed by
deterministic feasibility checks and cost functions. We couple the RL sampler
with a world model (WM) based on a decodable deep set encoder, enabling both
variable numbers of traffic participants and reconstructable latent
representations. The approach is evaluated in the CommonRoad simulation
environment, showing up to 99% fewer required samples and a runtime reduction
of up to 84% while maintaining planning quality in terms of success and
collision-free rates. These improvements lead to faster, more reliable
decision-making for autonomous vehicles in urban environments, achieving safer
and more responsive navigation under real-world constraints. Code and trained
artifacts are publicly available at:
https://github.com/TUM-AVS/Learning-to-Sample

</details>


### [624] [SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm](https://arxiv.org/abs/2509.24321)
*Yao Wang,Zhirui Sun,Wenzheng Chi,Baozhi Jia,Wenjun Xu,Jiankun Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的跨模态推理方法SONAR，有效提升了机器人在未知环境中的视觉-语言导航能力，兼顾泛化性与场景适应性。


<details>
  <summary>Details</summary>
Motivation: 现有模块化方法依赖高质量训练数据，泛化能力差，而基于视觉-语言模型的方法在语义线索弱时表现不佳。因此需要一种能兼顾泛化和语义适应性的导航方法。

Method: 提出了SONAR方法，将基于语义地图的目标预测模块与基于视觉-语言模型的价值地图模块结合。同时，目标定位上结合多尺度语义地图与置信度地图，减少目标物体的误检。

Result: 在Gazebo模拟器和Matterport 3D (MP3D)数据集上实验，SONAR导航任务成功率达到38.4%，SPL指标为17.7%。

Conclusion: SONAR方法在弱语义环境下依然能保持较好的导航效果，提升了泛化能力和适应不同场景的能力，实现了更健壮的人机交互导航。

Abstract: Understanding human instructions and accomplishing Vision-Language Navigation
tasks in unknown environments is essential for robots. However, existing
modular approaches heavily rely on the quality of training data and often
exhibit poor generalization. Vision-Language Model based methods, while
demonstrating strong generalization capabilities, tend to perform
unsatisfactorily when semantic cues are weak. To address these issues, this
paper proposes SONAR, an aggregated reasoning approach through a cross modal
paradigm. The proposed method integrates a semantic map based target prediction
module with a Vision-Language Model based value map module, enabling more
robust navigation in unknown environments with varying levels of semantic cues,
and effectively balancing generalization ability with scene adaptability. In
terms of target localization, we propose a strategy that integrates multi-scale
semantic maps with confidence maps, aiming to mitigate false detections of
target objects. We conducted an evaluation of the SONAR within the Gazebo
simulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the
experimental benchmark. Experimental results demonstrate that SONAR achieves a
success rate of 38.4% and an SPL of 17.7%.

</details>


### [625] [AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation](https://arxiv.org/abs/2509.24387)
*Xin Ding,Jianyu Wei,Yifan Yang,Shiqi Jiang,Qianxi Zhang,Hao Wu,Fucheng Jia,Liang Mi,Yuxuan Yan,Weijun Wang,Yunxin Liu,Zhibo Chen,Ting Cao*

Main category: cs.RO

TL;DR: 提出了一种面向视觉-语言导航任务的自适应推理框架AdaNav，有效提升了导航智能体在有限数据下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航任务中，固定步数推理方式会导致表现不佳和计算浪费，而显式推理虽可增强一致性和感知-动作对齐，但需要动态平衡推理开销与效果。

Method: 提出了不确定性驱动的自适应推理模块UAR，以及将动作熵作为推理先验，通过启发式到强化学习的逐步训练使智能体学习基于难度的推理策略，适应严格数据受限的场景。

Result: 在只有6000个训练样本的情况下，AdaNav取得了比在百万级数据集上闭源模型更好的表现：R2R val-unseen提升20%，RxR-CE提升11.7%，真实场景提升11.4%。

Conclusion: AdaNav表明，通过不确定性自适应推理可以在视觉-语言导航任务中，以极少数据显著优于大规模训练模型，具备实际应用潜力。

Abstract: Vision Language Navigation (VLN) requires agents to follow natural language
instructions by grounding them in sequential visual observations over long
horizons. Explicit reasoning could enhance temporal consistency and perception
action alignment, but reasoning at fixed steps often leads to suboptimal
performance and unnecessary computation. To address this, we propose AdaNav, an
uncertainty-based adaptive reasoning framework for VLN. At its core is the
Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that
dynamically triggers reasoning. We introduce Action Entropy as a policy prior
for UAR and progressively refine it through a Heuristics to RL training method,
enabling agents to learn difficulty aware reasoning policies under the strict
data limitations of embodied tasks. Results show that with only 6K training
samples, AdaNav achieves substantial gains over closed source models trained on
million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on
RxR-CE, and 11.4% in real world scenes. The code is available at
https://github.com/xinding-sys/AdaNav.

</details>


### [626] [DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability](https://arxiv.org/abs/2509.24413)
*Tianqiang Yan,Ziqiao Lin,Sicheng Wang,Tianwei Zhang,Zhenglong Sun*

Main category: cs.RO

TL;DR: 本文提出了DynaMIC框架，旨在帮助机器人识别和应对来自人类的误导性指令（directive counterfactuals），提升机器人执行任务的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着基于自然语言的大型预训练模型在机器人领域的应用，机器人能更好地理解和执行人类指令。然而，如果机器人严格按照含有误导信息的指令操作，可能会导致任务失败甚至造成安全问题。类似“反事实”在NLP领域的讨论，该问题在机器人领域尚未引起足够关注，因此作者希望推动对此类场景的研究。

Method: 作者引入了directive counterfactuals（误导性指令反事实），并提出DynaMIC框架。该框架能生成任务流程，主动检测任务中的DCF，并实时向人类反馈，以提升机器人对潜在误导性指令的敏感性。作者还进行了语义层面的实验与消融实验以验证方法有效性。

Result: 实验结果表明，DynaMIC能有效识别和应对误导性指令，提升机器人在执行任务过程中的安全性和可靠性。消融实验进一步证明各部分组件对整体性能的贡献。

Conclusion: 通过引入DCF概念和DynaMIC框架，本文展示了应对人类误导性指令的重要性和可行性，为提升基于大模型驱动的机器人系统的可靠性和安全性提供了新方向。

Abstract: The emergence of large pre-trained models based on natural language has
breathed new life into robotics development. Extensive research has integrated
large models with robots, utilizing the powerful semantic understanding and
generation capabilities of large models to facilitate robot control through
natural language instructions gradually. However, we found that robots that
strictly adhere to human instructions, especially those containing misleading
information, may encounter errors during task execution, potentially leading to
safety hazards. This resembles the concept of counterfactuals in natural
language processing (NLP), which has not yet attracted much attention in
robotic research. In an effort to highlight this issue for future studies, this
paper introduced directive counterfactuals (DCFs) arising from misleading human
directives. We present DynaMIC, a framework for generating robot task flows to
identify DCFs and relay feedback to humans proactively. This capability can
help robots be sensitive to potential DCFs within a task, thus enhancing the
reliability of the execution process. We conducted semantic-level experiments
and ablation studies, showcasing the effectiveness of this framework.

</details>


### [627] [PhysiAgent: An Embodied Agent Framework in Physical World](https://arxiv.org/abs/2509.24524)
*Zhihao Wang,Jianxiong Li,Jinliang Zheng,Wencong Zhang,Dongxiu Liu,Yinan Zheng,Haoyi Niu,Junzhi Yu,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 本文提出了PhysiAgent框架，通过自我反思和工具协作，有效联动视觉-语言模型（VLMs）和视觉-语言-行动模型（VLAs），大幅提升了机器人复杂任务的执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在执行复杂真实世界任务时泛化能力有限，且主流做法将VLM和VLA以僵化、串行方式结合，无法充分发挥双方协作潜力，导致场景理解和行动执行脱节。

Method: PhysiAgent框架引入了监控、记忆、自我反思机制和轻量级工具箱，根据VLA反馈动态调度和组织VLM模块，使VLM和VLA基于自身能力实现更高效的协作与学习。该系统支持在物理环境下自适应调整。

Result: 在复杂真实世界机器人任务中，PhysiAgent显著提升了任务解决表现，实现了VLM自我调节、工具协同和执行过程中的持续进化。

Conclusion: PhysiAgent为VLM和VLA的深度融合提供了创新可行的方案，有效推动了具身智能体技术在现实中的落地和发展。

Abstract: Vision-Language-Action (VLA) models have achieved notable success but often
struggle with limited generalizations. To address this, integrating generalized
Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular
solution. However, current approaches often combine these models in rigid,
sequential structures: using VLMs primarily for high-level scene understanding
and task planning, and VLAs merely as executors of lower-level actions, leading
to ineffective collaboration and poor grounding challenges. In this paper, we
propose an embodied agent framework, PhysiAgent, tailored to operate
effectively in physical environments. By incorporating monitor, memory,
self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent
offers an autonomous scaffolding framework to prompt VLMs to organize different
components based on real-time proficiency feedback from VLAs to maximally
exploit VLAs' capabilities. Experimental results demonstrate significant
improvements in task-solving performance on complex real-world robotic tasks,
showcasing effective self-regulation of VLMs, coherent tool collaboration, and
adaptive evolution of the framework during execution. PhysiAgent makes
practical and pioneering efforts to integrate VLMs and VLAs, effectively
grounding embodied agent frameworks in real-world settings.

</details>


### [628] [Game Theory to Study Cooperation in Human-Robot Mixed Groups: Exploring the Potential of the Public Good Game](https://arxiv.org/abs/2509.24530)
*Giulia Pusceddu,Sara Mongile,Francesco Rea,Alessandra Sciutti*

Main category: cs.RO

TL;DR: 本文利用博弈论中的公共物品博弈模型，探讨人在与仿人机器人混合团队中的合作与信任机制，并分析不同机器人的策略对人类合作意愿的影响。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作应用场景的增加，理解人类与机器人在混合群体中的合作与信任机制十分重要。本研究旨在探索机器人策略如何影响人类在合作性任务中的表现和态度，为未来社会型机器人设计奠定基础。

Method: 采用经过修改的公共物品博弈（PGG）模型，三位人类参与者与iCub机器人共同游戏，iCub机器人根据预设策略（如始终合作、始终搭便车、以牙还牙）参与。通过一项包含十九名参与者的初步实验，评估不同机器人策略对人类合作倾向的影响。

Result: 初步分析显示，尽管参与者认为机器人慷慨，但他们仍然倾向于不向公共基金投入资金。说明机器人积极合作并未显著提升人类的合作意愿。

Conclusion: 机器人采用不同的合作策略在混合组内对人类的合作行为影响有限，但研究结果有助于理解如何设计能促进信任与合作的社交机器人，对未来提升人机协作效率具有启发意义。

Abstract: In this study, we explore the potential of Game Theory as a means to
investigate cooperation and trust in human-robot mixed groups. Particularly, we
introduce the Public Good Game (PGG), a model highlighting the tension between
individual self-interest and collective well-being. In this work, we present a
modified version of the PGG, where three human participants engage in the game
with the humanoid robot iCub to assess whether various robot game strategies
(e.g., always cooperate, always free ride, and tit-for-tat) can influence the
participants' inclination to cooperate. We test our setup during a pilot study
with nineteen participants. A preliminary analysis indicates that participants
prefer not to invest their money in the common pool, despite they perceive the
robot as generous. By conducting this research, we seek to gain valuable
insights into the role that robots can play in promoting trust and cohesion
during human-robot interactions within group contexts. The results of this
study may hold considerable potential for developing social robots capable of
fostering trust and cooperation within mixed human-robot groups.

</details>


### [629] [Unlocking the Potential of Soft Actor-Critic for Imitation Learning](https://arxiv.org/abs/2509.24539)
*Nayari Marie Lessa,Melya Boukheddimi,Frank Kirchner*

Main category: cs.RO

TL;DR: 本论文提出一种结合Adversarial Motion Priors（AMP）和Soft Actor-Critic（SAC）的新型模仿学习框架，用于提升机器人仿生运动的自然性、适应性和数据效率，实验显示其优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有主流机器人仿生运动模仿学习主要依赖PPO算法，虽然稳定但样本效率低且策略泛化能力有限。提升数据效率和泛化性成为亟需解决的问题。

Method: 作者提出将AMP与SAC结合，利用SAC的离策略学习和熵调节探索能力，提高数据利用率、自然性和任务表现。具体在四足机器人多个运动和多样地形场景下对框架进行评测。

Result: 与主流的AMP+PPO方法相比，所提AMP+SAC方法在保证任务稳定执行的同时，获得了更高的模仿奖励，展现出更为自然和鲁棒的机器人运动能力。

Conclusion: 结合AMP和SAC的离策略模仿学习框架在生物仿真机器人运动生成领域展现出更高效、更鲁棒及泛化性更强的潜力。

Abstract: Learning-based methods have enabled robots to acquire bio-inspired movements
with increasing levels of naturalness and adaptability. Among these, Imitation
Learning (IL) has proven effective in transferring complex motion patterns from
animals to robotic systems. However, current state-of-the-art frameworks
predominantly rely on Proximal Policy Optimization (PPO), an on-policy
algorithm that prioritizes stability over sample efficiency and policy
generalization. This paper proposes a novel IL framework that combines
Adversarial Motion Priors (AMP) with the off-policy Soft Actor-Critic (SAC)
algorithm to overcome these limitations. This integration leverages
replay-driven learning and entropy-regularized exploration, enabling
naturalistic behavior and task execution, improving data efficiency and
robustness. We evaluate the proposed approach (AMP+SAC) on quadruped gaits
involving multiple reference motions and diverse terrains. Experimental results
demonstrate that the proposed framework not only maintains stable task
execution but also achieves higher imitation rewards compared to the widely
used AMP+PPO method. These findings highlight the potential of an off-policy IL
formulation for advancing motion generation in robotics.

</details>


### [630] [Prompting Robot Teams with Natural Language](https://arxiv.org/abs/2509.24575)
*Nicolas Pfitzer,Eduardo Sebastián,Ajay Shankar,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一个利用自然语言表达来为多机器人队伍下达高层任务的框架。该方法通过语言模型理解和分解意图，并结合RNN和GNN，实现了多机器人协作与决策。


<details>
  <summary>Details</summary>
Motivation: 随着多机器人应用需求增长，如何用自然语言高效、灵活地指挥和协调多机器人完成复杂任务成为亟需解决的问题，而现有方法在表达、理解和实时性方面存在挑战。

Method: 框架主要包括三步：1）将自然语言描述的任务通过语言模型转化为DFA（确定性有限自动机）；2）用RNN学习编码任务逻辑和子任务分解，将其内部状态对齐到具体任务语义上；3）训练GNN控制策略，结合RNN隐藏状态和语言向量，使机器人能分布式地执行任务。

Result: 这一方法在多种仿真和实际多机器人任务（包括需要顺序性和协作性的任务）中进行了实验，并证明了模型的高效性、解释性及实时分布式协作能力。

Conclusion: 文章证明了基于语言模型、RNN与GNN的多机器人任务分解与执行框架在实际应用中具有良好的效果，为自然语言驱动的多机器人协作提供了新途径。

Abstract: This paper presents a framework towards prompting multi-robot teams with
high-level tasks using natural language expressions. Our objective is to use
the reasoning capabilities demonstrated by recent language models in
understanding and decomposing human expressions of intent, and repurpose these
for multi-robot collaboration and decision-making. The key challenge is that an
individual's behavior in a collective can be hard to specify and interpret, and
must continuously adapt to actions from others. This necessitates a framework
that possesses the representational capacity required by the logic and
semantics of a task, and yet supports decentralized and interactive real-time
operation. We solve this dilemma by recognizing that a task can be represented
as a deterministic finite automaton (DFA), and that recurrent neural networks
(RNNs) can encode numerous automata. This allows us to distill the logic and
sequential decompositions of sub-tasks obtained from a language model into an
RNN, and align its internal states with the semantics of a given task. By
training a graph neural network (GNN) control policy that is conditioned on the
hidden states of the RNN and the language embeddings, our method enables robots
to execute task-relevant actions in a decentralized manner. We present
evaluations of this single light-weight interpretable model on various
simulated and real-world multi-robot tasks that require sequential and
collaborative behavior by the team -- sites.google.com/view/prompting-teams.

</details>


### [631] [U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation](https://arxiv.org/abs/2509.24579)
*Linzhi Wu,Aoran Mei,Xiyue Wang,Guo-Niu Zhu,Zhongxue Gan*

Main category: cs.RO

TL;DR: 本文提出了一种名为U-DiT Policy的U型扩散Transformer框架，用于提升机器人端到端视觉运动控制的表现，并在仿真与真实任务中显著优于现有扩散U-Net及Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散方法广泛用于机器人端到端视觉运动控制，但主流的U-Net扩散方法存在全局建模能力不足和输出过度平滑的问题，需要改进其表达能力和泛化能力。

Method: 提出U-DiT Policy，将U-Net的多尺度特征融合能力与Transformer的全局上下文建模能力结合起来，形成U型扩散Transformer架构，并在仿真与现实世界的机器人操作任务上进行评估。

Result: 在仿真任务中，U-DiT相较于基础方法平均提升10%，比采用AdaLN模块的Transformer扩散策略（DP-T）提升6%；在真实任务上，U-DiT相比扩散U-Net提升22.5%，对干扰与光照变化也表现出更高的鲁棒性与泛化能力。

Conclusion: U-DiT Policy显著提升了基于扩散的机器人操作策略的效率、泛化能力和鲁棒性，有潜力成为该领域新的基础方法。

Abstract: Diffusion-based methods have been acknowledged as a powerful paradigm for
end-to-end visuomotor control in robotics. Most existing approaches adopt a
Diffusion Policy in U-Net architecture (DP-U), which, while effective, suffers
from limited global context modeling and over-smoothing artifacts. To address
these issues, we propose U-DiT Policy, a novel U-shaped Diffusion Transformer
framework. U-DiT preserves the multi-scale feature fusion advantages of U-Net
while integrating the global context modeling capability of Transformers,
thereby enhancing representational power and policy expressiveness. We evaluate
U-DiT extensively across both simulation and real-world robotic manipulation
tasks. In simulation, U-DiT achieves an average performance gain of 10\% over
baseline methods and surpasses Transformer-based diffusion policies (DP-T) that
use AdaLN blocks by 6\% under comparable parameter budgets. On real-world
robotic tasks, U-DiT demonstrates superior generalization and robustness,
achieving an average improvement of 22.5\% over DP-U. In addition, robustness
and generalization experiments under distractor and lighting variations further
highlight the advantages of U-DiT. These results highlight the effectiveness
and practical potential of U-DiT Policy as a new foundation for diffusion-based
robotic manipulation.

</details>


### [632] [PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control](https://arxiv.org/abs/2509.24591)
*Haozhuo Zhang,Michele Caprio,Jing Shao,Qiang Zhang,Jian Tang,Shanghang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: PoseDiff 是一种将机器人状态估计与控制统一于单一框架的条件扩散模型，实现了从单张RGB图像到机器人的结构化状态预测，并能在长时序任务中输出连贯动作序列，达到了高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人感知与控制通常依赖多阶段流程和多种传感器，效率低且难以扩展。急需一种端到端、可扩展且高效连接视觉感知与动作控制的方法。

Method: 提出PoseDiff，将条件扩散模型用于机器人任务。先利用扩散模型从单张RGB图像直接预测3D关键点或关节角度。再通过以稀疏关键帧为条件，实现从视频到动作的逆动力学预测，并用重叠平均（overlap-averaging）策略优化动作序列平滑性。

Result: 在DREAM数据集上，PoseDiff在姿态估计任务中达到最新最优的精度和实时表现；在Libero-Object操作任务中，PoseDiff在严格的离线设定下，显著提升了任务成功率。

Conclusion: PoseDiff 能高效、准确地统一感知、规划和控制环节，实现了可扩展的具身智能AI系统。

Abstract: We present PoseDiff, a conditional diffusion model that unifies robot state
estimation and control within a single framework. At its core, PoseDiff maps
raw visual observations into structured robot states-such as 3D keypoints or
joint angles-from a single RGB image, eliminating the need for multi-stage
pipelines or auxiliary modalities. Building upon this foundation, PoseDiff
extends naturally to video-to-action inverse dynamics: by conditioning on
sparse video keyframes generated by world models, it produces smooth and
continuous long-horizon action sequences through an overlap-averaging strategy.
This unified design enables scalable and efficient integration of perception
and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy
and real-time performance for pose estimation. On Libero-Object manipulation
tasks, it substantially improves success rates over existing inverse dynamics
modules, even under strict offline settings. Together, these results show that
PoseDiff provides a scalable, accurate, and efficient bridge between
perception, planning, and control in embodied AI. The video visualization
results can be found on the project page:
https://haozhuo-zhang.github.io/PoseDiff-project-page/.

</details>


### [633] [CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations](https://arxiv.org/abs/2509.24661)
*Zhiyuan Wu,Rolandos Alexandros Potamias,Xuyang Zhang,Zhongqun Zhang,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法CEDex，实现针对不同类型机械手的灵巧抓取生成，并构建了大规模跨结构抓取数据集。该方法优于现有技术，提升了跨结构抓取的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机械手抓取合成方法要么依赖物理优化，缺乏类似人类的运动学认知；要么需要大量人工采集数据，且局限在类人结构。为了提升多样环境下机械手的适应性，需要大量多样化抓取数据和能够泛化于各种机械手的方法。

Method: 提出CEDex方法，将人类抓取运动学与机器人运动学对齐。具体流程为：1）利用条件变分自编码器（CVAE）在人体抓取数据上预训练，生成类似人类的接触表征；2）通过拓扑融合方式将多人手部零件整合到机器人组件上，进行运动学对齐；3）基于有符号距离场和物理约束做抓取优化。最终，建立了包含四种机械手、500K目标物体和2000万条抓取的大型数据集。

Result: CEDex方法在大规模实验中优于当前最先进方法。实验结果说明该方法生成的数据集具有多样性和高质量，显著提升了跨结构抓取学习的效果。

Conclusion: 论文方法实现了能泛化于多种机械手的灵巧抓取合成系统，为多形态机器人适应复杂环境提供了强有力的数据和方法支撑。

Abstract: Cross-embodiment dexterous grasp synthesis refers to adaptively generating
and optimizing grasps for various robotic hands with different morphologies.
This capability is crucial for achieving versatile robotic manipulation in
diverse environments and requires substantial amounts of reliable and diverse
grasp data for effective model training and robust generalization. However,
existing approaches either rely on physics-based optimization that lacks
human-like kinematic understanding or require extensive manual data collection
processes that are limited to anthropomorphic structures. In this paper, we
propose CEDex, a novel cross-embodiment dexterous grasp synthesis method at
scale that bridges human grasping kinematics and robot kinematics by aligning
robot kinematic models with generated human-like contact representations. Given
an object's point cloud and an arbitrary robotic hand model, CEDex first
generates human-like contact representations using a Conditional Variational
Auto-encoder pretrained on human contact data. It then performs kinematic human
contact alignment through topological merging to consolidate multiple human
hand parts into unified robot components, followed by a signed distance
field-based grasp optimization with physics-aware constraints. Using CEDex, we
construct the largest cross-embodiment grasp dataset to date, comprising 500K
objects across four gripper types with 20M total grasps. Extensive experiments
show that CEDex outperforms state-of-the-art approaches and our dataset
benefits cross-embodiment grasp learning with high-quality diverse grasps.

</details>


### [634] [Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering](https://arxiv.org/abs/2509.24697)
*Evelyn D'Elia,Paolo Maria Viceconte,Lorenzo Rapetti,Diego Ferigo,Giulio Romualdi,Giuseppe L'Erario,Raffaello Camoriano,Daniele Pucci*

Main category: cs.RO

TL;DR: 本论文提出了一种结合物理知识和模仿学习的人形机器人轨迹生成方法，显著提升了轨迹的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的人形机器人运动方法虽然可生成更平滑、更拟人的轨迹，但受限于数据量且缺乏对物理规律的考量，实机表现中容易出现不符合物理的轨迹甚至导致失稳。

Method: 作者采用双管齐下的学习策略：一是在监督式模仿学习中引入物理先验以提升轨迹可行性；二是在推理阶段直接对生成的状态应用比例-积分控制器，减少偏移。实际在ergoCub人形机器人上，通过引入物理相关损失项，鼓励脚接触瞬时速度接近零。

Result: 实验结果表明，该方法与多种控制器兼容，能显著提升真实机器人上的轨迹准确性及物理约束符合度。

Conclusion: 通过融入物理先验和控制原理，所提方法有效弥补了仅凭模仿学习生成轨迹的物理一致性不足，在人形机器人真实运动控制中表现优异。

Abstract: Recent trends in humanoid robot control have successfully employed imitation
learning to enable the learned generation of smooth, human-like trajectories
from human data. While these approaches make more realistic motions possible,
they are limited by the amount of available motion data, and do not incorporate
prior knowledge about the physical laws governing the system and its
interactions with the environment. Thus they may violate such laws, leading to
divergent trajectories and sliding contacts which limit real-world stability.
We address such limitations via a two-pronged learning strategy which leverages
the known physics of the system and fundamental control principles. First, we
encode physics priors during supervised imitation learning to promote
trajectory feasibility. Second, we minimize drift at inference time by applying
a proportional-integral controller directly to the generated output state. We
validate our method on various locomotion behaviors for the ergoCub humanoid
robot, where a physics-informed loss encourages zero contact foot velocity. Our
experiments demonstrate that the proposed approach is compatible with multiple
controllers on a real robot and significantly improves the accuracy and
physical constraint conformity of generated trajectories.

</details>


### [635] [LLM-Handover:Exploiting LLMs for Task-Oriented Robot-Human Handovers](https://arxiv.org/abs/2509.24706)
*Andreea Tulbure,Rene Zurbruegg,Timm Grigat,Marco Hutter*

Main category: cs.RO

TL;DR: 作者提出了LLM-Handover框架，将大语言模型的推理能力与部位分割结合，实现了对任务场景的感知抓取选择，可显著提升机器人与人类之间的协作递物体验。实验表明该方法抓取成功率高、适应多种任务限制，用户体验好。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往忽略人类在接手物品后的具体操作，假设限制了机器递物系统的泛化能力，因此亟需能够理解接手后动作意图的递物策略。

Method: 提出将大语言模型推理与物体部位分割结合的LLM-Handover系统，输入为RGB-D图像与具体任务描述，经模型推理确定物体相关抓取部位，实现智能抓取和递交。提出新数据集用于评估。

Result: （1）在递物场景下提升了当前主流物体部位分割方法的表现。（2）系统抓取成功率高，能适应多样的递物后任务约束。（3）硬件实验零样本条件下递物成功率达83%。（4）用户调查显示86%偏好该方法。

Conclusion: LLM-Handover能够完成更智能、直观、任务感知的递物操作，有效提升人机协作体验。

Abstract: Effective human-robot collaboration depends on task-oriented handovers, where
robots present objects in ways that support the partners intended use. However,
many existing approaches neglect the humans post-handover action, relying on
assumptions that limit generalizability. To address this gap, we propose
LLM-Handover, a novel framework that integrates large language model
(LLM)-based reasoning with part segmentation to enable context-aware grasp
selection and execution. Given an RGB-D image and a task description, our
system infers relevant object parts and selects grasps that optimize
post-handover usability. To support evaluation, we introduce a new dataset of
60 household objects spanning 12 categories, each annotated with detailed part
labels. We first demonstrate that our approach improves the performance of the
used state-of-the-art part segmentation method, in the context of robot-human
handovers. Next, we show that LLM-Handover achieves higher grasp success rates
and adapts better to post-handover task constraints. During hardware
experiments, we achieve a success rate of 83% in a zero-shot setting over
conventional and unconventional post-handover tasks. Finally, our user study
underlines that our method enables more intuitive, context-aware handovers,
with participants preferring it in 86% of cases.

</details>


### [636] [APREBot: Active Perception System for Reflexive Evasion Robot](https://arxiv.org/abs/2509.24733)
*Zihao Xu,Kuankuan Sima,Junhao Deng,Zixuan Zhuang,Chunzheng Wang,Ce Hao,Jin Song Dong*

Main category: cs.RO

TL;DR: 本文提出了一种名为APREBot的新型主动感知系统，结合了LiDAR和摄像头信息，实现了四足机器人对环境的全面感知与敏捷避障，在仿真和实际测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人在动态环境中避障时，单一传感器面临能力局限：LiDAR视野广但缺乏纹理细节，摄像头细节丰富但视野有限。因此亟需突破感知瓶颈，提升机器人在障碍物突发情况下的反应与安全性。

Method: 提出APREBot架构，融合LiDAR全向扫描与摄像头主动聚焦，实现层级式主动感知。系统结合了反射式规避机制与多源感知优势，并在多种复杂障碍实验下进行了仿真与现实验证。

Result: 与现有最优基线方法比较，APREBot系统在避障安全性指标和运行效率方面均有显著提升，适应不同障碍物类型及多方向突发情况。

Conclusion: APREBot显著增强了四足机器人在动态和高危环境下的自主避障能力，为安全关键场景下的机器人自主导航提供了新方法，有望在实际应用中获得广泛推广。

Abstract: Reliable onboard perception is critical for quadruped robots navigating
dynamic environments, where obstacles can emerge from any direction under
strict reaction-time constraints. Single-sensor systems face inherent
limitations: LiDAR provides omnidirectional coverage but lacks rich texture
information, while cameras capture high-resolution detail but suffer from
restricted field of view. We introduce APREBot (Active Perception System for
Reflexive Evasion Robot), a novel framework that integrates reflexive evasion
with active hierarchical perception. APREBot strategically combines LiDAR-based
omnidirectional scanning with camera-based active focusing, achieving
comprehensive environmental awareness essential for agile obstacle avoidance in
quadruped robots. We validate APREBot through extensive sim-to-real experiments
on a quadruped platform, evaluating diverse obstacle types, trajectories, and
approach directions. Our results demonstrate substantial improvements over
state-of-the-art baselines in both safety metrics and operational efficiency,
highlighting APREBot's potential for dependable autonomy in safety-critical
scenarios. Videos are available at https://sites.google.com/view/aprebot/

</details>


### [637] [SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework](https://arxiv.org/abs/2509.24763)
*Xiangyi Meng,Delun Li,Zihao Mao,Yi Yang,Wenjie Song*

Main category: cs.RO

TL;DR: 该论文提出了一种结合空间与语义的新型零样本目标导航方法SSR-ZSON，有效提升了机器人在未知环境中寻找目标物体的能力，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前零样本目标导航任务受限于语义指引不足和空间记忆局限，导致探索效率低下和容易陷入局部区域，急需提升探索有效性和全局导航能力。

Method: 提出SSR-ZSON方法，基于TARE分层探索框架，创新性地结合了空间覆盖与语义密度平衡的视角生成策略和大语言模型驱动的全局语义导航机制，分别提升探索广度、降低无效搜索，并通过语义关联性引导，避免局部困陷。

Result: 在模拟和真实环境（Hybrid Habitat-Gazebo平台）上线实时运行，并在Matterport3D和Habitat-Matterport3D数据集上，相比SOTA方法，SR提升18.5%和11.2%，SPL提升0.181和0.140。

Conclusion: SSR-ZSON能够在保证实时性的前提下显著提升零样本目标导航的效率和成功率，验证了其优越性和实际应用潜力。

Abstract: Zero-shot object navigation in unknown environments presents significant
challenges, mainly due to two key limitations: insufficient semantic guidance
leads to inefficient exploration, while limited spatial memory resulting from
environmental structure causes entrapment in local regions. To address these
issues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object
navigation method based on the TARE hierarchical exploration framework,
integrating a viewpoint generation strategy balancing spatial coverage and
semantic density with an LLM-based global guidance mechanism. The performance
improvement of the proposed method is due to two key innovations. First, the
viewpoint generation strategy prioritizes areas of high semantic density within
traversable sub-regions to maximize spatial coverage and minimize invalid
exploration. Second, coupled with an LLM-based global guidance mechanism, it
assesses semantic associations to direct navigation toward high-value spaces,
preventing local entrapment and ensuring efficient exploration. Deployed on
hybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves
real-time operation and superior performance. On Matterport3D and
Habitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\% and
11.2\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,
respectively, over state-of-the-art methods.

</details>


### [638] [IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks](https://arxiv.org/abs/2509.24768)
*Eric Hannus,Miika Malin,Tran Nguyen Le,Ville Kyrki*

Main category: cs.RO

TL;DR: 本文提出一种新框架IA-VLA，通过大型视觉语言模型提升机器人操控任务中的复杂语言理解能力，有效解决处理视觉重复物体等难题，并显著提升基线VLA的表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型需要实时输出动作受控于小型语言模型，导致对复杂语义指令的理解力有限。部分操作任务依赖复杂、细致的语言描述（如通过相对位置辨别目标物），现有VLA对此支持不足。

Method: 提出IA-VLA框架，将大型视觉语言模型作为预处理模块，提升语义理解后，为机器人VLA模型提供增强上下文输入。实验设计包含三种带有重复物体的场景，用于评估基线VLA与增强版VLA的性能差别。

Result: 增强方案显著提升了VLA对于高语义复杂度任务（如处理视觉重复物体）的表现，尤其在遇到演示中未见的指令概念时，泛化能力大幅增强。

Conclusion: IA-VLA框架能有效提升VLA模型对复杂语言指令的理解和操作能力，尤其适用于含有视觉重复物体的复杂操作场景。

Abstract: Vision-language-action models (VLAs) have become an increasingly popular
approach for addressing robot manipulation problems in recent years. However,
such models need to output actions at a rate suitable for robot control, which
limits the size of the language model they can be based on, and consequently,
their language understanding capabilities. Manipulation tasks may require
complex language instructions, such as identifying target objects by their
relative positions, to specify human intention. Therefore, we introduce IA-VLA,
a framework that utilizes the extensive language understanding of a large
vision language model as a pre-processing stage to generate improved context to
augment the input of a VLA. We evaluate the framework on a set of semantically
complex tasks which have been underexplored in VLA literature, namely tasks
involving visual duplicates, i.e., visually indistinguishable objects. A
dataset of three types of scenes with duplicate objects is used to compare a
baseline VLA against two augmented variants. The experiments show that the VLA
benefits from the augmentation scheme, especially when faced with language
instructions that require the VLA to extrapolate from concepts it has seen in
the demonstrations. For the code, dataset, and videos, see
https://sites.google.com/view/ia-vla.

</details>


### [639] [Fidelity-Aware Data Composition for Robust Robot Generalization](https://arxiv.org/abs/2509.24797)
*Zizhao Tong,Di Chen,Sicheng Hu,Hongwei Fan,Liliang Chen,Guanghui Ren,Hao Tang,Hao Dong,Ling Shao*

Main category: cs.RO

TL;DR: 本文提出了一种结合信息保真度考虑的数据混合方法，有效提升了通用机器人策略在分布外任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用型机器人策略受益于大规模视觉数据训练，但由于数据同质性，易产生捷径学习，导致其在分布外任务上的泛化能力较弱。现有生成式数据增强虽然增加了多样性，却可能破坏信息保真度，影响学习效果。

Method: 作者提出Coherent Information Fidelity Tuning（CIFT）框架，将数据混合建模为信息保真度优化问题。CIFT通过特征空间的几何属性估算信息保真度，辅助识别训练失稳的“退相干点（Decoherence Point）”。框架配合提出的多视角视频增强引擎（MVAug），生成具因果性解耦的数据，以便于调试和完善数据成分。

Result: 将CIFT应用于现有机器人策略（如π₀和Diffusion Policy），分布外任务成功率提升超过54%。

Conclusion: 仅有数据多样性生成还不够，结合保真度感知的数据组合机制，是提升泛化型机器人成绩的核心因素。

Abstract: Generalist robot policies trained on large-scale, visually homogeneous
datasets can be susceptible to shortcut learning, which impairs their
out-of-distribution (OOD) generalization. While generative data augmentation is
a common approach to introduce diversity, it presents a subtle challenge: data
composition. Naively mixing real and synthetic data can corrupt the learning
signal, as this process often prioritizes visual diversity at the expense of
information fidelity. This paper suggests that robust generalization depends on
principled, fidelity-aware data composition. We introduce Coherent Information
Fidelity Tuning (CIFT), a framework that treats data composition as an
optimization problem. CIFT uses a practical proxy for Information Fidelity
based on the feature-space geometry of a dataset. This enables the
identification of a phase transition, termed the Decoherence Point, where
training stability degrades. The framework includes a generative engine,
Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled
data spectrum for this tuning process. Applying CIFT to policy architectures
such as $\pi_0$ and Diffusion Policy improves OOD success rates by over 54\%.
These results indicate that fidelity-aware composition, beyond data synthesis
alone, is an important component for developing robust, general-purpose robots.

</details>


### [640] [Towards Modular and Accessible AUV Systems](https://arxiv.org/abs/2509.24864)
*Mingxi Zhou,Farhang Naderi,Yuewei Fu,Tony Jacob,Lin Zhao,Manavi Panjnani,Chengzhi Yuan,William McConnell,Emir Cem Gezer*

Main category: cs.RO

TL;DR: 该论文提出了一个名为MVP的开源模块化框架，用于提高自主水下机器人（AUV）的定制化和性能。


<details>
  <summary>Details</summary>
Motivation: 目前AUV（自主水下机器人）的定制化和集成开发存在困难，难以满足不同科研需求。作者旨在通过设计统一并易于扩展的软硬件框架，降低AUV开发的门槛。

Method: 作者开发了MVP框架，包含可扩展的硬件系统和模块化软件架构，集成了新型的推进器和高级图形用户界面（GUI）。此外，通过仿真与实地测试验证了该系统的性能和兼容性。

Result: 论文展示了MVP框架在不同实验中的表现，包括仿真和实际应用环境的测试，证明了其良好的性能和广泛的兼容性。

Conclusion: MVP框架能够显著提升AUV的定制性及易用性，为水下机器人研究和应用提供了强有力的支持。

Abstract: This paper reports the development of a new open- access modular framework,
called Marine Vehicle Packages (MVP), for Autonomous Underwater Vehicles. The
framework consists of both software and hardware designs allowing easy
construction of AUV for research with increased customizability and sufficient
payload capacity. This paper will present the scalable hardware system design
and the modular software design architecture. New features, such as articulated
thruster integra- tion and high-level Graphic User Interface will be discussed.
Both simulation and field experiments results are shown to highlight the
performance and compatibility of the MVP.

</details>


### [641] [Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction](https://arxiv.org/abs/2509.24867)
*Mariadas Capsran Roshan,Edgar M Hidalgo,Mats Isaksson,Michelle Dunn,Jagannatha Charjee Pyaraka*

Main category: cs.RO

TL;DR: 提出了一种基于机器人搭载2D激光雷达(2D LiDAR)的自动超声探头起始姿态估计方法，有效提高远程机器人超声检查的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前超声心动图极度依赖操作者经验，受训技师在偏远或资源有限地区难以获得。现有远程机器人超声虽然解决了远程操作问题，但检查时间长，非专业任务如探头初始定位仍耗费大量时间，亟需自动化手段减少操作负担。

Method: 采用2D激光雷达扫描胸部曲面，通过平面外参标定与非刚性模板匹配，实现人体胸部三维重建和超声探头起始姿态自动估计，是首次将机器人搭载2D LiDAR用于人体表面3D重建。标定RMS残差1.8毫米，旋转不确定小于0.2度，利用两次线性LiDAR扫描重建胸表面，并对齐医学模板确定探头定位。

Result: 模型测试表明，人体模型表面重建误差均值为2.78±0.21毫米。5例人体实验中，自动估计的探头初始点距离临床定义点通常为20-30毫米，同一受试者重复试验变异性小于4毫米。

Conclusion: 该方法首次将2D LiDAR用于人体表面三维重建并用于机器人超声起始定位，自动化精度达到临床应用前提，为远程超声操作提效降负，并展现良好一致性。

Abstract: Echocardiography is a key imaging modality for cardiac assessment but remains
highly operator-dependent, and access to trained sonographers is limited in
underserved settings. Teleoperated robotic echocardiography has been proposed
as a solution; however, clinical studies report longer examination times than
manual procedures, increasing diagnostic delays and operator workload.
Automating non-expert tasks, such as automatically moving the probe to an ideal
starting pose, offers a pathway to reduce this burden. Prior vision- and
depth-based approaches to estimate an initial probe pose are sensitive to
lighting, texture, and anatomical variability. We propose a robot-mounted 2D
LiDAR-based approach that reconstructs the chest surface in 3D and estimates
the initial probe pose automatically. To the best of our knowledge, this is the
first demonstration of robot-mounted 2D LiDAR used for 3D reconstruction of a
human body surface. Through plane-based extrinsic calibration, the
transformation between the LiDAR and robot base frames was estimated with an
overall root mean square (RMS) residual of 1.8 mm and rotational uncertainty
below 0.2{\deg}. The chest front surface, reconstructed from two linear LiDAR
sweeps, was aligned with non-rigid templates to identify an initial probe pose.
A mannequin-based study assessing reconstruction accuracy showed mean surface
errors of 2.78 +/- 0.21 mm. Human trials (N=5) evaluating the proposed approach
found probe initial points typically 20-30 mm from the clinically defined
initial point, while the variation across repeated trials on the same subject
was less than 4 mm.

</details>


### [642] [JuggleRL: Mastering Ball Juggling with a Quadrotor via Deep Reinforcement Learning](https://arxiv.org/abs/2509.24892)
*Shilong Ji,Yinuo Chen,Chuqi Wang,Jiayu Chen,Ruize Zhang,Feng Gao,Wenhao Tang,Shu'ang Yu,Sirui Xiang,Xinlei Chen,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机打球系统JuggleRL，能在真实环境中长期稳定地完成高难度空中接球任务，并显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 无人机与物体进行动态、高精度的交互具有很大挑战性——例如需稳定控制、适应不确定性，尤其是在像空中颠球这样要求高频精确动作感知的任务中。现有方法难以同时实现精确性和鲁棒性，因此亟需更智能的控制策略。

Method: 提出JuggleRL，基于强化学习训练无人机打球策略。采用大规模仿真+真实动力学标定减少仿真到现实的差距，通过奖励设计鼓励球拍中心击球和维持颠球时间，域随机化提升策略适应性。策略输出中层控制命令，由低层控制器执行。系统结合轻量级通讯协议优化感知反应时延，实现实时控制。

Result: JuggleRL在真实环境下平均连续颠球311次，最高462次，远超传统模型方法（平均3.1次，最高14次）。策略能适应不同球重环境，且在新环境下同样表现出较强的泛化能力。

Conclusion: 强化学习能赋予空中机器人在复杂动态任务中的强大、稳健的感知与控制能力，有望推进无人机智能交互技术的发展。

Abstract: Aerial robots interacting with objects must perform precise, contact-rich
maneuvers under uncertainty. In this paper, we study the problem of aerial ball
juggling using a quadrotor equipped with a racket, a task that demands accurate
timing, stable control, and continuous adaptation. We propose JuggleRL, the
first reinforcement learning-based system for aerial juggling. It learns
closed-loop policies in large-scale simulation using systematic calibration of
quadrotor and ball dynamics to reduce the sim-to-real gap. The training
incorporates reward shaping to encourage racket-centered hits and sustained
juggling, as well as domain randomization over ball position and coefficient of
restitution to enhance robustness and transferability. The learned policy
outputs mid-level commands executed by a low-level controller and is deployed
zero-shot on real hardware, where an enhanced perception module with a
lightweight communication protocol reduces delays in high-frequency state
estimation and ensures real-time control. Experiments show that JuggleRL
achieves an average of $311$ hits over $10$ consecutive trials in the real
world, with a maximum of $462$ hits observed, far exceeding a model-based
baseline that reaches at most $14$ hits with an average of $3.1$. Moreover, the
policy generalizes to unseen conditions, successfully juggling a lighter $5$ g
ball with an average of $145.9$ hits. This work demonstrates that reinforcement
learning can empower aerial robots with robust and stable control in dynamic
interaction tasks.

</details>


### [643] [DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits](https://arxiv.org/abs/2509.24903)
*Lantao Li,Kang Yang,Rui Song,Chen Sun*

Main category: cs.RO

TL;DR: 本文提出了DRCP框架，通过跨模态、跨车辆的高精度感知和创新性的轻量扩散模块提升车辆协同感知的鲁棒性和精度，实现了在动态环境下的实时部署。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的协同感知和多智能体融合技术已取得进展，但在实际应用中仍受到部分检测困难和噪声累积的挑战，影响检测精度。本文旨在解决这些现实场景下的问题，提升自动驾驶和移动机器人平台的环境感知能力。

Method: DRCP框架包含两个核心模块：(1)精确金字塔跨模态跨车辆协同感知模块，利用带有相机内参感知的角度划分和注意力机制融合不同智能体的图像特征，同时采用自适应卷积提升外部特征利用率；(2)掩码-扩散-掩码聚合模块，这是一种新的轻量级扩散方法，可增强系统对特征干扰的鲁棒性，并优化鸟瞰视角下特征的表示。

Result: DRCP在移动平台上实现了实时性能，并在具有挑战性的动态驾驶环境下显著提高了检测的鲁棒性和准确率。

Conclusion: DRCP框架为协同感知领域提供了有效且实用的解决方案，提升了困难场景下的下游检测表现，有望推动自动驾驶等领域的真实部署，相关代码将在2025年底公布。

Abstract: Cooperative perception enabled by Vehicle-to-Everything communication has
shown great promise in enhancing situational awareness for autonomous vehicles
and other mobile robotic platforms. Despite recent advances in perception
backbones and multi-agent fusion, real-world deployments remain challenged by
hard detection cases, exemplified by partial detections and noise accumulation
which limit downstream detection accuracy. This work presents Diffusion on
Reinforced Cooperative Perception (DRCP), a real-time deployable framework
designed to address aforementioned issues in dynamic driving environments. DRCP
integrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,
a cross-modal cooperative perception module that leverages
camera-intrinsic-aware angular partitioning for attention-based fusion and
adaptive convolution to better exploit external features; and (2)
Mask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement
module that encourages robustness against feature perturbations and aligns
bird's-eye-view features closer to the task-optimal manifold. The proposed
system achieves real-time performance on mobile platforms while significantly
improving robustness under challenging conditions. Code will be released in
late 2025.

</details>


### [644] [Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation](https://arxiv.org/abs/2509.24907)
*Thanh Long Nguyen,Duc Phu Nguyen,Thanh Thao Ton Nu,Quan Le,Thuan Hoang Tran,Manh Duong Phung*

Main category: cs.RO

TL;DR: 本文提出了一种新的人群交互识别框架，帮助社交机器人更好地理解和适应群体行为，并提高导航的社会意识。


<details>
  <summary>Details</summary>
Motivation: 现有机器人多注重避障，忽视了群体社会互动等线索，导致在人机共处环境中的适应性和自然性差，亟需改进。

Method: 方法基于单目RGB-D相机获取彩色和深度帧，通过3D人体关键点估计和空间位置分析，结合主成分分析（PCA）提取群体主交互方向，利用鞋带公式（shoelace formula）计算兴趣点和互动区域，并以ROS 2程序包实现。

Result: 大量实验表明，该方法可在不同人数和场景下，高效、准确地识别人群互动，单帧处理约4ms，满足实际社交机器人对速度与准确性的要求。

Conclusion: 所提方法有效提高了机器人对人群互动的感知和社交导航能力，易于集成进现有导航系统，对社交机器人应用具有实际价值。

Abstract: {Recognizing human interactions is essential for social robots as it enables
them to navigate safely and naturally in shared environments. Conventional
robotic systems however often focus on obstacle avoidance, neglecting social
cues necessary for seamless human-robot interaction. To address this gap, we
propose a framework to recognize human group interactions for socially aware
navigation. Our method utilizes color and depth frames from a monocular RGB-D
camera to estimate 3D human keypoints and positions. Principal component
analysis (PCA) is then used to determine dominant interaction directions. The
shoelace formula is finally applied to compute interest points and engagement
areas. Extensive experiments have been conducted to evaluate the validity of
the proposed method. The results show that our method is capable of recognizing
group interactions across different scenarios with varying numbers of
individuals. It also achieves high-speed performance, processing each frame in
approximately 4 ms on a single-board computer used in robotic systems. The
method is implemented as a ROS 2 package making it simple to integrate into
existing navigation systems. Source code is available at
https://github.com/thanhlong103/social-interaction-detector

</details>


### [645] [From Code to Action: Hierarchical Learning of Diffusion-VLM Policies](https://arxiv.org/abs/2509.24917)
*Markus Peschl,Pietro Mazzaglia,Daniel Dijkman*

Main category: cs.RO

TL;DR: 本文提出了一种结合代码生成型视觉-语言模型（VLM）和低层扩散策略的分层模仿学习框架，有效提升了机器人操作任务中的模仿与泛化能力，尤其适用于复杂的长时程任务。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习在复杂、长时程的机器人操作任务中面临泛化能力弱和数据稀缺的问题，亟需新的方法提升系统的学习和适应能力。

Method: 构建分层框架：高层用VLM解析任务描述并生成可执行子程序；低层用扩散策略模仿相应机器人子任务行为。利用开源机器人API的子任务函数结构作为有意义的监督标签。为应对非马尔可夫性问题，引入记忆机制保持子任务上下文信息。

Result: 该框架实现了可解释的策略分解，与传统平坦策略相比，泛化能力显著提升，并支持对高层规划和低层控制进行分离评估。

Conclusion: 通过将任务拆分为结构化的子任务并结合VLM及扩散策略方法，该框架有效提升了机器人模仿学习的可解释性、泛化性和评估灵活性，尤其适用于复杂的机器人操作场景。

Abstract: Imitation learning for robotic manipulation often suffers from limited
generalization and data scarcity, especially in complex, long-horizon tasks. In
this work, we introduce a hierarchical framework that leverages code-generating
vision-language models (VLMs) in combination with low-level diffusion policies
to effectively imitate and generalize robotic behavior. Our key insight is to
treat open-source robotic APIs not only as execution interfaces but also as
sources of structured supervision: the associated subtask functions - when
exposed - can serve as modular, semantically meaningful labels. We train a VLM
to decompose task descriptions into executable subroutines, which are then
grounded through a diffusion policy trained to imitate the corresponding robot
behavior. To handle the non-Markovian nature of both code execution and certain
real-world tasks, such as object swapping, our architecture incorporates a
memory mechanism that maintains subtask context across time. We find that this
design enables interpretable policy decomposition, improves generalization when
compared to flat policies and enables separate evaluation of high-level
planning and low-level control.

</details>


### [646] [CineWild: Balancing Art and Robotics for Ethical Wildlife Documentary Filmmaking](https://arxiv.org/abs/2509.24921)
*Pablo Pueyo,Fernando Caballero,Ana Cristina Murillo,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文提出了CineWild，一种结合了机器人学、电影摄影学与伦理学的无人机自主拍摄框架，可用于野生动物纪录片拍摄，在保证画面质量的同时最大程度降低对动物的干扰。


<details>
  <summary>Details</summary>
Motivation: 无人机在纪录片电影，尤其是野生动物纪录片领域应用广泛，能够带来独特的拍摄视角，但同样存在可能惊扰被拍摄动物的伦理问题。因此亟需一种兼顾电影美学和动物福利的无人机拍摄方案。

Method: CineWild框架基于模型预测控制（MPC），能够动态调整飞行路径与相机参数。它包含：自适应变焦以便在安全距离外拍摄、路径规划规避动物视线范围、以及低噪、平滑的飞行动作。这些机制共同实现对动物的最小干扰。

Result: CineWild系统经过仿真环境验证，效果良好。系统代码将在论文被接收后公开。

Conclusion: CineWild实现了机器人、电影叙事与环境伦理的跨学科融合，有望推动野生动物纪录片拍摄技术和伦理实践的进步。

Abstract: Drones, or unmanned aerial vehicles (UAVs), have become powerful tools across
domains-from industry to the arts. In documentary filmmaking, they offer
dynamic, otherwise unreachable perspectives, transforming how stories are told.
Wildlife documentaries especially benefit, yet drones also raise ethical
concerns: the risk of disturbing the animals they aim to capture. This paper
introduces CineWild, an autonomous UAV framework that combines robotics,
cinematography, and ethics. Built on model predictive control, CineWild
dynamically adjusts flight paths and camera settings to balance cinematic
quality with animal welfare. Key features include adaptive zoom for filming
from acoustic and visual safe distances, path-planning that avoids an animal's
field of view, and smooth, low-noise maneuvers. CineWild exemplifies
interdisciplinary innovation-bridging engineering, visual storytelling, and
environmental ethics. We validate the system through simulation studies and
will release the code upon acceptance.

</details>


### [647] [Trajectory Prediction via Bayesian Intention Inference under Unknown Goals and Kinematics](https://arxiv.org/abs/2509.24928)
*Shunan Yin,Zehui Lu,Shaoshuai Mou*

Main category: cs.RO

TL;DR: 提出了一种自适应贝叶斯算法，通过意图推断实现实时轨迹预测，同时估计目标的当前意图和其遵循最短路径策略的程度，对目标突变和未知动态具有鲁棒性。实验表明该方法优于非自适应方法，可广泛应用于机器人系统。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测在实际应用中经常面临目标意图未知且可能变化，且运动特性不确定，导致预测难度加大。现有方法对意图突变和动态变化的适应性不足，难以满足实时性与准确性的要求。

Method: 算法将目标的意图建模为隐马尔可夫状态，意图参数为目标对最短路径策略的遵循度。通过联合更新的方法，同时自适应估计这两个变量，结合采样方式实现带不确定性量化的概率轨迹预测。

Result: 在两组消融实验和500次蒙特卡洛分析中，以及四旋翼和四足机器人等硬件平台验证中，所提方法在准确度和适应性上显著优于非自适应和部分自适应的对比方法，且实时性可达270Hz，无需训练或目标行为的详细先验知识。

Conclusion: 该算法在无须详细先验的前提下，实现了对动态、易变目标的高效实时轨迹预测，在机器人系统中有广泛应用前景。

Abstract: This work introduces an adaptive Bayesian algorithm for real-time trajectory
prediction via intention inference, where a target's intentions and motion
characteristics are unknown and subject to change. The method concurrently
estimates two critical variables: the target's current intention, modeled as a
Markovian latent state, and an intention parameter that describes the target's
adherence to a shortest-path policy. By integrating this joint update
technique, the algorithm maintains robustness against abrupt intention shifts
and unknown motion dynamics. A sampling-based trajectory prediction mechanism
then exploits these adaptive estimates to generate probabilistic forecasts with
quantified uncertainty. We validate the framework through numerical
experiments: Ablation studies of two cases, and a 500-trial Monte Carlo
analysis; Hardware demonstrations on quadrotor and quadrupedal platforms.
Experimental results demonstrate that the proposed approach significantly
outperforms non-adaptive and partially adaptive methods. The method operates in
real time around 270 Hz without requiring training or detailed prior knowledge
of target behavior, showcasing its applicability in various robotic systems.

</details>


### [648] [World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training](https://arxiv.org/abs/2509.24948)
*Junjin Xiao,Yandan Yang,Xinyuan Chang,Ronghan Chen,Feng Xiong,Mu Xu,Wei-Shi Zheng,Qing Zhang*

Main category: cs.RO

TL;DR: 提出World-Env框架，利用世界模型虚拟环境进行RL后训练，有效提升VLA模型在数据稀缺下的表现及任务完成率。


<details>
  <summary>Details</summary>
Motivation: VLA模型依赖大规模示范数据，数据稀缺时性能严重下降。虽然用强化学习（RL）后训练可缓解这一问题，但现实环境无法频繁重置，且任务完成检测机制不足，导致执行冗余动作与低成功率，尤其在高风险场景（如工业自动化）问题更突出。

Method: 提出World-Env，一种基于世界模型模拟环境的RL后训练方案，包括（1）生成时序一致视觉观测的视频世界模拟器，和（2）VLM指导的即时反馈器，提供连续奖励信号与动作终止预测，帮助实现高效、安全的虚拟探索和泛化能力提升。

Result: 在复杂机器人操作任务上，World-Env方法仅需每项任务五条专家示范，即显著提升VLA模型泛化与任务成功率，优于传统依赖真实环境交互的VLA模型。

Conclusion: World-Env突破了数据低效、安全限制和任务执行冗余的局限，为VLA模型提供了实用且可扩展的后训练方案，尤其适用于资源受限的情景。

Abstract: Vision-Language-Action (VLA) models trained via imitation learning suffer
from significant performance degradation in data-scarce scenarios due to their
reliance on large-scale demonstration datasets. Although reinforcement learning
(RL)-based post-training has proven effective in addressing data scarcity, its
application to VLA models is hindered by the non-resettable nature of
real-world environments. This limitation is particularly critical in high-risk
domains such as industrial automation, where interactions often induce state
changes that are costly or infeasible to revert. Furthermore, existing VLA
approaches lack a reliable mechanism for detecting task completion, leading to
redundant actions that reduce overall task success rates. To address these
challenges, we propose World-Env, an RL-based post-training framework that
replaces physical interaction with a low-cost, world model-based virtual
simulator. World-Env consists of two key components: (1) a video-based world
simulator that generates temporally consistent future visual observations, and
(2) a vision-language model (VLM)-guided instant reflector that provides
continuous reward signals and predicts action termination. This simulated
environment enables VLA models to safely explore and generalize beyond their
initial imitation learning distribution. Our method achieves notable
performance gains with as few as five expert demonstrations per task.
Experiments on complex robotic manipulation tasks demonstrate that World-Env
effectively overcomes the data inefficiency, safety constraints, and
inefficient execution of conventional VLA models that rely on real-world
interaction, offering a practical and scalable solution for post-training in
resource-constrained settings.

</details>


### [649] [MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation](https://arxiv.org/abs/2509.24956)
*Jan Ole von Hartz,Lukas Schweizer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: 本文提出了一种名为MSG（Multi-Stream Generative Policy）的新型生成式策略组合框架，可大幅度提升泛化能力与样本效率，极大降低对演示样本的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前的生成机器人策略如Flow Matching支持多模态的策略学习，但样本效率低下。虽然面向对象的策略在一定程度上提升了样本效率，但这一问题仍未根本解决。实际应用中，尤其是在机器人学习中，高质量演示难以获得，因此急需提升策略样本效率和泛化能力的方法。

Method: 作者提出MSG，一个推理时组合的框架：在训练阶段分别训练多个面向对象的策略，推理时将这些策略组合使用。MSG为模型无关，并且只在推理阶段组合，因此适用于多种生成策略与训练方式。实验包括仿真和真实机器人上的大量对比及消融实验。

Result: MSG方法仅需5次演示就能学到高质量的生成政策，相较传统单流方法可减少95%的演示，并将策略性能提高89%。同时MSG支持零样本对象实例迁移，并给出了不同组合方式的消融对比和部署建议。

Conclusion: MSG显著提升了生成式机器人策略在样本效率和泛化方面的性能，且部署灵活。作者同时开源了代码，促进了后续研究与应用。

Abstract: Generative robot policies such as Flow Matching offer flexible, multi-modal
policy learning but are sample-inefficient. Although object-centric policies
improve sample efficiency, it does not resolve this limitation. In this work,
we propose Multi-Stream Generative Policy (MSG), an inference-time composition
framework that trains multiple object-centric policies and combines them at
inference to improve generalization and sample efficiency. MSG is
model-agnostic and inference-only, hence widely applicable to various
generative policies and training paradigms. We perform extensive experiments
both in simulation and on a real robot, demonstrating that our approach learns
high-quality generative policies from as few as five demonstrations, resulting
in a 95% reduction in demonstrations, and improves policy performance by 89
percent compared to single-stream approaches. Furthermore, we present
comprehensive ablation studies on various composition strategies and provide
practical recommendations for deployment. Finally, MSG enables zero-shot object
instance transfer. We make our code publicly available at
https://msg.cs.uni-freiburg.de.

</details>


### [650] [Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks](https://arxiv.org/abs/2509.24972)
*Vijja Wichitwechkarn,Emlyn Williams,Charles Fox,Ruchi Choudhary*

Main category: cs.RO

TL;DR: 本文提出了一种新的一次性模仿学习方法，仅需一次人类演示即可让机器人学会多步骤操作任务，无需额外模型训练或手动标注，且在多个任务上取得了高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的一次性模仿学习方法虽然在单步任务中表现良好，但难以扩展到需要多个步骤的复杂任务，并且通常需要额外的模型训练或人工标注，增加了应用难度。本文旨在解决此类多步任务的一次性学习问题，提高泛化能力与效率。

Method: 该方法允许机器人仅通过一次人类演示、无需额外模型训练或人工标注，学习并完成多步骤操作任务。方法还包括比较不同预训练特征提取器在本框架下的表现和计算效率。

Result: 在多步操作任务中，该方法平均成功率为82.5%；在单步骤任务中为90%，在两类任务中均能达到甚至超过现有基线方法的表现。同时，分析了不同预训练特征提取器在方法框架下的表现差异和计算效率。

Conclusion: 本文提出的方法能显著提升机器人在多步骤、一段演示下的模仿学习能力，减少了对额外训练和人工标注的依赖，为机器人模仿学习在复杂任务中的应用带来了新的突破。

Abstract: Recent advances in one-shot imitation learning have enabled robots to acquire
new manipulation skills from a single human demonstration. While existing
methods achieve strong performance on single-step tasks, they remain limited in
their ability to handle long-horizon, multi-step tasks without additional model
training or manual annotation. We propose a method that can be applied to this
setting provided a single demonstration without additional model training or
manual annotation. We evaluated our method on multi-step and single-step
manipulation tasks where our method achieves an average success rate of 82.5%
and 90%, respectively. Our method matches and exceeds the performance of the
baselines in both these cases. We also compare the performance and
computational efficiency of alternative pre-trained feature extractors within
our framework.

</details>


### [651] [Path Diffuser: Diffusion Model for Data-Driven Traffic Simulator](https://arxiv.org/abs/2509.24995)
*Da Saem Lee,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: 本文提出了一种名为Path Diffuser (PD) 的两阶段扩散模型，可在没有历史轨迹数据的条件下，利用地图信息生成多样化且现实的交通场景，从而仿真和测试自动驾驶规划系统。


<details>
  <summary>Details</summary>
Motivation: 现有的交通场景仿真方法主要存在两大挑战：基于规则的规划器缺乏多样性和现实性，而基于学习的方法过度依赖历史完整轨迹数据，导致无法生成全新且多样的交通场景。对于实际应用，很多路上目标的历史轨迹或位姿信息并不总是可用，这严重制约了场景仿真的规模和性能。因此，亟需一种新方法可在缺乏历史数据的情况下，生成逼真且合规的交通场景。

Method: 作者提出Path Diffuser（PD），这是一种两阶段的扩散模型：首先根据地图条件生成初始位姿，然后再生成相应的动态轨迹，整个过程无需依赖任何历史轨迹数据。该方法还引入以Frenet帧为基础的运动原语作为先验，提升生成轨迹的多样性，并保证其道路合规性。作者还对多智能体交互建模的不同设计方案进行了探索。

Result: 在Argoverse2数据集以及各种分布外地图（如弯道路等）上，PD显著优于现有基线方法，在分布多样性、常识性以及道路合规性等对抗性基准上，分别取得了1.92倍、1.14倍、1.62倍的提升。

Conclusion: PD能够在缺乏历史轨迹数据的条件下，仍然生成多样、合理且道路合规的交通场景，有助于大规模自动驾驶仿真测试，其方法具有良好的泛化能力和实用推进意义。

Abstract: Simulating diverse and realistic traffic scenarios is critical for developing
and testing autonomous planning. Traditional rule-based planners lack diversity
and realism, while learning-based simulators often replay, forecast, or edit
scenarios using historical agent trajectories. However, they struggle to
generate new scenarios, limiting scalability and diversity due to their
reliance on fully annotated logs and historical data. Thus, a key challenge for
a learning-based simulator's performance is that it requires agents' past
trajectories and pose information in addition to map data, which might not be
available for all agents on the road.Without which, generated scenarios often
produce unrealistic trajectories that deviate from drivable areas, particularly
under out-of-distribution (OOD) map scenes (e.g., curved roads). To address
this, we propose Path Diffuser (PD): a two-stage, diffusion model for
generating agent pose initializations and their corresponding trajectories
conditioned on the map, free of any historical context of agents' trajectories.
Furthermore, PD incorporates a motion primitive-based prior, leveraging Frenet
frame candidate trajectories to enhance diversity while ensuring road-compliant
trajectory generation. We also explore various design choices for modeling
complex multi-agent interactions. We demonstrate the effectiveness of our
method through extensive experiments on the Argoverse2 Dataset and additionally
evaluate the generalizability of the approach on OOD map variants. Notably,
Path Diffuser outperforms the baseline methods by 1.92x on distribution
metrics, 1.14x on common-sense metrics, and 1.62x on road compliance from
adversarial benchmarks.

</details>


### [652] [AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation](https://arxiv.org/abs/2509.25032)
*Ryosuke Takanami,Petr Khrapchenkov,Shu Morikuni,Jumpei Arima,Yuta Takaba,Shunsuke Maeda,Takuya Okubo,Genki Sano,Satoshi Sekioka,Aoi Kadoya,Motonari Kambara,Naoya Nishiura,Haruto Suzuki,Takanori Yoshimoto,Koya Sakamoto,Shinnosuke Ono,Hu Yang,Daichi Yashima,Aoi Horo,Tomohiro Motoda,Kensuke Chiyoma,Hiroshi Ito,Koki Fukuda,Akihito Goto,Kazumi Morinaga,Yuya Ikeda,Riko Kawada,Masaki Yoshikawa,Norio Kosuge,Yuki Noguchi,Kei Ota,Tatsuya Matsushima,Yusuke Iwasawa,Yutaka Matsuo,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本文提出了AIRoA MoMa数据集，这是一个面向移动操作机器人的大规模、多模态现实世界数据集，具备同步的视觉、动力与层级标注，填补了现有资源的空白，为机器人任务理解和模型评测提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 随着机器人逐步进入复杂的人类环境，实现能够鲁棒地理解自然语言指令并完成任务的通用智能体至关重要。然而，现有大规模多模态数据集在力矩传感、分层标注和失败案例等核心要素上存在明显不足，制约了机器人移动操作领域的进步。

Method: 作者构建了AIRoA MoMa这一数据集，收集了RGB图像、关节状态、六维力矩传感信号和机器人内部状态，并设计了双层标注格式（子目标和原始动作），以便于分层学习和错误分析。数据集总计25469条实际操作记录，约94小时，采用标准化格式发布。

Result: AIRoA MoMa数据集整合了移动操作、接触丰富的交互以及长时序结构，涵盖大量多模态同步数据和层级标注，充分记录了任务中的成功与失败案例，弥补了现有数据集的多项不足。

Conclusion: AIRoA MoMa为视觉-语言-行动一体化模型的研究提供了关键基准资源，将推动机器人在自然语言指令下完成复杂移动操作任务的能力，首版数据已公开发布。

Abstract: As robots transition from controlled settings to unstructured human
environments, building generalist agents that can reliably follow natural
language instructions remains a central challenge. Progress in robust mobile
manipulation requires large-scale multimodal datasets that capture contact-rich
and long-horizon tasks, yet existing resources lack synchronized force-torque
sensing, hierarchical annotations, and explicit failure cases. We address this
gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset
for mobile manipulation. It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis. The initial dataset comprises 25,469
episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is
fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile
manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa
provides a critical benchmark for advancing the next generation of
Vision-Language-Action models. The first version of our dataset is now
available at https://huggingface.co/datasets/airoa-org/airoa-moma .

</details>


### [653] [AgriCruiser: An Open Source Agriculture Robot for Over-the-row Navigation](https://arxiv.org/abs/2509.25056)
*Kenny Truong,Yongkyu Lee,Jason Irie,Shivam Kumar Panda,Shahab Ahmad,Md. Mukhlesur Rahman,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 本文介绍了AgriCruiser，这是一款低成本、可配置、开源的跨垄农业机器人。该机器人不仅实现了高效除草和作物保护，而且结构可调、易于复制，适用于多种作物和地形。设计资料亦已开源，推动农业自动化领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人普遍价格高昂、适应性差，限制了自动化技术在不同作物和农田布局中的推广。该研究旨在开发一种低成本、易于快速调整的农业机器人平台，适用于多种作物和地形，降低农田自动化门槛。

Method: 设计并实现一台行间可调的跨垄机器人（AgriCruiser），底盘可定制宽度和离地间隙，具备小半径转弯能力，结构采用标准型材组装。集成了高精喷雾除草系统，并在不同地形、作物地块中进行除草和机动性测试。

Result: 在12块亚麻田中，一次机器人喷雾除草处理后，杂草数量比人工除草减少24-42倍，且对作物损伤更低。机动性测试表明该机器人在水泥、沥青、碎石、草地及湿/干土壤上都能稳定通过。整机成本约为5000-6000美元。

Conclusion: 实验结果显示，低成本、可重构的跨垄农业机器人不但可显著提升除草效率、降低人工和作物损伤，还能提供多用途基础平台，加速农田自动化和智能农业的推广。设计和实施细节公开，有助于科研与应用落地。

Abstract: We present the AgriCruiser, an open-source over-the-row agricultural robot
developed for low-cost deployment and rapid adaptation across diverse crops and
row layouts. The chassis provides an adjustable track width of 1.42 m to 1.57
m, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact
pivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland
maneuvers. The platform is designed for the integration of the other
subsystems, and in this study, a precision spraying system was implemented to
assess its effectiveness in weed management. In twelve flax plots, a single
robotic spray pass reduced total weed populations (pigweed and Venice mallow)
by 24- to 42-fold compared to manual weeding in four flax plots, while also
causing less crop damage. Mobility experiments conducted on concrete, asphalt,
gravel, grass, and both wet and dry soil confirmed reliable traversal
consistent with torque sizing. The complete chassis can be constructed from
commodity T-slot extrusion with minimal machining, resulting in a bill of
materials costing approximately $5,000 - $6,000, which enables replication and
customization. The mentioned results demonstrate that low-cost, reconfigurable
over-the-row robots can achieve effective weed management with reduced crop
damage and labor requirements, while providing a versatile foundation for
phenotyping, sensing, and other agriculture applications. Design files and
implementation details are released to accelerate research and adoption of
modular agricultural robotics.

</details>


### [654] [Crop Spirals: Re-thinking the field layout for future robotic agriculture](https://arxiv.org/abs/2509.25091)
*Lakshan Lavan,Lanojithan Thiyagarasa,Udara Muthugala,Rajitha de Silva*

Main category: cs.RO

TL;DR: 提出了一种机器人友好的方形螺旋农作物布局，并结合导航算法，有效提升了农业机器人的导航效率。


<details>
  <summary>Details</summary>
Motivation: 传统为拖拉机优化的线性农田布局不利于机器人导航，存在转弯困难、路径冗长和感知混淆等问题，制约了农业机器人应用效率。

Method: 提出了以机器人为中心的方形螺旋作物布局，并开发了结合DH-ResNet18航点回归、像素到里程计映射、A*路径规划和模型预测控制的导航系统。

Result: 在仿真中，螺旋布局相较线性布局使任务路径缩短28%，执行速度提升25%，与优化U型布局全场覆盖效果相当。多机协作下，“贪婪分配”在螺旋布局上比匈牙利分配法快33-37%。

Conclusion: 重新设计田地几何能大大提升智能农机的导航与协作效率，对自动化农业具有重要意义。

Abstract: Conventional linear crop layouts, optimised for tractors, hinder robotic
navigation with tight turns, long travel distances, and perceptual aliasing. We
propose a robot-centric square spiral layout with a central tramline, enabling
simpler motion and more efficient coverage. To exploit this geometry, we
develop a navigation stack combining DH-ResNet18 waypoint regression,
pixel-to-odometry mapping, A* planning, and model predictive control (MPC). In
simulations, the spiral layout yields up to 28% shorter paths and about 25%
faster execution for waypoint-based tasks across 500 waypoints than linear
layouts, while full-field coverage performance is comparable to an optimised
linear U-turn strategy. Multi-robot studies demonstrate efficient coordination
on the spirals rule-constrained graph, with a greedy allocator achieving 33-37%
lower batch completion times than a Hungarian assignment under our setup. These
results highlight the potential of redesigning field geometry to better suit
autonomous agriculture.

</details>


### [655] [Curriculum Imitation Learning of Distributed Multi-Robot Policies](https://arxiv.org/abs/2509.25097)
*Jesús Roche,Eduardo Sebastián,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文提出了两种新方法，提升多机器人系统（MRS）中仿真与现实中长期协作和感知鲁棒性的学习，通过逐步增长专家轨迹长度和将全局演示转化为本地观测，实现高效鲁棒的分布式控制策略学习。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中长期协作困难，且真实训练数据难以获得，限制了有效策略的学习。本研究旨在解决长期协作和可用感知信息受限的问题。

Method: （1）创新课程学习方案，通过逐步增加专家演示轨迹长度，提高机器人长期协作能力和长期行为准确性；（2）提出将第三人称全局状态演示转换为每个机器人自中心观测的方法，包括邻居筛选、参考系转换和传感器噪声建模；（3）两种方法结合形成基于物理的可扩展仿真分布式策略。

Result: 在两项多机器人任务中、不同队伍规模和噪声水平下实验，结果表明，渐进式课程学习显著提升长期行为准确性，感知估计方法增强了策略对现实不确定性的鲁棒性。

Conclusion: 本文方法可从全局演示中学习出鲁棒且可扩展的分布式控制器，即使缺少专家动作或实际测量也能可靠工作，有效推进多机器人系统协同控制的发展。

Abstract: Learning control policies for multi-robot systems (MRS) remains a major
challenge due to long-term coordination and the difficulty of obtaining
realistic training data. In this work, we address both limitations within an
imitation learning framework. First, we shift the typical role of Curriculum
Learning in MRS, from scalability with the number of robots, to focus on
improving long-term coordination. We propose a curriculum strategy that
gradually increases the length of expert trajectories during training,
stabilizing learning and enhancing the accuracy of long-term behaviors. Second,
we introduce a method to approximate the egocentric perception of each robot
using only third-person global state demonstrations. Our approach transforms
idealized trajectories into locally available observations by filtering
neighbors, converting reference frames, and simulating onboard sensor
variability. Both contributions are integrated into a physics-informed
technique to produce scalable, distributed policies from observations. We
conduct experiments across two tasks with varying team sizes and noise levels.
Results show that our curriculum improves long-term accuracy, while our
perceptual estimation method yields policies that are robust to realistic
uncertainty. Together, these strategies enable the learning of robust,
distributed controllers from global demonstrations, even in the absence of
expert actions or onboard measurements.

</details>


### [656] [Safe Planning in Unknown Environments using Conformalized Semantic Maps](https://arxiv.org/abs/2509.25124)
*David Smith Sundarsingh,Yifei Li,Tianji Tang,George J. Pappas,Nikolay Atanasov,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出了一种解决在未知环境下语义规划任务的算法，尤其适用于存在感知不确定性的场景。该方法无需事先了解传感器模型或噪声特性，通过对实时生成的语义地图进行不依赖模型的置信推断，实现了在用户需求概率下完成任务，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人规划方法要么忽略感知不确定性，无法保证任务的正确性，要么需要已知精确的传感器模型和噪声分布。在真实世界场景下，往往无法满足这些假设，因此需要一个能在感知不确定性下保障任务完成概率的规划方法。

Method: 作者首次提出了一种结合语义地图与无模型、无分布假设的置信预测（conformal prediction）的规划方法。机器人在环境中自主构建语义地图，通过置信推断量化地图的不确定性，并据此生成满足用户要求概率的规划路径，无需已知任何传感器模型或噪声特性。

Result: 大量实验表明，该方法能够持续、稳定地实现用户设定的任务完成概率，并在多项基准对比中在任务成功率方面明显优于其他现有方法。实验同时验证了理论上的成功概率。

Conclusion: 本文方法实现了在感知不确定性条件下的安全高效语义任务规划，无需预知传感器特性，能够显著提升任务完成的可靠性，对实际应用具有广阔前景。

Abstract: This paper addresses semantic planning problems in unknown environments under
perceptual uncertainty. The environment contains multiple unknown semantically
labeled regions or objects, and the robot must reach desired locations while
maintaining class-dependent distances from them. We aim to compute robot paths
that complete such semantic reach-avoid tasks with user-defined probability
despite uncertain perception. Existing planning algorithms either ignore
perceptual uncertainty - thus lacking correctness guarantees - or assume known
sensor models and noise characteristics. In contrast, we present the first
planner for semantic reach-avoid tasks that achieves user-specified mission
completion rates without requiring any knowledge of sensor models or noise.
This is enabled by quantifying uncertainty in semantic maps - constructed
on-the-fly from perceptual measurements - using conformal prediction in a
model- and distribution-free manner. We validate our approach and the
theoretical mission completion rates through extensive experiments, showing
that it consistently outperforms baselines in mission success rates.

</details>
