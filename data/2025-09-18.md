<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.RO](#cs.RO) [Total: 64]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出了一种新的基于证据的检索机制，用于更加透明、可审计的不确定性感知决策，在CIFAR-10/100实验中表现优于传统熵阈值法。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性决策多采用全局阈值（如熵），但这样容易导致不够个性化、不可解释，且在实际应用中容易出现置信错误等问题。

Method: 为每个待测样本在嵌入空间检索临近样本（exemplars），通过Dempster-Shafer理论融合这些样本的预测分布，利用融合的信念值作为该实例的动态判决阈值，实现基于证据、实例自适应的不确定性感知。

Result: 在CIFAR-10/100数据集和不同骨干网络（BiT和ViT）上进行了实验，新方法在决策时，错误但置信的样本显著减少，与使用全局熵阈值相比能实现更好的或相当的不确定性感知性能，且审查负载可控。只需少数几个证据样本即可获得良好效果，增加更多证据收益有限。

Conclusion: 基于证据的个性化阈值机制在实际应用中比固定熵阈值更加可靠和可解释，为运用不确定性感知决策提供了优选方案。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 本文系统比较了混合量子-经典神经网络与纯经典神经网络在MNIST、CIFAR100和STL10三个基准数据集上的表现，评估了性能、效率与鲁棒性。混合模型在准确率、训练速度、参数数量等方面均有优势，尤其在复杂任务上优势更明显。


<details>
  <summary>Details</summary>
Motivation: 近期量子计算与人工智能结合受到关注，但其实际优势尚未明确。该研究旨在从多角度全面检验混合量子-经典神经网络能否优于传统全经典神经网络，尤其在复杂视觉任务和实际资源使用时的表现。

Method: 将可参数化的量子电路集成到经典深度学习体系架构中，构建混合模型，并与传统CNN模型在三个标准视觉数据集（MNIST、CIFAR100、STL10）上进行对比。每个模型训练50轮，记录验证集/测试集准确率、训练时长、计算资源消耗及对抗鲁棒性（ε=0.1）的差异。

Result: 混合模型在所有数据集上最终准确率均高于经典模型，且在CIFAR100和STL10复杂任务上提升幅度更大。混合模型训练速度更快，参数和内存消耗更低，且在简单任务上对抗鲁棒性明显增强（如MNIST），在复杂数据集上对抗攻击表现相当。

Conclusion: 混合量子-经典架构在分类精度、训练效率、参数可扩展性等方面较传统纯经典架构具有显著优势，特别适用于复杂视觉任务。

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 本文提出了一个集成技术框架，有效提升高速公路拥堵检测与预警的准确性和可靠性。通过改进YOLO和DeepSort算法，提高了交通流感知精度，并利用GRU-Attention模型显著增强了拥堵预测的时效性和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有的高速公路拥堵检测-预测系统在车辆遮挡情况下识别准确率低、对长时间序列的预测能力差，导致预警和管控效果有限。因此，亟需一种能够提升感知和预测性能的技术方案驱动智能交通发展。

Method: （1）在交通流感知方面，YOLOv11通过用DIoU Loss替代GIoU Loss被优化为YOLOv11-DIoU，同时DeepSort算法结合了马氏（运动）与余弦（外观）距离提升多目标跟踪表现；（2）在拥堵预测方面，提出了GRU-Attention模型，能更好捕捉流量、密度、速度等多维度数据的时序与依赖特征，实现提前10分钟的拥堵预警。

Result: 优化后的YOLOv11-DIoU在高速公路实测视频中mAP提升至95.7%，遮挡漏检率仅5.3% ；改进的DeepSort在指标MOTA上达到93.8%，ID-Switch极低。GRU-Attention模型在拥堵预警准确率、提前量和空间重叠率等方面远优于传统方法，具备极佳泛化和移植能力。

Conclusion: 该集成框架为高速公路拥堵智能检测与预警提供了强有力的技术支撑，有助于提升高速公路运行效率和区域互联水平，具有良好智能交通产业应用前景。

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 本文提出了一种基于车载超声波传感器数据的云端实时路边停车信息服务，通过机器学习实现测试流程的自动化，大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 目前测试停车服务需要大量人工参与，效率低且成本高，因此需要优化测试流程，减少人力资源消耗。

Method: 采用卷积神经网络（CNN）对超声波检测到的信息进行图像模式识别，将人工分析过程自动化，丰富数据库，用机器替代人工分析。

Result: 通过设定的性能指标，自动化工具将人工资源耗时减少了99.58%。

Conclusion: 自动化工具显著提高数据分析和测试效率，极大减轻人工工作负担，并为未来进一步发展与应用提供了方向。

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文系统分析了视觉-语言模型（VLM）如CLIP在零样本OOD检测中的机理、优势和行为鲁棒性。VLM不仅优于单模态方法，而且其表现与嵌入空间特性有关。同时，VLM对图像噪声鲁棒但对提示词非常敏感。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现VLM在零样本OOD检测表现突出，但其有效性原因、相对单模态方法的优势及鲁棒性等方面的理解仍然不充分。为推动VLM在可靠AI系统中的应用，有必要对其OOD检测机理及弱点进行深入剖析。

Method: 作者通过构建系统化实验，结合ID（分布内）与OOD（分布外）提示，详尽分析VLM嵌入空间的关键特性，并将VLM与各种单模态方法进行对比。实验同时关注VLM对不同图像噪声和提示词表达方式的灵敏度。

Result: 结果显示VLM在零样本OOD检测上明显优于单模态方法，其优势源于其可以捕捉丰富语义新颖性。同时，VLM对常见图像噪声具有较强鲁棒性，但对提示词表达方式十分敏感，表现出未充分研究的行为不对称性。

Conclusion: 本文为VLM在OOD检测中的优势与脆弱性提供了结构化新认识，为未来设计更健壮、更可靠的检测方法提供了实证基础和方向性建议。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [6] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 本文提出了一种基于曲率的几何分析方法，用于量化和评估离散度量空间的数据表示及降维效果。


<details>
  <summary>Details</summary>
Motivation: 在处理高维数据与复杂网络时，理解其内在几何结构（如曲率）能揭示数据的本质特性。本研究旨在通过曲率理论扩展对离散度量空间的描述，并量化各种数据降维方法的表现。

Method: 基于最新发展的截面曲率抽象概念，作者提出了一种新方法，利用离散度量空间中点集间的曲率关系，构建几何曲率剖面。再据此曲率剖面，提出一个定量指标，衡量如降维后数据表示的效果。

Result: 通过实验证明，该曲率分析能有效估计数据的内在维数，并用于分析大规模实证网络的几何结构，进而评价多种降维方法的优劣。

Conclusion: 基于曲率的几何分析为评估数据内在结构及降维方法有效性提供了有力工具，拓展了对复杂离散空间的理解。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [7] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本文利用机器学习和遥感技术对斐济Nadi地区2013-2024年土地利用/覆盖变化进行了对比分析，重点监测城市区域的变化。


<details>
  <summary>Details</summary>
Motivation: 斐济作为发展中国家，正经历快速城市化，对土地利用/覆盖变化的监测和建模具有重要意义。

Method: 采用Landsat-8卫星影像，结合Google Earth Engine，实现了有监督学习（卷积神经网络）与无监督学习（k-means聚类）的方法，生成土地覆盖地图并检测时序变化。

Result: 成功实现了土地利用/覆盖分类，提供了城市区域随时间变化的可视化结果。

Conclusion: 本研究为斐济城市化过程中土地利用/覆盖变化检测和建模提供了技术支持。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [8] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于电力传输系统实时异物入侵检测与跟踪的新型三阶段框架，在真实场景下具备高精度、强鲁棒性且资源消耗低，适用于低成本边缘设备。


<details>
  <summary>Details</summary>
Motivation: 电力传输系统中的异物入侵（FOI）有可能引发安全事故，因此需开发一种高效、实时、可在资源受限终端部署的检测和跟踪方法。现有方法在实时性、准确性或部署灵活性上存在不足。

Method: 提出一个三阶段的处理架构：（1）使用YOLOv7分割模型进行快速、鲁棒的异物定位；（2）利用基于ConvNeXt的特征提取器结合三元组损失训练，产生辨识性强的特征嵌入；（3）通过特征辅助IoU多目标跟踪，有效提升遮挡和动态场景下的跟踪表现。同时，整套流程针对低成本边缘计算硬件进行混合精度推理优化，并允许通过增量式添加新目标嵌入实现系统持续更新，无需反复训练主模型。

Result: 在真实监控和无人机视频数据集上，实验显示该框架在多种复杂FOI场景下具有高准确率和强鲁棒性。硬件测试也证实其在NVIDIA Jetson等嵌入式设备上的实用性与可扩展性。

Conclusion: 该三阶段框架不仅能高效、准确、实时地检测与跟踪电力系统异物，还支持边缘设备部署和系统增量更新，具有很高的实际应用前景。

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [9] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: 本论文提出了EdiVal-Agent——一个面向对象、自动化且可扩展的多轮指令图像编辑评估框架，并构建了多轮编辑基准EdiVal-Bench，以更精细、自动化和贴近人类直觉地评估当前图像编辑方法表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑评估存在瓶颈：要么依赖成对参考图像，范围有限、存在生成模型偏差；要么完全依赖零样本视觉-语言模型，对指令执行、内容一致性、视觉质量评估不精确。为此，作者需要一种既精细又可扩展、能更好对齐人类判断的自动评估体系。

Method: 提出EdiVal-Agent框架：1）将图像分解为有语义的对象；2）自动生成多样的上下文相关编辑指令；3）集成VLMs与开放词汇目标检测器实现更精细的指令执行评估；4）用语义特征提取器评价内容一致性；5）通过人类偏好模型评价视觉质量。其流程为模块化，便于后续工具集成和提升。

Result: 实验表明，将VLM和目标检测器结合，在指令执行度评估上比单用VLM或CLIP更贴合人类评价。同时，利用EdiVal-Bench基准覆盖9类指令与11个主流模型，框架能有效发现当前方法易出现的失败模式。

Conclusion: EdiVal-Agent提供了更可靠、可解释、可扩展的多轮指令图像编辑自动化评估方法，推动领域更准确地量化与分析模型表现，为未来编辑系统改进提供有力支撑。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [10] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: 本文提出了MapAnything，一种统一的Transformer前馈模型，能处理图像和几何信息，直接回归得到三维场景和相机参数，能够高效应对多种3D视觉任务，并在多项实验中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉任务（如结构恢复、深度预测、相机定位等）多依赖于专门的模型，不易泛化。希望开发一个通用、统一的模型，可以高效解决广泛的三维场景理解问题。

Method: MapAnything基于Transformer前馈架构，输入可以包含单/多张图像和可选的几何先验（如相机内参、姿态、深度、重建信息），模型输出为三维场景几何和相机参数。设计上采用分解多视角场景表示，包括深度图、本地光线映射、相机姿态和度量尺度因子，以实现不同任务统一建模。通过标准化监督与训练以及灵活输入增强，使模型可广泛适用多种3D任务。

Result: 实验表明MapAnything在包括无标定SFM、有标定多视角立体、单目深度预测、相机定位和深度补全等任务中表现优异，超越或匹敌各自任务的最优前馈模型。

Conclusion: MapAnything证实了通过统一Transformer模型处理多种3D视觉任务的可行性和高效性，有望成为通用的3D重建基础模块。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [11] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: 提出了一种基于高层语义增强的跨模态定位方法（SCM-PR），有效提升了在GPS不可用环境下，RGB图片与LiDAR地图间的定位准确性，并在KITTI等数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的视觉定位方法对环境变化（如天气、光照、季节）敏感，跨模态定位虽然融合了RGB与LiDAR，但在复杂场景、细粒度匹配及视角变化下表现仍有限。因此，亟需更鲁棒的跨模态地点识别方法。

Method: 方法提出了SCM-PR框架，主要包括：1）用VMamba主干网络对RGB图像进行特征提取；2）设计语义感知特征融合（SAFF）模块，将分割掩码与地点描述子融合；3）设计融合几何与语义的LiDAR描述子；4）在NetVLAD中引入跨模态语义注意力机制以提高匹配性能。此外，基于语义的一致性损失与多视角语义-几何匹配机制嵌入对比学习框架进一步提升性能。

Result: 在KITTI与KITTI-360数据集上的实验显示，所提SCM-PR方法在跨模态地点识别任务中，优于现有主流方法，达到了最新的精度水平。

Conclusion: 引入语义增强机制的跨模态定位框架能显著提升在多变环境下的鲁棒性和准确性，为无人车等系统在无GPS场景下提供了更实用的解决方案。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [12] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 本文提出一种场景自适应的晶格向量量化（SALVQ）方法，用于高效压缩3D高斯渲染（3DGS）数据，提升压缩率和保真度，并可灵活调整压缩比。


<details>
  <summary>Details</summary>
Motivation: 3DGS技术虽能实现高质量和实时的三维渲染，但其数据量庞大，影响存储和传输成本，亟需高效的数据压缩方法。目前的神经压缩方法多采用简单的标量量化（USQ），未充分挖掘更高效的量化策略潜力。

Method: 作者提出用晶格向量量化（LVQ）替换USQ，并对每个场景自适应优化晶格基，提升量化适应性和压缩率，同时在系统中集成SALVQ，实现最小修改和额外计算开销。此外，可动态调整晶格密度，支持多码率压缩。

Result: SALVQ在现有3DGS压缩架构中显著提升了率失真（R-D）性能，且可通过调整参数适配不同码率，减少所需模型数量、降低训练时间和内存消耗。

Conclusion: SALVQ方法兼顾了高效率压缩和低复杂度，实现了对3DGS数据高效、灵活的压缩，具有良好的实际应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [13] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: 本文提出一种新任务与方法，用于从图像中检测公共空间中群体层次的社会互动，并发布了大规模数据集支持研究。


<details>
  <summary>Details</summary>
Motivation: 理解公共空间中群体层次的社会互动对城市规划和设计包容性环境至关重要，但这需要识别图像中超越传统目标检测的复杂语义线索。

Method: 设计了一个社会群体区域检测任务，并提出MINGLE三阶段管道：第一步为人体检测和深度估计，第二步利用视觉语言模型（VLM）推理判断个体对之间的社会关联，第三步通过轻量级空间聚合算法定位社会关联群体。

Result: 发布了包含10万张城市街景图片的新数据集，带有个体和社会群体的标注（结合人工与MINGLE自动输出），验证了方法的实用性及高覆盖率。

Conclusion: 该研究为理解和检测图像中复杂的社会群体互动提供了有效的新方法与工具，并通过数据集推动相关研究发展。

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [14] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: 该论文提出了BiasMap框架，用于检测和缓解文本到图像生成模型中的概念层级偏见，揭示并量化构成偏见的概念纠缠问题，并提出新方法进行有效纠正。


<details>
  <summary>Details</summary>
Motivation: 当前黑盒生成模型（特别是文本到图像模型）的偏见检测主要关注输出层面的人口统计分布，但无法确保经过缓解后的概念表征实现真正解耦。因此，迫切需要方法深入到生成过程中的概念层面，识别和量化潜在的结构性偏见。

Method: 提出一个与模型无关的BiasMap框架，利用cross-attention归因图揭示人口统计学属性（如性别、种族）与语义属性（如职业）之间的表征纠缠。通过计算这些概念的归因图的IoU（交并比），定量描述它们的空间重叠。进一步通过能量引导的扩散采样，在去噪过程中直接修改潜在空间，并尽量缩小SoftIoU值，用以缓解偏见。

Result: 实验发现，现有的公平性方法虽然能降低输出分布的偏差，但往往无法消除概念层面的纠缠。而本方法不仅有效降低了概念层面的纠缠，还能配合分布性偏差的缓解手段，提高整体公平性。

Conclusion: BiasMap为发现和纠正文本到图像生成模型中的深层表征偏见提供了新的分析视角和缓解手段，能够补足现有仅注重分布层面的偏见缓解方法，提升生成模型的解释性和公平性。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [15] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: 本文提出了一款名为LivePyxel的Python图形界面工具，实现了与实验室成像设备的无缝集成，支持实时图像标注，解决了传统标注工具难以与实时数据流对接的难题。


<details>
  <summary>Details</summary>
Motivation: 现有图像标注软件多需上传预采集的数据集，流程繁琐，且不利于实验室环境下对显微镜等成像设备产生的实时图像进行标注，影响了AI模型在科学领域的实际部署效率。

Method: 作者开发了LivePyxel，它支持Python环境下，与网络摄像头、显微镜等成像系统直接集成。通过简单且易用的界面，用户能利用Bézier曲线、二值掩膜和非破坏性图层等工具对实时采集的图像进行精确标注。技术上集成了OpenCV和Numpy等高性能矩阵计算库，实现对视频、图像的高效处理。

Result: LivePyxel可以跨多种视频输入设备工作，优化了目标检测的标注流程，简化了数据收集和标注过程，加快了AI模型开发周期。

Conclusion: LivePyxel为实验室等实时数据采集环境提供了高效、灵活的图像标注工具，弥补了现有软件的不足，对AI模型的快速部署与科研流程自动化具有促进作用，且已开源。

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [16] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 本论文提出了DEFT-VTON方法，通过高效微调预训练扩散模型，显著提升虚拟试衣（VTO）的合成质量与推理效率，实现了更低训练与部署成本。


<details>
  <summary>Details</summary>
Motivation: 当前VTO系统虽能生成高质量虚拟试衣图像，但大模型的端到端训练在实际场景中因预算限制（训练/推理/部署成本）难以大规模应用。因此，需要开发高效、且保持性能的微调方法。

Method: 作者提出使用Doob's h-transform efficient fine-tuning（DEFT）方法，仅训练占预训练模型参数1.42%的一小部分h-transform网络，冻结原有参数，实现有条件的定制化。为提升推理速度，进一步提出自适应一致性损失，将一致性loss与去噪匹配loss按数据自适应方式结合，在无需完全蒸馏的前提下降低推理步数且维持性能。

Result: 实验显示，DEFT-VTON在虚拟试衣任务上达到了最优性能，仅需15步去噪即取得和现有方法相当甚至更优的效果，实现了低成本场景下的高性能VTO应用。

Conclusion: DEFT-VTON突破了现有高效微调的限制，有效降低实际应用中的资源消耗，同时保证模型性能，为大规模虚拟试衣系统的部署提供了新途径。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [17] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 本文提出了一种通过数据增强和生成网络来改进自动驾驶中行人识别的数据生成方式，增强了合成数据与真实数据的一致性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要应对各种特定交通场景，而合成数据虽重要，但与真实数据存在域差。提升行人识别准确性需要更真实的合成数据。

Method: 作者对Cityscapes数据集进行虚拟行人增强，并提出了一种新的生成网络，通过对抗学习匹配数据集的光照条件，从而提升合成数据的真实感。

Result: 在语义分割和实例分割任务上对方法进行了评估，展示了增强后的数据对识别性能的提升。

Conclusion: 通过引入拟真光照条件的虚拟行人增强方法，有效缩小了合成与真实数据间的差异，提高了行人识别与分割能力。

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [18] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 本文提出了一种新型的可解释神经网络框架FunKAN，用于医学图像增强和分割，创新性地将Kolmogorov-Arnold理论扩展到函数空间，并在多个医学数据集上优于现有KAN方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强和分割方法受限于复杂网络结构和解释性差，而现有可解释的KAN方法破坏了图像的空间结构，因此需要设计既可解释又适用于医学图像处理的新方法。

Method: 提出Functional Kolmogorov-Arnold Network (FunKAN)，将Kolmogorov-Arnold表示定理扩展到函数空间，采用傅里叶分解与Hermite函数进行内层函数学习，并应用于多项医学图像处理及分割任务，包括MR图像伪影抑制和多种医学分割（U-FunKAN）。

Result: 在多个医学图像增强与分割任务（IXI、BUSI、GlaS、CVC-ClinicDB等数据集）中，FunKAN及其分割变体U-FunKAN均在PSNR、TV、IoU、F1等指标上超越其他KAN类方法。

Conclusion: FunKAN框架兼顾理论函数逼近与实际医学图像分析，提供了一种对临床应用友好、可解释且高性能的解决方案，填补了理论与医学影像实际应用之间的空白。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [19] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 该论文针对仇恨视频检测提出了一种新颖的多模态双流图神经网络（GNN）模型，通过实例图和权重图突出仇恨内容并提升检测准确性。实验结果在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频分类方法虽然比单模态效果更好，但它们一般对所有内容一视同仁，忽略仇恨内容只需要极小部分就能决定视频是否涉仇，同时现有方法也难以系统性地建模视频中结构化信息，影响融合效果。

Method: 作者设计了多模态双流图神经网络模型。首先分割视频为多个实例构建实例图，提取实例级特征；接着通过权重图对这些实例特征赋予重要性权重，突出可能的仇恨实例。最后将权重与特征结合用于视频分类。整个过程采用图神经网络结构系统关联同模态和跨模态的信息。

Result: 在公开数据集上，本方法在仇恨视频分类任务中取得了最优效果（state-of-the-art），同时在模型可解释性方面也有较好表现。

Conclusion: 所提出的图神经网络模型能有效提升对仇恨视频的多模态检测准确率，尤其能突出少量关键仇恨内容，方法先进且具有较强解释性。

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [20] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 该论文提出了ColonCrafter，一种基于扩散模型的深度估计算法，能够从单目结肠镜视频中生成时序一致的深度图，并在C3VD数据集上实现了零样本下的最新性能。


<details>
  <summary>Details</summary>
Motivation: 目前用于内镜的深度估计模型在视频序列中缺乏时序一致性，这对3D场景重建造成了限制，因此需要一种能够输出时序一致深度图的新方法。

Method: 作者设计了ColonCrafter，基于扩散模型进行单目深度估计，并通过学习合成结肠镜序列中的几何先验来提升时序一致性。同时，通过风格迁移方法将真实临床视频适配到合成训练域，以保持几何结构。

Result: ColonCrafter在C3VD数据集上以零样本方式达到了最优表现，优于通用及内镜专用方法。

Conclusion: 该方法有效提升了内镜场景下的深度估计时序一致性，尽管全轨迹3D重建仍具挑战，但可用于3D点云生成和表面覆盖率评估等临床应用。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [21] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 该论文针对3D高斯铺展（3DGS）技术在嵌入式平台（如微型空中飞行器MAVs）上的应用瓶颈，提出了减少GPU内存占用并提升重建质量的新方法。通过在体素空间中按几何相似性合并冗余的3D高斯基元，以及基于Patch-Grid采样优化基元初始化，显著提升了渲染性能与质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS研究主要关注高性能桌面GPU上的渲染和重建效果，但对于计算资源有限的嵌入式平台关注较少，尤其是在GPU内存消耗和渲染质量之间难以权衡。因此，需要优化3DGS以适应资源受限设备。

Method: 1. 在体素空间根据几何相似性合并冗余的3D高斯基元，从而减少GPU内存占用。
2. 采用基于Patch-Grid的点采样方式对3D高斯基元进行初始化，提升对全场景的建模精度。

Result: 该方法在公开数据集上进行了定量和定性评测，结果表明：在不影响系统运行时性能的前提下，GPU内存占用降低且渲染质量有所提升。

Conclusion: 提出的基于体素空间合并与Patch-Grid采样的3DGS优化方法，能有效平衡嵌入式平台的资源消耗和重建质量，对资源受限场景下的3D重建和渲染具有推广意义。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [22] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架，针对自动驾驶轨迹预测中的分布外（OOD）检测问题，引入自适应机制实现复杂环境下的稳健检测，并在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在实际部署中不可避免地遇到训练数据与真实分布不一致的问题，尤其是罕见或未充分表示的交通场景会导致预测模型出现分布外（OOD）案例。现有OOD研究主要关注于计算机视觉任务如目标检测，而对轨迹层面的OOD检测探索较少。

Method: 在前人将轨迹级OOD检测表述为变点检测问题基础上，提出引入自适应机制，显式建模预测误差的不同模式及其随时间的动态演化。框架通过分析和建模数据集内误差分布演变，从而提升检测的鲁棒性与效率。

Result: 在多个真实世界轨迹预测数据集上，所提方法在分布外检测延迟和虚警率方面均取得了显著提升。综合实验表明，该框架在准确性与计算效率上均大幅优于现有不确定性量化（UQ）和视觉为主的OOD方法。

Conclusion: 该方法为实现更为可靠和驾驶环境感知的自动驾驶系统提出了实用可行的分布外检测路径，在实际部署中具有重要应用价值。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [23] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 本文提出利用卫星影像对比和深度学习检测亚马逊雨林砍伐，并用语义模型自动标注变化区域，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林对全球气候和生物多样性至关重要，但正遭受大规模毁林。准确、自动地监测和分析毁林现象对环境保护至关重要。

Method: 方法采用地球观测卫星获取的同一区域不同时期影像，利用深度学习对比分析并检测森林覆盖变化。同时构建视觉语义模型，从相关科学文献中提取关键字，为检测到的变化自动生成注释。

Result: 在亚马逊雨林影像对数据集上，方法能有效检测毁林区域，并自动生成相关语义标注，实验结果显示准确度较高。

Conclusion: 该方法为监测和研究毁林影响提供了实用工具，方法本身具备通用性，可扩展应用于其他领域的环境变化检测。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [24] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 提出了一种结合视觉与语言模型（VLMs）的多模态医学图像分析智能框架，可用于多种成像方式下的自动肿瘤检测和临床报告生成。该系统表现出较高的异常检测性能和良好的解释性，并集成了友好界面，具备零样本学习能力，但仍需进一步的临床验证。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学影像分析中的进步极大提升了诊断效率和决策过程，但临床场景下自动、多模态和高可解释性的智能诊断工具仍稀缺。针对现有系统在多模态适应性、信息提取和用户交互上的不足，开发统一且智能的影像分析平台成为需求。

Method: 本工作提出了整合Google Gemini 2.5 Flash的多模态VLM框架，用于CT、MRI、X光和超声等影像的肿瘤自动检测及报告生成。该方法融合视觉特征提取、自然语言处理、坐标验证与概率高斯建模多种算法，并通过多层可视化、精细化Prompt设计和文本分析，实现了结构化信息获取和临床友好的解读界面。

Result: 系统实现了跨多种医学影像模态的高效异常检测，定位精度平均偏差为80像素。展示了零样本学习能力并减少了对大型数据集的依赖，用户界面便捷，能很好地嵌入临床流程。

Conclusion: 该多模态VLM框架显著提高了自动诊断支持和放射学工作流效率。然而，该系统在临床实际部署前仍需进行多中心和大规模临床验证。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [25] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: 本文提出并推广了一种基于聚类的定位算法CLAP，能有效应对噪声与异常值，已在RoboCup 2024人形机器人足球赛中取得成果，并扩展到三维定位和图像拼接等更广泛领域。


<details>
  <summary>Details</summary>
Motivation: 传统定位算法面对噪声和异常特征点时表现不佳，如RANSAC等方案依赖于投影误差的验证，可能受限于数据分布的复杂性。为提升鲁棒性并兼顾算法效率，提出了聚类思想处理多重可能性下的定位。

Method: CLAP算法利用聚类策略识别多重可能的定位结果，通过对特征匹配结果进行聚类抑制异常值，从而提升定位精度。本文进一步将其从平面（2D）拓展至三维空间（3D）定位及图像拼接任务，并阐述CLAP与RANSAC、Hough变换的内在联系。

Result: CLAP在RoboCup 2024中帮助团队赢得冠军，显现出其抵抗异常值和噪声的显著效果。扩展后的CLAP同样能适用于3D定位与图像拼接领域，体现出方法的通用性和优越性。

Conclusion: 基于聚类的CLAP算法为多种定位与匹配问题提供了鲁棒、高效的解决途径，其广泛适用性和与其它经典算法的关联，使其成为应对不确定性和噪声的有力工具。

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [26] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: 本文提出了SAMIR框架，利用Segment Anything Model（SAM）增强医学图像配准的特征提取，显著提升配准精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像配准需要准确地提取与组织形态相关的特征。现有弱监督方法依赖解剖先验（如分割掩码、标记点），但这些标签并不总是易得，限制了实际应用。

Method: 提出了一种基于SAM的任务适应性管线，利用其图像编码器提取结构感知特征嵌入，并引入轻量级3D头进行特征精炼。此外，设计了分层特征一致性损失，实现由粗到细的特征匹配。

Result: 在心脏和腹部CT标准数据集上的实验表明，SAMIR在同类最新方法中表现优越，ACDC提升2.68%，腹部数据集提升6.44%。

Conclusion: SAMIR方法无需弱标记就能显著提升医学图像配准精度，具有良好应用前景，并将在GitHub开源。

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [27] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出了一种基于联邦学习（FL）的方法，用于通过分布式方式识别和定位卫星影像中的森林砍伐，实现了数据隐私保护与高效协作。


<details>
  <summary>Details</summary>
Motivation: 准确识别和定位卫星影像中的森林砍伐对于地理信息分析和生态保护至关重要。传统集中式训练方法需汇总各地数据，容易造成数据隐私泄露。为解决数据安全与隐私保护问题，亟需分布式、协作式模型训练方法。

Method: 作者提出在分布式边缘节点利用联邦学习框架FLOWER与RAY平台进行协作建模，各卫星数据中心作为独立客户端，本地处理并参与模型训练。实验使用了YOLOS-small（视觉Transformer变体）、Faster R-CNN + ResNet50骨干网和Faster R-CNN + MobileNetV3骨干网三种模型，使用公开数据集训练和测试。

Result: 实验表明，所提联邦学习框架在保护数据隐私的前提下，能够有效完成基于图像分割的森林砍伐检测任务，并实现多客户端的高效分布式协作。RAY工具确保了有效的客户端模拟环境创建，优化了实验流程。

Conclusion: 该研究展示了联邦学习在卫星遥感图像分析中的应用潜力，既保障了客户端数据安全，又实现了高效协同的森林砍伐检测，可为后续分布式遥感图像处理和其他隐私敏感的视觉任务提供借鉴。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新框架GARPS，通过直接对齐两个独立重建的3D场景，实现更精确且鲁棒的相机位姿估计，并在公开数据集上取得超过现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的两视图位姿估计方法只能获得相对位移的比例信息（即只知道方向，距离是模糊的），且在基线较宽或场景无纹理、反光表面时效果差。因此需要一种能获得真实尺度信息且更鲁棒的新方法。

Method: GARPS首先利用单目深度估计器和高斯场景重建器，为每张图片生成带度量信息的3D高斯混合模型（GMM）；接着，用现有两视图估计器提供初值，通过优化一种可微GMM对齐目标函数微调相机姿态。这个目标函数同时考虑几何结构、颜色、协方差和语义特征的一致性，无需显式2D匹配，对遮挡和无纹理区域具备鲁棒性。

Result: 在Real-Estate10K数据集上，GARPS显著优于传统与最新学习方法，包括MASt3R，表现出更好的鲁棒性和精度。

Conclusion: GARPS为多视角几何和单视角感知的结合提供了有效途径，实现了更精确且适用范围更广的绝对尺度相机相对位姿估计，具有实用价值。

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 本文提出用查找操作（lookup operation）替换传统卷积神经网络中的乘法运算，以减少计算量和能耗，并加速推理速度，适用于移动及边缘设备。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中大量的乘法操作计算复杂度高，能耗大，且推理慢，影响了其在移动及资源受限设备上的部署。查找表法在边缘设备已用以简化复杂计算，因此本文探索用查找操作简化CNN核心计算。

Method: 提出了一种通用且高效的查找操作，用查找表替代权重与激活的乘法，并设计了可微分的查找表构建方式及多种训练策略，实现端到端优化。将查找操作嵌入神经网络，在图像分类、图像超分辨率与点云分类任务上验证。

Result: 查找网络在能耗和推理速度提升显著，同时在各类任务（分类、回归）和多种数据类型（图像、点云）上表现与常规卷积网络接近甚至达到SOTA。

Conclusion: 查找操作可以有效替代卷积网络中的乘法，兼顾性能和效率，极大提升了深度网络在资源受限设备部署的可行性。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 该论文提出了一种利用SAM生成的语义超像素，实现更高效视觉token压缩的新型视觉投影方法，大幅减少视觉token数量并提升RIS任务效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型与SAM结合虽然在Referring Image Segmentation上有出色表现，但因视觉token冗余导致计算资源消耗巨大，如何有效压缩视觉token数量同时保持分割性能成为亟需解决的问题。

Method: 受文本分词器启发，提出以SAM生成的语义超像素为单位，将同类区域合成为“视觉词”，作为新的视觉token。通过自适应地依据场景复杂度缩短token序列，引入语义超像素位置信息嵌入和聚合模块，提升几何感知与上下文信息保真性，兼顾全局与细节。

Result: 实验结果表明，该方法能在不损失性能的前提下，将视觉token数量减少93%，显著提升多模态大模型训练与推理速度，并优于现有视觉压缩投影方法。

Conclusion: 利用语义超像素压缩视觉token是一种高效RIS模型提升路径，兼顾了计算效率与分割性能，具有广泛的应用推广潜力。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为FishBEV的BEV分割方法，有效提升了鱼眼相机下的自动驾驶场景感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有BEV分割方法主要针对针孔相机设计，面对鱼眼相机时因几何畸变、多视角匹配困难和时序特性不稳定，导致性能显著下降，急需有针对性的解决方案。

Method: FishBEV引入三项核心创新：1）畸变鲁棒多尺度特征提取（DRME）主干网络，能在鱼眼畸变下提取尺度一致的特征；2）不确定性感知空间跨注意力（U-SCA）机制，通过估计不确定性提升多视角对齐可靠性；3）距离感知时序自注意力（D-TSA）模块，自适应平衡近距离细节与远距离上下文以增强时序一致性。

Result: 在Synwoodscapes数据集上的大量实验表明，FishBEV在鱼眼环视BEV分割任务中，性能显著优于现有主流方法。

Conclusion: FishBEV为鱼眼相机下的自动驾驶BEV分割提供了有效的新思路，有效缓解了畸变、多视角与时序难题，对实际落地有明确推动作用。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 本研究提出了基于样条函数的Kolmogorov-Arnold Networks (KANs)，用于在资源有限和小样本下实现高效、可解释的医学图像分类。


<details>
  <summary>Details</summary>
Motivation: 由于医学图像分类精度、泛化能力以及可解释性需求高，尤其是在数据稀缺和计算资源有限的临床环境，现有大型CNN模型难以适应，因此需要开发更加轻量、易于解释的分类方法。

Method: 提出三种以B样条为核心的KAN架构：（1）SBTAYLOR-KAN（将B样条和泰勒级数结合）、（2）SBRBF-KAN（结合径向基函数）、（3）SBWAVELET-KAN（结合Morlet小波），利用样条近似同时捕捉局部与全局非线性，无需预处理直接对原始医学图像建模。并通过多类医学成像数据集对模型效果、泛化能力和数据稀缺场景进行验证，并用Grad-CAM增强模型解释性。

Result: SBTAYLOR-KAN在脑MRI、胸部/肺结核X线、皮肤病变等数据集上取得高准确率，最高可达98.93%，F1分数均衡；在仅用30%训练数据时准确率仍超86%；在皮肤癌极度类别不平衡情况下依旧表现优于主流方法（68.22%）。模型参数量仅2872个，远低于ResNet50（2418万个），但实现相近性能。

Conclusion: 提出的基于样条的KAN架构在医学图像分类中实现了高效、轻量级且可解释的性能，尤其适合在数据有限和设备受限的实际医疗环境中应用，有助于提升临床辅助诊断的实用性与可靠性。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为StyleProtect的方法，通过更新扩散模型中部分跨注意力层，有效防止作品艺术风格被模仿，尤其针对被微调的扩散生成模型。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型（如Stable Diffusion等）发展迅速，能够低成本复制艺术家的独特风格，给艺术家权益带来威胁，因此有必要探索如何保护艺术作品不被轻易仿制。

Method: 作者假设扩散模型中某些跨注意力层对艺术风格高度敏感，并通过对其激活强度进行分析，结合外部模型提取的特征，识别关键敏感层。在此基础上，仅对这些跨注意力层进行有针对性的更新，实现对风格仿制的有效防护。该方法轻量高效。

Result: 在WikiArt中30位风格鲜明的艺术家作品与Anita数据集动画中实验，StyleProtect能有效防止艺术风格被扩散模型微调仿制，同时对正常图片表现影响极小。

Conclusion: StyleProtect为艺术品及动漫风格防护提供了一种高效、实用的新路径，能显著减轻生成模型带来的艺术风格侵权风险。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: 本文提出了UM-Depth，结合了运动感知与不确定性感知细化机制，在无监督单目深度估计中提升了动态物体边界与低纹理区域的深度准确性。方法在训练期间仅在教师网络中使用光流，无需额外标签与推理开销，且有效提升了模型在KITTI和Cityscapes数据集中的表现。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计技术无需标注深度标签，但遇到低纹理或动态区域时易产生较大不确定性，导致深度估计准确度下降。如何提升这些易受不确定性影响区域的深度估计效果，是本文关注的难点。

Method: 作者提出了UM-Depth框架，采用教师-学生训练策略，将不确定性估计嵌入训练流程与网络结构。在训练阶段，教师网络专用光流信息，帮助模型聚焦于训练中光度弱区域的不确定性监督，无需在推理时增加额外开销。整个训练流程无须额外标签。

Result: 在KITTI与Cityscapes数据集上进行大量实验，UM-Depth在自监督单目深度与位姿估计任务上表现出优越性，超越了现有方法，在动态及低纹理区域均提升了估计效果。

Conclusion: UM-Depth展现出结合运动与不确定性感知的方法，在无需增加推理负担与额外标签的情况下，可大幅提升单目深度估计在复杂场景中的表现，达到了当前自监督方法的领先水平。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文针对指代性视频对象分割（RVOS）中，现有基于查询的方法容易受相似物体干扰导致查询选择偏差的问题，提出了Triple Query Former（TQF）模型，通过三种专用查询子模块和运动感知聚合，有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的RVOS方法依靠静态文本查询实现跨模态对齐，但这种方法在面对具有相似外观或动作的干扰物体时容易失效，导致查询选择偏差，影响分割准确性。因此，作者希望通过更结构化、动态的查询方式来提升鲁棒性。

Method: 提出了Triple Query Former (TQF) 框架，将查询因子分解为三种：用于静态属性的外观查询、用于空间关系的帧内交互查询和用于时序关联的帧间运动查询。这些查询不仅基于文本，还结合视觉信息动态构建。此外引入了两种运动感知聚合模块：帧内交互聚合用于融合同帧内的空间位置信息，帧间运动聚合用于结合跨帧的运动轨迹，实现时序关联。

Result: 在多个RVOS数据集上进行了大量实验，结果显示所提出的TQF方法及其结构化查询和运动感知聚合模块都优于现有方法。

Conclusion: TQF通过多元结构化查询和聚合方式，有效缓解了传统方法中的查询选择偏差问题，提高了指代性视频对象分割的性能，验证了其有效性和优势。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出了InstanceVG框架，首次联合解决了GREC（边框级识别）和GRES（像素级分割）两类广义视觉定位任务，并通过实例感知能力提升多粒度预测一致性，实现了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉定位任务（如GREC、GRES）常被分开处理，未能利用联合训练带来的预测一致性优势，同时现有方法往往忽视了实例级感知能力，导致识别框与分割掩码之间缺乏一致性。作者希望通过统一框和掩码的联合推理提升整体表现，并突破单一粒度任务的局限。

Method: 提出InstanceVG多任务广义视觉定位框架，利用实例查询实现对实例级边框和掩码的联合预测和一致性优化。针对每个实例查询分配先验参考点，用于目标匹配，保证点、框、掩码三重预测的一致性。

Result: 在四项任务涉及的十个公开数据集上进行了大量实验，InstanceVG在多项评测指标上均显著优于现有方法，达到了最新的性能水平。

Conclusion: InstanceVG首次实现了GREC与GRES任务的一体化处理，引入实例感知提升了多粒度预测一致性，为广义视觉定位任务提供了更优的解决方案，具有实际推广价值。

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种文本-图像人物检索(FMFA)的新方法，通过显式与隐式对齐机制结合，改进跨模态对齐效果，并在公开数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有TIPR方法在跨模态局部特征对齐时缺乏核查有效性，且过于关注于困难负样本，忽视了对错配的正样本的处理，导致全局对齐效果不佳。

Method: 作者提出FMFA框架，包括自适应相似性分布匹配（A-SDM）模块，用于拉近未对齐的正样本，和显式细粒度对齐（EFA）模块，通过稀疏化相似矩阵和硬编码方法强化细粒度交互，提升对齐准确性。

Result: 所提出的方法在三个公开数据集上进行了评估，相较于所有基于全局匹配的方法，FMFA取得了最优的性能。

Conclusion: FMFA有效融合了全局与细粒度对齐机制，在无需额外监督的情况下显著提升了TIPR任务表现。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 本文提出一种新的文本驱动图像颜色编辑方法，通过引入颜色映射模块，实现了对生成图像的精细化、连续及可控的颜色编辑。实验证明该方法在颜色连续性和可控性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动图像颜色编辑方法存在精度不足、缺乏连续控制等问题，且文本向量线性插值方法难以实现精确颜色范围控制，导致编辑结果不可控。为解决这一难题，作者希望实现可以精确指定颜色区间、并保持语义一致的自动化图像颜色编辑。

Method: 提出了颜色映射模块，将文本嵌入空间与图像RGB颜色空间建立显式对应关系。该模块能够根据目标RGB值预测相应的嵌入向量，使得用户可指定颜色范围，生成具有连续颜色变化的图像。

Result: 通过实验证明，提出的方法能实现对生成图像颜色的精确、连续且可控的编辑，且保持了图像语义的一致性，在颜色连续性和可控性指标上均有良好表现。

Conclusion: 该方法有效提升了文本驱动图像编辑的颜色精度和可控性，为实现更细粒度的交互式图像编辑提供了新思路。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉反馈的迭代式文本到图像生成安全机制，能更有效保证生成内容的安全性且保持用户意图。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像（T2I）模型在生成图像时，对提示词表述敏感，不同表述可能导致输出不安全或用户意图偏差。现有安全方法主要依赖大语言模型修改提示词，忽略实际生成的图像，有可能对本就安全的提示造成过度修改或无法杜绝不安全输出。急需一种能基于图像结果本身做出判断、提高生成内容安全性的方案。

Method: 提出了一种利用视觉语言模型（VLM）的迭代提示词优化算法。具体做法是：先用T2I模型生成图像，再用VLM分析原提示词及生成图像，根据分析反馈对提示词进行迭代优化，直到满足安全性要求。此外，作者还构建了包含文本和图像双重安全标注的新数据集，并用开源多模态大模型自动标注，用于监督微调。

Result: 实验显示，这种方法能在不降低对用户意图的忠实度情况下，有效提升生成内容的安全性。与只用LLM优化提示的方法相比，视觉反馈的引入避免了对安全提示的过度修改，也减少了不安全生成结果。新数据集提升了模型调优效果。

Conclusion: 论文方法能生成更安全、用户意图保留度高的图像，补足了现有只关注文本端安全的方案。提出的VLM迭代优化和新数据集也为T2I领域安全性提升提供了实用新思路和工具。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出了一种高效的RAW到RGB的图像信号处理方法（TA-ISP），在降低计算和参数量的同时提升了下游视觉任务的表现，适合在资源受限设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有直接利用RAW数据提升视觉任务表现的方法，要么计算开销大（如大规模ISP网络），要么表现能力有限（如传统ISP微调）。因此需要一种高效且表达力强的RAW到RGB方案。

Method: 设计了一种面向任务的图像信号处理（TA-ISP）框架，不依赖密集卷积，而是通过预测轻量多尺度的调制算子，从全局、区域和像素级别调控图像特性，实现灵活空间变换同时大幅降低内存、计算和延迟。

Result: 在多个RAW域的检测和分割基准测试（包含白天和夜间条件）上，TA-ISP提升了下游任务准确率，并显著减少了参数量与推理时间。

Conclusion: TA-ISP能在保证准确性的同时大幅降低资源消耗，适合部署在资源受限设备上，为RAW视觉任务提供了高效的解决方案。

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 本文关注大规模视觉语言模型（LVLM）中的目标幻觉问题，并提出新的细粒度评测基准以及新方法VisionWeaver以有效缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 目标幻觉影响了LVLM在实际应用中的可靠性。作者发现不同视觉编码器因训练范式不同，具有不同的归纳偏置，从而导致了多样化的幻觉表现。但现有基准只粗略检测幻觉，未能细致区分类型。

Method: 作者构建了VHBench-10，这是一个包含约1万样本、涵盖10类细粒度幻觉的新基准。基于此，设计了VisionWeaver模型，一种上下文感知的路由网络，利用全局视觉特征生成路由信号，动态整合多种专家的特征。

Result: 实验表明，不同视觉编码器确实在不同类型幻觉上行为各异。VisionWeaver模型能显著降低幻觉发生率，并提升模型整体表现。

Conclusion: 针对LVLM幻觉问题，细粒度测评有助于深入理解和分析不同编码器特性，而VisionWeaver通过有效集成专家能力，能显著抑制幻觉，提升模型适用性。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [42] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 本文提出了一种用于低光夜间环境下雨滴去除的新型深度学习网络NDLPNet，并构建了真实夜景雨滴数据集NSR，实现该任务的先进性能。


<details>
  <summary>Details</summary>
Motivation: 夜间下雨环境中的视觉退化会显著影响安防监控和自动驾驶等任务。然而，现有的去雨神经网络主要针对日间场景设计，无法应对夜间光照条件下雨滴的空间异质性和光相关变化。解决夜间去雨难题具有重要实际应用价值。

Method: 作者提出了夜间去雨感知增强网络NDLPNet，其中创新性地引入了位置感知模块（PPM），可捕捉空间位置及雨滴密度分布，有效区分并加强关键特征通道，提升对夜间雨滴的检测和去除效果。同时，构建了包含900对真实夜景雨图的NSR数据集，作为新的评价基准。

Result: 在现有夜间和新NSR数据集上的大量定性和定量实验表明，NDLPNet在去除夜间雨滴的能力上均显著超过当前主流方法（SOTA）。

Conclusion: NDLPNet能够有效消除夜间雨滴干扰，同时保存重要背景信息，并通过新的数据集NSR为夜间去雨研究提供了基础支持与评价标准，推动了该领域的进步。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [43] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了Dense Video Understanding（DVU）及首个专为高帧率视频理解设计的DIVE基准，发明了高效的Gated Residual Tokenization（GRT）方法，有效支持密集时序推理，在减少冗余计算的同时提升视频大模型的理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前的视频大语言模型（VLLMs）通常采用低帧率采样，丢失了大量时序细节，难以处理如讲座理解等每帧信息密集的任务，而这样做主要是为避免每帧建模成本过高；现有基准同样只关注粗粒度变化，忽略了密集的时序推理需求。

Method: 提出了一种两阶段的Gated Residual Tokenization（GRT）框架：（1）基于像素级运动估计的Motion-Compensated Inter-Gated Tokenization，可在分词时跳过静态区域，实现低于线性增长的计算和token数量；（2）语义场景内token合并（Semantic-Scene Intra-Tokenization Merging）在场景内部进一步跨静态区域融合token，持续削减冗余同时保留动态信息。并引入DIVE基准用于密集视频理解评估。

Result: 实验显示GRT在高帧率视频理解任务上显著超越更大的VLLM基线模型，且帧率越高，GRT优势更显著，验证了高时序密度信息的重要价值。

Conclusion: GRT方法能高效、可扩展地实现高帧率视频理解，为未来密集时序推理和相关任务奠定了基础，证明密集时序信息对视频理解大模型的提升至关重要。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [44] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: 本文提出了一种多模态框架VocSegMRI，将视频、音频和音韵学信号融合于实时磁共振成像（rtMRI）中的发音结构分割，并利用交叉注意力及对比学习实现目前最优的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 由于传统分割方法几乎只依赖视觉信息，导致对发音结构的实时精准分割仍有挑战，而音频及音韵学信号可以为视觉信息提供补充，实现更精确的分割。

Method: 提出VocSegMRI框架，采用交叉注意力融合技术，将视频、音频、音韵学三种模态的信息对齐，并引入对比学习目标，在推理阶段即使音频缺失也能提升表现。通过消融实验分析各模块贡献。

Result: 在USC-75 rtMRI数据子集上，Dice分数达到0.95，HD_95为4.20 mm，超过了现有的单模态与多模态基线方法。消融实验证实交叉注意力和对比学习均显著提升了分割的精度和鲁棒性。

Conclusion: 多模态融合和先进的特征对齐技术能极大提升实时MRI下发音结构分割的准确率，对声道分析具有重要价值。

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [45] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型先验的生成式编码框架，用于提升低码率下的图像压缩性能，并显著优于现有的压缩方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式技术的发展，视觉内容中自然图像与AI生成图像混杂，对高感知质量的高效编码技术提出了更高要求。传统与学习型编解码方法无法在高压缩比下保证主观质量，而现有生成式方法面临视觉保真与泛化性方面的挑战。

Method: 作者提出利用预优化编码器生成压缩域表征，通过轻量级适配器和注意力融合模块与预训练扩散模型的内部特征集成，并引入分布重归一（distribution renormalization）方法以提升重建质量。该框架能够高效利用现有预训练扩散模型，并通过低成本再训练适配不同模型及新需求。

Result: 广泛实验表明，本文方法在低码率下的视觉保真度超越现有方法，对比H.266/VVC压缩性能提升最高可达79%，并能高效适配AI生成内容与更广泛的内容类型。

Conclusion: 提出的方法不仅在低码率带来更优视觉质量，也具备良好的泛化与扩展性，是应对AI与自然图像复杂混合情形下高质量压缩的有效解决方案。

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [46] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: 提出AdaThinkDrive框架，通过自适应采用CoT推理，实现自动驾驶模型推理精度和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管推理技术如链式思维（CoT）已应用于视觉-语言-行动（VLA）模型并显示潜力，但在简单场景易引入额外计算开销却未提升决策质量，因此需要一种能根据场景自适应使用推理的模型。

Method: 提出AdaThinkDrive，在大规模自动驾驶问答和轨迹数据集上预训练，利用有/无CoT的监督微调，训练模型区分哪些场景需要推理；并提出Adaptive Think Reward奖励机制结合GRPO，利用不同推理模式间的轨迹质量奖励模型自适应选择推理方式。

Result: 在Navsim基准上，AdaThinkDrive的PDMS得分达到90.3，较最佳视觉基线提升1.7分。消融实验显示，相较“始终不推理”和“始终推理”基线，各提升2.0和1.4分，同时推理时间较后者减少14%。

Conclusion: AdaThinkDrive通过自适应推理，在保障推理精度的同时有效提升推理效率，实现了自动驾驶场景下的高效决策。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [47] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过独立地从局部和全局视角预测篡改区域，并利用形态学操作对结果进行融合，提高了深度伪造检测中篡改区域定位的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然深度伪造检测的准确率不断提升，但更精确地定位图像中被篡改的区域需求日益增长。现有方法往往忽略了局部细节与全局语义信息的互补性，且局部与全局预测结果的简单融合会带来噪声和错误，导致定位效果不佳。

Method: 论文提出独立地利用局部和全局视角对篡改区域进行预测，然后采用形态学操作融合两者输出，从而抑制噪声并增强空间一致性。

Result: 大量实验显示，本文提出的每个模块都有效提升了伪造区域定位的精度和鲁棒性。

Conclusion: 独立局部与全局预测结合形态学融合的方法，能够显著改善深度伪造检测中篡改区域的定位表现。

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [48] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 本论文提出Variable-Rate Spatial Event Mamba架构，能够无需中间表示、直接高效处理事件相机数据，实现窗口延迟与推理延迟的灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 事件相机能捕捉微秒级像素亮度变化，适用于高速视觉任务，但现有方法需将事件流转换为帧等中间表示，导致不可避免的窗口延迟。而点级检测方法又因计算量大难以实时高效。因此，急需一种高效且低延迟的事件流处理方法。

Method: 提出Variable-Rate Spatial Event Mamba方法：直接处理原始事件流、不依赖于帧等表示。设计轻量级因果空间邻域编码器捕获局部几何关系；利用基于Mamba的状态空间模型进行可扩展的时间建模，复杂度线性。推理阶段引入控制器，根据事件速率自适应调整处理速度，实现窗口延迟与推理延迟最优平衡。

Result: 该方法能高效且实时地直接处理原始事件流，无需预设时间窗口，显著减少延迟，同时保持模型的高精度和可扩展性。

Conclusion: Variable-Rate Spatial Event Mamba实现了事件流处理的高效性和低延迟，突破了现有处理方法在时间窗口和计算效率间的权衡限制。该架构为高速视觉任务提供了更优的技术路径。

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [49] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: 本文提出了一种无须重新训练的方法，通过区块级别特征缓存加速Diffusion Transformer (DiT)视频生成模型，最多可实现2.24倍推理加速且画质基本不损失。


<details>
  <summary>Details</summary>
Motivation: DiT作为当前视频生成的先进方法，但其顺序去噪导致推理慢，无法满足实际需求。现有加速方式要么损失画质，要么特征复用粒度不合适。为提升推理效率且维持画面效果，作者分析了DiT模型冗余，并提出新方案。

Method: 通过系统分析发现DiT块是主要的延时瓶颈，且多个时刻的块特征高度相似。基于此，作者提出Block-Wise Caching (BWCache)方法：动态缓存并重用DiT块在时序间的特征；并设计了相似度指示器，只有特征高度相似时才复用，以规避冗余计算。全流程无需模型结构改动和再训练。

Result: 实验覆盖多个主流视频扩散模型，结果显示BWCache可实现最高2.24倍推理加速，同时画面质量几乎不下降，达到了实际应用标准。

Conclusion: BWCache显著提升了DiT模型的视频生成速度，并最大程度保留了原有的画面质量，为扩散式视频生成在实际场景中的落地应用提供了有意义的加速新思路。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [50] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的有监督域自适应框架（SDA）用于航天器位姿估计（SPE）关键点回归，显著提升了从合成数据到真实数据的性能表现，方法轻量且高效，实验验证优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前SPE技术虽然在合成数据上成绩优异，但在真实或实验室数据上性能大幅下降，主要由于合成与真实数据之间的域差异。现有的无监督域自适应方法在有限标注真实样本时效果不佳，因此亟需一种利用少量标注真实数据提升泛化能力的新方法。

Method: 提出了首个针对SPE关键点回归的有监督域自适应框架，基于学习不变特征与风险（LIRR）范式，通过联合优化域不变特征表达和任务风险，利用合成标注数据和少量真实标注数据来提升域间泛化表现。

Result: 在SPEED+基准测试上，本方法在仅有5%标注目标域数据的情况下，其性能达到或超过了用更大量标注训练的oracle基线，并全面优于source-only和微调等方法。

Conclusion: 该方法轻量、与主干网络无关、计算高效，可大幅提高SPE在真实航天环境中的实用性和鲁棒性。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [51] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新型的基于语义权重的自适应粒子滤波方法（SWA-PF），并发布了多高度飞行数据集（MAFS），有效提升了无人机在无GNSS环境下的视觉定位能力，尤其适用于动态或复杂环境。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的无人机视觉定位方法在GNSS不可用时存在数据集不足、实时性差、环境适应性和泛化能力有限等问题，尤其在动态或时变环境下表现不佳。

Method: 作者构建了一个大规模多高度飞行片段数据集（MAFS），针对不同飞行高度情境，通过提出语义权重机制和优化的粒子滤波架构，将无人机图像和卫星影像中的语义特征有效融合，实现了高效鲁棒的视觉定位（SWA-PF方法）。

Result: 所提方法在新数据集上评估，计算效率比传统特征提取方法提升10倍，全球定位误差保持在10米以内，使用低分辨率卫星图便能在数秒内实现四自由度姿态估计。

Conclusion: SWA-PF方法和MAFS数据集有效提升了无人机视觉定位系统在无GNSS、复杂环境下的泛化与实时性能，为相关领域提供了新工具和实用数据资源。

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [52] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的辅助任务Masked Feature Modeling（MFM），通过在特征空间进行掩码和重建，提升了无监督领域自适应（UDA）语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习等自监督任务虽然提升了UDA中的特征可区分性，但基于掩码建模的方法由于架构不兼容和优化目标不匹配而鲜有应用。作者希望解决这一问题，探索掩码建模在UDA中的潜力。

Method: 提出MFM辅助任务：直接在特征空间进行掩码及重建，并引入轻量级辅助模块Rebuilder用于重建特征。MFM结合分割解码器对重建后的特征进行分类，使辅助目标与像素级分割主任务紧密结合。训练时使用Rebuilder，推理时不增加计算开销。

Result: MFM在多种主流架构（如DeepLab、DAFormer）和UDA基准数据集上进行了大量实验，结果表明MFM能显著且一致地提升领域自适应语义分割性能。

Conclusion: MFM是一种简单、高效、通用的UDA语义分割提升策略，能够无缝集成进现有架构，提升分割表现，并且无需增加推理时开销。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [53] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: 该论文探讨了在高光谱图像像素光谱分类任务中，MiniROCKET 和 HDC-MiniROCKET 相较于现有主流模型 1D-Justo-LiuNet 的表现。尤其是在训练数据有限的情况下，MiniROCKET 更具优势。


<details>
  <summary>Details</summary>
Motivation: 虽然结合空间和光谱信息的分类方法通常效果最佳，但仅使用光谱信息的分类具有模型更小、训练数据需求更低的优势。然而，在数据有限条件下，当前高效光谱分类模型（如1D-Justo-LiuNet）性能会大幅下降。因此，作者希望探索在数据有限时更鲁棒的模型。

Method: 论文引入了MiniROCKET及其变体HDC-MiniROCKET，这些方法在特征提取部分不需要可训练参数，通过低参数量的特征设计，提升了模型对有限数据的适应性。

Result: 实验结果显示：尽管MiniROCKET参数量较多，但在有限训练数据时效果优于1D-Justo-LiuNet，在一般情况下二者表现相当。

Conclusion: MiniROCKET方法在数据匮乏情景下展现更高的鲁棒性，可以作为低样本光谱分类的首选模型，还为未来空间-光谱结合方法的改进提供了灵感。

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [54] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了Semi-MOE方法，是首个用于半监督病理图像分割的多任务专家混合（MoE）框架，通过集成不同专家网络和动态伪标签策略，有效应对噪声伪标签和形态模糊造成的问题，在小标签场景下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督病理图像分割方法常因腺体边界模糊和形态误分导致伪标签噪声大，影响分割性能，需要一种既能应对多样形态特征又能提升伪标签可靠性的方法。

Method: 构建三种专家网络（分割、距离场回归、边界预测），通过多门控伪标签模块整合各专家特征，形成更强鲁棒性的融合与细化伪标签，并设计自适应多目标损失函数以自动动态平衡不同任务目标。

Result: 在GlaS和CRAG基准数据集上进行大量实验证明，该方法在标注数据极少的情况下，分割性能显著优于最新的半监督分割方法。

Conclusion: Semi-MOE提供了一种创新有效的MoE架构，在半监督病理图像分割任务中展现出很大潜力，并推动了相关技术进步。

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [55] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文质疑了表征学习中“无相关视角足够学习有意义表征”的假设，并提出一种新的自监督学习方法Consistent View Alignment，显式对齐不同视角下的补充信息，实验结果优越。


<details>
  <summary>Details</summary>
Motivation: 当前很多表征学习方法默认仅靠无相关视角就能提取有用的潜在结构，但作者发现结构信息并不会自然而然地形成，需要通过显式机制加以引导。

Method: 提出Consistent View Alignment方法，将同一数据点在不同视角下的表征进行显式对齐，融合多视角的补充信息，并避免错误的正对齐。

Result: 该方法在自监督学习基准任务中表现优越，在MICCAI 2025 SSL3D challenge中，分别利用Primus视觉Transformer和ResEnc卷积网络取得第一和第二名。

Conclusion: 必须通过有结构的视角对齐来促使表征学习获得有意义结构，所提方法有效提升了多项下游任务表现。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [56] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 本文提出了一种面向扩散模型加速的全新特征缓存范式SpecDiff，融合了自推断（未来信息）和历史信息，大幅提升了模型推理速度且几乎无质量损失。实验证明该方法在多个主流扩散模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅利用历史信息，导致推理加速受限，存在精度与速度不可兼得的瓶颈。作者希望通过引入未来信息改善这种局限，突破当前瓶颈。

Method: （1）提出基于自推断信息的特征选择算法，通过结合历史与未来信息动态评估特征重要性，并以此筛选缓存特征；（2）设计多级特征分类算法，根据特征的重要性分层决定计算策略，从而有效提升推理效率。整个策略无需重新训练模型。

Result: SpecDiff在Stable Diffusion 3、3.5和FLUX上分别达到了2.80倍、2.74倍、3.17倍的推理加速，且输出质量损失极小，优于现有RFlow方法。

Conclusion: SpecDiff通过历史+自推断信息混合缓存，突破了扩散模型推理的加速-准确率Pareto前沿，实现了训练无关、低损耗、高效的扩散模型推理加速新范式。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [57] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: 本文提出了EDITS框架，通过融合图像与文本语义信息，实现了更高质量的数据集蒸馏，提升了合成小数据集的表达能力和训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法只关注低层视觉特征，忽略了图像中蕴含的高层语义和结构信息，导致模型性能受限。为了解决这一问题，本文研究如何融合图像隐含的语义信息以提升蒸馏效果。

Method: EDITS框架首先通过视觉语言模型生成外部文本，并与图像特征在全局语义查询模块中融合，构建集群缓冲区；随后通过局部语义感知机制，选取有代表性的样本并构建图像和文本原型，其中文本原型由大语言模型根据精心设计的提示生成；最后，提出的双原型引导策略利用扩散模型合成最终小数据集。

Result: 大量实验表明，EDITS方法能在提升模型训练效率的同时，有效保持甚至超过传统方法的性能。

Conclusion: 融合图像与文本高阶语义特征对提高数据集蒸馏效果至关重要，EDITS为今后的数据合成和高效学习提供了新思路。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [58] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: 提出LamiGauss算法结合高斯光柵化和特定的几何变换模型，实现了在极少视角下对X射线层析成像的高质量重建，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CT在处理板状结构时由于几何约束存在局限，而层析成像（CL）在稀疏采样条件下重建高质量体数据仍有较大难度，因此需要更高效且效果更好的重建方法。

Method: 提出一种结合高斯splatting光柵化和专门的检测器-世界变换（引入层析倾角）的重建算法LamiGauss，并引入初始化策略过滤常见伪影，使模型聚焦于真实结构。算法支持直接从稀疏投影优化。

Result: 在合成和实际数据集上的大量实验表明，LamiGauss在仅用3%视角的数据下重建效果就超越了在全数据集上优化的主流迭代方法，显示出极强的性能和数据利用效率。

Conclusion: LamiGauss能在极少数据和稀疏采样条件下实现高质量层析重建，优于现有技术，对板状结构非破坏成像具有重要意义。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [59] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: 本文提出了针对于现有记忆型视频分割模型在目标跟踪中易受干扰问题的改进方法，并构建了新的数据集以分析与衡量干扰物对跟踪性能的影响。所提方法在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有如SAM2等记忆型视频分割模型虽然分割表现出色，但在目标跟踪场景下，容易被与目标视觉上相似的干扰物所影响，导致跟踪漂移等问题。解决这一挑战对提升跟踪鲁棒性和实际应用价值十分重要。

Method: 作者提出了一种感知干扰物的记忆模块和基于内省管理的方法，将其集成到了SAM2模型中，形成了新模型DAM4SAM。同时，构建了一个专门分析干扰物影响的数据集DiDi。此外，作者将提出的干扰物记忆模块集成到不同架构（高效实时跟踪器EfficientTAM和边缘跟踪器EdgeTAM）中以验证其通用性。

Result: DAM4SAM在十三个基准集上超越了SAM2.1模型，在其中十个基准上获得了最新的最佳结果。将其嵌入EfficientTAM后，实现了11%的性能提升，并在多项任务中达到了与非实时SAM2.1-L相当的表现。集成到EdgeTAM后也带来了4%的提升，证明了方法在多种架构中的泛化能力。

Conclusion: 所提干扰感知型记忆模块有效提升了目标跟踪过程中的抗干扰性和重检测能力，在多个主流基准上均创下新成绩，且可广泛应用于不同类型的跟踪器，具有较好的实用前景。

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [60] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: 本文提出了一种新型双流注意力网络PelFANet，通过融合原始X光片与分割骨骼图像，有效提升骨盆骨折（尤其是隐匿性骨折）的自动检测和分类准确率，在公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 骨盆骨折在X光片中常表现为不明显甚至不可见，现有自动诊断方法对隐匿性骨折检测存在较大挑战。因此，亟需开发融合更多解剖信息、提升隐匿骨折识别能力的新方法。

Method: 提出PelFANet网络，采用双流结构，分别处理原始X光影像和分割后的骨骼图像，通过FABlocks实现跨模态特征反复融合与优化。训练过程中，分割引导分类，两阶段优化网络表现。

Result: 在AMERI数据集上，针对可见骨折实现88.68%分类准确率和0.9334 AUC；对模型未训练过的隐匿骨折样本，也达到了82.29%准确率和0.8688 AUC，优于常规方法。

Conclusion: 实验证明，融入解剖信息的双输入结构能够极大提升骨折检测鲁棒性，特别适用于影像表现微弱的复杂场景，对临床工具开发具有重要意义。

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [61] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: 作者提出了EvHand-FPV，这是一个面向可穿戴设备、基于单个事件相机的第一视角3D手部追踪轻量框架，能高效准确地进行手部追踪。


<details>
  <summary>Details</summary>
Motivation: 扩展现实（XR）设备等资源受限平台对手部追踪提出了高准确度、低延迟和低功耗的要求，但传统帧基方法难以满足。事件相机具备高时间分辨率和低功耗的优势，为这一场景提供了新的可能。

Method: 1）构建独特的数据集，结合带3D标签的合成训练数据与带2D标签的真实评测数据；2）引入基于手腕几何线索的ROI定位；3）网络结构中集成ROI偏移量，减少计算量无需显式重建；4）采用多任务学习与辅助几何特征头，增强表达能力且测试时无额外开销。

Result: 在真实第一视角测试集上，EvHand-FPV将2D-AUCp从0.77提升到0.85，参数量从11.2M降至1.2M，FLOPs从1.648G降至0.185G，减少了89%；在合成数据上的3D-AUCp也达到0.84。

Conclusion: EvHand-FPV实现了高效准确的事件驱动第一视角手部追踪，适合在设备端XR应用部署。作者还开放了数据集与代码。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [62] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的先进原型生成方法，提升了小样本3D点云分割的精度。


<details>
  <summary>Details</summary>
Motivation: 现有小样本3D点云分割方法常用传统原型生成算法，如最远点采样，但该过程初始随机性大且研究不足，影响最终性能。因此，作者希望解决原型生成中的随机性和分布错位问题。

Method: 作者提出了基于注意力机制的原型生成方法，并设计了WARM（White Aggregation and Restoration Module）模块。具体做法是在注意力过程前后加入whitening和coloring变换，解决特征分布错位，提升原型的表达能力。

Result: 在多项FS-PCS基准测试上，提出方法以显著优势取得了最新最优性能。

Conclusion: 该方法能够更好捕捉支持集中语义关系，提升分割精度，为小样本3D点云分割提供了有效且鲁棒的原型生成方案。

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [63] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Self-Rationale Calibration（SRC）的新方法，通过校准视觉-语言大模型(LVLM)的推理过程，提升其答案与推理一致性和正确率，显著改善了模型泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型在视觉问答上表现出色，但在推理合理性和答案的对齐上仍有明显不足，经常出现推理过程和预测结果不一致或错误答案，限制了模型的实用性和可靠性。

Method: SRC方法包括三个关键步骤：1）通过“推理微调”，调整模型输出格式，使其优先输出推理过程，再给出答案，无需显式提示；2）针对每个样本，从微调后的模型中挖掘多样化的候选响应；3）使用定制的R-Scorer评分模型，采用成对比较策略，对候选结果的推理质量和事实一致性进行评估，然后基于带置信度的喜好选择进行“偏好微调”，从而解耦推理与答案的一致性校准流程。

Result: 实验显示，SRC方法在多个基准任务上的感知、推理和泛化能力均有大幅提升，验证了该校准机制的有效性。

Conclusion: 本文方法突出了以推理为核心的对齐策略，充分挖掘了LVLMs推理潜力，对推进视觉-语言模型的可靠推理和真实应用具有重要意义。

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [64] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: 本文提出了一种名为AntiPure的新型保护性扰动方法，可以提升对扩散模型合成图像的安全保护能力，即使在经过去噪净化（purification）后仍能有效防止恶意篡改。实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型如Stable Diffusion因其强大的定制能力受到关注，但同时带来了安全风险，包括深度伪造和版权问题。现有的保护性扰动虽然能防止图像被滥用，但常被净化（purification）操作消除，导致保护失效，因此有必要设计能抗净化的保护机制。

Method: 作者提出并形式化了反净化（anti-purification）任务，并提出AntiPure扰动作为诊断性保护方案。其核心在于引入两种引导机制：1）局部高频引导，通过扰动图像高频信息减弱净化效果；2）错误时间步引导，通过扰动扩散模型的不同时刻的去噪步骤，增强扰动的持久性。这些操作能够让保护性扰动在主流净化流程下仍具有效果。

Result: 实验表明，在净化-定制化流程下，AntiPure扰动能在保持图像感知一致性的基础上，实现最强烈的失真，保护能力优于现有方法。该方法对主流净化手段具有较强的鲁棒性。

Conclusion: AntiPure作为对现有图像保护机制的补充，加强了对扩散模型生成图像的安全防护，尤其是能够对抗净化操作，提升了防篡改能力。该方法可为后续相关安全应用提供参考。

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [65] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 本文提出了一种新的噪声水平优化方法——噪声水平引导（NLG），能够在无需额外数据或复杂网络的情况下提升扩散模型的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的初始高斯噪声会影响最终生成图像的质量和与输入提示（prompt）的契合度。而已有针对噪声控制的方法往往需要额外数据、辅助网络或复杂的反向传播优化，实际应用受限。因此，研究者希望提出一种简便、高效且无需额外资源的噪声优化方法。

Method: 作者提出了噪声水平引导（NLG），通过提升噪声与通用引导（general guidance）的一致性概率，改进了初始噪声分布。这种方法不需要额外的数据、网络或反向传播，在条件（conditional）和非条件（unconditional）扩散模型下均可通用，并能与不同的扩散过程引导协同使用。

Result: 在五个标准数据集上进行了大量实验，结果表明NLG方法显著提高了生成图像的质量，并增强了对输入条件（如提示）的遵从性。同时，NLG方法也能够与现有的引导算法无缝集成，保持较高的计算效率。

Conclusion: NLG作为一种通用、有效且可扩展的噪声优化方案，为扩散模型的质量提升提供了实际可用的新工具，有望在各种实际扩散模型生成任务中发挥作用。

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [66] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: 本文提出了一个专门用于评估视觉任务中细粒度目标计数能力的新基准数据集PairTally，旨在测试当前模型在复杂场景中针对用户意图进行精确计数的能力。实验结果显示，现有主流模型在细节复杂或类别接近的情况下计数效果不理想。该数据集有助于未来相关技术的诊断与提升。


<details>
  <summary>Details</summary>
Motivation: 尽管已有的一些视觉模型能够进行目标计数，但它们在辨别和计数特定类别或细粒度子类别目标上的表现和局限性尚不清楚。现有方法难以满足用户在复杂场景下对特定目标作出准确计数的需求，因此需要一个有针对性的评测数据集来推动该方向能力的发展。

Method: 作者提出了PairTally数据集，包含681张高分辨率图片，每张图片含有两种目标类别，涵盖了不同类别和同类细分两种场景。利用该数据集，作者对当前主流的计数方法，包括基于示例的模型、文本提示模型以及大规模视觉-语言模型（VLMs）进行了系统性测评。

Result: 实验发现，虽然各类主流模型在通用计数任务上有进展，但在细粒度、用户意图驱动的计数任务中仍然表现较差，尤其是在目标类别区分模糊或外观相似的场景中准确率较低。

Conclusion: PairTally数据集为细粒度视觉计数问题提供了新的评测平台。作者的实验表明，现有方法还无法满足复杂现实场景下的精确计数需求，PairTally将有助于未来研究者发现模型瓶颈并推动更加鲁棒、智能的计数系统的发展。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [67] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA是一种知识蒸馏方法，将多模态（视觉-语言）模型的区域级语义知识迁移到轻量级的仅视觉目标检测模型中。该方法通过特征空间对齐，有效提升仅视觉检测器的性能，无需文本输入，验证了其在小样本检测任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（如LLaVa）在视觉任务尤其是目标检测中表现优异，但这些模型参数庞大、推理效率低，不适合真实部署。现有知识蒸馏方法多以全图或密集对齐为主，忽视了对象级别的语义迁移，并且通常需要文本输入，限制了实际应用。因此，本工作希望将多模态教师的对象级知识有效注入高效的单视觉学生模型（如YOLO），实现高效、高质目标检测。

Method: 提出MOCHA方法，包括一个特征转换模块，将学生模型（比如YOLO）的特征映射到教师和学生共享的特征空间。训练时采用双重目标损失——既保证局部目标对象级别的特征对齐，又兼顾全局语义关系一致。整个蒸馏过程无需改变教师模型结构，也不需要学生在推理阶段使用文本输入。

Result: 在四个个性化小样本检测基准上进行了实验评测。MOCHA方法在所有基准上相较于已有基线方法取得了+10.1的平均分数提升，其紧凑的结构下能达到与大型多模态模型相当的性能。

Conclusion: MOCHA实现了多模态知识对轻量级视觉模型的高效迁移。其无需改变教师模型结构，无需推理时文本输入的特性，使其适合实际部署，在提高小样本检测准确性的同时兼顾模型高效性和实用性。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [68] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的YOLO-FEDER FusionNet方法，通过融合伪装检测与常规目标检测技术，在视觉复杂环境下实现更高效的无人机检测。实验验证了其性能大幅优于原有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景下（如背景杂乱、无人机体积小、伪装性强），现有YOLO等主流目标检测方法性能显著下降，因此需要特殊设计的新方法提升检测效果。

Method: 提出基于YOLO-FEDER FusionNet的改进方法：采用大量高仿真合成数据与少量真实数据混合训练，引入多尺度FEDER特征的融合与评估，对不同YOLO骨干网络配置进行检测性能对比。

Result: 在最佳配置（YOLOv8l骨干+基于DWD模块的FEDER特征）下，假阳性率（FNR）最多降低39.1个百分点，mAP@0.5提升62.8个百分点，相较初始基线有显著改进。

Conclusion: 通过对特征融合策略和骨干网络的系统升级，并合理利用合成与真实数据混合训练，可显著提升复杂背景下无人机检测的准确率与鲁棒性。

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [69] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: 本文提出了SAIL-VL2，一个面向多模态理解与推理的开放视觉-语言大模型，在2B和8B参数规模下于多项图像和视频基准取得了最先进性能。其三大创新包括大规模高质量数据筛选流水线、渐进式训练体系及稀疏混合专家架构。SAIL-VL2在106个数据集上表现优异，尤其在复杂推理任务如MMMU和MathVista上达到了最优。




<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的蓬勃发展，亟需构建兼具大规模、高效率和开放性的平台。在视觉-语言领域，现有模型在数据集建构、模型架构和多任务推理能力方面仍有诸多不足，难以覆盖丰富多样的实际需求。为此，作者提出了SAIL-VL2，希望打造面向开放社区、覆盖图像和视频任务、多层次理解与推理的新一代基础模型。

Method: 1. 搭建包含评分和筛选机制的大规模数据采集流水线，从标注多样性和数据质量两方面提升模型训练效率。
2. 采用渐进式训练框架，先以预训练强大的视觉编码器（SAIL-ViT），之后进行多模态预训练，最终利用思维-融合SFT-RL混合范式系统提升模型能力。
3. 模型架构上，突破传统稠密大模型，采用高效稀疏的混合专家（MoE）设计，提升参数利用率。

Result: SAIL-VL2在2B和8B参数规模下于106个视觉-语言数据集上表现优异。其在MMMU和MathVista等高难度多模态推理基准测试中取得了最优表现。此外，SAIL-VL2-2B在OpenCompass排行榜4B及以下参数规模的开源模型中排名第一。

Conclusion: SAIL-VL2作为面向开放社区的高性能视觉-语言基础模型，在数据工程、训练范式和模型架构三方面全面创新，实现了从感知到复杂推理的卓越能力，也为开源多模态领域提供了高效、可扩展的新范式。

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [70] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: 本文提出了一种融合多模态数据（临床、影像、病理）的嵌入式方法（PROFUSEme）用于预测前列腺癌患者在根治性前列腺切除术后的生化复发（BCR）。该方法在公开数据集和内部验证中表现优越，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 约30%的前列腺癌患者在手术后出现生化复发，导致死亡风险增加。若能在手术时准确预测BCR，有助于医生更早干预并改善患者预后。因此，需要多维度、高效的预测模型。

Method: 作者提出了PROFUSEme方法，利用中间融合策略，将临床、影像及病理数据进行交互式嵌入，结合Cox比例风险回归模型进行BCR预测。与后期融合策略进行定量比较，采用5折嵌套交叉验证及公开挑战榜单进行评估。

Result: 该方法在内部交叉验证中达到C-index均值0.861（标准差0.112），在CHIMERA 2025挑战验证集持出数据上C-index为0.7103，优于后期融合方法。

Conclusion: 多模态中间融合嵌入方法能有效提升前列腺癌切除术后生化复发预测的准确性，有望辅助临床决策并改善患者管理。

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [71] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate是一套角色动画与替换统一框架，能够将静态角色图片动画化，并实现角色在视频中的无缝替换，同时保留环境光照和色调，实现高保真和自然的视觉效果。


<details>
  <summary>Details</summary>
Motivation: 当前角色动画和角色替换技术在保持动作、表情精准和环境融合度等方面存在挑战。亟需方法统一实现高质量动画生成与角色替换，并提升环境适应能力。

Method: 该方法基于Wan模型，通过区分参考条件和生成区域的输入范式统一多个任务。利用空间对齐的骨骼信号复现身体动作，结合源图像的隐式表情特征复现表情。提出辅助的Relighting LoRA模块，增强角色替换时的环境光照融合。

Result: 实验结果表明，Wan-Animate在角色动画和替换效果上均达到业内领先性能，动画的可控性、表现力和环境融合度显著提升。

Conclusion: Wan-Animate作为统一框架，高质量实现了角色动画和替换，具有开源承诺，将推动角色视频生成及相关应用的发展。

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [72] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: 本文提出了一种融合视觉-语言模型语义信息的新型多目标跟踪方法（VSE-MOT），有效提升了在低质量视频中的跟踪表现。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪（MOT）方法在实际低质量视频中表现不佳，主要原因是对图像退化问题关注不足。因此，提升MOT算法在低质场景的应用能力具有实际意义。

Method: 设计了三分支架构，利用视觉-语言模型从图像中提取全局视觉语义信息，并与查询向量融合。提出了多目标跟踪适配器（MOT-Adapter）和视觉语义融合模块（VSFM），分别用于适配已提取的视觉语义信息和提升特征融合效果。

Result: 该方法在真实低质量视频数据上显著提升了跟踪性能，指标比现有算法高8%~20%，且在常规场景下依旧表现稳健。

Conclusion: 通过视觉语义增强，多目标跟踪方法在低质量视频中取得了显著优势，同时保持全面适用性，证明了所提方法的优越性。

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [73] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文首次将DINOv3基础视觉模型应用于零样本异常检测（ZSAD）任务，并提出多模态方法AD-DINOv3，通过结构优化和特定校准模块显著提升检测性能，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统的ZSAD大多基于CLIP模型，但新兴的视觉基础模型如DINOv3同样具有强大表达能力。作者希望利用DINOv3的优越特征迁移能力，解决当前ZSAD任务中的标注效率和可扩展性难题，并处理其在异常检测中的域偏差与全局语义偏向问题。

Method: 作者提出AD-DINOv3框架，首先将异常检测转化为多模态对比学习任务：用DINOv3提取patch和CLS token，用CLIP文本编码器表示正常和异常的文本提示。在两种模态中引入轻量级adapter以缩小域差，并设计了Anomaly-Aware Calibration Module（AACM）显式引导CLS token关注异常区域而非平常前景语义，从而增强区分性。

Result: 在8个工业和医疗基准测试上，AD-DINOv3的表现与现有最优方法持平或有超越，展现出方法的通用性和有效性。

Conclusion: AD-DINOv3通过创新的多模态结构和显式异常关注引导，突破了当前基础视觉模型在零样本异常检测上的主要局限性，验证了其作为ZSAD通用方案的优越性。

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [74] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的弱监督音视频解析方法，通过EMA伪监督和类感知跨模态一致性优化，实现了音频、视频及音视频事件的精确检测，显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督AVVP方法主要关注全局预测优化，忽视了片段级（segment-level）的稳定监督和类别感知的跨模态对齐，导致时序定位和模态一致性有限。作者希望通过引入更细粒度和稳定的监督，提升模型性能。

Method: 1）提出基于指数滑动平均（EMA）引导的伪监督框架，使用自适应阈值或top-k选择生成可靠的片段级掩码，提供超越全局标签的时序指导；2）设计类感知跨模态一致性（CMA）损失，在可靠的片段类别对上对齐音频和视觉特征，兼顾模态一致性与时序结构。

Result: 在LLP和UnAV-100两个公开数据集上，提出的方法在多项评测指标上均达到最新最优（SOTA）水平。

Conclusion: EMA引导的伪监督和CMA损失联合优化可有效提升弱监督音视频解析任务的精度和鲁棒性，为后续相关研究提供了新思路。

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [75] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本论文提出了一种通过引入Soft mixture-of-experts (MoE)机制的新方法，以提升遥感基础模型的计算效率，同时保持或提升其表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型存在训练与推理计算复杂度高，或表示能力有限的问题，导致其实际应用受限。作者希望提升模型效能并降低资源消耗。

Method: 作者将Soft MoE机制整合进Cross-Sensor Masked Autoencoder（CSMAE）模型，形成新的Cross-Sensor Mixture-of-Experts (CSMoE)模型，实现了模态特定专家和跨传感器共享特征学习。同时提出了一种基于主题-气候描述符的采样策略以构建多样化的训练集。

Result: 在场景分类、语义分割和基于内容的图像检索等任务上，CSMoE在保持或提升表现能力的同时，计算效率相比现有模型提升两倍以上。

Conclusion: 引入Soft MoE机制的CSMoE模型，在计算资源有限的场景下表现突出，在各类遥感任务上达到了高效与强表示能力的优良平衡。

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [76] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种更为健壮的虚拟染色框架，通过级联配准机制解决生成输出与真实标签之间的空间错配问题，有效提升了染色切片的虚拟分析质量。


<details>
  <summary>Details</summary>
Motivation: 传统病理诊断依赖于多重染色，过程耗时、费力且不环保。虚拟染色作为更高效且环保的替代方法，面临现有方法需要高质量对齐配对数据的问题，而获取这种数据极为困难，制约了其临床应用。

Method: 作者提出了一个虚拟染色框架，核心是引入级联配准机制，解决不同染色样本之间存在的配准、错位问题，从而允许在配准不严的情况下进行有效监督和训练。

Result: 在五个数据集上的实验结果显示，该方法在内部数据集上平均提升3.2%，外部数据集上提升10.1%。在配准误差较大的数据集上峰值信噪比提升23.8%，效果显著优于当前主流方法。

Conclusion: 所提方法显著提升了虚拟染色在不同场景下的鲁棒性，降低了高质量数据获取难度，有望推动虚拟染色技术的实际转化和应用。

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [77] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本文研究美颜滤镜对深度伪造（deepfake）和变脸攻击（morphing attacks）检测器性能的影响，发现美颜会削弱检测系统的有效性，揭示了检测模型在实际环境中的潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体数字美颜滤镜的盛行，以及基于人脸识别的安全系统对原始人脸图片和视频依赖的增加，社会对于美颜后图像真实性及自动人脸分析准确性的担忧日益上升，尤其是在深度伪造和变脸攻击等安全相关应用场景。本研究旨在厘清美颜滤镜是否会影响现有数字操控检测器的识别能力。

Method: 作者在多个基准数据集上，分别在使用前后对多种先进的深度伪造和变脸攻击检测器进行了评估，并系统性地应用了不同类型的美颜（平滑）滤镜，对比检测器在美颜前后的表现差异。

Result: 实验结果显示，一旦应用美颜滤镜，多个检测器的识别准确率明显下降，即美颜处理削弱了检测模型对人脸数字操控的辨识能力。

Conclusion: 文章指出当前检测模型对美颜类图像处理存在脆弱性，呼吁改善和发展对美颜滤镜具有鲁棒性的检测方法，以确保人脸识别与数字防伪系统的安全性和可靠性。

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [78] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025 Challenge集中于多模态推理，发布了针对现实和专业场景的新基准、数据集和评测，促进了MLLMs领域的发展。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型和多模态推理成为当前人工智能研究的热点领域。为了推进该领域的发展，需要有权威、系统的基准和广泛的应用场景，以便统一评测，并推动算法在实际和专业场景的应用能力。MARS2 2025 Challenge正是为了填补该需求，提升多模态推理领域的研究深度和广度。

Method: 组委会发布了两个新数据集（Lens和AdsQA），分别用于日常场景下的通用推理和广告视频下的专业推理。设置了三个比赛赛道：现实场景视觉定位、具空间理解的视觉问答、广告视频视觉推理。共评测了40多个基础模型，并开放了排名和数据。同时吸引了76支国际知名团队参与。

Result: 共收到超过1200份提交，有40余份有效结果被纳入官方排名，涵盖从通用型多模态大模型（MLLMs）到专用任务模型。所有数据集、基线模型、部分参赛方法及排名信息均已公开。

Conclusion: 本次挑战为多模态推理领域提供了权威基准和丰富的数据资源，推动了多模态大模型在现实及专业复杂场景的应用，也为今后研究与竞赛提供了基础平台。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [79] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 本文通过构建分层抽象图像数据集(HAID)，系统研究了抽象图像与光栅图像在视觉任务中的表现差异，并评估抽象图像作为语义信息载体的潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然利用基本图形生成的抽象图像能够让深度学习模型捕捉到一定语义信息，但与传统光栅图像相比，表现仍有差距，激发了对这种表现差距成因及抽象图像表现力的深入研究需求。

Method: 作者提出并构建了一个新的分层抽象图像数据集HAID，从普通光栅图像生成不同抽象层次的图片。然后在该数据集上，分别用传统视觉系统进行分类、分割和目标检测任务的训练与评估，系统对比了抽象与光栅图像表现的异同。

Result: 实验结果详尽对比了不同层次抽象图像与光栅图像在多项任务上的表现，揭示了抽象层次对高层语义信息保留的影响，并量化了两者的性能差距。

Conclusion: 抽象图像具备一定的视觉语义信息承载能力，在特定任务中可作为有效的信息表达形式，但目前的表现尚无法完全替代传统光栅图像。

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [80] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对多视角BEV（鸟瞰图）3D目标检测的跨域自适应方法，有效减缓了因域转移带来的性能退化，显著提升了跨域检测精度。


<details>
  <summary>Details</summary>
Motivation: 随着BEV感知在自动驾驶领域的重要性提升，现有研究多集中于提升效率或精度，但忽视了实际应用中的域转移（Domain Shift）问题，导致模型在新场景（如白天与夜晚、城市间）中的性能显著降低。因此亟需解决BEV多视角3D目标检测的域适应难题。

Method: 作者提出了几何感知的教师-学生（BEVUDA++）框架。该框架包括：1）可靠深度教师（RDT），融合目标域LiDAR和基于不确定性估计的深度预测，输出深度敏感特征用于改善目标域特征提取；2）几何一致的学生（GCS）模型，将多空间（2D、3D体素、BEV）特征映射至统一的几何嵌入空间，缩小域分布差异；3）引入不确定性引导的指数移动平均（UEMA），利用历史不确定性辅助权值更新，降低跨域误差累计。

Result: 该方法在四种典型跨域场景下进行了实验验证，显著优于现有方法。例如在白天-夜晚的域适应任务中，NDS指标提升12.9%，mAP提升9.5%。

Conclusion: BEVUDA++框架能有效缓解BEV 3D目标检测中的跨域性能退化，推动了自动驾驶BEV感知在复杂实际场景中的落地应用。

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [81] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: 本文提出STEP框架，通过动态patch融合和token剪枝，大幅提升ViT在高分辨率语义分割中的效率，明显减少计算与显存开销，同时保持精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 虽然ViT在语义分割任务中取得了最先进的性能，但其极高的计算和内存消耗限制了其在实际高分辨率场景（如1024x1024）中的应用。因此，提升ViT效率成为一个亟需解决的问题。

Method: STEP包含两个核心机制：一是dCTS动态CNN策略网络，用于灵活地将patch融合成superpatch，从而减少token数量；二是编码器层引入早期退出机制，高置信度的supertoken在中间层即可输出并被剪枝。STEP实现了混合token减少（token-reduction）框架。

Result: dCTS单独使用时，token数量可降至原本的1/2.5，相比标准patch方案，计算量减少2.6倍，吞吐量提高3.4倍。完整STEP框架可使计算复杂度降至1/4、推断速度提升1.7倍，同时最大精度损失控制在2%。有多达40%的token可在encoder早期终止预测，进一步降低负担。

Conclusion: STEP框架有效在保持语义分割准确率的同时，显著减轻了ViT的计算与显存负担，是提升高分辨率场景ViT应用可行性的重要技术。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了针对长篇电影理解的综合性基准数据集Cinéaste，主要用于评估多模态大模型在细粒度叙事推理方面的能力。该基准测试表明，现有模型在长程时序推理上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽然在视频理解方面取得了一些进展，但其对长篇电影等复杂叙事内容中深层语义与推理能力的评估仍存在空白。现有基准多为短片识别或模板问答，无法测评对精细情节与背景的推理理解能力。因此，需要一个能够覆盖多种推理类型且能反映模型实际理解与推理水平的新型基准。

Method: 作者构建了一个包含3119个多项选择题的电影理解数据集，涵盖200部电影、1805个场景与5种细粒度推理类型。试题采用GPT-4o融合多源上下文信息自动生成，并通过两阶段筛选方式保障题目质量：其一，剔除不需要视频上下文的题目（Context-Independence filtering）；其二，核查题目内容真实性（Contextual Veracity filtering），减少幻觉与错误。

Result: 实验结果显示，现有多模态大模型（MLLMs）在该数据集上的表现有限，最好的开源模型准确率仅为63.15%。进一步分析指出，模型在长时序推理能力上仍有显著瓶颈。

Conclusion: Cinéaste基准揭示现有模型在长篇电影精细叙事理解和推理上的技术短板，呼吁在多模态长程推理及深层上下文理解方面进行进一步研究与方法创新。

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: 本文提出了GenExam，这是第一个多学科文本到图像考试基准，旨在严格评测生成式模型在知识、推理和生成三方面的综合能力。实验发现，即使是最先进的模型在该基准上表现也很差，表明这一任务极具挑战性。


<details>
  <summary>Details</summary>
Motivation: 目前的大部分考试型基准专注于理解与推理，现有生成任务又偏重世界知识和视觉概念的表达，缺乏对严谨的绘图考试能力评估。本文希望弥补此空白，提出能全面评测模型综合能力的新基准。

Method: 作者设计了GenExam，包括10个学科、1000个考试题目，题目组织在四级分类体系下。每道题配有标准答案图片与细粒度评分点，用于精确评估语义正确性和视觉合理性。

Result: 实验结果显示，即使是GPT-Image-1和Gemini-2.5-Flash-Image等最先进模型，严格得分率也不足15%，大部分模型接近0%，表明任务很有挑战性。

Conclusion: GenExam从考试角度评估生成模型的综合能力，对通用人工智能的发展路线提供了有意义的见解。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文系统性评估了多种大语言模型（LLMs）在意大利语性别中立重写任务上的表现，并提出了兼顾中立性与语义保持的双维度评价框架。结果显示，开源大模型整体优于现有专用于意大利语GNR的模型，且小型微调模型在效率和表现之间有良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有性别中立重写方法在意大利语等存在语法性别的语言上面临艰巨挑战，缺乏系统评测和优质模型。为推进性别中立语言处理，评估和提升大模型在该领域的能力成为必要。

Method: 提出了一个双维度框架，分别衡量性别中立性与语义保持，并使用少量样例提示法评测多种LLM，针对性微调模型并对数据进行清洗以提升表现。

Result: 开源权重的LLMs在意大利语GNR任务上超越当前唯一专用模型。微调的小模型性能与最佳大模型持平或更好，同时体积更小、效率更高。

Conclusion: 大语言模型具备良好的意大利语GNR能力，通过数据清洗和微调可以在轻量模型获得媲美大模型的效果，在优化中立性与语义保持之间需充分权衡。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [85] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: 本文介绍了Op-Fed数据集，该数据集包含来自美国联邦公开市场委员会（FOMC）会议记录的1044个人工标注句子及其上下文，用于货币政策意见分析。作者提出创新的数据收集和标注方法，并对当前主流大模型在该数据集上的表现进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 分析和理解FOMC会议纪要中的货币政策立场对于经济学者和政策制定者非常重要，但相关数据稀缺，且句子极不均衡，标注难度高。因此，作者希望构建一个有标注的数据集，推动该方向自动化研究。

Method: 作者设计了包括意见、货币政策、立场和上下文需求在内的五阶段层次标注方案，同时通过主动学习从大量语料中筛选代表性句子，优化正例比例，并完成人工标注。之后评估了主流闭源大语言模型在该数据集上的零样本分类性能。

Result: 闭源大模型在意见分类任务上零样本准确率达0.80，但在货币政策立场分类上准确率仅为0.61，明显低于人工标注基线的0.89，表明智能模型目前存在短板。

Conclusion: Op-Fed数据集为货币政策文本分析提供了基础样本，将促进后续模型训练、信心校准及更大规模标注工作，对金融与自然语言处理领域具有重要意义。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [86] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: 本文介绍了DSTC12第1赛道关于对话系统评测的两项任务：多维自动评测和多语多文化安全检测，并分析了Llama-3及Llama-Guard基线模型及参赛系统的表现，强调评测与安全尤其在文化层面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，对话系统评估需求剧增，但当前评测方法和安全性界定存在局限，尤其在多维度、语言与文化适应性以及全面安全性方面存在明显短板，因此有必要提出更全面的评估框架。

Method: 本赛道设计了两个子任务：（1）10个对话维度的多维自动评测，采用Llama-3-8B作为基线；（2）多语种与多文化安全检测，使用Llama-Guard-3-1B作为安全性基线，所有参赛者使用主办方提供的数据集参与对比测试。

Result: 在多维度自动评测任务中，当前基线（Llama-3-8B）表现有限（斯皮尔曼相关系数仅0.1681）；多语安全检测中，部分团队显著超越基线（ROC-AUC高达0.9648），但文化适应安全评测上基线反而更优（0.5126 ROC-AUC），显示文化敏感安全检测难度较大。

Conclusion: 当前对话系统综合评测和安全性评估仍有很大提升空间，尤其是对文化维度的安全问题识别瓶颈明显。该赛道为社群提供了公开数据和基线模型，有助进一步推动多维、跨文化安全的评测方法创新。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [87] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 本文提出了一套分析框架，以理解大语言模型在跨任务迁移学习中的表现和复杂交互，并发现隐含统计因素比表层数据相似性更决定迁移效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被广泛应用于多样化任务，其中很多任务在训练时并未涉及，因此无法为所有任务获得高质量的训练数据。迫切需要分析和改进跨任务迁移学习的效果，以应对实践中的分布外请求。

Method: 作者提出了“迁移学习矩阵”结合降维方法的分析框架，对10个模型进行训练和剖析，识别模型间的潜在能力（如推理、情感分类、自然语言理解和算术等），并分析迁移学习过程中的副作用。

Result: 研究发现，性能提升往往与数据集表层相似性或数据质量无明显关联，反而是源数据集中的隐藏统计因素（如类别分布、生成长度偏好）以及具体语言特征起到了更大作用。

Conclusion: 该工作揭示了迁移学习动态的复杂性，强调需关注源数据隐含因素，为未来更可预测、有效地适应LLM提供了重要见解和理论基础。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [88] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 本论文发现大型语言模型（LLM）能够在其内部神经元层面紧凑地编码问题含糊性，并可通过操控相关神经元控制模型响应方式。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的问题常常带有歧义，但目前LLM在遇到含糊问题时，往往给出自信的直接答案，而非澄清问题。理解LLM内部如何处理含糊性，有助于让模型生成更合理、可解释的回应。

Method: 作者分析了LLM在处理问题时的内部表示，定位出编码问题含糊性的神经元（AENs）。通过微调探针用于检测和操控这些神经元，实验涵盖了层级分析、跨数据集泛化对比、以及与现有方法的效果比较。

Result: 极少数（甚至只需一个）神经元显著编码了问题含糊性，基于AENs的探针模型在各项含糊检测任务上优于基于提示和表示的主流方法。AEN主要出现在早期浅层，同时通过操控这些神经元，可以让模型从直接作答切换为选择回避。

Conclusion: LLM会在内部形成对问题含糊性的紧凑可控表示，相关神经元有良好的可解释性和可控性。这为开发更具解释力与灵活性的LLM提供了新的途径。

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [89] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 本文介绍了首个针对多学科中文学术写作的持续学习中文语法纠错（CGEC）基准数据集CL$^2$GEC，并评估了不同持续学习方法在跨领域语法纠错任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着自动写作辅助需求的增长，现有中文语法纠错系统在多学科适应性不足，缺乏跨学科专用测试集和持续学习能力，尤其在防止遗忘和适应语言风格变化方面存在短板。

Method: 作者提出CL$^2$GEC数据集，包含10个学科共1万条人工标注句子，模拟连续暴露于不同学科环境，并通过大语言模型分别应用顺序微调、参数高效适配和四种典型持续学习算法，结合标准GEC指标与任务级持续学习指标，系统评估各方法跨领域适应与遗忘情况。

Result: 正则化类持续学习方法在缓解遗忘方面优于重放类和普通顺序学习方法。

Conclusion: CL$^2$GEC为后续多领域自适应中文语法纠错研究提供了重要基准，验证了正则化方法在持续学习中的有效性，对提升系统学科适应性与科研应用具有推动作用。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [90] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: 提出了AgentCTG，一个基于多智能体协作的可控文本生成框架，能实现更精细和复杂的生成控制，在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有可控文本生成方法难以实现细粒度条件控制，且在实际应用中存在成本、可扩展性、领域知识学习与精确控制等挑战。

Method: 提出AgentCTG，模拟多智能体工作流进行文本生成控制，并引入自动提示模块（auto-prompt），探索多智能体间多种协作方式；并提出了新的Character-Driven Rewriting评测任务。

Result: AgentCTG在多个公开数据集上取得了最先进的效果，并在Character-Driven Rewriting任务中表现优异，能在满足人物设定的同时保留领域知识。在线角色扮演导航场景下，提升了内容交付和用户体验。

Conclusion: AgentCTG显著增强了文本生成的控制力和应用价值，提升了个性化和用户参与度，对实际线上社区和应用场景具有重要意义。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [91] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 提出了CARE框架，增强大语言模型在推理过程中对上下文证据的整合能力，显著提升了检索准确率和回答质量，且对标注数据需求较低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于已提供信息回答问题时，经常出现与上下文不一致的问题，现有方法要么依赖昂贵的监督微调生成证据，要么让模型学习网络检索但未必优化上下文利用效率，因此亟需一种高效、可靠地增强模型上下文利用能力的新方法。

Method: 提出了CARE（原生检索增强推理）框架，通过引导LLM在推理链中显式调用和整合上下文证据，并结合其自主检索能力，仅需少量标注证据训练数据，即可显著提升模型理解和利用信息的能力。

Result: 在多个真实及反事实问答基准测试上，该方法不仅超越了传统的监督微调、检索增强生成（RAG）及外部检索方案，还大幅提升了检索准确率和回答的表现。

Conclusion: CARE框架大幅加强了LLM在知识密集型任务中的准确性、可靠性和效率，是推动大模型真实应用的关键进展。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [92] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在处理日语比较结构推理上的表现，特别是在涉及数值和逻辑表达时的能力和局限。通过新构建的数据集，评估现有模型并分析其不足。


<details>
  <summary>Details</summary>
Motivation: 多数LLM在英文等主要语言的自然语言推理任务上表现优秀，但对于非主导语种（如日语）中的比较结构推理，其理解能力不明。此外，处理比较、数值、逻辑等表达历来是推理范畴的难点，需要专门评估和改进。

Method: 作者构建了针对日语比较结构的NLI数据集，并在零样本（zero-shot）和少样本（few-shot）设置下评测多种主流LLM。分析模型对不同提示（prompt）格式和示例标签的敏感性，并研究采用逻辑语义提示对模型表现的影响。

Result: 评测结果发现：一、在零样本条件下，模型对提示格式高度敏感；二、在少样本条件下，示例标签对模型有显著影响；三、在处理日语特有的语言现象时，现有模型表现不足；四、添加逻辑语义表示的提示，可显著提升模型在难例推理任务的正确率。

Conclusion: 当前LLM在非主导语言（如日语）的比较结构推理任务中仍有较大提升空间，合理设计提示（尤其是包含逻辑语义的提示）有助于模型推理能力提升。针对特定语言和推理类型的数据集构建与提示工程尤为重要。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [93] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 该论文探讨了如何将大语言模型（LLM）应用于包含结构化数据（如时间序列）的临床分类任务，并取得与专业多模态系统相当的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本生成上表现优异，但其处理结构化临床数据的能力尚未被充分研究。现有多模态系统虽然效果好，但复杂度高，适应性差。作者希望通过简化方法提升LLM在医疗领域中的应用价值。

Method: 作者采用了DSPy驱动的提示优化，将经过指令微调的LLM用于同时处理临床文本和结构化EHR（电子健康记录）数据。

Result: 该方法实现了与先进多模态系统相当的性能，同时由于模型结构简单、适应性强，可应用于多种任务。

Conclusion: 将LLM通过合适提示优化后联通结构化与非结构化医疗数据，能够有效兼顾多模态系统性能与模型简洁性，对临床任务具有更广泛适用性。

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [94] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: 提出了一种主动抑制大语言模型幻觉的新框架DSCC-HS，通过代理模型动态干预解码过程，在TruthfulQA和BioGEN上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成内容时容易产生“幻觉”（不真实或虚构内容），这是其实际应用的重要障碍。现有方法多为被动反应，难以主动防控幻觉。

Method: 提出动态自强化校准框架DSCC-HS。该方法受双过程认知理论启发，训练两个代理模型：事实对齐代理（FAP）与幻觉检测代理（HDP），两者分别担任对抗性角色。在解码过程中，根据这两个代理模型的logits差（实时引导向量）动态引导目标大模型的输出，无需修改目标模型参数，实现即插即用。

Result: 在TruthfulQA数据集上，DSCC-HS达到了99.2%的事实一致率（FCR）；在BioGEN长文本生成任务上获得了最高FActScore（46.50），均达到当前最优水平。

Conclusion: DSCC-HS是一种无侵入、有效且主动的策略，显著增强了大语言模型输出的事实性，为幻觉抑制提供了新的解决方案。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [95] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于自然语言处理（NLP）的筛查工具，可自动检测放射肿瘤学高严重性事件报告，并在两个不同机构的数据集上测试其泛化能力与准确性。


<details>
  <summary>Details</summary>
Motivation: 手动审核医疗事件报告费时且需专业知识，且现有方法难以高效、准确地识别高严重性事件。因此需要自动化工具辅助安全与质量改进工作。

Method: 收集两个机构共7665份已标注严重性的事件报告，采用支持向量机（SVM）和基于PubMed及患者数据预训练的大语言模型BlueBERT进行训练与测试，并通过迁移学习在不同机构间对模型泛化性进行评估。

Result: 本机构数据集上，SVM和BlueBERT的AUROC分别为0.82和0.81。跨机构直接测试时表现较差（SVM为0.42，BlueBERT为0.56），但通过迁移学习后BlueBERT_TRANSFER模型在新机构上AUROC提升到0.78。部分手工校正数据集上，BlueBERT_TRANSFER模型表现接近人工。

Conclusion: 开发的跨机构NLP模型在自动检测放射肿瘤学高严重性事件报告上，表现与人工相近，为提升医疗安全事件识别效率提供了潜在工具。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [96] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练、分两阶段的提示压缩方法（DSPC），在减少计算成本的同时，有效保持大模型推理性能，实验结果优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在NLP任务中的应用越来越广泛，驱动这些模型的提示（prompt）长度不断增加，导致计算成本显著提升，因此需要有效的提示压缩方法以控制开销。

Method: 作者提出了Dual-Stage Progressive Compression（DSPC），分为粗粒度和细粒度两个阶段。粗粒度阶段通过TF-IDF进行语义相关句子的筛选，剔除低价值句子。细粒度阶段结合注意力贡献、跨模型损失差异和位置重要性等指标，评估与裁剪低效词元，实现高效压缩。整个方法无需额外训练辅助模型。

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上，DSPC在限定token预算下均取得了持续提升。例如在Longbench数据集FewShot任务中，提示长度仅为原先1/3时，DSPC得分为49.17，相较于最优的LongLLMLingua提升了7.76分。

Conclusion: 提出的方法DSPC在无需辅助训练的情况下，有效压缩输入token数量，同时保持甚至提升了大语言模型的推理能力，优于当前主流的压缩算法。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [97] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文提出了一种基于组合语义学的日语对比句自然语言推理（NLI）逻辑推理系统ccg-jcomp，并证实其在含有对比表达的数据集上比大型语言模型（LLM）表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前NLI任务中涉及到对比句时面临挑战，尤其是量词及比较关系的理解。虽然已有英文的逻辑推理系统，但由于日英对比句在形态和语义上的差异，现有英文方法难以直接应用于日语，需要新的方法填补此空白。

Method: 作者提出ccg-jcomp系统，基于组合语义框架，针对日语对比句进行逻辑推理，并在包含对比表达的日语NLI数据集上进行了评测，与现有LLM进行了对比实验。

Result: 实验证明，ccg-jcomp系统在该任务中的准确率优于现有的大型语言模型。

Conclusion: 基于组合语义的逻辑推理系统ccg-jcomp能够更有效地理解和推理日语对比表达，优于仅依赖LLM的方法，为日语NLI提供了更可靠的解决方案。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [98] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文研究了多种高效利用数据与参数的方法用于阿拉伯方言识别，发现LoRA方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言识别存在数据有限与模型参数效率的挑战，因此作者希望探索如何用更少数据和参数高效提升识别性能，特别是在大模型的情况下。

Method: 作者实验了多种soft-prompt方法（如prefix-tuning，prompt-tuning，P-tuning，P-tuning V2）、LoRA重参数化，并在zero-shot与few-shot场景下测试硬提示。实验模型包括阿拉伯专用编码器、多语种和阿拉伯语特定解码器。

Result: 大语言模型在零样本或小样本下难以区分阿拉伯方言，编码器上的soft-prompt方法表现较好，LoRA微调表现最佳，甚至优于全量微调。

Conclusion: 基于LoRA的参数高效方法在阿拉伯方言识别任务上最有效，尤其适合阿拉伯语任务和资源有限场景。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [99] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出了一种新的指令微调课程学习框架CAMPUS，用于提升大语言模型的指令微调效率和效果，并取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前的课程学习方法在指令微调中有效，但存在“课程刚性”问题，即过分依赖静态难度评估，无法适应模型训练中能力的动态提升，导致训练路径固定且次优。

Method: 提出了CAMPUS框架，通过：1）动态子课程选择，2）基于模型能力的课程进度调整，3）多重难度调度，从多个角度和动态变化自动调整训练数据次序。

Result: 大量实验显示，CAMPUS在高效指令微调上优于主流强基线方法。

Conclusion: CAMPUS能够自适应模型能力和样本难度，提升指令微调效果，是更优的课程学习方案。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [100] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 该论文研究了职位头衔中的语法性别如何影响自动职位排名系统，并提出了一种基于性别控制的排名比较方法来评估性别偏见。作者还构建并公开了四种语法有性别的语言的测试集，并用这些数据评估了多语言模型的性别偏见。结果显示，所有测试模型都存在不同程度的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 随着自动职位推荐和排名系统的普及，职位头衔的性别表达可能影响系统的排名结果，从而带来性别偏见。现有评估方法尚不足以专门检测这一问题，因此需要更科学的评估标准与测试集。

Method: 作者提出利用RBO（Rank-Biased Overlap）等基于排名的指标，并针对四种语法性别语言（即职位头衔有阳性和阴性的语言）构建了测试集。测试集包含以阳性和阴性形式表达的职位，并进行了性别和相关性的标注。随后，作者用这些数据集与新指标评估了多个主流多语言模型作为基线。

Result: 所有评测的多语言模型在职位排名任务上都表现出不同程度的性别偏见。新测试集与评估指标有效揭示了模型中隐藏的性别偏见问题。

Conclusion: 职位头衔中的语法性别会带来自动职位排名系统的显著性别偏见。控制性别后进行排名比较和专门的数据集有助于发现并量化这种偏见，为后续改进模型和机制打下基础。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [101] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 本文提出了一种全新的几何框架，能在仅黑盒访问大语言模型的情况下，同时量化全局和局部不确定性，以更有效检测和减少模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽然在多任务上表现优异，但仍频繁出现幻觉现象，即生成内容在语言上合乎逻辑但事实不符。现有基于不确定性量化的方法，要么只能计算全局（整体）不确定性，要么依赖对模型内部的白盒访问，缺乏一种仅凭模型输出、即可同时衡量全局和局部（单个回应）不确定性的黑盒方法。

Method: 作者基于一批模型输出（回答）向量的原型分析，引入'几何体积'（Geometric Volume）和'几何怀疑分数'（Geometric Suspicion）：前者通过计算回答嵌入的凸包体积衡量全局不确定性，后者基于凸包边界点识别局部可疑回答，并据此筛选更可靠输出。此框架完全基于黑盒访问，无需获取模型内部状态。

Result: 实验显示，该框架在常见问答任务中具有与现有方法持平或更优的表现，尤其在医疗等对幻觉容忍度极低的数据集上效果优异。

Conclusion: 本文工作实现了仅用黑盒访问即可同时进行全局及局部不确定性量化，为幻觉检测与应对提供了有效工具。理论上还证明了凸包体积与信息熵之间的联系，进一步增强了方法的解释性和可靠性。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [102] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: 该论文介绍了AutoMin 2025会议自动纪要生成（minuting）和基于会议转录的问答（QA）任务，涉及多语种和多场景的自动化会议内容理解和总结。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，会议纪要自动生成和基于会议内容的问答能力提升成为关注焦点。AutoMin旨在为这些任务提供统一评测平台，评估不同方法在真实多语种、多领域场景中的表现。

Method: 本届AutoMin包含两个主要任务：1）纪要生成，涵盖英语和捷克语、项目会议和欧盟议会双领域；2）会议问答，仅面向项目会议，分为英语单语问答和捷克语基于英语内容的跨语问答。参与队伍较少，主办方补充了多种LLM基线系统以保证评测全面性。

Result: 2025年参赛队伍数量较往年减少。纪要生成只有一支队伍，问答任务有两队，但官方基线系统覆盖了主流LLM模型，完成了对各任务效果的系统性评价。

Conclusion: AutoMin 2025虽受限于参赛团队数量，依靠高质量基线补充，实现了对纪要生成及会议问答任务领域LLM能力的覆盖性评价，推动自动会议内容处理技术的前沿发展。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [103] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）是否对德国方言及其说话者表现出与人类社会相似的偏见。结果显示，所有测试的模型都展示了明显的对方言及方言使用者的负面刻板印象和决策偏见，尤其在明确标示“方言说话者”身份时，偏见反而被放大。


<details>
  <summary>Details</summary>
Motivation: 社会对方言说话者存在负面刻板印象，但目前不清楚这些刻板印象是否会被大型语言模型所复制。鉴于LLM在实际应用中越来越普及，研究其在方言辨识和使用上的潜在偏见具有重要的社会和伦理意义。

Method: 论文依据社会语言学中对方言认知的相关特质，构建了一套包含七种德国地方方言及其标准德语对照句的全新评价语料库。通过关联任务（以形容词为主的观感关联）和决策任务（模型对方言说话者的判断任务）测试LLM对方言及使用者的偏见程度。

Result: 1）在形容词关联任务中，所有评估的LLMs都表现出了针对德国方言说话者的负面“命名”和“使用”偏见；2）所有模型在决策任务中也重现了这种偏见；3）研究发现，对于“方言说话者”的显性标签，偏见更为明显，这与先前在性别、种族等领域的结论不同。

Conclusion: 大型语言模型不仅复制了社会对方言说话者的负面刻板印象，有时甚至比隐含暗示的偏见更显著。研究呼吁在设计和部署LLM时，应注意方言及其说话者的公平性问题，避免技术进一步加深社会偏见。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [104] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 本研究系统性分析了不同大型语言模型（LLM）在社会偏见场景下与人类价值观对齐的表现。发现LLM在不同类型问题下的对齐表现存在差异，且参数规模大不一定更优。模型家族内的一致性较高。还考察了模型对社会偏见的理解力及解释性，更小模型通过微调也能生成可读性高的解释，但与主流模型一致性较低。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在涉及社会敏感偏见时可能与人类价值观不符，而模型间、不同场景下的对齐表现差异尚不清楚。填补了大模型在具体偏见类型场景下是否表现一致，以及模型自解释能力等方面的空白。

Method: 对4个模型家族、共12个LLMs，基于4个数据集，从负面/非负面等不同偏见类型场景出发，系统评估它们与人类价值观对齐（HVSB）的表现，同时分析模型对偏见问题解释能力，并通过微调赋予小模型解释社会偏见的能力。

Result: 发现：（1）LLM参数规模大不一定更好，误差率和被攻击成功率未必更低；（2）同一模型家族内判断较一致；（3）不同模型对偏见场景有对齐偏好；（4）LLMs在社会偏见解释能力上差异不大，但倾向于偏好自己的解释；（5）小模型微调后可生成更通顺解释，但与权威模型结论一致性差。

Conclusion: LLM在多种社会偏见场景下的对齐表现并非一味随规模提升，场景和模型家族影响大，并且解释能力需要进一步提升和权衡可读性与结果一致性。这对LLM系统开发与公平性研究有重要启示。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [105] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 该论文提出一个结合科学证据检索、大语言模型推理与监督式真实性预测的生物医学事实核查框架CER，显著提升了医学领域自动事实核查的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动事实核查虽有发展，但在生物医学领域因术语复杂、需高专业知识且必须依托科学证据，导致难以准确验证医学相关信息。错误信息会直接威胁公共健康和对医疗系统的信任，因此急需更可靠的自动核查方案。

Method: 提出了CER框架，将先进的证据检索方法与大语言模型文本生成能力结合，利用多步骤流程，先检索高质量医学文献，再用语言模型进行推理和生成解释，最后通过监督学习进行真伪判别，确保输出结果有据可依，减少虚假生成（幻觉）风险。

Result: 在HealthFC、BioASQ-7b与SciFact等专家标注数据集上，CER框架实现了当前最优表现，并表现出强的跨数据集泛化能力。

Conclusion: CER通过结合证据检索和语言模型推理，提升了生物医学事实核查的准确性和可信度，为打击医疗虚假信息提供了有效工具，且框架和数据已公开，便于社区透明和复现。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [106] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 该论文提出一种新的生物医学领域事实核查框架 CER，实现更准确、可靠的健康信息自动核查。


<details>
  <summary>Details</summary>
Motivation: 医疗健康领域的错误信息严重危害公共健康和医疗公信力，目前自动化事实核查在生物医学领域依然面临专业性强、术语复杂、必须依托科学证据等难点，现有技术在此领域提升空间大。

Method: 该方法提出CER框架，结合科学证据检索、大型语言模型推理与监督判断，通过先进的检索能力搜集高质量生物医学证据，并用大模型推理和预测真假，有效减少模型幻觉，提升结果的证据性和可靠性。

Result: 在专家标注的HealthFC、BioASQ-7b和SciFact数据集上，CER取得了目前最优的事实核查性能，并展现出很好的跨数据集泛化能力。

Conclusion: CER框架可有效提升生物医学事实核查的准确性和可解释性，对提升公众健康信息可信度具有重要意义。相关代码和数据也已公开，便于复现和进一步研究。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [107] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 本论文评估了大语言模型（LLMs）对词义理解能力，尤其关注于词义消歧（WSD）和在生成性任务（定义、解释、例句）中的表现。结果显示，主流LLMs在WSD任务上的表现已与专用系统相当，并在生成任务中表现出极高的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量对LLMs的评测，但目前尚缺乏对其是否真正理解词义的深度探讨。作者希望通过实验系统地评估LLMs的词义感知和应用能力。

Method: 1）对指令微调后的LLMs进行词义消歧任务测试，与现有最优WSD系统做对比；2）评估两种顶尖（开源与闭源）LLMs在定义生成、自由解释、例子生成三种任务中的理解词义能力。

Result: 在WSD任务中，GPT-4o和DeepSeek-V3等主流LLMs的表现已与专用WSD系统持平，且在跨领域、不同难度情况下表现更为稳健。在生成任务中，LLMs对语境中词义的解释准确率高达98%，特别是在自由解释任务中表现最佳。

Conclusion: 主流LLMs不仅能在传统的WSD任务中媲美专用系统，还在生成性语境理解任务上展现出极高的准确性与鲁棒性，显示了其对词义及语境深度理解的能力。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [108] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 本论文研究了多语言检索增强生成系统（mRAG）在跨语言场景下的引用偏好，发现模型在英文查询下更偏好引用英文文献，并且这种偏好在低资源语言中更为明显，有时甚至会牺牲文献的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有mRAG系统能处理多语言知识密集型查询，但尚不清楚混合不同语言文献是否对生成和引用带来预期外的影响。

Method: 作者设计了一种控制变量的方法，通过分析模型内部机制，在文献相关性等因素保持不变的情况下，衡量模型对不同语言文献的偏好。实验覆盖八种语言和六个开源模型。

Result: 发现英文查询时，模型更倾向引用英文源文献，且低资源语言下这一偏好更强，在文献位于上下文中间位置时也会增强。部分情况下，模型会以语言偏好取代文献相关性作为引用选择。

Conclusion: 多语言语境下，模型的引用行为受到语言偏好的影响，并不完全由信息相关性驱动。这对理解和优化mRAG系统的多语行为具有重要意义。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [109] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 本论文提出了一种基于COMET框架的自动翻译质量评估系统，并展示了在长上下文数据增强下预测片段级误差跨度注释(ESA)分数的有效性。


<details>
  <summary>Details</summary>
Motivation: 翻译质量评估任务需要与人类判断尽可能高度相关的自动评分模型。现有方法普遍基于短句段，可能不足以捕捉长文本中的语境和复杂错误，因此有必要探索基于长上下文的自动评估方法。

Method: 作者将同领域、多句的人工注释句子拼接，构建长上下文训练数据，并根据句子评分加权平均获得整体分数。整合多个人工判断数据集（MQM、SQM和DA），通过归一化统一其评分尺度，训练多语言回归模型，输入源文、机器翻译假设与参考译文，输出质量分数。

Result: 实验证明，加入长上下文信息的模型与人工判断的相关性优于仅基于短段的传统模型。

Conclusion: 结合长上下文的信息以及多数据集一体化训练，可以提升自动翻译质量评估系统对人类判断的拟合度，未来应进一步探索上下文信息与模型融合的最佳方式。

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [110] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: 本文提出了Slim-SC方法，通过对比推理链之间的思维层级相似性，逐步剪枝冗余，从而极大提升大语言模型在推理类任务上的测试时推理效率，且不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自洽性(Self-Consistency, SC)技术虽然能提升LLM推理能力，但其并行生成多条推理链并多数投票的方式带来巨大计算开销，限制了实际应用，而现有加速方法效果和理论依据不足。

Method: 在分析SC低效性的基础上，作者提出Slim-SC，一种在推理链生成过程中，基于链间思维层级相似性，逐步识别并移除冗余推理链的剪枝策略。

Result: 在三个STEM领域推理数据集和两种主流LLM架构上的实验表明，Slim-SC方法在不降低甚至提升准确率的前提下，将推理延迟减少高达45%，Key-Value缓存使用量减少26%。

Conclusion: Slim-SC为大语言模型推理任务提供了一种简单高效的TTS加速方案，显著缓解了传统SC方法的效率瓶颈，具备广泛实际应用前景。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [111] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: 提出了一种名为ES-CoT的方法，通过在推理过程中检测答案收敛，能够早停生成推理链，大幅减少推理开销，且几乎无性能损失。


<details>
  <summary>Details</summary>
Motivation: 长链式思维（CoT）让大语言模型在复杂问题上表现更好，但也导致推理时消耗大量算力和时间，因此需要一种既能保持性能又能降低推理成本的方法。

Method: ES-CoT方法在每一步推理后让模型输出当前答案（step answer），然后跟踪连续相同step answer的次数。当这个次数突然跃升并超过设定阈值时，判定为答案已收敛并提前终止推理。理论和实验分析支持答案确实会稳步收敛。

Result: 在五个推理数据集和三个LLMs上的实验显示，ES-CoT平均能减少约41%的推理Token数量，同时保持与标准CoT方案相近的准确率。

Conclusion: ES-CoT能直接结合到现有主流推理方法，且对超参数不敏感，是一种高效实用的推理优化新方案。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [112] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文介绍了Hala，一个专注于阿拉伯语指令和翻译的大模型家族，采用创新的翻译-微调流程，显著提升阿拉伯语NLP的水平。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在阿拉伯语任务上的表现有限，缺乏高质量的数据和阿拉伯语专向的指令跟随能力。因此，研究者希望打造更优的阿拉伯语大模型，并推动阿拉伯语NLP发展。

Method: 首先，将强大的阿英翻译教师模型压缩到FP8精度，以提升推理速度并保持质量。用该模型生成高质量的双语监督数据；再用轻量级的语言模型LFM2-1.2B微调此数据，并用它把高质量英文指令集翻译为阿拉伯语，制成大规模指令微调语料。训练出多个参数规模的Hala模型，并采用slerp模型融合，平衡阿拉伯语能力与原始基座模型能力。

Result: Hala各参数规模模型在阿拉伯语相关的基准测试中均达到同类别（nano≤2B、小型7-9B）最优，全面超越了对应的基座模型。

Conclusion: Hala模型大幅提升了阿拉伯语NLP的能力，为后续相关研究提供了工具、数据和流程，有力推动了阿拉伯语大模型应用和研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [113] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 本文比较了基于文本和基于语音的机器翻译质量评估方法，发现语音评估和文本评估一致性较高，但在某些情况下更能体现系统差异，建议将语音评估纳入机器翻译评估体系。


<details>
  <summary>Details</summary>
Motivation: 机器翻译评估普遍依赖文本，而现实应用中翻译常以语音形式呈现，因此探索更自然、贴合实际的语音质量评估方式具有重要意义。

Method: 对WMT通用机器翻译任务的10个系统，分别以文本和语音两种方式，通过亚马逊众包平台收集评价，辅以统计显著性检验和自复制实验以分析语音评估的一致性及可靠性。

Result: 语音众包评估与文本评估结果大致一致，但在部分情况下，语音评价揭示了不同翻译系统间的显著差异。

Conclusion: 语音作为更自然的评价方式，对翻译质量判断具有独特价值，应在未来机器翻译评估框架中考虑引入语音评估。

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [114] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文系统性地验证了训练数据中缺乏包含丰富语境样本是影响机器翻译系统利用语境能力的关键瓶颈，并通过构建不同稀疏性的数据集展示了稀疏性与模型性能之间的强关联。提出并验证了提升语境利用能力的训练策略。


<details>
  <summary>Details</summary>
Motivation: 机器翻译在实现类似人类的翻译时，需要依赖上下文处理诸如代词消歧等复杂现象，但标准训练数据中包含丰富语境的样本稀缺，可能导致模型难以有效利用上下文。本文旨在系统性地验证这一假设，并探索缓解措施。

Method: 通过人为构建具有不同上下文相关样本比例的训练数据集，在单语和多语环境下，系统测试上下文稀疏性对模型性能的影响。同时，提出并实验了两种提升语境利用能力的训练策略，并评估其效果。

Result: 实验证实了上下文丰富样本稀疏性是模型性能提高的关键瓶颈，且在一个上下文现象上的提升无法泛化至其他现象。跨语种传递效果有限，且同语族内部并无明显更高提升。所提出的新型训练策略能在单语、多语环境下分别带来高达6和8个点的准确率提升。

Conclusion: 丰富上下文相关训练样本对提升机器翻译中的上下文利用非常关键，同时不同类型的上下文现象改进不能相互泛化。两个新训练策略被证明有效。未来研究需关注多样化丰富语境的样本获取与利用。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [115] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出在多智能体辩论（MAD）系统中引入“置信度表达”，让大型语言模型（LLMs）在辩论时能明确表达自身的置信度，提高系统表现。作者开发了ConfMAD框架并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: MAD系统利用多个LLM的协作提升任务表现，但由于缺乏置信度表达，不同LLM间难以有效沟通知识与推理优势，导致整体性能受限。错误的置信度还可能令系统无法纠正错误或过早达成次优结论。

Method: 提出将置信度表达机制集成进MAD系统，让LLM在辩论过程中明确传达自己的信心水平。作者具体开发了ConfMAD框架实现该想法，并进行了相关实验。

Result: 实验证明，引入置信度表达后，MAD系统的任务表现提升。作者还分析了置信度如何影响团队辩论的互动与最终表现。

Conclusion: 置信度表达有助于提升MAD系统效能，对设计更智能、更高效的多智能体协作语言模型有重要意义。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [116] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 本论文提出了一种新的手语翻译任务QB-SLT，通过融合对话语境提升手语翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译大多依赖于gloss标注，但gloss难以获得，而对话信息天然存在于交流中，并且更容易获取和标注。因此，探索如何高效利用对话语境辅助手语翻译具有重要意义。

Method: 作者提出了基于跨模态自监督学习并结合Sigmoid自注意力加权的融合方法（SSL-SSAW），通过对比学习对手语和问题文本进行特征对齐，并利用自注意力模块自适应提取问题与手语序列的关键信息，同时利用自监督方法提升表示能力。

Result: 在CSL-Daily-QA和PHOENIX-2014T-QA两个新构建的数据集上，SSL-SSAW方法达到了当前最优（SOTA）表现。实验还发现，易获得的问题文本辅助下的翻译结果可媲美甚至超过昂贵的gloss辅助。

Conclusion: 引入对话语境不仅可提升手语翻译效果，还为该领域在数据资源受限时提供了切实可行的技术路线。实验和可视化结果均证实了所提方法的有效性。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [117] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一款快速且强大的多语言语音识别（ASR）与语音翻译（AST）模型，支持25种语言，在准确率和速度上优于业界主流，特别是在英语ASR上领先，并且公开了新一代轻量模型Parakeet。


<details>
  <summary>Details</summary>
Motivation: 当前多语言语音识别和语音翻译模型在准确性、速度和多语言能力方面面临挑战，业界主流模型体积大、推理慢。作者旨在开发一种高效、准确且支持多语言的ASR与AST模型，以满足实际应用需求。

Method: 采用FastConformer编码器与Transformer解码器的结构，结合两阶段预训练和动态数据均衡的微调流程，加入非语音数据减少幻觉。实验对比了nGPT和FastConformer两种编码器，ASR分割时间戳采用NeMo Forced Aligner加辅助CTC模型。模型训练数据超过170万小时。

Result: Canary-1B-v2在英语ASR任务上表现优于Whisper-large-v3且推理速度快十倍，在多语言ASR及AST上与更大的Seamless-M4T-v2-large和LLM系统表现相当。同时发布了参数更小的Parakeet-TDT-0.6B-v3，依然支持25种语言。

Conclusion: Canary-1B-v2等系列模型证明了高效模型结构及数据处理策略能够在小体积下实现强劲的多语言ASR/AST性能，并推动更实用的语音系统发展。

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [118] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: 本文推出了CS-FLEURS数据集，覆盖了113组代码混合语言对，旨在促进代码混合语音识别与翻译研究，特别关注低资源语言。


<details>
  <summary>Details</summary>
Motivation: 现有的代码混合语音识别和翻译任务多集中于资源丰富的语言，而低资源语言被严重忽视。本文旨在解决代码混合研究中数据集覆盖面狭窄、低资源语言缺乏的问题。

Method: 作者构建了CS-FLEURS数据集，包含4个不同类型的测试集和一个训练集，涵盖了52种语言、113个代码混合语言对，利用真实语音、合成语音（包括生成式和拼接式TTS）生成数据。

Result: CS-FLEURS成功覆盖了以往未涉及的众多低资源语言和多样化的代码混合场景，为相关研究提供了大规模基础数据。

Conclusion: CS-FLEURS将推动代码混合语音识别与翻译领域在多语言、尤其是低资源语言上的发展，期望带动更多学术研究和应用探索。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [119] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出AssoCiAm基准，以更可靠地评估多模态大模型（MLLMs）的联想能力，避免现有方法中的歧义问题。


<details>
  <summary>Details</summary>
Motivation: 随着通用人工智能（AGI）研究的发展，创造力成为MLLMs需要具备的重要特质，而联想能力是创造力的基础。为实现对AGI创造力的有效评估，亟需对MLLMs的联想能力进行深入研究和客观评价。目前已有评测体系，但它们往往忽略了联想任务中固有的歧义性，导致评价结果缺乏可靠性。

Method: 作者将联想任务中的歧义性分为“内部歧义”和“外部歧义”两类，基于此设计了AssoCiAm基准。该基准借助混合计算方法，规避了歧义问题，有效提升了评估的一致性和准确性。随后，作者在多种MLLMs上进行了大量实验。

Result: 实验显示，MLLMs的认知能力与联想能力密切相关。当评估存在歧义时，模型表现更趋随机性。所提出的方法能带来更准确的评测效果，充分验证了其有效性。

Conclusion: AssoCiAm基准能够有效、可靠地评估MLLMs的联想能力，减少联想任务中歧义性的干扰，为AGI相关研究提供了更具说服力的评测工具。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [120] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本文提出了一种结合金融背景与行为金融学的端到端金融顾问数据生成与训练新框架，并通过微调Qwen-3-8B模型，实现了低成本、高性能的个性化财经建议系统。


<details>
  <summary>Details</summary>
Motivation: 目前个性化金融建议需要兼顾用户目标、约束、风险容忍度及法域，但现有LLM系统主要服务于投资者与理财师，且现有流水线维护成本高，实际效益低。因此作者希望开发一种更高效、可扩展的财经顾问系统。

Method: 作者提出结合相关金融语境与行为金融学方法构建监督数据的新框架，据此生成了19k推理样本数据集，并对Qwen-3-8B模型进行全面微调。模型评估依托划分出的测试集及盲测LLM评委。

Result: 该8B参数模型在事实准确性、流畅性及个性化指标上表现与大规模（14-32B参数）基线模型相当，但成本降低高达80%。

Conclusion: 得益于精心数据筛选和行为金融学集成，8B模型实现了与更大模型性能相当的个性化财经建议，成本大幅降低，为金融AI的可用性和落地提供了新思路。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [121] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文利用开放权重大模型，分析了英国议会75年来有关移民的讨论，并与美国国会的相应话语进行了对比，揭示了两国移民话语的演变及党派间立场变化。


<details>
  <summary>Details</summary>
Motivation: 当前关于移民的政治话语逐渐成为热点，但缺乏大规模、历史长时间跨度的数据分析来揭示不同国家及政党的话语演变和立场分化的趋势。该研究希望弥补这一空白，并评估大模型工具在话语分析中的应用潜力。

Method: 研究采用了开放权重的大语言模型（LLMs），为每条议会发言标注对移民的高层次立场，并统计不同时期及不同党派的整体态度变化。在英国案例中，还提出了半自动的细粒度叙事框架抽取方法，分析移民相关的新闻话语走向。

Result: 美国国会讨论中对移民话语日益两极化，而英国议会各党态度相对一致但工党和保守党之间存在长期的意识形态分歧，并在2025年达到最负面。英国关于移民治理的话语逐步从“社会融合”等方向转向“边境安全”“非法移民”等安全化框架，以及从国内法讨论转向国际法与人权。

Conclusion: 研究展示了LLMs在大规模、细粒度政治话语分析中的有效性，并揭示了英美两国移民话语的演变特征及其背后党派立场分化，为历史和政治语境下话语演变研究提供了新工具和新视角。

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [122] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的大语言模型套件，重点解决合规性和多语言能力，使用可公开数据训练，并完整开放开发全流程资源。


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型普遍存在数据合规性不足（如未遵守内容所有者权利、未复现数据管道等）和多语言代表性薄弱的问题。Apertus旨在通过合规数据和广泛的多语言覆盖来解决这些系统性短板。

Method: 模型仅基于完全开放的数据预训练，严格排除robots.txt禁止内容和敏感、有毒信息，并采用Goldfish预训练目标压制逐字复现以减轻记忆化风险。同时，Apertus在1800多种语言、15万亿tokens的数据上训练，其中40%为非英语数据，并开放8B与70B参数规模模型。

Result: Apertus在多语言基准测试中达到接近或超过其他开源同类模型的表现，实现了合规与性能的兼顾。

Conclusion: Apertus不仅在模型性能和合规性上树立新标杆，还完整开放了数据、脚本、代码和评测工具，促进了大语言模型研究的透明和可扩展性。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [123] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 本论文提出了一种基于强化学习的路径规划方法，用于实现超视距无人机（UAV）在蜂窝网络连接下的高效、安全飞行。


<details>
  <summary>Details</summary>
Motivation: 远程（BVLoS）无人机飞行在实际应用中对蜂窝通信连接提出了较高要求。当前路径规划方法难以兼顾路径长度与可靠通信，且缺乏对实际空域蜂窝覆盖约束的考量。因此，需要一种既能优化飞行距离，又能保证通信质量的新方法。

Method: 利用强化学习算法，将无人机与基站之间的通信质量作为奖励函数，引导智能体在考虑真实蜂窝空域覆盖和经验空中信道模型的基础上学习最优路径规划。方法可作为线下路径模块，与无人机地面控制系统集成。

Result: 仿真结果显示，该强化学习方法能够有效训练无人机选择可行路径，保持与地面基站的最优连接，实现安全、可靠的BVLoS飞行。

Conclusion: 该方法提升了蜂窝无人机路径规划的效率和安全性，适用于未来复杂远程无人机应用，并推动相关技术的发展。

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [124] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: 本文优化了基于深度神经网络的机器人视觉定位方法，通过改进损失函数提升了室内定位精度，并在实际机器人平台上实现实时导航。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的机器人定位容易受到感知混淆（aliasing）影响，且现有神经网络方案在精度和训练便捷性上存在权衡，因此需要改进模型以提升定位性能而不降低训练难度。

Method: 作者对现有的神经网络位置估计方法进行了改进，提出了一种新的损失函数，把位置误差与旋转误差结合，以提升对感知混淆的鲁棒性。另外作者利用摄影测量方式采集带位姿标签的数据，训练出适用于本地环境的模型，并将该模型集成进导航算法，在TurtleBot机器人上进行了实时实测。

Result: 改进后的模型在室内场景下提升了定位精度，定位和旋转的中位误差分别降低了9.64%和2.99%。在用摄影测量数据训练后，模型可实现0.11米和0.89度的误差。此外，该方法可快速采集数据，仅需330秒即可完成现场数据采集和模型训练。

Conclusion: 本文提出了一套从数据采集到模型训练再到机器人实际导航的完整解决方案，实现了任何室内场景下高精度、低成本的机器人定位导航，具有较强通用性和实用价值。

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [125] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: 本文提出使用联合嵌入预测架构(Point-JEPA)进行3D自监督预训练，以提升机械手抓取角度的预测准确性，尤其在标注数据稀缺时。实验显示该方法能有效降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 机械手抓取任务对抓取角度的精确预测依赖大量有标签数据，获取高质量标签昂贵且耗时。如何在标签有限的情况下提升预测准确度，是实际应用中的重要挑战。

Method: 作者利用从3D网格中提取的点云，应用在ShapeNet上预训练过的Point-JEPA编码器，再通过一个高效的多假设头（采用赢家通吃策略）进行角度预测，最后以top-logit得分评估性能。

Result: 在DLR-Hand II数据集任务上，Point-JEPA方法在标注极少的情况下，将均方根误差(RMSE)降低了最多26%，并在全监督情况下与传统方法表现持平。

Conclusion: JEPA风格的自监督预训练为提高抓取学习的数据效率提供了切实可行的新方案，在标签受限时优势明显。

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [126] [Using role-play and Hierarchical Task Analysis for designing human-robot interaction](https://arxiv.org/abs/2509.13378)
*Mattias Wingren,Sören Andersson,Sara Rosenberg,Malin Andtfolk,Susanne Hägglund,Prashani Jayasingha Arachchige,Linda Nyholm*

Main category: cs.RO

TL;DR: 本文介绍了两种在人机交互领域值得更多应用的方法：情景角色扮演和分层任务分析，并展示了它们在开发药房机器人助手过程中的优势。


<details>
  <summary>Details</summary>
Motivation: 目前在人机交互领域，这两种方法尚未得到充分利用。作者希望引入并验证它们在机器人开发，特别是社交型机器人的有效性。

Method: 在药房机器人应用的研发过程中，作者通过让药剂师进行角色扮演模拟顾客需求，以此收集实际的用户需求与行为模式。同时使用分层任务分析法梳理、建模和优化机器人所需执行的任务流程，确保机器人的行为设计更贴近真实应用。

Result: 角色扮演法有效提供了能灵活调整且可控的环境，帮助理解顾客需求，同时药剂师的行为也为机器人建模提供了范例。分层任务分析确保了行为建模的正确性，并通过共创设计加速了开发流程。

Conclusion: 角色扮演与分层任务分析在人机交互尤其是社会性机器人研发中优势明显。未来应聚焦于为社交机器人量身打造的专门任务分析方法开发。

Abstract: We present the use of two methods we believe warrant more use than they
currently have in the field of human-robot interaction: role-play and
Hierarchical Task Analysis. Some of its potential is showcased through our use
of them in an ongoing research project which entails developing a robot
application meant to assist at a community pharmacy. The two methods have
provided us with several advantages. The role-playing provided a controlled and
adjustable environment for understanding the customers' needs where pharmacists
could act as models for the robot's behavior; and the Hierarchical Task
Analysis ensured the behavior displayed was modelled correctly and aided
development through facilitating co-design. Future research could focus on
developing task analysis methods especially suited for social robot
interaction.

</details>


### [127] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: 本文介绍了ASTREA，这是第一个部署在飞行继承硬件（TRL 9）上的主体型系统，实现了航天器自治操作。作者将资源受限的大语言模型（LLM）与强化学习控制器结合，在航天级平台上进行异步架构集成。地面实验表明，LLM监督提升了热稳定性并减少违规。而空间站上的实际验证发现，由于推理延迟与轨道热循环不匹配，系统性能下降。该工作总结了LLM主体系统在实际空间环境中的机遇与局限性，并给出未来自治设计的建议。


<details>
  <summary>Details</summary>
Motivation: 传统航天器自治操作受限于资源、硬件能力以及控制策略的灵活性。本文旨在探索如何将前沿的人工智能方法（如大语言模型和强化学习）实际应用于有限资源、真实运行环境下的航天载体，提高航天任务的自治水平。

Method: 作者采用资源受限的大语言模型（LLM）作为智能代理，联合强化学习控制器，通过异步架构部署在空间认证硬件上。以热控为例，地面进行实验验证，并最终在国际空间站（ISS）上进行在轨测试。

Result: 地面实验中，LLM指导的监督控制显著提升了航天器热稳态能力并减少违规事件。但在ISS实测中，发现由于LLM推理延迟与低轨道卫星的快速热循环不匹配，导致性能下降。

Conclusion: 使用LLM和强化学习结合的主体系统在航天器自治上显示出潜力，但当前硬件与算法瓶颈（如推理延迟）限制了其实用性。研究为未来空间自主系统设计提供了实践建议和经验教训。

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [128] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 本文提出了一种层次化多智能体近端策略优化（H-MAPPO）新框架，有效提升多AUV协同任务下的隐蔽性与效率。


<details>
  <summary>Details</summary>
Motivation: 水下自主无人航行器（AUV）在协同侦查等任务中具有巨大潜力，但多AUV通信易暴露自身，如何兼顾协作效率与隐蔽性是关键挑战。

Method: 采用双时间尺度的层次化多智能体近端策略优化方法，高层决定参与任务的个体，低层通过功率和轨迹控制降低暴露概率。

Result: 仿真结果显示，所提方法收敛速度快，性能优于基准算法，能在保障隐蔽性的同时最大化长期协作效率。

Conclusion: 提出的H-MAPPO框架可为水下多AUV协同任务提供高效且隐蔽的解决方案，具备实际应用潜力。

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [129] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: 本论文提出了VEGA系统，一种基于AI的电动汽车充电感知路径优化导航方案，其无需额外传感器，仅用车速信号，实现了高效能耗管理和路径规划，并在多地区测试展现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义汽车需求增加和电动车计算能力提升，对于能够根据车辆状态智能规划充电和路径的导航系统需求日益增长，但现有方案往往需要额外硬件或泛化性不足。

Method: VEGA包括两个核心模块：一是物理信息神经算子（PINO），用车辆速度和电池功率日志训练，实时估算车辆动力学参数；二是强化学习（RL）代理，采纳PINO结果，在荷电状态（SoC）约束下优化路径、充电站停车点和时间，应用Proximal Policy Optimization（PPO）算法并结合A*引导。

Result: VEGA在如旧金山至纽约等实际长途路线测试中，路径规划、停车次数、停车时间、SoC管理及整体旅途时间均接近特斯拉Trip Planner，并在法国、日本等未训练地区展现良好泛化，表现略趋保守以适应车辆条件变动。

Conclusion: VEGA无需额外传感器、可用作虚拟能耗传感器，有望降低电动车成本。结合物理建模和强化学习的方法为电动汽车提供了实用的生态导航方案，且具有良好的跨区泛化能力。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [130] [A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies](https://arxiv.org/abs/2509.13434)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种用于模拟细丝与刚体之间接触相互作用的计算框架，兼具精确性与适应多场景应用的能力。


<details>
  <summary>Details</summary>
Motivation: 细丝由于是一维结构嵌在三维空间里，模拟上存在较大难度，现有方法通常假设细丝始终与刚体固定连接，难以真实模拟摩擦和复杂接触。

Method: 该框架融合了离散弹性杆（DER）建模、压力场补丁接触模型和凸接触公式，可以准确地模拟细长柔性物体与刚体之间的摩擦接触，并通过凸优化保证全局最优与接触互补关系。

Result: 经过实验验证，该方法能准确模拟摩擦力，物理真实性优于基线方法。

Conclusion: 该框架适用于软体机器人、可变形物体操作（如系鞋带等）等涉及细丝与刚体复杂接触的场景，具有通用性和高保真度。

Abstract: We present a computational framework for simulating filaments interacting
with rigid bodies through contact. Filaments are challenging to simulate due to
their codimensionality, i.e., they are one-dimensional structures embedded in
three-dimensional space. Existing methods often assume that filaments remain
permanently attached to rigid bodies. Our framework unifies discrete elastic
rod (DER) modeling, a pressure field patch contact model, and a convex contact
formulation to accurately simulate frictional interactions between slender
filaments and rigid bodies - capabilities not previously achievable. Owing to
the convex formulation of contact, each time step can be solved to global
optimality, guaranteeing complementarity between contact velocity and impulse.
We validate the framework by assessing the accuracy of frictional forces and
comparing its physical fidelity against baseline methods. Finally, we
demonstrate its applicability in both soft robotics, such as a stochastic
filament-based gripper, and deformable object manipulation, such as shoelace
tying, providing a versatile simulator for systems involving complex
filament-filament and filament-rigid body interactions.

</details>


### [131] [Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume](https://arxiv.org/abs/2509.13501)
*Hossein Gholampour,Logan E. Beaver*

Main category: cs.RO

TL;DR: 提出了一种针对可反馈线性化为二阶积分器（如机械臂）输出的机器人路径跟踪方法，能在遇到干扰时安全暂停并恢复，无需重新规划。


<details>
  <summary>Details</summary>
Motivation: 许多机器人系统在执行运动规划时，必须在遭遇人或物体干扰时能够安全地暂停并在消除干扰后继续运动，因此需要方法保证轨迹跟踪的安全性与鲁棒性。

Method: 方法包括两部分：离线阶段做可达性检测，确保运动轨迹符合速度和加速度限制；在线阶段采用二次规划诠释轨迹跟踪，同时有一步可达性检测评估系统最大可拒绝干扰。状态回到参考路径时可实现完美跟踪，并用KKT启发式权重修正误差。

Result: 实验表明，系统能有效处理安全停止和非预期偏离，并能无需重新规划回到原运动轨迹。仿真中优于pure pursuit算法。

Conclusion: 该方法提升了机械臂等系统在动态环境下的路径跟踪安全性和恢复能力，无需重复规划，具有较强实用性。

Abstract: Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.

</details>


### [132] [Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning](https://arxiv.org/abs/2509.13534)
*Chunxin Zheng,Kai Chen,Zhihai Bi,Yulin Li,Liang Pan,Jinni Zhou,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种结合人类动作先验与神经符号距离场（NSDF）的强化学习框架，使仿人机器人能够稳健地进行多接触、全身抱持操控，显著提升其适应多样物体、承载能力及现实应用表现。


<details>
  <summary>Details</summary>
Motivation: 传统仿人机器人以末端执行器抓取为主，但面对需要全身抱持大体积物体的场景时，稳定性和载重能力有限。因而，提升仿人机器人在复杂、实际环境下的整体操控能力成为亟需解决的问题。

Method: 作者设计了一个强化学习框架，通过预训练的人类运动先验与 NSDF 表示来指导全身动作生成。利用师生架构蒸馏大量人类运动数据，使机器人动作既自然又物理可行。NSDF 提供连续的几何感知，增强多接触和长期任务时的碰触感知，提升稳定性和载重能力。

Result: 系统在仿真和真实场景中被全面验证，展现了对各种不同形状和尺寸物体的适应性，并实现了仿真到真实环境的成功迁移。机器人在长期和复杂多接触操控任务中表现出更好的鲁棒性与实用性。

Conclusion: 该强化学习框架显著提升了仿人机器人在全身多接触操控中的实用效果，为实现复杂、长期、多物体操控任务提供了有效且具备落地能力的解决方案。

Abstract: Whole-body manipulation (WBM) for humanoid robots presents a promising
approach for executing embracing tasks involving bulky objects, where
traditional grasping relying on end-effectors only remains limited in such
scenarios due to inherent stability and payload constraints. This paper
introduces a reinforcement learning framework that integrates a pre-trained
human motion prior with a neural signed distance field (NSDF) representation to
achieve robust whole-body embracing. Our method leverages a teacher-student
architecture to distill large-scale human motion data, generating kinematically
natural and physically feasible whole-body motion patterns. This facilitates
coordinated control across the arms and torso, enabling stable multi-contact
interactions that enhance the robustness in manipulation and also the load
capacity. The embedded NSDF further provides accurate and continuous geometric
perception, improving contact awareness throughout long-horizon tasks. We
thoroughly evaluate the approach through comprehensive simulations and
real-world experiments. The results demonstrate improved adaptability to
diverse shapes and sizes of objects and also successful sim-to-real transfer.
These indicate that the proposed framework offers an effective and practical
solution for multi-contact and long-horizon WBM tasks of humanoid robots.

</details>


### [133] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 本论文提出了一种基于单目内窥镜视频、结合DROID-SLAM和语义分割模型的实时中心气道三维重建及标注方法，用于辅助中央气道阻塞（CAO）的机器人干预和自动化。该方法可实现高精度、实时的三维重建，对手术场景反映更为准确。


<details>
  <summary>Details</summary>
Motivation: 中央气道阻塞（CAO）是一种威胁生命的疾病，传统气道内镜或电烙等治疗手段并发症高。机器人与自动化技术可降低风险。然而，缺乏能实时且语义感知的三维气道场景理解方法。

Method: 采用DROID-SLAM实现实时三维重建，同时引入训练好的分割模型实时检测内窥镜视频中的堵塞组织，将分割掩码直接整合进SLAM流程，对气道三维点云进行语义标注。通过ex vivo模型验证，与CT真值对比进行定量和定性评估。

Result: 实验显示三维重建与CT真值高度相似（Chamfer距离0.62mm），能实时生成带有障碍区标注的三维气道地图，重建速度较以往方法更快，更能准确反映手术场景。

Conclusion: 首次在CAO内镜场景下，将语义分割与实时单目SLAM结合，实现气道障碍区的高效三维标注与重建，具备高泛化性，对推动自动化和机器人手术有积极意义。

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [134] [Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference](https://arxiv.org/abs/2509.13572)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.RO

TL;DR: 本研究评估了视觉语言模型（VLMs）在提升半自动假肢手感知能力方面的潜力，并建立了一个端到端的统一基准，对VLM自动完成目标识别和抓取推理的能力进行了系统测试。


<details>
  <summary>Details</summary>
Motivation: 传统假肢控制系统依赖复杂的多模块流水线（如目标检测、姿态估计和抓取规划），流程繁琐，响应不及时，难以实现高水平的自动化与智能化。作者希望通过统一的VLM模型简化流程，并探索其作为下一代感知模块的可行性和局限。

Method: 作者选取了8种主流VLM模型，设计统一任务：模型需从单张静态图片识别物体及其属性，并直接推断假肢手需要的抓取参数。所有输入以结构化JSON格式输出，并在34种常见物体快照数据集上进行实验，评估对象属性的准确率、数值推断误差、时延及成本等指标。

Result: 大部分VLM在物体识别、形状判别等类别属性上表现出较高性能，但在尺寸估算、最优抓取参数（如手腕旋转、手指开合等）推断上表现有波动，误差较大。各模型在处理细致抓取参数时表现出不一致性。

Conclusion: 当前VLM已具备作为半自动假肢感知模块的潜力，能胜任物体识别等任务，但在精确抓取推理方面仍有限制。该研究为后续提升VLM在假肢应用中的效果及拓展高功能假肢控制系统提供了基础，并指出了亟待突破的关键环节。

Abstract: This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.

</details>


### [135] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: 本文分析了flow matching在机器人生成策略中的泛化瓶颈，并提出通过训练时使用非均匀时间调度和推断时采用dense-jump积分策略，有效提升了策略效果。


<details>
  <summary>Details</summary>
Motivation: flow matching已成为高质量机器人生成策略的主流方法，但当前方法在flow trajectory上泛化能力提升有限，并且更细致的积分步骤非但无益反而降低性能，急需新的训练和推断策略提升泛化表现。

Method: 提出两项新策略：1）训练阶段使用非均匀（如U形）时间调度，更关注开头和结尾阶段，增强对多时段的泛化；2）推断阶段创新性地采用dense-jump积分，即在跳点后用一步积分代替多步积分，绕开积分不稳定区域。

Result: 该方法在多种机器人任务上，较当前最优基线性能提升最高达23.7%。

Conclusion: 通过时间调度优化和积分策略创新，显著克服了现有flow matching的泛化瓶颈，简单高效地提升了机器人生成策略性能。

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [136] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: 提出了一种新型自动驾驶规划器TreeIRL，将蒙特卡洛树搜索（MCTS）与逆向强化学习（IRL）相结合，在仿真与真实道路测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶规划方法很难在安全性、进度、舒适性和仿人性之间取得良好平衡，而且对规划器的评估通常局限于仿真，缺少真实道路下的全面测试。

Method: TreeIRL利用MCTS生成一组安全的候选轨迹，然后通过深度IRL评分函数从中筛选出最具人类驾驶风格的轨迹。通过大规模仿真和在拉斯维加斯500多英里真实道路上测试，并与传统及最新规划器进行对比。

Result: TreeIRL在密集城市交通、自适应巡航、加塞、红绿灯等多种测试场景下展现出最佳综合表现，在安全、进度、舒适性和仿人性之间实现优异平衡，超越其他方法。

Conclusion: 首次实现在公共道路上基于MCTS的规划，强调了多维度和真实环境下评估自动驾驶规划器的重要性。TreeIRL具有高度可扩展性，有望结合强化学习和模仿学习进一步提升，能够探索经典方法与学习方法的不同组合，以解决自动驾驶规划的瓶颈。

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [137] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出一种机器人双手协作，通过主动感知和探索，利用触觉数据实现鲁棒的物体位姿估计，无需已知物体几何信息。方法用强化学习引导，实验展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 物体位姿估计是机器人操作和交互的关键，但视觉信息常受限于光照、遮挡和外观差异，触觉传感器因接触有限，难以重建完整位姿，因而需要更有效的估计方法。

Method: 提出传感-动作一体化探索方法。用一只机械手固定物体，另一只手主动探索。强化学习训练机器人搜集触觉3D点云数据，通过迭代优化获得物体形状与位姿。

Result: 实验表明，该方法能有效主动探索物体表面，找出关键姿态特征，在不依赖物体先验几何知识情况下实现准确估计。相关演示与补充材料在所给网址提供。

Conclusion: 通过结合主动探索与强化学习，机器人能凭触觉数据高效、鲁棒地估计未知物体的位姿，为复杂环境下的机器人操作提供了新思路。

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [138] [Leg-Arm Coordinated Operation for Curtain Wall Installation](https://arxiv.org/abs/2509.13595)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 本论文提出了一种基于六足机器人的幕墙安装方案，并设计了分层优化的全身控制框架，实现了机械臂与机械腿的协调工作，显著提升了施工效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，高层建筑、公共设施增多，幕墙成为现代建筑的重要组成部分。但传统幕墙安装方法受场地、劳动强度、效率和安全风险等多因素困扰，尤其是大型面板安装难度大、成本高，亟需更高效智能的自动化解决方案。

Method: 作者基于六足幕墙安装机器人，提出了分层优化的全身控制框架，实现了机械腿移动与折叠机械臂、串-并联操作器的动作协调，涵盖幕墙、天花板和地板铺设三类核心任务，通过该框架协同调度各运动单元。

Result: 通过实验证明，该分层优化的机械臂-腿协调控制方法使六足机器人能够成功完成幕墙安装任务，有效提升了施工现场的适应性与作业能力。

Conclusion: 本文提出的分层优化协调控制框架验证了其在六足机器人建筑幕墙安装中的有效性，为机器人在复杂施工环境中的进一步应用奠定了技术基础。

Abstract: With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.

</details>


### [139] [Barometer-Aided Attitude Estimation](https://arxiv.org/abs/2509.13649)
*Méloné Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: cs.RO

TL;DR: 该论文提出了一种利用气压计辅助的姿态估计算法，通过结合非线性观测器与互补滤波器，提高无人自主系统在缺乏GNSS等辅助传感器时的姿态估计精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在高动态或GNSS信号不可用的环境下，IMU难以独立实现准确的倾斜和姿态估计，因此需要探索替代或补充的传感器观测手段，特别是在低成本和低复杂度要求下。

Method: 作者设计了一种融合气压计高度信息的姿态估计架构。该方法在SO(3)流形上构建非线性观测器，将确定性Riccati观测器与互补滤波器级联，利用气压计推算垂直速度，辅助IMU提升姿态估计性能，并给出了几何一致性与全局渐近稳定性的理论保证。

Result: 分析结果证明，气压计辅助的姿态估计方法能够在保持轻量级硬件需求的同时，实现几乎全局的渐近稳定性，对多种动态环境具备良好的跟踪和估计性能。

Conclusion: 气压计能够作为IMU的有力补充，在GNSS等速度观测不可用或紧张场景下，显著增强无人系统姿态估计的鲁棒性和可信度。

Abstract: Accurate and robust attitude estimation is a central challenge for autonomous
vehicles operating in GNSS-denied or highly dynamic environments. In such
cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable
tilt estimation due to the ambiguity between gravitational and inertial
accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,
Doppler radar, or visual odometry, are often used, they can be unavailable,
intermittent, or costly. This work introduces a barometer-aided attitude
estimation architecture that leverages barometric altitude measurements to
infer vertical velocity and attitude within a nonlinear observer on SO(3). The
design cascades a deterministic Riccati observer with a complementary filter,
ensuring Almost Global Asymptotic Stability (AGAS) under a uniform
observability condition while maintaining geometric consistency. The analysis
highlights barometer-aided estimation as a lightweight and effective
complementary modality.

</details>


### [140] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: 提出了一种名为DREAM的视觉语言模型（VLM）驱动的水下自主机器人系统，可高效执行长期水下生态监测任务，显著提升目标发现效率，降低探索时间和步骤。


<details>
  <summary>Details</summary>
Motivation: 全球海洋变暖和酸化对温敏贝类（如牡蛎）造成大规模死亡风险，因此亟需开展长时段、广区域的底栖生物监测。但人工监测成本高且水下作业危险，推进智能化自主机器人替代人工成为趋势。

Method: 本论文提出了DREAM框架，将视觉语言模型（VLM）引入水下自主机器人的决策过程，实现实时感知环境并自主选择探索目标，无需人工干预。通过该功能，机器人能持续高效地在海底区域监测特定目标（如牡蛎、沉船）。

Result: 在牡蛎监测任务中，相较于现有方法，DREAM框架实现了减少31.5%的搜索时间、减少23%的探索步骤，并覆盖了8.88%更多的牡蛎。在沉船场景探索中，无碰撞完成地图构建，步数减少27.5%，目标覆盖率由60.23%提升至100%。

Conclusion: DREAM框架显著提升了水下自主机器人的任务效率和安全性，可节省人力成本并扩大监测范围，为长期、低成本、广域海底环境监测提供了可行技术路径。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [141] [SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics](https://arxiv.org/abs/2509.13691)
*Songhao Huang,Yuwei Wu,Guangyao Shi,Gaurav S. Sukhatme,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文提出SPAR框架，利用大语言模型（LLM）自动将自然语言输入转化为有效、多样且语义准确的PDDL规划域，重点针对无人机（UAV）任务。


<details>
  <summary>Details</summary>
Motivation: 当前PDDL虽然是机器人规划中的标准，但手动设计和编码PDDL域既繁琐又易出错，极大限制了其在无人机巡逻、递送、检查等实际应用中的推广。需要一种自动化、高效且通用的PDDL域生成方法。

Method: 作者构建并验证了一个UAV领域的PDDL基准数据集，包含了真实PDDL域及相关问题，以及详细的域和动作描述。在此基础上，设计了基于提示的LLM生成方法，实现从自然语言到高质量PDDL域的自动转化，并通过语法、可执行性、可行性和可解释性多维度评估生成结果。

Result: LLM在PDDL域的生成方面表现优秀，能够大幅提升复杂规划域开发效率。生成的数据集和评测流程可复现，支持无需PDDL经验的领域专家实际应用和后续研究。

Conclusion: 利用LLM自动生成PDDL域可显著降低人工工作量，提升自动化规划在空中机器人等实际任务中的应用效率，对推动智能规划领域具有重要价值。

Abstract: We investigate the problem of automatic domain generation for the Planning
Domain Definition Language (PDDL) using Large Language Models (LLMs), with a
particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a
widely adopted standard in robotic planning, manually designing domains for
diverse applications such as surveillance, delivery, and inspection is
labor-intensive and error-prone, which hinders adoption and real-world
deployment. To address these challenges, we propose SPAR, a framework that
leverages the generative capabilities of LLMs to automatically produce valid,
diverse, and semantically accurate PDDL domains from natural language input. To
this end, we first introduce a systematically formulated and validated UAV
planning dataset, consisting of ground-truth PDDL domains and associated
problems, each paired with detailed domain and action descriptions. Building on
this dataset, we design a prompting framework that generates high-quality PDDL
domains from language input. The generated domains are evaluated through syntax
validation, executability, feasibility, and interpretability. Overall, this
work demonstrates that LLMs can substantially accelerate the creation of
complex planning domains, providing a reproducible dataset and evaluation
pipeline that enables application experts without prior experience to leverage
it for practical tasks and advance future research in aerial robotics and
automated planning.

</details>


### [142] [HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion](https://arxiv.org/abs/2509.13692)
*Yadan Zeng,Jiadong Zhou,Xiaohan Li,I-Ming Chen*

Main category: cs.RO

TL;DR: HGACNet提出了一种融合RGB图像和三维点云特征的层次化图注意力网络，实现了高精度点云补全，并在多项数据集与真实机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于自遮挡和传感器局限导致的点云不完整，严重影响了机器人感知与操作任务的效果，因此亟需一种精准、鲁棒的点云补全方法。

Method: 提出HGACNet框架，包括层次化图注意力（HGA）编码器，通过图注意力自适应选点并多层提取几何特征；利用多尺度跨模态融合（MSCF）模块融合点云与RGB视觉特征；引入对比损失（C-Loss）提升不同模态间特征一致性和补全质量。

Result: 在ShapeNet-ViPC和YCB-Complete等公开数据集实验中，HGACNet达到了目前最优的点云补全性能，并在真实机器人抓取等任务中表现出良好的通用性和实用价值。

Conclusion: HGACNet能够有效结合三维结构与图像信息，为单视角点云补全任务提供了新的解决方案，并为实际机器人感知和操作带来了实际应用价值。

Abstract: Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.

</details>


### [143] [EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility](https://arxiv.org/abs/2509.13720)
*Tianle Zeng,Jianwei Peng,Hanjing Ye,Guangcheng Chen,Senzi Luo,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种面向大规模室外环境的零样本物体导航（ZSON）系统，能够在遇到远距离、部分或完全遮挡等复杂情况下，稳定高效地找到目标。


<details>
  <summary>Details</summary>
Motivation: 在开阔室外环境中，零样本导航面临目标遥远导致图像内部目标微小、以及目标被遮挡导致间歇可见性的挑战。现有方法在这些情况下通常效率低或不健壮，需要新的解决方案。

Method: 提出一种对齐的多尺度图像分层结构，通过层级化目标显著性融合，将语义对比信息整合为大尺度区域显著性。系统利用区域显著性指示目标方向及可见性，实现了基于显著性的航向保持、关键帧记忆、历史航向融合和主动搜索机制。系统避免了全图缩放，保证效率且可零样本迁移。

Result: 在模拟和真实场景测试中，该系统可以检测150米之外的语义目标，在目标可见性变化下以82.6%的概率保持正确航向，整体任务成功率比最新方法提升17.5%。

Conclusion: 本系统在远距离、间歇可见目标的导航任务上表现鲁棒和高效，推动了室外零样本物体导航的发展。

Abstract: Zero-shot object navigation (ZSON) in large-scale outdoor environments faces
many challenges; we specifically address a coupled one: long-range targets that
reduce to tiny projections and intermittent visibility due to partial or
complete occlusion. We present a unified, lightweight closed-loop system built
on an aligned multi-scale image tile hierarchy. Through hierarchical
target-saliency fusion, it summarizes localized semantic contrast into a stable
coarse-layer regional saliency that provides the target direction and indicates
target visibility. This regional saliency supports visibility-aware heading
maintenance through keyframe memory, saliency-weighted fusion of historical
headings, and active search during temporary invisibility. The system avoids
whole-image rescaling, enables deterministic bottom-up aggregation, supports
zero-shot navigation, and runs efficiently on a mobile robot. Across simulation
and real-world outdoor trials, the system detects semantic targets beyond 150m,
maintains a correct heading through visibility changes with 82.6% probability,
and improves overall task success by 17.5% compared with the SOTA methods,
demonstrating robust ZSON toward distant and intermittently observable targets.

</details>


### [144] [Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings](https://arxiv.org/abs/2509.13731)
*Jeongwoo Park,Seabin Lee,Changmin Park,Wonjong Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出了一种基于基础模型的RL算法，实现了在仿真环境中训练工业插拔柔性扁平电缆（FFC）并成功迁移到真实环境，避免了物理损坏和人工干预。


<details>
  <summary>Details</summary>
Motivation: 工业柔性扁平电缆插拔需要高精度，且电缆易变形，人工引导轨迹生成耗时。直接用RL在真实环境训练存在危险，因此亟需安全、高效、自动化的插拔方法。

Method: 提出了一种基于仿真到现实迁移的RL方法，全程在仿真环境训练。利用Segment Anything Model 2（SAM2）提取插拔相关的语义分割掩膜，通过视觉-语言模型（VLM）自动化分割提示，消除人工干预。实现过程中仅关注几何和空间信息，并采用zero-shot策略，无需针对特定环境微调。

Result: 实验结果表明，该方法具备zero-shot能力，即可直接在真实工业环境部署，实现仿真到现实的无缝迁移，无需额外训练，插拔效果良好。

Conclusion: 所提方法有效解决了FFC插拔自动化中的准确性和安全性问题，通过基础模型和自动化分割实现了无人工干预和高泛化能力，具有实际工业应用前景。

Abstract: The industrial insertion of flexible flat cables (FFCs) into receptacles
presents a significant challenge owing to the need for submillimeter precision
when handling the deformable cables. In manufacturing processes, FFC insertion
with robotic manipulators often requires laborious human-guided trajectory
generation. While Reinforcement Learning (RL) offers a solution to automate
this task without modeling complex properties of FFCs, the nondeterminism
caused by the deformability of FFCs requires significant efforts and time on
training. Moreover, training directly in a real environment is dangerous as
industrial robots move fast and possess no safety measure. We propose an RL
algorithm for FFC insertion that leverages a foundation model-based real-to-sim
approach to reduce the training time and eliminate the risk of physical damages
to robots and surroundings. Training is done entirely in simulation, allowing
for random exploration without the risk of physical damages. Sim-to-real
transfer is achieved through semantic segmentation masks which leave only those
visual features relevant to the insertion tasks such as the geometric and
spatial information of the cables and receptacles. To enhance generality, we
use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human
intervention, we employ a Vision-Language Model (VLM) to automate the initial
prompting of SAM2 to find segmentation masks. In the experiments, our method
exhibits zero-shot capabilities, which enable direct deployments to real
environments without fine-tuning.

</details>


### [145] [FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/abs/2509.13733)
*Xiaolin Zhou,Tingyang Xiao,Liu Liu,Yucheng Wang,Maiyue Chen,Xinrui Meng,Xinjie Wang,Wei Feng,Wei Sui,Zhizhong Su*

Main category: cs.RO

TL;DR: FSR-VLN是一种新提出的视觉语言导航系统，通过结合分层多模态场景图和快慢联动导航推理机制，实现了更高效和准确的导航，在多个室内机器人导航数据集上达到了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航系统在长距离空间推理能力上有限，表现为成功率低、推理速度慢，特别是在长距离导航任务中。这限制了机器人系统在现实环境中的部署。在此背景下，需要一种既高效又精确的导航策略来满足多样化的应用需求。

Method: FSR-VLN系统结合了分层多模态场景图（HMSG），用于支持从粗粒度到细粒度的多层次场景检索导航，并提出了快慢联动导航推理（FSR）机制：先用快速匹配高效筛选候选房间、视角和目标，再用视觉语言模型（VLM）细化和甄别最终目标。当快速推理无法确定目标时，才触发慢速推理以保证准确性。此外，将该系统整合在Unitree-G1类人机器人上，结合语音交互、规划与控制模块，支持自然语言的实时导航。

Result: 在四个由类人机器人采集的室内数据集上进行评测，覆盖87种多样物体指令，FSR-VLN在所有数据集上检索成功率（RSR）达到目前最优水平，并相比纯VLM方法，响应时间降低了82%。系统还实现了自然语言交互和实时导航。

Conclusion: FSR-VLN显著提升了视觉语言导航在效率和精度上的表现，结合快慢推理与多模态场景图，兼顾了快速响应和高准确率，极大提升了机器人系统的实际应用能力。

Abstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.

</details>


### [146] [Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning](https://arxiv.org/abs/2509.13736)
*Muyuan Ma,Long Cheng,Lijun Han,Xiuze Xia,Houcheng Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于元模仿学习（meta-imitation learning）的个性化且可泛化的可穿戴外骨骼辅助算法，在多个任务和新用户场景下显著减少肌肉激活和代谢消耗。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时实现个性化与对不同任务具有良好泛化能力的外骨骼辅助算法一直是难题，目前很多外骨骼算法只能适应有限任务或特定用户，难以快速适配新任务和新用户。

Method: 提出了一种元模仿学习方法，利用任务特定的神经网络预测人体肘关节运动。通过从公开RGB视频和动作捕捉数据集提取全身关键点动作并模拟重定向，加速了数据收集。随后，利用模型无关元学习（MAML）框架训练神经网络，使其能在新任务和新用户上快速适应，再将输出轨迹用PD控制器跟踪实施辅助。

Result: 实验结果显示，所提系统能在新用户完成未训练任务时，有效降低肌肉激活和代谢成本，优于未佩戴外骨骼的情况。

Conclusion: 该元模仿学习框架能提升外骨骼系统的泛化能力和适应新用户的能力，对实现个性化、多任务辅助具有重要意义。

Abstract: Wearable exoskeletons can augment human strength and reduce muscle fatigue
during specific tasks. However, developing personalized and task-generalizable
assistance algorithms remains a critical challenge. To address this, a
meta-imitation learning approach is proposed. This approach leverages a
task-specific neural network to predict human elbow joint movements, enabling
effective assistance while enhancing generalization to new scenarios. To
accelerate data collection, full-body keypoint motions are extracted from
publicly available RGB video and motion-capture datasets across multiple tasks,
and subsequently retargeted in simulation. Elbow flexion trajectories generated
in simulation are then used to train the task-specific neural network within
the model-agnostic meta-learning (MAML) framework, which allows the network to
rapidly adapt to novel tasks and unseen users with only a few gradient updates.
The adapted network outputs personalized references tracked by a
gravity-compensated PD controller to ensure stable assistance. Experimental
results demonstrate that the exoskeleton significantly reduces both muscle
activation and metabolic cost for new users performing untrained tasks,
compared to performing without exoskeleton assistance. These findings suggest
that the proposed framework effectively improves task generalization and user
adaptability for wearable exoskeleton systems.

</details>


### [147] [Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control](https://arxiv.org/abs/2509.13737)
*Renjie Wang,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出了一种将腿部控制分为支撑腿和摆动腿的新型解耦框架，用于提升四足机器人应对新环境（如外部扰动、不同地形等）的适应能力和鲁棒性。实验验证其优越的仿真转实（sim-to-real）表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在腿足机器人控制中面临着仿真到现实（sim-to-real）落差、外部分布条件（OOD）下性能下降等挑战。以往多依赖域随机化来提升鲁棒性，但难以应对全部实际情景，存在局限性。

Method: 本文提出了一种解耦的控制框架，将腿足机器人的支撑腿（stance-leg）控制和摆动腿（swing-leg）控制分开，使控制策略可以分别适应不同任务需求。旨在提高系统在未知环境中的实时自适应能力，并减弱sim-to-real问题。

Result: 多项仿真及真实环境实验表明，该框架能够有效应对水平外力扰动、不平地形、重载和偏载等典型挑战，并显著减小仿真到现实的性能差距。

Conclusion: 解耦式腿部控制方法比单一端到端域随机化更具鲁棒性和适应性，为提升四足机器人实际环境下的运动控制性能提供了新思路。

Abstract: While Reinforcement Learning (RL) has achieved remarkable progress in legged
locomotion control, it often suffers from performance degradation in
out-of-distribution (OOD) conditions and discrepancies between the simulation
and the real environments. Instead of mainly relying on domain randomization
(DR) to best cover the real environments and thereby close the sim-to-real gap
and enhance robustness, this work proposes an emerging decoupled framework that
acquires fast online adaptation ability and mitigates the sim-to-real problems
in unfamiliar environments by isolating stance-leg control and swing-leg
control. Various simulation and real-world experiments demonstrate its
effectiveness against horizontal force disturbances, uneven terrains, heavy and
biased payloads, and sim-to-real gap.

</details>


### [148] [CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs](https://arxiv.org/abs/2509.13771)
*Mengzhu Li,Yunyu Zhou,He Ying,F. Richard Yu*

Main category: cs.RO

TL;DR: 本文提出了一种新方法——CDFlow，利用Neural ODEs在高自由度机器人运动规划中，学习配置空间下碰撞最近点的分布，有效克服传统方法在高维空间下的不足，并显著提升了规划性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于配置空间距离场（CDF）的方法在处理高自由度机器人时，主要存在两个问题：一是只返回最近的碰撞点，忽略了最小距离碰撞配置的多模态特性，导致梯度不明确；二是依赖稀疏采样，常找不到真正最近的点，进而在高维空间内产生过度平滑与几何扭曲。为此，作者希望提出更加全面、精确且适合高维空间的解决方案。

Method: 作者提出了CDFlow框架，利用神经常微分方程（Neural ODEs）在配置空间中建模最近碰撞配置的分布，而不仅仅是单个最邻近点。同时引入自适应细化采样策略，以生成高质量训练数据，从而更准确建模多模态分布。该方法输出平滑、一致的梯度场，能合理引导路径优化，并保留清晰的几何特征。

Result: CDFlow在多个高自由度机器人运动规划任务上的实验表明，相较于传统CDF方法，其路径规划效率、轨迹质量和鲁棒性均大幅提升。实验涵盖复杂环境下的碰撞规避，CDFlow展现了更高的健壮性和实用性。

Conclusion: 作者的方法克服了传统CDF在高维空间中只针对单点、过度平滑等缺陷，能够为复杂环境下的高自由度机器人提供更高效、更健壮的碰撞感知规划支持，具有较强的推广潜力。

Abstract: Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.

</details>


### [149] [Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](https://arxiv.org/abs/2509.13774)
*Piaopiao Jin,Qi Wang,Guokang Sun,Ziwen Cai,Pinjia He,Yangwei You*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习（RL）的人类参与双actor微调框架，用于提升视觉-语言-动作（VLA）模型在机器人操作任务中的泛化能力，尤其在真实世界多任务与长时序任务中获得显著效果。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型在简单环境下展现出较好泛化，但面对复杂真实任务时依赖高质量监督数据，存在性能瓶颈。强化学习能减轻这一依赖，同时结合人类反馈有望进一步提升表现。

Method: 提出基于RL的双actor微调框架：主actor负责多任务稳健性，精炼actor用于潜在空间的自适应。人类通过物理干预和轻量级语言反馈（talk-and-tweak），将修正转化为新策略的数据，从而丰富训练样本并提升模型能力。

Result: 在真实多任务机器人实验中，框架实现三项任务101分钟内100%成功率；长时任务12连次操作成功率50%。多机器人并行训练时，效率提升可达2倍。

Conclusion: 该方法充分结合RL与人类语言反馈，实现多任务机器人操作的高效适应与泛化，为实际部署提供了具有可行性与可扩展性的解决方案。

Abstract: Vision-language-action (VLA) models demonstrate strong generalization in
robotic manipulation but face challenges in complex, real-world tasks. While
supervised fine-tuning with demonstrations is constrained by data quality,
reinforcement learning (RL) offers a promising alternative. We propose a
human-in-the-loop dual-actor fine-tuning framework grounded in RL. The
framework integrates a primary actor for robust multi-task performance with a
refinement actor for latent-space adaptation. Beyond standard physical
interventions, we introduce a lightweight talk-and-tweak scheme that converts
human corrections into semantically grounded language commands, thereby
generating a new dataset for policy learning. In real-world multi-task
experiments, our approach achieves 100% success across three tasks within 101
minutes of online fine-tuning. For long-horizon tasks, it sustains a 50%
success rate over 12 consecutive operations. Furthermore, the framework scales
effectively to multi-robot training, achieving up to a 2 times improvement in
efficiency when using dual robots. The experiment videos are available at
https://sites.google.com/view/hil-daft/.

</details>


### [150] [Behavior Foundation Model for Humanoid Robots](https://arxiv.org/abs/2509.13780)
*Weishuai Zeng,Shunlin Lu,Kangning Yin,Xiaojie Niu,Minyue Dai,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出了一种名为行为基础模型（BFM）的生成式模型，用于提高仿人机器人全身控制（WBC）的通用性和适应性。该模型通过大规模行为数据预训练，实现跨任务通用和快速适应新行为，优于传统依赖奖赏设计的WBC方法。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人全身控制方法往往依赖繁琐的人为奖励设计，并且泛化能力有限，难以灵活应对多种控制任务和复杂的现实应用场景。

Method: 作者提出了行为基础模型（BFM），结合了掩码式在线蒸馏框架和条件变分自编码器（CVAE），用大规模行为数据预训练，学习可复用的泛化行为分布，不用每次都从头训练。

Result: 在仿真和实体仿人机器人上的大量实验表明，BFM能够在多种WBC任务间实现鲁棒泛化，并能快速适应新行为。

Conclusion: BFM为构建通用型仿人机器人控制基础模型提供了有前景的技术路径，有望推动机器人在复杂现实场景中的更广泛应用。

Abstract: Whole-body control (WBC) of humanoid robots has witnessed remarkable progress
in skill versatility, enabling a wide range of applications such as locomotion,
teleoperation, and motion tracking. Despite these achievements, existing WBC
frameworks remain largely task-specific, relying heavily on labor-intensive
reward engineering and demonstrating limited generalization across tasks and
skills. These limitations hinder their response to arbitrary control modes and
restrict their deployment in complex, real-world scenarios. To address these
challenges, we revisit existing WBC systems and identify a shared objective
across diverse tasks: the generation of appropriate behaviors that guide the
robot toward desired goal states. Building on this insight, we propose the
Behavior Foundation Model (BFM), a generative model pretrained on large-scale
behavioral datasets to capture broad, reusable behavioral knowledge for
humanoid robots. BFM integrates a masked online distillation framework with a
Conditional Variational Autoencoder (CVAE) to model behavioral distributions,
thereby enabling flexible operation across diverse control modes and efficient
acquisition of novel behaviors without retraining from scratch. Extensive
experiments in both simulation and on a physical humanoid platform demonstrate
that BFM generalizes robustly across diverse WBC tasks while rapidly adapting
to new behaviors. These results establish BFM as a promising step toward a
foundation model for general-purpose humanoid control.

</details>


### [151] [Shell-Type Soft Jig for Holding Objects during Disassembly](https://arxiv.org/abs/2509.13802)
*Takuya Kiyokawa,Ryunosuke Takebayashi,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人拆解的壳体型柔性夹具（软夹具），可通用且安全地固定不同形状的物体，降低专属夹具设计和高精度控制的需求。通过实验验证了其实用性和性能优劣。


<details>
  <summary>Details</summary>
Motivation: 传统夹具需专门设计，且对识别与控制精度要求高，在多样化物体拆解时效率低并有潜在损坏风险。本研究旨在设计一种柔性、通用且安全的夹具方案。

Method: 提出一种基于气囊的壳体型软夹具，通过气囊膨胀包覆方式使夹具适应不同形状和尺寸的目标，实现稳固但柔和的固定。通过与传统虎钳和灵感自果酱夹具的对比实验，测试性能和适用性。

Result: 实验表明，该夹具对多种物体具有较好的固定效果，相较于传统夹具减少了对精确感知和控制的依赖，也揭示了夹具在某些情况下的局限性。

Conclusion: 该新型柔性夹具在机器人领域具备较强的实际应用前景，特别适合不规则及易损物体的柔性拆解，但依然存在应用范围和性能上的优化空间。

Abstract: This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.

</details>


### [152] [Soft Regrasping Tool Inspired by Jamming Gripper](https://arxiv.org/abs/2509.13815)
*Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种基于夹持致密化原理的软夹具，用于提高机器人装配过程中的重抓取精度。相比传统刚性夹具，软夹具具有更强的通用性和适应性。通过对十种不同形状机械零件的实验，绝大多数情况下的放置成功率超过80%，圆柱体零件成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 传统刚性夹具适应性差，每种零件都需要专门设计，难以应对多样化机械零件自动装配带来的姿态不确定性。需要一种能灵活应对不同零件形状的新型夹具，以减少误差并简化自动装配流程。

Method: 作者受致密化原理启发，提出了软夹具方案，利用可变形的膜结构，通过三棱锥形工具压入并抽气，形成与零件匹配且具有稳定性的腔体。对压印深度进行优化，以兼顾零件的稳定摆放与抓取器的可接近性。

Result: 对十种不同形状机械零件进行实验，多数零件摆放成功率超过80%，其中圆柱体类零件达到90%以上。失败主要由于几何约束或膜材料特性。

Conclusion: 该软夹具方法具有通用性高、精度可控和高重复性的优势，可作为自动装配中的刚性夹具实际替代方案，但目前仍受限于膜材料和几何约束，未来有进一步优化空间。

Abstract: Regrasping on fixtures is a promising approach to reduce pose uncertainty in
robotic assembly, but conventional rigid fixtures lack adaptability and require
dedicated designs for each part. To overcome this limitation, we propose a soft
jig inspired by the jamming transition phenomenon, which can be continuously
deformed to accommodate diverse object geometries. By pressing a
triangular-pyramid-shaped tool into the membrane and evacuating the enclosed
air, a stable cavity is formed as a placement space. We further optimize the
stamping depth to balance placement stability and gripper accessibility. In
soft-jig-based regrasping, the key challenge lies in optimizing the cavity size
to achieve precise dropping; once the part is reliably placed, subsequent
grasping can be performed with reduced uncertainty. Accordingly, we conducted
drop experiments on ten mechanical parts of varying shapes, which achieved
placement success rates exceeding 80% for most objects and above 90% for
cylindrical ones, while failures were mainly caused by geometric constraints
and membrane properties. These results demonstrate that the proposed jig
enables general-purpose, accurate, and repeatable regrasping, while also
clarifying its current limitations and future potential as a practical
alternative to rigid fixtures in assembly automation.

</details>


### [153] [Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation](https://arxiv.org/abs/2509.13816)
*Yude Li,Zhexuan Zhou,Huizhe Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种异步强化学习框架，实现了自主飞行器在复杂环境下的高频率、鲁棒自主导航。通过解耦感知与控制环路，并引入时间编码模块处理感知延迟，实现高频率控制反应，并在现实环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自主航行飞行器需要在复杂环境下实现高机动性和高反应性。然而，感知模块频率低、延时大，与需要高频控制的飞行任务存在矛盾，导致整体性能受限。亟需新方法突破感知与控制频率不匹配问题。

Method: 作者提出了异步强化学习框架，将高频控制策略与低频感知特征解耦，使策略可基于最新IMU数据做出快速反应。引入时间编码模块（TEM），在策略中显式考虑感知延迟，并结合两阶段课程学习确保训练稳定高效。

Result: 方法在大量仿真中通过验证，并实现了零样本由仿真到真实（sim-to-real）的迁移。在实际嵌入式平台（NUC）上成功部署，达到了100Hz高频控制速率，在真实复杂环境中表现出高鲁棒性和敏捷导航能力。

Conclusion: 异步感知与控制结合时间编码的强化学习框架可有效提升自主飞行器在复杂环境下的导航性能。该方案为高频控制与低频感知的矛盾提供了新的解决思路，并具有良好的现实迁移性和实际应用前景。

Abstract: Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex
environments is a critical capability. However, modern end-to-end navigation
faces a key challenge: the high-frequency control loop needed for agile flight
conflicts with low-frequency perception streams, which are limited by sensor
update rates and significant computational cost. This mismatch forces
conventional synchronous models into undesirably low control rates. To resolve
this, we propose an asynchronous reinforcement learning framework that
decouples perception and control, enabling a high-frequency policy to act on
the latest IMU state for immediate reactivity, while incorporating perception
features asynchronously. To manage the resulting data staleness, we introduce a
theoretically-grounded Temporal Encoding Module (TEM) that explicitly
conditions the policy on perception delays, a strategy complemented by a
two-stage curriculum to ensure stable and efficient training. Validated in
extensive simulations, our method was successfully deployed in zero-shot
sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate
and demonstrates robust, agile navigation in cluttered real-world environments.
Our source code will be released for community reference.

</details>


### [154] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: 本文提出了一种基于苍蝇视觉神经元LPLC2的注意力驱动视觉运动控制策略，并首次将其嵌入到微型移动机器人中，实现高效的碰撞感知和自主逃逸反应。该系统仅需70KB内存，比拟昆虫级别低功耗运算，碰撞检测成功率高达96.1%，且机动性和适应性优于当前最先进的蝗虫模型。


<details>
  <summary>Details</summary>
Motivation: 苍蝇具备极高敏捷性的逃逸行为主要源自其视觉神经系统，尤其是碰撞选择性神经元。对自主机器人而言，具备类似能力极具吸引力，但常受限于算力与性能的权衡。因此，开发灵感来自昆虫、兼顾性能与低功耗的简化神经模型，具有重要研究和应用价值。

Method: 作者受苍蝇LPLC2视觉投射神经元及相关逃逸行为启发，提出基于多重注意力机制的仿生视觉运动控制模型，将其优化压缩到70KB内存，适用于Colias视觉微型机器人。同时，该模型引入分布式神经响应机制，提升机器人对来袭目标的快速选择性感知与反应能力。最后，系统性对比了本方法与现有顶尖的蝗虫灵感碰撞检测模型。

Result: 所开发的苍蝇仿生视觉运动感知模型，在碰撞检测上达到96.1%的高成功率，且能产生更加灵活与优雅的机动逃逸动作，整体性能与鲁棒性与现有最佳模型持平或优于后者。

Conclusion: 该工作不仅证明了苍蝇仿生神经视觉模型在机器人碰撞规避中的有效性，还展示了此类模型在推动群体行为及昆虫智能研究领域的创新潜力。

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [155] [UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography](https://arxiv.org/abs/2509.13832)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Xiangjie Yan,Xiang Li,Gao Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种基于层次化Transformer（UltraHiT）的自动化颈动脉超声扫描技术，显著提升了针对复杂内部颈动脉（ICA）自动定位的成功率。


<details>
  <summary>Details</summary>
Motivation: 内部颈动脉（ICA）因解剖结构复杂、深度位置、个体差异大等特性，导致自动化超声扫描技术难以应用，现有研究多未涉及此挑战。作者希望通过算法创新，解决难以自动定位ICA的问题。

Method: 作者提出UltraHiT框架，结合高层次变异识别与低层次动作决策，高层模块判断结构变异类型，并在需要时调用自适应校正模块。高层与校正模块均用因果Transformer实现，通过历史扫描序列做推断。作者还构建了首个大规模ICA扫描数据集（包含164条轨迹、72K样本、28名男女志愿者）以训练模型。

Result: UltraHiT方法在未见过的个体上ICA定位成功率达95%，明显优于基线方法，证明了方法的有效性和泛化能力。

Conclusion: 本研究首次针对复杂的ICA扫描自动化难题提出有效解决方案，并经大规模数据验证。方法为后续自动化血管超声领域提供了技术和数据基础。代码将在论文被接收后开源。

Abstract: Carotid ultrasound is crucial for the assessment of cerebrovascular health,
particularly the internal carotid artery (ICA). While previous research has
explored automating carotid ultrasound, none has tackled the challenging ICA.
This is primarily due to its deep location, tortuous course, and significant
individual variations, which greatly increase scanning complexity. To address
this, we propose a Hierarchical Transformer-based decision architecture, namely
UltraHiT, that integrates high-level variation assessment with low-level action
decision. Our motivation stems from conceptualizing individual vascular
structures as morphological variations derived from a standard vascular model.
The high-level module identifies variation and switches between two low-level
modules: an adaptive corrector for variations, or a standard executor for
normal cases. Specifically, both the high-level module and the adaptive
corrector are implemented as causal transformers that generate predictions
based on the historical scanning sequence. To ensure generalizability, we
collected the first large-scale ICA scanning dataset comprising 164
trajectories and 72K samples from 28 subjects of both genders. Based on the
above innovations, our approach achieves a 95% success rate in locating the ICA
on unseen individuals, outperforming baselines and demonstrating its
effectiveness. Our code will be released after acceptance.

</details>


### [156] [Track Any Motions under Any Disturbances](https://arxiv.org/abs/2509.13833)
*Zhikai Zhang,Jun Guo,Chao Chen,Jilong Wang,Chenghuai Lin,Yunrui Lian,Han Xue,Zhenrong Wang,Maoqi Liu,Huaping Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 本文提出了Any2Track，一个能够在多种真实世界干扰下跟踪多样化运动的人形机器人运动跟踪器，基于两阶段强化学习框架，在实际硬件上实现了零样本迁移和优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前的人形机器人运动跟踪系统难以同时适应复杂、多样的动态动作及应对物理环境扰动。为普遍实际应用，运动追踪器不仅需要精准跟踪各种动作，还得具备对地形、外力和物理属性改变等多种动态干扰的适应力。本文旨在提升追踪器于现实复杂环境下的稳定性和鲁棒性。

Method: 提出Any2Track双阶段强化学习（RL）框架，包括两个核心模块：1）AnyTracker——通过一系列设计，实现单个策略追踪多种不同动态动作；2）AnyAdapter——基于历史信息的自适应模块，使追踪器具备在线应对多种真实世界动态干扰的能力。同时，该方法能克服虚实差距，实现零样本迁移部署。

Result: Any2Track在Unitree G1硬件机器人上进行了部署，把在仿真中学到的策略直接应用于现实世界（零样本迁移），在多种真实环境干扰下依然能够稳定、精准地跟踪不同动作，表现优异，成功克服了常见的虚实差距和环境适应难题。

Conclusion: Any2Track有效提升了人形机器人在多扰动、多动态环境下的运动跟踪能力和适应性，为实现通用型、可落地的人形机器人迈出了重要一步。方法具备很强的泛化和实用价值。

Abstract: A foundational humanoid motion tracker is expected to be able to track
diverse, highly dynamic, and contact-rich motions. More importantly, it needs
to operate stably in real-world scenarios against various dynamics
disturbances, including terrains, external forces, and physical property
changes for general practical use. To achieve this goal, we propose Any2Track
(Track Any motions under Any disturbances), a two-stage RL framework to track
various motions under multiple disturbances in the real world. Any2Track
reformulates dynamics adaptability as an additional capability on top of basic
action execution and consists of two key components: AnyTracker and AnyAdapter.
AnyTracker is a general motion tracker with a series of careful designs to
track various motions within a single policy. AnyAdapter is a history-informed
adaptation module that endows the tracker with online dynamics adaptability to
overcome the sim2real gap and multiple real-world disturbances. We deploy
Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in
a zero-shot manner. Any2Track performs exceptionally well in tracking various
motions under multiple real-world disturbances.

</details>


### [157] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: 本文提出了一种可预测开集词汇物体操作任务未来成功率的模型，通过融合轨迹和自然语言信息，在操作前提升决策效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要在动作完成后才能判断成败，导致难以及时避免风险和低效重复操作。研究动机在于实现事前预判，提升操作安全性和序列效率。

Method: 提出了一种模型，融合操作前第一视角图像、规划轨迹与自然语言指令，通过多层次轨迹融合模块（Multi-Level Trajectory Fusion）并行结合深度状态空间模型和transformer编码器，从而捕获末端操作器轨迹的多层次时序自相关信息，实现任务成功率预测。

Result: 实验表明，所提方法优于现有方法，包括基础大模型，在预测成功率方面取得更佳表现。

Conclusion: 该方法可在物体操作任务中提前判定操作可能结果，增强决策效率与安全性，有望推广到实际机器人操作场景中。

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>


### [158] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: 提出InterKey框架，利用道路交叉口作为特征点，通过编码激光点云和OSM地图信息，实现低成本高精度车辆定位。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号受损场景（如城市峡谷、隧道等）下，车辆定位精度下降。目前高精地图虽然准确，但造价高、维护难。开放地图（如OSM）虽免费，但精度低，于是需要寻找一种基于开放资源且可高效匹配传感器数据的定位方法。

Method: 提出InterKey框架，以“道路交叉口”为显著地标。从激光点云和OSM上，联合编码出压缩的二值描述符，通过“差异消除、朝向判定、区域均衡采样”等策略弥合两种数据模态差异，实现健壮的跨源匹配。

Result: 在KITTI数据集上，InterKey方法定位精度显著优于近期领先方法，跨源定位性能达到新SOTA。

Conclusion: InterKey为自动驾驶提供了一种基于廉价开放地图的高效、稳健定位方案，对可产生高密度结构点云的传感器均适用，具有良好的可推广性和实用性。

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [159] [Using Petri Nets for Context-Adaptive Robot Explanations](https://arxiv.org/abs/2509.13861)
*Görkem Kılınç Soylu,Neziha Akalin,Maria Riveiro*

Main category: cs.RO

TL;DR: 本文提出利用Petri网（PNs）对机器人解释性行为中的情境信息进行建模，实现机器人根据上下文自适应地向人类解释其行为。通过模型分析，验证了该方法的稳健性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 在以人为本的机器人交互中，信任和理解至关重要，而实现自然透明的沟通依赖于机器人能否根据不同情境调整其解释内容。因此，需要一种有效地建模和分析情境信息的方法。

Method: 作者采用Petri网（PNs）对人-机器人交互中的上下文（如用户注意力与在场情况）进行形式化建模。Petri网能够表示并发行为、因果依赖和系统状态变化。此外，作者结合实例场景，展示机器人如何根据情境生成自适应解释，并通过模型分析验证死锁自由、情境敏感可达、有界性和活性等属性。

Result: 模型分析表明，所提出的Petri网方法在应对交互情境变化时具有死锁自由和健壮性，并能灵活适应不同的上下文需求，体现出良好的设计和验证能力。

Conclusion: Petri网能为人-机器人交互中的情境自适应解释提供形式化、可验证的解决方案，有助于提升机器人与人类的信任与沟通质量。

Abstract: In human-robot interaction, robots must communicate in a natural and
transparent manner to foster trust, which requires adapting their communication
to the context. In this paper, we propose using Petri nets (PNs) to model
contextual information for adaptive robot explanations. PNs provide a formal,
graphical method for representing concurrent actions, causal dependencies, and
system states, making them suitable for analyzing dynamic interactions between
humans and robots. We demonstrate this approach through a scenario involving a
robot that provides explanations based on contextual cues such as user
attention and presence. Model analysis confirms key properties, including
deadlock-freeness, context-sensitive reachability, boundedness, and liveness,
showing the robustness and flexibility of PNs for designing and verifying
context-adaptive explanations in human-robot interactions.

</details>


### [160] [Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning](https://arxiv.org/abs/2509.13882)
*Junhwa Hong,Beomjoon Lee,Woojin Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 本文提出了一种高效的多机械臂无碰撞运动规划方法，通过改进CBS算法，显著减少冲突数量和规划计算量。


<details>
  <summary>Details</summary>
Motivation: 多机械臂系统由于高维的联合配置空间，导致运动协调计算复杂度高。传统的CBS方法通过解耦化手段分步规划，但随着冲突的递归解决，约束树急剧膨胀，效率大幅下降。该研究旨在减少冲突树的扩展，提高规划效率。

Method: 在传统CBS的二层结构中，底层规划器采用基于人工势场的梯度下降方法来调整冲突机械臂轨迹，产生斥力避开其他机械臂，从而降低后续冲突。同时，提出了一种特殊情况下可直接一步找到无冲突解的策略，无需扩展约束树。

Result: 实验证明，该方法显著减少约束树扩展节点数量，提升了成功率与搜索速度，相比于增强型CBS和当前主流方法表现更优。

Conclusion: 改进后的CBS结合人工势场，引入新策略后，在多机械臂规划问题上有效提升了求解效率，有望促进实际多机器人系统的普及和应用。

Abstract: We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.

</details>


### [161] [PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](https://arxiv.org/abs/2509.13903)
*Artem Lykov,Jeffrin Sam,Hung Khang Nguyen,Vladislav Kozlovskiy,Yara Mahmoud,Valerii Serpiva,Miguel Altamirano Cabrera,Mikhail Konenkov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 提出PhysicalAgent框架，通过视频生成和闭环执行实现机器人操控任务的迭代推理和自我纠正，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作在复杂任务中易因执行错误而失败，缺乏高效的自动纠错和适应能力。急需一种可自我修正并具适应性的通用机器人操作策略。

Method: PhysicalAgent方法融合了迭代推理、基于扩散的视频生成及闭环执行：系统根据文本指令生成多段候选视频演示，选取并在机器人上执行，不成功时可自动重新规划和尝试，直至成功或达到设定上限。同时支持多感知模式（第一人称、第三人称、仿真）与多种机器人平台。

Result: 实验显示，PhysicalAgent优于各类最新基线方法，在熟悉的人类任务上最高达83%成功率；实际物理测试中，首轮成功率仅20-30%，但迭代纠错后整体达80%。

Conclusion: 基于视频生成的推理与迭代执行机制显著提升了泛化型机器人操作的健壮性和适应性，验证了该框架在自动纠错及多平台通用性方面的高潜力，为可扩展、强适应能力的机器人控制奠定基础。

Abstract: We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.

</details>


### [162] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于地图的端到端自动驾驶路径规划框架（MAP），通过显式融合语义地图特征显著提升了规划效果，并在多个数据集和竞赛中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法虽然综合了感知、预测和规划，但对在线建图模块利用不足，未能充分挖掘地图特征对路径规划的提升潜力。本文旨在弥补该短板，通过引入和融合地图信息优化轨迹规划。

Method: 提出MAP（Map-Assisted Planning）框架，由三个核心模块组成：Plan-enhancing Online Mapping模块负责显式集成基于分割的地图特征，Ego-status-guided Planning模块结合当前自车状态进行规划，Weight Adapter根据实时自车状态自适应调整各模块权重。系统端到端联合建模并在大型V2X序列数据集上实验验证。

Result: 在DAIR-V2X-seq-SPD数据集上，与现有UniV2X基线相比，MAP框架L2位移误差减少16.6%，脱轨率降低56.2%，总体得分提升44.5%。在CVPR2025 MEIS Workshop大赛中获得Track 2第一名，领先次优模型39.5%。

Conclusion: 实验充分证明了显式利用语义地图特征对端到端路径规划显著提升。该方法为自动驾驶系统结构设计和地图特征利用提供了新的思路。

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [163] [Reinforcement Learning for Autonomous Point-to-Point UAV Navigation](https://arxiv.org/abs/2509.13943)
*Salim Oyinlola,Nitesh Subedi,Soumik Sarkar*

Main category: cs.RO

TL;DR: 本论文提出了一种基于强化学习的方法，让无人机能够自主在预设点之间导航，并在实际环境中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在自动巡检、配送和导航任务中的应用日益广泛，对能够自主、可靠的导航系统的需求也在增加。如何减少人工干预、提升自主能力是亟需解决的问题。

Method: 该方法利用强化学习，通过试错方式学习无人机的导航策略。奖励函数不仅激励高效到达目标，还惩罚碰撞和不安全行为。系统集成了ROS与兼容Gym的训练环境，方便训练与测试。训练完成后，将策略部署在实际无人机平台进行评估。

Result: 实验结果表明，训练出的无人机策略能够实现自主导航，仅需极少人工监管，并且在实际环境下表现出色。

Conclusion: 研究验证了基于强化学习的方法可以有效用于无人机的自主点对点导航，凸显了其在现实应用中的实用性和可行性。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in automated
inspection, delivery, and navigation tasks that require reliable autonomy. This
project develops a reinforcement learning (RL) approach to enable a single UAV
to autonomously navigate between predefined points without manual intervention.
The drone learns navigation policies through trial-and-error interaction, using
a custom reward function that encourages goal-reaching efficiency while
penalizing collisions and unsafe behavior. The control system integrates ROS
with a Gym-compatible training environment, enabling flexible deployment and
testing. After training, the learned policy is deployed on a real UAV platform
and evaluated under practical conditions. Results show that the UAV can
successfully perform autonomous navigation with minimal human oversight,
demonstrating the viability of RL-based control for point-to-point drone
operations in real-world scenarios.

</details>


### [164] [The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot](https://arxiv.org/abs/2509.13948)
*Benedict Barrow,Roger K. Moore*

Main category: cs.RO

TL;DR: 本研究探讨了社交机器人面部特征对人类信任感的影响，证实了“婴儿脸”特征（如大眼睛）能够增强机器人的可信任度。


<details>
  <summary>Details</summary>
Motivation: 人类在与机器人互动时，如何建立对机器人的信任尚不完全清楚。了解影响信任的面部特征有助于优化机器人的设计，提高人机互动效果。

Method: 通过操控Furhat机器人的投影表情，重点改变机器人的眼睛形状和大小，考察这些变化对人类首次印象中可信任度的影响。

Result: 实验结果显示，眼形和眼睛大小显著影响人们对机器人可信任度的感知，大眼睛和更具“婴儿脸”特征的机器人更容易被信任。

Conclusion: 面部设计，特别是眼部特征，是影响人们信任社交机器人的重要因素。设计社交机器人时应着重考虑这些特征，以增强人机互动的有效性。

Abstract: Trust and the perception of trustworthiness play an important role in
decision-making and our behaviour towards others, and this is true not only of
human-human interactions but also of human-robot interactions. While
significant advances have been made in recent years in the field of social
robotics, there is still some way to go before we fully understand the factors
that influence human trust in robots. This paper presents the results of a
study into the first impressions created by a social robot's facial features,
based on the hypothesis that a `babyface' engenders trust. By manipulating the
back-projected face of a Furhat robot, the study confirms that eye shape and
size have a significant impact on the perception of trustworthiness. The work
thus contributes to an understanding of the design choices that need to be made
when developing social robots so as to optimise the effectiveness of
human-robot interaction.

</details>


### [165] [SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks](https://arxiv.org/abs/2509.13949)
*Jannick Stranghöner,Philipp Hartmann,Marco Braun,Sebastian Wrede,Klaus Neumann*

Main category: cs.RO

TL;DR: 本文提出了SHaRe-RL方法，通过引入多种先验知识和人机协作，提升了高混低量工业装配中强化学习的效率与安全性，并在实际工业任务中验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 小型和中型企业的高混低量生产需要高度灵活、精准且安全的自动化系统，而现有机器人方法（手动编程和普通强化学习）在应对频繁变化和复杂环境时效率低、成本高且安全性差。

Method: 提出了SHaRe-RL强化学习框架，方法亮点包括：（1）将技能结构化为基本操作单元；（2）融合人类示范和在线校正；（3）通过逐轴合规性约束交互力，保证操作安全。

Result: 在0.2-0.4毫米间隙的工业Harting连接器模块的装配实验中，SHaRe-RL在实际时间内达到了可靠的性能表现，验证了方法的高效性及安全性。

Conclusion: 结合工艺专家知识，使得没有机器人或RL背景的人员也能推动强化学习在工业装配的安全、鲁棒和经济部署，推动RL技术在实际产业中的应用落地。

Abstract: High-mix low-volume (HMLV) industrial assembly, common in small and
medium-sized enterprises (SMEs), requires the same precision, safety, and
reliability as high-volume automation while remaining flexible to product
variation and environmental uncertainty. Current robotic systems struggle to
meet these demands. Manual programming is brittle and costly to adapt, while
learning-based methods suffer from poor sample efficiency and unsafe
exploration in contact-rich tasks. To address this, we present SHaRe-RL, a
reinforcement learning framework that leverages multiple sources of prior
knowledge. By (i) structuring skills into manipulation primitives, (ii)
incorporating human demonstrations and online corrections, and (iii) bounding
interaction forces with per-axis compliance, SHaRe-RL enables efficient and
safe online learning for long-horizon, contact-rich industrial assembly tasks.
Experiments on the insertion of industrial Harting connector modules with
0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance
within practical time budgets. Our results show that process expertise, without
requiring robotics or RL knowledge, can meaningfully contribute to learning,
enabling safer, more robust, and more economically viable deployment of RL for
industrial assembly.

</details>


### [166] [SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning](https://arxiv.org/abs/2509.13956)
*Zewei Yang,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了SEG-Parking，一种端到端的强化学习框架，用于实现对交互敏感的自动泊车任务。通过专门构建的数据集和保守的离线RL策略显著提升了自动泊车的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动泊车在非结构化环境和动态交互中面临巨大挑战，现有方法难以高效且安全应对复杂停车情景，尤其是在与对向车辆互动时效果有限。因此，亟需一种能够理解和处理交互关系的自动泊车方法。

Method: 作者构建了包含多种复杂交互场景的专用泊车数据集，并预训练了一个基于目标条件的状态编码器，将多源感知信息映射至潜在空间。借助加入保守正则项的离线强化学习，优化出能有效应对分布外动作的策略。所有实验均在高保真CARLA模拟器中进行。

Result: 实验结果显示，所提出的SEG-Parking框架在泊车任务中取得了最高的成功率，并且在分布外场景下具备强泛化能力，显著优于对比方法。

Conclusion: SEG-Parking能够提升自动泊车的可靠性与适应性，为城市自动驾驶中的泊车问题提供了切实可行的技术路径，并承诺公开相关数据集和源码以促进研究发展。

Abstract: Autonomous parking is a critical component for achieving safe and efficient
urban autonomous driving. However, unstructured environments and dynamic
interactions pose significant challenges to autonomous parking tasks. To
address this problem, we propose SEG-Parking, a novel end-to-end offline
reinforcement learning (RL) framework to achieve interaction-aware autonomous
parking. Notably, a specialized parking dataset is constructed for parking
scenarios, which include those without interference from the opposite vehicle
(OV) and complex ones involving interactions with the OV. Based on this
dataset, a goal-conditioned state encoder is pretrained to map the fused
perception information into the latent space. Then, an offline RL policy is
optimized with a conservative regularizer that penalizes out-of-distribution
actions. Extensive closed-loop experiments are conducted in the high-fidelity
CARLA simulator. Comparative results demonstrate the superior performance of
our framework with the highest success rate and robust generalization to
out-of-distribution parking scenarios. The related dataset and source code will
be made publicly available after the paper is accepted.

</details>


### [167] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出了一种新方法MetricNet，为生成式导航策略加入了实际距离预测，有效提升了机器人导航和探索能力。


<details>
  <summary>Details</summary>
Motivation: 当前的生成式导航策略在路径生成时，存在抽象且未经比例缩放的轨迹、以及只朝向单一路径点而忽略完整路径的问题。这导致机器人在遇到障碍物时可能采取不安全、短视的动作。作者为了解决这些与现实世界坐标落地和路径安全相关的问题，提出了本项工作。

Method: 本文提出MetricNet模块，可作为生成式导航的加成，负责预测路径点间的实际物理距离，从而把抽象轨迹映射到现实世界坐标。同时，提出了MetricNav，将MetricNet集成到导航策略中，使机器人可以在避障的同时朝着目标前进。方法在仿真和真实环境下均有验证。

Result: 通过新的基准测试框架，在仿真和真实实验中，执行经过MetricNet比例缩放的路径点能够显著提升导航和探索性能。

Conclusion: MetricNet能够有效地将生成式导航策略的输出与实际世界坐标对齐，大幅提高机器人在避障和目标到达上的表现。其模块化设计可广泛应用于现有生成式导航架构中。

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [168] [BIM Informed Visual SLAM for Construction Monitoring](https://arxiv.org/abs/2509.13972)
*Asier Bikandi,Miguel Fernandez-Cortizas,Muhammad Shaheer,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 该论文提出了一种结合BIM先验信息的RGB-D SLAM系统，在施工现场实测可有效提升定位与地图精度。


<details>
  <summary>Details</summary>
Motivation: 传统的LiDAR SLAM虽精度高，但设备笨重不适用于移动终端；视觉SLAM虽便携但在工地等结构重复、遮挡多、纹理少的环境下易发生轨迹漂移，影响误差检测和工程进度管理。

Method: 提出一种在RGB-D SLAM中引入BIM建筑信息模型约束的新方法，实时检测墙体与BIM墙体建立对应关系，将其作为后端优化的约束项，从而减小视觉漂移。

Result: 在实际工地数据上实验，轨迹误差平均降低23.71%，地图RMSE降低7.14%，均优于纯视觉SLAM基线方法。

Conclusion: 结合BIM先验信息做SLAM，有效提升了数字设计蓝图与部分已建场景的对齐精度，增强了施工环境下SLAM的可靠性。

Abstract: Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.

</details>


### [169] [Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array](https://arxiv.org/abs/2509.13998)
*Bailey Dacre,Rodrigo Moreno,Serhat Demirtas,Ziqiao Wang,Yuhao Jiang,Jamie Paik,Kasper Stoy,Andrés Faíña*

Main category: cs.RO

TL;DR: 该论文提出了一种新型分布式机械手系统（DMS），采用3自由度折纸结构单元及柔性连接层，实现低致动器密度下的高效物体操作。


<details>
  <summary>Details</summary>
Motivation: 当前分布式机械手系统为提升操作能力，通常需高致动器密度，但这带来系统复杂度增加且适应性差。如何兼顾低复杂度和高操作能力是亟需解决的问题。

Method: 设计采用3自由度、受折纸启发的机器人单元，并以柔性的表面材料互连，整体形成连续可控的操作表面，使操作实现于每个表面点而非单一末端。分析系统运动空间，设计基础运动模块，并完成对几何物体的位移演示。

Result: 该设计利用连接材料可有效降低致动器密度，在不增加致动器数量的前提下，操作面积提升了1.84倍，展现出灵活且高效的物体操作能力。

Conclusion: 新系统为传统高密度致动器阵列提供了低成本、低复杂度的替代方案，也为依赖柔性表面的新型操作策略带来机遇。

Abstract: Object manipulation is a fundamental challenge in robotics, where systems
must balance trade-offs among manipulation capabilities, system complexity, and
throughput. Distributed manipulator systems (DMS) use the coordinated motion of
actuator arrays to perform complex object manipulation tasks, seeing widespread
exploration within the literature and in industry. However, existing DMS
designs typically rely on high actuator densities and impose constraints on
object-to-actuator scale ratios, limiting their adaptability. We present a
novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles
interconnected by a compliant surface layer. Unlike conventional DMS, our
approach enables manipulation not only at the actuator end effectors but also
across a flexible surface connecting all actuators; creating a continuous,
controllable manipulation surface. We analyse the combined workspace of such a
system, derive simple motion primitives, and demonstrate its capabilities to
translate simple geometric objects across an array of tiles. By leveraging the
inter-tile connective material, our approach significantly reduces actuator
density, increasing the area over which an object can be manipulated by x1.84
without an increase in the number of actuators. This design offers a lower cost
and complexity alternative to traditional high-density arrays, and introduces
new opportunities for manipulation strategies that leverage the flexibility of
the interconnected surface.

</details>


### [170] [Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization](https://arxiv.org/abs/2509.14010)
*Zong Chen,Shaoyang Li,Ben Liu,Min Li,Zhouping Yin,Yiqun Li*

Main category: cs.RO

TL;DR: 本文提出了一种全向轮腿四足机器人，集成了灵巧的机械手臂，能实现高机动性和高精度移动操控。通过全新的动力学优化控制框架与统一运动学模型，该机器人能高效处理复杂地形与多任务需求。实验和仿真验证了其在物流、自动化和服务等场景下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 面对移动操控需求日益提升的工业、物流和服务场景，融合腿部和轮子、集成机械臂的移动机器人具有广阔应用前景。然而，这类机器人的多自由度、复杂的轮地接触和行走/操控一体化带来了巨大的控制挑战。研究旨在突破当前同类机器人在统一控制和实时高维优化方面的瓶颈。

Method: 提出了一种包含独立转向模块与轮毂电机的全新四足轮式机器人平台。开发了一套同时面向点接触（操控）和线接触（轮地）统一建模的全身动力学优化框架，通过引入warm-start策略提升高维优化的实时性。此外，基于4WIS-4WID驱动构建统一运动学模型，实现不同运动模式间的无缝切换。

Result: 仿真和实物实验表明，该系统在各种场景下表现出优越的灵活地形适应能力、高速全向移动和高精度操控能力，显著提升了复杂环境下的运动和作业效率。

Conclusion: 所提机器人平台和优化控制框架有效提升了轮腿多功能机器人的机动性和操控能力，保障了多场景下的应用鲁棒性与一致性，为工厂自动化、城市物流与服务机器人等领域提供了有力的技术支撑。

Abstract: Wheel-legged robots with integrated manipulators hold great promise for
mobile manipulation in logistics, industrial automation, and human-robot
collaboration. However, unified control of such systems remains challenging due
to the redundancy in degrees of freedom, complex wheel-ground contact dynamics,
and the need for seamless coordination between locomotion and manipulation. In
this work, we present the design and whole-body motion control of an
omnidirectional wheel-legged quadrupedal robot equipped with a dexterous
manipulator. The proposed platform incorporates independently actuated steering
modules and hub-driven wheels, enabling agile omnidirectional locomotion with
high maneuverability in structured environments. To address the challenges of
contact-rich interaction, we develop a contact-aware whole-body dynamic
optimization framework that integrates point-contact modeling for manipulation
with line-contact modeling for wheel-ground interactions. A warm-start strategy
is introduced to accelerate online optimization, ensuring real-time feasibility
for high-dimensional control. Furthermore, a unified kinematic model tailored
for the robot's 4WIS-4WID actuation scheme eliminates the need for mode
switching across different locomotion strategies, improving control consistency
and robustness. Simulation and experimental results validate the effectiveness
of the proposed framework, demonstrating agile terrain traversal, high-speed
omnidirectional mobility, and precise manipulation under diverse scenarios,
underscoring the system's potential for factory automation, urban logistics,
and service robotics in semi-structured environments.

</details>


### [171] [TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems](https://arxiv.org/abs/2509.14025)
*Rui Huang,Zhiyu Gao,Siyu Tang,Jialin Zhang,Lei He,Ziqian Zhang,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出TransforMARS框架，实现了模块化空中机器人系统（MARS）在多处失效下的自适应重构，保持稳定飞行。区别于以往仅针对矩形结构和单一故障的工作，TransforMARS支持任意结构和多元故障，并通过实际验证展现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARS的容错重构机制只能应对单一旋翼或单元失效，且多局限于规则矩形形状，导致在复杂故障或不规则结构下缺乏实用性。实际应用中，MARS常面临多重故障及不定形结构，亟需一种更加通用且高效的自重构方法，以提升系统的鲁棒性和应用广度。

Method: 本文设计了一套通用容错重构算法。首先识别并构建包含故障单元的最小可控分组；随后，规划可行的拆解与重组序列，将分散的MARS单元/子模块合理移动、组装，达到新的稳定目标形态。该方法适配任意形状MARS，并支持多重旋翼/单元同时失效情境。

Result: 在具挑战性的非规则MARS构型实验中，TransforMARS展现了对更多失效情景和复杂结构的适应能力，在可容忍故障数量和重构多样性方面显著优于既有方法。

Conclusion: TransforMARS极大拓展了MARS系统的容错性和自重构能力，使其在实际复杂环境下更加实用且具有灵活部署能力。

Abstract: Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/

</details>


### [172] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: 该论文提出了Prompt2Auto，一个能实现几何不变性的一次性高斯过程学习框架，使机器人能够通过单次人类示范实现自动控制，并大幅减少示范数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人从示范学习方法需大量数据，且很难在坐标变换（如平移、旋转、缩放）下有良好泛化能力。机器人需要更高效、泛化性更强的学习方法来从有限示范中习得复杂技能。

Method: 作者提出了GeoGP（geometry-invariant Gaussian process）框架，通过基于坐标变换的数据集构建策略，使得模型对平移、旋转和缩放保持不变，并支持多步预测。同时，该方法对用户的动作变化鲁棒，支持多技能自主性。

Result: 通过数值仿真（结合用户图形界面）和两个现实机器人实验，验证了该方法的有效性。实验显示，Prompt2Auto能成功泛化到不同任务，且大大降低了演示需求。

Conclusion: Prompt2Auto实现了高效泛化且对几何变化鲁棒的从示范学习，降低了机器人学习复杂技能的数据负担，具备实际应用价值。

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [173] [Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace](https://arxiv.org/abs/2509.14063)
*Sundhar Vinodh Sangeetha,Chih-Yuan Chiu,Sarah H. Q. Li,Shreyas Kousik*

Main category: cs.RO

TL;DR: 本文提出了一种多模态框架，将自然语言理解与空间推理结合，实现无人机在无塔空域中对其他飞机目标的准确预测。实验结果显示，该方法能显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 在无塔空域中，飞机之间主要通过语音交流进行协调。无人机若要安全运行，必须理解并预测其他飞机的行动意图和目标位置。现有方法难以有效利用语音信息，预测存在局限。

Method: 方法结合了自动语音识别和大语言模型，对飞行员无线电呼叫进行转录和意图提取；结合观察到的飞机轨迹，将获得的离散意图标签与轨迹数据共同输入时序卷积网络和高斯混合模型，实现概率性目标预测。

Result: 与仅依赖运动历史的基线方法相比，该方法明显降低了目标预测误差，验证了引入语言信息提升预测准确性的有效性。

Conclusion: 本方法为机器人在基于社会互动和语言驱动的无人机路径规划奠定了基础，有望提升自主飞行系统在现实环境中的安全性和智能性。

Abstract: Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.

</details>


### [174] [Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots](https://arxiv.org/abs/2509.14075)
*Yu Li,Hamid Sadeghian,Zewen Yang,Valentin Le Mesle,Sami Haddadin*

Main category: cs.RO

TL;DR: 本文针对机器人辅助手术中对远心运动中心（RCM）约束高精度执行的难题，提出了一种新的基于投影的逆动力学约束一致性力矩控制方法，实现了更安全、高效的手术机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有手术机器人对RCM约束执行不够稳定，部分方法在关节力矩控制上鲁棒性不足，无法在复杂、动态操作场景下持续、精确地满足RCM约束，导致手术安全性和可靠性受限。为了解决这一问题，需开发能更好地满足RCM动力学约束的新型控制方法。

Method: 作者提出了一种将RCM视为流变全约束（rheonomic holonomic constraint），并将其嵌入到基于投影的逆动力学控制框架中的力矩控制器方法，实现了任务级与运动学级的统一控制。该方法通过力矩输出的平滑性与一致性设计，同时达到精确的手术器械端点跟踪与RCM约束的持续满足。

Result: 该方法在模拟仿真和实际RAMIS训练平台上进行了实验，并与业界先进方法进行了对比。实验表明，该控制器在包括螺旋轨迹、变插入力度、穿刺点移动和人与设备交互等多种临床相关场景下，RCM约束保持得更加精确、所需关节力矩更低且力矩输出更平滑稳定。

Conclusion: 该论文提出的约束一致性力矩控制可显著提升手术机器人系统RCM约束的可靠实现度、安全性与鲁棒性。其在临床相关复杂场景下的优良表现表明，该方法在未来手术机器人安全性和可靠性提升方面具有很大应用潜力。

Abstract: Robotic-assisted minimally invasive surgery (RAMIS) requires precise
enforcement of the remote center of motion (RCM) constraint to ensure safe tool
manipulation through a trocar. Achieving this constraint under dynamic and
interactive conditions remains challenging, as existing control methods either
lack robustness at the torque level or do not guarantee consistent RCM
constraint satisfaction. This paper proposes a constraint-consistent torque
controller that treats the RCM as a rheonomic holonomic constraint and embeds
it into a projection-based inverse-dynamics framework. The method unifies
task-level and kinematic formulations, enabling accurate tool-tip tracking
while maintaining smooth and efficient torque behavior. The controller is
validated both in simulation and on a RAMIS training platform, and is
benchmarked against state-of-the-art approaches. Results show improved RCM
constraint satisfaction, reduced required torque, and robust performance by
improving joint torque smoothness through the consistency formulation under
clinically relevant scenarios, including spiral trajectories, variable
insertion depths, moving trocars, and human interaction. These findings
demonstrate the potential of constraint-consistent torque control to enhance
safety and reliability in surgical robotics. The project page is available at:
https://rcmpc-cube.github.io

</details>


### [175] [FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video](https://arxiv.org/abs/2509.14082)
*Valerii Serpiva,Artem Lykov,Faryal Batool,Vladislav Kozlovskiy,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的新框架“FlightDiffusion”，可通过第一视角视频训练无人机进行自主导航，不仅直接学习策略，还可生成大规模、多样性的训练数据集，解决真实数据采集昂贵的问题。在仿真与现实环境下均表现出很好的性能和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 无人机自主导航需要大量多样化的训练数据，真实采集代价高，同时现有方法在复杂动态环境下推理能力和仿真迁移性能有限。该工作旨在解决数据收集瓶颈与增强无人机在复杂环境下的泛化能力。

Method: 提出FlightDiffusion框架，利用扩散生成模型从单帧图像预测视频序列及相应动作空间，既能直接用于策略学习，也能批量生成多样的FPV轨迹和状态-动作对，快速扩充训练集，并评估生成数据的物理可行性。

Result: 生成的无人机轨迹和动作对具有高物理可行性，平均位置误差0.25米、姿态误差0.19弧度。在仿真与现实中的导航任务中都取得较好成功率（仿真0.628、现实0.617），差异无统计学意义，显示良好的sim-to-real迁移能力。

Conclusion: 扩散推理为无人机导航、动作生成和数据合成提供了新范式，不仅提升策略学习效果，也提升数据集规模和质量，有助于未来无人机研究。

Abstract: We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.

</details>


### [176] [GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14117)
*Ali Abouzeid,Malak Mansour,Zezhou Sun,Dezhen Song*

Main category: cs.RO

TL;DR: 本文提出了GeoAware-VLA方法，通过融合强几何先验提升VLA模型在新摄像机视角下的泛化能力，无需额外三维数据也能显著提升实际和模拟机器人的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型对新摄像机视角泛化能力弱，主要因为难以从二维图像提取稳健的三维几何信息，影响了机器人系统的应用广度。

Method: 方法核心是不用额外训练视觉编码器，也不依赖显式三维数据，而是采用已经预训练好的几何视觉模型作为特征提取器。接着，用可训练的投影层将这些丰富的几何特征映射到动作策略解码器中，大大缓解了策略网络学习三维一致性的负担。

Result: 在LIBERO基准的多个子集上，GeoAware-VLA在模拟环境中新视角下零样本泛化能力显著提升，模拟成功率提高2倍以上。实物机器人测试同样表现优越，对未见过的摄像机角度尤为明显。

Conclusion: 把强几何特征引入VLA模型显著提升了不同任务和动作空间下的泛化能力，为设计更泛化的机器人智能体提供了关键技术。

Abstract: Vision-Language-Action (VLA) models often fail to generalize to novel camera
viewpoints, a limitation stemming from their difficulty in inferring robust 3D
geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective
approach that enhances viewpoint invariance by integrating strong geometric
priors into the vision backbone. Instead of training a visual encoder or
relying on explicit 3D data, we leverage a frozen, pretrained geometric vision
model as a feature extractor. A trainable projection layer then adapts these
geometrically-rich features for the policy decoder, relieving it of the burden
of learning 3D consistency from scratch. Through extensive evaluations on
LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial
improvements in zero-shot generalization to novel camera poses, boosting
success rates by over 2x in simulation. Crucially, these benefits translate to
the physical world; our model shows a significant performance gain on a real
robot, especially when evaluated from unseen camera angles. Our approach proves
effective across both continuous and discrete action spaces, highlighting that
robust geometric grounding is a key component for creating more generalizable
robotic agents.

</details>


### [177] [CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads](https://arxiv.org/abs/2509.14126)
*Viktor Lorentz,Khaled Wahba,Sayantan Auddy,Marc Toussaint,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 本文提出了一种名为CrazyMARL的去中心化强化学习框架，实现了多无人机（UAVs）协同运输缆绳悬挂载荷的任务，在应对扰动和非线性动力学等复杂问题时表现优异。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同运输悬挂载荷能增强运输能力，适应不同形状、高效应对复杂任务，但目前多为理想假设，未能解决真实条件下缆绳松弛与绷紧（slack-taut）切换时的控制难题。

Method: 提出一种去中心化强化学习方法CrazyMARL，通过多智能体RL训练无人机团队，使其能够适应缆绳从松弛到绷紧的动力学变化，并在有风或外部干扰等苛刻环境下保持鲁棒运行。

Result: 仿真实验证明，该方法在扰动抑制和跟踪精度上均优于传统去中心化控制器。在极端条件下，恢复率达到80%，而基线方法仅为44%。此外，方法可直接从模拟迁移到实际平台，表现出高度鲁棒性。

Conclusion: CrazyMARL为无人机团队在非结构化环境下自主鲁棒地执行复杂运输任务开辟了新道路，显著提高了多UAV运输系统在现实复杂情形下的实用性和适应性。

Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.

</details>


### [178] [Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks](https://arxiv.org/abs/2509.14127)
*Alkesh K. Srivastava,Jared Michael Levin,Philip Dames*

Main category: cs.RO

TL;DR: 提出了一种新颖的多机器人物流配送调度框架VCST-RCP，该方法能充分利用中继机制显著提升能效和任务效率。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人配送经常采用直接运输，未充分利用机器人协同的中继潜力，且在机器人载重有限时存在效率瓶颈。作者希望通过引入高效中继机制优化配送。

Method: 提出VCST-RCP框架，结合Voronoi-受约束的Steiner树优化方法构造稀疏中继主干，并自顶向下综合规划每台机器人的取件、中继与投递调度。将中继提升为核心协作要素。

Result: 大量实验表明，该框架相较于传统直接运输方式，整体表现提升最高可达34%，特别在能量消耗和运输效率方面表现突出。

Conclusion: VCST-RCP框架通过引入高效中继和协同调度机制，实现了多机器人有限容量条件下的高效低耗物流调度，具有良好可扩展性和现实意义。

Abstract: We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.

</details>


### [179] [SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14138)
*Ran Yang,Zijian An,Lifeng ZHou,Yiming Feng*

Main category: cs.RO

TL;DR: 本文提出了SeqVLA，这是一种能够感知子任务完成情况的新型视觉-语言-动作模型，有效提升了机器人在多阶段、长时序操作任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（如π_0）在复杂的长时序多任务环境下表现不佳，主要原因是缺乏对子任务完成的检测信号，导致下游失败积累。因此，需要增强模型对子任务完成的自我感知能力。

Method: SeqVLA在原模型基础上，增添了一个轻量级检测头，用于判断当前子任务是否完成。该结构能够在生成操作动作的同时，自主触发子任务间的切换。作者还探索了四种不同的微调方式，包括动作和检测头的优化策略（联合vs顺序微调）以及预训练知识的保留方式（全微调vs冻结骨干）。

Result: 在包含7个子任务的沙拉包装和包含4个子任务的糖果包装两项多阶段任务上，SeqVLA在整体成功率上显著优于π_0及其他强基线模型。尤其是联合微调且骨干不冻结时，子任务完成检测最为准确可靠，有效消除了序列相关失误。

Conclusion: 将动作生成与子任务感知相结合，能够显著提升多阶段机器人操作任务的可拓展性与稳健性，SeqVLA展现了其在长时序操作场景中的优越性。

Abstract: Long-horizon robotic manipulation tasks require executing multiple
interdependent subtasks in strict sequence, where errors in detecting subtask
completion can cascade into downstream failures. Existing
Vision-Language-Action (VLA) models such as $\pi_0$ excel at continuous
low-level control but lack an internal signal for identifying when a subtask
has finished, making them brittle in sequential settings. We propose SeqVLA, a
completion-aware extension of $\pi_0$ that augments the base architecture with
a lightweight detection head perceiving whether the current subtask is
complete. This dual-head design enables SeqVLA not only to generate
manipulation actions but also to autonomously trigger transitions between
subtasks. We investigate four finetuning strategies that vary in how the action
and detection heads are optimized (joint vs. sequential finetuning) and how
pretrained knowledge is preserved (full finetuning vs. frozen backbone).
Experiments are performed on two multi-stage tasks: salad packing with seven
distinct subtasks and candy packing with four distinct subtasks. Results show
that SeqVLA significantly outperforms the baseline $\pi_0$ and other strong
baselines in overall success rate. In particular, joint finetuning with an
unfrozen backbone yields the most decisive and statistically reliable
completion predictions, eliminating sequence-related failures and enabling
robust long-horizon execution. Our results highlight the importance of coupling
action generation with subtask-aware detection for scalable sequential
manipulation.

</details>


### [180] [CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](https://arxiv.org/abs/2509.14143)
*Zijian An,Ran Yang,Yiming Feng,Lifeng Zhou*

Main category: cs.RO

TL;DR: 本文提出CLAW框架，将条件监控与动作生成解耦，有效提升了机器人按语言指令和重量阈值完成任务的能力。CLAW在各种实验场景下均实现了对重量条件的准确控制，优于原始或微调的VLA模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在通过自然语言指导机器人任务时，难以应对需满足明确约束（如数字阈值）的情形，因为任务条件的监控并不显式，易导致策略不精确，影响实际应用。

Method: 提出CLAW框架，将条件判定与动作生成解耦。具体做法是利用微调后的CLIP模型持续监测秤的数字读数，根据任务权重阈值生成离散指令。这些指令与多视角摄像头观测一同输入流式VLA策略（π0），后者据此输出机器人的连续动作。

Result: 在单物体抓取和多物体双臂操作三种实验场景中，CLAW都能准确执行基于重量条件的操作，表现优于原生及仅微调的VLA模型。

Conclusion: CLAW通过融合符号化的重量推理与高频率视觉运动控制，实现了语言、视觉与条件判定的高效协同，为具备复杂约束的机器人控制提供了新思路。

Abstract: Vision-language-action (VLA) models have recently emerged as a promising
paradigm for robotic control, enabling end-to-end policies that ground natural
language instructions into visuomotor actions. However, current VLAs often
struggle to satisfy precise task constraints, such as stopping based on numeric
thresholds, since their observation-to-action mappings are implicitly shaped by
training data and lack explicit mechanisms for condition monitoring. In this
work, we propose CLAW (CLIP-Language-Action for Weight), a framework that
decouples condition evaluation from action generation. CLAW leverages a
fine-tuned CLIP model as a lightweight prompt generator, which continuously
monitors the digital readout of a scale and produces discrete directives based
on task-specific weight thresholds. These prompts are then consumed by $\pi_0$,
a flow-based VLA policy, which integrates the prompts with multi-view camera
observations to produce continuous robot actions. This design enables CLAW to
combine symbolic weight reasoning with high-frequency visuomotor control. We
validate CLAW on three experimental setups: single-object grasping and
mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW
reliably executes weight-aware behaviors and outperforms both raw-$\pi_0$ and
fine-tuned $\pi_0$ models. We have uploaded the videos as supplementary
materials.

</details>


### [181] [StableTracker: Learning to Stably Track Target via Differentiable Simulation](https://arxiv.org/abs/2509.14147)
*Fanxing Li,Shengyang Wang,Fangyu Sun,Shuyu Wu,Dexin Zuo,Wenxian Yu,Danping Zou*

Main category: cs.RO

TL;DR: 本文提出了StableTracker，一种基于学习的四旋翼控制策略，能从任意视角稳定追踪目标并保持稳定距离，相比传统方法具有更高的准确性和鲁棒性，并通过仿真和真实无人机实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前FPV（第一视角）目标跟踪方法依赖大量人工模块设计，导致硬件负载大和误差累积，尤其在目标快速加减速时性能下降，亟需更高效、鲁棒的追踪方法。

Method: 使用可微仿真环境，通过反向传播算法训练出学习式的控制策略，让无人机在追踪中自动保持目标在视觉中心和相对固定距离，实现自主空中摄像。

Result: 与传统和其他学习方法对比，StableTracker在仿真中表现出更高的准确度、稳定性和泛化能力，适应不同速度、距离和轨迹变化。

Conclusion: 学习驱动的StableTracker不仅在模拟环境下优于现有方法，而且在真实无人机平台上同样表现出良好的可行性，为FPV追踪问题提供了新的解决思路。

Abstract: FPV object tracking methods heavily rely on handcraft modular designs,
resulting in hardware overload and cumulative error, which seriously degrades
the tracking performance, especially for rapidly accelerating or decelerating
targets. To address these challenges, we present \textbf{StableTracker}, a
learning-based control policy that enables quadrotors to robustly follow the
moving target from arbitrary perspectives. The policy is trained using
backpropagation-through-time via differentiable simulation, allowing the
quadrotor to maintain the target at the center of the visual field in both
horizontal and vertical directions, while keeping a fixed relative distance,
thereby functioning as an autonomous aerial camera. We compare StableTracker
against both state-of-the-art traditional algorithms and learning baselines.
Simulation experiments demonstrate that our policy achieves superior accuracy,
stability and generalization across varying safe distances, trajectories, and
target velocities. Furthermore, a real-world experiment on a quadrotor with an
onboard computer validated practicality of the proposed approach.

</details>


### [182] [MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies](https://arxiv.org/abs/2509.14159)
*Dayi Dong,Maulik Bhatt,Seoyeon Choi,Negar Mehr*

Main category: cs.RO

TL;DR: 本文提出了一种新的多模态多智能体模仿学习方法MIMIC-D，结合扩散模型和集中训练-分散执行框架，实现机器人在多解环境下的高效协同。实验显示，其在多任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器人深入社会，多智能体在多模态任务中的协同能力变得关键。但现有模仿学习方法难以覆盖演示中的多样策略，特别是在缺乏集中规划或通信环境下需要隐式协同。

Method: 该文提出MIMIC-D方法，利用扩散模型对多智能体进行模仿学习。训练阶段采用集中训练（充分信息下联合优化），执行阶段则分散执行（各自凭本地信息决策），以实现无需通信的隐式协同。

Result: 在仿真和真实硬件实验中，该方法在多个任务和环境下均能恢复代理间的多模态协同行为，并在性能上超越了多种主流基线方法。

Conclusion: MIMIC-D为多智能体多模态模仿学习提供了更有效的解决方案，在实际机器人系统中表现优越，促进了复杂协同任务的实现。

Abstract: As robots become more integrated in society, their ability to coordinate with
other robots and humans on multi-modal tasks (those with multiple valid
solutions) is crucial. We propose to learn such behaviors from expert
demonstrations via imitation learning (IL). However, when expert demonstrations
are multi-modal, standard IL approaches can struggle to capture the diverse
strategies, hindering effective coordination. Diffusion models are known to be
effective at handling complex multi-modal trajectory distributions in
single-agent systems. Diffusion models have also excelled in multi-agent
scenarios where multi-modality is more common and crucial to learning
coordinated behaviors. Typically, diffusion-based approaches require a
centralized planner or explicit communication among agents, but this assumption
can fail in real-world scenarios where robots must operate independently or
with agents like humans that they cannot directly communicate with. Therefore,
we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)
paradigm for multi-modal multi-agent imitation learning using diffusion
policies. Agents are trained jointly with full information, but execute
policies using only local information to achieve implicit coordination. We
demonstrate in both simulation and hardware experiments that our method
recovers multi-modal coordination behavior among agents in a variety of tasks
and environments, while improving upon state-of-the-art baselines.

</details>


### [183] [\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video](https://arxiv.org/abs/2509.14178)
*Kai Ye,Yuhang Wu,Shuyuan Hu,Junliang Li,Meng Liu,Yongquan Chen,Rui Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种无需大量真人演示、利用生成视频驱动机器人灵巧操作技能的系统Gen2Real，显著降低数据采集成本并实现高效抓取。


<details>
  <summary>Details</summary>
Motivation: 传统灵巧操作学习依赖大量真人示范，获取难度大且成本高，严重限制了实际应用，因此亟需低成本、高效率的数据生成和利用方法。

Method: 作者提出Gen2Real系统：1）利用视频生成结合位姿与深度估计，将生成视频转换为手与物体的交互轨迹；2）应用物理感知的交互优化模型（PIOM）优化这些轨迹以保证其物理合理性；3）开发带锚定残差的PPO策略，将人类手势动作重定位至机器人手，并提升控制稳定性。

Result: 在仿真环境下，利用仅生成视频学习的策略在抓取任务中达到了77.3%的成功率，并在真实机器人上的执行表现连贯。消融实验验证了系统各组成部分的作用，并展示了Gen2Real可直接通过自然语言指定任务，实现对不同抓取任务的灵活泛化能力。

Conclusion: Gen2Real实现了从生成视频到实际机器人技能迁移，显著改善了抓取任务机器人学习的数据瓶颈，具有灵活且鲁棒的泛化能力，为机器人灵巧操作开辟了新途径。

Abstract: Dexterous manipulation remains a challenging robotics problem, largely due to
the difficulty of collecting extensive human demonstrations for learning. In
this paper, we introduce \textsc{Gen2Real}, which replaces costly human demos
with one generated video and drives robot skill from it: it combines
demonstration generation that leverages video generation with pose and depth
estimation to yield hand-object trajectories, trajectory optimization that uses
Physics-aware Interaction Optimization Model (PIOM) to impose physics
consistency, and demonstration learning that retargets human motions to a robot
hand and stabilizes control with an anchor-based residual Proximal Policy
Optimization (PPO) policy. Using only generated videos, the learned policy
achieves a 77.3\% success rate on grasping tasks in simulation and demonstrates
coherent executions on a real robot. We also conduct ablation studies to
validate the contribution of each component and demonstrate the ability to
directly specify tasks using natural language, highlighting the flexibility and
robustness of \textsc{Gen2Real} in generalizing grasping skills from imagined
videos to real-world execution.

</details>


### [184] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: 该论文提出了一种基于3D高斯投影（3D Gaussian Splatting）的纯RGB多相机SLAM系统（MCGS-SLAM），在合成和真实数据集上实现了高精度和高保真的实时地图重建，优于现有单目系统。


<details>
  <summary>Details</summary>
Motivation: 现有密集SLAM系统主要针对单目设置，导致鲁棒性和几何覆盖受限。实际应用（如机器人、自动驾驶）需要更大视场和更可靠的地图，因而需要多相机和更强的融合方法。

Method: 作者提出MCGS-SLAM，仅依赖多路RGB数据，在3D高斯投影上持续优化融合生成稠密地图。多相机束调整（MCBA）联合优化位姿与深度，同时通过尺度一致性模块对齐多视角标度，保证公制一致性。整个流程实现了大规模实时计算。

Result: 实验证明MCGS-SLAM在合成和真实数据集中都能取得比单目SLAM更高的姿态精度和更高保真的重建效果，尤其是在单目难以覆盖的区域（如侧视区域）展现了其独特优势。

Conclusion: MCGS-SLAM证明多相机高斯投影方案在机器人与自动驾驶等需要高保真地图的场景下极具应用前景，能提供更广覆盖和更可靠的地图重建，有望提升智能系统的自主能力。

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


### [185] [GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments](https://arxiv.org/abs/2509.14210)
*Seth Farrell,Chenghao Li,Hongzhan Yu,Hesam Mojtahedi,Sicun Gao,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 本文提出了一种结合两架无人机（UAV）与一辆无人地面车（UGV）的合作式空地搜索与救援（SAR）系统GLIDE，实现未知环境下快速受害者定位与避障导航。通过分工协作，提升了SAR效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前搜索与救援任务对速度和安全有极高要求，传统方法在复杂环境的实时目标识别与路径规划方面存在局限。作者希望通过多机器人合作提升受害者定位及时性、导航安全与任务整体效率。

Method: 系统采用两架UAV与一辆UGV协同工作：一架UAV负责实时受害者检测并给UGV指定目标，另一架UAV负责前方路径勘探以提供通行性信息。UGV结合空中线索与本地传感，基于A*算法进行高效路径规划与动态重规划。通过实体硬件和仿真实验评估方案性能。

Result: 实验结果显示，多UAV明确分工并结合地面引导规划可显著提升SAR任务的抵达速度与导航安全性。硬件演示与仿真实验均验证了方案有效性。

Conclusion: GLIDE框架证明了多无人机分工协作+地面引导可在时间紧迫的SAR任务中提升性能，为复杂环境下的智能搜索与救援提供了有力手段。

Abstract: We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.

</details>


### [186] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 本文提出了一种分布式多机器人源定位方法，结合机器学习与有限元模型，通过信息采集优化策略，有效提升了在复杂流场中的源定位效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂流场中定位化学泄漏源或追踪油泄漏的扩散是极具挑战性的任务。由于流体动力学的复杂和传感器数据间歇性，且机器人的算力有限，现有方法难以实现高效准确的源定位。

Method: 每个机器人搭载机器学习训练的有限元环境模型，基于互信息准则进行信息驱动采样，采用信息导航策略选择能最大化信息获取效益的感测区域，从而优化源的定位过程。

Result: 实验结果显示，与传统的采样策略和纯机器学习方法相比，本文方法能更快地降低定位误差，并实现更准确的源定位效果。

Conclusion: 该方法有效提升了多机器人系统在复杂环境中源定位的效率和准确性，为实际部署具有限制算力的机器人团队提供了新的解决方案。

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>
