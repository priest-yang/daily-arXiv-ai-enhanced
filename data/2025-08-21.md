<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: 本文研究了正采样Kantorovich（SK）算子及其收敛性，并在SK-算子框架下，综合分析了SK、Gaussian、Bilateral和基于小波阈值的算子的局部和全局近似性质，通过理论推导和数值实例对比了各类算子的误差表现以及不同图像指标。


<details>
  <summary>Details</summary>
Motivation: SK算子及其相关算子在图像处理与逼近理论有广泛应用，但其在不同采样与分辨率参数下的近似性能、误差特性等缺乏系统性对比，因此有必要进行详细分析与实验验证。

Method: 引入SK算子相关基本术语与逼近基本定理（FTA），设定不同算子的条件，结合理论推导与实例，分别在理想和非理想（实际图像）条件下，计算误差、均方误差（MSE）、斑点指数（SI）、斑点抑制指数（SSI）、斑点均值保持指数（SMPI）及等效视数（ENL）等参数，进行对比分析。

Result: 通过数值例子和指标分析，不同算子在特定采样水平与图像特征下表现各异，有的算子在某些特征下更有效，有的则表现一般；验证了逼近基本定理，在实际图像ROI区域上对比了各算子的性能。

Conclusion: 各类SK相关算子在图像不同特征处理时表现不一，部分算子更适合特定场景，强调了需结合实际图像特性选择合适算子。

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [2] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 本文提出基于联邦学习的骨骼动作识别框架，在工业环境下既保护隐私又提升了跨用户泛化能力，实验验证了优于传统集中式方法。


<details>
  <summary>Details</summary>
Motivation: 在智能制造中，准确实时识别工人动作对生产率、安全和人机协作至关重要。但多数现有骨骼动作识别方法依赖集中数据集，不适用于对隐私敏感的工业场景。

Method: 采集并处理了八种工业相关上半身动作的自定义骨骼数据集，采用改进的FastPose模型提取特征，并分别结合LSTM和Transformer作为时序骨干，设计了集中式、本地训练、联邦平均（FedAvg）、联邦集成学习（FedEnsemble）四种训练方案进行对比。

Result: 在全局测试集上，FL Transformer较集中式训练准确率提升12.4个百分点，FedEnsemble提升16.3个百分点。在新客户端数据上，FL和FedEnsemble分别比集中式训练提升了52.6和58.3个百分点。

Conclusion: 联邦学习不仅保护了数据隐私，还大幅增强了跨用户泛化能力，是面向工业异构环境中可扩展、隐私保护的动作识别应用的有效方案。

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [3] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的端到端图文联合推理与分割框架LENS，显著提升了文本提示下的图像分割效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本提示图像分割方法大多只关注分割本身，缺乏显式的推理链条，导致泛化能力有限，尤其是在面对新领域或未见过的提示时效果不佳。

Method: 引入LENS框架，使用端到端的强化学习，通过设计涵盖句子级、框级和分割级的统一奖励机制，联合优化模型的推理过程（生成推理链条）和分割质量。模型基于3B参数的Qwen2.5-VL-3B-Instruct视觉语言模型进行训练和评估。

Result: LENS在RefCOCO、RefCOCO+和RefCOCOg基准上取得了平均cIoU 81.2%的成绩，较现有的GLaMM方法最高提升5.6%。

Conclusion: 基于强化学习的链式推理不仅提升了文本提示图像分割的泛化能力，而且为“通用分割一切”模型开辟了新路径。

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [4] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC是一种针对具身认知设计的视频多模态大语言模型，实现了灵活的区域级视频理解和交互，在多个认知任务上达到了领先水平。作者还提出了新的数据生成流程和评价基准。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能体（如机器人等）的发展，对能够感知和理解物理世界、灵活进行区域级交互的多模态大模型需求增加，现有3D注释数据又稀缺，急需新模型与评价体系。

Method: RynnEC基于通用视觉-语言基础模型，集成了区域编码器和掩码解码器结构，实现对视频中不同区域的灵活感知与交互。作者设计了以人体视角为基础的视频生成流程，用于弥补3D数据的不足，并建立了新的RynnEC-Bench区域中心化评测基准。

Result: RynnEC模型在对象属性理解、对象分割和空间推理等多个具身认知任务上取得了最新性能。新提出的数据生成流程可有效扩充训练数据，RynnEC-Bench为多任务评测提供了支持。

Conclusion: RynnEC为具身智能体提供了一种区域中心、灵活感知的认知核心，对推动通用具身智能体发展、任务泛化具有促进作用。相关代码和模型已公开，便于进一步研究与应用。

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [5] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本文提出Deep Equilibrium Canonicalizer (DEC)，有效增强现有模型对局部尺度变化的建模能力，并提升模型在ImageNet等基准数据集上的表现。DEC模块可无缝结合各类深度网络，已在ViT、DeiT、Swin及BEiT等主流预训练模型上实证验证。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉任务中，同一类别的物体可能存在不同的尺寸，物体与摄像机距离也会影响其在图像中的显示尺寸，这些尺度变化在单张图片中是局部发生且复杂的，对模型准确识别造成很大挑战。现有大多数方法对尺度变化的建模能力有限，因此亟需一种通用、高效的方法来提升模型的局部尺度等变性。

Method: 作者提出Deep Equilibrium Canonicalizer (DEC)，作为一种独立模块，可嵌入并适配于当前流行的网络架构。DEC目标在于提升网络对物体局部尺度变化的等变性（即模型特征随着尺度变化能够相应调整），并且不需要从头训练，可直接加入到预训练模型中。

Result: 在ImageNet等主流数据集上，DEC集成于四种预训练深度神经网络（ViT、DeiT、Swin、BEiT）后，均取得了更高的分类性能和更强的局部尺度一致性。

Conclusion: DEC作为一个通用并易于集成的模块，能明显提升深度学习模型对局部尺度变化的鲁棒性和建模能力，在实际任务中有较强的实用价值。

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [6] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 作者提出了一种新的对称性检测方法CLIPSym，通过结合CLIP模型与旋转等变解码器，有效提升了图像对称性检测的效果。


<details>
  <summary>Details</summary>
Motivation: 对称性是计算机视觉中基本且重要的几何特征，但自动检测对称性一直具有挑战性。作者希望利用CLIP等大规模视觉-语言预训练模型中的语义信息，改善传统方法在对称性检测上的表现。

Method: 提出CLIPSym方法，结合CLIP的图像和语言编码器，以及融合Transformer与G-卷积的旋转等变解码器。同时提出了语义感知提示分组（SAPG）技术，通过聚合丰富的基于物体的提示，充分挖掘语言语义信息以优化对称性检测。

Result: 在三个主流数据集（DENDI、SDRW、LDRS）上，CLIPSym都超越了现有对称性检测的最优方法。还通过消融实验验证了CLIP预训练、等变解码器和SAPG技术的有效性。

Conclusion: CLIPSym有效利用了视觉-语言模型的多模态能力，通过结构创新，在图像对称性检测任务中获得了优异表现，为利用预训练模型和多模态提示推进几何视觉任务提供了新思路。

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [7] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 本文对视频异常检测（Video Anomaly Detection, VAD）领域进行了系统性的综述，涵盖监督程度、学习范式及应用场景，总结了当前方法的贡献与不足，并为后续理论和实际系统发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测作为计算机视觉的重要任务，与安防、交通、自动化等多个领域密切相关。尽管深度学习推动了该领域发展，但不同领域和学习范式下仍然缺乏统一的系统总结。因此，作者希望通过全面整理各类文献，帮助学者获得全局性理解，推动VAD理论与应用的进一步进展。

Method: 作者采用了系统综述的方法，对VAD文献进行了全面梳理，涵盖不同的监督方式（如全监督、弱监督、无监督等）及自适应学习方法（包括在线学习、主动学习、持续学习）。同时，将VAD应用按场景划分为以人为中心、以车辆为中心和以环境为中心三大类，归纳了每类应用的特点、挑战及设计考量。

Result: 本文分析并总结了当前VAD各类方法的基本贡献和主要局限性。通过跨领域的整合，展示了VAD研究的整体视野以及各类方法在不同场景下的表现。

Conclusion: 通过本综述，作者为研究者提供了系统的研究脉络和现有挑战汇总，为推进VAD理论研究与实际应用提供了有力参考，同时也指出了亟需解决的关键科学与工程问题。

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [8] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 本文提出了结合图卷积网络（GCN）与Voronoi图的新型图像分类框架，并展示了其在复杂场景和细粒度类别上的优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理复杂关系结构的数据时存在局限性，现有图像分类方法难以充分表达区域间的关系，因此需要新的分析范式。

Method: 将图像像素或区域作为图的顶点，通过Voronoi图和Delaunay三角剖分进行简化表示，并提出了归一化Voronoi图卷积网络（NVGCN）用于图像分类。

Result: 该方法在多个基准数据集上，预处理速度和分类准确率均优于现有主流模型，特别适用于复杂场景和细粒度类别任务。

Conclusion: GCN与Voronoi图的结合为图像分类提供了新思路，所提NVGCN网络不仅提升了效率，也为图学习方法在计算机视觉和非结构化数据领域扩展提供了可能。

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [9] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练机制，通过解决视觉和文本顺序重建问题，来提升大规模多模态模型（LMM）的稳健性、推理能力与跨模态对齐能力，并在多个基准任务上取得了最先进表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在视觉与文本特征对齐和泛化能力方面仍存在局限，难以应对鲁棒性和复杂推理场景。为此，作者希望通过改进训练机制，提升LMM的鲁棒对齐和理解能力。

Method: 引入两项新任务：图像顺序重建和文本顺序重建，将其融入LMM的预训练与微调阶段。同时，提出定向token方法以更好捕捉视觉和文本知识，并进一步引入图像到回答的引导损失，用以增强模型的视觉理解能力。

Result: 所提方法在学术界常用的多模态任务型与指令跟随型LMM基准测试中，均取得了优于以往模型的SoTA（最先进）成绩，显示出鲁棒性和多模态对齐能力的提升。

Conclusion: 通过重建图文顺序及引导损失，显著提升了LMM的推理能力、视觉理解与跨模态对齐，验证了所提机制的有效性。

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [10] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 研究探讨了不同数据增强方法对糖尿病视网膜病变（DR）分级的深度学习模型中，基于保形预测（Conformal Prediction, CP）不确定性量化效果的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然在医学影像分析如DR分级中表现出高准确率，但其临床应用受限于缺乏可靠的不确定性量化机制。而CP能为模型预测集提供统计覆盖保证。然而，常见的数据增强训练策略对CP统计保证的实际影响尚不清晰。

Method: 作者采用DDR数据集，对比ResNet-50和CoaT两种主干网络，在五种数据增强策略（无增强、几何变换、CLAHE、Mixup、CutMix）下训练模型，并系统分析各方法对CP相关指标（经验覆盖度、平均预测集大小、正确效率）的影响。

Result: Mixup和CutMix样本混合增强方法，不仅提升了模型准确率，还带来更稳健和高效的不确定性估计；而如CLAHE等增强方式会削弱模型对于不确定性的把控能力。

Conclusion: 数据增强策略应与下游不确定性量化机制协同设计，才能构建真正值得信赖的医学影像智能系统。

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [11] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 本文提出了一种结合条件扩散模型的新方法，实现了3D牙科CBCT影像的高精度合成，并可精细控制牙齿的在位与配置。结果显示，该方法在多项任务中均呈现高度真实与泛化能力，相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有牙科CBCT三维合成方法难以做到细致、可控，限制了临床中的个性化需求。因此，开发可细粒度控制、真实还原口腔结构的生成方法，满足外科规划、患者交流和AI增强等场景具有重要意义。

Method: 方法基于条件扩散框架，利用牙齿级别二值属性引导生成过程。采用小波去噪扩散、FiLM条件化及遮罩损失函数，确保学习聚焦于相关解剖结构，实现了对牙齿增减与完整牙列重建等多种任务的支持。

Result: 在包括牙齿增删、全口重建等多任务中，该模型均获得了低FID、高SSIM（>0.91）和强泛化性能，对未见过的数据依然展现出优异的真实感和重绘能力。

Conclusion: 本文方法有效实现了牙列的超真实、局部可控生成，无需重复扫描即可为手术规划、患者沟通及数据增强提供新工具，为牙科影像AI流程带来新机遇。

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [12] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA提出了一种结合3D高斯铺展（3DGS）和自监督对比学习的新方法，实现了对2D图像场景的细粒度、开放词汇3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 以往方法难以从2D图像中获取细致且能理解语义的3D表示，尤其是在开放词汇场景下，对不同对象类别进行区分和查询存在困难。

Method: 提出GALA框架，使用自监督对比学习来萃取场景特定的3D实例特征。核心创新点在于引入了包含两个可学习码本的跨注意力模块，用于编码视角无关的语义嵌入，不仅提升了特征表达效率，还支持2D和3D的开放词汇查询，同时减少了显存消耗。

Result: 在多个真实世界数据集上，GALA展现了优异的2D和3D开放词汇查询与理解能力。

Conclusion: GALA突破了现有方法在3D场景开放词汇理解上的瓶颈，实现了高效、低内存、语义丰富的3D表示。

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [13] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: 本文提出了一个多理由可解释目标识别基准数据集，并引入了一种无需训练的对比条件推断（CCI）框架，实现了更准确的分类与理由生成，推动了可解释目标识别技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等视觉-语言模型在可解释目标识别上依赖基于提示的条件生成，但其文本编码器受限，难以良好生成解释性理由，且以往数据集只含少量、噪声较大的单一理由，无法反映图像判别特征的多样性。因此，需要更强的基准与方法提升该领域的理论与实际表现。

Method: 构建了含多条真实理由标注的新数据集，并设计了更全面的评价指标。同时，提出了对比条件推断（CCI）框架，直接建模图像嵌入、类别标签和理由间的概率关系，无需训练，即可实现基于理由的目标预测。

Result: 新方法在多理由可解释目标识别基准数据集上取得了最优分类准确率和理由生成质量，零样本表现也很强，刷新了领域内的基准。

Conclusion: 多理由基准和CCI方法共同为可解释目标识别提供了更全面的评价与研究框架，推动了下一代模型的开发。代码将公开发布，便于学术和工业界采纳。

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [14] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: 本文提出了一种名为OccluNet的深度学习模型，能够通过分析数字减影血管造影（DSA）序列自动检测急性缺血性脑卒中（AIS）治疗中的血管堵塞，显著优于以往单帧检测方法。


<details>
  <summary>Details</summary>
Motivation: 急性缺血性脑卒中治疗过程中，及时、准确地检测血管堵塞位置对手术成败至关重要。但由于DSA影像序列解读困难、时间紧迫，人工检测存在局限，因此亟需自动化、智能化的检测方法。

Method: 作者设计了结合YOLOX目标检测和transformer时序注意力机制的OccluNet模型，实现对DSA序列血管堵塞的时空特征建模。分别尝试了纯时序注意力和分时空注意力两种变体，并与YOLOv11（单帧或投影）模型做了对比。

Result: 在MR CLEAN数据集上，OccluNet取得89.02%的准确率和74.87%的召回率，两种时空注意力方式表现接近且明显优于传统方法。

Conclusion: OccluNet能有效、自动地检测DSA序列中的血管堵塞，提升了临床检测的准确性与效率，对缺血性卒中介入治疗具有重要意义。

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [15] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: 本文提出了Pixels2Play-0.1（P2P0.1）模型，能够以类人方式玩各种3D电子游戏，无需针对具体游戏进行大量工程调整。


<details>
  <summary>Details</summary>
Motivation: 随着AI助手、可控NPC、个性化主播和自动化测试等新兴需求的增长，研究人员希望开发一种能够仅凭像素流（即玩家所见画面）进行泛化、无需针对特定游戏大幅工程调整的游戏AI。

Method: P2P0.1采用端到端行为克隆训练，结合带动作标签的人工演示数据和通过逆动力学模型推断的公开无标签视频动作数据。模型结构为自回归解码器Transformer，能够高效处理大动作空间，并易于在单张消费级GPU上部署。

Result: 实验结果表明，P2P0.1能在Roblox等简单3D游戏及经典MS-DOS游戏中表现出合格的游戏能力。通过消融实验探讨无标签数据的作用，并提出进一步提升模型到专家水平及文本控制的扩展方向。

Conclusion: P2P0.1展示了凭像素流泛化到多款游戏的可行性，是向通用、类人游戏AI迈步的重要进展。

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [16] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态多视角视频生成方法，实现对自动驾驶城市场景的高保真多模态视频生成，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视频生成多侧重于RGB视频，仅凭单一模态难以实现对复杂城市场景的全面理解。多模态（如深度、语义分割）对于辅助理解至关重要，但现有方法要么只支持单一模态，要么需多个模型协作，增加了部署难度。

Method: 提出统一的扩散Transformer模型，将模态共享和模态特异性组件结合。利用丰富的条件输入对场景结构和内容进行可控编码，从而统一实现多模态、多视角自动驾驶视频生成。

Result: 在自动驾驶真实数据集nuScenes上，所提出方法能够生成高保真、可控的多模态多视角城市场景视频，并且效果优于当前最先进的方法。

Conclusion: 该工作为自动驾驶领域多模态视频生成提供了统一、有效的解决方案，有望提升城市场景的综合感知和自动驾驶系统的安全性。

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [17] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: 本论文提出了一种面向单阶段多目标检测任务的新型损失函数（跨类别关系损失，ICR），有效提升小目标（如车牌）检测的性能，并发布了一个小型车辆多车牌数据集（SVMLP）。该方法可无缝集成进现有IoU损失，对高质量检测有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统IoU损失在训练时对小目标检测的梯度更新非常有限，尤其在多目标场景中小目标（如车牌）的学习更弱。为解决小目标检测梯度饱和和梯度不足的问题，作者提出结合目标间空间关系提升小目标检测性能。

Method: 作者设计了一种跨类别关系损失函数，通过惩罚小目标（如车牌）检测框没有合理位置关系（如没有被包含在对应车辆检测框内）的情形，不仅利用IoU，还引入了目标间的空间对应关系，实现梯度引导。同时，结合实际场景制作并开源了新的小型车辆多车牌数据集。

Result: 所提损失函数能简单融入YOLOv12-T与UAV-DETR等现有检测框架，并无须进一步调参，即可分别带来10.3%和1.6%的mAP提高。

Conclusion: 论文提出的跨类别关系损失法可提升现有单阶段多目标检测模型对小目标的检测效果，尤其在车牌检测等应用中无需复杂改动即可带来明显提升。同时公开的新数据集丰富了相关基准资源，为后续研究奠定基础。

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [18] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: 本文提出一种基于CMLPe的轻量级手语生成模型，并采用合成数据预训练方法，有效提升了手语识别的准确率，在LSFB和DiSPLaY数据集上刷新了最新识别效果。


<details>
  <summary>Details</summary>
Motivation: 手语识别模型因缺乏足够的训练数据而性能受限，如何利用有限数据提升识别效果是该领域的重要挑战。

Method: 提出基于CMLPe的手语生成模型，通过合成数据进行预训练，并与现有增强方法结合，搭配Mamba-SL及Transformer-SL分类器进行识别。

Result: 在LSFB和DiSPLaY数据集上取得了新的最优成绩。合成数据预训练在部分情境下优于传统数据增强方法，且二者结合效果互补。

Conclusion: 合成数据预训练和高效的手语生成模型能够广泛提升手语识别性能，为手语识别数据扩充与模型效率带来了实用且有效的解决方案。

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [19] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: 本文提出了一个用于监测Taxol药物处理下C6胶质瘤细胞的显微图像数据集，并提出了基于ResNet-50和注意力机制的分类基线方法（ResAttention-KNN），以实现在细胞水平上对Taxol浓度的自动分析。


<details>
  <summary>Details</summary>
Motivation: 目前的Taxol药物作用检测方法昂贵、费时且不适合高通量和实时分析，且缺乏公开可用的用于该药物暴露下细胞形态自动分析的数据集。

Method: 作者建立了一个新显微图像数据集，涵盖不同Taxol浓度下的C6胶质瘤细胞，并提出ResAttention-KNN模型（结合ResNet-50主干、卷积块注意力模块和KNN分类器），用于对图像中的Taxol浓度进行分类。

Result: 提出的数据集和ResAttention-KNN基线模型均已公开发布，用于自动化、高通量和可复现的细胞形态分析。模型结合了注意力机制及非参数分类，提升了稳健性和可解释性。

Conclusion: 该工作为基于视觉的生物医学分析提供了重要资源和基线方法，有助于未来相关领域的研究发展。

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [20] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的仅基于深度数据的类别级物体6D位姿估计方法（HRC-Pose），通过对点云实施对比学习，实现更好的一致性和泛化能力，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有类别级物体6D位姿估计方法，基本只用6D位姿监督信号，未明确建模位姿的连续性，导致对新颖姿态泛化能力不强、预测不一致。

Method: HRC-Pose方法引入对比学习，设计了一套6D位姿感知的分层排序机制，在多任务多类别情境下，通过兼顾旋转、平移、类别知识的信息，对点云进行对比学习，使网络学会保持位姿连续性的特征空间。同时，将物体位姿分解为旋转与平移，分别进行编码与后续处理。

Result: 在REAL275与CAMERA25公开基准数据集上，HRC-Pose有优于现有仅用深度数据的主流方法的表现，并可实现实时推理。

Conclusion: HRC-Pose能够成功学习到保持位姿连续性的点云特征空间，提升了类别级物体位姿估计的一致性与泛化能力，适用于实际应用场景。

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [21] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的可控情感的说话人脸生成方法，通过解耦音频与视频情感因素，并引入情感锚点，显著提升了生成视频的真实度与情感表现力。


<details>
  <summary>Details</summary>
Motivation: 现有说话人脸生成方法在情感控制和保留个体特征方面效果有限。为生成能真实反映多模态情感关系且身份保持良好的情感说话人脸视频，亟需设计更有效的多模态关系建模与利用机制。

Method: 1）用两种预训练方式将音频解耦成独立成分，并将视频量化为视觉tokens；2）提出情感锚点（EA）表征，将情感信息融入视觉tokens中；3）设计自回归Transformer，建模带条件的视觉token分布，用于生成带目标情感的视频索引序列。

Result: 在MEAD数据集上进行了实验，方法可根据不同情感音频控制视频情感，实验结果在定性和定量指标上都优于现有方法。

Conclusion: 本文方法能有效实现情感可控的说话人脸生成，并在身份保持和情感表达方面取得显著提升，对于推动多模态情感生成领域具有重要意义。

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [22] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: 该论文提出了一种通用多目标跟踪框架，尤其适用于在复杂交通场景下的车辆跟踪，并证明其在车辆和行人数据集上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪方法主要针对行人，通用性差，难以扩展到车辆等其他类别，因此需要能适应多类别目标、尤其是复杂交通环境的跟踪方法。

Method: 该方法包含两个核心组件：（1）感知遮挡的再识别机制，提升强遮挡目标的身份保持能力；（2）基于道路结构的轨迹优化策略，利用车道方向、人行道和道路边界等语义信息提升轨迹连续性和准确率。此外还构建了一个多样车辆类别及帧级标注数据集用于车辆跟踪评测。

Result: 实验证明，该方法在新引入的数据集以及多个公开基准上都实现了强劲的跟踪性能。其中在MOT17和MOT20测试集的HOTA评分分别为66.4和65.7。

Conclusion: 所提框架不仅适用于通用多类别目标跟踪，亦在传统行人跟踪任务上取得了竞争性性能，有力验证了其通用性和实际应用价值。

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [23] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的粗到细点云运动网络（TCFNet），用于正颌外科手术中面骨点云的模拟变形，能生成更精确的点位移动路径，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的生物力学模拟方法在正颌外科手术规划中存在计算耗时长、手工数据处理繁琐及准确率低的问题，而现有深度学习方法对于大规模点云的处理能力有限，感受野小致使预测点噪声大，还需复杂的预处理和后处理，限制其实际应用。

Method: 提出TCFNet：采用Transformer网络提取全局特征和粗配准，结合本地信息聚合网络（LIA-Net）细化点位移动路径，两阶段协同工作提升匹配精度。LIA-Net建模局部几何结构以补偿Transformer的邻域精度损失，并引入门控循环单元（GRU）融合全局指导。受医学图像配准启发，设计专家知识辅助损失用于关键器官点重建。

Result: 在收集的正颌面骨-面部点云数据集上，TCFNet在多项评估指标和可视化结果上均优于当前最新方法。

Conclusion: TCFNet同时兼顾全局和局部点云特征，提高了面骨变形模拟的性能，减少了对传统复杂预处理和后处理的依赖，具有更强的实用性和泛化能力。

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [24] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了一种名为QuadINR的高效隐式神经表示（INR）方法，利用分段二次激活函数在大幅降低硬件消耗的同时提升表现，尤其适用于高频信号的表达。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法通过复杂的激活函数减缓频谱偏置，但显著增加了硬件开销。需要一种同时兼顾表现与硬件效率的INR方案，特别是在嵌入式与资源受限场景下。

Method: 提出分段二次激活函数（piecewise quadratic AFs）并构建统一的N级流水线硬件实现框架，通过神经切线核（NTK）分析其高频信号表现力，并分别在FPGA（VCU128）和28nm ASIC上实现验证。

Result: QuadINR在图像和视频任务中，较之前方法PSNR提升最高可达2.06dB，芯片面积仅1914μm²，动态功耗6.14mW，资源和功耗最多降97%，延迟提升93%。

Conclusion: QuadINR在保证高表达能力的前提下，显著降低了硬件资源消耗和延迟，适用于对硬件高效性要求强的应用场景，是INR领域的有力创新。

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [25] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Img2ST-Net的新框架，可从常规组织学图像高效生成高分辨率空间转录组(ST)数据，显著提升ST数据预测的效率和精度，降低制备成本，并通过新评价指标增强了方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率空间转录组数据制备成本高且耗时，而现有基于逐点回归的AI方法在高分辨率下效率低下且不稳定。此外，高分辨率ST数据天生稀疏且表达水平低，进一步加大了推断与评价的难度。因此，亟需一种高效、健壮且适应高分辨率的平台辅助ST生成方法。

Method: 作者提出Img2ST-Net，一种全卷积结构的深度学习框架，将常规病理切片图像映射为高分辨率ST的超像素基因表达图。该方法采用并行推断而非传统的逐点推断，大幅提升效率，同时重构生物空间组织结构。为更好评估稀疏表达，该论文还引入基于结构相似度的SSIM-ST评价指标。

Result: Img2ST-Net在高分辨率ST数据生成任务中展示了高效、准确和稳健的性能，优于传统方法，并可适应更大规模和更高分辨的空间组学任务。

Conclusion: Img2ST-Net充分解决了高分辨率ST生成中的效率与精度难题，同时提出的SSIM-ST评价指标有效提升了分析的鲁棒性。该框架为新一代高分辨率空间转录组数据建模奠定了基础，具有良好的生物学解释性和应用前景。源码已开放。

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [26] [Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization](https://arxiv.org/abs/2508.14561)
*Sukhyun Jeong,Hong-Gi Shin,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 本文提出一种结合离散姿态编码和连续特征的新方法，有效提升了文本到3D人体动作生成的精度与可控性。实验表明该方法在细节捕捉与动作控制方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前的可控动作生成方法大多依赖离散姿态编码，但难以捕获细腻、复杂的高频动作细节，导致生成动作缺乏表现力，因此需要引入更细粒度的特征。

Method: 作者提出利用残差向量量化（RVQ）增强基于姿态编码的潜变量，将离散和连续特征结合起来，既保留了姿态编码的可解释性与可控性，又能精准刻画细节动作。

Result: 在HumanML3D数据集上的实验结果显示，所提模型的Frechet inception distance (FID)从0.041降至0.015，Top-1 R-Precision从0.508提升至0.510，改善了动作生成品质与与文本匹配度。定性分析也证明模型支持高质量动作编辑与控制。

Conclusion: 连续特征与离散姿态编码的融合能有效提升文本驱动的人体动作生成的表现力与可控性，为高质量的动作编辑提供了新范式。

Abstract: Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.

</details>


### [27] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Chinese Text Adapter-Flux（CTA-Flux）的新方法，使强大的英文文本到图像生成模型Flux能够更好地处理中文输入，提升中文语义与文化的理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型（如Flux）主要基于英文语料进行训练，其在处理非英文（如中文）提示时效果较差，主要由于数据集的英语主导性带来的语言与文化偏见。以往的翻译或微调方法常在语义与文化表达方面存在不足，影响图片的真实性和质量。

Method: CTA-Flux方法利用了MultiModal Diffusion Transformer（MMDiT），直接控制Flux主干模型，实现参数的显著减少，同时增强了对中文语义的掌握。与依赖ControlNet等大参数规模的方案不同，CTA-Flux无需大规模重训练，并且兼容现有的文本到图像插件。

Result: 实验证明，CTA-Flux能够同时支持中英文输入，生成图像的质量更高、视觉真实性更强，且能更忠实地表现中文语义和文化特征。

Conclusion: CTA-Flux有效提升了英文主导的生成模型对中文文本的处理能力，在保证高图像质量和文化真实性的同时，具有较高的兼容性和参数效率。

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [28] [Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels](https://arxiv.org/abs/2508.14767)
*Fabian Holst,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 本文提出了一种通过融合单目RGB图像与AIS数据，自动生成海上船舶6D位姿标注数据集的新方法，并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 现有AIS定位存在设备失效、数据操控与传输延迟等问题，单独依赖AIS难以获得准确的船只位姿数据，且人工标注6D位姿数据集成本高昂。

Method: 结合YOLOX-X目标检测网络检测的船只在图像中的位置与AIS消息，通过两种变换方法（单应变换和PnP）将AIS数据对齐到图像坐标，进而生成3D包围盒，获得船舶6D位姿。评估不同检测模型和空间变换方法的效果。

Result: YOLOX-X模型对于相关类别船只，在IoU阈值为0.5时的mAP为0.80。PnP方法的投影误差远低于传统的单应变换方法。

Conclusion: 提出的方法可自动生成带3D包围盒6D位姿注释的船舶数据集，无需人工标注。公开了新的BONK-pose数据集（3753张带3D标注图片）和一套1000张带2D框注释图片，可用于6D位姿估计网络的训练与评测。

Abstract: The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.

</details>


### [29] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出了一种新型去摩尔纹方法——Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former)，有效移除因相机与显示屏采样冲突导致的摩尔纹，性能优于现有去摩尔纹模型。


<details>
  <summary>Details</summary>
Motivation: 随着便携式成像技术的发展，屏幕拍摄变得极为普及，但相机色彩滤波器阵列和显示屏子像素的频率混叠导致严重的摩尔纹。这些摩尔纹影响照片和视频质量，而现有去摩尔纹方法在空间、结构、通道和时间一致性上存在不足。该论文旨在系统性解决这些问题。

Method: 提出MoCHA-former结构，包含两个核心模块：一是Decoupled Moiré Adaptive Demoiréing (DMAD)，通过摩尔纹解耦块（MDB）、细节解耦块（DDB）和摩尔纹条件块（MCB）分离内容与摩尔纹并进行自适应处理；二是Spatio-Temporal Adaptive Demoiréing (STAD)，通过空间融合块（SFB）和特征通道注意力（FCA）建模大尺度结构和通道特性，同时隐式实现帧间对齐以保证时序一致性。

Result: 在两个视频数据集（包括RAW和sRGB域）上对MoCHA-former进行了定性和定量评估。MoCHA-former在PSNR、SSIM和LPIPS等指标上均显著超过现有去摩尔纹方法，且能更好地处理空间、结构、通道和时序上的摩尔纹问题。

Conclusion: MoCHA-former能有效应对摄屏去摩尔纹的多种挑战，在视频恢复质量和一致性上优于以往方法，对实际便携成像场景有广泛应用价值。

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [30] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: 本文提出了“Virtual Community”平台，用于研究人机共存与社会协作，支持复杂仿真与多体智能体的交互。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术飞速发展，人类与机器人将在社会中共存，这将带来前所未有的机遇与挑战。作者希望探索未来人机如何在共享社区中有效互动与共处。

Method: 作者搭建了一个基于真实3D场景与物理引擎的开源虚拟社区仿真平台，提供多智能体相互作用的环境。设计了两类挑战任务（社区规划挑战与社区机器人挑战），以评估多体智能体的推理与协作能力。

Result: 作者定义并测试了一些基线方法，发现不论是高层任务规划还是低层协作控制，在开放世界环境下都存在重大挑战。

Conclusion: Virtual Community为大规模、开放世界下的人机共存与社会智能研究提供了新平台，期望促进该领域的深入探索。

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>


### [31] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: 本文提出HyperDiff方法，将扩散模型与HyperGCN结合，用于单目3D人体姿态估计，显著提升姿态估计的精度并兼顾计算效率。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿态估计在2D到3D的转换过程中容易受到深度不确定性和遮挡的影响，传统方法又常常忽视骨架结构的多尺度特征，导致估计精度降低。作者希望解决这些关键难点。

Method: 提出的HyperDiff方法融合了扩散模型和HyperGCN。扩散模型用于建模数据中的不确定性，有效缓解深度歧义与遮挡带来的问题；HyperGCN作为去噪器，通过多粒度结构对关节点间的高阶相关进行建模，提高对复杂姿态的还原能力。

Result: 在Human3.6M和MPI-INF-3DHP数据集上的实验显示，HyperDiff方法取得了新的最优效果（state-of-the-art），并能适应不同的计算资源，在精度与效率之间灵活平衡。

Conclusion: HyperDiff不仅改善了深度歧义和遮挡下的3D姿态估计，还兼顾了准确率和资源效率，在相关公开数据集上表现优异，具有较强应用前景。

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [32] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为FOCUS的新颖频域调控方法，通过扩散模型引导的输入自适应，实现模型在测试时对领域变化的自适应，同时最大程度保留与任务相关的语义信息。


<details>
  <summary>Details</summary>
Motivation: 测试时领域自适应方法面临的主要挑战是如何在适应新领域分布的同时避免遗忘原有的任务知识，尤其在领域转移显著时容易出现灾难性遗忘。针对这一问题，作者提出利用频域信息调控扩散过程，以保留关键语义内容。

Method: 提出了一种频域调控的扩散输入自适应方法（FOCUS），其中包括一个轻量的Y型频率预测网络（Y-FPN），可从噪声图像中解耦高低频信息。Y-FPN通过新颖的数据增强方法FrequencyMix训练，能打散不同频段，从而提升模型对多样扰动的鲁棒性。该方法通过在扩散反向去噪过程注入频域先验，引导模型保护关键语义。

Result: FOCUS在语义分割和单目深度估计任务上，跨越三大数据集、15类腐蚀类型，均实现了平均性能的最新最优。同时，FOCUS可为现有方法提供伪标签，作为额外监督信号，有效缓解自适应过程的灾难性遗忘问题。

Conclusion: FOCUS不仅提升了自身在受扰环境下的适应与预测性能，还能作为通用增强组件用于现有适应方法，有助于保留关键知识、提升模型整体鲁棒性。

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


### [33] [MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](https://arxiv.org/abs/2508.14440)
*Fei Peng,Junqiang Wu,Yan Li,Tingting Gao,Di Zhang,Huiyuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型MUSE，实现了多主体布局可控的文本到图像生成，在空间精确性和主体身份还原方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像扩散模型虽然生成质量很高，但在多个主体的精确空间布局控制和身份保持方面仍有明显不足。现有方法无法同时兼顾布局精度和主体一致性，因此需要一种新方法解决该复合性任务。

Method: 作者提出了MUSE框架，利用级联交叉注意力（CCA）机制，把布局信息和文本引导结合起来，通过语义空间扩展实现空间与文本的联合控制。同时，设计了两阶段渐进式训练策略，将任务分解为可学习的子任务，更易于优化。

Result: 实验结果显示，MUSE在零样本、端到端的图像生成过程中，相比现有方法展现了更高的空间精度和主体身份一致性。

Conclusion: MUSE推动了可控图像合成的前沿发展，实现了多主体、高精度、身份一致的布局可控文本到图像生成。研究代码已开源。

Abstract: Existing text-to-image diffusion models have demonstrated remarkable
capabilities in generating high-quality images guided by textual prompts.
However, achieving multi-subject compositional synthesis with precise spatial
control remains a significant challenge. In this work, we address the task of
layout-controllable multi-subject synthesis (LMS), which requires both faithful
reconstruction of reference subjects and their accurate placement in specified
regions within a unified image. While recent advancements have separately
improved layout control and subject synthesis, existing approaches struggle to
simultaneously satisfy the dual requirements of spatial precision and identity
preservation in this composite task. To bridge this gap, we propose MUSE, a
unified synthesis framework that employs concatenated cross-attention (CCA) to
seamlessly integrate layout specifications with textual guidance through
explicit semantic space expansion. The proposed CCA mechanism enables
bidirectional modality alignment between spatial constraints and textual
descriptions without interference. Furthermore, we design a progressive
two-stage training strategy that decomposes the LMS task into learnable
sub-objectives for effective optimization. Extensive experiments demonstrate
that MUSE achieves zero-shot end-to-end generation with superior spatial
accuracy and identity consistency compared to existing solutions, advancing the
frontier of controllable image synthesis. Our code and model are available at
https://github.com/pf0607/MUSE.

</details>


### [34] [Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2508.14443)
*Gyusam Chang,Tuan-Anh Vu,Vivek Alumootil,Harris Song,Deanna Pham,Sangpil Kim,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 本文提出了NIRPlant多模态数据集和NIRSplat新方法，利用近红外图像、RGB、文本信息、深度和激光雷达数据提升农业场景的3D重建能力，在多项挑战性任务中显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 尽管3D Gaussian Splatting（3DGS）在三维重建领域发展迅速，但在农业应用中研究较少。农业场景作为研究对象，存在光照不均、遮挡多、可视角度有限等挑战现有3D重建方法的问题，急需新的数据集和方法提升该领域表现。

Method: 作者首先构建了包含近红外（NIR）、RGB、文本元数据、深度和激光雷达在多种光照条件下采集的多模态数据集NIRPlant。方法上，提出了NIRSplat架构，通过结合3D点位置信息编码与多模态交叉注意力机制，有效融合各类多模态特征，以增强3D重建的鲁棒性和植物生理信息获取能力。

Result: 实验结果表明，NIRSplat在农业复杂场景下，明显优于主流3D重建方法（如3DGS、CoR-GS、InstantSplat），在重建效果和相关应用指标上取得更优成绩。

Conclusion: 本研究通过整合近红外和多模态数据，显著提升了农业场景下3D重建的鲁棒性与准确性，为农业数字化和智能化管理提供了有力工具。相关数据与代码已公开发布。

Abstract: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR

</details>


### [35] [Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448)
*Yangche Yu,Yin Chen,Jia Li,Peng Jia,Yu Zhang,Li Dai,Zhenzhen Hu,Meng Wang,Richang Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为DAPA的框架，通过域自适应和并行注意力机制，有效提升了对话参与度建模的泛化能力，并在多个跨文化、跨语言数据集上取得新SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互系统中，准确的参与度估计对于自适应响应至关重要，但现有方法泛化性差，难以适用于不同领域或文化背景，且在建模复杂交互动态方面存在挑战。

Method: 提出DAPA框架，包括（1）域提示机制——在输入前添加可学习的域特定向量，使模型显式感知数据来源，促进领域适应同时保持通用的参与度表征；（2）并行交叉注意力模块——显式对齐参与者间的反应性（前向BiLSTM）与预期性（后向BiLSTM）状态，捕捉交互同步性。

Result: 在多个跨文化和跨语言的评测基准（如NoXi-J）上，DAPA取得了Concordance Correlation Coefficient（CCC）的显著提升（相较强基线提升0.45），并在MultiMediate'25多领域参与度估计挑战赛中获得第一名。

Conclusion: DAPA框架有效解决了参与度估计中的领域泛化和交互动态建模难题，极大提升了跨域、跨文化场景中的应用潜力，并为相关任务提供了新基线。

Abstract: Accurate engagement estimation is essential for adaptive human-computer
interaction systems, yet robust deployment is hindered by poor generalizability
across diverse domains and challenges in modeling complex interaction
dynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel
Attention), a novel framework for generalizable conversational engagement
modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable
domain-specific vectors to the input, explicitly conditioning the model on the
data's origin to facilitate domain-aware adaptation while preserving
generalizable engagement representations. To capture interactional synchrony,
the framework also incorporates a Parallel Cross-Attention module that
explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)
states between participants.Extensive experiments demonstrate that DAPA
establishes a new state-of-the-art performance on several cross-cultural and
cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45
in Concordance Correlation Coefficient (CCC) over a strong baseline on the
NoXi-J test set. The superiority of our method was also confirmed by winning
the first place in the Multi-Domain Engagement Estimation Challenge at
MultiMediate'25.

</details>


### [36] [D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis](https://arxiv.org/abs/2508.14449)
*Yuhang Guo,Kaijun Deng,Siyang Song,Jindong Xie,Wenhui Ma,Linlin Shen*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D说话人合成方法D^3-Talker，通过分离音频与面部运动信号，更好地拟合个性化和通用变形，并采用多阶段优化和对比损失函数提升唇形同步和图像质量，在有限训练数据下效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D说话人生成面临每个新身份都需大量视频训练的挑战，少量数据时唇形同步和影像质量难以兼顾。现有做法难以充分从有限帧数据中学习个性化唇动，常出现同步误差和模糊。

Method: 提出D^3-Talker模型，静态3D高斯属性场基础上，分别利用音频与面部运动信号控制两组高斯属性变形场，实现通用与个性化变形的解耦。预训练时引入创新的对比损失函数加强解耦并提升表现。加入由粗到细的渲染模块优化运动时图像清晰度。

Result: 大量实验显示，在有限训练帧下，D^3-Talker在高保真渲染和音频-唇同步准确性上均超过最新方法，视觉效果与同步性显著提升。

Conclusion: D^3-Talker有效解决了少量数据下唇形同步差和成像模糊的问题，是推动3D说话人生成技术实用化的重要进展，具有广阔的应用前景。

Abstract: A key challenge in 3D talking head synthesis lies in the reliance on a
long-duration talking head video to train a new model for each target identity
from scratch. Recent methods have attempted to address this issue by extracting
general features from audio through pre-training models. However, since audio
contains information irrelevant to lip motion, existing approaches typically
struggle to map the given audio to realistic lip behaviors in the target face
when trained on only a few frames, causing poor lip synchronization and talking
head image quality. This paper proposes D^3-Talker, a novel approach that
constructs a static 3D Gaussian attribute field and employs audio and Facial
Motion signals to independently control two distinct Gaussian attribute
deformation fields, effectively decoupling the predictions of general and
personalized deformations. We design a novel similarity contrastive loss
function during pre-training to achieve more thorough decoupling. Furthermore,
we integrate a Coarse-to-Fine module to refine the rendered images, alleviating
blurriness caused by head movements and enhancing overall image quality.
Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art
methods in both high-fidelity rendering and accurate audio-lip synchronization
with limited training data. Our code will be provided upon acceptance.

</details>


### [37] [Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](https://arxiv.org/abs/2508.14461)
*Shanlin Sun,Yifan Wang,Hanwen Zhang,Yifeng Xiong,Qin Ren,Ruogu Fang,Xiaohui Xie,Chenyu You*

Main category: cs.CV

TL;DR: 本文提出了Ouroboros框架，由两个单步扩散模型组成，实现了前向与逆渲染的相互增强，提升了一致性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有多步扩散模型虽推动了前向与逆渲染的发展，但常将二者独立处理，导致结果不一致且推理速度较慢，亟需一种高效统一的方案。

Method: Ouroboros框架结合了两个单步扩散模型，分别用于前向和逆渲染。该框架扩展了室内外场景的内在分解能力，并引入循环一致性机制，确保前向与逆渲染结果相互匹配。

Result: 实验显示，Ouroboros在多种场景下达到最新性能，同时推理速度显著优于其它扩散模型。此外，该框架可无训练直接应用于视频分解，减少时序不一致，同时保持高质量逐帧逆渲染。

Conclusion: Ouroboros实现了高效且一致的前向与逆渲染，提升了适用性与速度，并改善了视频等复杂任务下的表现。

Abstract: While multi-step diffusion models have advanced both forward and inverse
rendering, existing approaches often treat these problems independently,
leading to cycle inconsistency and slow inference speed. In this work, we
present Ouroboros, a framework composed of two single-step diffusion models
that handle forward and inverse rendering with mutual reinforcement. Our
approach extends intrinsic decomposition to both indoor and outdoor scenes and
introduces a cycle consistency mechanism that ensures coherence between forward
and inverse rendering outputs. Experimental results demonstrate
state-of-the-art performance across diverse scenes while achieving
substantially faster inference speed compared to other diffusion-based methods.
We also demonstrate that Ouroboros can transfer to video decomposition in a
training-free manner, reducing temporal inconsistency in video sequences while
maintaining high-quality per-frame inverse rendering.

</details>


### [38] [DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing](https://arxiv.org/abs/2508.14465)
*Weitao Wang,Zichen Wang,Hongdeng Shen,Yulei Lu,Xirui Fan,Suhui Wu,Jun Zhang,Haoqian Wang,Hao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DreamSwapV的视频主体交换框架，通过用户指定的掩码和参考图像，可对任意视频中的任意主体进行交换，实现高度定制化的视频编辑，实验证明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前视频定制编辑需求快速增长，主体交换作为关键环节受到关注。而现有方法多局限于特定场景或依赖不直接且不精准的编辑方式，难以满足通用、高保真的主体交换需求。

Method: 提出了DreamSwapV框架，采用掩码引导，支持任何视频中任意主体的交换。创新点包括多条件融合模块以实现细粒度指导，以及自适应掩码策略以适应不同规模和属性的主体。此外，构建了两阶段数据集和训练流程以提升模型效果。

Result: DreamSwapV在VBench指标和作者提出的新DreamSwapV-Benchmark上经过综合实验测试，结果优于现有主体交换方法。

Conclusion: DreamSwapV框架实现了灵活、高保真的通用视频主体交换，为视频定制编辑提供了强有力的工具，并在多个基准上取得了领先表现。

Abstract: With the rapid progress of video generation, demand for customized video
editing is surging, where subject swapping constitutes a key component yet
remains under-explored. Prevailing swapping approaches either specialize in
narrow domains--such as human-body animation or hand-object interaction--or
rely on some indirect editing paradigm or ambiguous text prompts that
compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided,
subject-agnostic, end-to-end framework that swaps any subject in any video for
customization with a user-specified mask and reference image. To inject
fine-grained guidance, we introduce multiple conditions and a dedicated
condition fusion module that integrates them efficiently. In addition, an
adaptive mask strategy is designed to accommodate subjects of varying scales
and attributes, further improving interactions between the swapped subject and
its surrounding context. Through our elaborate two-phase dataset construction
and training scheme, our DreamSwapV outperforms existing methods, as validated
by comprehensive experiments on VBench indicators and our first introduced
DreamSwapV-Benchmark.

</details>


### [39] [LookOut: Real-World Humanoid Egocentric Navigation](https://arxiv.org/abs/2508.14466)
*Boxiao Pan,Adam W. Harley,C. Karen Liu,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: 本文提出了一种预测未来6D头部姿态的前瞻性方法，并公开了新的真实场景导航数据集，有效提升了基于第一人称视频的人体导航行为建模能力。


<details>
  <summary>Details</summary>
Motivation: 在机器人、虚拟现实/增强现实和辅助导航等领域，对于能自适应环境并安全规避碰撞的未来路径预测至关重要。然而，如何基于第一视角视频预测头部运动并学习蕴含的主动信息收集行为仍具挑战，且缺乏高质量训练数据。

Method: 提出了一种利用时序聚合的3D潜特征的预测框架，通过3D几何与语义信息联合建模，实现对环境中静态和动态部分的约束推理。此外，作者基于Project Aria眼镜搭建了数据采集流程，构建了4小时长的Aria Navigation Dataset (AND)新数据集。

Result: 实验表明，所提模型不仅能预测头部平移与旋转，捕捉如等待、减速、绕行、观察路况等人类典型导航行为，还能泛化到未见过的新环境。

Conclusion: 该工作为真实世界第一视角导航提供了有效方法和数据资源，推动了人性化行为建模的发展，并为相关领域后续研究奠定了基础。

Abstract: The ability to predict collision-free future trajectories from egocentric
observations is crucial in applications such as humanoid robotics, VR / AR, and
assistive navigation. In this work, we introduce the challenging problem of
predicting a sequence of future 6D head poses from an egocentric video. In
particular, we predict both head translations and rotations to learn the active
information-gathering behavior expressed through head-turning events. To solve
this task, we propose a framework that reasons over temporally aggregated 3D
latent features, which models the geometric and semantic constraints for both
the static and dynamic parts of the environment. Motivated by the lack of
training data in this space, we further contribute a data collection pipeline
using the Project Aria glasses, and present a dataset collected through this
approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4
hours of recording of users navigating in real-world scenarios. It includes
diverse situations and navigation behaviors, providing a valuable resource for
learning real-world egocentric navigation policies. Extensive experiments show
that our model learns human-like navigation behaviors such as waiting / slowing
down, rerouting, and looking around for traffic while generalizing to unseen
environments. Check out our project webpage at
https://sites.google.com/stanford.edu/lookout.

</details>


### [40] [Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://arxiv.org/abs/2508.14483)
*Haoran Bai,Xiaoxu Chen,Canqian Yang,Zongyao He,Sibin Deng,Ying Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Vivid-VR的DiT基础生成式视频修复方法，结合了ControlNet以增强内容一致性，并通过创新训练策略和架构设计，显著提升了纹理与时序表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于T2V的视频生成模型在可控性调整时常因多模态对齐不完善导致分布漂移，影响纹理真实感与时序连贯性。本论文旨在解决这一控制过程中的质量与一致性下降问题。

Method: 提出Vivid-VR方法，融合ControlNet以提升视频内容一致性。应用概念蒸馏训练策略，将预训练的T2V模型用于合成嵌入文本概念的训练样本，实现视频纹理和时序质量的知识迁移。还设计了两部分控制结构：一是control feature projector过滤退化伪影，二是双分支的ControlNet connector联用MLP特征映射和交叉注意力以动态控制特征检索和信号调节。

Result: 在多种合成及实世界基准测试和AIGC视频上，Vivid-VR均优于现有方法，在纹理真实性、视觉生动性和时间一致性等方面取得了显著提升。

Conclusion: Vivid-VR有效缓解了现有视频修复模型因控制调整带来的质量损失问题，实现了可控性与高质量视频生成的统一。

Abstract: We present Vivid-VR, a DiT-based generative video restoration method built
upon an advanced T2V foundation model, where ControlNet is leveraged to control
the generation process, ensuring content consistency. However, conventional
fine-tuning of such controllable pipelines frequently suffers from distribution
drift due to limitations in imperfect multimodal alignment, resulting in
compromised texture realism and temporal coherence. To tackle this challenge,
we propose a concept distillation training strategy that utilizes the
pretrained T2V model to synthesize training samples with embedded textual
concepts, thereby distilling its conceptual understanding to preserve texture
and temporal quality. To enhance generation controllability, we redesign the
control architecture with two key components: 1) a control feature projector
that filters degradation artifacts from input video latents to minimize their
propagation through the generation pipeline, and 2) a new ControlNet connector
employing a dual-branch design. This connector synergistically combines
MLP-based feature mapping with cross-attention mechanism for dynamic control
feature retrieval, enabling both content preservation and adaptive control
signal modulation. Extensive experiments show that Vivid-VR performs favorably
against existing approaches on both synthetic and real-world benchmarks, as
well as AIGC videos, achieving impressive texture realism, visual vividness,
and temporal consistency. The codes and checkpoints are publicly available at
https://github.com/csbhr/Vivid-VR.

</details>


### [41] [WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification](https://arxiv.org/abs/2508.14486)
*Toqi Tahamid Sarker,Khaled R Ahmed,Taminul Islam,Cristiana Bernardi Rankrape,Karla Gage*

Main category: cs.CV

TL;DR: 本文提出了WeedSense，一个多任务学习架构，实现对杂草的语义分割、高度估计及生长阶段分类，显著提升了杂草分析的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 农业杂草管理极为重要，但当前对杂草的监测和分析依赖大量人力和资源，亟需自动化、高效且可持续的方法。

Method: WeedSense采用了多任务学习架构，包含双通道编码器（Universal Inverted Bottleneck blocks）和基于transformer的多任务分叉解码器，实现了对杂草种类的分割、高度估计和生长阶段判别。同时，作者自建了包含16种杂草、覆盖11周生长周期的精细标注数据集。

Result: WeedSense在该多任务数据集上的语义分割mIoU达89.78%，高度估计MAE仅1.67cm，生长阶段分类准确率高达99.99%，并能以160FPS实时运行，推理速度比串行单任务快3倍，参数量减少32.4%。

Conclusion: WeedSense在杂草多任务分析方面表现优越，兼顾高效性、准确性和实时性，为精细农业和可持续管理提供了强有力的技术支撑。

Abstract: Weed management represents a critical challenge in agriculture, significantly
impacting crop yields and requiring substantial resources for control.
Effective weed monitoring and analysis strategies are crucial for implementing
sustainable agricultural practices and site-specific management approaches. We
introduce WeedSense, a novel multi-task learning architecture for comprehensive
weed analysis that jointly performs semantic segmentation, height estimation,
and growth stage classification. We present a unique dataset capturing 16 weed
species over an 11-week growth cycle with pixel-level annotations, height
measurements, and temporal labels. WeedSense leverages a dual-path encoder
incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated
Decoder with transformer-based feature fusion to generate multi-scale features
and enable simultaneous prediction across multiple tasks. WeedSense outperforms
other state-of-the-art models on our comprehensive evaluation. On our
multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm
MAE for height estimation, and 99.99% accuracy for growth stage classification
while maintaining real-time inference at 160 FPS. Our multitask approach
achieves 3$\times$ faster inference than sequential single-task execution and
uses 32.4% fewer parameters. Please see our project page at
weedsense.github.io.

</details>


### [42] [SATURN: Autoregressive Image Generation Guided by Scene Graphs](https://arxiv.org/abs/2508.14502)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: SATURN是一种将场景图结构高效集成到主流文本到图像生成架构中的方法，大幅提升了生成图像在布局和关系理解方面的表现，同时保持更快的速度和更优的效果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像生成模型在真实感渲染方面表现出色，但在处理复杂描述中的布局和多物体关系时表现不足。而场景图作为结构先验能有效弥补这一短板，但以往相关工作大多依赖复杂且较慢的生成器（如GAN、扩散模型），难以兼顾速度和质量。

Method: 提出SATURN方法，将场景图编码为显著性排序的token序列，并输入VAR-CLIP架构，只微调VAR transformer部分，利用冻结的CLIP-VQ-VAE骨干，实现结构信息高效融合。

Result: 在Visual Genome数据集上，SATURN将FID从56.45%降至21.62%，Inception Score从16.03提高到24.78，超越了SG2IM和SGDiff等已有方法，无需额外模块或多阶段训练。

Conclusion: SATURN不仅提升了生成图像的物体数、空间关系准确性，还兼顾了结构感知和当前自回归网络的高保真度，是生成领域结构引导的高效解决方案。

Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but
often struggle to capture the layout and object relationships implied by
complex prompts. Scene graphs provide a natural structural prior, yet previous
graph-guided approaches have typically relied on heavy GAN or diffusion
pipelines, which lag behind modern autoregressive architectures in both speed
and fidelity. We introduce SATURN (Structured Arrangement of Triplets for
Unified Rendering Networks), a lightweight extension to VAR-CLIP that
translates a scene graph into a salience-ordered token sequence, enabling a
frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only
the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from
56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,
outperforming prior methods such as SG2IM and SGDiff without requiring extra
modules or multi-stage training. Qualitative results further confirm
improvements in object count fidelity and spatial relation accuracy, showing
that SATURN effectively combines structural awareness with state-of-the-art
autoregressive fidelity.

</details>


### [43] [PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments](https://arxiv.org/abs/2508.14504)
*Bernd Hofmann,Albert Scheck,Joerg Franke,Patrick Bruendl*

Main category: cs.CV

TL;DR: 本论文提出了一种新的工业异常检测方法PB-IAD，充分利用基础模型的多模态和推理能力，实现了在数据稀缺和动态生产环境下更高效、用户友好的异常检测，对比现有方法在数据稀疏的场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的工业异常检测方法依赖大量标注数据，在动态和数据稀缺的生产环境中适应性和灵活性有限。近年来基础模型感知能力的提升为此带来了新的解决思路，作者希望用基础模型克服现有方法的瓶颈，提高检测准确性和用户的易用性。

Method: 作者提出PB-IAD架构，利用基础模型（如GPT-4.1）的多模态理解与推理能力，并设计了面向领域用户的提示（prompt）模板与输入预处理模块，使非数据科学背景的领域专家也可灵活定制异常检测系统。方法被应用于多个制造场景和不同数据模态，并通过消融实验系统评估语义指令的作用。

Result: PB-IAD在三个真实制造场景和两种数据模态的实验中表现出色，尤其在数据稀缺和低样本条件下，仅通过语义指令就显著优于当前最先进的异常检测方法（如PatchCore）。

Conclusion: PB-IAD框架凭借基础模型的多模态与推理能力，实现了工业异常检测在动态和数据稀缺环境下的高适应性、低门槛与高准确率，有望成为工业异常检测的新方向。

Abstract: The detection of anomalies in manufacturing processes is crucial to ensure
product quality and identify process deviations. Statistical and data-driven
approaches remain the standard in industrial anomaly detection, yet their
adaptability and usability are constrained by the dependence on extensive
annotated datasets and limited flexibility under dynamic production conditions.
Recent advances in the perception capabilities of foundation models provide
promising opportunities for their adaptation to this downstream task. This
paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel
framework that leverages the multimodal and reasoning capabilities of
foundation models for industrial anomaly detection. Specifically, PB-IAD
addresses three key requirements of dynamic production environments: data
sparsity, agile adaptability, and domain user centricity. In addition to the
anomaly detection, the framework includes a prompt template that is
specifically designed for iteratively implementing domain-specific process
knowledge, as well as a pre-processing module that translates domain user
inputs into effective system prompts. This user-centric design allows domain
experts to customise the system flexibly without requiring data science
expertise. The proposed framework is evaluated by utilizing GPT-4.1 across
three distinct manufacturing scenarios, two data modalities, and an ablation
study to systematically assess the contribution of semantic instructions.
Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly
detection such as PatchCore. The results demonstrate superior performance,
particularly in data-sparse scenarios and low-shot settings, achieved solely
through semantic instructions.

</details>


### [44] [Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles](https://arxiv.org/abs/2508.14527)
*Jiangfan Liu,Yongkang Guo,Fangzhi Zhong,Tianyuan Zhang,Zonglei Jing,Siyuan Liang,Jiakai Wang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ScenGE的自动驾驶仿真场景生成框架，可自动推理和放大对自动驾驶汽车具有威胁性的场景，有效提升碰撞测试严苛程度，并增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全仿真场景主要依赖预设威胁模式或规则，难以发掘多样且未知的失效情形，限制了自动驾驶汽车部署前的安全评估可靠性。

Method: ScenGE 提出使用大型语言模型推理出新颖的对抗性威胁场景（Meta-Scenario Generation），通过结构化驾驶知识生成具威胁且合理的敌对行为代理，并将其转化成可执行代码用于仿真。随后，通过复杂交通流构建“对抗协作图”，进一步扰动相关车辆轨迹，放大威胁并制造盲区，综合提升测试难度。

Result: 在多个强化学习自动驾驶模型与主流基线对比下，ScenGE 能平均多发现31.96%的严重碰撞场景，对大模型自动驾驶系统同样适用，并可迁移于不同仿真器。对抗性训练后，模型鲁棒性有效提升。

Conclusion: ScenGE 能生成更真实、关键的安全威胁场景，并提升自动驾驶系统的可靠性，其方法已通过仿真、实车测试和人工评估验证，有助于促进自动驾驶安全部署及公众信任。

Abstract: The generation of safety-critical scenarios in simulation has become
increasingly crucial for safety evaluation in autonomous vehicles prior to road
deployment in society. However, current approaches largely rely on predefined
threat patterns or rule-based strategies, which limit their ability to expose
diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a
framework that can generate plentiful safety-critical scenarios by reasoning
novel adversarial cases and then amplifying them with complex traffic flows.
Given a simple prompt of a benign scene, it first performs Meta-Scenario
Generation, where a large language model, grounded in structured driving
knowledge, infers an adversarial agent whose behavior poses a threat that is
both plausible and deliberately challenging. This meta-scenario is then
specified in executable code for precise in-simulator control. Subsequently,
Complex Scenario Evolution uses background vehicles to amplify the core threat
introduced by Meta-Scenario. It builds an adversarial collaborator graph to
identify key agent trajectories for optimization. These perturbations are
designed to simultaneously reduce the ego vehicle's maneuvering space and
create critical occlusions. Extensive experiments conducted on multiple
reinforcement learning based AV models show that ScenGE uncovers more severe
collision cases (+31.96%) on average than SoTA baselines. Additionally, our
ScenGE can be applied to large model based AV systems and deployed on different
simulators; we further observe that adversarial training on our scenarios
improves the model robustness. Finally, we validate our framework through
real-world vehicle tests and human evaluation, confirming that the generated
scenarios are both plausible and critical. We hope our paper can build up a
critical step towards building public trust and ensuring their safe deployment.

</details>


### [45] [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/abs/2508.14537)
*Yonghan Shin,SeungKyu Kim,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 本文提出WISE-FUSE方法，通过智能选择相关区域高效编码病理全景切片图像，大幅减少运算时间且不损失诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 全景切片图像（WSI）体量极大，必须处理成千上万个高分辨率图块，编码成本极高，限制实际部署。

Method: 该方法用视觉-文本模型与大语言模型，根据低分辨率图像与文本描述相似度，过滤并只选取对诊断有意义的区域，再对这些区域进行高分辨率编码，并结合文本增强诊断信息。

Result: WISE-FUSE将编码时间缩短三倍以上，同时在诊断准确率方面与传统全量处理方法持平甚至更优。

Conclusion: WISE-FUSE大幅提升了计算效率，为计算病理学实际部署提供了可扩展和高效的新方案。

Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major
computational challenge due to their gigapixel scale, often requiring the
processing of tens to hundreds of thousands of high-resolution patches per
slide. This results in prohibitive encoding costs, with preprocessing and
training times extending to days or even weeks-making WSI encoding the most
significant bottleneck in real-world deployment. In this work, we propose
WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain
vision-language models and large language models to address this challenge by
selectively processing diagnostically relevant regions. WISE-FUSE first
computes similarity scores between low-resolution patches and class-specific
textual descriptions using a knowledge distillation mechanism that preserves
fine-grained diagnostic features. Based on these similarity scores, we select a
small subset of informative regions for the target task, which quickly
eliminates irrelevant patches at the coarse level. The corresponding
high-resolution patches are then selectively encoded and fused with textual
embeddings to reinforce diagnostic context. Extensive experiments demonstrate
that WISE-FUSE reduces WSI encoding time by over threefold while achieving
diagnostic performance comparable to or surpassing that of exhaustive patch
processing, offering a scalable and practical solution for CPath.

</details>


### [46] [Improving OCR using internal document redundancy](https://arxiv.org/abs/2508.14557)
*Diego Belzarena,Seginus Mowlavi,Aitor Artola,Camilo Mariño,Marina Gardella,Ignacio Ramírez,Antoine Tadros,Roy He,Natalia Bottaioli,Boshra Rajaei,Gregory Randall,Jean-Michel Morel*

Main category: cs.CV

TL;DR: 本文提出了一种无监督方法，利用文档内部的字符形状冗余修正现有OCR系统的不完善识别结果，并优化聚类效果。在不同水平降质的文档中均获得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统对低质量文档的识别仍有困难，且未能充分利用同一文档中的字符冗余信息。为提升降质文档的识别准确率，亟需新的方法。

Method: 作者提出基于高斯混合模型（GMM），结合期望最大化（EM）算法，交替进行聚类内对齐和正态性统计检验的无监督方法。该方法利用每份文档内重复字符形状的信息，对OCR输出进行二次修正和更精细的聚类。

Result: 在多种降质文档（如乌拉圭军事档案、17至20世纪欧洲报纸）测试中，该方法能有效提升OCR识别准确率。

Conclusion: 通过文档内字符冗余的利用及创新聚类方法，可显著改善现有OCR识别的效果，尤其适用于高变异性文档域。

Abstract: Current OCR systems are based on deep learning models trained on large
amounts of data. Although they have shown some ability to generalize to unseen
data, especially in detection tasks, they can struggle with recognizing
low-quality data. This is particularly evident for printed documents, where
intra-domain data variability is typically low, but inter-domain data
variability is high. In that context, current OCR methods do not fully exploit
each document's redundancy. We propose an unsupervised method by leveraging the
redundancy of character shapes within a document to correct imperfect outputs
of a given OCR system and suggest better clustering. To this aim, we introduce
an extended Gaussian Mixture Model (GMM) by alternating an
Expectation-Maximization (EM) algorithm with an intra-cluster realignment
process and normality statistical testing. We demonstrate improvements in
documents with various levels of degradation, including recovered Uruguayan
military archives and 17th to mid-20th century European newspapers.

</details>


### [47] [A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives](https://arxiv.org/abs/2508.14558)
*Juepeng Zheng,Zi Ye,Yibin Wen,Jianxi Huang,Zhiwei Zhang,Qingmei Li,Qiong Hu,Baodong Xu,Lingyuan Zhao,Haohuan Fu*

Main category: cs.CV

TL;DR: 本综述系统梳理了利用遥感高分辨率影像进行农业地块与边界识别（APBD）的研究进展，总结了主要算法类型及应用领域，并对未来研究方向进行展望。


<details>
  <summary>Details</summary>
Motivation: 随着多种遥感传感器技术进步，高分辨率影像可实现自动化、低成本且高精度的农业地块清查和分析。因此，有必要系统总结现有农业地块与边界识别的相关方法，为研究人员提供知识图谱。

Method: 本文对近年来APBD相关文献进行了全面综述和元数据分析，内容涵盖算法种类、研究区域、作物类型、传感器类型及评估方法。方法划分为：1）传统图像处理方法（像素、边缘、区域）；2）传统机器学习方法（如随机森林、决策树）；3）深度学习方法（如语义分割、目标检测、基于Transformer的模型）。还讨论了多传感器数据、单任务与多任务学习、不同算法与任务比较等问题。

Result: 论文归纳了近年主流APBD研究成果，发现以深度学习为主的方法占主导地位，探讨了多种实际应用场景及当前面临的主要问题。

Conclusion: 该综述为APBD领域研究者梳理了现状和挑战，展望了未来研究方向及潜在热点，有助于相关科研人员跟踪技术发展与趋势。

Abstract: Powered by advances in multiple remote sensing sensors, the production of
high spatial resolution images provides great potential to achieve
cost-efficient and high-accuracy agricultural inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory of the level
of each agricultural parcel have generated many methods for Agricultural Parcel
and Boundary Delineation (APBD). This review covers APBD methods for detecting
and delineating agricultural parcels and systematically reviews the past and
present of APBD-related research applied to remote sensing images. With the
goal to provide a clear knowledge map of existing APBD efforts, we conduct a
comprehensive review of recent APBD papers to build a meta-data analysis,
including the algorithm, the study site, the crop type, the sensor type, the
evaluation method, etc. We categorize the methods into three classes: (1)
traditional image processing methods (including pixel-based, edge-based and
region-based); (2) traditional machine learning methods (such as random forest,
decision tree); and (3) deep learning-based methods. With deep
learning-oriented approaches contributing to a majority, we further discuss
deep learning-based methods like semantic segmentation-based, object
detection-based and Transformer-based methods. In addition, we discuss five
APBD-related issues to further comprehend the APBD domain using remote sensing
data, such as multi-sensor data in APBD task, comparisons between single-task
learning and multi-task learning in the APBD domain, comparisons among
different algorithms and different APBD tasks, etc. Finally, this review
proposes some APBD-related applications and a few exciting prospects and
potential hot topics in future APBD research. We hope this review help
researchers who involved in APBD domain to keep track of its development and
tendency.

</details>


### [48] [Locality-aware Concept Bottleneck Model](https://arxiv.org/abs/2508.14562)
*Sujin Jeon,Hyundo Lee,Eungseo Kim,Sanghack Lee,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种本地性感知的概念瓶颈模型（LCBM），利用基础模型与原型学习，提升了无需人工标注的概念模型在空间定位上的准确性，同时保持了良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的概念瓶颈模型（CBMs）需要耗费大量人工标注来获得图像的概念信息，影响了其广泛应用。虽然近期方法尝试用基础模型自动获取概念，但本质问题在于无法准确定位相关图像区域，模型可能关注无关区域，影响模型的解释性和有效性。

Method: 作者提出LCBM框架，结合基础模型的丰富表征能力和原型学习。对于每个可解释的概念，分配一个可学习的原型，使其代表概念在图像中的典型特征。通过训练，原型被促使聚焦于反映该概念的局部区域，同时基础模型保证原型与其抽象的概念相关联。利用这些原型，引导模型在识别概念时关注于正确的图像局部区域。

Result: 实验证明，LCBM能有效识别图像中的概念，并显著提高概念的空间定位能力，同时其整体分类准确率与现有无监督CBM方法持平。

Conclusion: LCBM方法在提升无需人工标注的CBM模型空间定位能力的同时，保持了解释性和分类性能，为无监督CBM提供了有效的途径。

Abstract: Concept bottleneck models (CBMs) are inherently interpretable models that
make predictions based on human-understandable visual cues, referred to as
concepts. As obtaining dense concept annotations with human labeling is
demanding and costly, recent approaches utilize foundation models to determine
the concepts existing in the images. However, such label-free CBMs often fail
to localize concepts in relevant regions, attending to visually unrelated
regions when predicting concept presence. To this end, we propose a framework,
coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes rich
information from foundation models and adopts prototype learning to ensure
accurate spatial localization of the concepts. Specifically, we assign one
prototype to each concept, promoted to represent a prototypical image feature
of that concept. These prototypes are learned by encouraging them to encode
similar local regions, leveraging foundation models to assure the relevance of
each prototype to its associated concept. Then we use the prototypes to
facilitate the learning process of identifying the proper local region from
which each concept should be predicted. Experimental results demonstrate that
LCBM effectively identifies present concepts in the images and exhibits
improved localization while maintaining comparable classification performance.

</details>


### [49] [GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels](https://arxiv.org/abs/2508.14563)
*Xingyuan Yang,Min Wei*

Main category: cs.CV

TL;DR: 本文提出GOGS，一种基于2D高斯surfels的逆向渲染方法，有效解决了现有方法在重建带有高光物体时的偏差和效率瓶颈，并显著提升了几何、材质分离和真实感重光的效果。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF等方法尽管在重建上表现优异，但计算开销大，3D Gaussian Splatting虽高效，却难以处理高光反射，造成重建噪声和结构失真，同时物理渲染简化影响了材料真实性，因此需要更高效且更准确的方法。

Method: 提出两阶段GOGS框架：（1）利用分步和物理渲染结合通用模型几何先验，实现鲁棒的曲面重建；（2）借助蒙特卡洛采样完成材料分解，通过可微2D高斯光线追踪模拟间接光，融合方向编码处理各向异性高光。

Result: 大量实验显示GOGS在几何重建、材料分离和新照明条件下的真实感重光方面优于现有逆向渲染技术，达到新的最优水平。

Conclusion: GOGS克服了高光物体逆向渲染中的核心难题，在效率和真实感上较现有方法有明显突破，推动了三维视觉与渲染领域的进步。

Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

</details>


### [50] [Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset](https://arxiv.org/abs/2508.14567)
*Walter Zimmer,Ross Greer,Xingcheng Zhou,Rui Song,Marc Pavel,Daniel Lehmberg,Ahmed Ghita,Akshay Gopalkrishnan,Mohan Trivedi,Alois Knoll*

Main category: cs.CV

TL;DR: 论文介绍了TUMTraf-A交通事故数据集和一种名为Accid3nD的事故检测模型，用于提升交通事故检测的研究基础和实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有很多提升交通安全的工作，交通事故仍频繁发生。了解和检测事故对于提升道路安全至关重要，但高质量真实世界交通事故数据集稀缺，模型研究受到限制。

Method: 作者构建了TUMTraf-A数据集，包含高速公路真实车辆碰撞场景，涵盖大量2D与3D标注，支持多相机与激光雷达数据。提出的Accid3nD模型结合规则驱动与学习驱动方法进行事故检测，并对模型进行了实验与消融分析。

Result: 实验与消融分析表明，Accid3nD模型在该数据集上表现鲁棒，能够有效检测交通事故。

Conclusion: TUMTraf-A数据集和Accid3nD模型为交通事故检测提供了新的基准和工具，有望推动学术和实际应用研究发展。

Abstract: Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as an unavoidable and sporadic outcome of traffic networks. We
present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of
real-world highway accidents. It contains ten sequences of vehicle crashes at
high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and
track IDs within 48,144 labeled frames recorded from four roadside cameras and
LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the
OpenLABEL format. We propose Accid3nD, an accident detection model that
combines a rule-based approach with a learning-based one. Experiments and
ablation studies on our dataset show the robustness of our proposed method. The
dataset, model, and code are available on our project website:
https://tum-traffic-dataset.github.io/tumtraf-a.

</details>


### [51] [Controllable Latent Space Augmentation for Digital Pathology](https://arxiv.org/abs/2508.14588)
*Sofiène Boutaj,Marin Scalbert,Pierre Marza,Florent Couzinie-Devy,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: 本文提出了一种名为HistAug的方法，通过在潜在空间中进行可控的数据增强，有效提升了数字病理学中的多实例学习（MIL）模型在WSI（Whole Slide Image）分析任务中的表现，尤其是在数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 数字病理的WSI图像体积大且需要密集标注信号，但标注稀缺，现有数据增强（尤其是补丁级别）方法计算代价高，特征级增强又缺乏语义控制。因此，需要开发高效且可控的增强方法以提升模型泛化能力。

Method: 提出了HistAug，一种基于生成模型的方法，通过对补丁级显式变换（如色调、腐蚀）进行条件控制，在潜在空间中生成语义保真的增强特征，并支持一次前向传递高效处理大量补丁。

Result: 在多个片层级任务与不同器官的实验中，HistAug均超过了现有方法，特别是在低数据情况下表现突出。消融实验进一步验证了控制变换比基于噪声的扰动优越，并强调了WSI级别均匀增强的重要性。

Conclusion: HistAug为WSI分析提供了一种高效、可控的数据增强新范式，能显著提升MIL模型效果，尤其适用于数据有限场景，为数字病理学领域数据增强方法带来新思路。

Abstract: Whole slide image (WSI) analysis in digital pathology presents unique
challenges due to the gigapixel resolution of WSIs and the scarcity of dense
supervision signals. While Multiple Instance Learning (MIL) is a natural fit
for slide-level tasks, training robust models requires large and diverse
datasets. Even though image augmentation techniques could be utilized to
increase data variability and reduce overfitting, implementing them effectively
is not a trivial task. Traditional patch-level augmentation is prohibitively
expensive due to the large number of patches extracted from each WSI, and
existing feature-level augmentation methods lack control over transformation
semantics. We introduce HistAug, a fast and efficient generative model for
controllable augmentations in the latent space for digital pathology. By
conditioning on explicit patch-level transformations (e.g., hue, erosion),
HistAug generates realistic augmented embeddings while preserving initial
semantic information. Our method allows the processing of a large number of
patches in a single forward pass efficiently, while at the same time
consistently improving MIL model performance. Experiments across multiple
slide-level tasks and diverse organs show that HistAug outperforms existing
methods, particularly in low-data regimes. Ablation studies confirm the
benefits of learned transformations over noise-based perturbations and
highlight the importance of uniform WSI-wise augmentation. Code is available at
https://github.com/MICS-Lab/HistAug.

</details>


### [52] [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](https://arxiv.org/abs/2508.14597)
*Nitish Kumar Mahala,Muzammil Khan,Pushpendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种无需多传感器阵列、仅基于单眼图像，实现鲁棒烟雾检测的信息融合框架，并采用不确定性感知的Transformer架构。实验结果优于现有方法，提升了早期火灾预警的可靠性。


<details>
  <summary>Details</summary>
Motivation: 火灾烟雾早期检测对生命和基础设施安全至关重要，但烟雾的变化性和环境干扰导致传统检测方式可靠性不足。为克服多传感器成本高、部署复杂等实际挑战，亟需一种仅用单目图像也能高效稳健检测烟雾的方法。

Method: 作者提出了一种两阶段不确定性感知Shifted Windows Transformer模型：先基于光流分割和外观信息融合生成烟雾区域掩码（光流是用受四色定理启发的分数阶变分模型获得，能保留运动边界），再输入到特制的Transformer，分阶段优化分类准确率及不确定性估计（异质性与认知不确定性），提升模型鲁棒性。

Result: 在多项指标和与主流先进方法对比测试中，作者提出的方法展现出更优异的泛化能力和鲁棒性，在烟雾检测的准确性、可靠性等方面均有提升。

Conclusion: 所提方法在单目摄像早期火灾预警场景下取得了更可靠的烟雾检测效果，有助于工业安全、自动监测等应用场景的广泛推广。

Abstract: Fire outbreaks pose critical threats to human life and infrastructure,
necessitating high-fidelity early-warning systems that detect combustion
precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal
dynamics influenced by illumination variability, flow kinematics, and
environmental noise, undermining the reliability of traditional detectors. To
address these challenges without the logistical complexity of multi-sensor
arrays, we propose an information-fusion framework by integrating smoke feature
representations extracted from monocular imagery. Specifically, a Two-Phase
Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke
detection, leveraging a novel smoke segmentation dataset, constructed via
optical flow-based motion encoding, is proposed. The optical flow estimation is
performed with a four-color-theorem-inspired dual-phase level-set
fractional-order variational model, which preserves motion discontinuities. The
resulting color-encoded optical flow maps are fused with appearance cues via a
Gaussian Mixture Model to generate binary segmentation masks of the smoke
regions. These fused representations are fed into the novel Shifted-Windows
Transformer, which is augmented with a multi-scale uncertainty estimation head
and trained under a two-phase learning regimen. First learning phase optimizes
smoke detection accuracy, while during the second phase, the model learns to
estimate plausibility confidence in its predictions by jointly modeling
aleatoric and epistemic uncertainties. Extensive experiments using multiple
evaluation metrics and comparative analysis with state-of-the-art approaches
demonstrate superior generalization and robustness, offering a reliable
solution for early fire detection in surveillance, industrial safety, and
autonomous monitoring applications.

</details>


### [53] [Incremental Object Detection with Prompt-based Methods](https://arxiv.org/abs/2508.14599)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文评估了视觉提示（visual prompt）方法在增量目标检测（IOD）中的应用表现，发现单独使用提示方法效果较弱，但结合少量数据重放能得到最佳结果。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉提示方法在图像分类的增量学习中很受关注，但尚未有人系统研究其在增量目标检测中的适用性，因此本工作尝试填补该空白并分析其泛化能力。

Method: 本文设计了三种不同的提示方法，并在复杂的领域增量学习设置下进行了全面测试，同时与多种基准方法对比。此外，还探究了提示长度和初始化对效果的影响。

Result: 实验显示，单独的提示方法在增量目标检测中表现不佳；但将视觉提示与对少量旧数据的重放相结合后，性能显著提升并优于对比方法。

Conclusion: 单一提示在IOD中效果有限，但与重放结合后可显著提高表现。相关研究为后续增量目标检测和提示方法的发展提供了有益见解。

Abstract: Visual prompt-based methods have seen growing interest in incremental
learning (IL) for image classification. These approaches learn additional
embedding vectors while keeping the model frozen, making them efficient to
train. However, no prior work has applied such methods to incremental object
detection (IOD), leaving their generalizability unclear. In this paper, we
analyze three different prompt-based methods under a complex domain-incremental
learning setting. We additionally provide a wide range of reference baselines
for comparison. Empirically, we show that the prompt-based approaches we tested
underperform in this setting. However, a strong yet practical method, combining
visual prompts with replaying a small portion of previous data, achieves the
best results. Together with additional experiments on prompt length and
initialization, our findings offer valuable insights for advancing prompt-based
IL in IOD.

</details>


### [54] [UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://arxiv.org/abs/2508.14604)
*Peiming Li,Ziyi Wang,Yulin Yuan,Hong Liu,Xiangming Meng,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于点云视频的时空状态空间模型（UST-SSM），通过结构改进提升对3D动态人体动作的识别效果，并在多个数据集验证了效果优越。


<details>
  <summary>Details</summary>
Motivation: 点云视频因具有抗光照与视角变化能力，对细微、连续的人体动作识别非常有效。但由于其时空顺序无序，现有的状态空间模型在对其一维序列展开时表现受限，难以捕捉复杂的时空特征。

Method: 提出了统一本时空状态空间模型（UST-SSM）。具体包含：1）空间-时序选择扫描（STSS），通过语义感知聚类将点重排为有意义的序列；2）时空结构聚合（STSA），整合并补充4D几何与运动特征；3）时间交互采样（TIS），利用非锚帧和扩大感受野加强细粒度时间依赖。

Result: 在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上进行实验，所提方法优于现有方法，验证了模型的有效性。

Conclusion: UST-SSM有效提升了点云视频的人体动作识别能力，解决了传统SSM在点云时空无序性上的短板，具有广泛的应用潜力。

Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of
lighting and viewpoint variations, making them highly effective for recognizing
subtle and continuous human actions. Although Selective State Space Models
(SSMs) have shown good performance in sequence modeling with linear complexity,
the spatio-temporal disorder of point cloud videos hinders their unidirectional
modeling when directly unfolding the point cloud video into a 1D sequence
through temporally sequential scanning. To address this challenge, we propose
the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the
latest advancements in SSMs to point cloud videos. Specifically, we introduce
Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points
into semantic-aware sequences through prompt-guided clustering, thereby
enabling the effective utilization of points that are spatially and temporally
distant yet similar within the sequence. For missing 4D geometric and motion
details, Spatio-Temporal Structure Aggregation (STSA) aggregates
spatio-temporal features and compensates. To improve temporal interaction
within the sampled sequence, Temporal Interaction Sampling (TIS) enhances
fine-grained temporal dependencies through non-anchor frame utilization and
expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,
and Synthia 4D datasets validate the effectiveness of our method. Our code is
available at https://github.com/wangzy01/UST-SSM.

</details>


### [55] [SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos](https://arxiv.org/abs/2508.14607)
*Pengzhi Zhong,Xinzhe Wang,Dan Zeng,Qihua Zhou,Feixiang He,Shuiwang Li*

Main category: cs.CV

TL;DR: 本论文提出了SMTrack，这是第一个面向标准RGB视频进行多目标跟踪（MOT）的端到端深度脉冲神经网络（SNN）框架，并在多个数据集上实现了与主流人工神经网络（ANN）方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然脉冲神经网络（SNN）因其低功耗特性在视觉任务中具有巨大潜力，但其应用主要局限于图像分类、目标检测和基于事件的跟踪。现实世界的视觉系统大多还是基于传统RGB视频，对SNN在复杂时序任务（如多目标跟踪）上的直接训练和应用仍有很大探索空间。因此，研究旨在推动SNN在标准视频多目标跟踪任务上的应用。

Method: 作者提出了SMTrack架构，首次实现了对标准RGB视频直接端到端训练的深度SNN多目标跟踪框架。其方法创新地引入了自适应和尺度感知的归一化Wasserstein距离损失（Asa-NWDLoss），根据每个批次的目标平均尺寸动态调整归一化因子，提高对小目标的灵敏度。同时，利用TrackTrack身份模块增强轨迹的连贯性和鲁棒性。

Result: 在BEE24、MOT17、MOT20、DanceTrack等数据集上，SMTrack的多目标跟踪性能与先进的基于ANN的方法持平，验证了其鲁棒性和精度，尤其在复杂场景下表现良好。

Conclusion: 本研究表明，端到端直接训练的SNN可实现复杂视频多目标跟踪，性能可与现有ANN方法媲美，为低能耗视觉系统在实际场景中的应用拓展了新方向。

Abstract: Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential
for low-power computation, yet their application in visual tasks remains
largely confined to image classification, object detection, and event-based
tracking. In contrast, real-world vision systems still widely use conventional
RGB video streams, where the potential of directly-trained SNNs for complex
temporal tasks such as multi-object tracking (MOT) remains underexplored. To
address this challenge, we propose SMTrack-the first directly trained deep SNN
framework for end-to-end multi-object tracking on standard RGB videos. SMTrack
introduces an adaptive and scale-aware Normalized Wasserstein Distance loss
(Asa-NWDLoss) to improve detection and localization performance under varying
object scales and densities. Specifically, the method computes the average
object size within each training batch and dynamically adjusts the
normalization factor, thereby enhancing sensitivity to small objects. For the
association stage, we incorporate the TrackTrack identity module to maintain
robust and consistent object trajectories. Extensive evaluations on BEE24,
MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with
leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking
in complex scenarios.

</details>


### [56] [AnchorSync: Global Consistency Optimization for Long Video Editing](https://arxiv.org/abs/2508.14609)
*Zichi Liu,Yinggui Wang,Tao Wei,Chao Ma*

Main category: cs.CV

TL;DR: 提出了AnchorSync，一种基于扩散模型的视频编辑框架，可高质量地编辑长视频，通过锚点帧编辑与插帧插补相结合方式，显著提升了视觉质量和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 长视频编辑需要在数千帧中保持整体一致性和时间连贯性，现有方法常出现结构漂移和时间伪影问题，尤其在分钟以上的视频中更为突出，亟需一种高质量的长视频编辑方案。

Method: 提出AnchorSync，将长视频编辑任务分解为稀疏锚点帧编辑和中间帧顺滑插补；采用渐进去噪流程确保结构一致性，通过多模态引导维持时间动态一致。

Result: 大量实验证明，AnchorSync在视觉质量与时间稳定性方面优于现有方法，能够生成连贯且高保真的视频编辑结果。

Conclusion: AnchorSync为长视频编辑提供了新的高效工具，有效解决了现有方法中的主要问题。

Abstract: Editing long videos remains a challenging task due to the need for
maintaining both global consistency and temporal coherence across thousands of
frames. Existing methods often suffer from structural drift or temporal
artifacts, particularly in minute-long sequences. We introduce AnchorSync, a
novel diffusion-based framework that enables high-quality, long-term video
editing by decoupling the task into sparse anchor frame editing and smooth
intermediate frame interpolation. Our approach enforces structural consistency
through a progressive denoising process and preserves temporal dynamics via
multimodal guidance. Extensive experiments show that AnchorSync produces
coherent, high-fidelity edits, surpassing prior methods in visual quality and
temporal stability.

</details>


### [57] [Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images](https://arxiv.org/abs/2508.14660)
*Muhammad Ibraheem Siddiqui,Muhammad Umer Sheikh,Hassan Abid,Kevin Henry,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一种名为PerSense的端到端、无需训练、模型无关的一次性个性化实例分割框架，及其增强版本PerSense++，能有效应对密集场景下的遮挡、背景杂乱和尺度变化等难题，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 密集视觉场景中的实例分割任务复杂，常因遮挡、杂乱背景和尺度变化等因素造成挑战，现有方法难以实现高效且个性化的分割。因此，作者尝试提出无需训练且可泛化的新方案，提升密集场景分割效果。

Method: PerSense通过引入密度图生成实例级点提示的实例检测模块（IDM）、利用自适应阈值和空间门控筛选点的点提示选择模块（PPSM）、以及可自动选取有效示例提升分割质量的反馈机制完成分割任务。增强版PerSense++进一步融入特征与尺度多样性示例选择、结合轮廓和峰值的混合IDM，以及基于离群分析的无关掩码剔除模块（IMRM）。同时，作者还构建了专用的PerSense-D基准数据集。

Result: 在多个基准数据集上，PerSense++在密集场景下表现优异，分割效果显著超过现有方法，特别是在应对遮挡与背景干扰、提升实例分离等方面展现领先性能。

Conclusion: PerSense及其增强版本有效提升了密集视觉场景中的个性化实例分割能力，为此类任务提供了训练无关、高度鲁棒的新思路，并通过新建数据集完善了此方向的研究基础。

Abstract: Segmentation in dense visual scenes poses significant challenges due to
occlusions, background clutter, and scale variations. To address this, we
introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot
framework for Personalized instance Segmentation in dense images. PerSense
employs a novel Instance Detection Module (IDM) that leverages density maps
(DMs) to generate instance-level candidate point prompts, followed by a Point
Prompt Selection Module (PPSM) that filters false positives via adaptive
thresholding and spatial gating. A feedback mechanism further enhances
segmentation by automatically selecting effective exemplars to improve DM
quality. We additionally present PerSense++, an enhanced variant that
incorporates three additional components to improve robustness in cluttered
scenes: (i) a diversity-aware exemplar selection strategy that leverages
feature and scale diversity for better DM generation; (ii) a hybrid IDM
combining contour and peak-based prompt generation for improved instance
separation within complex density patterns; and (iii) an Irrelevant Mask
Rejection Module (IMRM) that discards spatially inconsistent masks using
outlier analysis. Finally, to support this underexplored task, we introduce
PerSense-D, a dedicated benchmark for personalized segmentation in dense
images. Extensive experiments across multiple benchmarks demonstrate that
PerSense++ outperforms existing methods in dense settings.

</details>


### [58] [GeMS: Efficient Gaussian Splatting for Extreme Motion Blur](https://arxiv.org/abs/2508.14682)
*Gopi Raju Matta,Trisha Reddypalli,Vemunuri Divya Madhuri,Kaushik Mitra*

Main category: cs.CV

TL;DR: 本文提出了GeMS框架，首次直接用极度模糊图像完成三维高斯喷溅(3DGS)重建，同时提出升级版GeMS-E，可结合事件相机数据进一步提升重建质量，均达到了当前最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有极端模糊去模糊方法和高斯喷溅重建方法，通常假设可获得清晰图像用于相机位姿估计和点云生成，但这在实际强模糊场景下不切实际，常用初始化方案如COLMAP在此条件下也会失败，需要新方法能直接用模糊图像稳健重建。

Method: 1）提出VGGSfM，基于深度学习的SFM管线，可直接从模糊输入估算相机位姿及点云。2）提出3DGS-MCMC，把高斯粒子视为概率分布样本实现稳健初始化，免去经验性稠密化与剪枝。3）相机轨迹与高斯参数联合优化。4）扩展方案GeMS-E结合事件相机，先用事件数据去模糊再进入GeMS流程，提升重建准确度。

Result: GeMS及GeMS-E在合成和真实数据集上均获得了最优的重建表现，相机位姿估计、点云生成和整体三维重建效果优于现有最新方法。

Conclusion: GeMS系列是首个能直接用极度运动模糊输入图像进行三维高斯喷溅重建的框架，为极端运动模糊场景下的3D重建问题提供了有效解决方案，推动了该领域技术进步。

Abstract: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.

</details>


### [59] [Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models](https://arxiv.org/abs/2508.14707)
*Jiabo Huang,Chen Chen,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本论文提出了一种无需大规模标注数据、集成多领域预训练模型训练视觉基础模型(VFM)的新方法。通过融合和迁移预训练模型知识，显著提升了VFM在多项视觉任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VFM依赖大量高质量标注数据和计算资源，阻碍了大部分机构的使用。而大量开放预训练模型虽然蕴含丰富专业知识，但未被充分利用以训练通用VFM。本研究旨在解决如何有效利用这些预训练模型及其知识瓶颈。

Method: 方法上，作者以多预训练教师模型统一映射至共享潜空间，有效缓解教师模型分布不一致带来的“失衡迁移”问题。同时，使用知识保护策略，将通用型教师模型作为基础，通过适配器模块集成专业型教师模型的知识，最终构建无需大规模标注数据训练的强大VFM。

Result: 实验结果表明，该VFM在图像分类、目标检测、语义分割及实例分割四大视觉任务中均优于现有依赖数据驱动的方法。

Conclusion: 论文证明了通过集成和迁移多领域预训练模型知识，可在无需庞大标注数据前提下，训练出具备高通用性和多任务能力的视觉基础模型，拓宽了VFM训练和应用的新路径。

Abstract: Vision foundation models (VFMs) are predominantly developed using
data-centric methods. These methods require training on vast amounts of data
usually with high-quality labels, which poses a bottleneck for most
institutions that lack both large-scale data and high-end GPUs. On the other
hand, many open-source vision models have been pretrained on domain-specific
data, enabling them to distill and represent core knowledge in a form that is
transferable across diverse applications. Even though these models are highly
valuable assets, they remain largely under-explored in empowering the
development of a general-purpose VFM. In this paper, we presents a new
model-driven approach for training VFMs through joint knowledge transfer and
preservation. Our method unifies multiple pre-trained teacher models in a
shared latent space to mitigate the ``imbalanced transfer'' issue caused by
their distributional gaps. Besides, we introduce a knowledge preservation
strategy to take a general-purpose teacher as a knowledge base for integrating
knowledge from the remaining purpose-specific teachers using an adapter module.
By unifying and aggregating existing models, we build a powerful VFM to inherit
teachers' expertise without needing to train on a large amount of labeled data.
Our model not only provides generalizable visual features, but also inherently
supports multiple downstream tasks. Extensive experiments demonstrate that our
VFM outperforms existing data-centric models across four fundamental vision
tasks, including image classification, object detection, semantic and instance
segmentation.

</details>


### [60] [GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting](https://arxiv.org/abs/2508.14717)
*Jiaxin Wei,Stefan Leutenegger,Simon Schaefer*

Main category: cs.CV

TL;DR: 本文提出了GSFix3D框架，通过将扩散模型的先验知识蒸馏到3D高斯表达中，提高了从极端新视角或观测不完整区域中生成高质量渲染效果的能力。提出的GSFixer扩散模型和随机遮罩增强策略，使得该方法能在各种重建方法场景下实现稳健的视角修复和缺失区修复，达到当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting提升了新视角合成效果，但遇到极端视角或部分观测区域时画质下降，而现有扩散模型虽具生成能力，但仅依赖文本提示、不了解具体场景，难以精确重建3D。作者希望结合扩散模型强大的生成先验和3D高斯表达的场景一致性，弥补各自不足，提升不完整区域的新视角合成质量。

Method: 核心方法是提出GSFix3D框架，包括定制微调生成的GSFixer潜在扩散模型，该模型可同时适应mesh和3D高斯信息，将预训练扩散模型的生成能力迁移到具体场景。引入随机遮罩增强，以实现对不完整区域的合理修复。整个流程只需很少场景特定微调，便可适配不同重建数据和输入画质。

Result: 在多个具有挑战性的公开基准上，GSFix3D与GSFixer均达到目前最优性能，无论在极端视角还是缺失区域恢复上均显著优于对比方法。此外，在真实世界测试时，该方法对相机参数不准等误差具有良好抗扰能力。

Conclusion: GSFix3D通过引入扩散模型蒸馏，极大提升了3D重建中极端视角和缺失区域的视觉质量且易于适配新场景。该方法在新视角修复问题上表现出色，对潜在误差具备强健性，适合实际应用和进一步研究。

Abstract: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.

</details>


### [61] [Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving](https://arxiv.org/abs/2508.14729)
*Leila Cheshmi,Mennatullah Siam*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频Transformer方法，无需光流，以运动线索检测未知物体，实现端到端的类别无关分割，在保证精度和效率的同时，尤其适用于自动驾驶等安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要应对未知物体和未预见的驾驶场景。现有的视频分割方法依赖已知训练类别，难以处理新类别，同时高效性要求又较高。

Method: 作者设计了多阶段、多尺度、带记忆的视频Transformer解码架构，采用scale-specific random drop-token策略以及共享可学习的内存模块，避免特征压缩，保留多尺度高分辨率信息，无需光流，实现端到端训练及类别无关分割。

Result: 在DAVIS'16、KITTI和Cityscapes三个数据集上，所提方法在分割准确率、GPU内存消耗和运行速度方面均优于多尺度基线方法。

Conclusion: 该视频Transformer架构在提升效率的同时，能准确检测未知物体并实现高效的像素级分割，为安全关键机器人（如自动驾驶）中的实时、鲁棒密集预测提供了一种有前景的方案。

Abstract: Ensuring safety in autonomous driving is a complex challenge requiring
handling unknown objects and unforeseen driving scenarios. We develop
multiscale video transformers capable of detecting unknown objects using only
motion cues. Video semantic and panoptic segmentation often relies on known
classes seen during training, overlooking novel categories. Recent visual
grounding with large language models is computationally expensive, especially
for pixel-level output. We propose an efficient video transformer trained
end-to-end for class-agnostic segmentation without optical flow. Our method
uses multi-stage multiscale query-memory decoding and a scale-specific random
drop-token to ensure efficiency and accuracy, maintaining detailed
spatiotemporal features with a shared, learnable memory module. Unlike
conventional decoders that compress features, our memory-centric design
preserves high-resolution information at multiple scales. We evaluate on
DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale
baselines while being efficient in GPU memory and run-time, demonstrating a
promising direction for real-time, robust dense prediction in safety-critical
robotics.

</details>


### [62] [Improved Mapping Between Illuminations and Sensors for RAW Images](https://arxiv.org/abs/2508.14730)
*Abhijith Punnappurath,Luxi Zhao,Hoang Le,Abdelrahman Abdelhamed,SaiKiran Kumar Tedla,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一个包含多种照明和多传感器的RAW图像数据集，并基于该数据集实现了一种高效的神经网络方法，能够在不同照明和传感器间转换RAW图像，提升了数据扩展和神经成像系统的训练效率。


<details>
  <summary>Details</summary>
Motivation: 由于RAW图像为未经处理的传感器输出，同时受传感器和场景照明影响显著，因此跨传感器和多照明下获取足够的RAW数据对于深度学习十分昂贵又繁琐。因此，发展能扩充照明情况和传感器映射能力的方法，有助于减少大规模数据采集难度。

Method: 作者搭建了可以调节光谱的定制光箱，利用多台摄像机和多样化场景采集了涵盖390种照明、4种摄像机和18个场景的RAW图像数据集。在此基础上，提出了一种轻量级的神经网络，用于实现不同照明和传感器之间的RAW图像映射。

Result: 新提出的神经网络方法在RAW图像的照明和传感器映射任务上优于现有方法，并能够用于提升后续神经成像系统（ISP）的训练和性能。

Conclusion: 该工作首次系统性地构建了多照明、多摄像机下的RAW图像数据集，提出的深度学习方法为跨传感器和照明的RAW图像映射提供了解决方案，可有效缓解数据采集压力并促进相关深度学习研究的发展。

Abstract: RAW images are unprocessed camera sensor output with sensor-specific RGB
values based on the sensor's color filter spectral sensitivities. RAW images
also incur strong color casts due to the sensor's response to the spectral
properties of scene illumination. The sensor- and illumination-specific nature
of RAW images makes it challenging to capture RAW datasets for deep learning
methods, as scenes need to be captured for each sensor and under a wide range
of illumination. Methods for illumination augmentation for a given sensor and
the ability to map RAW images between sensors are important for reducing the
burden of data capture. To explore this problem, we introduce the
first-of-its-kind dataset comprising carefully captured scenes under a wide
range of illumination. Specifically, we use a customized lightbox with tunable
illumination spectra to capture several scenes with different cameras. Our
illumination and sensor mapping dataset has 390 illuminations, four cameras,
and 18 scenes. Using this dataset, we introduce a lightweight neural network
approach for illumination and sensor mapping that outperforms competing
methods. We demonstrate the utility of our approach on the downstream task of
training a neural ISP. Link to project page:
https://github.com/SamsungLabs/illum-sensor-mapping.

</details>


### [63] [6-DoF Object Tracking with Event-based Optical Flow and Frames](https://arxiv.org/abs/2508.14776)
*Zhichao Li,Arren Glover,Chiara Bartolozzi,Lorenzo Natale*

Main category: cs.CV

TL;DR: 该论文提出了一种结合事件相机和传统RGB相机的6自由度（6-DoF）高速物体位姿跟踪方法，对高速运动物体的实时跟踪效果显著提升，尤其在运动模糊严重的场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 高速运动物体的位姿跟踪由于传统相机的帧率限制和运动模糊问题一直是机器人交互中的难题。事件相机高时空分辨率、低延迟和高动态范围等特点，可以有效减轻运动模糊等问题。

Method: 作者提出结合事件相机的光流算法和RGB相机基础上的全局位姿估计算法：先用事件光流算法实时跟踪物体的6-DoF速度，再把该速度与RGB相机低频率获得的全局位姿相结合，实现对高速运动物体的连续6-DoF跟踪。该方法同时利用了两类视觉传感器的优点。

Result: 算法在合成数据和真实数据上进行了测试和验证，结果表明在高速运动的场景下表现优越，可靠性和准确性得到提升。

Conclusion: 该方法有效融合了事件相机与传统相机的优势，大幅提升了高速物体的实时6-DoF位姿跟踪能力，尤其在运动模糊严重的情况下表现突出。

Abstract: Tracking the position and orientation of objects in space (i.e., in 6-DoF) in
real time is a fundamental problem in robotics for environment interaction. It
becomes more challenging when objects move at high-speed due to frame rate
limitations in conventional cameras and motion blur. Event cameras are
characterized by high temporal resolution, low latency and high dynamic range,
that can potentially overcome the impacts of motion blur. Traditional RGB
cameras provide rich visual information that is more suitable for the
challenging task of single-shot object pose estimation. In this work, we
propose using event-based optical flow combined with an RGB based global object
pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the
core advantages of both types of vision sensors. Specifically, we propose an
event-based optical flow algorithm for object motion measurement to implement
an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF
velocity with low frequency estimated pose from the global pose estimator, the
method can track pose when objects move at high-speed. The proposed algorithm
is tested and validated on both synthetic and real world data, demonstrating
its effectiveness, especially in high-speed motion scenarios.

</details>


### [64] [Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification](https://arxiv.org/abs/2508.14779)
*Mengliang Zhang,Jacob M. Luber*

Main category: cs.CV

TL;DR: 本论文系统性研究了病理基础模型（PFM）在全切片图像诊断中因医院来源差异而带来的领域偏置，并提出了一种轻量级对抗框架来去除模型中隐含的医院特征，从而提升模型在不同医院间的泛化与公平性。


<details>
  <summary>Details</summary>
Motivation: PFM在病理图像诊断领域表现优异，但不同医院因硬件、预处理等因素生成的图像存在显著差异，导致模型容易学习到医院特有特征，影响其跨医院推广应用的安全性和效果。因此有必要系统性分析和解决PFM中的医院偏置问题。

Method: 作者提出：1）构建PFM领域偏置量化流程；2）对多种模型的领域偏置进行评估和比较；3）设计一种轻量级对抗框架，在不修改模型编码器的前提下，通过增加可训练的adapter和引入与梯度反转层连接的领域分类器，有效去除隐藏表示中的医院特征，仅保留与任务相关的跨域表征。

Result: 在多中心病理数据集上的实验显示，该方法显著降低了医院来源的可预测性，即有效去除了医院偏置，同时在疾病分类尤其是在未见过的医院场景下能够保持甚至提升模型的任务性能。同时，辅助分析（如医院检测和特征空间可视化）也验证了方法有效性。

Conclusion: 所提出的方法能有效消除PFM中医院特异性偏置，同时兼顾甚至提升了疾病分类准确率，为PFM的临床安全部署提供了切实可行的技术路径和分析框架。

Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in
whole-slide image (WSI) diagnosis. However, pathology images from different
hospitals often vary due to differences in scanning hardware and preprocessing
styles, which may lead PFMs to inadvertently learn hospital-specific features,
posing risks for clinical deployment. In this work, we present the first
systematic study of domain bias in PFMs arising from hospital source
characteristics. Specifically, we (1) construct a pipeline for quantifying
domain bias in PFMs, (2) evaluate and compare the performance of multiple
models, and (3) propose a lightweight adversarial framework that removes latent
hospital-specific features from frozen representations without modifying the
encoder itself. By introducing a trainable adapter and a domain classifier
connected through a gradient reversal layer (GRL), our method learns
task-discriminative yet domain-invariant representations. Experiments on
multi-center histopathology datasets demonstrate that our approach
substantially reduces domain predictability while maintaining or even improving
disease classification performance, particularly in out-of-domain (unseen
hospital) scenarios. Further analyses, including hospital detection and feature
space visualization, confirm the effectiveness of our method in mitigating
hospital bias. We will provide our code based on acceptance.

</details>


### [65] [MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow](https://arxiv.org/abs/2508.14797)
*Kihyun Na,Junseok Oh,Youngkwan Cho,Bumjin Kim,Sungmin Cho,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种面向低质车牌图像序列的多帧车牌恢复与识别方法MF-LPR²，并构建了新的真实车牌数据集，对抗动态场景下常见的模糊与低分辨率问题。实验显示该方法远优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的车牌识别系统在处理行车记录仪等动态、低质量场景下的车牌区域时准确率低，特别容易受到低分辨率、运动模糊与强烈眩光影响，且基于预训练先验的生成模型难以有效还原出高质量图像。实际执法和监控等场景急需更强健的低质车牌识别方法。

Method: 作者提出MF-LPR²框架，通过多帧车牌图像联合恢复和识别，使用最先进的光流估计器及自设计的异常光流检测与修正算法，充分利用序列帧的时空一致性，进行准确的帧对齐和特征融合。该方法无需显式依赖预训练生成先验，更好恢复证据信息。同时，构建了包含200组真实低质-高质车牌序列配对的RLPR数据集进行评测。

Result: 在PSNR、SSIM、LPIPS等图像恢复指标上，MF-LPR²均显著超过8个现有车牌恢复模型。在识别任务中，MF-LPR²的准确率达86.44%，远高于最佳单帧基线（14.04%）与多帧基线（82.55%）。消融实验进一步验证了核心模块（光流修正等）对性能提升的关键作用。

Conclusion: 通过创新性地结合多帧序列信息、光流过滤与修正策略，MF-LPR²在真实复杂环境下极大提升了车牌恢复与识别准确率，展示了新方法在实际执法、监控领域的巨大应用潜力。

Abstract: License plate recognition (LPR) is important for traffic law enforcement,
crime investigation, and surveillance. However, license plate areas in dash cam
images often suffer from low resolution, motion blur, and glare, which make
accurate recognition challenging. Existing generative models that rely on
pretrained priors cannot reliably restore such poor-quality images, frequently
introducing severe artifacts and distortions. To address this issue, we propose
a novel multi-frame license plate restoration and recognition framework,
MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and
aggregating neighboring frames instead of relying on pretrained knowledge. To
achieve accurate frame alignment, we employ a state-of-the-art optical flow
estimator in conjunction with carefully designed algorithms that detect and
correct erroneous optical flow estimations by leveraging the spatio-temporal
consistency inherent in license plate image sequences. Our approach enhances
both image quality and recognition accuracy while preserving the evidential
content of the input images. In addition, we constructed a novel Realistic LPR
(RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of
low-quality license plate image sequences and high-quality pseudo ground-truth
images, reflecting the complexities of real-world scenarios. In experiments,
MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM,
and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an
accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and
the multi-frame LPR (82.55%) among the eleven baseline models. The results of
ablation studies confirm that our filtering and refinement algorithms
significantly contribute to these improvements.

</details>


### [66] [DINOv3 with Test-Time Training for Medical Image Registration](https://arxiv.org/abs/2508.14809)
*Shansong Wang,Mojtaba Safari,Mingzhe Hu,Qiang Li,Chih-Wei Chang,Richard LJ Qiu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练数据的医学图像配准方法，利用冻结的DINOv3特征编码器，在测试时对变形场进行优化，取得了优异的配准精度和变形规则性，适合临床应用。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的医学图像配准方法大都依赖大量训练数据，这限制了其实际临床应用。

Method: 作者提出了一条全新的训练免疫管线：使用冻结的DINOv3编码器，在特征空间内于测试时优化变形场，无需训练过程。

Result: 在Abdomen MR-CT和ACDC cardiac MRI两个基准测试集上，该方法取得了目前最优的平均Dice系数和最低的HD95、SDLogJ等评估指标，显著优于初始配准。

Conclusion: 在测试时于预训练特征空间中优化变形场，可以提供实用且通用的临床配准方案，无需额外训练数据。

Abstract: Prior medical image registration approaches, particularly learning-based
methods, often require large amounts of training data, which constrains
clinical adoption. To overcome this limitation, we propose a training-free
pipeline that relies on a frozen DINOv3 encoder and test-time optimization of
the deformation field in feature space. Across two representative benchmarks,
the method is accurate and yields regular deformations. On Abdomen MR-CT, it
attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th
percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard
deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it
improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked
gain over the initial alignment. The results indicate that operating in a
compact foundation feature space at test time offers a practical and general
solution for clinical registration without additional training.

</details>


### [67] [Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization](https://arxiv.org/abs/2508.14811)
*Canyu Zhao,Xiaoman Li,Tianjian Feng,Zhiyue Zhao,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: Tinker是一款无需针对每个场景单独微调的高保真3D编辑框架，能够在一张或几张图片下实现多视角一致的编辑，并开放了大规模多视角编辑数据集，具备最先进编辑与新视角合成性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法往往需要对每个场景进行繁重的优化处理，难以实现高效、通用和一致性强的多视角编辑，限制了3D内容的大规模应用。

Method: Tinker利用了预训练扩散模型的3D潜在感知能力，开发了两大核心组件：1）参考多视角编辑器，实现精准且跨视角一致的编辑；2）任意视角到视频合成器，结合视频扩散模型的时空先验，可从稀疏输入中补全场景并生成新颖视角。无需针对新场景训练或优化。作者还自建了首个大规模多视角编辑数据集。

Result: Tinker在多个编辑任务、新视角合成及渲染增强任务中均达到了最先进效果，极大降低了高质量3D内容创作的门槛，支持极少量输入（一到两张图片）即可得到多视角一致的编辑结果。

Conclusion: Tinker是朝着可扩展、零样本3D编辑迈出的重要一步，为通用、高效的3D内容创建带来了新的可能性，并贡献了相关大数据集推动社区发展。

Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that
operates in both one-shot and few-shot regimes without any per-scene
finetuning. Unlike prior techniques that demand extensive per-scene
optimization to ensure multi-view consistency or to produce dozens of
consistent edited input views, Tinker delivers robust, multi-view consistent
edits from as few as one or two images. This capability stems from repurposing
pretrained diffusion models, which unlocks their latent 3D awareness. To drive
research in this space, we curate the first large-scale multi-view editing
dataset and data pipeline, spanning diverse scenes and styles. Building on this
dataset, we develop our framework capable of generating multi-view consistent
edited views without per-scene training, which consists of two novel
components: (1) Referring multi-view editor: Enables precise, reference-driven
edits that remain coherent across all viewpoints. (2) Any-view-to-video
synthesizer: Leverages spatial-temporal priors from video diffusion to perform
high-quality scene completion and novel-view generation even from sparse
inputs. Through extensive experiments, Tinker significantly reduces the barrier
to generalizable 3D content creation, achieving state-of-the-art performance on
editing, novel-view synthesis, and rendering enhancement tasks. We believe that
Tinker represents a key step towards truly scalable, zero-shot 3D editing.
Project webpage: https://aim-uofa.github.io/Tinker

</details>


### [68] [Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives](https://arxiv.org/abs/2508.14812)
*Haoyu Zhao,Jiaxi Gu,Shicong Wang,Xing Zhang,Hang Xu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频-语言检索框架，通过细粒度特征对齐和创新的推理流程，在无需额外大规模预训练的情况下取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作普遍依赖大规模预训练来提升视频-语言检索的准确率，但这导致了高昂的计算成本。同时，视频和文本中细粒度信息未被充分利用。作者旨在设计一种高效易用、能充分挖掘细粒度特征、同时减少训练资源消耗的方法。

Method: 作者设计了一种粗到细（coarse-to-fine）学习目标，包括对比学习和匹配学习。通过Granularity-Aware Representation模块，基于视频帧与字幕单词的相似性分析，自动提取用于训练的细粒度数据。在推理阶段，发现重复关键词能加强视频-文本对齐，因此提出包含投票机制和新型Matching Entropy指标的推理流程，无需额外预训练即可提升检索表现。

Result: 在MSR-VTT、DiDeMo等四个基准数据集上，所提方法均优于以往方法。在MSR-VTT上Recall@1提升2.1%，在DiDeMo上提升1.6%。

Conclusion: 提出的新框架和推理方法能有效提升视频-语言检索精度，降低资源消耗，并具备良好的泛化能力。

Abstract: The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.

</details>


### [69] [TransLight: Image-Guided Customized Lighting Control with Generative Decoupling](https://arxiv.org/abs/2508.14814)
*Zongming Li,Lianghui Zhu,Haocheng Shen,Longjin Ran,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: TransLight提出了一种高保真和高自由度的光照效果转移框架，首次实现了复杂光效从参考图像到目标图像的自定义迁移，并显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 目前的光照编辑方法很难在保证内容完整性的同时，对光效进行个性化控制，特别是在将复杂光效从参考图像迁移到目标图像时，实际应用受限。

Method: 该方法提出了“生成性解耦”策略，利用两组微调的扩散模型，将图像内容和光照效果有效分离，构建了百万级的内容-光效样本三元组大数据集。训练时，采用IC-Light作为生成模型，并以参考光照图像作为条件进行训练，从而实现光照效果的高度可控与自然迁移。

Result: 实验表明，TransLight首次实现了任意图像间复杂光效的转移，效果优于现有所有方法，且实现了更为灵活和自定义的光照控制。

Conclusion: TransLight为图像光照和编辑领域开辟了新的方向，极大提升了光照编辑的实用性和灵活性，对照明和风格迁移等任务具有重要意义。

Abstract: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.

</details>


### [70] [EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention](https://arxiv.org/abs/2508.14856)
*Lakshmi Annamalai,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: 本文提出EventSSEG方法，利用事件相机和概率注意力机制进行道路分割，无需大量标注数据，实验结果达到了最新性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的摄像头在道路分割时难以做到低延迟和低计算量。事件相机因为低功耗成为有吸引力的替代，但事件相机的数据领域缺乏充足标注数据，阻碍了深度学习模型的应用和权重迁移。

Method: 提出了EventSSEG，一种只用事件数据（event only computing）并结合概率注意力机制的自监督道路分割方法。为了克服缺乏标注事件数据的问题，方法采用了基于事件的自监督学习，无需大量人工标注。

Result: 在DSEC-Semantic和DDD17两个数据集上实验表明，EventSSEG用极少标注事件即可达到业界最优性能。

Conclusion: EventSSEG充分发挥了事件相机的优势，同时克服了缺少标注数据的难题，为低功耗、高性能的道路分割提供了新方法。

Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low
latency and low compute solutions using frame based cameras remains a
challenge. Event cameras offer a promising alternative. To leverage their low
power sensing, we introduce EventSSEG, a method for road segmentation that uses
event only computing and a probabilistic attention mechanism. Event only
computing poses a challenge in transferring pretrained weights from the
conventional camera domain, requiring abundant labeled data, which is scarce.
To overcome this, EventSSEG employs event-based self supervised learning,
eliminating the need for extensive labeled data. Experiments on DSEC-Semantic
and DDD17 show that EventSSEG achieves state of the art performance with
minimal labeled events. This approach maximizes event cameras capabilities and
addresses the lack of labeled events.

</details>


### [71] [Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on Largescale Clinical Imaging](https://arxiv.org/abs/2508.14878)
*Lucas W. Remedios,Chloe Cho,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Thomas A. Lasko,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 本研究通过分析CT和MRI影像数据，建立了胰腺形态随年龄变化的参照标准，并发现2型糖尿病患者胰腺形态与正常人显著不同。


<details>
  <summary>Details</summary>
Motivation: 胰腺随年龄及疾病状态变化，但尚缺乏系统的影像学参照标准，不利于2型糖尿病等疾病的早期检测和诊断。

Method: 收集2533例腹部CT/MRI临床影像，采用自动分割与特征提取，分析13项胰腺形态学指标，并对1350名匹配的2型糖尿病及正常对照者应用GAMLSS回归比较差异。

Result: 经混杂因素调整，13项形态学指标中有10项在2型糖尿病与正常组间显示随年龄变化存在统计学差异（校正后p < 0.05）；MRI与CT AI测量的结果存在差异。

Conclusion: 2型糖尿病患者胰腺体积及形态随年龄呈现不同于健康人的变化趋势，并再次证实其胰腺体积较小；本研究提供了正常人群胰腺形态的年龄分布参考数据。

Abstract: Purpose: Understanding how the pancreas changes is critical for detecting
deviations in type 2 diabetes and other pancreatic disease. We measure pancreas
size and shape using morphological measurements from ages 0 to 90. Our goals
are to 1) identify reliable clinical imaging modalities for AI-based pancreas
measurement, 2) establish normative morphological aging trends, and 3) detect
potential deviations in type 2 diabetes.
  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged
with abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,
segmented the pancreas using automated methods, and extracted 13 morphological
pancreas features across the lifespan. First, we assessed CT and MRI
measurements to determine which modalities provide consistent lifespan trends.
Second, we characterized distributions of normative morphological patterns
stratified by age group and sex. Third, we used GAMLSS regression to model
pancreas morphology trends in 1350 patients matched for age, sex, and type 2
diabetes status to identify any deviations from normative aging associated with
type 2 diabetes.
  Results: When adjusting for confounders, the aging trends for 10 of 13
morphological features were significantly different between patients with type
2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons
corrections). Additionally, MRI appeared to yield different pancreas
measurements than CT using our AI-based method.
  Conclusions: We provide lifespan trends demonstrating that the size and shape
of the pancreas is altered in type 2 diabetes using 675 control patients and
675 diabetes patients. Moreover, our findings reinforce that the pancreas is
smaller in type 2 diabetes. Additionally, we contribute a reference of lifespan
pancreas morphology from a large cohort of non-diabetic control patients in a
clinical setting.

</details>


### [72] [MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition](https://arxiv.org/abs/2508.14889)
*Mert Kiray,Alvaro Ritter,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 本文提出了一种多骨架对比学习方法（MS-CLR），能够提升骨架动作识别中模型的泛化能力，并在NTU RGB+D 60和120数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作识别的对比学习方法仅依赖单一骨架描述方式（skeleton convention），但不同数据集之间的骨架关节点结构多样，现有方法泛化性不足。本文旨在解决多骨架结构带来的泛化能力和表现力缺陷。

Method: 提出MS-CLR，即多骨架对比学习框架，将同一动作序列在不同骨架结构下的表示进行对齐，提升模型对结构不变性的感知。为适应不同骨架，改进了ST-GCN架构，并设计了统一的表示方式处理不同关节点布局和尺度。

Result: 在NTU RGB+D 60和NTU RGB+D 120两个主流数据集上，MS-CLR方法均优于单骨架对比学习基线。多骨架集成方法进一步提升表现，在两个数据集上均获得新SOTA。

Conclusion: MS-CLR提升了骨架动作识别任务中的特征泛化性和表达力，能适应不同关节点布局的数据，效果优于现有方法。

Abstract: Contrastive learning has gained significant attention in skeleton-based
action recognition for its ability to learn robust representations from
unlabeled data. However, existing methods rely on a single skeleton convention,
which limits their ability to generalize across datasets with diverse joint
structures and anatomical coverage. We propose Multi-Skeleton Contrastive
Learning (MS-CLR), a general self-supervised framework that aligns pose
representations across multiple skeleton conventions extracted from the same
sequence. This encourages the model to learn structural invariances and capture
diverse anatomical cues, resulting in more expressive and generalizable
features. To support this, we adapt the ST-GCN architecture to handle skeletons
with varying joint layouts and scales through a unified representation scheme.
Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR
consistently improves performance over strong single-skeleton contrastive
learning baselines. A multi-skeleton ensemble further boosts performance,
setting new state-of-the-art results on both datasets.

</details>


### [73] [GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects](https://arxiv.org/abs/2508.14891)
*Licheng Shen,Saining Zhang,Honghan Li,Peilin Yang,Zihao Huang,Zongzheng Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出一种将几何与运动统一建模的表征方法，并在90类多关节物体数据集上评估，显著提升了多部分物体的三维重建和运动估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将几何重建与运动估计分开处理，导致流程复杂且不易扩展，尤其难以处理多部分复杂关节物体。因此需要一种能够统一、简化流程并提升扩展性的表示和方法。

Method: 作者提出用关节三维高斯模型（articulated 3D Gaussians）统一描述物体几何和运动分解。同时构建了包含90个多关节物体（20类，多个配件/运动类型）的新数据集MPArt-90，用于系统评估方法的扩展性和泛化性。

Result: 实验结果表明，该方法在多种物体类型下表现出更高的分部几何重建和运动估计精度，尤其对于拥有多达20个活动部件的复杂物体，显著优于只能处理2-3部分的以往方法。

Conclusion: 统一的关节物体表征大幅提升了多分部物体重建的准确性和扩展性，在下游如机器人仿真和人体场景交互等任务中具备广泛应用潜力。

Abstract: Reconstructing articulated objects is essential for building digital twins of
interactive environments. However, prior methods typically decouple geometry
and motion by first reconstructing object shape in distinct states and then
estimating articulation through post-hoc alignment. This separation complicates
the reconstruction pipeline and restricts scalability, especially for objects
with complex, multi-part articulation. We introduce a unified representation
that jointly models geometry and motion using articulated 3D Gaussians. This
formulation improves robustness in motion decomposition and supports
articulated objects with up to 20 parts, significantly outperforming prior
approaches that often struggle beyond 2--3 parts due to brittle initialization.
To systematically assess scalability and generalization, we propose MPArt-90, a
new benchmark consisting of 90 articulated objects across 20 categories, each
with diverse part counts and motion configurations. Extensive experiments show
that our method consistently achieves superior accuracy in part-level geometry
reconstruction and motion estimation across a broad range of object types. We
further demonstrate applicability to downstream tasks such as robotic
simulation and human-scene interaction modeling, highlighting the potential of
unified articulated representations in scalable physical modeling.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: 本文提出一种新的视觉故事生成方法，将视觉故事看作是图像描述的扩展，分步骤先生成图像描述，再将其转化为连贯的故事文本。方法不仅提升了故事生成的质量，还加速了训练过程，并提出了一种全新评价指标 ideality，用于衡量与理想模型的差距。


<details>
  <summary>Details</summary>
Motivation: 视觉故事生成要求生成兼具图像内容和故事连贯性的文本，以前的方法难以平衡这两点。作者希望设计一种方法，在保证贴合图像的同时，又能生成更具故事性的文本。

Method: 先用视觉到语言模型为输入图像获得图像描述（caption），再用语言到语言方法把图像描述转化为连贯的故事文本，实现画像描述与故事生成的分步结合。最后评估方法还提出了“ideality”新指标评测模型表现。

Result: 实验表明，该统一的描述-故事框架提升了故事生成的质量，对比此前多种方法在评价指标与人工评测上均有优势，同时训练更快，框架易于复现。此外，提出的 idealiy 指标能模拟人类标准下的“理想”表现。

Conclusion: 将图像描述和视觉故事生成整合在统一框架下，不仅提升了生成故事的相关性和连贯性，也提高了训练效率和可复用性；新提出的评价方法对任务标准化和人类类比评测有积极作用。

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [75] [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
*Kezia Oketch,John P. Lalor,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 本文首次基于分类体系对斯瓦希里语自然语言处理（NLP）进行评估，通过收集肯尼亚说话者的健康相关自由文本响应，分析模型在多元社会语言背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有斯瓦希里语NLP评测未充分考虑社会语言学多样性，尤其是各部落、城市俚语、混合用语和借词对模型的影响。

Method: 收集了2170条肯尼亚人口的健康相关自由文本数据，开发了结构化社会语言学分类法，并用该分类法分析不同类型的预训练和指令微调语言模型在预测中的错误。

Result: 数据显示自由文本中频繁出现部落特征、城市俚语、代码混合和大量借词。基于分类法分析揭示，社会语言多样性会显著影响模型性能，不同模型面对不同类型语言现象时出错情况各异。

Conclusion: 提出了一种更具文化基础的模型评测框架，突出了社会语言学变异对NLP模型表现的重要影响，为今后设计更具包容性的非英语NLP评估提供了新思路。

Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing
gaps in sociolinguistic diversity. Drawing on health-related psychometric
tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.
The data exhibits tribal influences, urban vernacular, code-mixing, and
loanwords. We develop a structured taxonomy and use it as a lens for examining
model prediction errors across pre-trained and instruction-tuned language
models. Our findings advance culturally grounded evaluation frameworks and
highlight the role of sociolinguistic variation in shaping model performance.

</details>


### [76] [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 本文基于英汉新闻语料，通过分析功能性成分（如状语）的语序差异，揭示了两种语言在信息结构上的系统性偏好与动态适应性。


<details>
  <summary>Details</summary>
Motivation: 现有英汉对比研究多侧重语法或词汇层面，对功能块（尤其是具有状语功能的块）在新闻文本中的分布和顺序关注较少。随着大模型标注提升数据精度，有必要系统探讨英汉新闻文本中此类功能块的排序与信息结构的关联。

Method: 基于大语言模型注释的英汉可比新闻语料库，统计并分析英汉新闻中具状语功能的块的典型位置、分布模式和共现时的排列顺序，重点考察主谓宾结构下的不同表现。

Result: 1）英语新闻倾向于核心信息先行，功能块多后置；汉语新闻倾向于先交代背景，功能块多前置。2）在SVO结构中，两种语言功能块分布有所不同，汉语前置倾向更突出。3）共现时，英汉新闻在功能块顺序调整上均具有高度灵活性，受信息和语用目的驱动。

Conclusion: 英汉新闻语序既有系统偏好，也具动态适应性。此研究为英汉新闻文本信息结构的对比分析提供了新的实证依据。

Abstract: Based on comparable English-Chinese news corpora annotated by Large Language
Model (LLM), this paper attempts to explore the differences in constituent
order of English-Chinese news from the perspective of functional chunks with
adverbial roles, and analyze their typical positional preferences and
distribution patterns. It is found that: (1) English news prefers linear
narrative of core information first, and functional chunks are mostly
post-positioned, while Chinese news prefers overall presentation mode of
background first, and functional chunks are often pre-positioned; (2) In SVO
structure, both English and Chinese news show differences in the distribution
of functional chunks, but the tendency of Chinese pre-positioning is more
significant, while that of English post-positioning is relatively mild; (3)
When function blocks are co-occurring, both English and Chinese news show high
flexibility, and the order adjustment is driven by information and pragmatic
purposes. The study reveals that word order has both systematic preference and
dynamic adaptability, providing new empirical support for contrastive study of
English-Chinese information structure.

</details>


### [77] [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
*Tim Luka Horstmann,Baptiste Geisenberger,Mehwish Alam*

Main category: cs.CL

TL;DR: 本文介绍了T-REX，这是一个基于先进大语言模型，能对多模态、多语言表格进行事实核查并为普通用户友好交互的新工具。


<details>
  <summary>Details</summary>
Motivation: 当前表格事实核查虽然取得进展，但现有方法对普通用户不够友好，限制了其广泛应用。该研究希望解决非专家也能使用强大事实核查工具的问题。

Method: 提出T-REX系统，利用最先进的指令微调大模型，支持多模态与多语言表格上的文本主张核查，具备交互式用户界面，设计易用且高透明度。

Result: 实现了可在线公开访问的T-REX工具，实现在表格事实核查任务上准确可靠，并大大降低了使用门槛。

Conclusion: T-REX让先进表格事实核查技术对非专家开放，提高了可用性和透明度，有助于这个领域的技术推广和实际应用。

Abstract: Verifying textual claims against structured tabular data is a critical yet
challenging task in Natural Language Processing with broad real-world impact.
While recent advances in Large Language Models (LLMs) have enabled significant
progress in table fact-checking, current solutions remain inaccessible to
non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),
the first live, interactive tool for claim verification over multimodal,
multilingual tables using state-of-the-art instruction-tuned reasoning LLMs.
Designed for accuracy and transparency, T-REX empowers non-experts by providing
access to advanced fact-checking technology. The system is openly available
online.

</details>


### [78] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文研究如何在无法获得金标准答案的情况下，评估大模型生成的SQL查询的可信度。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型广泛应用于text-to-SQL任务时，输出的SQL查询可能存在错误，且通常无法直接获得标准答案。因此，开发一种无需参考答案也能评估生成SQL准确性的方法非常重要。

Method: 作者分别研究了黑盒（black-box）和白盒（white-box）两类可信度估算方法，在跨领域text-to-SQL数据集上进行了系统评估。黑盒方法侧重基于生成结果的一致性，白盒方法则利用模型logits及SQL语法知识。作者还引入了基于执行结果的补充信号。

Result: 实验显示，在黑盒方法中，基于一致性的方式表现最佳；在白盒场景下，结合SQL语法解释logits的方法优势明显。此外，基于查询的执行结果作为补充信号可进一步提升所有方法的效果。

Conclusion: 一致性和SQL-syntax-aware的方法在不同场景下分别适用于LLMs的text-to-SQL可信度估算，结合查询的执行信息能增强整体评估能力。

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [79] [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
*Badrinath Ramakrishnan,Akshaya Balaji*

Main category: cs.CL

TL;DR: 本文分析了微调大型语言模型（LLM）时数据记忆和隐私泄露的问题，并提出了多层次隐私保护框架。通过多种方法组合，可大幅减少敏感数据泄漏，同时基本保留模型效用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各类任务中的广泛应用，其在微调过程中对训练数据的记忆能力会带来严重的隐私风险。尤其是当敏感数据被多次暴露于微调过程中时，模型泄漏隐私的概率显著增加，因此需要深入分析和解决这一问题。

Method: 作者对多款主流LLM（如GPT-2、Phi-3、Gemma-2）进行受控实验，评估微调过程中敏感数据泄漏率。同时，提出并评估了四种互补的隐私保护措施：语义数据去重、生成过程中的差分隐私、基于熵的过滤、基于模式的内容过滤。

Result: 实验数据显示，重复敏感数据微调会使隐私泄漏率从基线的0-5%提升到60-75%，平均提升64.2%。组合上述四种隐私保护方法后，敏感数据泄漏率降至0%，同时模型效用仅下降至原来的94.7%。

Conclusion: 微调LLM存在严重的隐私泄露风险，但结合多层次的隐私保护方法能够有效防止数据泄漏，并在很大程度上保留模型性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks, but their tendency to memorize
training data poses significant privacy risks, particularly during fine-tuning
processes. This paper presents a comprehensive empirical analysis of data
memorization in fine-tuned LLMs and introduces a novel multi-layered privacy
protection framework. Through controlled experiments on modern LLM
architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that
fine-tuning with repeated sensitive data increases privacy leakage rates from
baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across
tested models. We propose and rigorously evaluate four complementary privacy
protection methods: semantic data deduplication, differential privacy during
generation, entropy-based filtering, and pattern-based content filtering. Our
experimental results show that these techniques can reduce data leakage to 0%
while maintaining 94.7% of original model utility.

</details>


### [80] [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
*Sonakshi Chauhan,Maheep Chaudhary,Koby Choy,Samuel Nellessen,Nandi Schoots*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）中信息的收集位置及其在各层之间的传播方式，尤其关注了标点符号和推理成分在模型处理中的作用及机制差异。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解模型内部各层如何处理输入信息，尤其是标点等常被低估成分，揭示模型推理机制及其可解释性。前人发现标点对注意力和记忆有特殊作用，本文进一步量化和分析其效果。

Method: 作者采用基于干预的方法（如必要性/充分性测试、层互换等），针对GPT-2、DeepSeek、Gemma等模型，评估不同输入成分（标点、主语、形容词等）在各层信息传播中的作用，并实验推理规则（如条件语句、全称量化）的处理机制。

Result: 发现GPT-2在多个层面对标点既依赖且足以维持原有性能，而DeepSeek和Gemma依赖性较小；对于推理规则，不同类型内容（如条件句与全称量化）在各模型层内的处理方式也有显著差异。

Conclusion: 标点和推理成分在不同LLM中的内在机制和重要性存在模型特异性，这一发现为模型可解释性研究提供了新的视角。

Abstract: In this paper we explore where information is collected and how it is
propagated throughout layers in large language models (LLMs). We begin by
examining the surprising computational importance of punctuation tokens which
previous work has identified as attention sinks and memory aids. Using
intervention-based techniques, we evaluate the necessity and sufficiency (for
preserving model performance) of punctuation tokens across layers in GPT-2,
DeepSeek, and Gemma. Our results show stark model-specific differences: for
GPT-2, punctuation is both necessary and sufficient in multiple layers, while
this holds far less in DeepSeek and not at all in Gemma. Extending beyond
punctuation, we ask whether LLMs process different components of input (e.g.,
subjects, adjectives, punctuation, full sentences) by forming early static
summaries reused across the network, or if the model remains sensitive to
changes in these components across layers. Extending beyond punctuation, we
investigate whether different reasoning rules are processed differently by
LLMs. In particular, through interchange intervention and layer-swapping
experiments, we find that conditional statements (if, then), and universal
quantification (for all) are processed very differently. Our findings offer new
insight into the internal mechanisms of punctuation usage and reasoning in LLMs
and have implications for interpretability.

</details>


### [81] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
*Chen Xu,Dawei Yang*

Main category: cs.CL

TL;DR: 本文提出了一种专为扩散式大型语言模型（DLLMs）量身定制的后训练量化（PTQ）框架——DLLMQuant，解决现有PTQ方法在DLLMs上遇到的精度和泛化下降问题，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的PTQ方法在应用于DLLMs时，会导致模型精度大幅下降和泛化能力减弱，无法满足实际部署需求。因此，亟需设计能够适应DLLMs机制的新型PTQ方法。

Method: 作者系统分析了DLLMs中动态掩码、迭代生成和双向注意力与量化之间的冲突，并提出三项创新技术：（1）时序-掩码自适应采样（TMAS），用于精确刻画不同步、不同掩码下的分布特性；（2）交互感知激活量化（IA-AQ），根据双向注意力动态分配量化资源；（3）确定性引导量化（CGQ），将掩码状态和分数结合到误差补偿中，提升权重量化效果。

Result: 实验表明，DLLMQuant在提升了DLLMs量化后性能的同时，大幅改善了推理效率，优于现有PTQ方法（如AWQ等）。

Conclusion: DLLMQuant有效消除了DLLMs与现有PTQ方法之间的兼容性障碍，显著改善了量化后模型的精度与效率，适用于未来DLLMs的高效部署。

Abstract: Diffusion-based large language models (DLLMs) have shown promise for
non-autoregressive text generation, but their deployment is constrained by
large model sizes and heavy computational costs. Post-training quantization
(PTQ), a widely used method for compressing and accelerating Large Language
Models (LLMs), suffers from severe accuracy degradation and reduced
generalization performance when directly applied to DLLMs (e.g., AWQ suffers a
16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key
mechanisms - dynamic masking, iterative generation, bidirectional attention -
clash with quantization. We identify three core issues: 1) Iterative generation
and dynamic masking ratios lead to distinct token distributions across decoding
steps, which are not adequately captured by existing PTQ calibration methods;
2) Quantization errors are accumulated and amplified progressively during
iteration in DLLMs, causing quantized models to perform worse as decoding steps
progress; 3) Unmasked tokens stabilize while masked remain probabilistic,
making overall feature distribution incompatible with existing PTQ methods. To
address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,
which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling
(TMAS), a calibration method that accounts for both time and mask factors, with
the capacity to capture distributions across timesteps. 2) Interaction-Aware
Activation Quantization (IA-AQ), which utilizes bidirectional attention's
interaction signals to dynamically allocate quantization resources. 3)
Certainty-Guided Quantization (CGQ), which integrates mask status and token
scores as key weighting criteria into error compensation, making weight
quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves
significant performance gains while enhancing efficiency.

</details>


### [82] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
*Xian Gao,Jiacheng Ruan,Zongyun Zhang,Jingsheng Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.CL

TL;DR: 论文提出了一个新的跨学科多模态学术论文评审基准MMReview，用于系统评价大语言模型（LLMs）在生成评审意见方面的能力。该基准覆盖17个研究领域和多种多模态内容，实验验证了该基准的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被用于协助学术论文评审，但目前缺乏能全面评估其在真实、多模态评审任务中表现的统一标准和基准，导致模型在准确性、全面性和与人类意见一致性上的评估不足。

Method: 作者构建了MMReview基准，涵盖人工智能、自然科学、工程科学和社会科学等4大类共17个研究方向。基准包含240篇论文的多模态内容及专家点评，并设计了13个细分任务，覆盖评审生成、结果归纳、人类偏好一致性和对抗鲁棒性等方面。作者在16个开源模型和5个闭源模型上进行了实验。

Result: 实验展示了这一基准的全面性和挑战性，能够有效对比LLMs和多模态LLMs（MLLMs）的评审能力，对模型生成的数据多维度表现进行了系统测试。

Conclusion: MMReview为自动化学术论文评审系统的评估和未来开发建立了统一、标准化的测试基础，对于推动该领域进步具有重要意义。

Abstract: With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

</details>


### [83] [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
*Xinhua Chen,Sitao Huang,Cong Guo,Chiyue Wei,Yintao He,Jianyi Zhang,Hai "Hellen" Li,Yiran Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Diffusion Scratchpad (DPad) 的新方法，有效提升了基于扩散的大语言模型（dLLMs）在文本生成中的推理效率，在大幅加速的同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式大语言模型在生成文本时虽并行性强，但每步需预测所有后续token，计算开销大而冗余，亟需提升生成效率。

Method: DPad是在无需重新训练的前提下，通过限制注意力仅关注邻近suffix tokens来减少冗余。具体包括两种策略：(1) 滑动窗口，保留一个定长的suffix窗口；(2) 距离衰减dropout，计算注意力前剔除距离较远的suffix token。此方法兼容现有优化手段，代码实现简洁。

Result: 在LLaDA-1.5和Dream等多个基准测试中，DPad实现了最高61.4倍的加速，并能保持与原始dLLMs相当的准确率。

Conclusion: DPad极大提升了扩散式大语言模型的推理效率，为大规模和长序列生成提供了可扩展的解决方案，有良好的实际应用前景。

Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by
framing decoding as a denoising process, but suffer from high computational
overhead since they predict all future suffix tokens at each step while
retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a
training-free method that restricts attention to a small set of nearby suffix
tokens, preserving fidelity while eliminating redundancy. DPad integrates two
strategies: (i) a sliding window, which maintains a fixed-length suffix window,
and (ii) distance-decay dropout, which deterministically removes distant suffix
tokens before attention computation. This simple design is compatible with
existing optimizations such as prefix caching and can be implemented with only
a few lines of code. Comprehensive evaluations across multiple benchmarks on
LLaDA-1.5 and Dream models demonstrate that DPad delivers up to
$\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable
accuracy, highlighting its potential for efficient and scalable long-sequence
inference. Our code is available at https://github.com/Crys-Chen/DPad.

</details>


### [84] [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
*Johannes Zschache,Tilman Hartwig*

Main category: cs.CL

TL;DR: 本文系统性评估了文本分类推理阶段中，大语言模型（LLMs）在准确率与能耗之间的权衡，发现较大模型推理能耗更高且准确率不一定更好，且能耗受模型、硬件影响很大。推理时间可以作为能耗的简易估计。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs部署广泛，但大部分研究关注训练时能耗，推理阶段能耗则关注较少。该文希望补足推理能耗与准确率关系的研究空白，推进NLP任务的可持续发展。

Method: 作者通过实证分析，在不同模型结构、模型规模和硬件配置下，评估文本分类推理的模型准确率与能耗之间的权衡，并研究推理能耗的影响因素。

Result: 结果显示，准确率最好的模型也可以非常节能，而规模更大的LLMs能耗明显增加但准确率未必更高。推理期间能耗差异巨大（mWh到kWh不等），主要受模型和硬件影响。另外，能耗与推理时长高度相关，推理时长可用于能耗估算。

Conclusion: 研究为可持续AI开发提供了实用洞见，可指导科研、工业和政策制定者在NLP应用中实现性能与资源消耗的平衡。

Abstract: The increasing deployment of large language models (LLMs) in natural language
processing (NLP) tasks raises concerns about energy efficiency and
sustainability. While prior research has largely focused on energy consumption
during model training, the inference phase has received comparatively less
attention. This study systematically evaluates the trade-offs between model
accuracy and energy consumption in text classification inference across various
model architectures and hardware configurations. Our empirical analysis shows
that the best-performing model in terms of accuracy can also be
energy-efficient, while larger LLMs tend to consume significantly more energy
with lower classification accuracy. We observe substantial variability in
inference energy consumption ($<$mWh to $>$kWh), influenced by model type,
model size, and hardware specifications. Additionally, we find a strong
correlation between inference energy consumption and model runtime, indicating
that execution time can serve as a practical proxy for energy usage in settings
where direct measurement is not feasible. These findings have implications for
sustainable AI development, providing actionable insights for researchers,
industry practitioners, and policymakers seeking to balance performance and
resource efficiency in NLP applications.

</details>


### [85] [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
*Krishna Garg,Firoz Shaikh,Sambaran Bandyopadhyay,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本论文提出了一个新的任务——科学论文引言生成（SciIG），并基于NAACL 2025和ICLR 2025最新论文，系统评估了多种主流大语言模型（LLM）在该任务中的表现，发现LLaMA-4 Maverick模型总体表现最佳，三次示例提示（三-shot prompting）方法优于少量示例方法。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的研究人员使用大语言模型辅助写作，如何生成高质量的论文引言成为一个重要且具有挑战性的任务。本论文致力于系统评估当前大语言模型在该任务中的能力，并为未来的学术辅助写作方向提供参考依据。

Method: 作者从NAACL 2025和ICLR 2025论文中整理新数据集，设计了科学论文引言生成（SciIG）任务，要求模型根据标题、摘要和相关工作生成引言。针对DeepSeek-v3、Gemma-3-12B、LLaMA 4-Maverick、MistralAI Small 3.1及GPT-4o等主流开放源及闭源模型，从词汇重叠、语义相似性、内容覆盖、事实性、一致性、引用准确性和叙事质量等多个维度，用自动化指标和LLM判别结合的评测框架进行综合对比。

Result: LLaMA-4 Maverick模型在大多数评价指标上表现最佳，尤其在语义相似性和事实性上领先。同时，采用三次示例提示（三-shot prompting）在所有方法中表现最优，明显优于更少示例的方法。

Conclusion: 本研究为开发高效可靠的科研写作助手提供了实证依据，也为LLM辅助写作设定了更为现实的预期。所有代码与数据集将公开，促进可复现性及后续研究。

Abstract: As researchers increasingly adopt LLMs as writing assistants, generating
high-quality research paper introductions remains both challenging and
essential. We introduce Scientific Introduction Generation (SciIG), a task that
evaluates LLMs' ability to produce coherent introductions from titles,
abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR
2025 papers, we assess five state-of-the-art models, including both open-source
(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and
closed-source GPT-4o systems, across multiple dimensions: lexical overlap,
semantic similarity, content coverage, faithfulness, consistency, citation
correctness, and narrative quality. Our comprehensive framework combines
automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4
Maverick's superior performance on most metrics, particularly in semantic
similarity and faithfulness. Moreover, three-shot prompting consistently
outperforms fewer-shot approaches. These findings provide practical insights
into developing effective research writing assistants and set realistic
expectations for LLM-assisted academic writing. To foster reproducibility and
future research, we will publicly release all code and datasets.

</details>


### [86] [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
*Cliff O'Reilly,Ernesto Jimenez-Ruiz,Tillman Weyde*

Main category: cs.CL

TL;DR: 本文提出通过多语言下稀疏自编码器获得的概念激活均值，提升大语言模型（LLM）中概念语义的分离与解释性。实验显示概念均值与本体类别真实关系高度相关，优于单语结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的嵌入和稀疏自编码器方法在表示文本时语义与语法、语言特定信息纠缠，难以分离、辨认概念本身。如何让LLM语义表示更可解释、贴近真实概念关系，是亟需解决的问题。

Method: 将OWL本体类别转为英文描述，并翻译成法语和中文，分别输入Gemma 2B大语言模型。通过Gemma Scope开源稀疏自编码器，获取多语言下每一类别的概念激活，最后对不同语言的激活结果做均值处理，以获得独立于语言的概念表征。再与本体类的真实关系做相关性分析。

Result: 多语言概念激活的均值，与本体类别的真实语义关系高度相关，明显优于仅用某一种语言的表现。

Conclusion: 该方法有望为LLM内在状态的可解释性分析带来新途径，通过多语言激活均值提升了概念表征的准确性和机制解释能力。

Abstract: Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.

</details>


### [87] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 本文提出并开放了一个用于评估LLM在罗马尼亚语语法理解和讲解能力上的基准数据集GRILE，揭示了当前模型在低资源语言教育任务上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已广泛应用于自然语言处理，但它们在低资源语言，尤其是教学场景下的实用性和可靠性尚不清楚。为此，作者希望以罗马尼亚语这一低资源语言为案例，评测不同LLM是否具备用于教育的能力。

Method: 作者构建了GRILE数据集，收集并整理了1,151道来自罗马尼亚重要考试的多项选择题，设计了针对七个多语言/罗马尼亚语专属LLM的两个评测任务：一是准确选出正确答案，二是给出准确的语言学解释。通过专家复核分析模型输出的解释，并进行详细误差分析。

Result: Gemini 2.5 Pro取得了83%的答题准确率，而大多数开放权重模型低于65%；此外，有48%模型解释存在事实或教学上的错误，常见弱点包括词形变化和最新正字法规范的应用。

Conclusion: 当前LLM在低资源语言教育场景中依然存在可靠性和解释性挑战。GRILE为可控解释生成及评测提供了全新基准，期待推动后续相关研究。

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [88] [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım,Demircan Çelik*

Main category: cs.CL

TL;DR: 本文提出了一种结合了规则与统计的混合分词框架，既能保持词素信息，又提高了词表效率，对土耳其语等形态丰富语言的分词效果显著提升，实验结果优于现有主流分词器，并具备跨语言适用性。


<details>
  <summary>Details</summary>
Motivation: 现有主流的子词分词方法（如BPE和WordPiece）大多基于高频共同字符串，忽略了形态学结构，对于形态丰富或黏着语（如土耳其语）的分词和意义表达有局限性，导致分词粒度不合理、词义割裂。本文旨在解决这些问题，提高分词的语言学意义和下游模型性能。

Method: 提出混合分词方案，融合规则驱动的形态分析与统计子词切分。具体作法包括：(1) 语音规范化；(2) 根词-词缀词典与词素分析，新算法兼顾词素保存与词表精简；(3) 为语音变化（如词缀-ler与-lar）和词形变化（如kitap与kitabı）统一分配编码，减少冗余；(4) 增设空格和大小写特殊标记，防止分词表因首字母大写膨胀；(5) 使用BPE增强未登录词覆盖，但不破坏词素边界，维持词形一致性和可解释性。

Result: 在TR-MMLU土耳其语基准上，该分词器取得了最高的土耳其语Token覆盖率（90.29%）和纯Token率（85.8%），显著优于LLaMA、Gemma和GPT等主流分词器，获得了更具语言学意义和一致性的分词结果。

Conclusion: 该混合分词方法有效克服了现有子词分词的语言学缺陷，显著提升了形态复杂语言的分词表现。其方案具有语言无关性，可推广到其它语言，为多语言NLP提供更可解释和高效的分词新路径。

Abstract: Tokenization plays a pivotal role in natural language processing (NLP),
shaping how text is segmented and interpreted by language models. While subword
methods such as Byte Pair Encoding (BPE) and WordPiece have been effective,
they often struggle with morphologically rich and agglutinative languages
because they rely on frequency rather than linguistic structure. We introduce a
hybrid tokenization framework that combines rule-based morphological analysis
with statistical subword segmentation. The method uses phonological
normalization, root-affix dictionaries, and a novel algorithm that balances
morpheme preservation with vocabulary efficiency. It assigns shared identifiers
to phonologically variant affixes (e.g., -ler and -lar) and altered root forms
(e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic
integrity. Special tokens are added for whitespace and case, including an
UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is
integrated for out-of-vocabulary coverage without harming morphological
coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish
Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with
tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and
coherent tokens. Although demonstrated on Turkish, the approach is
language-independent and adaptable to other languages, offering a practical
path toward more interpretable and effective multilingual NLP systems.

</details>


### [89] [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
*Demian Inostroza,Mel Mistica,Ekaterina Vylomova,Chris Guest,Kemal Kurniawan*

Main category: cs.CL

TL;DR: 本文提出了一种多任务联合模型，针对UniDive 2025 Morpho-Syntactic Parsing共享任务，实现了形态和句法联合分析，并取得了最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 当前UD注释方案要求模型同时处理形态和句法分析，且适用于多种类型语言。作者希望开发一种高效模型，提升跨语言的综合表现。

Method: 采用共享的XLM-RoBERTa编码器，并设计了三个专用解码器，分别用于内容词识别、依存句法分析和形态句法特征预测。

Result: 模型在九种类型多样的语言上取得了最佳分数，平均MSLAS为78.7%、LAS为80.1%、Feats F1为90.3%。消融实验发现正确的分词和内容词识别对性能至关重要。

Conclusion: 该模型在多语言形态-句法联合分析任务表现优异，但在主格-宾格区分及名词范畴特征方面存在不足。

Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic
Parsing shared task, where systems predict both morphological and syntactic
analyses following novel UD annotation scheme. Our system uses a shared
XLM-RoBERTa encoder with three specialized decoders for content word
identification, dependency parsing, and morphosyntactic feature prediction. Our
model achieves the best overall performance on the shared task's leaderboard
covering nine typologically diverse languages, with an average MSLAS score of
78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation
studies show that matching the task's gold tokenization and content word
identification are crucial to model performance. Error analysis reveals that
our model struggles with core grammatical cases (particularly Nom-Acc) and
nominal features across languages.

</details>


### [90] [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
*Aman Goel,Daniel Schwartz,Yanjun Qi*

Main category: cs.CL

TL;DR: 本文提出了Finch-Zk黑盒框架，通过多模型一致性检测和细粒度修正，无需外部知识源即可检测并减缓大语言模型（LLM）幻觉（事实错误），且在多个基准数据集与SOTA模型上显著提升了检测与答案准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能力强，但易产生幻觉，即生成内容看似合理但有事实错误。现有检测和纠正方案存在局限性，且常依赖外部知识。作者希望提出无需外部资源、能高效检测与纠正幻觉的方法。

Method: 提出Finch-Zk框架，包含两大创新：一是利用语义等价但表达不同的多模型回复，进行细粒度一致性检测，精准发现局部错误区域；二是对检测到的错误段落进行有针对性的修正，保留原有准确内容。整个流程无需外部知识源，完全黑盒操作。

Result: 在FELM数据集上，Finch-Zk框架的幻觉检测F1分数较现有方法提升6-39%；在GPQA-diamond数据集上，针对Llama 4 Maverick和Claude 4 Sonnet等模型，答案准确率提升7-8个百分点。

Conclusion: Finch-Zk是一套实用、可部署的解决方案，能在生产环境提升LLM事实可靠性，为幻觉检测与纠正提供了更优的效果和便利。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, but they remain susceptible to hallucinations--generating
content that appears plausible but contains factual inaccuracies. We present
Finch-Zk, a black-box framework that leverages FINe-grained Cross-model
consistency to detect and mitigate Hallucinations in LLM outputs without
requiring external knowledge sources. Finch-Zk introduces two key innovations:
1) a cross-model consistency checking strategy that reveals fine-grained
inaccuracies by comparing responses generated by diverse models from
semantically-equivalent prompts, and 2) a targeted mitigation technique that
applies precise corrections to problematic segments while preserving accurate
content. Experiments on the FELM dataset show Finch-Zk improves hallucination
detection F1 scores by 6-39\% compared to existing approaches. For mitigation,
Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy
on the GPQA-diamond dataset when applied to state-of-the-art models like Llama
4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models
demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for
enhancing factual reliability in production LLM systems.

</details>


### [91] [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
*Jing Chen,Zhiheng Yang,Yixian Shen,Jie Liu,Adam Belloum,Chrysa Papagainni,Paola Grosso*

Main category: cs.CL

TL;DR: SurveyGen-I 是一种用于自动生成综述论文的系统，通过多阶段检索和记忆机制提升生成内容的连贯性和引用覆盖率，在多个学科领域实验证明优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于大模型的自动综述生成方法常常在长文档连贯性和引用全面性方面存在不足。为了更好地自动化科学综述的生成，需要一种能更好检索、结构化和摘要，同时保证连贯与引用全面的新方法。

Method: 提出了一套新的自动综述生成框架 SurveyGen-I。该系统在生成过程中分为粗到细的多阶段检索、动态规划和“记忆引导”生成。首先系统会进行整体性检索与梳理，生成初步大纲；利用记忆机制在生成过程中存储内容和术语，保证章节间连贯性；若发现上下文语境不足则进行更细粒度的检索以补充内容。

Result: 在四个科学领域的实验表明，SurveyGen-I 在内容质量、结构一致性和引用全面性等方面均优于已有方法。

Conclusion: SurveyGen-I 成功解决了自动生成综述时长文档连贯性与引用全面性的问题，极大提升了自动综述生成系统的实用性，为学术领域自动综述生成提供了更优的解决方案。

Abstract: Survey papers play a critical role in scientific communication by
consolidating progress across a field. Recent advances in Large Language Models
(LLMs) offer a promising solution by automating key steps in the
survey-generation pipeline, such as retrieval, structuring, and summarization.
However, existing LLM-based approaches often struggle with maintaining
coherence across long, multi-section surveys and providing comprehensive
citation coverage. To address these limitations, we introduce SurveyGen-I, an
automatic survey generation framework that combines coarse-to-fine retrieval,
adaptive planning, and memory-guided generation. SurveyGen-I first performs
survey-level retrieval to construct the initial outline and writing plan, and
then dynamically refines both during generation through a memory mechanism that
stores previously written content and terminology, ensuring coherence across
subsections. When the system detects insufficient context, it triggers
fine-grained subsection-level retrieval. During generation, SurveyGen-I
leverages this memory mechanism to maintain coherence across subsections.
Experiments across four scientific domains demonstrate that SurveyGen-I
consistently outperforms previous works in content quality, consistency, and
citation coverage.

</details>


### [92] [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
*Yixin Chen,Ying Xiong,Shangyu Wu,Yufei Cui,Xue Liu,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过训练行为一致性检索器（BAR）为大模型提供行为一致的示例，显著降低了工具增强大模型在调用外部函数时出错的次数，并保持了较高的任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强大模型在调用外部函数时，容易因不准确的调用导致效率低和成本增加。常见改进方法如微调和基于演示的提示存在较高训练成本，并常因示例不一致而误导模型。作者希望找到既低成本又高效、能提升调用准确率的方法。

Method: 作者提出并训练了一种行为一致性检索器（BAR），通过对包含多样函数调用行为（调用和不调用）的语料，利用对比学习（定制正负样本及双负对比损失），使BAR能检索出与当前场景行为一致的演示示例。

Result: 实验结果表明，使用BAR为大模型选择演示案例，能大幅减少错误的函数调用，并在多项任务中保持高性能。

Conclusion: 作者的方法无需对大模型微调，能以较低成本、高效率，提升工具增强大模型在函数调用场景下的决策准确性，对实际应用具有良好前景。

Abstract: Tool-augmented large language models (LLMs) leverage external functions to
extend their capabilities, but inaccurate function calls can lead to
inefficiencies and increased costs.Existing methods address this challenge by
fine-tuning LLMs or using demonstration-based prompting, yet they often suffer
from high training overhead and fail to account for inconsistent demonstration
samples, which misguide the model's invocation behavior. In this paper, we
trained a behavior-aligned retriever (BAR), which provides behaviorally
consistent demonstrations to help LLMs make more accurate tool-using decisions.
To train the BAR, we construct a corpus including different function-calling
behaviors, i.e., calling or non-calling.We use the contrastive learning
framework to train the BAR with customized positive/negative pairs and a
dual-negative contrastive loss, ensuring robust retrieval of behaviorally
consistent examples.Experiments demonstrate that our approach significantly
reduces erroneous function calls while maintaining high task performance,
offering a cost-effective and efficient solution for tool-augmented LLMs.

</details>


### [93] [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
*Charles Welch,Allison Lahnala,Vasudha Varadarajan,Lucie Flek,Rada Mihalcea,J. Lomax Boyd,João Sedoc*

Main category: cs.CL

TL;DR: 本文提出了一种低计算量的非生成式对话系统，可用于实现标准化的访谈式对话代理，支持定性数据收集与定量分析，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前在社会科学等领域，访谈性对话的定量与定性研究需求日益增长，但高计算成本和开发门槛使现有系统难以推广。为了提高效率与可用性，亟需一种低计算、易操作的访谈对话平台。

Method: 系统为非生成式结构，依靠预定义对话流，结合在线管理面板让用户无需编码即可创建和调整访谈流程。通过两个具体案例（COVID-19相关的Expressive Interviewing系统、神经技术舆论调研访谈）展示其实用性。

Result: 该系统能够低成本、高效率实施标准化访谈，实现受控互动和数据收集分析。实践案例验证了其灵活性和可用性。系统开源，允许二次开发和功能扩展。

Conclusion: 提出了一种实用、便捷、易扩展的低计算访谈对话系统，为学界和业界提供了有价值的工具。开源策略促进了该领域的后续发展与创新。

Abstract: We present a low-compute non-generative system for implementing
interview-style conversational agents which can be used to facilitate
qualitative data collection through controlled interactions and quantitative
analysis. Use cases include applications to tracking attitude formation or
behavior change, where control or standardization over the conversational flow
is desired. We show how our system can be easily adjusted through an online
administrative panel to create new interviews, making the tool accessible
without coding. Two case studies are presented as example applications, one
regarding the Expressive Interviewing system for COVID-19 and the other a
semi-structured interview to survey public opinion on emerging neurotechnology.
Our code is open-source, allowing others to build off of our work and develop
extensions for additional functionality.

</details>


### [94] [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
*Wenhan Dong,Zhen Sun,Yuemeng Zhao,Zifan Peng,Jun Wu,Jingyi Zheng,Yule Liu,Xinlei He,Yu Wang,Ruiming Wang,Xinyi Huang,Lei Mo*

Main category: cs.CL

TL;DR: 本文提出了一个面向中文阅读理解难度评估的新基准ZPD-SCA，评估大型语言模型(LLMs)能否根据学生的认知阶段准确判断读物难易程度。实验显示，现有LLMs零样本表现不佳，示例引导后表现提升但仍存在偏差。


<details>
  <summary>Details</summary>
Motivation: 大模型在教育场景有应用潜力，但是否能够准确把握读物与学生认知发展阶段的一致性仍未充分探索，尤其在中文教育领域。对儿童认知发展至关重要的信息缺乏系统性研究，需要新的基准来检验和推动模型的发展。

Method: 作者构建了ZPD-SCA基准，涵盖不同年级和文本体裁，由全国顶尖特级教师进行标注。通过对比零样本和上下文示例条件下多个大模型的阅读难度判断结果，量化模型在各年龄阶段对读物认知难易的诊断能力。

Result: 实验发现，LLMs在零样本条件下表现低下，有的甚至不如随机猜测。通过上下文少量示例（in-context learning），模型精度显著提升，但依然存在系统性偏差，且在不同文本体裁间表现差异明显。

Conclusion: LLMs在认知难度判断方面已展现初步能力，但训练与教育场景的认知对齐还很有限。ZPD-SCA为后续模型教育应用评测和改进提供了标准依据。

Abstract: Large language models (LLMs) have demonstrated potential in educational
applications, yet their capacity to accurately assess the cognitive alignment
of reading materials with students' developmental stages remains insufficiently
explored. This gap is particularly critical given the foundational educational
principle of the Zone of Proximal Development (ZPD), which emphasizes the need
to match learning resources with Students' Cognitive Abilities (SCA). Despite
the importance of this alignment, there is a notable absence of comprehensive
studies investigating LLMs' ability to evaluate reading comprehension
difficulty across different student age groups, especially in the context of
Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel
benchmark specifically designed to assess stage-level Chinese reading
comprehension difficulty. The benchmark is annotated by 60 Special Grade
teachers, a group that represents the top 0.15% of all in-service teachers
nationwide. Experimental results reveal that LLMs perform poorly in zero-shot
learning scenarios, with Qwen-max and GLM even falling below the probability of
random guessing. When provided with in-context examples, LLMs performance
improves substantially, with some models achieving nearly double the accuracy
of their zero-shot baselines. These results reveal that LLMs possess emerging
abilities to assess reading difficulty, while also exposing limitations in
their current training for educationally aligned judgment. Notably, even the
best-performing models display systematic directional biases, suggesting
difficulties in accurately aligning material difficulty with SCA. Furthermore,
significant variations in model performance across different genres underscore
the complexity of task. We envision that ZPD-SCA can provide a foundation for
evaluating and improving LLMs in cognitively aligned educational applications.

</details>


### [95] [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
*Ke Fang,Tianyi Zhao,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的基于提示的校准框架，通过结构化的互动和反馈机制，提高大语言模型（LLM）的置信度校准能力，不需额外监督或参数更新，实验表明该方法在多种模型和环境下提升明显。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键决策领域的应用，保证它们置信度的可靠性变得尤为重要。现有校准方法多依赖于后处理或辅助模型训练，往往需要额外的监督和参数调整，存在使用上的局限。

Method: 作者受“Credence Calibration Game”启发，设计了基于游戏的提示校准框架。其核心是让LLM根据自身预测置信度的正确性获得反馈，并以自然语言总结过去表现。通过循环式提示与反馈，动态优化模型的置信度校准，无需更改模型参数或新增监督。

Result: 在多个模型和游戏配置下的大量实验表明，该方法可持续提升校准相关指标。结果展示了基于提示的游戏化校准在提升置信度一致性方面的有效性。

Conclusion: 基于提示和反馈的游戏化校准框架，为大语言模型的置信度校准提供了一种无需额外监督和参数更新的有效手段。实验验证其跨任务、跨模型的通用性和提升性能的潜力。

Abstract: As Large Language Models (LLMs) are increasingly deployed in
decision-critical domains, it becomes essential to ensure that their confidence
estimates faithfully correspond to their actual correctness. Existing
calibration methods have primarily focused on post-hoc adjustments or auxiliary
model training; however, many of these approaches necessitate additional
supervision or parameter updates. In this work, we propose a novel prompt-based
calibration framework inspired by the Credence Calibration Game. Our method
establishes a structured interaction loop wherein LLMs receive feedback based
on the alignment of their predicted confidence with correctness. Through
feedback-driven prompting and natural language summaries of prior performance,
our framework dynamically improves model calibration. Extensive experiments
across models and game configurations demonstrate consistent improvements in
evaluation metrics. Our results highlight the potential of game-based prompting
as an effective strategy for LLM calibration. Code and data are available at
https://anonymous.4open.science/r/LLM-Calibration/.

</details>


### [96] [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
*Yupei Yang,Fan Feng,Lin Yang,Wanxi Deng,Lin Qu,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.CL

TL;DR: 本文提出了DEPTH框架，通过依存句法简化和分层细化，有效提升了大语言模型进行关系抽取的准确性，减少了虚假预测和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型虽然在关系抽取任务中展现出潜力，但主要聚焦于关系分类，并面临虚假预测问题，尤其在复杂句法结构下更容易出错，造成知识图谱中的噪声并影响下游任务。作者针对LLM难以判断关系是否真实存在的问题，提出新的解决思路。

Method: 提出了DEPTH框架，包括两个阶段：1）Grounding模块利用实体对间最短依存路径简化句子，过滤句法噪声，保留核心语义，提取关系；2）Refinement模块对所有关系预测进行整体校正，修正遗漏和不一致。同时引入基于因果奖励模型，通过剥离虚假相关性，结合人类反馈强化学习，提升模型鲁棒性。

Result: 在六个基准数据集上的实验表明，DEPTH将平均幻觉率降低到7.0%，平均F1分数较最优基线提高了17.2%。

Conclusion: DEPTH有效降低了知识图谱建设中的虚假关系和噪声，提高了大语言模型的关系抽取精度，为结构化知识构建和下游应用提供了更可靠的基础。

Abstract: Relation extraction enables the construction of structured knowledge for many
downstream applications. While large language models (LLMs) have shown great
promise in this domain, most existing methods concentrate on relation
classification, which predicts the semantic relation type between a related
entity pair. However, we observe that LLMs often struggle to reliably determine
whether a relation exists, especially in cases involving complex sentence
structures or intricate semantics, which leads to spurious predictions. Such
hallucinations can introduce noisy edges in knowledge graphs, compromising the
integrity of structured knowledge and downstream reliability. To address these
challenges, we propose DEPTH, a framework that integrates Dependency-aware
sEntence simPlification and Two-tiered Hierarchical refinement into the
relation extraction pipeline. Given a sentence and its candidate entity pairs,
DEPTH operates in two stages: (1) the Grounding module extracts relations for
each pair by leveraging their shortest dependency path, distilling the sentence
into a minimal yet coherent relational context that reduces syntactic noise
while preserving key semantics; (2) the Refinement module aggregates all local
predictions and revises them based on a holistic understanding of the sentence,
correcting omissions and inconsistencies. We further introduce a
causality-driven reward model that mitigates reward hacking by disentangling
spurious correlations, enabling robust fine-tuning via reinforcement learning
with human feedback. Experiments on six benchmarks demonstrate that DEPTH
reduces the average hallucination rate to 7.0\% while achieving a 17.2\%
improvement in average F1 score over state-of-the-art baselines.

</details>


### [97] [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
*Yinghan Zhou,Weifeng Zhu,Juan Wen,Wanli Peng,Zhengxian Wu,Yiming Xue*

Main category: cs.CL

TL;DR: 该论文针对大语言模型（LLM）自我识别文本能力在不同测试范式下的表现进行了分析，并提出了提升该能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前LLM在“单文本判断范式”（IPP）下难以准确判断文本是否出自自身，而该现象的原因及改进方法尚未系统研究。

Method: 作者提出了‘Cognitive Surgery（CoSur）’框架，包括表征提取、领域构建、文本归属判别、认知编辑四个模块，用于唤醒模型在向量空间中的隐含自我识别能力（ITA）。

Result: 该方法在三种不同LLM的单文本判别任务中取得了明显提升，平均准确率分别为83.25%、66.19%和88.01%。

Conclusion: 通过认知手术方法能够显著增强LLM对自身生成文本的判别力，为提升AI自我监测与安全性提供了新路径。

Abstract: Large language models (LLMs) have been shown to possess a degree of
self-recognition capability-the ability to identify whether a given text was
generated by themselves. Prior work has demonstrated that this capability is
reliably expressed under the Pair Presentation Paradigm (PPP), where the model
is presented with two texts and asked to choose which one it authored. However,
performance deteriorates sharply under the Individual Presentation Paradigm
(IPP), where the model is given a single text to judge authorship. Although
this phenomenon has been observed, its underlying causes have not been
systematically analyzed. In this paper, we first replicate existing findings to
confirm that LLMs struggle to distinguish self- from other-generated text under
IPP. We then investigate the reasons for this failure and attribute it to a
phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent
ability to distinguish self- and other-texts in representational space, which
remains unexpressed in its output behavior. To awaken the ITA of LLMs, we
propose Cognitive Surgery (CoSur), a novel framework comprising four main
modules: representation extraction, territory construction, authorship
discrimination and cognitive editing. Experimental results demonstrate that our
proposed method improves the performance of three different LLMs in the IPP
scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,
respectively.

</details>


### [98] [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
*Wuyang Zhang,Yexin Tian,Xiandong Meng,Mengjie Wang,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出了一种将知识图谱注入到大语言模型中的微调算法，有效提升了模型在结构化知识任务中的推理与语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理结构化知识任务时，推理链条缺失且难以把握实体级别语义，亟需融合结构知识以提升其表现。

Method: 基于预训练模型，通过引入知识图谱结构信息，采用图神经网络对实体及其关系进行编码，并设计融合机制联合建模图谱嵌入与上下文表示。为增强知识融合的鲁棒性，引入门控机制动态调节语言语义与结构知识的贡献，占训练阶段采用联合损失进行优化。

Result: 通过系统的敏感性实验，验证了学习率、图谱覆盖度和结构扰动对模型性能的影响。实验结果表明，该方法在实体识别、问答和文本生成等任务上显著提高了模型对复杂语义的表达与结构推理能力。

Conclusion: 结构感知微调框架能有效提升大模型的语义一致性和上下文逻辑建模，在结构化知识和实体相关场景下表现优异。

Abstract: This paper addresses the problems of missing reasoning chains and
insufficient entity-level semantic understanding in large language models when
dealing with tasks that require structured knowledge. It proposes a fine-tuning
algorithm framework based on knowledge graph injection. The method builds on
pretrained language models and introduces structured graph information for
auxiliary learning. A graph neural network is used to encode entities and their
relations, constructing a graph-based semantic representation. A fusion
mechanism is then designed to jointly model the knowledge graph embeddings with
the contextual representations from the language model. To enhance the
robustness of knowledge integration, a gating mechanism is introduced to
dynamically balance the contributions of linguistic semantics and structural
knowledge. This effectively mitigates conflicts between different
representational spaces. During training, a joint loss function is constructed
to account for both task performance and structural alignment objectives. This
helps improve the accuracy of entity prediction and semantic reasoning. The
study also includes a series of systematic sensitivity experiments. It
evaluates the effects of learning rate, graph coverage, and structural
perturbations on model performance. The results further validate the
effectiveness and stability of the proposed method across tasks such as entity
recognition, question answering, and language generation. Experimental findings
show that the proposed structure-aware fine-tuning framework significantly
enhances the model's ability to represent complex semantic units. It
demonstrates better semantic consistency and contextual logic modeling in
scenarios involving structural reasoning and entity extraction.

</details>


### [99] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
*NVIDIA,:,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal,Arham Mehta,Arun Venkatesan,Ashton Sharabiani,Ashwath Aithal,Ashwin Poojary,Ayush Dattagupta,Balaram Buddharaju,Banghua Zhu,Barnaby Simkin,Bilal Kartal,Bita Darvish Rouhani,Bobby Chen,Boris Ginsburg,Brandon Norick,Brian Yu,Bryan Catanzaro,Charles Wang,Charlie Truong,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christian Munley,Christopher Parisien,Dan Su,Daniel Afrimi,Daniel Korzekwa,Daniel Rohrer,Daria Gitman,David Mosallanezhad,Deepak Narayanan,Dima Rekesh,Dina Yared,Dmytro Pykhtar,Dong Ahn,Duncan Riach,Eileen Long,Elliott Ning,Eric Chung,Erick Galinkin,Evelina Bakhturina,Gargi Prasad,Gerald Shen,Haim Elisha,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Hoo Chang Shin,Hua Huang,Iain Cunningham,Igor Gitman,Ivan Moshkov,Jaehun Jung,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jimmy Zhang,Jinze Xue,Jocelyn Huang,Joey Conway,John Kamalu,Jonathan Cohen,Joseph Jennings,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kezhi Kong,Krzysztof Pawelec,Kumar Anik,Kunlun Li,Kushan Ahmadian,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Luis Vega,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Marcin Chochowski,Mark Cai,Markus Kliegl,Marta Stepniewska-Dziubinska,Matvei Novikov,Mehrzad Samadi,Meredith Price,Meriem Boubdir,Michael Boone,Michael Evans,Michal Bien,Michal Zawalski,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Namit Dhameja,Nave Assaf,Negar Habibi,Nidhi Bhatia,Nikki Pope,Nima Tajbakhsh,Nirmal Kumar Juluru,Oleg Rybakov,Oleksii Hrinchuk,Oleksii Kuchaiev,Oluwatobi Olabiyi,Pablo Ribalta,Padmavathy Subramanian,Parth Chadha,Pavlo Molchanov,Peter Dykas,Peter Jin,Piotr Bialecki,Piotr Januszewski,Pradeep Thalasta,Prashant Gaikwad,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Rabeeh Karimi Mahabadi,Rajen Patel,Ran El-Yaniv,Ranjit Rajan,Ria Cheruvu,Rima Shahbazyan,Ritika Borkar,Ritu Gala,Roger Waleffe,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Sahil Jain,Samuel Kriman,Sanjeev Satheesh,Saori Kaji,Sarah Yurick,Saurav Muralidharan,Sean Narenthiran,Seonmyeong Bak,Sepehr Sameni,Seungju Han,Shanmugam Ramasamy,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shizhe Diao,Shreya Gopal,Shrimai Prabhumoye,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Siddhartha Jain,Somshubra Majumdar,Stefania Alborghetti,Syeda Nahida Akter,Terry Kong,Tim Moon,Tomasz Hliwiak,Tomer Asida,Tony Wang,Twinkle Vashishth,Tyler Poon,Udi Karpas,Vahid Noroozi,Venkat Srinivasan,Vijay Korthikanti,Vikram Fugro,Vineeth Kalluru,Vitaly Kurin,Vitaly Lavrukhin,Wasi Uddin Ahmad,Wei Du,Wonmin Byeon,Ximing Lu,Xin Dong,Yashaswi Karnati,Yejin Choi,Yian Zhang,Ying Lin,Yonggan Fu,Yoshi Suhara,Zhen Dong,Zhiyu Li,Zhongbo Zhu,Zijia Chen*

Main category: cs.CL

TL;DR: Nemotron-Nano-9B-v2是一种融合Mamba和Transformer结构的新型语言模型，推理任务推理速度大幅提升且精度领先同类模型。作者将12B参数模型压缩蒸馏到9B，并能在单张A10G GPU上推断极长文本（最高128k tokens），同时开源模型及数据。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理需要复杂推理的大规模文本时，推理速度受限且资源消耗高。为了提升推理吞吐率并兼顾高精度，作者尝试替换部分自注意力层，改进模型结构，满足实际应用对高效、低成本语言模型的需求。

Method: 在Nemotron-H架构基础上，将Transformer的大部分自注意力层替换为Mamba-2层，提升推理时长文本处理速度。首先训练了一个12B参数模型Nemotron-Nano-12B-v2-Base，并采用FP8训练提升效率。之后通过Minitron策略对模型进行压缩和蒸馏，得到9B参数模型，使其能在单张22GiB A10G显卡上高效推理。

Result: Nemotron-Nano-9B-v2在推理类基准上取得与Qwen3-8B等同等或更优的精度，在高推理需求场景下（如8k输入+16k输出）推理吞吐量提升最高达6倍。同时支持最高128k tokens的处理能力。

Conclusion: Nemotron-Nano-9B-v2结合Mamba和Transformer优点，兼具推理性能与精度，显著降低算力需求，适用于长文本推理等实际应用，并已广泛开源模型和相关数据，便于社区复现和应用。

Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model
designed to increase throughput for reasoning workloads while achieving
state-of-the-art accuracy compared to similarly-sized models.
Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the
majority of the self-attention layers in the common Transformer architecture
are replaced with Mamba-2 layers, to achieve improved inference speed when
generating the long thinking traces needed for reasoning. We create
Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model
(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.
After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to
compress and distill the model with the goal of enabling inference on up to
128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).
Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that
Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks
while achieving up to 6x higher inference throughput in reasoning settings like
8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,
Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with
the majority of our pre- and post-training datasets on Hugging Face.

</details>


### [100] [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
*Lei Pang,Hanyi Mao,Quanjia Xiao,HaiXiao Liu,Xiangyi Li*

Main category: cs.CL

TL;DR: 本文介绍了In2x研究团队在WMT25通用机器翻译竞赛中的开源系统提交，重点关注日语相关的翻译任务，旨在探索LLM向其他语言扩展的通用方案，特别关注数据构建和奖励模型设计。最终目标是提升LLM在低资源或小语种上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在主流高资源语言上的翻译效果显著，但在低资源或小语种上存在性能瓶颈。因此，研究团队致力于开发一种可推广的方法，使LLM能更好支持多语言，尤其是资源稀缺语言。

Method: 论文提出了一种通用的LLM多语言扩展范式，包括数据集的构建方法与奖励模型的设计，通过这些手段提升系统在日语及类似低资源语种上的翻译能力。

Result: 所提交的系统在WMT25竞赛中针对日语相关的机器翻译任务表现优异，验证了所提范式在提升低资源语言翻译性能上的有效性。

Conclusion: 研究展示了一种有潜力的通用LLM多语言扩展方法，在提升低资源语言翻译性能方面具有良好应用前景。

Abstract: This paper presents the open-system submission by the In2x research team for
the WMT25 General Machine Translation Shared Task. Our submission focuses on
Japanese-related translation tasks, aiming to explore a generalizable paradigm
for extending large language models (LLMs) to other languages. This paradigm
encompasses aspects such as data construction methods and reward model design.
The ultimate goal is to enable large language model systems to achieve
exceptional performance in low-resource or less commonly spoken languages.

</details>


### [101] [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
*Krunal Shah,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出通过识别和抽取自然语言论证的逻辑结构（RLS），提升对论证推理过程的解释能力和适应性，在多个推理数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于transformer等模型的推理方法虽然能进行规则链式推理，但存在过程不透明（难以解释）和难以扩展到归纳、矛盾识别等等价推理任务的局限。本研究旨在弥补现有方法在推理透明度和通用性方面的不足。

Method: 引入了逻辑结构表示（RLS），即将自然语言论证映射为由‘逻辑原子’和相关规则构成的中间表征。拥有RLS后，可直接进行确定性推理，并支持更丰富的推理方式如多层推理、即时纠错和交互式探讨。作者在三个主流推理数据集上训练和评测了该方法。

Result: 结果显示，提出的方法能够以高准确率识别并提取自然语言论证的逻辑结构，并生成对推理过程的解释，从而显著提升推理能力和应用范围。

Conclusion: 通过中间表征RLS，方法不仅增强了推理的解释性，还能支持更复杂多样的推理任务。该框架为自然语言论证推理开辟了更广阔的研究和应用前景。

Abstract: Convincing someone of the truth value of a premise requires understanding and
articulating the core logical structure of the argument which proves or
disproves the premise. Understanding the logical structure of an argument
refers to understanding the underlying "reasons" which make up the proof or
disproof of the premise - as a function of the "logical atoms" in the argument.
While it has been shown that transformers can "chain" rules to derive simple
arguments, the challenge of articulating the "reasons" remains. Not only do
current approaches to chaining rules suffer in terms of their interpretability,
they are also quite constrained in their ability to accommodate extensions to
theoretically equivalent reasoning tasks - a model trained to chain rules
cannot support abduction or identify contradictions. In this work we suggest
addressing these shortcomings by identifying an intermediate representation
(which we call the Representation of the Logical Structure (RLS) of the
argument) that possesses an understanding of the logical structure of a natural
language argument - the logical atoms in the argument and the rules
incorporating them. Given the logical structure, reasoning is deterministic and
easy to compute. Therefore, our approach supports all forms of reasoning that
depend on the logical structure of the natural language argument, including
arbitrary depths of reasoning, on-the-fly mistake rectification and interactive
discussion with respect to an argument. We show that we can identify and
extract the logical structure of natural language arguments in three popular
reasoning datasets with high accuracies, thus supporting explanation generation
and extending the reasoning capabilities significantly.

</details>


### [102] [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
*Maja J. Hjuler,Harald V. Skat-Rørdam,Line H. Clemmensen,Sneha Das*

Main category: cs.CL

TL;DR: 本文提出了一个新的丹麦语和英语情感语音语料库EmoTale，并使用自监督语音模型（SSLM）和openSMILE特征提取方法开发情感识别模型，验证了该数据集的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 目前主流情感语音数据库多用于主流语言，比如英语，而像丹麦语这样的较小语种资源稀缺，现有公开情感语音库（如DES）也较为陈旧，限制了相关研究的发展。因此，需要新的、更丰富的丹麦语情感语音数据。

Method: 作者构建了EmoTale语料库，收录了丹麦语和英语的情感语音录音及其标注。随后，基于该数据开发了情感识别模型：利用自监督语音模型（SSLM）生成嵌入表示，以及使用openSMILE提取手工特征，并在EmoTale和参考数据库（如DES）上分别进行建模与评估。

Result: 试验结果显示，自监督学习得到的嵌入特征优于openSMILE手工特征。在EmoTale语料库上，最佳模型在跨说话人验证条件下，未加权平均召回率（UAR）达到了64.1%，与现有的DES数据库结果相当。

Conclusion: EmoTale语料库为丹麦语情感语音研究提供了新的有效资源，且自监督语音表征方法有助于提升情感识别模型的性能。该数据集可以推动丹麦语及类似小语种的情感计算技术研究。

Abstract: While multiple emotional speech corpora exist for commonly spoken languages,
there is a lack of functional datasets for smaller (spoken) languages, such as
Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is
the only other database of Danish emotional speech. We present EmoTale; a
corpus comprising Danish and English speech recordings with their associated
enacted emotion annotations. We demonstrate the validity of the dataset by
investigating and presenting its predictive power using speech emotion
recognition (SER) models. We develop SER models for EmoTale and the reference
datasets using self-supervised speech model (SSLM) embeddings and the openSMILE
feature extractor. We find the embeddings superior to the hand-crafted
features. The best model achieves an unweighted average recall (UAR) of 64.1%
on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable
to the performance on DES.

</details>


### [103] [Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning](https://arxiv.org/abs/2508.14574)
*Guilhem Fauré,Mostafa Sadeghi,Sam Bigeard,Slim Ouni*

Main category: cs.CL

TL;DR: 本文提出了两种改进Progressive Transformers（PT）架构的方法，以提升手语生成模型对手语动作变化的适应性。通过在骨骼姿态编码上应用四元数和引入语义对比损失，显著提升了姿态准确性和语义表达能力。


<details>
  <summary>Details</summary>
Motivation: 神经手语生成面临手势在同类内部变化大的挑战，这主要源于手势操作者身体结构和动作风格的多样性。现有方法对这些变化的鲁棒性不足，因此需要新的建模方式提升模型泛化和表达能力。

Method: 1. 用四元数空间对骨架关节旋转进行编码，并采用测地损失来提升角度运动的准确性和清晰度；2. 在解码器端加入基于语义相似性的对比损失（使用词汇重叠或SBERT句子相似度），以便弱化与语义无关的解剖或风格特征。

Result: 仅使用对比损失，在Phoenix14T数据集上的关键点准确率提升16%；结合四元数编码后，骨骼角误差降低6%。

Conclusion: 将骨骼结构建模和语义驱动的对比学习目标引入Transformer型手语生成模型，能有效提升其动作表达的精度和语义一致性，对应对手语多样性具有积极意义。

Abstract: One of the main challenges in neural sign language production (SLP) lies in
the high intra-class variability of signs, arising from signer morphology and
stylistic variety in the training data. To improve robustness to such
variations, we propose two enhancements to the standard Progressive
Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses
using bone rotations in quaternion space and train with a geodesic loss to
improve the accuracy and clarity of angular joint movements. Second, we
introduce a contrastive loss to structure decoder embeddings by semantic
similarity, using either gloss overlap or SBERT-based sentence similarity,
aiming to filter out anatomical and stylistic features that do not convey
relevant semantic information. On the Phoenix14T dataset, the contrastive loss
alone yields a 16% improvement in Probability of Correct Keypoint over the PT
baseline. When combined with quaternion-based pose encoding, the model achieves
a 6% reduction in Mean Bone Angle Error. These results point to the benefit of
incorporating skeletal structure modeling and semantically guided contrastive
objectives on sign pose representations into the training of Transformer-based
SLP models.

</details>


### [104] [Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek](https://arxiv.org/abs/2508.14586)
*Mukhammadsaid Mamasaidov,Azizullah Aral,Abror Shopulatov,Mironshoh Inomjonov*

Main category: cs.CL

TL;DR: 本文为南部乌兹别克语（Southern Uzbek，uzs）机器翻译任务构建了新资源，包含多种平行语料集和微调模型，并提出新方法改善阿拉伯字母写法处理。所有资源均开放，促进低资源语言研究。


<details>
  <summary>Details</summary>
Motivation: 南部乌兹别克语在阿富汗有约500万使用者，在音系、词汇和正字法上与北部乌兹别克语（uzn）显著不同。尽管使用人数多，但自然语言处理领域对此关注甚少，缺乏足够资源和工具。

Method: 作者构建了997句的FLORES+开发集，收集了来自词典、文学和网络的39,994对平行句子，并在NLLB-200模型基础上针对南部乌兹别克语进行微调（lutfiy）。还提出了一种后处理方法，用于修复阿拉伯文半空格字符，以更好处理形态学边界问题。

Result: 新资源和模型能更好地支持南部乌兹别克语的机器翻译；提出的修复半空格字符方法提升了对阿拉伯文形态学边界的处理效果。

Conclusion: 论文首次系统性提供南部乌兹别克语的公开机器翻译资源和工具，为该语言及其他低资源语种的自然语言处理应用创造了基础。

Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million
people in Afghanistan and differs significantly from Northern Uzbek (uzn) in
phonology, lexicon, and orthography. Despite the large number of speakers,
Southern Uzbek is underrepresented in natural language processing. We present
new resources for Southern Uzbek machine translation, including a 997-sentence
FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web
sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a
post-processing method for restoring Arabic-script half-space characters, which
improves handling of morphological boundaries. All datasets, models, and tools
are released publicly to support future work on Southern Uzbek and other
low-resource languages.

</details>


### [105] [Continuous sentiment scores for literary and multilingual contexts](https://arxiv.org/abs/2508.14620)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的连续型情感评分方法，可更细腻地量化文学文本中的情感表达，并在多语言场景下优于传统工具和主流深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 文学文本由于修辞、风格和情感表达方式复杂，使得常用情感分析工具（如基于词典的方法）表现不佳，且主流transformer模型输出过于粗糙，难以实现细致分析。因此有必要开发更适用于文学分析的细粒度情感分析方法，尤其关注多语种和低资源语言。

Method: 提出基于概念向量投影的连续型情感评分方法，在多语种文学语料上进行训练。这一方法可以跨体裁、语言、历史时期，捕捉更为细腻的情感表达变化。

Result: 在英文和丹麦文文学文本上的实验表明，新方法优于现有情感分析工具，所得到的情感评分分布与人工标注结果高度一致。

Conclusion: 这种创新的情感评分方法为文学作品提供了更精确的情感分析和情感弧建模手段，有望提升在文学、文化和跨语言研究中的应用价值。

Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its
application to literary texts poses unique challenges due to figurative
language, stylistic ambiguity, as well as sentiment evocation strategies.
Traditional dictionary-based tools often underperform, especially for
low-resource languages, and transformer models, while promising, typically
output coarse categorical labels that limit fine-grained analysis. We introduce
a novel continuous sentiment scoring method based on concept vector projection,
trained on multilingual literary data, which more effectively captures nuanced
sentiment expressions across genres, languages, and historical periods. Our
approach outperforms existing tools on English and Danish texts, producing
sentiment scores whose distribution closely matches human ratings, enabling
more accurate analysis and sentiment arc modeling in literature.

</details>


### [106] [Improving in-context learning with a better scoring function](https://arxiv.org/abs/2508.14685)
*Omar Naim,Swarnadeep Bhar,Jérôme Bolte,Nicholas Asher*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLM）的类比学习能力受到Softmax注意力机制的限制，提出了一种替代机制SSA，并实验证明其能显著提升多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LLM的in-context learning能力有限，尤其是在涉及一阶量词和线性函数任务中表现不足。作者推测注意力机制中的Softmax可能是瓶颈，因此寻求更优的方法。

Method: 作者分析了Softmax在注意力机制中的作用，并提出scaled signed averaging（SSA）作为新的分数聚合方法。随后在涉及一阶量词、线性函数和一系列语言探测任务上，对比了采用SSA和Softmax的模型性能。

Result: 实验表明，SSA显著提升了在目标任务上的表现，并且无论是编码器Transformer还是解码器Transformer，采用SSA后在多项语言探测任务上均达到或超过传统Softmax模型的水平。

Conclusion: Softmax作为注意力得分函数限制了LLM的in-context learning能力，SSA提供了更优的选择，能显著提升模型泛化和迁移能力。

Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by
analogy, known as in-context learning (ICL). However, recent studies have
revealed limitations in this ability. In this paper, we examine these
limitations on tasks involving first-order quantifiers such as {\em all} and
{\em some}, as well as on ICL with linear functions. We identify Softmax, the
scoring function in attention mechanism, as a contributing factor to these
constraints. To address this, we propose \textbf{scaled signed averaging
(SSA)}, a novel alternative to Softmax. Empirical results show that SSA
dramatically improves performance on our target tasks. Furthermore, we evaluate
both encoder-only and decoder-only transformers models with SSA, demonstrating
that they match or exceed their Softmax-based counterparts across a variety of
linguistic probing tasks.

</details>


### [107] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出了ShizhenGPT，这是首个针对中医领域开发的多模态大型语言模型（LLM），能结合文本、图像、音频及生理信号，实现多感官统一感知与诊断。ShizhenGPT在自建的超大规模中医多模态数据集上进行预训练和微调，并在多项基准测试中优于同规模LLM，在中医视觉理解方面领先。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多个领域取得突破，但在中医领域应用有限，主要由于高质量中医数据稀缺，以及中医诊断的多模态特性（望闻问切）超越传统LLM的处理能力。本文旨在突破上述限制，推动中医AI的发展。

Method: 作者构建了迄今为止最大的中医多模态数据集，包括超过100GB文本、200GB多模态数据（120万图像、200小时音频和生理信号）。基于该数据集，预训练并指令微调开发ShizhenGPT，实现多模态医学推理和中医知识掌握。评估时，收集了最新国考题和中医视觉基准，考察识药和视觉诊断能力。

Result: 实验表明，ShizhenGPT在多个测试中优于同规模LLMs，某些方面可与更大私有模型竞争，尤其在中医视觉理解能力上表现突出，并具备跨模态（声音、脉搏、气味和视觉）的统一感知能力。

Conclusion: ShizhenGPT代表了中医多模态LLM的重大进展，可以实现整体多感官感知和诊断，有望推动中医AI自动化与智能化。作者已公开相关数据、模型与代码，呼吁学界持续探索该领域。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


### [108] [The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation](https://arxiv.org/abs/2508.14718)
*Shubham Pundhir,Ganesh Bagler*

Main category: cs.CL

TL;DR: 该论文建立了一个用于基于文本的食谱生成的基准体系，并对比了GPT-2不同版本及传统神经网络模型，还提出了一种优化分词方案用于提升食谱结构与数值表达的准确性。


<details>
  <summary>Details</summary>
Motivation: 推动自然语言生成中特定领域（食谱生成）的性能提升，解决通用分词器对食谱结构和数量词处理不佳的问题。

Method: 对比分析了fine-tuned GPT-2 large（774M参数）、GPT-2 small（124M参数）和LSTM/RNN基线在RecipeDB的5种菜系语料上的表现；提出了一种针对食谱的分词策略，将23个常用分数和定制结构标记添加到词表中以保护食谱结构和数量表达。

Result: 实验显示，GPT-2 large在BERTScore（F1）指标上比最佳RNN基线提高了20%（0.92对0.72），困惑度降幅近70%。

Conclusion: 提出的分词方法与大型预训练Transformer模型显著提升了食谱自动生成的表现，但仍存在事实准确性方面的挑战。该工作为食谱生成未来结合实际约束和多模态输入提供了基础。

Abstract: We established a rigorous benchmark for text-based recipe generation, a
fundamental task in natural language generation. We present a comprehensive
comparative study contrasting a fine-tuned GPT-2 large (774M) model against the
GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine
corpus from RecipeDB. Our key contribution is a targeted tokenization strategy
that augments the vocabulary with 23 common fraction tokens and custom
structural markers. This approach addresses a critical limitation of generic
tokenizers by preserving essential recipe structures and precise numerical
quantities, thereby enhancing domain specificity. Performance is evaluated
using a comprehensive suite of seven automatic metrics spanning fluency
(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and
diversity. Our experiments show that the large transformer-based approach
yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the
best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a
discussion of remaining challenges, particularly regarding factual accuracy,
and outline how this foundational study paves the way for integrating
real-world constraints and multi-modal inputs in advanced recipe generation
research.

</details>


### [109] [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
*Guangzhan Wang,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的基于大语言模型（LLM）的文本增强方法 LMTransplant，通过“移植再生”策略提升数据增强的多样性和创新性，并在多项任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统文本增强方法如反向翻译（Back-translation）主要产生词汇级的同义复写，难以在内容和结构上实现多样性。当前利用大语言模型的增强虽然提高了创造性，但对输出风格和结构的控制较弱，需要复杂的提示（prompt）设计。因此，亟需一种既能充分利用LLM知识，又能有效控制生成文本内容和风格的新型增强方法。

Method: 作者提出了 LMTransplant 方法：首先将待增强文本（seed text）嵌入由LLM扩展的语境之中，然后引导LLM基于该扩展语境生成新的变体。这一“移植-再生”流程既借助了LLM的知识与创造性，又能更好地控制内容与风格，生成多样且贴合原文核心属性的文本增强样本。

Result: 在多个文本相关任务上进行实验，LMTransplant 在数据多样性、样本质量和扩展性等方面均优于现有文本增强方法。特别是在增强数据规模增长时，LMTransplant 展现出极佳的可扩展性。

Conclusion: LMTransplant 方法能高效利用LLM进行文本内容层面的多样化增强，同时保留原文关键信息，在实际应用中具有显著效果和良好扩展能力，优于现有传统和基于LLM的增强策略。

Abstract: Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.

</details>


### [110] [Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference](https://arxiv.org/abs/2508.14735)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出了一种多语言自然语言推理（NLI）的评估框架，利用合成的逻辑型前提-假设对，并将其翻译为多种类型语言。实验发现在混合语言（code-switching）环境下，大型语言模型（LLM）的推理表现没有下降，反而可能提升。分析还确认了翻译对语义的保真性。结果揭示了LLM跨语言推理的潜力与脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被广泛应用于多语言环境，其跨语言、一致且有逻辑支撑的推理能力却未被充分研究。作者希望通过受控测试来深入理解LLM多语言下的推理表现，特别是混合语言条件的影响。

Method: 作者提出了一个可控的多语言自然语言推理评估框架：1）合成逻辑基础的前提-假设句对，2）将这些句对翻译成多种语言（包括典型上差异很大的语种），3）在单语和混合语言（code-switching）条件下测试LLM表现。通过嵌入相似度和可视化验证语义一致性。

Result: 结果发现，混合语言（code-switching）不仅没有降低LLM的推理性能，有时还提升了表现。这表明翻译引入的词汇多样性可能起到类似正则化的作用，增强多语环境下模型稳健性。

Conclusion: 当前LLM在跨语言推理上既展现出潜力也存在脆弱性。混合语言输入是一种有希望提升多语稳健性的手段。该评测框架及代码为未来多语NLI研究和模型改进提供了有力工具。

Abstract: Large language models (LLMs) are increasingly applied in multilingual
contexts, yet their capacity for consistent, logically grounded alignment
across languages remains underexplored. We present a controlled evaluation
framework for multilingual natural language inference (NLI) that generates
synthetic, logic-based premise-hypothesis pairs and translates them into a
typologically diverse set of languages. This design enables precise control
over semantic relations and allows testing in both monolingual and
mixed-language (code-switched) conditions. Surprisingly, code-switching does
not degrade, and can even improve, performance, suggesting that
translation-induced lexical variation may serve as a regularization signal. We
validate semantic preservation through embedding-based similarity analyses and
cross-lingual alignment visualizations, confirming the fidelity of translated
pairs. Our findings expose both the potential and the brittleness of current
LLM cross-lingual reasoning, and identify code-switching as a promising lever
for improving multilingual robustness. Code available at:
https://github.com/KurbanIntelligenceLab/nli-stress-testing

</details>


### [111] [TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting](https://arxiv.org/abs/2508.14782)
*Jiaming Leng,Yunying Bi,Chuan Qin,Bing Yin,Yanyong Zhang,Chao Wang*

Main category: cs.CL

TL;DR: TransLLM是一个结合大语言模型和时空建模的统一框架，能高效处理交通预测等多任务，并在多数据集和任务下表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前交通系统任务面临多样挑战，现有小规模深度学习模型针对性强但通用性差、大模型难以处理结构化时空数据及数值推理，急需通用可扩展的解决方案。

Method: 提出TransLLM框架，利用轻量级时空编码器（膨胀卷积+双邻接图注意力网络）提取时空依赖，通过结构化嵌入与大语言模型对接，并通过实例级prompt路由机制（强化学习训练）动态个性化指令，最终生成针对不同交通任务的预测输出。

Result: 在七个数据集、三类任务上，TransLLM在有监督和零样本情况下均显示出色表现，对比十个基线模型具有竞争力，展现了良好的泛化性和跨任务适应能力。

Conclusion: TransLLM实现了时空交通数据与大语言模型的有效融合，能动态适应不同任务，提升交通领域多任务建模的通用性和准确性，推动智能交通系统的发展。

Abstract: Urban transportation systems encounter diverse challenges across multiple
tasks, such as traffic forecasting, electric vehicle (EV) charging demand
prediction, and taxi dispatch. Existing approaches suffer from two key
limitations: small-scale deep learning models are task-specific and
data-hungry, limiting their generalizability across diverse scenarios, while
large language models (LLMs), despite offering flexibility through natural
language interfaces, struggle with structured spatiotemporal data and numerical
reasoning in transportation domains. To address these limitations, we propose
TransLLM, a unified foundation framework that integrates spatiotemporal
modeling with large language models through learnable prompt composition. Our
approach features a lightweight spatiotemporal encoder that captures complex
dependencies via dilated temporal convolutions and dual-adjacency graph
attention networks, seamlessly interfacing with LLMs through structured
embeddings. A novel instance-level prompt routing mechanism, trained via
reinforcement learning, dynamically personalizes prompts based on input
characteristics, moving beyond fixed task-specific templates. The framework
operates by encoding spatiotemporal patterns into contextual representations,
dynamically composing personalized prompts to guide LLM reasoning, and
projecting the resulting representations through specialized output layers to
generate task-specific predictions. Experiments across seven datasets and three
tasks demonstrate the exceptional effectiveness of TransLLM in both supervised
and zero-shot settings. Compared to ten baseline models, it delivers
competitive performance on both regression and planning problems, showing
strong generalization and cross-task adaptability. Our code is available at
https://github.com/BiYunying/TransLLM.

</details>


### [112] [Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs](https://arxiv.org/abs/2508.14817)
*Skatje Myers,Dmitriy Dligach,Timothy A. Miller,Samantha Barr,Yanjun Gao,Matthew Churpek,Anoop Mayampurath,Majid Afshar*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大语言模型（LLMs）及其结合检索增强生成（RAG）技术，有效地处理电子健康记录（EHRs）中的冗长和杂乱无章的文本，提高临床任务的自动化能力。作者设计了三项易于复现的临床任务，并在真实患者EHR数据上进行评估。结果显示RAG方法在精度和效率方面都具备优势。


<details>
  <summary>Details</summary>
Motivation: EHR文本很长且杂乱，医生阅读负担大，现有LLM难以处理全部内容，需高效办法从EHR中提取所需信息。RAG方法可能减少所需输入长度并提升效率，因此需要验证其在临床实际任务中的表现。

Method: 作者提出三项能跨医疗系统复用的临床任务：1）影像学检查提取，2）抗生素使用时间轴生成，3）主要诊断识别。在真实住院患者的EHR数据上，用三种先进LLM（不同长度上下文输入）分别比较RAG检索和仅用最新病历的方法。

Result: RAG方法与仅用最新病历的方法效果相当甚至更优，而与LLM使用全部上下文相比，RAG所需输入token大幅减少，但性能接近甚至超过。

Conclusion: 即使在LLM能处理更长文本的背景下，RAG依然是一种高效且具有竞争力的EHR文本处理方法，具备实际应用前景。

Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing
a major challenge for the clinicians who must navigate them. Large language
models (LLMs) offer a promising solution for extracting and reasoning over this
unstructured text, but the length of clinical notes often exceeds even
state-of-the-art models' extended context windows. Retrieval-augmented
generation (RAG) offers an alternative by retrieving task-relevant passages
from across the entire EHR, potentially reducing the amount of required input
tokens. In this work, we propose three clinical tasks designed to be replicable
across health systems with minimal effort: 1) extracting imaging procedures, 2)
generating timelines of antibiotic use, and 3) identifying key diagnoses. Using
EHRs from actual hospitalized patients, we test three state-of-the-art LLMs
with varying amounts of provided context, using either targeted text retrieval
or the most recent clinical notes. We find that RAG closely matches or exceeds
the performance of using recent notes, and approaches the performance of using
the models' full context while requiring drastically fewer input tokens. Our
results suggest that RAG remains a competitive and efficient approach even as
newer models become capable of handling increasingly longer amounts of text.

</details>


### [113] [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
*Josh Barua,Seun Eisape,Kayo Yin,Alane Suhr*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLM）在多语言长链思维推理（CoT）任务中的表现，并对中译本的数据集及模型进行系统性研究，发现不同语言在长CoT迁移上的表现及提升空间存在差异，并发布相关译本以促进多语言推理研究。


<details>
  <summary>Details</summary>
Motivation: 虽然长CoT极大提升了LLM的推理能力，但此前的研究几乎都集中在英文，对其他语言的效果不明。作者希望研究多语种推理任务中的表现，从而推动跨语种推理模型的发展。

Method: 作者将两个广泛使用的英文推理数据集译成法语、日语、拉脱维亚语和斯瓦希里语，并基于Qwen 2.5（7B）和Qwen 3（8B）进行微调。系统性评测了英文枢纽语言技巧、模型多语种预训练及不同训练集规模/质量的影响。

Result: (1) 英语作为推理中介的作用受目标语言影响：对法语无益，对日语和拉脱维亚语有效，对斯瓦希里语效果很差；(2) Qwen 3多语种预训练缩小了跨语种表现差距，但未完全消除；(3) 小规模高质量数据集足够支撑英语和法语推理，大而噪声的数据更适合斯瓦希里语和拉脱维亚语。

Conclusion: 长CoT迁移效果强依赖于目标语言特性，需区别对待，不同语言应采用差异化的数据和训练策略。文中还发布了多语种数据集资源，促进后续公平的多语言推理研究。

Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.

</details>


### [114] [MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
*Ailing Yu,Lan Yao,Jingnan Liu,Zhe Chen,Jiajun Yin,Yuan Wang,Xinhao Liao,Zhiling Ye,Ji Li,Yun Yue,Hansong Xiao,Hualei Zhou,Chunxiao Guo,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了一种医学领域的深度研究智能体，通过医学知识图谱与专用医学检索引擎，显著提升了大语言模型在医学复杂任务中的表现，超越了主流商用大模型在相关基准上的成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管通用大型语言模型表现强大，但在医学等专业领域存在知识密度不足、缺乏专用工具等瓶颈，因此需要开发更适合医学场景的解题智能体。

Method: 1. 利用医学知识图谱合成数据，从罕见医学实体子图中提取最长路径，生成复杂的多跳问答对；2. 融合专门定制的医学检索引擎与通用工具；3. 采用“监督微调+在线强化学习（复合奖励机制）”双阶段训练范式。

Result: 提出的MedResearcher-R1-32B模型在12个医学专科、2100+问答轨迹上表现优越，在多个医学基准上取得了SOTA（最新最优），并在通用研究任务上保持了竞争力。

Conclusion: 通过在模型结构、工具设计和训练数据等方面引入医学领域专有创新，中小规模开源模型可在垂直领域超越大体量闭源系统，对医学智能体研究与应用具有重要推动意义。

Abstract: Recent developments in Large Language Model (LLM)-based agents have shown
impressive capabilities spanning multiple domains, exemplified by deep research
systems that demonstrate superior performance on complex information-seeking
and synthesis tasks. While general-purpose deep research agents have shown
impressive capabilities, they struggle significantly with medical domain
challenges, as evidenced by leading proprietary systems achieving limited
accuracy on complex medical benchmarks. The key limitations are: (1) the model
lacks sufficient dense medical knowledge for clinical reasoning, and (2) the
framework is constrained by the absence of specialized retrieval tools tailored
for medical contexts.We present a medical deep research agent that addresses
these challenges through two core innovations. First, we develop a novel data
synthesis framework using medical knowledge graphs, extracting the longest
chains from subgraphs around rare medical entities to generate complex
multi-hop question-answer pairs. Second, we integrate a custom-built private
medical retrieval engine alongside general-purpose tools, enabling accurate
medical information synthesis. Our approach generates 2100+ diverse
trajectories across 12 medical specialties, each averaging 4.2 tool
interactions.Through a two-stage training paradigm combining supervised
fine-tuning and online reinforcement learning with composite rewards, our
MedResearcher-R1-32B model demonstrates exceptional performance, establishing
new state-of-the-art results on medical benchmarks while maintaining
competitive performance on general deep research tasks. Our work demonstrates
that strategic domain-specific innovations in architecture, tool design, and
training data construction can enable smaller open-source models to outperform
much larger proprietary systems in specialized domains.

</details>


### [115] [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
*Haokun Lin,Haobo Xu,Yichen Wu,Ziyu Guo,Renrui Zhang,Zhichao Lu,Ying Wei,Qingfu Zhang,Zhenan Sun*

Main category: cs.CL

TL;DR: 本文首次系统性地研究了扩散型大语言模型（dLLMs）的量化方法，以支持其在资源受限设备上的高效部署。


<details>
  <summary>Details</summary>
Motivation: 虽然自回归大语言模型（AR LLMs）已广泛采用量化进行压缩，但扩散型大语言模型在边缘设备部署时仍面临参数量大和资源消耗高的问题，且量化应用研究稀少。

Method: 作者分析了dLLMs激活值中存在的离群点对低比特量化带来的挑战，并实现了多种主流后训练量化（PTQ）方法。通过比特宽度、量化方法、任务类型及模型类型四个维度，系统评估了dLLMs的量化表现。

Result: 结果揭示了激活离群值对低比特量化的影响，并通过多维度评测提供了dLLMs在不同配置下的量化行为洞察。

Conclusion: 工作为dLLMs量化和高效部署研究奠定了基础，相关代码和实验设置即将开源，有助于推动社区发展。

Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [Research on UAV Applications in Public Administration: Based on an Improved RRT Algorithm](https://arxiv.org/abs/2508.14096)
*Zhanxi Xie,Baili Lu,Yanzhao Gu,Zikun Li,Junhao Wei,Ngai Cheong*

Main category: cs.RO

TL;DR: 该论文研究了无人机在公共管理中的路径规划优化，提出了一种改进型RRT算法（dRRT），能有效提升路径规划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济政策和智慧城市需求推动，无人机角色从技术工具向治理基础设施转变，但面临能耗、避障和空域限制等挑战，需要高效的路径规划方法。

Method: 提出了dRRT算法，融合了目标偏向、动态步长、优先绕行和平滑路径四种策略，并在500立方米的城市环境仿真中与传统RRT、A*和蚁群算法（ACO）进行了对比。

Result: dRRT在仿真测试中达到了100%的成功率，平均运行时间仅0.01468秒，路径更短、航点更少、轨迹更平滑（偏航角<45°），明显优于传统方法。

Conclusion: dRRT算法在提升无人机公共管理（如应急响应、交通监控）部署效率方面展示了巨大潜力，但也面临计算开销和易陷局部最优等限制，未来需与实时避障技术结合以进一步提高实用性。

Abstract: This study investigates the application of unmanned aerial vehicles (UAVs) in
public management, focusing on optimizing path planning to address challenges
such as energy consumption, obstacle avoidance, and airspace constraints. As
UAVs transition from 'technical tools' to 'governance infrastructure', driven
by advancements in low-altitude economy policies and smart city demands,
efficient path planning becomes critical. The research proposes an enhanced
Rapidly-exploring Random Tree algorithm (dRRT), incorporating four strategies:
Target Bias (to accelerate convergence), Dynamic Step Size (to balance
exploration and obstacle navigation), Detour Priority (to prioritize horizontal
detours over vertical ascents), and B-spline smoothing (to enhance path
smoothness). Simulations in a 500 m3 urban environment with randomized
buildings demonstrate dRRT's superiority over traditional RRT, A*, and Ant
Colony Optimization (ACO). Results show dRRT achieves a 100\% success rate with
an average runtime of 0.01468s, shorter path lengths, fewer waypoints, and
smoother trajectories (maximum yaw angles <45{\deg}). Despite improvements,
limitations include increased computational overhead from added mechanisms and
potential local optima due to goal biasing. The study highlights dRRT's
potential for efficient UAV deployment in public management scenarios like
emergency response and traffic monitoring, while underscoring the need for
integration with real-time obstacle avoidance frameworks. This work contributes
to interdisciplinary advancements in urban governance, robotics, and
computational optimization.

</details>


### [117] [No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets](https://arxiv.org/abs/2508.14098)
*Pranay Dugar,Mohitvishnu S. Gadde,Jonah Siekmann,Yesh Godse,Aayam Shrestha,Alan Fern*

Main category: cs.RO

TL;DR: 本文提出了一种直接面向SE(2)目标位姿的人形机器人强化学习控制方法，强调自然、高效并能顺利从仿真转移到真实硬件。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动学习方法多以速度跟踪为目标，这在执行短距离、基于目标的动作时效率低下。本研究希望提升人形机器人以任务为导向的短程运动能力，实现更快、更稳健和更节能的移动。

Method: 作者提出将强化学习应用于人形机器人，使其直接优化达到SE(2)目标的位置和朝向。此外，文中设计了一种基于星座的奖励函数，鼓励机器人在靠近目标时，采用自然且高效的行动策略。性能评估框架则综合考虑能耗、达到目标时间与步数，从多个角度对算法进行量化。

Result: 实验结果显示，所提出方法在能耗、时间与步数方面均优于标准方法。同时，该方法在仿真到真实硬件的转移上表现良好。

Conclusion: 文中工作验证了有针对性的奖励设计对于提升人形机器人短距离、高效运动的重要性，并展示了其实际应用价值。

Abstract: Humanoids operating in real-world workspaces must frequently execute
task-driven, short-range movements to SE(2) target poses. To be practical,
these transitions must be fast, robust, and energy efficient. While
learning-based locomotion has made significant progress, most existing methods
optimize for velocity-tracking rather than direct pose reaching, resulting in
inefficient, marching-style behavior when applied to short-range tasks. In this
work, we develop a reinforcement learning approach that directly optimizes
humanoid locomotion for SE(2) targets. Central to this approach is a new
constellation-based reward function that encourages natural and efficient
target-oriented movement. To evaluate performance, we introduce a benchmarking
framework that measures energy consumption, time-to-target, and footstep count
on a distribution of SE(2) goals. Our results show that the proposed approach
consistently outperforms standard methods and enables successful transfer from
simulation to hardware, highlighting the importance of targeted reward design
for practical short-range humanoid locomotion.

</details>


### [118] [Task and Motion Planning for Humanoid Loco-manipulation](https://arxiv.org/abs/2508.14099)
*Michal Ciebielski,Victor Dhédin,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出了一种基于优化的任务与运动规划（TAMP）框架，统一了机器人行走和操作的规划，能够生成连贯且复杂的人形机器人操作行为。


<details>
  <summary>Details</summary>
Motivation: 现有机器人规划方法通常将任务规划和运动规划分开处理，难以同时处理机器人行走（locomotion）和操作（manipulation）中复杂的物理动态约束，亟需一种能统一两者的高效方法。

Method: 提出了一种基于优化的TAMP框架，通过以接触模式变化定义符号动作，将高层任务规划与低层运动规划统一。该方法能够在包含全身动力学及各种约束条件下，进行任务、接触和运动的统一规划搜索。

Result: 在仿真人形机器人平台上，所提方法能够在长时间动作序列中，生成多样且物理一致的行走-操作行为，展示出对复杂推理需求下任务的良好处理能力。

Conclusion: 该工作首次实现了在全身动力学和驱动约束下的人形机器人行走-操作问题中，任务与运动规划的完整统一，有望提升机器人复杂任务的自主执行能力。

Abstract: This work presents an optimization-based task and motion planning (TAMP)
framework that unifies planning for locomotion and manipulation through a
shared representation of contact modes. We define symbolic actions as contact
mode changes, grounding high-level planning in low-level motion. This enables a
unified search that spans task, contact, and motion planning while
incorporating whole-body dynamics, as well as all constraints between the
robot, the manipulated object, and the environment. Results on a humanoid
platform show that our method can generate a broad range of physically
consistent loco-manipulation behaviors over long action sequences requiring
complex reasoning. To the best of our knowledge, this is the first work that
enables the resolution of an integrated TAMP formulation with fully acyclic
planning and whole body dynamics with actuation constraints for the humanoid
loco-manipulation problem.

</details>


### [119] [Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network](https://arxiv.org/abs/2508.14100)
*Nilay Kushawaha,Carlo Alessi,Lorenzo Fruzzetti,Egidio Falotico*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件循环生成对抗网络（CCGAN）的跨域知识迁移框架，将在一种物理属性下训练得到的姿态控制器适应到另一种属性显著不同的域（如粘度增加十倍），解决了软体机器人材料老化导致的特性变化和动力学建模难题。


<details>
  <summary>Details</summary>
Motivation: 软体机器人结构与材料特性复杂且非线性，传统解析模型要求精准了解这些属性，但软体机器人一方面难以获取完整参数，另一方面其材料会随时间退化。因此，需要一种能在不同物理域间迁移控制策略的方法，提高软体机器人控制器的适应性和通用性。

Method: 采用基于条件cycleGAN（CCGAN）的域转换框架，从压力信号和末端执行器位置/姿态的对应关系中学习，在源域（标准模拟环境）和目标域（高粘度环境）间实现知识迁移。动态学习的控制器在一个域训练后，通过CCGAN-GP适应到另一个域。

Result: 通过对五种不同形状软体机器人的轨迹跟踪实验，以及在噪声干扰和周期性测试下的评估，结果表明CCGAN-GP能够有效实现跨域技能迁移，具有较强的适应性和鲁棒性。

Conclusion: CCGAN-GP方法为软体机器人控制的跨域知识迁移提供了有效手段，使控制器面对物理属性变化时更具普适性和适应性，有望推动软体机器人控制向更通用、强健的方向发展。

Abstract: Deep learning provides a powerful method for modeling the dynamics of soft
robots, offering advantages over traditional analytical approaches that require
precise knowledge of the robot's structure, material properties, and other
physical characteristics. Given the inherent complexity and non-linearity of
these systems, extracting such details can be challenging. The mappings learned
in one domain cannot be directly transferred to another domain with different
physical properties. This challenge is particularly relevant for soft robots,
as their materials gradually degrade over time. In this paper, we introduce a
domain translation framework based on a conditional cycle generative
adversarial network (CCGAN) to enable knowledge transfer from a source domain
to a target domain. Specifically, we employ a dynamic learning approach to
adapt a pose controller trained in a standard simulation environment to a
domain with tenfold increased viscosity. Our model learns from input pressure
signals conditioned on corresponding end-effector positions and orientations in
both domains. We evaluate our approach through trajectory-tracking experiments
across five distinct shapes and further assess its robustness under noise
perturbations and periodicity tests. The results demonstrate that CCGAN-GP
effectively facilitates cross-domain skill transfer, paving the way for more
adaptable and generalizable soft robotic controllers.

</details>


### [120] [Efficient Environment Design for Multi-Robot Navigation via Continuous Control](https://arxiv.org/abs/2508.14105)
*Jahid Chowdhury Choton,John Woods,William Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种高效且高度可定制的多机器人连续控制导航环境，并在复杂环境和有限资源下，评估了多种深度强化学习算法的表现，验证了其现实应用的可行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多机器人在连续状态和动作空间、不确定环境下的导航和路径规划是一个尚未完全解决的挑战。现有基于深度强化学习的方法样本效率低、训练周期长，且对实际应用缺乏正式性能保证。因此，亟需一个高效、易定制且贴近实际的问题环境来推动相关算法研究。

Method: 作者构建了一个可定制的连续控制多机器人导航环境，并将导航任务建模为马尔可夫决策过程（MDP），进一步将其描述为优化问题。在该环境下，使用A2C、PPO、TRPO、TQC、CrossQ和ARS等梯度/非梯度RL方法进行性能评测。最后，通过CoppeliaSim机器人仿真平台在实际农田场景下测试模型推断鲁棒性。

Result: 多种主流RL算法均在所提出环境中进行了实验，验证了算法在不同变体环境下的表现及收敛效率，展示了模型能在实际不确定三维农业场景中保持鲁棒性。

Conclusion: 该工作为多机器人导航提供了一个标准化、实用、可扩展的评测平台，有助于研究和应用基于MDP的RL方法，有效推进仿真到真实系统的过渡和现实场景的落地。

Abstract: Multi-robot navigation and path planning in continuous state and action
spaces with uncertain environments remains an open challenge. Deep
Reinforcement Learning (RL) is one of the most popular paradigms for solving
this task, but its real-world application has been limited due to sample
inefficiency and long training periods. Moreover, the existing works using RL
for multi-robot navigation lack formal guarantees while designing the
environment. In this paper, we introduce an efficient and highly customizable
environment for continuous-control multi-robot navigation, where the robots
must visit a set of regions of interest (ROIs) by following the shortest paths.
The task is formally modeled as a Markov Decision Process (MDP). We describe
the multi-robot navigation task as an optimization problem and relate it to
finding an optimal policy for the MDP. We crafted several variations of the
environment and measured the performance using both gradient and non-gradient
based RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world
applicability, we deployed our environment to a 3-D agricultural field with
uncertainties using the CoppeliaSim robot simulator and measured the robustness
by running inference on the learned models. We believe our work will guide the
researchers on how to develop MDP-based environments that are applicable to
real-world systems and solve them using the existing state-of-the-art RL
methods with limited resources and within reasonable time periods.

</details>


### [121] [SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning](https://arxiv.org/abs/2508.14120)
*Yuhang Lin,Yijia Xie,Jiahong Xie,Yuehao Huang,Ruoyu Wang,Jiajun Lv,Yukai Ma,Xingxing Zuo*

Main category: cs.RO

TL;DR: SimGenHOI提出了一种结合生成建模和强化学习的新框架，有效提升了仿人机器人与物体交互的物理真实感与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的仿人机器人与物体交互生成方法常出现不真实的接触、穿透以及不自然的动作，缺乏物理可行性，难以在实际环境中应用。

Method: SimGenHOI框架利用基于Diffusion Transformers（DiT）的生成模型，结合文本提示、物体几何、稀疏路径点及初始姿态，预测关键动作并插值为平滑动作轨迹。同时设计了接触感知的全身控制策略，通过强化学习训练，跟踪生成动作并修复交互中的物理瑕疵。两者通过互相细化提升整体表现。

Result: 实验表明SimGenHOI能生成多样且物理可信的仿人-物体交互，在跟踪成功率和长时段操控任务中显著优于现有方法。

Conclusion: SimGenHOI有效融合生成模型与强化学习，大幅提升了仿人机器人与物体交互的物理真实感与可控性，为更复杂的机器人操控提供了支撑。

Abstract: Generating physically realistic humanoid-object interactions (HOI) is a
fundamental challenge in robotics. Existing HOI generation approaches, such as
diffusion-based models, often suffer from artifacts such as implausible
contacts, penetrations, and unrealistic whole-body actions, which hinder
successful execution in physical environments. To address these challenges, we
introduce SimGenHOI, a unified framework that combines the strengths of
generative modeling and reinforcement learning to produce controllable and
physically plausible HOI. Our HOI generative model, based on Diffusion
Transformers (DiT), predicts a set of key actions conditioned on text prompts,
object geometry, sparse object waypoints, and the initial humanoid pose. These
key actions capture essential interaction dynamics and are interpolated into
smooth motion trajectories, naturally supporting long-horizon generation. To
ensure physical realism, we design a contact-aware whole-body control policy
trained with reinforcement learning, which tracks the generated motions while
correcting artifacts such as penetration and foot sliding. Furthermore, we
introduce a mutual fine-tuning strategy, where the generative model and the
control policy iteratively refine each other, improving both motion realism and
tracking robustness. Extensive experiments demonstrate that SimGenHOI generates
realistic, diverse, and physically plausible humanoid-object interactions,
achieving significantly higher tracking success rates in simulation and
enabling long-horizon manipulation tasks. Code will be released upon acceptance
on our project page: https://xingxingzuo.github.io/simgen_hoi.

</details>


### [122] [Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method](https://arxiv.org/abs/2508.14185)
*Evanns Morales-Cuadrado,Luke Baird,Yorai Wardi,Samuel Coogan*

Main category: cs.RO

TL;DR: 本文在现实飞行实验中验证了一种基于Newton-Raphson方法的轻量级追踪控制器，在性能与能耗上优于或不逊于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有移动机器人追踪控制方法如反馈线性化、非线性模型预测控制等，虽有良好理论性能，但实际部署常受限于硬件算力与能耗，需要轻量且高效的新方法。

Method: 采用基于Newton-Raphson流的追踪控制器，分别在微型飞艇和中型四旋翼无人机平台上，通过实际飞行实验，与反馈线性化和非线性模型预测控制进行性能对比。

Result: Newton-Raphson流追踪控制器的轨迹均方根误差与基线方法持平或更优，且计算速度更快，能耗更低。

Conclusion: 该方法能在实际受限计算资源与能耗条件下，维持或超越现有标准控制器的性能，是有效且有潜力的追踪控制解决方案。

Abstract: We investigate the performance of a lightweight tracking controller, based on
a flow version of the Newton-Raphson method, applied to a miniature blimp and a
mid-size quadrotor. This tracking technique has been shown to enjoy theoretical
guarantees of performance and has been applied with success in simulation
studies and on mobile robots with simple motion models. This paper investigates
the technique through real-world flight experiments on aerial hardware
platforms subject to realistic deployment and onboard computational
constraints. The technique's performance is assessed in comparison with the
established control frameworks of feedback linearization for the blimp, and
nonlinear model predictive control for both quadrotor and blimp. The
performance metrics under consideration are (i) root mean square error of
flight trajectories with respect to target trajectories, (ii) algorithms'
computation times, and (iii) CPU energy consumption associated with the control
algorithms. The experimental findings show that the Newton-Raphson flow-based
tracking controller achieves comparable or superior tracking performance to the
baseline methods with substantially reduced computation time and energy
expenditure.

</details>


### [123] [SLAM-based Safe Indoor Exploration Strategy](https://arxiv.org/abs/2508.14235)
*Omar Mostafa,Nikolaos Evangeliou,Anthony Tzes*

Main category: cs.RO

TL;DR: 该论文提出了一种适用于带有障碍物的平面空间的2D探索策略，重点是提升移动机器人在安全下的自主探索能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数探索策略假设机器人能够瞬间调整自身姿态或采用点机器人模型，这在实际使用中有局限性。因此，作者针对不能瞬间调整姿态、具有圆形足迹的传统移动机器人，提出更贴合实际的探索方法，以提升实用性和安全性。

Method: 系统采用自平衡的双轮差速驱动移动平台，配备有线性加速度计、角速度陀螺仪、3D-LiDAR和RGB-D摄像头。结合IMU和LiDAR实现RTAB-SLAM，摄像头用于闭环检测。探索策略采用安全骨架法，让机器人始终保持远离障碍物，并优先朝向空间开放区域前进，最大化安全性并探索未知区域。

Result: 在实验中，基于ROS的移动机器人验证了该路径规划和空间探索策略的有效性。实验展示了机器人在探索过程中能够有效避障并规划路径。

Conclusion: 提出的探索策略在实际移动机器人平台上工作良好，重点保障了机器人与障碍物之间的安全距离，同时有效推动了未知空间的探索。

Abstract: This paper suggests a 2D exploration strategy for a planar space cluttered
with obstacles. Rather than using point robots capable of adjusting their
position and altitude instantly, this research is tailored to classical agents
with circular footprints that cannot control instantly their pose. Inhere, a
self-balanced dual-wheeled differential drive system is used to explore the
place. The system is equipped with linear accelerometers and angular
gyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs
RTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop
closures. The mobile agent explores the planar space using a safe skeleton
approach that places the agent as far as possible from the static obstacles.
During the exploration strategy, the heading is towards any offered openings of
the space. This space exploration strategy has as its highest priority the
agent's safety in avoiding the obstacles followed by the exploration of
undetected space. Experimental studies with a ROS-enabled mobile agent are
presented indicating the path planning strategy while exploring the space.

</details>


### [124] [Adapting Biological Reflexes for Dynamic Reorientation in Space Manipulator Systems](https://arxiv.org/abs/2508.14258)
*Daegyun Choi,Alhim Vera,Donghoon Kim*

Main category: cs.RO

TL;DR: 本文提出借鉴蜥蜴等动物的腾空翻正运动，将其动作轨迹用于太空机器人臂控制，以提升轨道作业能力。


<details>
  <summary>Details</summary>
Motivation: 太空机械臂由于其与航天器本体存在动力耦合，在微重力环境下控制难度极大。传统控制方法存在诸多局限，急需新的控制策略提升其在轨装配、卫星维修及碎片清理等任务时的机动性和鲁棒性。动物在空中快速调整自身姿态，表现出高效运动控制能力，启发了对生物启发式方法的探索。

Method: 研究提取了以蜥蜴为代表的动物腾空翻正的运动轨迹，利用计算机视觉技术从高速视频中提取运动数据。通过多目标优化框架，分析这些轨迹背后的关键行为目标及其相对重要性，并将得到的运动轮廓作为太空机械臂控制的参考轨迹，结合基线控制器进行跟踪验证。

Result: 动物演化出的动作轨迹被成功应用于太空机械臂的控制，初步验证了基于生物启发的轨迹能提升运动的可解释性与自适应性。

Conclusion: 本文工作为将进化动物行为转化为面向太空机器人的自适应控制策略开辟了新思路，对未来太空操作任务的机动性与鲁棒性提升具有重要意义。

Abstract: Robotic arms mounted on spacecraft, known as space manipulator systems
(SMSs), are critical for enabling on-orbit assembly, satellite servicing, and
debris removal. However, controlling these systems in microgravity remains a
significant challenge due to the dynamic coupling between the manipulator and
the spacecraft base. This study explores the potential of using biological
inspiration to address this issue, focusing on animals, particularly lizards,
that exhibit mid-air righting reflexes. Based on similarities between SMSs and
these animals in terms of behavior, morphology, and environment, their
air-righting motion trajectories are extracted from high-speed video recordings
using computer vision techniques. These trajectories are analyzed within a
multi-objective optimization framework to identify the key behavioral goals and
assess their relative importance. The resulting motion profiles are then
applied as reference trajectories for SMS control, with baseline controllers
used to track them. The findings provide a step toward translating evolved
animal behaviors into interpretable, adaptive control strategies for space
robotics, with implications for improving maneuverability and robustness in
future missions.

</details>


### [125] [D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy](https://arxiv.org/abs/2508.14355)
*Guodong Yao,Hao Wang,Qing Chang*

Main category: cs.RO

TL;DR: 提出了一种结合自适应鲁棒特征匹配与scan-to-submap配准的新型激光雷达-惯导里程计方法，有效提升了在退化场景下的定位与建图精度。


<details>
  <summary>Details</summary>
Motivation: 激光雷达在复杂环境下常因特征退化导致状态估计不可靠。为提升鲁棒性和精度，需要解决退化场景下特征匹配与位姿估计的问题。

Method: 1) 设计了基于点到传感器距离及运动幅度动态调整的自适应离群点剔除阈值，提高匹配鲁棒性。2) 融合IMU数据的scan-to-submap配准框架，优化弱特征或退化场景下的位姿估计。3) 构造结合IMU预积分协方差和退化度量的加权矩阵，进一步提升融合精度。

Result: 在室内外、特征稀疏或退化环境下进行了大量实验，结果显示本文方法在定位鲁棒性和精度上均优于当前主流方法。

Conclusion: 通过自适应离群点剔除、IMU辅助配准与权重融合，显著提升了激光雷达-惯导里程计算法在复杂退化场景下的表现，具有良好的鲁棒性和推广性。

Abstract: LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate
localization and mapping, especially in complex environments. However, the
presence of LiDAR feature degeneracy poses a major challenge to reliable state
estimation. To overcome this issue, we propose an enhanced LIO framework that
integrates adaptive outlier-tolerant correspondence with a scan-to-submap
registration strategy. The core contribution lies in an adaptive outlier
removal threshold, which dynamically adjusts based on point-to-sensor distance
and the motion amplitude of platform. This mechanism improves the robustness of
feature matching in varying conditions. Moreover, we introduce a flexible
scan-to-submap registration method that leverages IMU data to refine pose
estimation, particularly in degenerate geometric configurations. To further
enhance localization accuracy, we design a novel weighting matrix that fuses
IMU preintegration covariance with a degeneration metric derived from the
scan-to-submap process. Extensive experiments conducted in both indoor and
outdoor environments-characterized by sparse or degenerate features-demonstrate
that our method consistently outperforms state-of-the-art approaches in terms
of both robustness and accuracy.

</details>


### [126] [Action-Constrained Imitation Learning](https://arxiv.org/abs/2508.14379)
*Chia-Han Yeh,Tse-Sheng Nan,Risto Vuorio,Wei Hung,Hung-Yen Wu,Shao-Hua Sun,Ping-Chun Hsieh*

Main category: cs.RO

TL;DR: 该论文提出了一种在动作受限条件下模仿学习的新方法DTWIL，能让受限代理高效学习更高级专家的策略，有效解决了状态分布不匹配、提升了机器人控制任务表现。


<details>
  <summary>Details</summary>
Motivation: 在现实中的机器人控制和资源分配等任务中，为保证安全，常常对行动做出约束，导致模仿学习时受限代理与无约束专家之间的行为分布严重不匹配，这限制了传统模仿学习算法的效果。

Method: 作者提出了动作受限模仿学习（ACIL）问题，并引入DTWIL方法：用动态时间规整（DTW）定义轨迹对齐的距离，通过模型预测控制（MPC）规划生成动作受限代理能够实现、但在状态空间上尽量接近专家的代理轨迹，替换原有专家示范数据作为新的训练集。

Result: 实验表明，使用DTWIL生成的新数据集，动作受限代理在多个机器人控制任务中表现显著优于基线模仿学习算法，并具有更高样本效率。

Conclusion: DTWIL有效缓解了动作受限条件下模仿学习时专家-代理间的分布不匹配问题，有望广泛应用于真实约束环境下的机器人和策略学习领域。

Abstract: Policy learning under action constraints plays a central role in ensuring
safe behaviors in various robot control and resource allocation applications.
In this paper, we study a new problem setting termed Action-Constrained
Imitation Learning (ACIL), where an action-constrained imitator aims to learn
from a demonstrative expert with larger action space. The fundamental challenge
of ACIL lies in the unavoidable mismatch of occupancy measure between the
expert and the imitator caused by the action constraints. We tackle this
mismatch through \textit{trajectory alignment} and propose DTWIL, which
replaces the original expert demonstrations with a surrogate dataset that
follows similar state trajectories while adhering to the action constraints.
Specifically, we recast trajectory alignment as a planning problem and solve it
via Model Predictive Control, which aligns the surrogate trajectories with the
expert trajectories based on the Dynamic Time Warping (DTW) distance. Through
extensive experiments, we demonstrate that learning from the dataset generated
by DTWIL significantly enhances performance across multiple robot control tasks
and outperforms various benchmark imitation learning algorithms in terms of
sample efficiency. Our code is publicly available at
https://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.

</details>


### [127] [Fair-CoPlan: Negotiated Flight Planning with Fair Deconfliction for Urban Air Mobility](https://arxiv.org/abs/2508.14380)
*Nicole Fronda,Phil Smith,Bardh Hoxha,Yash Pant,Houssam Abbas*

Main category: cs.RO

TL;DR: 本文提出了一种用于城市空中交通（UAM）的半分布式公正航线规划器Fair-CoPlan，能在不同运营商之间公平分配航线长度，实验显示该方法较现有方法更公平但略有延迟。


<details>
  <summary>Details</summary>
Motivation: 由于运营城市空中交通的无人机系统（UAS）多由不同甚至竞争的运营方控制，现有的航线规划容易导致部分无人机得到过短航线，损害公平性，因此需要更公平的分配方法。

Method: 提出了Fair-CoPlan公正航线规划器，分三步：1）服务提供者（PSU）根据垂直起降场容量约束起降点；2）各运营商在该约束下独立规划；3）PSU对有冲突航线进行调整优化，以实现路径长度的公平。

Result: 通过仿真实验，Fair-CoPlan规划器比传统的非公平规划器获得了更公平的分配结果，虽然公平性提升以略有延迟作为代价。

Conclusion: Fair-CoPlan能够在保证空域安全和鼓励更多运营商参与的同时，实现无人机航线长度的公平分配，有助于城市空中出行的健康发展。

Abstract: Urban Air Mobility (UAM) is an emerging transportation paradigm in which
Uncrewed Aerial Systems (UAS) autonomously transport passengers and goods in
cities. The UAS have different operators with different, sometimes competing
goals, yet must share the airspace. We propose a negotiated, semi-distributed
flight planner that optimizes UAS' flight lengths {\em in a fair manner}.
Current flight planners might result in some UAS being given disproportionately
shorter flight paths at the expense of others. We introduce Fair-CoPlan, a
planner in which operators and a Provider of Service to the UAM (PSU) together
compute \emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU
constrains take-off and landing choices for flights based on capacity at and
around vertiports. Then, operators plan independently under these constraints.
Finally, the PSU resolves any conflicting paths, optimizing for path length
fairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages
wider participation in UAM, ensures safety of the airspace and the areas below
it, and promotes greater operator flexibility. We demonstrate Fair-CoPlan
through simulation experiments and find fairer outcomes than a non-fair planner
with minor delays as a trade-off.

</details>


### [128] [FiReFly: Fair Distributed Receding Horizon Planning for Multiple UAVs](https://arxiv.org/abs/2508.14381)
*Nicole Fronda,Bardh Hoxha,Houssam Abbas*

Main category: cs.RO

TL;DR: 提出了一种在多机器人运动规划中注入公平性的算法FiReFly，用于在资源竞争条件下优化机器人的能量消耗分配，并在仿真实验中取得了更公平的轨迹和更高的任务完成率。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，机器人间经常存在资源竞争，公平性不足可能导致少数机器人资源耗尽或任务执行效率下降，因此需要在运动规划中引入公平机制来合理分配能量资源。

Method: 提出了一种分布式的公平运动规划器，将其与安全控制器集成，设计了FiReFly算法，实现在多机器人（如无人机）中可实时运行的公平规划。通过模拟reach-avoid任务，评估轨迹的公平性、任务完成率及系统可扩展性。

Result: 仿真结果显示FiReFly算法相比不考虑公平的传统规划器，能够生成能量消耗更公平的轨迹，同时提升任务的完成率。在15架UAV下可实时运行，扩展到50架UAV时可以通过权衡运行时间和公平性来实现。

Conclusion: FiReFly算法有效提升了多机器人系统中的能量使用公平性和任务完成率，具有良好的可扩展性和实际应用前景。

Abstract: We propose injecting notions of fairness into multi-robot motion planning.
When robots have competing interests, it is important to optimize for some kind
of fairness in their usage of resources. In this work, we explore how the
robots' energy expenditures might be fairly distributed among them, while
maintaining mission success. We formulate a distributed fair motion planner and
integrate it with safe controllers in a algorithm called FiReFly. For simulated
reach-avoid missions, FiReFly produces fairer trajectories and improves mission
success rates over a non-fair planner. We find that real-time performance is
achievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with
trade-offs between runtime and fairness improvements.

</details>


### [129] [Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations](https://arxiv.org/abs/2508.14383)
*Haitong Ma,Bo Dai,Zhaolin Ren,Yebin Wang,Na Li*

Main category: cs.RO

TL;DR: 本文提出通过预训练动力学表征来增强离线模仿学习（IL）在专家数据有限情况下的表现，可高效利用大量非专家数据，提高学习效率与效果。


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习依赖专家数据，但收集这些数据通常昂贵且困难，导致数据受限成为提升模型能力的瓶颈，因此需要方法突破有限数据的限制。

Method: 引入预训练阶段，利用动力学因式分解学习环境的动力学表征。理论上证明最优决策变量可压缩在表征空间，从而减少下游IL要学习的参数量。动力学表征可由同一动力学下任意数据（包括非专家数据）学习，并提出受噪声对比估计启发的损失函数来训练表征。

Result: 在MuJoCo仿真实验中，仅用极少量专家轨迹甚至一条轨迹即可实现有效模仿学习；在真实四足机器人实验中，借助仿真动力学表征，仅需少量真实演示即可学会行走。

Conclusion: 所提方法能有效利用有限专家数据和大量非专家数据，显著提升离线模仿学习的表现和效率，具有良好的泛化能力和实际应用价值。

Abstract: Limited data has become a major bottleneck in scaling up offline imitation
learning (IL). In this paper, we propose enhancing IL performance under limited
expert data by introducing a pre-training stage that learns dynamics
representations, derived from factorizations of the transition dynamics. We
first theoretically justify that the optimal decision variable of offline IL
lies in the representation space, significantly reducing the parameters to
learn in the downstream IL. Moreover, the dynamics representations can be
learned from arbitrary data collected with the same dynamics, allowing the
reuse of massive non-expert data and mitigating the limited data issues. We
present a tractable loss function inspired by noise contrastive estimation to
learn the dynamics representations at the pre-training stage. Experiments on
MuJoCo demonstrate that our proposed algorithm can mimic expert policies with
as few as a single trajectory. Experiments on real quadrupeds show that we can
leverage pre-trained dynamics representations from simulator data to learn to
walk from a few real-world demonstrations.

</details>


### [130] [DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models](https://arxiv.org/abs/2508.14387)
*Yuxiao Zhu,Junfeng Chen,Xintong Zhang,Meng Guo,Zhongkui Li*

Main category: cs.RO

TL;DR: 本文提出了DEXTER-LLM，一个结合大语言模型和模型优化的新型动态多机器人任务规划框架，实现了在未知开放环境下的任务在线分解、分配及自适应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法多只适用于已知环境，一次性端到端规划，缺乏动态临场适应与规划过程解释性，难以应对多机器人在开放未知环境所遇的任务突发与语义特征变化。

Method: 提出DEXTER-LLM框架，包含四大模块：1）任务理解——解析LTL或自然语言的任务偏序关系；2）基于LLM的多阶段推理子任务生成——增强分解准确性与可解释性；3）基于搜索优化的子任务分配与调度；4）动态自适应与人工在环验证机制，实现事件驱动的任务/分配更新。

Result: 实验在多场景下实现了100%任务完成率，平均每场景完成160个任务与480个子任务，性能超基线方法3倍，自适应期间LLM调用减少62%，复杂任务的计划质量提升2倍。

Conclusion: DEXTER-LLM融合LLM开放式推理与模型优化分配，兼顾在线适应与解释性，显著提升了开放未知环境中多机器人系统的动态任务规划与执行能力。

Abstract: Online coordination of multi-robot systems in open and unknown environments
faces significant challenges, particularly when semantic features detected
during operation dynamically trigger new tasks. Recent large language model
(LLMs)-based approaches for scene reasoning and planning primarily focus on
one-shot, end-to-end solutions in known environments, lacking both dynamic
adaptation capabilities for online operation and explainability in the
processes of planning. To address these issues, a novel framework (DEXTER-LLM)
for dynamic task planning in unknown environments, integrates four modules: (i)
a mission comprehension module that resolves partial ordering of tasks
specified by natural languages or linear temporal logic formulas (LTL); (ii) an
online subtask generator based on LLMs that improves the accuracy and
explainability of task decomposition via multi-stage reasoning; (iii) an
optimal subtask assigner and scheduler that allocates subtasks to robots via
search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop
verification module that implements multi-rate, event-based updates for both
subtasks and their assignments, to cope with new features and tasks detected
online. The framework effectively combines LLMs' open-world reasoning
capabilities with the optimality of model-based assignment methods,
simultaneously addressing the critical issue of online adaptability and
explainability. Experimental evaluations demonstrate exceptional performances,
with 100% success rates across all scenarios, 160 tasks and 480 subtasks
completed on average (3 times the baselines), 62% less queries to LLMs during
adaptation, and superior plan quality (2 times higher) for compound tasks.
Project page at https://tcxm.github.io/DEXTER-LLM/

</details>


### [131] [FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy](https://arxiv.org/abs/2508.14441)
*Yijin Chen,Wenqiang Xu,Zhenjun Yu,Tutian Tang,Yutong Li,Siqiong Yao,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了FBI（Flow Before Imitation）框架，通过动态融合视觉和触觉信息，提高了机器手灵巧操作的表现，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 灵巧的手内操作因复杂的接触动力学和部分可观测性问题而成为长期挑战，现有方法通常侧重于单一模态，导致适应性受限。

Method: 提出FBI框架，基于动力学感知潜在模型，通过transformer模块动态融合由运动获得的触觉特征与视觉输入，训练一步扩散策略实现实时操作。

Result: 在两个自定义和三个标准灵巧操作任务的仿真与现实实验中，FBI均显著优于基线方法。

Conclusion: FBI方法能够有效结合视觉与触觉信息，增强机器人灵巧操作能力，并在多项任务中取得领先表现。

Abstract: Dexterous in-hand manipulation is a long-standing challenge in robotics due
to complex contact dynamics and partial observability. While humans synergize
vision and touch for such tasks, robotic approaches often prioritize one
modality, therefore limiting adaptability. This paper introduces Flow Before
Imitation (FBI), a visuotactile imitation learning framework that dynamically
fuses tactile interactions with visual observations through motion dynamics.
Unlike prior static fusion methods, FBI establishes a causal link between
tactile signals and object motion via a dynamics-aware latent model. FBI
employs a transformer-based interaction module to fuse flow-derived tactile
features with visual inputs, training a one-step diffusion policy for real-time
execution. Extensive experiments demonstrate that the proposed method
outperforms the baseline methods in both simulation and the real world on two
customized in-hand manipulation tasks and three standard dexterous manipulation
tasks. Code, models, and more results are available in the website
https://sites.google.com/view/dex-fbi.

</details>


### [132] [Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation](https://arxiv.org/abs/2508.14542)
*Weize Li,Zhengxiao Han,Lixin Xu,Xiangyu Chen,Harrison Bounds,Chenrui Zhang,Yifan Xu*

Main category: cs.RO

TL;DR: 本文介绍了在ICRA 2025 WBCD比赛中餐桌服务赛道获得冠军的解决方案，结合了VR远程操作和演示学习，在速度、精度和可靠性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在应对餐桌服务中的高挑战性任务（如布料展开、机器人抓取与容器操作），在满足高效、高精度和高可靠性的基础上，实现更强的自主能力。

Method: 方法上，作者融合了VR远程遥操作与基于演示的学习（LfD）。绝大多数子任务采用高保真远程操作完成，而披萨摆放则采用基于ACT的策略，通过100次现场遥操作演示数据，在随机初始情况下训练得到。全流程还综合考虑了评分规则、任务特性与现有技术水平。

Result: 通过该混合方案，系统达到了高效率和高可靠性，最终取得比赛第一名。

Conclusion: 作者验证了远程遥操作与学习方法融合在复杂餐桌作业上的优势，为双臂机器人在实际应用中的平衡鲁棒性和自主性提供了有效范例。

Abstract: This technical report presents the champion solution of the Table Service
Track in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a
series of demanding tasks under strict requirements for speed, precision, and
reliability: unfolding a tablecloth (deformable-object manipulation), placing a
pizza onto the table (pick-and-place), and opening and closing a food container
with the lid. Our solution combines VR-based teleoperation and Learning from
Demonstrations (LfD) to balance robustness and autonomy. Most subtasks were
executed through high-fidelity remote teleoperation, while the pizza placement
was handled by an ACT-based policy trained from 100 in-person teleoperated
demonstrations with randomized initial configurations. By carefully integrating
scoring rules, task characteristics, and current technical capabilities, our
approach achieved both high efficiency and reliability, ultimately securing the
first place in the competition.

</details>


### [133] [EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR](https://arxiv.org/abs/2508.14554)
*Xinkai Liang,Yigu Ge,Yangxi Shi,Haoyu Yang,Xu Cao,Hao Fang*

Main category: cs.RO

TL;DR: 本文提出了一种用于开放顶部环境中无人机（UAV）的新型硬件-算法协同定位与轨迹规划框架EAROL，并在物理实验中显著提升了定位精度和任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 在屋顶坍塌、迷宫等开放顶部场景中，无人机会遇到定位漂移严重和感知-规划耦合问题，这直接影响灾后搜救等任务的可靠性和效率。

Method: 提出一种俯视安装且倾斜（20度）的LiDAR配置，并结合紧耦合LiDAR-惯导里程计系统（LIO）与分层轨迹-偏航优化算法，利用Iterative Error-State Kalman Filter（IESKF）配合动态运动补偿以提升定位准确性。

Result: 实验证明该方法在室内迷宫和60米级室外环境等任务中，实现了81%的目标跟踪误差下降、22%的感知覆盖提升，并基本消除了垂直漂移。

Conclusion: 本文开创了一种硬件与算法协同设计的新范式，为灾后搜救等无人机自主任务提供了高鲁棒性的解决方案，并承诺开源软件和硬件，促进社区发展。

Abstract: To address the challenges of localization drift and perception-planning
coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios
(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel
framework with a downward-mounted tilted LiDAR configuration (20{\deg}
inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a
hierarchical trajectory-yaw optimization algorithm. The hardware innovation
enables constraint enhancement via dense ground point cloud acquisition and
forward environmental awareness for dynamic obstacle detection. A
tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter
(IESKF) with dynamic motion compensation, achieves high level 6-DoF
localization accuracy in feature-sparse environments. The planner, augmented by
environment, balancing environmental exploration, target tracking precision,
and energy efficiency. Physical experiments demonstrate 81% tracking error
reduction, 22% improvement in perceptual coverage, and near-zero vertical drift
across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a
hardware-algorithm co-design paradigm, offering a robust solution for UAV
autonomy in post-disaster search and rescue missions. We will release our
software and hardware as an open-source package for the community. Video:
https://youtu.be/7av2ueLSiYw.

</details>


### [134] [TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with Uncertain Obstacle Spatial-temporal Avoidance](https://arxiv.org/abs/2508.14610)
*Junzhi Li,Teng Long,Jingliang Sun,Jianxin Zhong*

Main category: cs.RO

TL;DR: 本文提出了一种名为TRUST-Planner的分层规划框架，通过结合拓扑引导及动态障碍规避方法，提高了无人机在复杂动态环境中的路径规划鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自主飞行器(AAVs)运动规划方法在面对复杂动态环境时，容易陷入局部最小值和死锁，导致碰撞风险大。

Method: 提出TRUST-Planner框架，包括：前端使用动态增强可见概率路网(DEV-PRM)快速在拓扑空间中探索路径；后端结合统一无终端极小控制多项式(UTF-MINCO)与动态距离场(DDF)实现高效预测避障及并行计算；增量式多分支轨迹管理机制支持时空与拓扑决策，并利用历史信息减少重规划时间。

Result: 在仿真实验中，TRUST-Planner的成功率达到96%，且计算效率为毫秒级，优于基线方法。

Conclusion: TRUST-Planner在复杂动态环境中的实用性和可行性得到验证，能有效提升无人机的空间-时序避障能力。

Abstract: Despite extensive developments in motion planning of autonomous aerial
vehicles (AAVs), existing frameworks faces the challenges of local minima and
deadlock in complex dynamic environments, leading to increased collision risks.
To address these challenges, we present TRUST-Planner, a topology-guided
hierarchical planning framework for robust spatial-temporal obstacle avoidance.
In the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is
proposed to rapidly explore topological paths for global guidance. The backend
utilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and
dynamic distance field (DDF) to enable efficient predictive obstacle avoidance
and fast parallel computation. Furthermore, an incremental multi-branch
trajectory management framework is introduced to enable spatio-temporal
topological decision-making, while efficiently leveraging historical
information to reduce replanning time. Simulation results show that
TRUST-Planner outperforms baseline competitors, achieving a 96\% success rate
and millisecond-level computation efficiency in tested complex environments.
Real-world experiments further validate the feasibility and practicality of the
proposed method.

</details>


### [135] [Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination](https://arxiv.org/abs/2508.14635)
*João Vitor de Carvalho Silva,Douglas G. Macharet*

Main category: cs.RO

TL;DR: 本文分析了大型语言模型（LLM）在多智能体协作救援任务中的表现，利用图结构任务和多项协作相关指标进行评估，揭示LLM在实际多智能体任务中的优劣和改进空间。


<details>
  <summary>Details</summary>
Motivation: 多智能体的协作能力对解决复杂现实问题至关重要。随着LLM在交流、规划和推理能力上的突破，研究人员希望探索其在多智能体协作中的潜力与局限。

Method: 作者设定了一个结构化的受害者救援任务，环境为完全已知的图结构，涉及劳动力分配、资源优先级评估和团队合作规划。用LLM驱动的智能体协作完成任务，通过多项指标（如任务成功率、冗余行动、冲突及效率）系统评估性能。

Result: LLM智能体能够在部分任务中实现有效协作，但仍存在协作冗余、冲突和效率提升空间。多指标分析揭示其在物理落地的多智能体任务中的表现特点。

Conclusion: 本研究拓展了LLM在多智能体协作任务中的应用认知，为未来的相关基准测试和LLM体系结构优化提供了有价值的参考。

Abstract: The ability to coordinate actions across multiple agents is critical for
solving complex, real-world problems. Large Language Models (LLMs) have shown
strong capabilities in communication, planning, and reasoning, raising the
question of whether they can also support effective collaboration in
multi-agent settings. In this work, we investigate the use of LLM agents to
solve a structured victim rescue task that requires division of labor,
prioritization, and cooperative planning. Agents operate in a fully known
graph-based environment and must allocate resources to victims with varying
needs and urgency levels. We systematically evaluate their performance using a
suite of coordination-sensitive metrics, including task success rate, redundant
actions, room conflicts, and urgency-weighted efficiency. This study offers new
insights into the strengths and failure modes of LLMs in physically grounded
multi-agent collaboration tasks, contributing to future benchmarks and
architectural improvements.

</details>


### [136] [An Informative Planning Framework for Target Tracking and Active Mapping in Dynamic Environments with ASVs](https://arxiv.org/abs/2508.14636)
*Sanjeev Ramkumar Sudha,Marija Popović,Erlend M. Coates*

Main category: cs.RO

TL;DR: 本文提出了一种面向动态环境中移动目标的主动地图构建和追踪方法，通过信息路径规划有效提升目标追踪性能，并在仿真与实地测试中验证效果优于仅使用熵减目标。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人在环境监测、搜救及污染清理等领域应用的增加，如何在受环境扰动（如风和水流）影响下有效追踪和定位漂浮目标成为亟需解决的难题。由于目标及环境的时空动态变化，现有路径规划仅以熵减为目标，无法有效提升追踪效率。

Method: 文中提出了一种信息路径规划框架，针对数量未知、初始位置不明的移动目标，利用时空预测网络对目标位置分布进行动态预测，并设计了可自适应的规划目标函数，结合预测结果实时优化机器人行动路径。

Result: 仿真实验表明，采用自适应规划目标的方式在目标追踪性能上优于单纯以熵减为目的的方法。实地测试中，基于该方法的自主水面机器人在真实监测场景下表现突出，能够有效追踪多个移动目标。

Conclusion: 所提出的方法能显著提升动态环境下对移动目标的追踪与建图能力，具有良好的实际应用价值和扩展性。

Abstract: Mobile robot platforms are increasingly being used to automate information
gathering tasks such as environmental monitoring. Efficient target tracking in
dynamic environments is critical for applications such as search and rescue and
pollutant cleanups. In this letter, we study active mapping of floating targets
that drift due to environmental disturbances such as wind and currents. This is
a challenging problem as it involves predicting both spatial and temporal
variations in the map due to changing conditions. We propose an informative
path planning framework to map an arbitrary number of moving targets with
initially unknown positions in dynamic environments. A key component of our
approach is a spatiotemporal prediction network that predicts target position
distributions over time. We propose an adaptive planning objective for target
tracking that leverages these predictions. Simulation experiments show that our
proposed planning objective improves target tracking performance compared to
existing methods that consider only entropy reduction as the planning
objective. Finally, we validate our approach in field tests using an autonomous
surface vehicle, showcasing its ability to track targets in real-world
monitoring scenarios.

</details>


### [137] [Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds](https://arxiv.org/abs/2508.14661)
*Alexander Raab,Stephan Weiss,Alessandro Fornasier,Christian Brommer,Abdalrahman Ibrahim*

Main category: cs.RO

TL;DR: 本文提出了一种新的流形误差状态扩展卡尔曼滤波器（M-ESEKF），用于提高地面车辆定位的鲁棒性和长期精度。


<details>
  <summary>Details</summary>
Motivation: 许多扩展卡尔曼滤波器（EKF）在地面车辆定位过程中，由于对状态空间的建模不完备或简化，长期一致性和精度受到影响。因此，作者希望改进EKF以解决这些一致性问题。

Method: 论文提出通过将机器人的位姿表示在降维后的流形空间中，设计了M-ESEKF。引入了新的测量模型和矫正方案，使其能兼容常见的多传感器融合方式，并能自动融入地表几何信息，从而提高不确定性评估的准确性。整个算法可以归入已有的状态估计框架中。

Result: 多组蒙特卡洛仿真和不同传感器组合的测试显示，M-ESEKF在一致性、稳定性方面优于传统方法，无需针对特定场景进行参数调整。

Conclusion: M-ESEKF能显著提升车辆定位在不同场景下的一致性和准确性，简化实际部署流程，适用于多种现实应用。

Abstract: Aiming to enhance the consistency and thus long-term accuracy of Extended
Kalman Filters for terrestrial vehicle localization, this paper introduces the
Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the
robot's pose in a space with reduced dimensionality, the approach ensures
feasible estimates on generic smooth surfaces, without introducing artificial
constraints or simplifications that may degrade a filter's performance. The
accompanying measurement models are compatible with common loosely- and
tightly-coupled sensor modalities and also implicitly account for the ground
geometry. We extend the formulation by introducing a novel correction scheme
that embeds additional domain knowledge into the sensor data, giving more
accurate uncertainty approximations and further enhancing filter consistency.
The proposed estimator is seamlessly integrated into a validated modular state
estimation framework, demonstrating compatibility with existing
implementations. Extensive Monte Carlo simulations across diverse scenarios and
dynamic sensor configurations show that the M-ESEKF outperforms classical
filter formulations in terms of consistency and stability. Moreover, it
eliminates the need for scenario-specific parameter tuning, enabling its
application in a variety of real-world settings.

</details>


### [138] [Safe and Transparent Robots for Human-in-the-Loop Meat Processing](https://arxiv.org/abs/2508.14763)
*Sagar Parekh,Casey Grothoff,Ryan Wright,Robin White,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本论文提出了一种通用型机器人系统框架，用于辅助肉类加工行业，重点解决与人协作时的安全与透明性问题。


<details>
  <summary>Details</summary>
Motivation: 受劳动力短缺困扰的肉类加工行业需要更灵活、通用的自动化技术，而当前的自动化方案不仅昂贵且不灵活，不能适应多变的作业需求。

Method: 通过行业专家调查，识别人机协作中的主要挑战：人身安全和过程透明。提出安全与透明性双重框架，包括基于手部检测的安全措施（可自动停止机器人）和基于力传感器的刀具区分物体类型。同时，利用LED与图形界面向人类主动传达机器人动作的不确定性，并允许人工反馈。

Result: 开发并实现机器人-人协作安全与透明框架，通过用户实验验证其能有效提升安全性与人机信息流通。

Conclusion: 该框架可提升肉类加工场所人机协作的安全与信任，有助于推动通用型机器人在该行业的应用。

Abstract: Labor shortages have severely affected the meat processing sector. Automated
technology has the potential to support the meat industry, assist workers, and
enhance job quality. However, existing automation in meat processing is highly
specialized, inflexible, and cost intensive. Instead of forcing manufacturers
to buy a separate device for each step of the process, our objective is to
develop general-purpose robotic systems that work alongside humans to perform
multiple meat processing tasks. Through a recently conducted survey of industry
experts, we identified two main challenges associated with integrating these
collaborative robots alongside human workers. First, there must be measures to
ensure the safety of human coworkers; second, the coworkers need to understand
what the robot is doing. This paper addresses both challenges by introducing a
safety and transparency framework for general-purpose meat processing robots.
For safety, we implement a hand-detection system that continuously monitors
nearby humans. This system can halt the robot in situations where the human
comes into close proximity of the operating robot. We also develop an
instrumented knife equipped with a force sensor that can differentiate contact
between objects such as meat, bone, or fixtures. For transparency, we introduce
a method that detects the robot's uncertainty about its performance and uses an
LED interface to communicate that uncertainty to the human. Additionally, we
design a graphical interface that displays the robot's plans and allows the
human to provide feedback on the planned cut. Overall, our framework can ensure
safe operation while keeping human workers in-the-loop about the robot's
actions which we validate through a user study.

</details>
