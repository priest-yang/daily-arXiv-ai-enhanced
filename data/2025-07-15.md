<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 160]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.RO](#cs.RO) [Total: 45]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 本文提出了应对视角变化的新方法VIL，有效提升了视觉-语言导航任务中机器人的鲁棒性，在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言导航方法对相机视角的变化（如高度和角度）非常敏感，极易导致导航性能下降。因此，亟需寻找能提升对视角变化鲁棒性的方法。

Method: 作者提出了一种视角不变学习（VIL）策略，将对比学习框架用于提取稀疏且视角不变的特征；同时在导航核心模块采用教师-学生框架，让视角相关的教师模型指导视角无关的学生模型。训练过程中各模块端到端联合优化，无需分别训练。

Result: 在两个标准基准数据集（R2R-CE和RxR-CE）上，该方法在成功率指标上比SOTA方法高8-15%。在更具挑战性的RxR-CE数据集上，各项指标均创新高。即便在标准未变化视角的任务设置下，VIL依然带来性能提升。

Conclusion: VIL方法不仅强化了导航策略对视角变化的鲁棒性，在原有标准场景下也不降低性能，可作为现有方法的即插即用后训练策略。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [2] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 本文提出了一种新的机器学习技术，通过检测面部生物特征中的异常模式来识别深度伪造视频。该方法在多个数据集及不同类型的深伪生成器上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 随着声音克隆和面部、嘴型相关的视频深度伪造技术发展，伪造视频越来越真实，增加了诈骗与政治虚假信息的风险。因此，需要更加可靠、高效的深伪检测方法。

Method: 作者提出一种新的法证型机器学习技术，通过分析深度伪造视频中面部生物特征的非常规模式来进行检测。该方法对多种深度伪造技术进行评估，并检测其对洗白视频（video laundering）和未见过的深伪生成器的泛化能力。

Result: 该检测方法在大量不同深度伪造技术与假扮样本的数据集上进行了测试，并验证了其对视频处理（laundering）和新型深伪生成器有较好的检测能力和泛化性。

Conclusion: 利用面部生物特征异常的检测方法能够有效判别深度伪造视频，为打击基于深伪伪装的欺诈和虚假信息传播提供了有力工具。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [3] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: 本文提出了一种名为PRISM的数据无关、任务无关的视觉-语言模型（VLM）去偏方法，无需额外数据或预定义偏见类别，在主流数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（如CLIP）由于训练数据中的偏见，会继承并放大这些偏见，导致预测不公。当前很多去偏方法依赖特定类别或额外数据，局限性较强，因此亟需无需额外数据，也无需预定义偏见类别的通用去偏方法。

Method: PRISM分两步操作：1）通过LLM和类提示，自动生成包含伪相关场景描述，2）采用对比风格的去偏损失函数，学习一个投影，将特征嵌入到减少伪相关但保持图像-文本对齐的潜空间。整个过程不依赖其他数据或偏见定义。

Result: 在Waterbirds和CelebA等常用偏见检测数据集上，PRISM在减少模型偏见和保持对齐精度上都优于现有主流去偏方法。

Conclusion: PRISM为VLMs去偏问题提供了新的无数据、无任务限制的有效解决方案，推进了相关模型公平性和通用性，为实际应用提供了更广泛的去偏工具。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [4] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种结合时序信息和运动学关系的视频人体网格恢复方法（HMR-ViT），利用Vision Transformer，在3DPW和Human3.6M数据集上取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体网格恢复方法通常只利用时序信息或运动学关系，而没有同时结合两者。本文旨在通过融合时序与运动学信息，提高人体网格恢复的精度和鲁棒性。

Method: 本文提出HMR-ViT方法。首先用图像编码器从视频帧中提取特征向量，构建时序-运动学特征图，并通过通道重排矩阵（CRM）使运动学上相似的特征空间上靠近。之后使用Vision Transformer进一步编码特征图，最后通过回归网络推断SMPL骨架参数。

Result: 在3DPW和Human3.6M两大主流数据集上进行实验证明，该方法在人体网格恢复任务中表现出较强的竞争力，精度优于许多现有方法。

Conclusion: 本文提出的HMR-ViT方法有效融合了时序和运动学信息，提升了人体网格恢复的效果，为后续相关研究提供了新思路。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [5] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 本文提出一种融合神经辐射场（NeRF）与材料点法（MPM）模拟的新框架，仅通过视觉观测对颗粒材料属性进行反演。


<details>
  <summary>Details</summary>
Motivation: 在许多现实情况下，直接测量颗粒材料的物理参数（如摩擦角）非常困难，因此亟需一种无需直接测量即可估算材料属性的有效方法。

Method: 首先用MPM模拟沙子与犁的相互作用，生成合成实验数据，并渲染成多视角和时序照片。接着利用NeRF重建初始三维几何，并给MPM模拟初始化材料点位置。通过贝叶斯优化，将模拟视觉结果与真实观测进行图像损失对比，以反推最优摩擦角参数。

Result: 所提方法能够将摩擦角估计误差控制在2度以内，证明了该视觉反演方法的准确性。

Conclusion: 该方法可为无法直接测量材料参数的实际情况提供有效解决方案，通过纯视觉观察实现对颗粒材料参数的精确反演。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: 提出了一种视觉分析框架VISTA，聚焦于提升多模态基础模型自动标注数据的质量，通过与人类专家协同验证提高下游任务效果。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型如CLIP、LLaVA等能为大规模数据集自动标注标签，有助于提升复杂下游任务表现。然而，这些自动生成标签的质量常被忽视，现有方法更关注数据量，缺乏有效的大规模无真值数据验证手段。

Method: 提出VISTA视觉分析框架，专为开放词汇图像分割领域设计。其通过多阶段数据验证策略，结合人类专家的知识，帮助人工及时定位、理解和校正FM生成标签中的隐藏问题。

Result: 在两个基准数据集上，结合案例分析和专家评审，证明了VISTA在定量和定性方面有效提升了数据质量。

Conclusion: VISTA能够高效提升多模态FM自动标注数据的质量，进而增强下游任务表现，填补了现有方法在数据验证和纠错上的不足。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个用于构建脑损伤影像分析流程的Python工具包，具有模块化、易用性强、并支持多种操作。


<details>
  <summary>Details</summary>
Motivation: 当前脑损伤（如胶质瘤、转移瘤和多发性硬化等）影像分析需要高效的自动化工具以简化工作流程，提高分析效率和准确性。现有工具可能较为繁琐，难以自定义和扩展，因此需要一个更简单灵活的解决方案。

Method: BrainLesion Suite以模块化设计为核心，提供可定制的预处理流程（如共配准、图谱配准、去颅骨、匿名化）；集成BraTS竞赛算法来补全缺失模态、病灶修复以及肿瘤分割；内置分割结果评估工具如panoptica。用户可以通过Python轻松定制与扩展影像分析流程。

Result: 该工具包不仅实现了脑损伤影像分析流程的自动化和高扩展性，可支持不同疾病类型和流程需求，还兼容任意模态输入。并且其子包与教程均已开源，便于学术和临床用户使用和推广。

Conclusion: BrainLesion Suite大幅简化了脑损伤影像分析的开发流程，不仅适用于脑损伤类疾病，还可扩展至更多生物医学影像分析场景，有助于推动相关科研与临床应用的发展。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 本文提出了一种通过对比损失提升长尾类别（样本量少的类别）图像多样性的方法，有效提高了条件扩散模型在不平衡数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 在类别条件图像生成任务中，数据经常呈现长尾分布，导致尾部类别的生成图像单调、缺乏多样性。现有方法多聚焦于头部类别，往往牺牲了尾部类别的生成质量。

Method: 提出两种对比损失：1）利用无监督InfoNCE损失区分不同生成样本，尤其针对尾部类别提升多样性；2）在大推断步数下对条件与无条件生成结果施加均方误差损失（MSE），使初始去噪过程对类别不敏感，从头部类别迁移知识到尾部类别。这样的条件-无条件对齐首次用于扩散模型。

Result: 实验在多个长尾数据集（如CIFAR10/100-LT、PlacesLT、TinyImageNetLT、ImageNetLT）上验证，所提框架优于标准DDPM和现有其他方法，在尾部类别提升多样性的同时保持整体生成质量。

Conclusion: 提出的方法结构简单，易于实现，有效缓解了长尾分布下条件扩散模型的受限多样性问题，推动了该领域的发展。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出了“无限视频理解”这一新蓝图——让模型能够持续、无限时长地处理和理解视频数据，并指出目前技术尚无法解决长视频的时序一致性、事件追踪和细节保留等难题。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型和多模态技术推动了视频理解进步，但在处理长达数小时甚至无限时长的视频时，现有方法因计算和存储限制，以及时间一致性和事件追踪等方面的挑战，尚无法满足实际需要。提出“无限视频理解”作为未来研究方向。

Method: 本论文为立场性论文（position paper），并未提出具体新技术，而是回顾目前如Video-XL-2、HoPE、VideoRoPE++等在长视频理解上的方法，讨论其局限性，并提出迈向“无限视频理解”的研究方向，包括：流式架构、持久化记忆机制、分层与自适应表征、事件中心的推理、以及新的评估方法。

Result: 论文通过分析现有技术瓶颈，总结了无限视频理解面临的核心挑战和研究方向，为学界指明了下一步努力的方向。

Conclusion: 实现无限视频理解具有重要意义，将驱动流媒体架构、长期记忆机制、事件推理等方向的创新。作者呼吁将此蓝点目标作为未来多媒体与AI领域的“北极星”，以推动长视频和超长视频理解的突破。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: 本文提出BlindSight方法，通过利用注意力计算中的稀疏性，显著减少多模态视觉-语言大模型（VLM）的推理计算量且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在同时处理文本和图片时，会导致输入长度急剧增加，注意力机制的计算复杂度增大，推理推前（prefill）时间大幅增加，成为应用瓶颈。

Method: 作者通过分析VLM中的注意力模式，发现除图片关键token外，大部分层的跨图片注意力很稀疏。据此提出无需重新训练的BlindSight方法，即根据输入模板自动为每个注意力头分配稀疏mask（如sink-only、document mask、hybrid等），大幅优化推理过程。

Result: 在Qwen2-VL、Qwen2.5-VL和Gemma-3等模型上验证，BlindSight在多图像理解任务中平均减少32%-41%的FLOPs，精度变化仅-2%到+2%。

Conclusion: BlindSight充分利用了VLM中存在的天然稀疏性，用工程简单、无需训练的方式有效提升了推理效率，降低了多模态大模型的部署门槛。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 本文综述了遥感反演方法从物理模型到机器学习，再到基础模型（FM）的发展，并比较了各类方法的适用场景和局限，展望了未来基础模型在遥感反演领域的应用。


<details>
  <summary>Details</summary>
Motivation: 定量遥感反演在生态监测、碳核算与土地管理等领域具有核心作用，而随着遥感与人工智能的进步，传统物理模型面临应用与泛化的局限，因此亟需系统梳理新兴的数据驱动方法和基础模型的发展与挑战。

Method: 论文对现有遥感反演方法进行了系统性梳理，从经典的物理模型（如PROSPECT、SCOPE、DART），到基于机器学习的方法（如深度学习、多模态融合），再到当前的基础模型（如SatMAE、GFM、mmEarth），并对这些范式的假设、应用和限制进行对比分析。

Result: 论文重点分析了基础模型在自监督预训练、多模态集成和跨任务适应等方面的最新进展，并指出了物理可解释性、领域泛化、有限监督和不确定性量化等依然存在的挑战。

Conclusion: 作者呼吁推动新一代遥感反演基础模型的发展，强调模型的统一建模能力、跨领域泛化和物理可解释性，以实现遥感反演更广泛且可靠的应用。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 本文提出一种无需微调、通过对自监督视频生成模型进行推理提示（prompting）即可实现光流（optical flow）提取的新方法，并显著优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 以往读取光流往往依赖标签进行微调，但高质量流标签稀缺，合成集又与真实数据存在差距。作者希望基于最新的视频生成模型，仅凭测试时干预即可跨越这个障碍。

Method: 受Counterfactual World Model（CWM）启发，作者在具备分布式预测、因子化潜变量和任意局部条件解码特性的高效生成模型（如LRAS）上，提出KL-tracing：在首帧局部注入微弱扰动，预测下一帧，并计算扰动前后分布的KL散度，从而获得高质量光流。

Result: 方法在无需针对光流微调模型的前提下，在真实世界的TAP-Vid DAVIS数据集（终点误差提升16.6%）和合成的TAP-Vid Kubric数据集（提升4.7%）上均超过SOTA（最优）基线。

Conclusion: 在支持反事实推理的生成视频模型中，利用KL-tracing进行零样本prompt式光流提取可高效实现高质量流估计，为光流问题提供了超越传统有监督和基于光度损失方法的新方向。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种新的可视化解释方法MI CAM，可以更好地解释卷积神经网络的推断机制，并在定性和定量上优于部分现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗、自动化电厂等重要领域的应用，人们越来越关注神经网络内部机制及其推理依据，因此需要更有效的解释方法。

Method: 提出了一种基于激活映射的可视化后验解释方法MI CAM。它通过计算每个特征图与输入图像之间的互信息，对特征图加权，然后线性组合这些加权后的激活图，得到显著性可视化结果。同时，利用反事实分析验证其因果解释能力。

Result: MI CAM的可视化表现优异，解释模型推理过程更加公正、无偏，在与多种主流方法的对比中，MI CAM在某些定性、定量指标上表现更好。

Conclusion: MI CAM方法能有效帮助解释卷积神经网络的推断过程，对提升机器视觉系统的透明度和可信度具有积极意义，且已开源实现。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: 该论文提出用眼动视频（而非传统热图）输入，提升大规模视觉语言模型在胸部X光影像分析中的性能，尤其在报告生成和疾病诊断任务上显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅用热图或文本提示表达放射科医生的注视信息，忽略了注视的时序顺序。而时序信息在实际诊断过程中很重要，有助于模型更好地理解医生关注和判断的逻辑。

Method: 作者提出RadEyeVideo方法，将放射科医生的眼动注视数据以视频序列形式作为输入，结合LVLMs（具有视频输入能力）进行胸部X光报告生成和疾病诊断任务的联合评估。

Result: 用眼动视频作为提示时，模型在报告生成任务性能提升最高达24.6%，两个任务平均提升15.2%。此外，RadEyeVideo使通用LVLM（如LLaVA-OneVision）超越了在医学影像上专门训练的模型（如MAIRA-2和CheXagent）。

Conclusion: 有效整合领域专家知识（如眼动信息）能够大幅增强通用视觉语言模型在临床任务中的能力。RadEyeVideo为实现可扩展、人本导向的医学影像AI分析开辟了新方向。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出了PointSD框架，将大规模文本到图像扩散模型（如Stable Diffusion, SD）引入3D自监督学习，通过点云引导图像去噪，以提升点云表示的学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D自监督学习扩散模型受限于训练数据集规模，性能有限，而SD等2D扩散模型在大规模数据集上训练表现出强大的特征学习能力。作者希望借助SD模型提升3D自监督点云学习的表达能力。

Method: 作者将SD模型的文本编码器替换为三维编码器，训练点云到图像的扩散模型，使点云特征能够在扩散过程中引导渲染的图像去噪，并利用去噪后的图像和点云提取SD特征，通过特征对齐训练3D骨干网络，实现3D表示的自监督学习。

Result: 在多个点云下游任务和消融实验中，PointSD框架都取得了优于现有方法的性能，证明了SD模型对点云自监督学习的提升作用。

Conclusion: 将大规模训练的2D扩散模型成功迁移到3D点云表示学习，为低资源3D领域带来了更强的特征能力和表现，有助于提升点云的自监督学习水平。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 本文提出了一种结合自回归与扩散模型的混合方法用于手语生成，显著提升了生成质量和实时效率。


<details>
  <summary>Details</summary>
Motivation: 传统手语生成模型多采用自回归方式，虽然训练时能防止模型崩溃，但推理时容易出现误差累积。同时，扩散模型虽生成效果好，但推理速度慢，不适用于实时场景。如何兼顾顺序依赖性、生成质量和实时性是难题。

Method: 首次将自回归与扩散模型结合用于手语生成，利用自回归强顺序建模和扩散模型的输出优化能力。为细致刻画肢体动作，设计多尺度姿态表示模块与融合模块。还提出置信度感知因果注意力机制，利用关节点置信度动态引导动作生成。

Result: 在PHOENIX14T和How2Sign数据集上进行大量实验证明，该方法在生成质量和实时流式性能上均优于现有方法。

Conclusion: 混合自回归与扩散模型、引入多尺度表示和置信度机制的方法，有效提升了手语生成的准确性、细腻性与实时表现，对实际应用有重要意义。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本论文提出了第一个专注于人-物交互（HOI）检测的健壮性基准RoHOI，并提出了提升模型鲁棒性的SAMPL策略，有效增强现实场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型虽在干净数据集表现良好，但在现实世界的环境波动、遮挡、噪声等干扰下表现显著下降，缺乏系统性的健壮性评价与专注鲁棒性的改进方法。

Method: 1）构建RoHOI健壮性基准，包含基于HICO-DET和V-COCO的20种真实场景扰动类型与新鲁棒性评价指标；2）提出语义感知遮罩渐进学习（SAMPL）策略，通过全局与局部特征动态优化模型，促进模型学到鲁棒表征。

Result: 系统评测显示，传统模型在各种扰动下性能大幅下降；而采用SAMPL策略的新模型在多种挑战下表现优于当前主流方法，在鲁棒性上树立新标杆。

Conclusion: RoHOI为HOI检测鲁棒性研究提供了标准基准，SAMPL方法则显著提高了模型的鲁棒性能，对实际智能体交互任务具有重要意义。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于模态差距的新方法（MG-CLIP），显著提升了CLIP模型在持续学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然CLIP在多任务下表现优异，但现有持续学习方法未考虑其内在的模态差距，这是影响泛化和适应性的关键因素。作者意图揭示并利用这一潜在机制来提升CLIP在持续学习中的能力。

Method: 首先分析了视觉-语言预训练模型微调过程中模态差距的变化，并发现模态差距能反映知识保留程度。基于此，提出MG-CLIP方法，通过保留模态差距减少遗忘，并利用模态差距补偿来提高对新数据的学习能力。该方法无须额外重放数据即可参照模态差距进行优化。

Result: 在多个持续学习基准上进行广泛实验，MG-CLIP优于现有方法，并取得了更好的任务增量学习表现。

Conclusion: 模态差距是影响持续学习的重要因素。MG-CLIP有效提升了CLIP在持续学习中的表现，为该领域带来了新的思路。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: 本文提出了SnapMoGen，一个包含高质量动作捕捉数据和详细文本描述的大型数据集，并基于此开发了新的生成模型MoMask++，在多个基准数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法由于缺乏大规模、细粒度标注的数据集，通常仅能基于简短或泛化的文本描述合成动作，缺乏对复杂、长文本的生成能力和对未见提示的泛化能力。

Method: 1) 构建SnapMoGen数据集，包含2万段、总时长44小时的动作片段与12.2万条平均48词的详细文本注释。2) 设计MoMask++，将动作转为多尺度token序列，并用单个生成遮蔽Transformer学习全部生成任务。3) 利用大语言模型（LLM）将用户输入转为SnapMoGen风格的富表达文本，实现对自然提示的处理能力。

Result: MoMask++在HumanML3D和SnapMoGen两个主流基准上都达到业界最优表现，展现了对复杂、长文本描述下动作生成的强大能力和对多样文本类型的适应性。

Conclusion: 本文提出的数据集和生成方法显著提升了文本驱动动作生成的表达力、可控性及泛化能力，为长序列动作生成和实际应用中的用户自定义提示处理奠定了基础。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于大语言模型的人体姿态估计算法PoseLLM，通过非线性MLP连接器实现视觉与文本特征的更佳融合，相较于现有方法提升了精度和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态估计方法往往预设关键点先验，导致对新奇姿态或未见关键点泛化能力弱。最近基于语言引导的方法虽启用零样本泛化，但如LocLLM采用线性模块难以有效结合空间与文本特征。

Method: 提出PoseLLM，创新性地用两层带GELU激活的非线性MLP替代原始线性视觉-语言投影模块，提升视觉patch与文本描述的层次化交互，促进视觉与语言信息更深层次融合。

Result: 仅用COCO数据训练的PoseLLM在COCO验证集上达到77.8 AP，比LocLLM提升0.4分，在Human-Art与MPII数据上保持强大的零样本泛化能力。

Conclusion: 简单而有效的非线性连接器能大幅提升基于语言的人体姿态定位精度，同时保持优秀的泛化性，为语言引导的人体姿态估计方法带来了新的发展。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 本文提出了一种高效的4D占据预测框架$I^2$-World，在复杂三维场景及未来场景生成中刷新了性能与效率纪录。其通过分阶段（空间与时间）自适应tokenizer，有效解决3D场景token化难题，并在多个指标及资源占用上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前3D场景的占据世界建模对于自动驾驶系统应对极端情况很有潜力，但高效实现3D场景token化仍然非常困难。现有基于token化的图像、视频生成取得突破，而复杂的3D世界模型亟需更优解决方案。

Method: $I^2$-World将场景tokenizer分为两步：1）内部采用多尺度残差量化，实现3D场景分层压缩、保留空间细节；2）外部利用残差聚合对跨时间步依赖建模。架构上放弃GPT单解码器风格，采用编码器-解码器结构：编码器聚合空间上下文并预测变换矩阵实现场景高级控制，解码器借助历史token与该矩阵，保证生成过程的时序一致性。

Result: $I^2$-World在4D占据预测任务上mIoU提升25.1%，IoU提升36.9%，均优于当前最新方法，并且训练时内存开销极低（仅2.9GB），推理速度高达37 FPS，具备实时性。

Conclusion: 该工作首次在效率与表现上兼顾了复杂动态3D场景的token化难题，为智能体世界模型及自动驾驶提供了更具应用前景的解决思路。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [22] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了稳定分数蒸馏（SSD）框架，提升了基于文本的图像与三维编辑的稳定性与编辑精度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的文本引导编辑（如Delta Denoising Score）在稳定性、空间控制和编辑强度方面存在瓶颈，主要由于依赖复杂辅助结构，导致优化信号冲突，难以实现精确、局部的编辑。

Method: 提出了Stable Score Distillation（SSD）框架：通过锚定单一分类器到原始提示词，引入Classifier-Free Guidance公式实现跨提示词对齐，同时加入常数项的null-text分支稳定优化过程，并增设提示词增强分支提升风格编辑的强度。

Result: SSD在2D、3D编辑任务（包括NeRF与风格编辑）中达到了SOTA（最优）表现，收敛更快，模型结构更简单。

Conclusion: SSD提供了一种稳定、高效且灵活的文本驱动编辑方案，兼顾内容结构一致性和编辑强度，助力图像与三维内容的精细化修改。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [23] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉Transformer的主干网络，通过融合RGB和深度信息提升跨域泛化能力，并利用对比自监督学习和灵活课程学习提升强化学习中的sim2real迁移效果。


<details>
  <summary>Details</summary>
Motivation: 深度信息因其对3D空间细节的强表达能力和对场景变化的鲁棒性，被广泛应用于视觉任务。传统方法在将深度信息与RGB融合、泛化到真实场景时效果有限，因此需要设计更有效的融合机制和迁移策略。

Method: 1. 使用独立CNN提取RGB和深度信息特征，再结合后输入可扩展的Vision Transformer进行深度融合。2. 设计基于掩码/非掩码token的对比自监督学习机制，提升强化学习中的样本效率。3. 提出灵活课程学习方法，在训练过程中动态开展域随机化以加强sim2real迁移。

Result: 实验表明，该方法在泛化和sim2real迁移方面优于传统视觉主干和融合方法，提升了强化学习任务在真实场景中的表现。

Conclusion: 融合RGB和深度信息的Vision Transformer主干结合对比自监督和课程学习策略，有效提升了视觉任务在仿真到真实环境中的适应性和泛化能力。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [24] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新的局部-全局空间提示（LGSP-Prompt）方法，解决了在少样本类增量学习（FSCIL）场景下池化提示方法性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前FSCIL面临数据稀缺与增量学习的双重挑战。尽管池化提示方法在传统增量学习中取得了成功，但在FSCIL中的有效性尚未得到验证，且现有方法在增量阶段出现性能下降，因此亟需新的方法以提升FSCIL效率。

Method: 作者首先分析了现有池化提示在FSCIL中的表现，发现由于数据有限，过多提示在token维度争夺信息导致过拟合。为此，作者提出LGSP-Prompt：将池化提示学习从token维度转移到空间维度，同时结合局部空间特征与全局频域信息构造空间提示池，并采用动态提示选择机制以兼顾知识保持和增量学习。

Result: 在多个FSCIL基准数据集上进行的大量实验证明，LGSP-Prompt在基础知识保持和增量学习能力上都优于现有方法，取得了新的最佳性能。

Conclusion: 本文的空间提示新范式有效缓解了token维度饱和引起的过拟合问题，拓展了提示学习在FSCIL领域的应用范围，为后续相关研究提供了新的思路。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [25] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MCA-LLaVA的新型位置编码方法，通过缓解大模型中因位置编码衰减造成的图像对齐偏差，减少了多模态幻觉，提高了模型整体表现。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（LVLMs）常因多模态特征失配而出现幻觉问题。以Rotary Position Encoding（RoPE）为代表的现有位置编码方法在长序列情况下带来对图像位置信息的偏见，导致图像-文本对齐不足，需要新的方法优化多模态交互。

Method: 作者分析了RoPE带来的长程衰减现象后，提出MCA-LLaVA方法：以曼哈顿距离为基础，实现图像token在二维空间的多方向衰减，综合一维序列顺序和二维空间位置共同进行位置建模，从而改善指令token对图像各位置信息的感知。

Result: 实验证明，MCA-LLaVA在多项与幻觉相关的基准测试和通用基准上都取得了更优异的性能，显示了其对缓解幻觉问题和提升模型泛化能力的有效性。

Conclusion: 通过引入MCA-LLaVA的二维空间位置建模策略，有效缓解了因位置编码衰减引起的图像对齐偏差（image alignment bias），提升了视觉语言模型实时理解和生成的准确性与稳健性。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [26] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种新的面向视频的场景图生成方法THYME，有效提升了动态图景理解的空间细致性与时序一致性，并发布了全新空中视频数据集AeroEye-v1.0。实验显示该方法优于现有主流技术。


<details>
  <summary>Details</summary>
Motivation: 随着视频在自动驾驶、监控和体育分析等领域的激增，动态场景理解成为亟需攻克的问题。而现有视频场景图生成方法在同时刻画精细空间细节和长期时序关系方面表现有限，导致场景表达支离破碎，难以满足实际应用需求。

Method: 提出Temporal Hierarchical Cyclic Scene Graph (THYME)方法，通过分层特征聚合提升多尺度空间表达，并引入循环式时序细化机制确保帧间时序一致性。同时，推出包含丰富交互类型的新型空中视频数据集AeroEye-v1.0，作为动态图景图生成的基准。

Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME方法在多个评价指标上显著优于主流方法，有效提升了地面视角及空中视角下的视频场景理解精度和一致性。

Conclusion: THYME方法为动态视频场景图生成提供了更细致且一致的表达能力，配套数据集AeroEye-v1.0增强了领域研究基础，对自动驾驶等实际应用具有较强推动作用。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [27] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 本文提出了一种通过视频分析材料表面波传播，从而推断其厚度和刚度的无创方法。


<details>
  <summary>Details</summary>
Motivation: 传统的材料厚度和刚度测量方法大多需要专业仪器或破坏性测试，而许多应用场景（如家庭健康监测等）亟需一种非接触、低成本的检测手段。

Method: 该方法通过分析材料表面波的传播视频，先提取出色散关系，再结合基于物理的优化算法，反推最符合实际的厚度和刚度参数。

Result: 经仿真和真实数据实验验证，所提方法在两种情况下测得的参数结果与真实值高度一致。

Conclusion: 该技术为居家进行组织健康信息检测等提供了可行思路，同时也可用于人机交互等其他领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [28] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练即可将医疗视觉语言模型（MedVLM）与临床知识对齐的新框架Expert-CFG，该方法通过嵌入专家参与和不依赖分类器的引导机制，有效提升模型在医学问答等任务的准确性和安全性。实验表明，即使模型参数量远小于主流大模型，Expert-CFG在多个基准数据集上均取得了更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态助手系统在实际应用中会因模型的不确定性而出现错误或未经验证的回答，这在医学领域可能造成严重后果。以往通过数据和模型结构的训练优化方式代价高且尚无法充分对齐临床专家知识，因此亟需无训练、低成本、专家参与的模型优化手段。

Method: 提出Expert-CFG框架：包括针对模型输出不确定性的估计、自动检索并引用相关医学参考、通过专家标注高亮关键术语，并结合无分类器的引导机制对模型输出进行嵌入优化，从而校正模型输出并增强其对专家知识的对齐性。整个流程无需新模型训练，仅需少量专家干预。

Result: 在三个医疗视觉问答基准测试中，Expert-CFG在仅用4.2B参数和有限专家注释的条件下，整体超过了主流的13B大模型的性能，说明方法的高效与实用性。

Conclusion: Expert-CFG框架能够高效对齐MedVLM与临床专家知识，提高模型的安全性和准确率，且易于资源受限的医疗场景部署，展示出在实际临床应用中的潜力与可行性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [29] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于立体视觉的三维异常物体检测方法S3AD，有效提升了模型对异常物体（尤其是未见类别）的检测能力，并构建了新的增强现实立体数据集KITTI-AR进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有三维检测模型多在封闭集上训练，对于路面罕见异常目标易造成漏检或误检，影响自动驾驶安全。因此需提升三维检测对任意形状目标的泛化检测和异常筛查能力。

Method: 1）提出S3AD算法，通过解耦三维与二维训练策略，提升三维前景检测的泛化能力；2）开发基于前景置信度预测的异常打分方法，实现目标级别的异常检测；3）使用三维渲染方法合成KITTI-AR数据集，包含常见及罕见类别，支持异常检测算法的训练与评估。

Result: 实验表明S3AD算法在新建数据集上有效提高对异常目标的三维检测准确率。数据集中的KITTI-AR-OoD子集用零样本评估异常检测能力，验证算法优越性。

Conclusion: S3AD算法及KITTI-AR数据集提升了三维检测对未知异常目标的泛化检测和异常筛查能力，为提高自动驾驶异常处理安全性提供新思路。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [30] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的全景图像球面采样方法，利用已有的2D图像预训练模型，解决了现有模型对全景图像失真与不连续性识别能力不足的问题。该方法在全景图像分割任务中于公共数据集取得了良好表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模全景图像数据集，导致相关任务普遍依赖于二维预训练模型。但这些模型无法处理全景图像特有的失真和不连续性，性能受限。

Method: 作者提出球面离散采样方法，通过基于预训练模型权重进行采样，使得原本2D模型在处理全景图像时能够减小失真影响，同时获得良好的初始训练值。该方法也将特征用于作为特定通道注意力的掩码。

Result: 该方法应用于全景图像分割任务，在常用室内数据集Stanford2D3D上取得了不错的实验结果。

Conclusion: 提出的方法有效提升了二维预训练模型在全景图像相关任务上的适应性和表现，显示出良好的应用前景。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [31] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Online长时点追踪模型Track-On，并在多个公开基准上达到了新SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有大多数长时点追踪方法依赖于offline设置，即可利用未来帧优化结果，但现实场景常常要求online预测，仅能访问当前与历史帧，具备实时性和时序因果性，意味着相关模型设计亟需突破。

Method: 作者首先评估了视觉基础模型（Visual Foundation Models, VFM）在追踪任务上的作用，发现其能为后续追踪提供良好初始化，但不足以解决在线长时追踪。为克服在线因果条件导致的时间一致性问题，作者提出Track-On：基于Transformer结构，将每个被追踪点视为单独查询，并为其设计了跨帧记忆传递机制，实现了逐帧处理、仅凭历史帧完成长时追踪。

Result: Track-On无需访问未来信息即可实现长时点追踪，在7个公开基准上取得了最新的SOTA效果，优于多项现有方法。

Conclusion: 本论文证明了视觉基础模型能为点追踪提供空间特征和初始化，但真正的在线长时追踪需要更专门的设计。Track-On模型通过跨帧记忆与因果推理，实现在纯在线情形下的高效准确追踪，推动了此类实际需求任务的进步。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [32] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: 提出StaRFM统一框架，解决大模型在视觉和医学影像领域低样本迁移时的分布偏移与置信度错配问题，方法简单、易集成，实验效果显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型（如CLIP、SAM）在计算机视觉及医学影像中表现优异，但受分布偏移与置信度错配影响，实际部署效果受限，且现有应对策略多为任务或领域特定，缺乏通用解决方案。

Method: 提出StaRFM统一框架：1）用Fisher信息惩罚（FIP），并通过patch正则化扩展到3D医学数据，缓解特征转移中的协变量偏移；2）引入置信度错配惩罚（CMP），针对体素级预测调整模型不确定性，实现更好校准。理论上，通过PAC-Bayes边界和Brier分数证明方法对泛化和校准的有效性。

Result: 在19个视觉数据集上提升3.5%准确率，降低28%校准误差（ECE）；在医学分割数据集上达84.7% DSC、4.8mm HD95，跨域性能差距减少40%，整体大幅优于基线方法。

Conclusion: StaRFM能有效泛化到不同任务与领域，易于与各类基础模型集成，显著提升了模型鲁棒性和实用性，有望广泛应用于计算机视觉与医学影像。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [33] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该论文提出了一种将第一人称俯视图（egocentric）单帧图像转化为可动画化虚拟形象的生成式方法，提升了数字远程在场感体验。


<details>
  <summary>Details</summary>
Motivation: 数字远程在场感需要精准还原人的身体、服饰和动作。然而，第一人称视角设备便携、低成本，但会带来遮挡和比例失真等问题，目前很少有研究能从该视角重建外观，尤其未见生成模型相关工作。作者希望借助生成式模型降低对多视角数据的依赖，实现更泛化、易用的化身生成。

Method: 作者提出了一种基于Stable Diffusion和ControlNet的管线，能将具有遮挡和畸变的egocentric图像还原为真实的正面视角，并可喂入动作生成模型，最终用于动画化虚拟化身。此方法受SiTH与MagicMan等技术启发，利用生成式骨干网络提升还原精度和训练简便性。

Result: 该方法显著降低了训练所需的数据量与难度，并提高了从单一top-down egocentric图像重建虚拟形象的泛化能力和真实性。

Conclusion: 本研究首次展示了利用生成模型从第一人称视角单帧图像生成可动画化虚拟形象的可行性，实现了用极少输入推动更高可达性与泛化性的远程在场系统。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [34] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本论文提出了首个关注绘画过程的评价框架，包括新数据集PPAD和基于Transformer的评估模型PPJudge，实现了对绘画过程更符合人类认知的自动评价。


<details>
  <summary>Details</summary>
Motivation: 当前艺术图像评估主要关注于静态最终作品，忽略了绘画的动态多阶段过程，导致评价不全面；本研究旨在弥补这一空白。

Method: 作者构建了包含真实与合成绘画过程图像及八项专业属性标注的大型数据集PPAD，并提出了融合时间感知位置编码和异质专家混合架构的Transformer模型PPJudge，用于智能判断绘画过程。

Result: PPJudge模型在准确率、鲁棒性以及与人类专家判断一致性等方面，均优于现有基线方法。

Conclusion: 本研究为绘画过程中符合人类视角的自动评估提供了新方法，对计算创造力与美术教育领域具有启发和推动作用。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [35] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: 提出了AGCD-Net模型，通过因果干预和注意力机制，有效去除情境中的偏差，实现对复杂场景中情感识别的提升。


<details>
  <summary>Details</summary>
Motivation: 传统的情境感知情感识别易受背景偏差影响，导致模型将无关背景与情感错误关联（如将“花园”与“高兴”关联）。亟需方法减少这类偏差以提升情感识别鲁棒性。

Method: 提出了AGCD-Net模型，包含新增的Hybrid ConvNeXt卷积编码器（融合空间变换网络和Squeeze-and-Excitation层），提升特征重校能力。核心的AG-CIM模块依据因果理论扰动情境特征，隔离虚假相关性，并通过面部特征引导注意力进行纠正，从而减轻情境背景偏差。

Result: 在CAER-S数据集上，AGCD-Net实现了当前最优的情感识别准确率，验证了因果去偏策略的有效性。

Conclusion: AGCD-Net模型可显著减弱情境偏差，提高现实复杂环境下的情感识别鲁棒性，因果去偏是情感识别领域不可忽视的重要方向。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [36] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的图文匹配方法AAHR，针对现有方法在处理高阶关系与语义歧义方面的不足，通过动态聚类、全局和局部特征提取、邻域关系建模及对比学习，有效提升了图文匹配的准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的图文匹配方法常在处理软正样本（语义相似但标签错误）与软负样本（局部匹配但整体不一致）、以及批次内邻域关系利用不充分时存在缺陷，从而影响模型对高阶共享知识的学习与歧义消解。

Method: 本文提出“歧义感知高阶关系学习框架（AAHR）”，主要包括：1）动态聚类原型对比学习，统一表达空间并缓解软正样本问题；2）全局与局部特征提取、以及自适应聚合增强语义理解；3）使用图神经网络（GNN）结合自适应相关矩阵深入分析样本间邻域关系，强化语义交互；4）动量对比学习机制以扩展负样本集合，提升判别能力。

Result: 在Flickr30K、MSCOCO、ECCV Caption等主流数据集上，AAHR均显著优于现有最先进方法，在图文匹配准确率和效率方面取得新突破。

Conclusion: AAHR框架有效增强了模型对特征的判别能力与歧义处理能力，极大提升了图文匹配效果，对图像-文本理解任务具有重要推动作用。代码与模型已开源。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [37] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出一种无Gloss手语翻译的新方法，通过基于段落的视觉分词显著减少输入序列长度，从而提高大规模数据集的可扩展性和效率，并在不增加模型复杂度的情况下超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 虽然无Gloss手语翻译取得了较大进展，但现有方法通常依赖更复杂的模型和更高的计算资源，导致在处理大规模手语数据集时难以扩展。因此，需要一种方法既能提升效率，又不增加模型复杂度。

Method: 本文提出了一种基于手语段落分割的视觉分词框架，将连续视频转化为离散的、基于段的信息视觉token，有效缩短了输入序列长度。此外，设计了token间对比对齐损失及双层监督机制，同时对齐语言嵌入和中间隐状态，提升序列的跨模态对齐精度，无需依赖Gloss标注。

Result: 在PHOENIX14T基准上，该方法不仅在不增加复杂度的情况下显著性能优于现有SOTA方法，还将输入序列长度缩短最高达50%，内存消耗降低约2.67倍。此外，在相同序列长度下也明显优于以往方法。

Conclusion: 本文策略有效地促进了手语翻译模型的可扩展性与性能提升，结构简洁且更便于处理大规模数据集，为无Gloss手语翻译任务提供了高效且准确的新思路。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [38] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态与跨架构的知识蒸馏方法（CKD），显著提升了脉冲神经网络（SNNs）在事件驱动（DVS）数据上的表现，并超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 尽管SNNs在能效和生物可信度方面有优势，但因事件数据集标注有限且结构不成熟，其表现不及人工神经网络（ANNs）。因此，作者希望通过利用现有优秀的ANNs和RGB数据，通过知识蒸馏提升SNNs在本地优势数据（DVS数据）上的性能。

Method: 作者设计了一种跨知识蒸馏（CKD）方案，针对跨模态和跨架构的挑战：1）通过语义相似性和滑动替换缓解跨模态（RGB到DVS）知识迁移问题；2）采用间接分阶段知识蒸馏，促进ANN模型到SNN模型的知识迁移，以解决结构差异带来的困难。

Result: 该方法在N-Caltech101和CEP-DVS两个主流神经形态数据集上进行了实验证明，相对于当前最优方法，取得了更好的性能。

Conclusion: CKD有效促进了ANN到SNN的知识迁移，显著提升了SNNs在事件驱动视觉任务上的表现，表明了跨模态及跨架构知识蒸馏的可行性和优势。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [39] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: 本文针对多模态大模型（MLLMs）在医疗领域应用中，因对Prompt敏感和高置信度下错误回答的问题，提出了Prompt4Trust——首个基于强化学习的置信度校准Prompt增强框架。该方法通过训练轻量模型生成辅助Prompt，引导MLLMs生成更可信的预测，在提升置信度校准的同时也提高了答案准确率，在医学视觉问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险场景，MLLMs常因提问方式不同导致输出变化，并可能自信地产生错误答复，危害临床决策。现有置信度校准方法未能针对临床安全需求优化，因此亟需能够提升可信度表达与预测准确一致性的Prompt增强与校准技术。

Method: 提出Prompt4Trust框架：用强化学习训练轻量LLM，生成场景相关的辅助Prompt，指导目标MLLM生成的每条答复的置信度更准确表达其正确率。此方法侧重满足临床决策中对置信度校准的实际需求，支持小模型对大模型的可迁移零样本校准。

Result: Prompt4Trust在医学多选视觉问答（PMC-VQA）数据集上取得了当前最佳表现。实验显示，只需用小模型训练的Prompt4Trust，也可高效提升大模型在新任务下的校准与准确率，同时节省大量计算资源。

Conclusion: Prompt4Trust能够显著提升MLLMs在医疗等安全关键场景下的可信度与准确率，对提升自动化人类对齐Prompt工程和临床应用安全具有重要意义。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [40] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度生成模型的盲运动去模糊（BMD）方法，通过生成模型编码模糊核先验，并为核估计提供高质量初始化，从而有效提升去模糊性能并缓解对初始核敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度先验的盲运动去模糊方法在优化过程中高度非凸，导致结果极其依赖初始模糊核，影响泛化和实用性。作者试图减少这种对初始化的敏感性，提升BMD算法的稳定性和效果。

Method: 作者设计了一个基于生成对抗网络（GAN）的核生成器来学习和表达模糊核的先验分布，同时提出了一个核初始化器，为后续核估计提供更优的起点。这两部分以可插拔形式与现有BMD方法结合同步优化，还可适配于非均匀运动去模糊场景。

Result: 所提出方法在多个具有挑战性的基准数据集上实现了当前最优的盲去模糊效果，有效提升了估计精度，源代码已公开。

Conclusion: 利用深度生成模型提供核先验并优化初始化能显著改善盲运动去模糊的稳定性和最终性能，该框架通用易集成，有助于实际应用的推广。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [41] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本论文提出了一种同时整合深度与语义信息的室内平面图定位框架，在现有基准数据集上显著提升了准确率，优于当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的平面图定位方法主要依赖于深度等结构特征，忽略了平面图中丰富的语义信息（如门窗位置等），导致定位的准确性和效率有限。

Method: 作者提出了一种语义感知的定位框架，该框架联合估计深度射线与语义射线，并据此构建结构-语义概率体积（probability volume）。该概率体积采用“由粗到细”的采样策略，先对少量射线做低分辨率估计，再在高概率区域细致采样以得到更精确的2D位置与朝向角预测。此外，框架支持融合额外元数据（如房间标签）以提升性能。

Result: 在两个主流平面图定位基准上，所提方法在召回率等指标上大幅超越现有最优方法。同时，集成房间标签等元数据后，定位精度和效率进一步提升。

Conclusion: 联合深度与语义信息的定位框架能显著提升精度，并具备灵活扩展性，是平面图定位方向的重要进展。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [42] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: 本论文提出了一种结合RGB和深度信息的几何感知网络Geo-RepNet，用于提升内镜黏膜下剥离手术（ESD）中手术阶段识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ESD等微创手术的各阶段在视觉上区别很小，单靠RGB图像难以有效识别各阶段，影响智能辅助系统的开发。深度信息能提供补充的几何结构线索，有望解决这一问题。

Method: 提出Geo-RepNet，该网络以RepVGG为骨干，设计Depth-Guided Geometric Prior Generation（DGPG）模块从深度图提取几何先验，并提出Geometry-Enhanced Multi-scale Attention（GEMA）模块，通过几何感知的跨尺度注意力机制融合空间信息。

Result: 作者自建九阶段ESD数据集，包含真实手术视频的帧级标注。在该数据集上，Geo-RepNet在复杂低纹理环境下取得了优于现有方法的识别性能，并具备较强的鲁棒性和高计算效率。

Conclusion: 综合RGB和深度信息能显著提升微创手术阶段识别的表现，Geo-RepNet具备广泛的临床和智能辅助手术场景应用潜力。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [43] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: 本文提出了ViT-ProtoNet，将Vision Transformer（ViT）引入Prototypical Network，用于提升小样本图像分类效果，并在多个基准数据集上取得了显著领先。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT具有强大的表达能力，但目前在小样本分类场景中尚未被充分利用，亟需探索其潜力提升小样本学习效果。

Method: 作者将ViT-Small作为主干网络，并嵌入原型网络框架，通过对少量支持样本的类别条件token嵌入进行平均，生成能泛化于新类别的鲁棒原型。设计了多项消融实验，系统考查transformer深度、patch大小和微调方式等因素。

Result: 在Mini-ImageNet、FC100、CUB-200和CIFAR-FS等公开数据集及其鲁棒性变体上，ViT-ProtoNet相较于基于CNN的同类方法，5-shot精度提升最高达3.2%。其在特征空间的可分离性也表现更优，在使用更轻量主干的前提下超过或媲美其他transformer基方法。

Conclusion: ViT-ProtoNet在小样本分类上表现强大且灵活，为基于transformer的元学习方法树立了新基线，并促进了复现性（已开源代码和预训练权重）。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [44] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种新型路径平滑性学习方法DAA*，通过引入路径角度自由度（PAF），提升了模仿学习中路径的相似性和适应性，显著超过了现有Neural A*和TransPath算法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习大多忽视了路径平滑性，使得生成路径可能不自然或与专家演示路径不够相似，因此需要通过新方法提升路径的平滑性与相似性。

Method: 在A*算法中引入路径角度自由度（PAF），探究移动角度对路径节点扩展的影响，在最小与最大角度间权衡，实现自适应路径平滑。DAA*联合最短路径与平滑多目标优化（分别对应启发式距离与PAF），提升与专家路径的相似度。

Result: 在7个数据集上的实验（包括迷宫、电子游戏和无人机现实场景）显示，DAA*在路径相似性指标（SPR、ASIM、PSIM）上相比Neural A*提升9.0%、6.9%、3.9%；对比SOTA算法TransPath提升6.7%、6.5%、3.7%。

Conclusion: DAA*能更好地在路径相似性与最优性之间平衡，生成长度更短、平滑度更高且更接近参考路径的结果，在多种任务和场景下优于现有方法，但在部分情形下会略微降低搜索效率。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [45] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 这篇论文提出了第一个全面的 RGBA 基准数据集 ALPHA，并提出了一种统一的端到端 RGBA VAE（ALPHAVAE），能够有效重建和生成高质量透明图片。相比于之前的方法，显著提升了重建效果。


<details>
  <summary>Details</summary>
Motivation: 虽然隐变量扩散模型在高保真度 RGB 图像合成方面取得了显著进展，但对于透明或分层内容（即 RGBA 图像）的生成研究较少，主要受限于缺乏大规模基准。作者希望填补这一空白。

Method: 1. 提出并公布了 ALPHA 数据集，将标准 RGB 评测指标拓展到 RGBA 图像，并通过 alpha blending 融合到标准背景上进行评测。
2. 推出 ALPHAVAE，将已有 RGB 预训练 VAE 扩展，加入专用的 alpha 通道。模型使用复合损失（包含 alpha 混合重建、patch 级别保真、感知一致性和双 KL 散度），确保 RGB 和 alpha 的潜空间信息都能有效表达和重建。

Result: ALPHAVAE 只用 8000 张图片训练，在重建任务上比 LayerDiffuse（需要 1M 图片）PSNR 提高了 4.9 dB，SSIM 提升 3.2%。微调在扩散模型中后，透明图片生成效果也更优。

Conclusion: 该方法无需极大规模数据即可为 RGBA 图像生成带来高质量提升，为透明内容的生成研究提供了数据集和工具，推动了该方向的发展。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [46] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 该论文提出了ProactiveBench，这是一个用于评估多模态对话系统主动交互能力的基准，及其专用评测指标PAUC，证明了PAUC更贴近用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统的发展，用户不仅期望系统能做被动应答，还希望系统能主动把握时机进行多轮互动，尤其在如视频播放等需要实时反应的场景。但目前缺乏专门用于评估这种主动交互能力的标准和指标。

Method: 作者提出了ProactiveBench，一个综合评测主动交互能力的多模态对话基准，并根据模型响应的时间动态特性，设计了首个可衡量时序主动性表现的评价指标PAUC。通过对多种基线系统的大量实验和用户偏好调查，比较了PAUC与传统文本内容指标的一致性。

Result: 实验结果显示，PAUC指标与人类偏好结果高度一致，优于传统仅基于文本内容的评价方法。PAUC能够准确反映实际用户对主动交互体验的评价。

Conclusion: PAUC是评估主动交互多模态系统的更优评价指标，能够更真实体现用户体验，有助于未来主动对话系统的发展和改进。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [47] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 该论文提出了一种高效且能在航天器上实时运行的神经网络方法，实现对天体表面地形特征的检测与跟踪，提升自主航天任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前空间任务中，对地形特征的检测与跟踪对于TRN、EDL、风险分析和科学数据采集至关重要，但传统方法依赖大量先验成像与离线处理，不仅计算能力受限，还存在泛化性不足、任务周期长和费用高等问题。新兴的学习型计算机视觉虽然增强了自主性，但运算量大，难以在航天器硬件上实时实现，同时缺乏多样训练数据。

Method: 作者提出了两项创新：（1）提出改进的领域自适应方法，通过可廉价获取的训练数据提升地形特征检测能力；（2）提出注意力对齐的特征描述方法，使地标特征在视角大幅变化下依旧保持匹配鲁棒性。两者结合形成高效统一的地标跟踪系统，采用轻量级神经网络可在航天器处理器上实时运行。

Result: 实验表明，该系统在性能上优于现有主流技术，有更好的检测和跟踪表现，且具备实时处理能力。

Conclusion: 论文展示的方法为空间自主任务中的地形特征检测与跟踪提供了有效且可实时落地的方案，有望降低任务成本、提高效率并改善泛化性。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [48] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度类别级对齐的音频-视频编码器DICCAE，提升了模型区分相似类别的能力，并通过自监督预训练应对数据稀缺，性能逼近SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音视频预训练方法只关注模态对齐，缺乏对容易混淆类别的认知归纳和对比，导致模型难以区分相似事件。该文旨在解决类别混淆，提升活动识别能力。

Method: 提出DICCAE编码器，根据类别间混淆度动态调整损失，实现细粒度音视频对齐。还设计了包含音频、视频及其融合的新训练框架，并提出基于聚类引导的自监督预训练策略，缓解音视频数据稀缺。

Result: 在VGGSound数据集上取得65.5% top-1准确率，接近最先进水平。大量消融实验进一步说明各模块设计的必要性和有效性。

Conclusion: DICCAE通过动态调整类别混淆损失，显著提升了音视频活动识别的区分类别能力，并通过自监督扩展了其适用性，整体结果和设计有效。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [49] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D 是一种用于3D点云实例分割的新框架，结合了注意力机制、嵌入学习和跨模态对齐，实现了无监督的实例分割和与自然语言的跨模态语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前的3D点云实例分割方法在泛化性、跨模态语义理解和监督需求方面存在局限。本文旨在提出一种方法，实现更高效的实例分割，并支持与自然语言的无缝对接，便于实际应用和部署。

Method: 提出了SegVec3D框架，包括：1）层次化特征提取器加强几何建模；2）采用对比聚类实现无监督实例分割；3）3D数据与自然语言查询对齐到共享语义空间，实现零样本检索。对比Mask3D、ULIP等方法，创新性在于统一了实例分割与多模态理解。

Result: 实验表明，SegVec3D在实例分割任务和跨模态检索任务上取得了优异的性能，并且在极少监督的条件下具有良好的应用价值。

Conclusion: SegVec3D在最小化监督需求的同时，实现了3D实例分割和多模态语义理解的统一，展现出良好的实际部署潜力。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [50] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Fast3D的新型3D多模态大模型视觉token裁剪框架，能高效加速3D场景理解，同时维持精度。


<details>
  <summary>Details</summary>
Motivation: 3D多模态大模型因需要处理大量对象级视觉token，导致推理效率低下。2D领域token裁剪虽有效，但3D结构差异使其难以直接应用。为此，作者希望解决3D视觉token冗余、提升计算效率的问题。

Method: 提出Fast3D框架，包含两个创新技术：1）全局注意力预测（GAP），用轻量神经网络预测目标模型的全局注意力分布，从而精确评估token的重要性并指导裁剪；2）样本自适应token裁剪（SAP），根据attention复杂度动态调整各层裁剪比例，实现输入自适应。该方法无需修改原模型参数，属于插拔式方案。

Result: 在五个基准数据集上验证，Fast3D在高裁剪比例场景下依然能显著加速推理过程，同时有效维持模型性能。

Conclusion: Fast3D为3D多模态大模型提供高效的视觉token裁剪手段，能广泛提升推理效率且易于集成。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [51] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 本文提出了LifelongPR框架，有效解决了点云定位识别（PCPR）在持续学习过程中遗忘旧知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCPR模型在适应新环境或传感器时存在灾难性遗忘，导致模型在已学场景上的性能大幅下降，影响实际部署的可扩展性和维护。

Method: 提出了两个核心方法：1）基于动态样本分配和空间多样性的重放样本选择机制，以保留关键信息；2）融合轻量级Prompt模块和两阶段训练的持续学习框架，实现特定领域的特征自适应并减少遗忘现象。

Result: 在大规模公开和自采集数据集上的实验结果显示，方法相比SOTA提升了6.50%的mIR@1、7.96%的mR@1，并减少了8.95%的遗忘率（F值）。

Conclusion: LifelongPR显著提升了PCPR系统的持续学习能力和适应性，降低了灾难性遗忘的影响，具备更强的实际部署可行性和推广价值。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [52] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 本文提出了一种架构简单的仅编码器视频ViT方法用于自车视角交通异常检测（TAD），并证明得益于强大的预训练，该方法可超越或匹配复杂的SOTA方法，且更高效。


<details>
  <summary>Details</summary>
Motivation: 当前TAD方法多依赖复杂的多阶段或多表示融合结构，但是否有必要如此复杂尚不清楚。同时，视觉感知领域近期发现，基础模型和高级预训练有时可让简单架构胜过专用设计。

Method: 作者采用只有编码器的Video Vision Transformer（Video ViT）架构，重点研究了不同预训练方式（弱监督、全监督、自监督的掩码视频建模MVM和针对驾驶场景的领域适应预训练DAPT）对TAD性能的影响。

Result: （1）强预训练能让简单ViT模型达到或超越SOTA，效率优势明显；（2）自监督MVM比弱监督及全监督预训练对TAD效果更佳；（3）DAPT在无标注异常的情况下进一步提升性能。

Conclusion: 预训练（尤其是自监督和领域适应）对TAD至关重要，极简架构也可实现高效可扩展的异常检测。相关代码和模型已开源以促进后续研究。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [53] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 本文提出了一套融合多系统的跌倒检测框架，通过多模态协同提升跌倒识别的及时性和准确率，同时兼顾用户隐私保护。整个系统由三个部分组成：基于半监督联邦学习的可穿戴设备跌倒检测、室内定位与导航系统、以及基于视觉的机器人识别。实验证明，整体检测准确率高达99.99%。


<details>
  <summary>Details</summary>
Motivation: 老年人群体快速增长，跌倒导致的伤害和医疗费用显著上升，需要有效、及时且兼顾隐私的跌倒检测方法。传统系统容易产生误报、漏报，或存在隐私风险，因此亟需创新型检测框架。

Method: 创新性地结合三种互补检测方法：1）可穿戴设备和边缘设备联合实现基于半监督联邦学习的跌倒检测（SF2D）；2）采用室内定位技术判定跌倒具体位置，并引导机器人前往检查；3）配备摄像头的机器人再用视觉识别技术检测人员是否跌倒。多系统融合，提升检测可靠性。

Result: SF2D检测准确率达99.19%，视觉检测准确率96.3%，导航成功率95%。三系统结合后，整体跌倒检测的准确率提升至99.99%。

Conclusion: 提出的多系统融合跌倒检测框架大幅提升了检测准确率和及时性，同时充分考虑用户隐私，是老年人跌倒风险场景下的一种高效、可靠且隐私保护的解决方案。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [54] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的作物病害识别系统，利用叶片图像自动检测和分类八种常见病害，融入诊断和用药建议并支持移动端实时应用。


<details>
  <summary>Details</summary>
Motivation: 大规模农业病害早期识别困难且误差较大，影响农作物产量和全球粮食安全，因此亟需自动化、可扩展的病害检测工具。

Method: 采用深度学习完整流程：数据集采集、图像预处理（尺寸调整、归一化、增强）、用TensorFlow和Keras建立含三层卷积和池化的CNN模型，进行训练和测试；最后加设用药推荐模块，并部署于开源移动平台。

Result: 训练集准确率约90%，验证集准确率约60%，显示模型有轻微过拟合，系统可提供针对不同病害的药剂推荐并支持远程实时诊断。

Conclusion: 该系统为精准农业和可持续病害管理提供了一种可扩展且易获取的工具，减少人工检测依赖，提升病害监测的智能化和作物生产韧性。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [55] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种适用于资源有限的小型科研团队的本地相机陷阱数据处理方案，集成了简易的ML/AI能力，以便更高效地分析野生动物监测数据。


<details>
  <summary>Details</summary>
Motivation: 随着相机陷阱应用的普及，野外数据收集量大幅增加，但数据的处理、标注与管理方法，尤其是ML/AI工具的集成与优化，仍旧面临所需算力、专业性强和流程兼容等诸多难题，限制了小团队的实际应用。

Method: 作者设计了一套低资源消耗的数据处理管线，支持在本地环境下实施，并结合了基础的ML/AI技术，无需高昂算力或深度技术背景。流程涵盖数据传输、机器学习推论、评估等环节，尤其注重可迁移性和易用性。

Result: 该管线提升了数据处理效率，降低了专业技术与资源的门槛，使小规模团队能高效管理、分析并从大量的相机陷阱数据中提取有效信息。

Conclusion: 文中低资源本地管线为资源有限研究团队提供了切实可行的解决方案，促进了野生动物监测数据的利用和研究深度，有望推动ML/AI技术更广泛地应用于生态数据分析。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [56] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D多人物体运动预测模型，简化了空间与时间上的人物交互建模，在保持精度的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 多人物体3D运动预测不仅需要考虑个体的历史动作，还要有效建模人物之间的交互，但复杂的交互建模方法常伴随高昂的计算开销。因此，研究如何在保证准确性的同时，提升多人物体运动预测的效率具有重要意义。

Method: 作者设计了轻量级的双分支结构，分别获取个体和多人物体的局部与全局时空特征。此外，提出新颖的跨层交互模块用于融合空间和时间特征，并通过显式地引入空间人物间距离嵌入，进一步优化交互建模。整体方法极大提升了模型的运算效率。

Result: 该模型在CMU-Mocap、MuPoTS-3D和3DPW等标准数据集上，多个评价指标均取得了最新的SOTA（最优）结果，并显著减少了计算资源消耗。

Conclusion: 通过高效的时空结构设计和交互优化，模型达成了准确性与计算效率的双提升，为多人物体3D运动预测任务提供了可扩展、实用的解决方案。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [57] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架CKAA，解决了多任务增量学习中任务识别错误导致决策混淆的问题，并在PEFT基础上进一步提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于PEFT的持续学习方法会为每个任务分配独立子模块，但由于子模块特征空间不对齐，遇到错误任务识别时易产生歧义决策，影响模型鲁棒性。

Method: CKAA框架包含两个关键创新：1）双层知识对齐（DKA），通过对齐不同子空间的同类特征分布及特征模拟训练全局分类器，使模型能区分正确和错误子空间的特征；2）任务置信度引导的适配器混合（TC-MoA），在推理时根据任务置信度自适应聚合相关子模块知识，避免任务识别出错时的过度自信。

Result: 大量实验表明，CKAA在性能上优于现有的基于PEFT的持续学习方法。

Conclusion: CKAA有效提升了PEFT持续学习中的模型鲁棒性与性能，能更好应对任务识别不准的实际问题。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [58] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间下结合遮蔽图像建模（MIM）和知识蒸馏的新方法HMID-Net，能够高效捕捉视觉-语义分层结构，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究如MERU利用多模态学习将视觉语义信息映射到双曲空间并捕捉分层结构，但如何更高效地训练模型以利用这种分层信息仍未解决。

Method: 提出了HMID-Net，将遮蔽图像建模（MIM）和知识蒸馏首次有效结合于双曲空间，并设计了专门的双曲空间蒸馏损失以增强知识迁移效果。

Result: 实验显示，MIM和知识蒸馏在双曲空间中效果与欧氏空间类似出色。在广泛下游任务评估，如图像分类与检索方面，HMID-Net显著优于MERU和CLIP等现有模型。

Conclusion: 方法验证了双曲空间下结合MIM和知识蒸馏的有效性，为视觉语义分层表达和高效模型训练提供了新范式，有望推广到更多视觉与多模态任务。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [59] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出GLIMPSE新基准，专为评估大型视觉-语言模型（LVLMs）是否能进行真正视频级推理而设计，结果显示现有LVLMs尚未达到这一水平。


<details>
  <summary>Details</summary>
Motivation: 目前许多视频理解评测与图像类任务类似，模型可通过查看少量关键帧获取答案，未能真正考察模型的时序推理和完整视频理解能力。因此需要一种新基准来衡量LVLMs对视频的深度理解能力。

Method: 作者构建了GLIMPSE基准，包含3269个视频与4342条高度依赖视觉语境的问题，覆盖如轨迹分析、时序推理、取证检测等11大类。这些问题均需分析整段视频且由人工精心设计，无法依赖单帧或文字回答。

Result: 人工参与下GLIMPSE上达到了94.82%的准确率，但最好的LVLM（GPT-o3）仅取得66.43%，表现出明显差距，说明当前LVLM难以精细地进行视频级理解。

Conclusion: 现有LVLMs仍停留在表层的帧级分析，难以实现真正意义上的视频理解，GLIMPSE基准为评估和推动该领域发展提供了有力工具。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [60] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 本文提出一种自适应张量正则化网络（SDTN）及其扩展的张量正则化网络（TRN），用于提升高光谱图像分类的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类应用于精确农业，如作物健康监测、病害检测和土壤分析。然而，现有方法在处理高维数据、谱-空间冗余以及标注样本稀缺方面表现不足，导致分类性能不佳。

Method: 提出SDTN模型，将张量分解与正则化机制结合，动态调整张量秩，实现特征自适应表达。进一步发展为TRN网络，将SDTN提取的多尺度谱-空间特征整合至轻量级网络，兼顾准确率和计算效率。

Result: 在PaviaU数据集实验中，新方法在分类准确率和模型参数数量上显著优于现有主流方法。

Conclusion: 所提方法为资源受限环境下的高光谱图像实时分类提供了高效解决方案，兼具高精度与低计算复杂度。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [61] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ReTA的可靠测试时自适应方法，通过在缓存生成和决策边界上进行改进，提升了视觉-语言模型在分布变化场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在无标注数据时面对分布变化（分布偏移）下表现下降，为此测试时自适应（TTA）成为关注焦点。现有基于缓存的TTA虽有提升，但在选择缓存样本和决策边界上存在可靠性问题。

Method: 提出可靠测试时自适应（ReTA）。一方面，引入一致性感知熵重加权（CER）改进缓存样本选择，结合预测一致性提升缓存质量。另一方面，提出多样性驱动分布校准（DDC），以高斯分布拟合类别文本嵌入，实现自适应决策边界。

Result: 大量实验证明ReTA在多组真实分布转移任务下，相比最先进方法具备更优表现，验证了方案有效性。

Conclusion: ReTA通过改进缓存选择策略和决策边界建模，显著提升了视觉-语言模型在分布偏移测试场景下的适应能力和可靠性。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [62] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的微手势在线识别方法HFUT-VUT，在IJCAI 2025 MiGA比赛中获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 微手势识别任务相比传统动作检测要求更细致地分类和定位瞬时手势，但现有方法难以区分微手势种类及其边界。

Method: 采用手工设计的数据增强技术和时空注意力机制，提高了模型对手势类别和起止时间的识别和定位能力。

Result: 该方法在F1分数上获得38.03，超过前SOTA37.9%，排名第一。

Conclusion: 所提方法能更准确地识别和定位微手势实例，在该领域取得了领先效果。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [63] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: 论文提出了QuarterMap，一种可以在不重新训练的情况下，通过稀疏空间激活提升SSM模型推理效率的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SSM的视觉主干（如VMamba）虽在计算复杂度上优于Transformer，但其四方向扫描仍存在空间冗余，影响推理速度。因此，亟需一种无需重新训练即可进一步提升推理效率的方案。

Method: QuarterMap是一种训练后激活剪枝方法。该方法在扫描之前剪除冗余的空间激活，然后通过最近邻上采样恢复原始维度，从而提升推理吞吐量。与token merging方法不同，QuarterMap专为SSM设计，避免了昂贵的合并-拆分操作。

Result: 在ImageNet-1K数据集上，QuarterMap使VMamba提速11%，精度损失小于0.9%；在ADE20K分割任务上也有类似提升。在医学图像领域内同样有效，对MedMamba等结构有一致加速且精度保持。

Conclusion: QuarterMap是一个无需重新训练即可提升SSM模型推理效率的即插即用工具，在多个通用及领域特定任务上均表现良好，对SSM尤其适用，具备良好的可迁移性和实用价值。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [64] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于Schrödinger Bridge的无配对去雾方法（DehazeSB），通过最优传输理论加强了去雾效果，结合细节保持和利用CLIP模型的多模态能力，在多个真实数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法受限于生成器的传输映射能力，导致在无配对训练下无法充分发挥去雾效果，因此需要新的理论工具和方法提高去雾的映射效率和质量。

Method: 引入了Schrödinger Bridge理论，利用最优传输（OT）直接在雾图像和清晰图像分布间建立映射，并通过细节保持正则化保证还原图像结构与细节，同时利用提示学习机制，结合CLIP模型的视觉-语言能力区分雾图和清晰图，实现端到端去雾。

Result: 在多个真实世界数据集上的大量实验表明，提出的方法在去雾质量和细节还原方面优于现有主流方法。

Conclusion: 结合最优传输理论、细节保持机制以及多模态学习能力，可以显著提升无配对去雾的效果，为真实世界条件下的图像去雾提供了新的解决思路。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [65] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: 本文提出VDInstruct模型，通过引入内容感知的分词策略，实现对视觉文档（如收据和合同）中关键信息的高效准确提取，显著减少了图像分词数量，并在多个KIE基准上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理结构复杂、内容密集的文档时表现不佳，且图像分词方法随图片尺寸线性增加，造成计算与内存浪费。因此，需要一种更高效且结构感知的KIE方法。

Method: VDInstruct模型将空间区域检测与语义特征提取分离，采用内容感知分词策略——根据文档复杂度调整分词数量，避免无效计算，保持关键信息结构。模型采取三阶段训练流程，并在KIE任务中进行测试。

Result: 相比主流方法，VDInstruct在KIE基准上的准确率达到或超越SOTA表现，分词数量减少约3.6倍；零样本测试中，VDInstruct相较DocOwl 1.5等基线模型F1分数提升5.5点。

Conclusion: 内容感知分词结合显式版面建模能够显著提升文档理解系统效率和准确性，为视觉文档分析提供了有前景的方向。相关数据、代码及模型权重将公开。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [66] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 本文提出了动态鲁棒主成分分析网络（DRPCA-Net），结合了稀疏性先验与动态超网络机制，显著提升了红外小目标检测的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测深度学习方法依赖复杂的堆叠架构，导致可解释性、参数效率及泛化能力下降，同时忽视了小目标稀疏先验。本文旨在通过显式建模稀疏性先验，提升模型的性能和效率。

Method: 作者基于鲁棒主成分分析（RPCA）理论，提出深度展开网络DRPCA-Net，引入动态展开机制：利用轻量级超网络根据输入场景自适应地生成每次迭代的参数。同时设计了动态残差组（DRG）模块，有效建模背景变化并提升低秩估计能力。

Result: 在多个公开红外小目标检测数据集上，DRPCA-Net在检测精度方面显著优于现有主流方法，具备更强的鲁棒性和泛化能力。

Conclusion: DRPCA-Net证实了显式引入稀疏先验和动态参数生成机制能高效提升红外小目标检测性能，为后续研究提供了 interpretable、参数高效且泛化性强的新范式。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [67] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 本文针对红外影像中密集小目标混合难以分离的问题，首次提出了序列密集小目标解混合任务，并发布了数据集和工具包，同时提出DeRefNet网络实现更精确的目标检测。


<details>
  <summary>Details</summary>
Motivation: 红外成像中，受限于镜头焦距与探测器分辨率，远距离密集小目标通常表现为难以分辨的混合斑点。目前尚无高质量数据集和有效方法，可针对复杂场景实现亚像素级别的目标分离和检测，限制了相关研究的进展。作者为推动此领域发展，提出新任务、数据集与方法。

Method: 1. 建立并公开SeqCSIST数据集及工具包，支持序列级密集小目标解混合任务；2. 提出Deformable Refinement Network（DeRefNet），引入Temporal Deformable Feature Alignment（TDFA）模块，实现跨帧自适应信息聚合，提升子像素定位能力；3. 实现并评测了23种相关方法。

Result: 在SeqCSIST数据集上的实验表明，所提DeRefNet方法在mean Average Precision（mAP）指标上比现有最优方法提升了5.3%。相关数据集与代码均已开源。

Conclusion: 本文首次系统性推动了红外密集小目标序列解混合任务的研究，提供了数据、评测工具与强有力的算法，极大促进后续自动化目标检测相关研究与应用。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [68] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的分段式3D手势关键点估计算法EHPE，有效缓解了远端关节误差累积，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部姿态估计方法往往忽视了手指尖端和手腕（TIP和Wrist）在整个关键点定位中的重要性，导致手部远端关节在推理过程中出现较大误差，使姿态估计产生偏差和伪影，整体重建质量下降。

Method: 提出了一种增强型手部姿态估计新结构EHPE，将TIP和Wrist的定位作为独立阶段（TW-stage）提取，获得更准确的初始配置；然后在先验引导下（PG-stage），借助双分支交互网络精细预测剩余关节位置，从而整体提升预测精度。

Result: 在两个主流公开数据集上，EHPE都取得了优于现有方法的手部关节估计精度，实验表明该方法能够有效减小远端关节误差累积问题。

Conclusion: EHPE方法通过针对TIP与Wrist的分段局部提取与整体优化，显著提升了手部姿态估计的准确度和鲁棒性，为视觉手势识别等下游应用奠定了基础。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [69] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: 提出MENTOR方法，实现多模态条件下自回归图像生成，解决现有模型难以精准控制和高效多模态训练的问题，并在多个基准上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成虽然质量提升明显，但对多模态输入的精密控制和有效融合仍存在挑战，通常需依赖大规模训练资源和复杂结构。为解决多模态对齐难、控制力弱和训练成本高等问题，提出新方法。

Method: MENTOR是一种自回归多模态图像生成框架，核心是将自回归图像生成器与两阶段训练相结合，无需额外适配器或交叉注意力模块：1）第一阶段实现像素和语义级别的多模态对齐，2）第二阶段通过多模态指令微调提升模型的控制力和多模态融合能力。

Result: 即便在模型规模有限和资源不足条件下，MENTOR在DreamBench++基准等任务上的概念保持和指令遵循表现优异，图像重建质量好，适应多任务能力强，训练效率超过扩散模型，还优于主流对比方法。

Conclusion: MENTOR有效提升了多模态自回归图像生成的精度、控制力和效率，并降低资源需求。该方法在现有条件下具有较高的实用性和推广价值。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [70] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 该论文系统综述了SAM模型及其变体中提示工程（prompt engineering）技术的研究进展，详述了方法、应用与挑战，并为该领域后续研究提供框架与方向。


<details>
  <summary>Details</summary>
Motivation: SAM模型通过提示生成方式在分割领域取得突破，但提示工程策略的系统探讨仍十分缺乏。因此，迫切需要一份关于提示工程在SAM及其扩展上的全面综述，以总结经验并指引后续发展。

Method: 作者对当前关于SAM及其提示工程的文献进行了系统梳理，涵盖了基础方法、实际应用（如医学图像和遥感）及其面临的挑战。此外，对从简单几何输入到复杂多模态提示的提示工程演化进行了归纳和分析。

Result: 论文揭示了SAM提示工程方法的发展脉络，总结了优化提示相关的主要难点，并指明了该领域的研究热点和未来发展方向。

Conclusion: 本文为理解和推进分割领域基础模型的提示工程提供了结构化框架和全面视角，为今后学术界和工业界在这一新兴方向上的进一步探索与创新奠定了基础。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [71] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 本文提出了一种视频-文本交错的推理范式ViTCoT，并构建了相应的数据集ViTIB，实验证明这种范式比传统的纯文本推理表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的视频推理方法主要依赖文本，忽略了人类推理中对视觉内容的反复回看，导致推理过程和人类不符。为此，作者引入更符合认知的视频-文本交错推理方式。

Method: 提出Video-Text Interleaved CoT（ViTCoT）推理范式，实现视频信息和文本推理过程的交替结合；构建了ViTIB基准数据集，该集通过多模态大模型筛选关键视频并人工校验；设计实验评估ViTCoT在视频理解任务中的效果。

Result: 实验结果显示，ViTCoT方法在视频理解任务上较传统纯文本CoT方法有显著性能提升，并能够有效激活多模态大模型更多的神经元值。

Conclusion: 引入视觉信息至推理过程能够提升视频理解表现，ViTCoT为视频推理提供了更符合人类认知和更有效的范式，对多模态人工智能发展具有积极意义。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [72] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个结合扩散模型和大语言模型的互动式艺术字体设计系统，实现了高质量、可交互、多语言、多区域风格化字体生成，极大提升了用户创作体验和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有艺术字体生成方法要么依赖手工设计，要么生成模型缺乏深入的交互能力，如局部编辑、迭代润色、多字符组合以及复杂提示词理解等，无法满足高级创作需求。

Method: WordCraft系统集成了训练无关的区域注意力机制，实现了精细的多区域生成，并通过噪声融合支持连贯的反复润色而不损失画质。同时引入大语言模型理解用户抽象及具体提示，结构化生成指令。

Result: 系统能够生成高质量、风格多样、支持多字符和多语种的艺术字体，使用者可以进行局部修改、持续优化和复杂提示交互，适应多样化设计流程。

Conclusion: WordCraft极大提升了艺术字体自动生成的交互性和灵活性，为艺术家和设计师提供了更强大便捷的创作工具，拓宽了创意表达的可能性。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [73] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 本文对主流的视觉-语言模型（VLMs）在“bouba-kiki效应”上的能力进行了系统评估，发现其整合跨模态信息的方式与人类认知存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态模型发展迅速，但这种模型在处理视觉和语言信息时，是否真的模拟了人类的认知整合方式仍存疑。'bouba-kiki效应'被认为是此类研究的经典范例，然而此前关于VLMs能否展现出该效应的结论并不一致，因此作者希望对两个典型VLM（CLIP的ResNet和ViT版本）进行更为全面且类比人类实验的验证。

Method: 作者采用两种紧密借鉴人类实验的方法：1）基于prompt的评估，用概率表征模型偏好；2）创新性地利用Grad-CAM技术解释模型在形状-词语匹配任务中的视觉注意。对ResNet和ViT两种CLIP模型进行了详尽测试。

Result: 实验结果显示，CLIP的ResNet和ViT模型并未一致地表现出bouba-kiki效应。虽然ResNet对圆形有一定偏好，但总体上两种模型对于形状与词语的关联远没有达到人类的稳健、一体化表现。与同一任务下的人类数据直接对比也证实了这一差距。

Conclusion: 当前主流VLMs对视觉与语言的跨模态理解能力有限，尚未达到人类模态整合的深度和一致性。这为VLMs内在表征的合理性和与人类直觉的契合度提出了质疑，突显了其进一步优化的必要性。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [74] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本论文提出了FaceLLM，一种专门针对人脸图像理解的多模态大语言模型，并通过生成高质量问答对的数据集FairFaceGPT显著提升了各类人脸相关任务的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的多模态大语言模型主要基于通用数据集训练，无法很好地应对如人脸结构、表情、情感等领域特定视觉线索的理解需求。这导致MLLMs在人脸图像分析等关键任务上的表现和应用受限，因此亟需面向人脸领域的大规模高质量数据和专门的模型。

Method: 本文创新性地提出了一个弱监督的数据构建流程，利用ChatGPT与属性感知型提示词，对FairFace数据集中的人脸图像自动生成涵盖表情、姿势、皮肤质感、取证信息等属性的问题和答案，构建了FairFaceGPT数据集，并在此基础上训练了专注于人脸理解的FaceLLM模型。

Result: FaceLLM在多项人脸相关任务上提升了MLLMs的表现，并达到了目前最优的性能水平，显示了该模型在领域专用视觉理解上的有效性。

Conclusion: 通过利用语言大模型的合成监督，可以高效构建专业领域的多模态模型。该研究不仅丰富了人脸理解的数据和模型资源，也为可信赖且以人为本的多模态AI系统建设提供了新途径。FairFaceGPT数据集和FaceLLM模型已公开发布。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [75] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: 该论文提出了MA-SAM2方法，解决了SAM2在复杂手术视频分割中的内存与遮挡问题，并在无需训练和增加参数的情况下大幅提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的SAM2模型虽然在手术视频分割领域取得了进步，但由于其贪婪的选择记忆设计，难以应对手术视频中器械快速运动、频繁遮挡和复杂交互的问题，导致在复杂长视频中的分割性能下降。

Method: 提出Memory Augmented（MA）-SAM2，这是一种训练自由、带有上下文感知和强遮挡鲁棒性的内存模块，通过多目标单环路单提示推理方式提升多器械场景分割与跟踪效率。

Result: 在无需新增参数或额外训练的前提下，MA-SAM2在EndoVis2017和EndoVis2018数据集上对比SAM2性能提升了4.36%和6.1%。

Conclusion: MA-SAM2方法显著增强了手术视频分割的鲁棒性和准确性，具有较高的临床实际应用潜力。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [76] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度卷积神经网络的手写天城文字符识别方法，在公开数据集DHCD上取得了较高的识别精度。


<details>
  <summary>Details</summary>
Motivation: 天城文（Devanagari）是印度的重要书写系统，但当前缺乏有效的手写字符数字化工具。鉴于手写字符识别在搜索引擎、社交媒体和推荐系统等领域的应用前景广阔，研究者迫切需要开发高效的识别方法以提升数字化水平和处理效率。

Method: 作者采用包含两个深度卷积层的神经网络，对公开的Devanagari handwritten character dataset（DHCD）进行训练和测试，该数据集包含36类字符，每类1700张图片。通过优化神经网络结构以提升识别准确率。

Result: 该方法获得了96.36%的测试准确率和99.55%的训练准确率，显示出在该识别任务上具有很好的性能。

Conclusion: 提出的基于深度卷积神经网络的自动识别方法能有效识别天城文手写字符，为该脚本的数字化提供了可靠的工具，有助于推动相关实际应用的发展。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [77] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: 本文通过逆向工程分析公开源码，揭示和总结了FLUX.1文本到图像生成模型的架构与特点。


<details>
  <summary>Details</summary>
Motivation: FLUX作为新一代文本到图像生成模型，被广泛认可为领域内的先进技术，但缺乏官方的技术文档。为便于研究者采纳和改进，亟需搞清楚其具体架构与实现细节。

Method: 作者通过对FLUX.1开源代码进行逆向工程，系统分析模型结构、训练方式及与其他主流模型的对比。

Result: 报告详细总结了FLUX.1的架构要素、推理机制及其在文本-图像一致性和图片质量、多样性上的突出表现；对比发现其性能优于Midjourney、DALL-E 3、Stable Diffusion 3等主流模型。

Conclusion: 本文为社区提供了FLUX.1架构非官方全面解读，有助于推动后继研究与实际应用，但并非原开发团队正式发布。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [78] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 研究团队提出了CrisisLandMark这一大规模多传感器、结构化遥感影像与文本注释数据集，并开发了CLOSP框架，使多模态图像检索系统能有效结合SAR和多光谱影像信息，实现对遥感影像库更高效准确的文本检索。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于文本的遥感影像检索系统只利用RGB数据，忽略了合成孔径雷达（SAR）等其他传感器提供的独特物理信息，导致难以满足灾害响应和长期气候监测等场景需求。

Method: 1）构建CrisisLandMark，包含64.7万对Sentinel-1 SAR与Sentinel-2多光谱影像及结构化文本注释；2）提出CLOSP，通过对比学习框架以文本为桥梁，将未配对的光学与SAR图像嵌入到统一特征空间；3）进一步提出GeoCLOSP，在模型中引入地理坐标以提升对与地理位置密切相关的危机事件与地理特征的检索能力。

Result: CLOSP达到新的最优效果，提升检索nDGC（归一化累积增益）54%；联合训练策略使模型能将光学影像的语义知识间接迁移至SAR影像，提高对SAR难以解读内容的理解能力。GeoCLOSP可在通用语义检索和位置特定危机事件检索之间达成有力平衡。

Conclusion: 融合多源遥感影像与地理上下文能显著提升遥感大数据的信息检索效率和能力，为灾害响应、特定地理事件监测等应用释放更大潜力。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [79] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种高效的交互式分割方法Inter2Former，在保持高精度的同时显著提升了CPU上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有交互式分割方法在CPU上难以兼顾高精细度（准确性高、细节还原好）与推理效率，尤其是密集token方案在主流设备上速度难以接受，而快速度的稀疏token方案（如SAM）表现又不够精细。需要创新的方法在不损失精度的前提下，提升CPU推理效率。

Method: 提出Inter2Former，通过以下创新提升性能：1）动态提示嵌入（DPE）仅在兴趣区域处理，减少背景token计算；2）动态混合注意力（DHA）使用前一帧分割掩模区分边界区域和非边界区域，边界用全注意力，其余用高效注意力机制；3）混合专家网络（HMoE），在FFN中进行类似的自适应计算并针对CPU优化并行；4）动态局部上采样（DLU）在目标区域用轻量级MLP精细上采样。

Result: 在高精度交互式分割基准测试上，Inter2Former在保证精度的同时，大幅度提升了CPU上的计算效率，达到了当前最优水平（SOTA）。

Conclusion: Inter2Former通过动态计算分配和针对关键区域的处理，成功实现了交互式分割任务在CPU设备上的高效与高精度，可广泛应用于实际场景。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [80] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了EmRACE-3K数据集，用于评测和提升视觉-语言模型（VLMs）在主动交互环境中的表现，并基准测试发现主流VLMs在该数据集上的表现较弱，经过进一步训练后可显著提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 虽然当前VLMs在离线图像和视频理解任务中表现优异，但在需要主动交互和场景理解的具身环境（如第一人称任务）中表现有限，尤其在空间推理和长时跨度任务上存在不足，因此需要专用的数据集和基准评测来推动该领域发展。

Method: 作者构建了EmRACE-3K数据集，包含3000多个基于虚幻引擎与UnrealCV-Zoo生成的写实环境中的语言引导任务，任务涵盖导航、物体操作和多阶段目标执行，每个任务都配有第一人称视角视觉观测、高阶指令、动作和自然语言解释。并基于此基准评测主流VLMs的具身推理能力，并尝试对Qwen2.5-VL-7B模型进行有监督和强化学习微调。

Result: 在零样本评测下，所有现有主流VLMs（如GPT-4o、Claude 3.5 Sonnet、Gemini 2.5 Pro等）在EmRACE-3K任务中的成功率均低于20%，证明任务难度和模型不足。对Qwen2.5-VL-7B进行微调后，三大挑战类别下模型表现均有显著提升。

Conclusion: EmRACE-3K数据集为评测和提升VLMs在具身推理任务中的表现提供了全新基准，有助于开发更强大的具身智能系统，微调实验显示该数据集对训练具身推理能力具有明显促进作用。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [81] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了FAIR方法，通过动态对齐图像局部特征与描述性文本嵌入，有效提升了视觉-语言模型（VLM）在无监督细粒度分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM无监督自适应方法要么依赖于静态的对齐分数，难以捕捉细微类别差异，要么需要高昂计算成本的伪标签策略，影响了大规模应用。因此，需要一种能动态建模图像与文本之间细致交互、提升伪标签质量的方法。

Method: 作者提出FAIR方法，设计了Class Description Anchors（CDA）动态地将图像局部特征与文本嵌入对齐，并据此构建自适应分类器（Learned Alignment Score, LAS），改善了交叉模态自训练。此外引入伪标签加权机制，以解决类别间的模糊问题。

Result: 在13个细粒度数据集上，FAIR方法比最先进方法平均提升了2.78%的性能，显著优于当前主流无监督自适应方法。

Conclusion: FAIR通过加强模态间交互、动态对齐和伪标签优化，为VLM在无监督细粒度分类任务提供了更优的适应策略，有效推动了该领域的发展。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [82] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 该论文提出了一种用于工业制造异常检测的高质量异常样本生成框架GAA，以提升异常检测的样本多样性和真实性，从而显著增强定位和分类等任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法受限于异常样本稀缺，而采用传统的数据增强合成异常样本方式面临真实感低、掩膜配准差、泛化性弱等诸多问题，因此亟需开发一种能够生成高质量、语义对齐异常样本和掩膜的新方法。

Method: 作者提出了Generate Aligned Anomaly (GAA)方法，该方法基于预训练潜在扩散模型，只需少量样本即可生成真实、多样且语义对齐的异常及其掩膜。其主要流程包括：利用局部概念分解联合建模异常的语义和空间特征，借助自适应多轮聚类实现异常语义细粒度划分，通过区域引导式掩膜生成确保异常与掩膜精确对齐，并引入低质量样本过滤模块提升样本整体质量。

Result: GAA方法在MVTec AD和LOCO等数据集上的大量实验证明，无论在异常样本生成质量还是异常定位和分类等下游任务中都优于现有方法。

Conclusion: GAA能够有效缓解异常样本稀缺问题，提升异常检测模型对定位和分类等任务的泛化能力和精度，是工业异常检测领域的有效解决途径。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [83] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 本研究提出了一种基于人工智能的多分类中风诊断框架，利用CT图像将中风分为缺血性、出血性和无中风三类。该方法采用了先进的Vision Transformer（MaxViT）模型，并通过数据增强技术提升模型表现。实验结果显示MaxViT模型准确率与F1分数均达98%。为提升模型透明度，引入了解释性人工智能（Grad-CAM++）以辅助临床应用。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死亡原因之一，早期且准确的诊断能显著改善患者预后。现有CT辅助诊断方法存在准确性和可解释性等问题。因此，需开发一种高效、可信且可解释的AI辅助诊断工具，提升急诊早期中风鉴别能力。

Method: 以土耳其卫生部提供的CT图像数据集为基础，采用深度学习中的Vision Transformer（MaxViT）及其变体进行中风分类，并使用数据增强（含合成图像生成）缓解类别不平衡。此外，应用Grad-CAM++进行模型可解释性可视化，提升临床信任度。

Result: 采用数据增强的MaxViT模型在多类别中风分类中表现最好，准确率与F1分数均达到98%，优于其他模型和基线方法。融合解释性AI后，模型可对CT扫描结果高亮显示相关病灶区域。

Conclusion: 提出的AI框架在中风早期分类中高效、准确且可解释，有望作为临床诊断工具，助力急诊快速决策，提升中风治疗及时性，具有实际的临床应用价值。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [84] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 本研究评估了三种图像训练模型（ConvNeXt V2、DINOv2 和 Swin V2）在糖尿病视网膜病变（DR）预测中的公平性与性能，并探讨了解纠偏技术对模型偏差的缓解效果。结果显示，各模型DR预测表现优异，但在敏感属性亚群体间存在一定公平性差异。解纠偏技术对不同模型的效果不一，提示医学影像AI公平性需进一步关注与优化。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致工作年龄成人视力丧失的主要原因，而传统影像筛查手段受限于成本与可及性。尽管AI模型为诊断提供了可扩展的解决方案，但其在实际应用中面临公平性和泛化能力的质疑，亟需评估不同模型在多样化人群中的表现和偏差缓解方法的有效性。

Method: 作者利用多样性较强的mBRSET眼底图像数据集，分别训练了ConvNeXt V2、DINOv2 和 Swin V2三种模型，预测DR及敏感属性（如年龄、性别）。通过在敏感属性子群体间比较模型AUC等性能指标，评估模型公平性。同时对模型采用解纠偏技术，将敏感属性与DR预测解耦，观察其对性能与公平性的影响。

Result: 所有模型在DR预测上达到较高表现（最高94% AUROC），对年龄和性别预测能力也较强（91%和77% AUROC）。但在敏感属性各亚群体间存在AUROC表现差异，如DINOv2模型中不同年龄组预测效果相差10%。应用解纠偏策略后，DINOv2性能提升（AUROC提升约2%），但ConvNeXt V2和Swin V2性能下降（分别降低7%、3%）。

Conclusion: 对敏感属性解纠偏在不同模型上效果不一，显示医学影像AI在实现公平性时仍具挑战。研究强调在实际医疗AI应用中，需关注模型公平性与可靠性，以确保医疗服务的公平性和普惠性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [85] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了EyeSeg，一个专为AR/VR等场景下的眼部分割设计的新方法，通过引入不确定性感知实现对运动模糊、眼睑遮挡、域间差异等问题的鲁棒性提升，并在公开数据库上取得了优于现有方法的分割和注视估计表现。


<details>
  <summary>Details</summary>
Motivation: 现有的AR/VR人机交互依赖于准确的注视估计，而高质量的眼部分割是关键环节。现实中常遇到运动模糊、眼睑遮挡、和源/目标域差异等问题，严重影响分割和注视估计的性能，亟需一种对这些因素鲁棒的新方法。

Method: 提出EyeSeg框架，核心创新在于将贝叶斯不确定性学习应用于眼部分割，通过对后验分布的建模，量化并输出分割不确定性。模型不仅输出分割结果，还给出不确定度分数，并通过加权融合多个注视估计结果，提升整体鲁棒性。理论部分证明后验统计量能作为不确定性指标。

Result: EyeSeg在运动模糊、眼睑遮挡和跨域场景下均表现出更强的鲁棒性，在注视估计等下游任务中比现有方法取得更优表现。分割指标（如MIoU、E1、F1、ACC）有明显提升。

Conclusion: EyeSeg能够有效地提升AI系统在AR/VR场景下的眼部分割和注视估计性能，尤其适用于易受干扰的实际应用环境。其不确定性感知机制为后续AR/VR的人机交互系统提供了更高的可靠性。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [86] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: 本文提出了VST-Pose，用于通过WiFi信号实现准确、连续的人体姿态估计，具有强大的隐私保护和穿透能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: WiFi信号具有穿透性和隐私优势，相较于视频方法，适用于需要隐私保护的室内环境（如智能家居护理），因此希望利用WiFi替代视觉手段实现人体姿态估计。

Method: 提出一种新型深度学习框架VST-Pose：核心为ViSTA-Former时空注意力骨干网，采用双流架构分别捕捉关节时序依赖与结构关系；增加速度建模分支以提升对微小运动的敏感性和细粒度动作表征能力；并构建专用数据集用于验证。

Result: 在自建的智能家居场景2D姿态数据集上，方法在PCK@50指标下达到了92.2%准确率，比现有方法提升了8.3%；在公开MMFi数据集上也验证了其在3D姿态估计上的鲁棒性和有效性。

Conclusion: 本方法为室内环境下的连续、隐私感知的人体运动分析提供了可靠解决方案，技术具有很强的实际应用前景。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [87] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 本论文提出了一种新的高分辨率数字高程模型（DEM）估算框架，通过结合低分辨率SRTM数据和高分辨率RGB影像，利用提示式单目深度估算和视觉Transformer模型，实现了分辨率跨越式提高，并在多个场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的DEM高分辨率估算方法存在局限性——超分辨率方法受限于放大倍数，单目深度估算缺乏全球高程基准，导致DEM拼接受限。因此亟需一种能结合低分辨率全局高程信息与高分辨率相对深度的创新方法。

Method: 本方法以SRTM低分辨率高程为提示（prompt），结合NAIP高分辨率RGB影像，基于视觉Transformer编码器，并通过LiDAR DEM进行微调，采用多样化提示策略，支持DEM估算、空缺填补及动态更新任务。

Result: 所提出框架实现了从30米到30厘米（100倍）的分辨率提升，明显优于以往方法；在美国3个不同地区测试，较SRTM提升高达18%，与激光雷达DEM相比，平均误差不足5米，并能有效捕捉城市结构与细小地貌特征。

Conclusion: 该框架可扩展至大范围区域，适用于灾害与环境研究，具有很强的泛化能力和实际应用价值。所有代码和预训练模型已公开。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [88] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 本论文提出自动化生成多学科科学实验讲解的任务，并发布相关数据集与模型，显著提升了讲解生成效果。


<details>
  <summary>Details</summary>
Motivation: 实验讲解对于描述实验过程、解析科学原理及安全指导非常重要，但目前主要依赖教师的专业知识且耗时高，因此有自动化需求。

Method: 作者构建了ExpInstruct数据集，涵盖21门学科共7000余条分步讲解，并提出了检索增强型自动实验讲解生成模型ExpStar，可以自适应地调用和应用外部知识。

Result: ExpStar在广泛实验中明显优于14个主流大规模多模态模型，验证了所提出数据集和模型的优越性。

Conclusion: ExpStar具有推动AI辅助科学实验教学的巨大潜力。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [89] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文系统梳理和比较了ViT模型中的token压缩方法，并首次评估了这些方法在标准和紧凑型ViT架构上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前token压缩方法多用于提升ViT推理效率，但缺乏系统性分类和比较，且大都只在标准ViT上测试，忽视了实际部署中常见的结构压缩模型。

Method: 提出了第一个token压缩方法的系统化分类体系，并选取代表性方法，在标准与紧凑型ViT模型上进行了对比实验。

Result: 实验显示，token压缩方法在标准ViT上效果良好，但直接应用于紧凑型ViT时性能下降。

Conclusion: 研究揭示了token压缩在实际紧凑型模型上的局限，指出未来需针对边缘AI和小型模型设计专门的token优化技术。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [90] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文提出了一种改进的基于分数蒸馏的Text-to-3D生成方法，称为线性化前瞻变分分数蒸馏（$L^2$-VSD），有效提升了生成质量和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数蒸馏的Text-to-3D生成方法（如VSD）在理论上合理，但实际训练中存在收敛慢、不稳定等问题，主要源于分数模型和3D模型分布不匹配。

Method: 作者对分数模型和3D模型的相互作用进行了深入分析，发现可以通过调整优化顺序让分数模型更好地修正梯度，实现前瞻式训练。为缓解前瞻式分数蒸馏的训练不稳定问题，进一步提出了线性化变体（$L^2$-VSD），利用正向自动微分方法高效实现。

Result: 大量实验验证了$L^2$-VSD的方法有效性，生成质量和稳定性均优于以往方法。

Conclusion: $L^2$-VSD方法不仅提升了文本到3D生成的性能，还能无缝集成到其他VSD类的Text-to-3D框架中，是分数蒸馏方向的重要进展。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [91] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本论文提出了一种结合几何和图像特征的碎片配对兼容性计算方法，无需对碎片形状、尺寸或内容作出假设，并在考古碎片还原任务中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数碎片还原算法核心在于配对兼容性计算，但现有方法难以处理真实情况下多样、复杂和侵蚀后的碎片形状。研究者们希望提出能够适用于任何几何和图像特征的通用兼容性方法。

Method: 作者提出了一种高效的混合兼容性计算方法，融合了几何与图像（图像内容）特征。同时，构建了新的碎片图像数据集，该数据集通过创新的图像碎片化和侵蚀模型生成，更贴近真实考古情景，并提供兼容性评价指标。最后将所提方法嵌入考古拼图框架中进行实验考察。

Result: 在RePAIR 2D数据集测试结果表明，所提出的方法在碎片邻近关系预测的精度与召回率上达到了最新最优水平，直接证明了兼容性计算能力的提升。

Conclusion: 该研究提出的无形状假设、结合几何与图像的碎片兼容性计算方法适用于复杂真实场景，在考古碎片还原等实际任务中具有很好的应用前景和效果。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [92] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: 本文提出NegRefine方法，通过精炼负标签集和动态打分机制，提高了基于视觉-语言模型的零样本OOD检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于负标签的视觉-语言零样本OOD（分布外）检测方法，如NegLabel和CSP，虽有效但经常将分布内样本误判为OOD，尤其是负标签为细分类或专有名词，且难以处理图像与多个标签同时匹配的情况。

Method: NegRefine引入两个核心创新：一是过滤机制，自动排除负标签集中的细分类和专有名词；二是多匹配感知打分函数，能动态调节一个图像命中多个标签时每个标签所占的贡献，从而提升标签判别的鲁棒性。

Result: 在如ImageNet-1K等大规模基准上，NegRefine实现了比现有技术更鲁棒的分布内与OOD样本分离能力，误判率得到有效改善。

Conclusion: NegRefine通过优化负标签集和打分机制，有效提升了视觉-语言模型零样本OOD检测的表现，对相关任务具有重要实际与理论意义。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [93] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: 该论文提出了一个名为VRU-Accident的大规模视觉语言基准，用于评估多模态大语言模型（MLLM）在涉及易受伤道路使用者（如行人和骑行者）的复杂交通安全场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在保障易受伤道路使用者安全方面面临挑战，现有多模态大语言模型虽有应用潜力，但缺乏标准化评估其在高风险交通情境中的推理和判断能力的基准数据。

Method: 作者构建了VRU-Accident基准，包括1000个真实行车记录仪的事故视频、6000个多选问答对（横跨六类安全关键类别）、24000个候选答案和3400个独特答案、以及1000个密集场景描述。该基准提供了细粒度的时空和因果语义注释，专注于易受伤道路使用者与车辆事故的复杂性。作者对17个主流MLLM模型在多选VQA和密集描述任务上进行了系统评测。

Result: 实验结果显示，现有MLLM在图像属性识别上表现尚可，但在事故原因、类型和可预防性等推理描述方面存在显著困难。

Conclusion: MLLM虽能处理基础视觉信息，但其在安全关键交通场景中的复杂推理能力仍待提升。VRU-Accident基准为相关模型迭代和行业评测提供了参考标准。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [94] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 本文对比了人类和两种深度学习模型在稀疏3D点云物体识别上的表现，发现视觉Transformer模型在模拟人类表现上优于CNN模型，主要归因于其支持3D形状分层抽象的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型能在人类级别上识别3D点云物体，但尚不清楚这些模型是否形成了类似人类视觉的3D形状表征。作者希望探究不同模型在局部和整体3D形状表征方面与人类的异同。

Method: 作者设计了两个人体实验，系统操纵了点密度、物体方向和局部几何结构，对比分析了两种3D点云识别模型（DGCNN和point transformer）与人类的表现。

Result: 人类在所有条件下表现稳定。与人类表现的拟合度上，视觉Transformer模型优于基于卷积的DGCNN，其优势来源于支持3D形状分层抽象的机制。

Conclusion: 视觉Transformer（point transformer）模型更好地模拟了人类在3D点云识别任务中的表现，尤其在对3D形状进行层级抽象方面，相较于传统卷积模型具有明显优势。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [95] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本文综述了多模态大模型（MLLM）在视觉丰富文档理解（VRDU）领域的最新进展，系统总结了特征融合方法、训练范式与关键数据集，并指明领域挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动处理包含复杂视觉、文本和布局信息的文档需求增长，如何高效理解和提取此类文档的信息成为一大挑战，推动了VRDU这一领域的发展。

Method: 本文围绕三大核心：（1）对文本、视觉及布局特征的编码与融合方式进行综述；（2）梳理了包括预训练、指令-响应调优及不同模块可训练性等训练范式；（3）总结了预训练、指令调优和有监督微调常用的数据集。

Result: 通过系统梳理VRDU中MLLM的研究进展，明确了目前主流方法、训练策略和数据资源，帮助归纳当前主流技术路线及其优缺点。

Conclusion: MLLM为VRDU带来了新的机遇，提高了文档理解的效率与准确性。尽管已取得进展，但在效率、泛化能力和鲁棒性等方面仍有挑战，未来应继续推动技术演进和实际应用落地。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [96] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 本文介绍了SpeakerVid-5M数据集，这是首个面向音视听互动虚拟人生成的大规模高质量数据集，共包含8743小时、520万段虚拟人影像，用于推动学术界相关研究。


<details>
  <summary>Details</summary>
Motivation: 随着大模型赋能下数字人领域进展迅速，音视听互动虚拟人成为下一个学术热点，但缺乏配套的大规模高质量数据集限制了进一步发展。

Method: 构建了包含多种互动类型、数据分层明确（大规模预训练子集与高质量精标子集）的SpeakerVid-5M数据集。其类别按互动情形细分，并为2D虚拟人任务广泛适用。此外，基于该数据集训练了自回归视频聊天基线模型，并提供了配套评测基准VidChatBench。

Result: 建立并公开了SpeakerVid-5M数据集、配套数据处理代码、基线模型与测试基准，有效填补了领域空白，为后续研究提供了统一资源和基准。

Conclusion: SpeakerVid-5M为音视听互动虚拟人研究提供了首个大规模、高质量数据基础，将显著促进相关技术迭代和学术发展。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [97] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了首个4D人体分析框架，既可大幅加快推理速度，又支持开放词汇解析能力，突破了以往方法仅能处理固定类别且速度慢的局限。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实与扩展现实应用需要高效且灵活的3D动态人体解析。但现有方法不仅依赖封闭数据集，类别受限，且推理速度慢，难以满足实际需求。

Method: 以现有最优的开放词汇3D人体解析技术为基础，提出三大创新：1）利用基于mask的视频目标跟踪，提升空间和时间对应效率，免去逐帧分割；2）设计新颖的Mask Validation模块，识别新目标并减小跟踪失败影响；3）提出4D Mask Fusion模块，通过带记忆的注意力和logits均衡，实现鲁棒的嵌入融合。

Result: 在多个4D人体解析任务上开展大量实验，新方法相较于之前只支持固定类别的方法，推理速度提升高达93.3%。

Conclusion: 该方法不仅大幅提升了解析速度，还具备处理开放词汇类别的能力，为4D人体视频解析带来更高实用性和灵活性。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [98] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: 本文提出一种新的反事实视觉解释框架CECAS，将因果推理引入反事实生成过程，有效避免了以往方法中对虚假相关因素的干扰，并在多项基准数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前的反事实视觉解释方法通过对图片进行视觉扰动以改变模型预测，提升模型可解释性。但这些方法忽略了图像生成过程中的因果关系和虚假相关，经常导致生成的反事实图像包含非目标因素的无意更改，降低了解释质量。因此，迫切需要结合因果信息以提升反事实解释的准确性和有效性。

Method: 提出了CECAS框架，采用因果指导的对抗生成方法，在生成反事实图像时明确利用因果关系，控制扰动仅作用于与任务相关的因果因素，避免影响到虚假的或无关因素。通过整合因果视角，减少反事实图像中的不期望变化，并提升解释结果的有效性。

Result: 在多个基准数据集上的大量实验结果表明，CECAS在有效性、稀疏性、接近性和真实感等多个评估维度上均优于现有的主流反事实视觉解释方法。

Conclusion: 本文方法通过融合因果推理与对抗生成，有效提升了反事实视觉解释的质量，实现了解释的准确性、简明性与真实感的平衡，为AI模型的可解释性研究提供了新的方向。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [99] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: 提出了一种两阶段的高光谱图像重建方法MCGA，从RGB图像有效重建高光谱图像，并达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 直接利用复杂注意力机制进行RGB与HSI映射的方法未能有效解决低维到高维信息过渡的本质难题，导致重建质量受限。

Method: 该方法分两阶段：第一阶段利用多尺度的VQ-VAE从异构HSI数据集中学习光谱模式，提取混合码本（MoC）；第二阶段通过从MoC中查询特征，替换隐空间HSI表征，从而精细化RGB到HSI映射，并引入先验知识。还提出了黑白感知注意力和量化自注意力机制以自适应调整特征图，实现高效的高光谱重建。此外，用基于熵的测试时自适应策略提升方法的鲁棒性。

Result: 在多个数据集上进行了大量实验，MCGA取得了当前最优的高光谱图像重建结果。

Conclusion: MCGA结合谱先验与高效注意力机制，有效克服了直接低维到高维映射的挑战，实现了轻量、强鲁棒性和高精度高光谱图像重建。代码和模型即将开源。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [100] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 本文旨在解决遥感影像中任意方向目标的旋转等变问题，提出了一种新颖的多分支头部严格旋转等变检测器MessDet，在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的目标通常呈现任意朝向，现有方法大多通过数据增强或近似的旋转等变网络学习旋转不变特征，但严格旋转等变对于检测性能的影响及其必要性还不清楚。本文希望定量比较严格与近似旋转等变带来的检测效果差异。

Method: 作者设计了一个包含严格旋转等变的主干和Neck网络，并与近似旋转等变结构做对比。利用旋转等变特征的分组特性，提出了轻量化多分支检测头结构，降低参数数量同时提升精度，整体方法被称为MessDet。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R等遥感检测数据集上，MessDet以远低于以往方法的参数量实现了最新最优性能。

Conclusion: 严格旋转等变网络和创新的分支头结构可以显著提升遥感图像检测的准确率与效率，为相关领域提供了新思路。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [101] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为Instructional Graphic Designer (IGD)的系统，该系统能根据自然语言指令生成具有可编辑多模态分层的图形设计文件。


<details>
  <summary>Details</summary>
Motivation: 现有图形设计自动化方法面临创意和智能不足的问题，且基于扩散模型的方法仅能生成不可编辑且文本可读性差的图片，难以真正提升实际工作效率。

Method: 作者提出IGD方法，首先开发了设计平台并建立了多场景的标准化设计文件格式；利用多模态大语言模型(MLLM)进行属性预测、层次排序和布局；使用扩散模型合成资产图像内容；整体为端到端训练框架，支持在复杂设计任务下的可扩展性。

Result: 实验结果显示，IGD在图形设计自动化生成上效果优越，能够创造可编辑且高质量的设计，并在多种场景下表现出良好的扩展性和灵活性。

Conclusion: IGD为实际自动化图形设计提供了一种全新的解决思路，有望显著提高设计效率和可用性。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [102] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文提出了一种名为Crucial-Diff的新框架，通过生成有针对性的训练样本，提升数据稀缺场景下的检测和分割性能，并在多个数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 数据不足（如医疗、工业、自动驾驶等领域）会导致模型过拟合和数据集不平衡，影响检测与分割任务的效果。而现有生成模型产生的合成样本往往过于简单、缺乏关键性信息，且不同对象需要分别训练，效率低下，未能有效提升下游模型的弱点。

Method: Crucial-Diff采用了两个核心模块：一是场景无关特征提取器（SAFE），统一提取目标信息；二是弱点感知样本挖掘器（WASM），根据下游模型的检测结果反馈生成难检测样本。两模块输出融合后，生成多样化、高质量的训练数据。

Result: 在MVTec数据集上，Crucial-Diff实现了83.63%的像素级AP和78.12%的F1-MAX；在polyp数据集上，分别达到了81.64%的mIoU和87.69%的mDice，优于现有方法。

Conclusion: Crucial-Diff框架能够自动且高效地利用下游模型反馈，生成有针对性的高质量样本，显著提升了不同数据稀缺领域中目标检测和分割任务的表现，具有良好实用性与推广价值。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [103] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 本文对时尚零售产品属性识别任务中主流多模态大模型（如Gemini 2.0 Flash和GPT-4o-mini）进行了零样本评估，以分析其在时尚商品识别场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 精准的商品属性识别对时尚电商至关重要，可提升商品目录的组织效率和用户的发现体验，但当前大型语言模型（LLMs）在细粒度时尚属性识别上的能力尚未被充分探索。

Method: 本研究采用DeepFashion-MultiModal数据集中仅图片作为输入，评价了Gemini 2.0 Flash与GPT-4o-mini在18类时尚商品属性的零样本识别任务中的表现，兼顾了模型速度和成本效益；并通过错误分析进一步理解各模型优势与局限。

Result: Gemini 2.0 Flash在所有属性类别上的macro F1分数为56.79%，表现优于GPT-4o-mini的43.28%。详细错误分析揭示了模型的表现差异及实际应用中的挑战。

Conclusion: Gemini 2.0 Flash在时尚属性零样本识别上展现出最佳综合表现，但要实现电商产品属性自动化的落地应用，仍需关注领域内针对性微调。研究为未来时尚AI及多模态属性提取奠定基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [104] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 本文提出了一种结合多图像超分辨率（MISR）与卷积神经网络（CNN）的方法，实现在超低剂量下对易损材料进行原子级分辨成像，显著扩展了4D-STEM电子显微镜在辐射敏感材料中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统电子显微镜虽然能提供原子级结构信息，但对蛋白质、二维材料等对束流敏感的样品，容易造成辐射损伤，难以实现高分辨成像。为此，研究者希望突破电子剂量的限制，提升在低剂量情况下的成像分辨率。

Method: 借鉴遥感领域广泛应用的多图像超分辨率（MISR）原理，融合多幅低分辨、亚像素位移的观测视图，并使用基于双路径、注意力机制的卷积神经网络进行特征整合和重建，提升超低剂量数据的重构效果。

Result: 在非晶、半晶态和晶态的电子束敏感样品上，所提方法能在超低剂量下实现原子级分辨率成像。各类代表性材料上的系统评价显示，与传统的ptychography成像方法相比，具有可比的空间分辨能力。

Conclusion: 该方法显著提升了4D-STEM电子显微技术在辐射易损材料结构分析中的适用性，为此类材料提供了一种普适且有效的原子尺度成像手段。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [105] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: 本文提出KPHD-Net，采用Hölder散度和Dempster-Shafer证据理论，提升多视图分类与聚类结果的准确性、鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前多视图学习方法虽然通过信息融合提升性能，但在不确定性估计和异质模态整合方面仍存在不足，特别是在有噪声或数据损坏时。常用的KL散度忽略了不同模态间的域差异，本研究旨在解决这一关键问题。

Method: KPHD-Net以变分Dirichlet分布建模多视图下的类别概率分布，并用Hölder散度来衡量分布间的差异。进一步，结合Dempster-Shafer证据理论（DST）和卡尔曼滤波器，将多视图信息融合用于更准确的不确定性估计和状态预测。

Result: 理论分析证明，Hölder散度比传统方法在分布度量上更有效，大量实验显示KPHD-Net在分类和聚类任务中，在准确性、鲁棒性和可靠性方面均优于现有SOTA方法。

Conclusion: KPHD-Net显著提升了多视图学习中的融合决策可靠性和不确定性估计效果，为信息融合和鲁棒决策提供了更优解决方案。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [106] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 本文针对LDMs（潜变量扩散模型）中的自编码器设计进行研究，提出了一种结合变分与掩码自编码器（VMAEs）的新方法，并将其整合到LDM框架中，实验验证生成质量和效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LDM中自编码器难以兼顾潜变量流形的平滑性、感知压缩质量和重建质量三大关键特性。为提升LDM整体性能，有必要深入研究自编码器设计，并寻找能同时满足上述特征的新方案。

Method: 提出变分掩码自编码器（VMAEs），利用掩码自编码器保持的层次特征构建新的潜变量空间；将VMAEs集成于LDM系统，实现Latent Diffusion Models with Masked AutoEncoders（LDMAEs）。

Result: 通过全面实验，LDMAEs在图像生成质量和计算效率上均取得了显著提升，优于现有主流方法。

Conclusion: 改进自编码器结构能有效提升LDM性能，VMAEs方法为实现高质量、效率兼备的潜变量扩散模型提供了新思路。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [107] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本论文提出了一种新型的3D高斯基对抗攻击(3DGAA)方法，通过联合优化物体的几何与外观属性，实现了高度真实与可转移性的物理对抗样本生成，大幅降低了自动驾驶感知系统下的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理对抗攻击多以纹理扰动为主，难以兼顾物理现实性与攻击稳健性，导致实际场景下攻击效果有限。因此，急需一种能够物理可实现并具高攻击效果的新方法来检验自动驾驶系统安全性。

Method: 提出3D Gaussian-based Adversarial Attack(3DGAA)框架，利用3D高斯Splatting方法的14维参数空间，联合优化对象的几何（形状、比例、旋转）与外观（颜色、不透明度）属性，并引入物理过滤和物理增强模块以提升物理真实性和场景适应能力。

Result: 在虚拟和物理实验中（使用微缩模型车），3DGAA将目标检测mAP从87.21%降低至7.38%，攻击效果显著优于现有3D对抗攻击方法，且具备高度跨场景可转移性。

Conclusion: 3DGAA为物理可实现的对抗攻击树立新标杆，可作为自动驾驶感知系统攻防评测协议的有效工具，并推动系统安全性提升。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [108] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 本研究提出了一种基于Swin Transformer的深度学习框架，利用多壳层扩散MRI数据，实现阿尔茨海默病及淀粉样蛋白积累的早期检测与分类。


<details>
  <summary>Details</summary>
Motivation: 早期诊断阿尔茨海默病以及检测淀粉样蛋白积聚对疾病管理具有重要意义。但现有方法在特征提取和数据规模上存在局限，需要新方法提升识别能力。

Method: 提出了一个使用Swin Transformer的分类流程，将DTI与NODDI等多壳层dMRI关键指标投影为二维图像，结合低秩自适应技术以应对医学影像数据标注稀缺问题，并通过ImageNet预训练实现迁移学习，对阿尔茨海默病、轻度认知障碍以及淀粉样蛋白状态分类进行评估。

Result: 使用NODDI指标将认知正常与阿尔茨海默病痴呆区分的平衡准确率达到95.2%；在淀粉样蛋白检测中，平衡准确率分别达到77.2%（认知障碍/阿尔茨海默病患者）和67.9%（认知正常人群）。Grad-CAM可解释性分析发现了海马旁回、海马等关键脑区。

Conclusion: 本研究证明了多壳层扩散MRI结合Transformer模型可用于阿尔茨海默病与相关病理的早期筛查，有助于生物标志物驱动的诊断，特别适合数据量有限的医学场景。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [109] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 本文综述了反无人机（Anti-UAV）检测与跟踪领域的现状、挑战、数据集和主流方法，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用的普及，反无人机跟踪在公共安全等复杂环境中愈发重要，亟需高效、精准的检测与跟踪技术。

Method: 首先，梳理当前反无人机检测与跟踪的特点与挑战；其次，收集并整理了公开数据集，便于研究人员使用；再者，系统分析了近年提出的基于视觉与多传感器融合的检测与跟踪算法；最后，总结未来可能的研究方向。

Result: 系统性地整理了领域内主流公开数据集，对主流算法进行了归纳评述，为后续研究提供了便利。提出了未来发展建议。

Conclusion: 本文为反无人机检测与跟踪领域的技术现状、挑战、数据集、主流方法与研究方向做了全面梳理与展望，对推动该领域发展具有参考价值。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [110] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的相移轮廓术自补偿方法（I-BSC），可显著减少动态测量下的运动误差，提升3D扫描的速度与准确性，且大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统相移轮廓术（PSP）虽然高精度，但要求被测物体静止，在动态场景下易受运动影响，造成严重误差。现有对策如P-BSC虽然能降低误差，但计算量大、速度慢，实用性受限。因此亟需更快、更高效、更抗运动误差的方法。

Method: 本文借鉴P-BSC思想，将“二项式加权自补偿”从相位序列推广到条纹图像序列，仅需一次arctan运算，实现了图像序列加权求和，极大简化了计算流程，并保持逐像素、逐帧处理特性。

Result: 实验和仿真结果显示，I-BSC可有效降低运动误差，达到类似单帧处理的高速度（深度图帧率匹配相机采集率），其运动误差收敛速度和计算效率均明显优于P-BSC，计算复杂度降低一个多项式阶，速度提升数倍以上。

Conclusion: I-BSC方法能在动态3D扫描速度和精度上超越现有自补偿技术，兼顾高效率和高分辨率，为高性能动态3D测量提供了有效技术途径。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [111] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hyma的超网络模型对齐方法，能高效优化多模态基础模型中，单模态模型的选择与连接器的联合训练，显著降低计算成本，效果优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型通常采用多种预训练的单模态模型进行组合，通过训练连接器对齐两者的表达空间。然而，随着单模态模型数量剧增，对模型选择和连接器联合训练的计算需求不断上升，导致效率低下，这一挑战尚缺乏充分研究。

Method: 作者提出Hypernetwork Model Alignment (Hyma)方法。通过超网络的参数预测能力，为所有N × M种单模态模型组合联合生成连接器参数，实现单一网络覆盖多组模型选择与连接需求。

Result: 在多种主流多模态任务基准测试实验中，Hyma在确保模型排名和连接器效果不逊于暴力搜索网格法的同时，将最佳单模态组合选择的搜索成本平均降低了10倍。

Conclusion: Hyma极大提高了多模态基础模型拼接流程中模型选择与训练的效率，为大规模多模态系统的建设提供了高效可行的全新解决方案。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [112] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种记忆高效的文本到图像扩散模型个性化方法，通过自适应选择不同优化策略，实现高质量、低内存的边缘设备微调。


<details>
  <summary>Details</summary>
Motivation: 当前个性化适应文本到图像扩散模型需大量内存，且易泄露隐私，难以在边缘设备高效运行。因此亟需提出在内存受限环境下有效的个性化方法。

Method: 结合低分辨率反向传播（BP-low）和高分辨率零阶优化（ZO-high），并设计步长感知概率函数自适应选择优化策略，实现个性化、高质量、低内存的微调。

Result: 该方法在内存消耗显著降低的前提下，个性化性能与现有方法相近；同时保证了高质量输出且未增加推理时延。

Conclusion: 提出的方法能在边缘设备上实现高效、可扩展且高质量的扩散模型个性化，为隐私保护场景下模型适配提供有效方案。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [113] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 论文提出了一种专用于漫画书页面流分割（PSS）的多模态Transformer模型CoSMo，并显著优于现有方法和通用大模型，建立了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 漫画内容理解是自动化分析中的重要环节，PSS是实现角色分析、故事索引等下游任务的关键前置步骤。当前漫画PSS缺乏专门数据集和高效模型。

Method: 作者将PSS在漫画这一特殊媒介下形式化，并新建了20,800页有标注数据集。提出的CoSMo模型有纯视觉和多模态两种形式，利用视觉及视觉-文本特征实现页面分割。

Result: CoSMo在F1-Macro、全景质量（Panoptic Quality）和流级别指标上，显著优于传统基线和大规模通用视觉-语言模型。视觉特征主导宏观结构识别，多模态则有助于解决棘手歧义。

Conclusion: CoSMo为漫画书分析树立了新SOTA，展示了可扩展的自动理解潜力，为下游任务和大规模应用打下基础。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [114] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种基于轻量级机器学习的方法，通过分析鸡粪便图像，实现对家禽常见疾病的高效检测。所提方法不仅在准确率上媲美主流深度学习模型，而且计算资源消耗极低，适合资源有限的农业环境应用。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖在全球食品供应链中占有重要地位，但极易受到如球虫病、沙门氏菌病和新城疫等传染病侵害。传统检测方法成本高、效率低，深度学习模型虽准确但资源消耗大，不适合低资源环境。因此亟需开发一种既高效又经济的疾病检测方法。

Method: 提出通过多色彩空间（RGB、HSV、LAB）特征提取及颜色、纹理、形状描述符（色彩直方图、LBP、小波变换、边缘检测等）构建全局特征集，并利用PCA和XGBoost筛选最优特征。采用人工神经网络（ANN）进行分类，通过系统消融实验验证各模块贡献。

Result: 优化后的ANN模型在无需GPU的情况下，实现了95.85%的准确率，在Google Colab运行仅需638秒。与Xception、MobileNetV3等深度学习模型相比，所提方法在准确率相当的同时，计算资源消耗大幅下降。

Conclusion: 该方法为低资源农业环境下的家禽疾病实时检测提供了一种经济、高效、可解释、易于扩展的解决方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [115] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一种能够从单目视频中快速合成四维动态新视角的前馈模型，通过高效的高斯原语和像素对齐网格，首次统一了场景外观、几何和运动的建模。


<details>
  <summary>Details</summary>
Motivation: 现有动态三维场景建模和新视角合成方法通常效率低或只关注部分属性（如外观或几何），难以实现高效、统一的动态场景建模和各种任务扩展。

Method: MoVieS采用像素对齐的高斯原语网格显式监督动态三维场景的时变运动，集成外观、几何和运动三重要素。模型以前馈方式工作，允许大规模数据集训练且无需大量任务特定标注，功能上包括新视角合成、三维重建和点跟踪。

Result: MoVieS在多项任务上展示了竞争性的性能，且推断速度相比以往方案提升数个数量级，实现了高效和多任务扩展。

Conclusion: MoVieS为动态三维场景的统一快速建模提供了新范式，在无监督新任务等应用上表现卓越，证明了该方法的有效性和效率。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [116] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 本文针对扩散模型中普遍存在的曝光偏差问题，提出了一种基于频域波段调节的新方法。该方法通过频域分析实现更精细的低高频能量调整，提升生成质量并减弱曝光偏差，无需重新训练即可适用于多种扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成能力强，但因为预测时逐步生成，容易积累曝光偏差。作者观察到在扩散过程中预测图像能量会降低，并且这种能量降低在低频和高频子带上表现不同，由此揭示曝光偏差的深层结构动因。

Method: 利用小波变换将图像信号分解为低频和高频子带，提出频域调节机制分别对低频和高频能量进行矫正。该方法为即插即用（plug-and-play）形式，无需重新训练模型，可直接应用于现有各种扩散模型上。

Result: 该方法在多种扩散模型上验证，显著提升了生成图像的质量，减弱了曝光偏差，并表现出良好的通用性和鲁棒性。

Conclusion: 作者的新颖频域调节方法为扩散模型中的曝光偏差提供了高效、实用的解决方案，有助于拓展扩散模型在高质量生成任务中的应用场景。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [117] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 本文提出了一种基于SegFormer模型的两阶段迁移学习策略，用于遥感影像水体分割，在源域和目标域上有效提升了分割精度，尤其解决了域不一致和小样本问题。


<details>
  <summary>Details</summary>
Motivation: 遥感影像水体分割在目标区域常常面临域间差异（domain shift）和可用训练样本少的问题，导致模型迁移后性能显著下降。传统直接迁移方法在新的复杂地形和光谱环境下表现很差，亟需新的技术策略提升模型适应性和分割精度。

Method: 首先在多样化的源域数据集上训练基础分割模型（以SegFormer为骨干），初始IOU达到68.80%；然后在目标域（以扎达土林地区为例）小样本上进一步微调，实现模型针对特殊地理与光谱环境的适应。

Result: 在扎达土林这样地形与光谱高度复杂的区域，通过两阶段迁移学习，目标域水体分割IOU从直接迁移的25.50%显著提升至64.84%。

Conclusion: 该策略有效缓解了由域不一致引起的模型性能退化，适用于数据稀缺、环境特殊的高精度遥感信息提取，并为类似场景下的遥感分割任务提供了实用技术范式。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [118] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP通过创新性方法解决了CLIP在处理长文本输入（>77 tokens）任务中的表现不足，实现了长短文本检索的新SOTA，并可直接应用于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP因文本编码器输入长度受限，无法有效处理需要长文本理解的任务。为此，本文目标是提升CLIP在长文本场景下的表现。

Method: 提出FIX-CLIP，包含三个新模块：（1）双分支训练机制，分别对齐短文本+掩码图片和长文本+原图，提高长文本表征；（2）Transformer层中的可学习区域prompt及单向mask以提取区域信息；（3）分层特征对齐模块加强多尺度特征一致性。此外，构建了3000万图像的数据集，通过MLLM自动生成长文本描述用于训练。

Result: FIX-CLIP在长文本与短文本检索基准测试中均取得了SOTA表现。

Conclusion: FIX-CLIP有效提升了CLIP对长文本输入的处理能力，并可作为长文本输入的即插即用文本编码器应用于扩散模型等下游任务。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [119] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 该论文提出了一种结合外观和轨迹信息的多摄像头多目标跟踪系统，实现了跨视角的一致身份分配。


<details>
  <summary>Details</summary>
Motivation: 多摄像头系统下的目标跟踪面临身份切换和跨视角匹配困难，需保证全局身份一致性。

Method: 方法流程为：首先用BoT-SORT进行单摄像机跟踪，然后通过轨迹和特征匹配初始化全局ID，后续通过优先级的全局匹配策略将新轨迹分配到已有ID，若无足够相似的匹配才分配新ID，同时用深度图及标定结果估算3D位置做空间验证。

Result: 实现了跨摄像头目标跟踪的全局ID一致性分配，提高了多摄像头下跟踪的准确性。

Conclusion: 本文方法有效实现了跨摄像头多目标一致性跟踪，优势在于结合轨迹、外观特征与空间验证，提升了实际应用中的身份管理能力。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [120] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督全景分割方法DEARLi，结合了两个基础模型，通过提升识别和定位性能，极大提升了在标签稀缺情况下的分割效果。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高、耗时。半监督分割希望利用少量有标签及大量无标签数据来缓解标签稀缺，但如何更好地利用基础模型还缺乏有效机制。

Method: 作者提出DEARLi方法：1）利用无监督mask-transformer一致性与CLIP特征的零样本分类相结合，提升识别能力；2）利用SAM伪标签进行类别无关解码器预热，提升定位能力。两种增强分别关注于识别和定位。

Result: 在ADE20K数据集上，DEARLi只用158张标注图像即可达到29.9 PQ和38.9 mIoU。与现有半监督语义分割SOTA方法相比，性能领先，并节省了8倍显存。

Conclusion: DEARLi显著提升了半监督分割特别是在类别多、标注少的极端场景下的表现，同时效率高，对全景任务有效，推动了基础模型在分割标注稀缺情况下的实际应用。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [121] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 本文研究了面向超声心动图的先进点跟踪方法，提出了改进训练策略与轻量级网络，有效提升了追踪精度和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 心脏超声图像中的组织形变跟踪对于临床心脏功能测量至关重要，但传统方法在处理复杂心脏运动时效果有限。近年来点跟踪技术在其他领域表现优异，但在超声心动图中的应用与表现尚未充分研究。

Method: 作者分析了当前方法存在的方向性运动偏置问题，并提出了优化训练流程和定制的数据增强策略，降低偏置提升追踪鲁棒性。此外，基于空间上下文的多尺度代价体积，提出了一个轻量级神经网络，与现有复杂时空模型进行对比。

Result: 基于新训练策略微调后，模型在常规与分布外数据上的性能显著提升。以EchoTracker为例，位置精度提升60.7%，轨迹误差下降61.5%。值得注意的是，部分复杂点跟踪模型的表现反而不如作者提出的简单模型。

Conclusion: 改进的训练策略与新型网络提升了心脏组织点的追踪精度与泛化能力，同时在临床测量如GLS中取得了更优重现性，为实际应用提供了更可靠的工具。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [122] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 本文提出了一种受预测编码启发的反馈机制，将输出端结果反馈到输入端，使神经网络能够通过多步迭代不断优化其内部状态。该机制集成于U-Net架构中，并引入softmax投影和指数衰减确保反馈环的稳定性。实验表明，该反馈模型在噪声环境和有限监督情况下表现优于传统前馈模型，提升了鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈机制逐步优化感知过程，而多数人工神经网络仅采用单步前馈计算，无法充分利用迭代感知的优势。本文的动机是将生物启发的反馈机制引入深度学习架构，以提升网络在噪声和小样本条件下的表现。

Method: 作者在标准U-Net架构中嵌入了一个从输出到输入的反馈回路，允许模型多次迭代优化内部状态。为保证该反馈回路的稳定性，引入了softmax投影和指数衰减两种生物学启发的操作，并在合成分割任务上进行了对比实验。

Result: 在噪声条件和有限监督（小样本）任务上，反馈模型显著优于同等规模的前馈模型。在仅用两个训练样本时，反馈模型可取得明显高于随机水平的性能，而前馈模型需至少四个样本才能达到类似表现。

Conclusion: 引入预测编码式反馈机制能显著增强神经网络的鲁棒性和数据效率，表明具有生物启发的反馈结构有望推动构建更自适应、高效的神经网络模型。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [123] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: 论文提出了一种基于AI的视频分析系统SlumpGuard，实现对混凝土工作性（坍落度）的实时、自动化监测，提升了质量控制的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的混凝土坍落度测试主要依赖人工操作，存在耗时、效率低、结果不一致等问题，这限制了在实际工程中对混凝土质量的实时监控。为解决这些痛点，需发展更高效、自动化的检测手段。

Method: 作者设计了一套名为SlumpGuard的AI视频分析系统，通过拍摄混凝土从搅拌车滑槽流下的画面，利用专门构建的数据集训练模型，实现实时、自动评估混凝土的工作性，无需人工干预。论文介绍了系统架构和数据集构建，并进行了实际工地部署测试。

Result: 实测结果表明，SlumpGuard系统在真实工地环境下运行良好，能够准确、高效地自动检测混凝土的工作性，完成了全批次质量检验，且无需人工介入。

Conclusion: SlumpGuard是一种切实可行的现代混凝土质量保障方案，可实现高效、可靠的自动化工作性检测，有助于提升混凝土施工的质量和效率。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [124] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种在合成数据和真实数据领域间进行双层域适应的文本描述人物检索方法，实现了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的人物检索主要依赖大量合成数据进行预训练，但真实数据与合成数据的领域差异（如光照、视角等）显著，导致预训练微调效果受限。解决领域差距对提高检索准确率至关重要。

Method: 提出了统一的检索流程，包括两大核心模块：（1）Domain-aware Diffusion（DaD）：通过扩散模型使预训练图片的分布更贴近目标真实数据集。（2）Multi-granularity Relation Alignment（MRA）：在区域级别对视觉区域和文本描述进行精细对齐，实现更细致的领域适应。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid三大公开数据集上，该方法取得了目前最优的检索效果，优于现有对比方法。

Conclusion: 通过同时考虑全局（图片）和局部（区域）的域适应，显著提升了文本描述人物检索的性能，为利用合成数据提升实际应用系统检索准确率提供了有效方式。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [125] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型（MLLM）无法有效处理高分辨率图像中的精细定位与推理任务的局限，提出了一种无需训练、面向任务无关的两阶段新框架Extract Candidate then Predict（ECP）。通过先用低分辨率信息进行粗定位，再聚焦候选区域进行细粒度预测，显著提升了MLLM在高分辨率场景下的表现。


<details>
  <summary>Details</summary>
Motivation: MLLM在视觉-语言任务取得了良好成绩，但面对4K/8K等高分辨率图像时，因模型训练和测试时分辨率不一致，导致泛化性变差，直接降采样又丢失了细粒度的视觉信息，限制了模型在需要精细识别和推理的任务中的应用。

Method: 提出Extract Candidate then Predict（ECP）框架：第一阶段，使用下采样图像输入MLLM获取粗定位或初步预测，从中提取候选区域；第二阶段，对原始高分辨率图像中的候选区域再进行深入分析，实现任务目标。该方法无需重新训练模型，兼容多种任务。

Result: 在4K GUI grounding、4K和8K多模态感知等基准任务上，ECP框架分别带来了+21.3%、+5.8%、+5.2%的绝对性能提升，相对于基线表现显著。

Conclusion: ECP在无需训练和任务无关的情况下，有效解决了高分辨率图像下MLLM定位与推理能力受限的问题，兼顾了分辨率一致性和细粒度细节保留，对提升多模态模型在实际高分辨率场景的实用性具有示范作用。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [126] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 该论文提出了多模态学习中常见的“欠优化”问题并挑战现有的均衡优化方式，提出了一种基于方差反比原则的非均衡优化策略——ARL，以提升多模态学习性能。


<details>
  <summary>Details</summary>
Motivation: 作者发现多模态学习有时其表现甚至不如单模态，现有方法普遍只关注各模态优化的均衡性。作者认为均衡学习不是最优解，通过理论分析提出更优的非均衡学习设定。

Method: 论文提出Asymmetric Representation Learning（ARL）策略，通过为每个模态引入辅助正则器以计算其预测方差，然后根据单模态方差反比原理重新加权各模态优化，使各模态依赖比例与方差比成反比。同时引入预测偏差项，联合优化以降低泛化误差。所有辅助正则共享参数且不增额外参数、无关结构与融合模式。

Result: ARL在多组数据集上的实验显示，相较于以往均衡优化方法，在有效性和通用性方面均有显著提升。

Conclusion: 作者证明了非均衡依赖各模态，根据方差反比设定的优化方式能获得更佳多模态学习效果，ARL为多模态学习提供了一个有效且灵活的优化策略。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [127] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 本研究提出并验证了少数群体的微表情识别受种族影响，并开发了一个结合种族语境的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 以往微表情研究普遍基于情感普遍性假设，认为不同文化和种族的人情感表达高度相似，然而现实中可能存在种族差异。作者想揭示微表情识别中种族因素的影响。

Method: 构建了一个跨文化微表情数据库，通过算法标注种族信息，并在单一/多种族受试者环境下对比微表情表现。并提出一种将种族语境纳入情感特征学习过程的微表情识别框架。

Result: 实验揭示了种族偏见确实对微表情识别产生影响，种族语境的引入提升了微表情识别框架的表现和公正性。

Conclusion: 情感普遍性假设存在过度泛化，微表情识别应考虑种族因素。所提出的种族感知微表情识别框架为后续相关研究提供了基础。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [128] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 在多模态学习中，现有方法未能解释为什么多模态模型中的主导模态表现比单模态模型差。本文发现这是由于模态编码器和融合模块间的优化冲突，并提出了解耦梯度学习(DGL)框架以解决这一问题。实验验证DGL能显著提升多模态模型表现。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常出现优化不足，导致其表现比单模态更差。主流观点归因于模态间学习不平衡，通过梯度调整来均衡，但没能解释主导模态为何也表现下降。因此，作者试图找到更根本的原因并设计新方法解决。

Method: 作者证明多模态融合过程会削弱回传到各自模态编码器的梯度，导致各模态表现下降。为此提出DGL：把多模态损失回传到编码器的梯度截断，改用单模态损失的梯度；同时不让单模态损失影响融合模块，从而消除梯度相互影响，实现解耦优化。

Result: 在多种模态、任务和框架上做了大量实验（包含强交互的多模态模型），显示DGL方法在各类指标和任务上均提升了多模态模型的性能，表现出有效性和通用性。

Conclusion: 解耦模态编码器与融合模块的梯度优化能够显著提升多模态模型，在理论与实验上均得到支持。DGL框架可作为改进多模态学习的重要方法。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [129] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 本文提出了一种新的人像个性化生成方法Wardrobe Polyptych LoRA，在保证生成图像高保真和一致性的同时，显著降低推理计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成方法在保持身份、衣着等精细属性方面存在困难，并且推理时需微调或用大规模数据训练，这导致计算资源消耗大，难以实时应用。

Method: 作者提出Wardrobe Polyptych LoRA，只训练LoRA层，推理阶段无需额外计算开销。该方法引入服饰条件和空间参考，减少信息丢失，提高生成一致性；创新性地设计了选择性主体区域损失，使模型在训练时能够忽略部分参考图像，同时更好地结合文本提示。

Result: 该方法无需每个新主体都微调或大规模训练，只需单模型和少量样本即可生成高质量新主体图片。实验结果表明，其在保真度和一致性上明显优于现有方法。

Conclusion: Wardrobe Polyptych LoRA极大减轻了个性化人像生成的推理计算负担，实现了高保真、可控且保留身份特征的全身图像合成，为个性化图像生成应用提供了更高效实用的解决方案。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [130] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成模型的新方法VRFNO，通过优化噪声与历史速度项联合训练，提升了模型在单步和少步生成任务中的图像质量，克服了以往Reflow方法的局限。


<details>
  <summary>Details</summary>
Motivation: 当前Reflow操作虽然能改善图像生成质量，但因其噪声与图像的确定性耦合导致生成分布与真实分布存在间隙，限制了高质量图片的快速生成。

Method: VRFNO是一种结合编码器和神经速度场的联合训练框架。创新点在于：（1）引入历史速度项提升轨迹区分能力；（2）通过重参数化进行噪声优化，使噪声与真实图片形成更优耦合，从而改善训练。

Result: 实验表明，VRFNO在合成数据和多分辨率真实数据集上，均有效缓解了Reflow的不足，实现了单步和少步图像生成的最新最优性能。

Conclusion: VRFNO通过噪声优化和历史速度增强，极大提升了图像生成质量，特别适合于对生成速度和质量均有高要求的应用，是Reflow优良的替代方案。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [131] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的空间提升方法（SL），将2D图像数据映射到更高维空间（如3D），利用适配的高维网络（如3D U-Net）实现高精度、低参数量和低推理成本的密集预测，并在19个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习中密集预测任务（如分割、深度估计）通常依赖大参数量和高推理成本的2D网络。当前方法存在资源消耗大且参数冗余的问题。作者希望突破常规，通过空间提升探索更高效的实现方式。

Method: SL方法将标准2D输入升维到更高维度（如3D），再用高维网络（如3D U-Net）处理。升维后输出在新维度上有自然结构，训练中可用密集监督，测试时可高效评估预测质量。

Result: 在19个基准数据集（13个语义分割、6个深度估计）上，SL方法实现了与主流方法相当甚至更优的预测性能，同时在U-Net场景下模型参数量减少超过98%，显著降低了推理成本。

Conclusion: 空间提升（SL）方法为视觉密集预测提供了新的范式，结合高效、准确和可靠性，未来有望成为更高效的视觉深度神经网络方案。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [132] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 本文介绍了ProGait数据集，这是一个专为视觉机助步态分析任务而构建的多用途数据集，包含了假肢用户的多种步态视图，并提供基线模型和基准测试任务。


<details>
  <summary>Details</summary>
Motivation: 视觉机学习方法在步态分析中具有非侵入性和可扩展性，但由于假肢的特殊外观和运动模式，现有方法难以准确检测与分析假肢。现有缺乏针对假肢人士的视觉步态数据集，影响相关算法发展。

Method: 作者构建了ProGait数据集，其中包含412段四名膝上截肢者使用新装假肢行走的视频，涵盖视频目标分割、2D姿态估计和步态分析等视觉任务。与此同时，提出基线任务和微调模型，并与现有的预训练视觉模型效果进行对比。

Result: 经过在ProGait数据集上微调后的模型，在假肢相关的检测和分析任务上，相比现有的预训练视觉模型展现出更好的泛化能力和性能。

Conclusion: ProGait数据集为假肢步态分析和相关视觉任务研究提供了重要基础资源，有望推动假肢检测、姿态估计及步态分析领域的发展。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [133] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SynOOD的新方法，通过基础模型（扩散模型和多模态大语言模型MLLMs）生成具有挑战性的OOD（分布外）样本，用于微调CLIP模型，从而提升近边界的分布判别能力，在ImageNet等大规模基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练视觉-语言模型（如CLIP）在检测OOD样本上表现不俗，但对于那些在特征空间与分布内样本非常接近的困难OOD样本，仍可能造成误分类。基础模型（如扩散模型和MLLMs）的发展为生成特定难度的合成样本提供了新思路，有望改进上述问题。

Method: 提出SynOOD框架：利用基础模型中的扩散模型和多模态LLMs，基于上下文提示对图像进行迭代in-painting生成边界对齐的细致OOD样本；结合能量分数等OOD指标引导噪声调整，更好地采样于InD/OOD边界区域。使用合成样本对CLIP的图像编码器和文本编码器导出的负标签特征进行联合微调，以增强对近边界OOD的判别能力。

Result: 在ImageNet大规模基准测试中，SynOOD获得了最优结果。相较于现有方法，不仅参数和运行时间几乎没有增加，还提升AUROC 2.8%，降低FPR95 11.13%。

Conclusion: 通过利用基础模型合成并精细设计的边界样本，SynOOD显著提升了InD-OOD判别能力，是目前最优的OOD检测方法之一。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [134] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本文针对AI生成图像检测（AID）在现实世界中的挑战，提出了新的社交媒体数据集，并系统分析影响检测性能的关键因素。通过优化模型和数据处理方法，AID模型在真实场景下AUC平均提升26.87%。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展迅速，带来了信息真实性和社会信任的巨大挑战。AI生成图像已能高质量欺骗人类，现有检测模型虽然在标注数据集上表现良好，却严重缺乏现实适应性。

Method: 提出并构建了ITW-SM数据集，包括主流社交媒体上收集的真实与AI生成图像。系统性分析骨干网络、训练数据组成、预处理策略、数据增强组合四个关键因素对检测性能的影响，并在这些方面做出改进。

Result: 改进后的AID模型在各种真实世界条件下，AUC平均提升26.87%。

Conclusion: 当前AID模型在现实环境中存在显著短板。通过针对性地提升模型结构、数据处理和数据增强，可以大幅增强其在真实场景下的检测能力。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [135] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 本研究探讨在语义分割中，采用风格迁移方法是否能减少神经网络对纹理信息的依赖，并提高其对图像扰动的鲁棒性。实验结果表明，风格迁移增强可以有效地降低纹理偏置，并提升网络对常规扰动和对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往在图像分类领域发现DNN存在过度依赖纹理的问题，而通过风格化训练可以缓解该问题并增强鲁棒性。这项工作希望验证这种方法在更复杂的语义分割任务中是否同样有效。

Method: 本文使用风格迁移方法，在图像的随机分区（基于Voronoi单元）内进行不同风格的替换，然后用这种风格迁移增强后的数据来训练语义分割的深度神经网络（包括CNN和Transformer）。

Result: 在Cityscapes和PASCAL Context数据集上，所提出的方法显著减弱了网络的纹理偏置，并极大提高了模型对常规扰动和对抗攻击的鲁棒性。

Conclusion: 风格迁移增强不仅能用于分类，在语义分割中同样可以减少网络的纹理偏置，强调形状特征，提高网络鲁棒性，并且对不同架构和数据集均适用，具有较强的普适性。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [136] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Kaleidoscopic Background Attack (KBA)的新型攻击方法，通过在图片背景中植入多重径向对称的相似结构，有效干扰相机位姿估计算法，实验验证了该方法的广泛攻击有效性。


<details>
  <summary>Details</summary>
Motivation: 在目标为中心、背景稀疏的图像场景中，相机位姿估计算法的精度容易受到占据主要画面部分的背景纹理的影响。本研究旨在探讨并利用这一影响，设计针对性的攻击方式，提高对现有位姿估计算法鲁棒性的威胁和认识。

Method: 作者提出了Kaleidoscopic Background Attack（KBA）方法，将重复相同纹理片段组成多重径向对称的“光盘”嵌入图像背景。这些结构能够在不同视角下保持高度相似性，从而迷惑和干扰位姿估计算法。此外，论文还提出了一种投影方向一致性损失函数，用以优化这些光盘片段，提高攻击效果。

Result: 实验结果显示，通过优化的对抗性万花筒背景可以对多种主流相机位姿估计算法产生显著的攻击效果，证明了该方法的有效性和通用性。

Conclusion: 本文展示了背景设计上的对抗性攻击能够对姿态估计算法构成严峻威胁，也提示了相关算法在鲁棒性和安全性上需要进一步加强背景扰动的防御能力。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [137] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的Transformer架构FTCFormer，通过语义驱动的聚类方式动态生成视觉token，有效提升了视觉Transformer在多个数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer将图像分割为规则网格的视觉token，忽视了图像区域的语义信息，导致特征表达能力不足。因此，亟需一种方法根据语义重要性动态生成token，更好地表征不同区域。

Method: 1. 提出FTCFormer，采用聚类驱动的下采样模块按语义生成token。
2. 引入DPC-FKNN聚类机制确定token中心，利用SCS进行token分配，提出Cmerge方法实现通道融合。通过上述新机制实现更精细的语义分割和特征提取。

Result: 在32个不同领域的数据集上进行实验，FTCFormer在特征分类上相较于TCFormer基线模型显著提升，包括在五个细粒度数据集上提升1.43%、六个自然图像数据集上提升1.09%、三个医学数据集上提升0.97%、四个遥感数据集上提升0.55%。

Conclusion: FTCFormer采用基于语义的token生成和聚类机制，有效提升了Transformer在图像分类等任务中的表现，优于传统空间均匀分割的Transformer结构。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [138] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种利用参考人脸图像作为视觉提示的新方法IP-FVR，可在极端降质情况下实现更高质量、更具身份一致性的人脸视频复原。其在身份保持与画质提升上，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸视频复原方法在严重退化时，难以保持个体独特的细粒度身份特征，往往生成普通缺乏个性的人脸视频。因此，亟需一种兼顾高画质与身份一致性的解决方案。

Method: IP-FVR方法利用高质量参考人脸图像作为身份条件，通过解耦的交叉注意力机制在去噪过程中融合身份信息。为减少视频片段内外的身份漂移，分别引入了基于余弦相似度奖励和后缀加权时序聚合的反馈学习方法（针对24帧内），以及跨片段的指数融合策略。此外，同时使用多流负提示，强化模型关注人脸关键属性，抑制不正确的特征生成。

Result: 在合成和真实数据集上的大量实验表明，IP-FVR在视频质量和身份保持方面，均优于目前主流的人脸视频复原方法。

Conclusion: IP-FVR有效克服了传统FVR方法的身份丢失与画质下降问题，可为实际人脸视频复原应用带来更为真实和一致的恢复效果。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [139] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对视频多模态大语言模型（video MLLMs）的新型视觉封装方法DisCo，有效提升了视频内容语义表达的清晰度与时间一致性，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLMs多采用线性投影封装视觉信息，但这种方式会引入语义模糊和时间不连贯等问题，影响模型理解和推理。尽管resampler结构有改善潜力，但相关有效方法尚未被充分探索。

Method: 作者受resampler结构启发，提出DisCo封装方法。其包含两个核心模块：（1）视觉概念判别器（VCD），通过将视觉token与视频中的判别性概念配对，赋予Token独特语义；（2）时间聚焦校准器（TFC），确保视觉token在每一帧都与视频元素保持时间一致性。

Result: 在多种视频MLLM框架和视频理解基准上进行大量实验，DisCo在准确性和token效率上均显著超越现有最优技术，尤其通过减少语义不清Token提升了效率。

Conclusion: DisCo作为视频内容视觉封装新方案，显著增强视频MLLM的语义区分度和时间一致性，有助于提升模型整体表现，为相关技术提供了有效思路。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [140] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 该论文提出了一种无需gloss注释的双视觉编码器手语翻译框架，并在主流数据集取得了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译系统多依赖中间的gloss注释，但gloss标注昂贵且无法充分捕捉连续手语的复杂性，现有方法受限。作者希望解决无需gloss标注下手语翻译效果提升的问题。

Method: 提出了一种两阶段、双视觉编码器的框架，采用对比式视觉-语言预训练。预训练阶段，用两个互补的视觉骨干网络，各自输出的特征通过对比损失与句子级文本嵌入联合对齐；在下游翻译任务中，将双视觉特征融合后送入编码-解码模型生成文本。

Result: 在Phoenix-2014T数据集上，该双编码器结构在无需gloss注释的手语翻译方案中取得了当前最高的BLEU-4评分，并且始终优于单分支结构。

Conclusion: 双视觉编码器结构和对比式视觉-语言预训练为gloss-free手语翻译提供了有效方法，有望推动该领域进一步发展。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [141] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 本论文提出了一种新框架IMD，通过利用预训练生成式扩散模型和创新的跨图像交互提示模块，从而解决了视觉基础模型在图像特征匹配时存在的对齐问题，并在新建多实例基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型多聚焦于单张图像理解，而特征匹配任务需跨图像理解。现有方法忽略了二者在任务需求上的偏差，导致在多实例匹配情形下性能受限，因此作者旨在解决这些模型引入特征匹配时的失配问题。

Method: 1）采用生成式扩散模型来弥补基础模型对实例级细节捕捉能力的不足；2）提出了跨图像交互提示模块，通过提示机制加强图像对信息交互。并设计了IMIM基准更准确评测失配问题。

Result: 所提出的IMD框架在通用基准上刷新了最新SOTA，在新多实例基准IMIM上性能提升12%，证明了方法能有效缓解失配问题。

Conclusion: IMD结合扩散模型和交互提示机制，显著提升了特征匹配，尤其是在多实例场景下，为相关任务和基础模型利用提供新方向。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [142] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: 本文提出了一种名为QLIP的新型扩散模型量化方法，通过利用文本提示信息自适应调整每层每步的比特精度，从而提升量化效率并保持图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在图像生成任务中表现优异，但其庞大的计算复杂度限制了在资源受限环境下的应用。常规量化方法未能有效利用文本提示等输入条件信息，导致量化效率和生成质量无法兼顾。

Method: 提出QLIP方法，在扩散模型量化过程中引入文本提示作为指导，根据不同文本提示自适应调整每层在每个时间步的比特精度。该方法可以无缝集成到现有主流量化方法中。

Result: 在多个数据集上进行实验，结果表明QLIP在降低模型计算复杂度的同时，有效提升了生成图像的质量，优于现有扩散模型量化技术。

Conclusion: QLIP作为一种创新的条件自适应量化方法，不仅优化了扩散模型的效率，还兼顾了生成效果，为资源受限场景下高质量图像生成提供了新思路。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [143] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: 提出了一种新型的多头特征引导语义分割架构FGSSNet，通过特征注入提高了平面图中墙体分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有平面图墙体分割方法泛化能力有限，需要一种方式让模型更好地捕捉墙体特征，提升分割准确率。

Method: 采用基于U-Net的分割骨干网络，辅以多头特征提取器。特征提取器以选定的墙体图块为训练样本，通过编码-解码结构输出墙体特征并预测墙体宽度，将压缩后的墙体特征注入U-Net潜在空间以引导分割。

Result: 实验结果显示，特征注入的FGSSNet在性能上优于普通U-Net，验证了方法的有效性。

Conclusion: 多头特征引导和特征注入方法能够有效提升平面图墙体分割的泛化性能，对后续相关研究具有启发性。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [144] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于文本的适配器方法VRGAdapter，利用随机图模型更好地捕捉类别描述的多样性和类间关系，从而将视觉-语言模型的知识迁移到下游任务，并提出多分支融合机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本适配器的方法无法充分建模每个类别文本描述的多样性，且缺少对类别间关系的建模，这限制了下游视觉任务的表现。

Method: 提出了VRGAdapter，首先利用顶点随机知识图（VRKG）模型同时建模类别内部的丰富语义描述和不同类别之间的关系。接着在VRKG上进行概率化信息传播，获得每个类别节点的上下文感知分布式表示，并通过重参数化采样学习文本适配器。同时提出不确定性引导的多分支融合（UMF）机制，集成多个预训练模型实现更稳健的性能。

Result: 在多个主流数据集上进行大量实验，验证了VRGAdapter和UMF方案的有效性，优于现有方法。

Conclusion: VRGAdapter能捕捉类别描述多样性和类间关系，提升了从视觉-语言模型到下游任务的适配能力，多分支融合机制进一步增强了稳健性。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [145] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文首次提出了细粒度零样本目标检测（FG-ZSD）任务，提出了一种MSHC方法，并构建了一个全新的鸟类细粒度检测数据集FGZSD-Birds。实验表明，该方法优于现有的零样本检测方法。


<details>
  <summary>Details</summary>
Motivation: 以往零样本目标检测主要集中于粗粒度类别识别，类别间视觉差异大，但现实中常常需要区分细粒度类别（如不同种类的鸟、鱼、花），这些类别间差异微小，难度更大。

Method: 作者提出了一种基于改进的两阶段检测器MSHC方法，并设计多层次语义感知的嵌入对齐损失，实现视觉空间与语义空间的紧密耦合。同时，构建了细粒度新基准数据集FGZSD-Birds，覆盖范围广泛。

Result: 在FGZSD-Birds细粒度零样本数据集上进行广泛实验，结果显示MSHC方法显著优于现有零样本目标检测模型。

Conclusion: 本文为细粒度零样本目标检测任务奠定了基础，提出的MSHC方法和数据集具有较高应用与研究价值，推动ZSD领域从粗粒度向细粒度发展。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [146] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL提出了一种利用基础模型视觉先验，在测试时数据驱动地提升视觉鲁棒性的框架，无需额外训练或架构变动，对多种环境变化具有良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉感知方法要么依赖于特定结构改造，要么严重依赖于针对性的数据增强，导致泛化能力有限。如何在不增加训练复杂度的前提下实现对各种现实变换的适应，是实际场景下的迫切需求。

Method: FOCAL是一种可在测试时运行的数据驱动框架，它利用基础视觉模型（如CLIP和SAM）的先验知识，通过生成并优化一系列候选变换，使输入图像趋向于典型的“规范视角”，从而提升模型的鲁棒性，无需重新训练或改变网络结构。

Result: 在多个具有挑战性的变换任务上（例如2D/3D旋转、光照变化和昼夜变化），FOCAL显著提升了CLIP和SAM等基础模型的鲁棒性。

Conclusion: FOCAL证明了无需专门的数据增强训练，也能通过测试时调整，实现广泛的变换不变性，为视觉系统提供了一种可扩展的新范式。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [147] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 本文提出将拓扑数据分析（TDA）特征与深度学习（CNNs）模型结合，用于遥感影像分类任务，取得了比传统方法更优的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然卷积神经网络（CNNs）在图像分类领域广泛应用，但它们通常偏向于提取局部纹理特征，难以充分挖掘全局几何与拓扑结构信息。拓扑数据分析（TDA）因其鲁棒性和捕捉复杂数据几何结构的能力，在图像分析中显示出潜力。但TDA与深度学习模型的结合尚未充分探索，尤其是在遥感影像场景分类中。作者正是为弥补这一空白，提出TDA特征工程流程并整合进ResNet模型。

Method: 提出了一套TDA特征工程流程，从影像数据中利用持久同调（persistence homology）提取拓扑特征，并将提取结果与深度学习（如ResNet18）模型特征进行拼接或融合，用于分类任务。主要在EuroSAT和RESISC45遥感影像数据集上进行实验，比较集成TDA特征前后模型精度的提升。

Result: 引入TDA特征后，ResNet18模型在EuroSAT数据集上的分类准确率提高了1.44%，达到99.33%，超越了ResNet50及XL Vision Transformers等更大模型。RESISC45数据集同样提升1.82%。这些实验结果均优于现有文献中单模型的最好结果。

Conclusion: TDA特征可有效集成进深度学习模型，提升遥感影像场景分类性能，即使在缺乏显式拓扑结构的数据集中也适用。这项工作是TDA特征首次在卫星场景分类与深度学习结合的尝试，具有较强的应用推广意义。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [148] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 本文结合了代数、数值计算与计算机视觉，探讨如何有效求解参数化代数方程组，特别关注其在RANSAC等鲁棒模型拟合中的应用。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前计算机视觉中如RANSAC等鲁棒模型拟合常常需多次高效求解参数化的代数（多项式或有理函数）方程组，这一问题计算复杂且实用需求旺盛。

Method: 作者回顾并介绍了过去五年多中针对参数化代数系统求解难度衡量及更有效算法设计的相关工作，关注既有理论研究，也侧重于实践中可行的数值与符号算法。

Result: 近年在定量分析参数化方程系统固有难度和实际求解方法上取得了进展，部分方法已能在如RANSAC等实际模型拟合场景中应用。

Conclusion: 论文强调该领域仍具挑战性，但理论和方法均取得实质进展，有助于推动计算机视觉中的鲁棒参数估计与模型拟合的效能提升。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [149] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的AI生成图像质量评估方法SC-AGIQA，结合了文本-视觉语义约束，有效提升了对AI生成图像的一致性和感知失真评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如CLIP、BLIP）在AI生成图像的文本-图像一致性和视觉细节感知上存在语义不匹配和细节感知不足的问题。随着AI生成图像技术的发展，对其高质量评估的需求越来越迫切，需要更精确的方法。

Method: 提出SC-AGIQA框架，通过两个核心模块提升评估能力：（1）TSAM模块，利用多模态大模型MLLMs生成图像描述，并与原始文本进行对比，提升文本-图像语义一致性评估的准确性；（2）FFDPM模块，模仿人类视觉系统，利用频域分析和感知敏感性加权，更好地捕捉图像的细微失真和高质量细节。整体方案融合多模型能力，综合评价AI生成图像。

Result: 在多个公开基准数据集上进行了大量实验，结果显示SC-AGIQA在文本-图像一致性和感知质量评测方面超过了现有最先进方法。

Conclusion: SC-AGIQA通过引入文本-视觉语义约束和频域细粒度分析，在AI生成图像质量评估任务中取得了更优表现，有助于推动相关应用和研究发展。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [150] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal提出了一种无需稀疏关键点注释即可从视频重建可动画3D动物的新方法，通过引入密集特征网络和层次化对齐策略，实现了更高效、稳定且准确的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏关键点拟合三维动物模型，关键点标注耗时且检测器鲁棒性不足，亟需一种无需关键点标注的三维重建方法。

Method: 提出4D-Animal框架，核心为密集特征网络，将2D特征映射到SMAL参数，结合层次化对齐策略，融合轮廓、局部、像素级及时序信息，实现对视频中动物的三维重建。

Result: 在多个实验中，4D-Animal在精准性与时序连贯性上均优于现有基于模型和非模型的基线方法。

Conclusion: 本方法突破性地免去了对稀疏关键点的依赖，能够高效、准确地重建高质量的三维动物资产，为大规模三维任务和应用带来新的可能。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [151] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: 该论文提出了CoralVQA，这是首个大规模的珊瑚礁视觉问答(VQA)数据集，推动了基于图像的珊瑚礁生态研究。


<details>
  <summary>Details</summary>
Motivation: 由于珊瑚礁生态系统脆弱且重要，需要持续监测，而珊瑚影像的解读过程依赖专业知识，自动化分析能力显得尤为关键。现有Visual Question Answering (VQA)方法缺乏面向珊瑚领域的支持数据集。

Method: 作者与海洋生物学家合作，开发了半自动化流程，收集了12,805张来自67种珊瑚属、涵盖3大洋的真实珊瑚影像，并标注了277,653对覆盖生态与健康维度的问答对，形成CoralVQA数据集。同时，采用这一数据集对多种主流LVLM模型进行基准测试分析。

Result: 实验评估显示，现有LVLM模型在珊瑚领域存在诸多局限，但也揭示了改进空间，CoralVQA数据集为模型表现提供了详尽且具挑战的新型基准。

Conclusion: CoralVQA不仅为VQA领域引入了重要的数据资产，也为未来LVLM模型在珊瑚保护等细分领域的研究与应用奠定基础。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [152] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAPNet的新型全卷积神经网络，用于遥感图像融合（pansharpening），能够更好地融合空间细节与光谱信息，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络（CNN）在遥感图像融合中虽然表现优异，但固定的卷积核忽视了图像不同区域的内容变化，导致空间与光谱细节处理有限。因此需要一种能自适应调整卷积核、感知局部特征的方法。

Method: 提出RAPNet，核心在于RAPConv模块，可针对图像不同区域生成自适应卷积核，实现内容感知的空间特征提取，同时引入PAN-DFF模块利用注意力机制，权衡并融合空间与光谱信息。

Result: 在公开数据集上的定量和定性实验都表明，RAPNet在细节增强和光谱保持方面优于现有主流方法。消融实验也证实了各自适应模块的有效性。

Conclusion: RAPNet利用内容自适应卷积和动态特征融合，有效提升了图像融合质量，为遥感图像融合任务提供了新思路和更优的解决方案。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [153] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出了RefSTAR方法，通过参考高质量人脸图像，改进了盲人脸图像修复的细节和身份保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有盲人脸图像修复方法在保留身份特征方面表现不佳，主要因为参考图像特征引入不当，特别是在修复细致纹理时易导致身份混淆。鉴于人脸的高敏感性和退化图像的不确定性，亟需更有效地利用高质量参考图像以提升修复效果和身份一致性。

Method: 提出RefSTAR方法，融合了参考选择（RefSel）、特征转移与融合、及重构机制。具体包括：1）设计RefSel模块，通过自建的RefSel-HQ数据集（含1万对带mask标注的真值-参考对）进行训练；2）对跨注意力融合易出现的无效融合问题，特设特征融合范式，强制参考图像特征融合到目标；3）创新性地提出带mask的循环一致性损失机制，提升输出图像的参考特征呈现。

Result: 在多种主干网络和公共数据集上的大量实验表明，RefSTAR在身份保留和参考特征转移方面均优于现有方法，修复质量和一致性表现更佳。

Conclusion: RefSTAR有效整合参考高质量人脸特征，显著提升了盲人脸图像修复的身份一致性和细节表现，为该领域提供了新的技术路线。代码和数据集均开源。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [154] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种创新的时间戳预测与地理定位联合方法GT-Loc，能够仅通过图像视觉信息同时预测拍摄时间和地理坐标，并在多个任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉时间预测依赖于地理位置信息，如亮度、色调和影子等视觉线索高度受位置影响，因此时间戳预测与地理定位密切相关。单独建模难以取得理想效果，急需解决两者的依赖关系，实现时间与地点的联合推断。

Method: 作者提出GT-Loc方法，通过建立图像、时间及空间（地理位置）三个独立编码器，将三类特征对齐到统一高维特征空间。针对时间的循环特性，提出基于环面（toroidal surface）的软目标度量学习替代常规对比学习。该方法为图像的拍摄时间（小时和月份）和地理坐标（GPS）联合建模，同时兼容文本条件，实现多模式检索。

Result: 在作者建立的新基准测试下，GT-Loc对时间预测准确率超过了现有方法（包括那些推理阶段可用真实地理坐标作为输入的方法），在标准地理定位任务中表现也十分领先。联合特征空间增强了组合及基于文本的图像检索能力。

Conclusion: GT-Loc有效利用视图、时间和空间信号，实现了时间和地理定位联合预测，不但优于当前主流时间预测方法，还提升了多模态和组合检索的可能性，对相关应用和研究具有重要意义。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [155] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 本文提出了一种基于置信度的自蒸馏方法，实现了更高效且泛化性更好的结肠镜息肉分割，在不增加推理资源消耗的情况下超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习息肉检测/分割模型参数量大、容易过拟合，而且在跨不同数据集时泛化能力较差。传统知识蒸馏和自蒸馏虽然能缓解这些问题，但训练过程资源消耗大。

Method: 作者提出了一种基于置信度的自蒸馏方法。具体方法是在批次训练时，通过保存和利用前一次迭代的数据，与当前迭代计算损失，并引入动态置信系数来引导训练，无需在测试时增加计算或内存开销。

Result: 在息肉分割实验中，该方法不仅在多个临床中心数据集上取得了超越现有最佳模型的效果，还展示了良好的泛化能力。

Conclusion: 该置信度自蒸馏方法能够有效缓解大模型的过拟合与泛化差问题，性能优越，且推理阶段资源效率高，具有实际应用前景。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [156] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出了一个系统且全面的视网膜异常检测基准，并公开了该基准，有效推动了该领域算法的评估与发展。基于以往方法的缺陷，文中进一步提出了NFM-DRA方法，达到了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前视网膜异常检测缺乏标准、全面且公开的数据基准，导致现有方法评估不公平，泛化能力不足，难以推进领域进展。

Method: 作者搭建了一个涵盖丰富异常类型的全新视网膜异常检测基准，并对已有方法进行系统分类和评测。在此基础上，提出NFM-DRA模型，将异常解耦表征(DRA)与正常特征记忆机制相结合，以提升对未见异常的适应能力。

Result: 在新基准上，监督式DRA方法虽整体表现最优，但面对部分未见异常类型时性能大幅下降。NFM-DRA方法显著缓解了这一问题，在综合评价中刷新了SOTA。

Conclusion: 本文基准的建立及NFM-DRA方法为视网膜异常检测的公平评测与方法创新奠定了坚实基础，有助于实际临床场景下的应用推广。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [157] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文比较了多种将相机几何信息融入多视角Transformer的方法，并提出了一种新的相对位置编码方法——PRoPE，有效提升了3D视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer在多视角视觉任务中普及，但如何充分利用相机几何关系来提升3D感知效果仍有待探索。需要开发方法让Transformer能更好地感知和利用各视角之间的相机几何信息。

Method: 实验比较了三种多视角Transformer中的相机调节方法：token级射线映射编码、注意力级相对姿态编码，以及作者提出的Projective Positional Encoding（PRoPE）。PRoPE能将完整的相机视锥信息（包括内参和外参）编码为相对位置表示，并在不同设置（如内参一致或变化、序列长度/内参分布外泛化等）下与其他方法进行性能对比。

Result: 实验显示，采用相对相机调节方式能提升新视角合成等任务表现，并且PRoPE进一步带来性能提升。这一结论适用于不同场景、任务和Transformer模型规模，包括深度估计、空间认知等。PRoPE在泛化能力方面也表现突出。

Conclusion: 引入完整相机几何信息的相对位置编码（PRoPE），能有效提升多视角Transformer在3D视觉相关任务中的表现与泛化能力，优于传统编码方法，适用于多种应用和规模。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [158] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 本研究利用高分辨率（1.5米）遥感数据和深度迁移学习，在国家尺度上首次实现对莫桑比克2100万个农田地块的高精度识别和分割，推动了小农农业空间分析的前沿。


<details>
  <summary>Details</summary>
Motivation: 当前小农农业的政策制定受限于对耕地空间分布和地块大小等基本系统属性了解的不足，而现有全球或区域尺度的数据往往精度不够、难以反映真实情况，致使可持续性提升受阻。

Method: 研究结合高分辨率地球观测影像和深度迁移学习方法，极大降低了手工标注需求，实现在复杂小农体系下，国家范围内高效分割和识别农田地块。通过与现有数据对比、精度评价指标如IoU进行验证，确保方法的准确性和泛化能力。

Result: 成功生成了覆盖莫桑比克全国约80万平方公里、涉及2100万块独立农田的详细分割地图。分辨农田和非农业用地的准确率达93%，地块边界相似度IoU中位数为0.81。地图首次细致揭示了此前全球数据库未能分辨的小规模分散耕地分布及其人口承载区。

Conclusion: 地块规模高度不均、不足，且与人口密度、土地可达性、森林变化等空间变量密切相关，不同经营主体交织。地块大小不仅是农业产出、民生、生态环境效果及其权衡的重要指示器，也为面向小农群体的政策和可持续性管理提供了科学支撑。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [159] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReVQ的高效视觉分词器训练框架，它能大幅降低训练高压缩比VQ-VAE所需的算力和时间，同时保持优秀的重构质量。


<details>
  <summary>Details</summary>
Motivation: 现有高压缩率VQ-VAE模型在训练过程中需要大量的计算资源和时间，这对于研究和应用构成了较高的门槛，亟需找到降低训练成本的新方法。

Method: 该方法提出了Quantize-then-Rectify (ReVQ) 框架，将预训练的VAE高效转化为VQ-VAE，并引入通道多组量化以扩充字典容量，以及后置修正器减少量化误差。此外，ReVQ控制量化噪声在VAE可容忍范围内，从而实现高压缩率和高重构质量。

Result: 在ImageNet图片压缩任务上，ReVQ最多用512个Token重构图像，保持了rFID=1.06的优异重构质量。同时，训练成本大幅下降，只需一张4090显卡约22小时完成训练，而主流方法需32张A100显卡4.5天，其效率提高两个数量级。

Conclusion: ReVQ显著提升了效率与重构质量的平衡，大幅降低了VQ-VAE的训练门槛，为多模态视觉-语言大模型的高效视觉分词器训练提供了新思路。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [160] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种完全自监督的方法，利用DINOv2框架从无标签的相机陷阱视频中学习黑猩猩面部特征嵌入，无需身份标签，表现超过有监督基线。


<details>
  <summary>Details</summary>
Motivation: 动物个体识别是野生动物监测中的瓶颈。手动标注身份费时费力，亟需无需人工标注的新方法。

Method: 作者利用DINOv2自监督学习框架，自动提取并训练黑猩猩面部图像嵌入，全程无需身份标签。采用Vision Transformer架构，通过无监督的方式挖掘相机陷阱数据中的面部区域。

Result: 提出的方法在Bossou等具有挑战性的基准数据集上进行开集再识别测试，无需标签但性能优于有监督基线。

Conclusion: 自监督学习方法可以有效应用于无标签野生动物监测，有望推动大规模、无侵入性的种群研究。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [161] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 本文提出了Spatial ModernBERT模型，结合文本和空间特征，提升财务文档中的表格与键值信息抽取效果，并取得了优秀实验结果。


<details>
  <summary>Details</summary>
Motivation: 传统的财务文档表格与键值对抽取面临格式复杂、版面多样等挑战，准确提取关键信息对审计、数据分析和自动化处理非常重要，因此需要更强的模型来处理文本和空间结构。

Method: 作者提出了Spatial ModernBERT，一种基于transformer并融合空间嵌入的模型。模型包含三个分类头，用于：1）标签分类（如PO号、日期等），2）列索引预测，3）行起始位置识别。模型先在公开大规模表格数据集预训练，再在财务文档专用数据集微调。输出用特殊标签与后处理，实现表格重构与关键信息提取。

Result: 实验证明Spatial ModernBERT在真实财务文档中能结合文本与空间信息，表格和键值对抽取任务均取得了鲁棒且精确的表现。

Conclusion: Spatial ModernBERT有效提升了财务文档中表格及关键信息的自动抽取能力，为审计和数据自动化处理提供了有力工具。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [162] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: 该论文提出了一种多语言安全护栏SEALGuard，有效提升了大模型系统对多语言不安全/越狱输入的检测能力，并显著优于此前的LlamaGuard。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全护栏（如LlamaGuard）在英文检测下表现优异，但在低资源语言（如东南亚语言）中的不安全输入和越狱提示检测效果显著下降，存在安全对齐缺口。

Method: 作者采用低秩适配（LoRA）方法，将通用多语言大模型调整为多语言安全护栏，并构建了包含十种语言、26万条（安全、不安全、越狱）数据的大规模多语言安全对齐数据集SEALSBench，在此基准上评估SEALGuard等系统。

Result: 实验证明LlamaGuard在多语言不安全与越狱检测的防御成功率分别比英文低9%和18%；而SEALGuard在多语言下的防御成功率比LlamaGuard高48%，且在准确率、F1等指标上均表现最优。消融实验还展示了适配策略和模型规模对性能的影响。

Conclusion: SEALGuard作为新型多语言安全护栏，有效弥补了现有LLM护栏在多语言场景下的安全短板，提升了多语言环境下的大模型安全对齐能力。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [163] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 本文评估了当前医学领域大语言模型（LLMs）在问答能力测试中所用数据集的局限性，并提出需建立更高质量的标准化评测体系。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答大模型评测数据集（如MedQA、MedMCQA、PubMedQA等）在临床真实性和透明度上存在不足。研究者需要更科学地评估LLMs在医疗场景下的能力，避免被低质量或有偏数据引导出不准确结论。

Method: 作者回顾并对比了多种广泛应用的评测数据集，系统分析它们在考察严谨性、透明度及与真实临床场景的相关性上的表现，同时分析了医学期刊中的公开挑战题作为无偏评测工具的可行性。

Result: 大多数现有数据集缺乏对临床复杂性的反映、透明度不足且验证流程不完善。虽然挑战题有一定优点，但受限于题量小、范围窄且部分内容已被模型接触过，难以成为理想评测工具。因此，目前主流数据集难以支持LLMs在医疗领域的可靠评估。

Conclusion: 医学大模型评测亟需标准化框架，只有跨机构、政策制定者间的协同合作，才能确保数据集和评测方法的科学性、公正性和对临床复杂性的真实反映。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [164] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文提出了两个用于评估大语言模型工业领域能力的韩语专家级基准数据集：KMMLU-Redux 和 KMMLU-Pro。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评测多集中于学术领域，缺少能全面反映实际工业知识的基准，尤其是在特定语言和国家（如韩国）中。因此，亟需开发更加贴近真实工业和专业场景的评测工具。

Method: 作者构建了两个新的韩语基准：KMMLU-Redux 基于韩国国家技术资格考试题目，在去除重要错误后提升了可靠性；KMMLU-Pro 则基于韩国国家职业执照考试体现专业知识。通过实验，对这些基准的代表性进行了验证。

Result: 实验表明，这两个基准能够全面反映韩国的工业与专业知识，有效评估大语言模型在实际应用场景中的适用性。

Conclusion: 提出的韩语专家级基准为评价大语言模型在工业领域的能力提供了更全面、真实的工具，且数据集已公开发布，将助力相关研究与应用发展。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [165] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无需外部标注的新型自我改进模型引导框架SIMS，可以动态调整大语言模型在推理过程中的行为以更好符合人类偏好。该方法自生成对比样本，通过自我迭代提升效果，并采用新颖排名与采样策略。实验表明SIMS具有极佳的有效性和自适应性。


<details>
  <summary>Details</summary>
Motivation: 现有模型引导技术高度依赖外部标注数据，不仅限制了模型在不同上下文间的适应能力，也使其性能受限于标注质量，因而需要一种脱离外部监督、自动适应上下文的引导方法。

Method: SIMS框架通过自动生成和优化对比样本，采用自我改进迭代策略实现模型引导，同时引入了提示排名和对比采样等新的技术，以增强引导效果的适应性和有效性。

Result: 在多个大语言模型和基准测试中，SIMS在引导有效性和自适应性方面都显著优于现有方法。

Conclusion: SIMS实现了无需外部监督的自适应模型引导，成为实现LLM推理时人类偏好对齐的有前景的方法方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [166] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 该论文通过分析MIMIC-III电子健康记录，发现种族、就医险种和疾病类型等因素与负面标签和怀疑性词汇的频率相关，显示医疗记录中的歧视性语言在某些患者群体中更普遍。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）是医疗团队相互传递信息的重要工具，但同时可能成为扩散和加剧患者污名化的重要途径。明确EHR中存在的歧视性语言特征，有助于理解和改善医疗系统中的结构性偏见。

Method: 作者利用扩展的词汇匹配法和监督学习分类器，识别MIMIC-III数据库中EHR文本的怀疑性词汇及污名化标签。通过Poisson回归模型分析种族、保险类型、疾病类型等变量与歧视性语言频率之间的关系。

Result: 黑人或非裔美国人、Medicare/Medicaid及政府保险、自费患者，以及伴随某些污名化疾病/精神健康状况的患者，其医疗记录中含有歧视性标签的比例更高。不确定性用词的分布趋势类似且男性更常见。护士和社会工作者在描述中使用污名化标签的频率也较高。

Conclusion: 历史上被污名化的患者群体在医疗记录中更容易被贴上歧视性标签，而且这种现象存在于多种医疗提供者之间。EHR中的语言使用会加剧患者污名化问题。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [167] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 本研究通过分析4000多名参与者在Ganzflicker视觉幻觉实验中的自由文本描述，发现视觉意象能力强的人报告更为复杂和自然的幻觉内容，而意象能力弱的人则报告更简单的图形。视觉-语言模型较文本模型更能区分这些感知差异。


<details>
  <summary>Details</summary>
Motivation: 研究个体视觉意象能力差异（如无意象、典型意象、极强意象）对其内在视觉体验复杂性的影响，扩展对视觉系统如何生成和调控幻觉内容的理解。

Method: 利用自然语言处理技术，分析4000多名参与者在Ganzflicker诱发幻觉中的自由文本描述，并比较拥有不同视觉意象特征者的描述内容。还用视觉-语言模型与文本语言模型的嵌入向量分析这些描述之间的差异。

Result: 视觉意象能力强者报告复杂、具自然特性的幻觉内容，能力弱者则为简单几何图形。视觉-语言模型对这些主观差异的捕捉能力优于文本模型。强视觉意象者的描述语言也体现出更丰富的感官和动作关联特征。

Conclusion: 视觉意象能力影响Ganzflicker诱发的视觉幻觉内容复杂度，个体在视觉信息处理的早期区域与更高阶大脑区域的协调性或是这种差异的神经基础。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [168] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一种针对大语言模型的线性化框架，能在保持性能的同时显著降低长上下文生成的计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的大语言模型在处理很长文本时，因softmax注意力和KV缓存的二次复杂度，导致内存和计算瓶颈，限制了模型扩展到无限长上下文的能力。

Method: Lizard提出了一种可灵活适配的次二次复杂度注意力机制，能较好地逼近softmax注意力。与以往线性化方法不同，加入了最新线性模型启发的门控模块，实现自适应记忆控制。此外，方法结合了门控线性注意力用于全局语境压缩，以及带有元记忆的滑动窗口注意力，兼顾长距离依赖和局部细粒度交互。同时引入硬件感知的算法提升训练效率。

Result: Lizard在标准语言建模任务中的表现接近原有模型（几乎无性能损失），且在5-shot MMLU和关联召回等任务上比现有线性化方法大幅提升，MMLU提升高达18分。

Conclusion: Lizard极大缓解了长上下文生成中的资源瓶颈，保证模型性能的同时显著优于现有类似方法，有望推动无限上下文大模型的实际应用。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [169] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: 本论文提出了ALIGN系统，一种通过提示词实现大语言模型（LLM）决策行为个性化对齐的新方法，并对比分析了不同对齐算法在不同应用领域的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于辅助决策，用户的价值观和偏好差异带来了决策对齐和个性化的新挑战。现有对比工具局限于知识问答等基准评测，缺乏对个性化决策对齐的支持，因此需要新的方法实现对多样化用户价值的动态适应。

Method: 作者提出ALIGN系统，通过提示词将LLM对齐到一组细粒度属性，实现个性化决策。该系统支持强大的配置管理、结构化输出和多种易于切换的对齐算法，并可针对不同因素灵活实现分析。系统前端可直观对比不同LLM的对齐效果，后端高度模块化，便于算法扩展。作者还在群体意见调查中的人口属性对齐、医疗分诊中的价值对齐两个领域，定量对比了多种对齐方法。

Result: ALIGN系统实现了多种决策对齐算法在不同领域的灵活部署，并通过实验证明系统在实现人口属性和价值对齐时具备定量评估能力，为LLM个性化提供了新工具。

Conclusion: ALIGN框架为LLM决策个性化和可靠性研究提供了开放工具，支持未来在LLM个性化、可靠与负责任应用方面的深入探究和实践。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [170] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: 该论文提出了OpenCodeReasoning-II数据集，包含大量代码生成与批判样本，并通过双阶段微调方法，显著提升了大模型的代码生成和批判能力，扩展了代码评测基准。


<details>
  <summary>Details</summary>
Motivation: 目前高质量、规模化的数据集是提升大模型代码生成与批判能力的关键，现有相关公开数据集规模不足，且评测基准不够完善，限制了模型能力的发展和公平评测。

Method: 作者构建了包含250万组问题-解决方案-批判三元组的新数据集，并使用两阶段的有监督微调：第一阶段针对代码生成任务微调，第二阶段则联合训练代码生成和批判两个任务。

Result: 微调后的Qwen2.5-Instruct模型在代码生成任务上表现达到或超越现有最优的开源蒸馏模型，并在结合生成与批判模型后，编程竞赛表现显著提升。此外扩展了LiveCodeBench支持C++语言，提升了基准评测能力。

Conclusion: 大规模、高质量的数据集和结合批判能力的微调方法能显著提升LLM在代码生成领域的实用性和评测全面性。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [171] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 提出了一种动态参数记忆（DPM）机制，使语音大语言模型（SLLM）能够突破上下文窗口限制，有效处理和理解长语音下的情感信息，提升了对话情感识别（ERC）的效果。


<details>
  <summary>Details</summary>
Motivation: 现有SLLM用于语音情感识别时，由于语音模态高采样率和有限上下文窗口，处理长音频困难，并且现有压缩方法忽视了情感跨句连续性，限制情感理解能力。

Method: 提出动态参数记忆（DPM）机制，将语句级语义和情感编码到推理过程中的临时LoRA模块中，实现了对“上下文信息”的动态记忆，从而能在有限上下文窗口下处理无限长度的音频输入。该机制被集成到训练好的情感SLLM主干模型，专用于对话情感识别。

Result: 在IEMOCAP数据集上实验，DPM机制显著提升了SLLM处理长音频时的情感识别能力，取得了该任务中的最新最好表现（state-of-the-art）。

Conclusion: DPM机制有效解决了SLLM的上下文窗口受限问题，能够更好地理解跨句连续的情感，实现更强的长序列情感识别能力，对未来对话理解和语音AI有重要参考价值。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [172] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本论文提出了CompassJudger-2，一款通用型大语言模型评审（Judge）模型，通过多领域、任务驱动数据和创新的训练目标显著提升了评审的泛化能力与准确性，并建立了新的评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评审（Judge）模型存在专业性窄、鲁棒性不足等问题，难以对LLM进行全面、公正的评价。因此，迫切需要更加通用且强健的Judge模型来提高评测结果的可信度与适用性。

Method: 采用了多领域、任务驱动的数据策划策略，并引入基于可验证奖励信号的监督方式，通过拒绝采样提升模型的批判性推理能力。此外，提出了边际策略梯度损失函数（margin policy gradient loss）优化训练过程，并推出了用于系统评测的Judge标准基准JudgerBenchV2。

Result: CompassJudger-2在多个Judge模型和奖励基准测试中表现出色，7B规模模型在判别准确率上接近或超越了DeepSeek-V3和Qwen3-235B-A22B等更大模型。JudgerBenchV2也有效提升了Judge模型的评测合理性与一致性。

Conclusion: CompassJudger-2提升了大语言模型评审的泛化能力和鲁棒性，推动了Judge模型技术和标准的发展，为后续LLM评价体系的稳定和进步奠定了基础。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [173] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: 本文提出了一种名为OPENXRD的开放式书本问答管道，旨在用于结晶学问题，通过GPT-4.5生成简洁的专业知识辅助小模型理解X射线衍射相关概念，并在多个模型间进行了效果评估。


<details>
  <summary>Details</summary>
Motivation: 传统上，晶体学问题回答依赖于扫描课本，这存在侵权和效率低下问题。小型模型缺乏领域知识，难以胜任结晶学高阶问题。研究需要一种既合法又高效的方式提升这些模型的专业性。

Method: OPENXRD系统通过GPT-4.5为小模型自动生成领域内紧凑的知识参考，不直接利用课本内容。以217道高阶XRD问题为评测集，将GPT-4、LLaVA等多种模型分别在有/无知识支持材料的条件下进行比较。

Result: 实验显示，借助GPT-4.5生成的知识摘要，尤其是基础预训练较少的模型，其准确率均有显著提升。OPENXRD能有效弥补小模型在晶体学领域的知识空白。

Conclusion: OPENXRD证明了基于AI自动生成的开放书本问答系统在材料科学等领域的价值，并为日后扩展到包含图像和更广泛科学NLP工具奠定基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [174] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 本文提出了一种轻量且高效的战略对话欺骗检测模型PU-Lie，通过PU学习应对只有少部分已标记欺骗数据的极度类别不均衡问题，并取得了最佳宏F1分数0.60。


<details>
  <summary>Details</summary>
Motivation: 战略对话中欺骗检测因语言微妙且数据极度不均衡十分困难，尤其在Diplomacy数据集上，欺骗信息占比不到5%，传统二分类器难以应对。作者旨在提升对罕见欺骗行为的检测能力。

Method: 提出结合冻结BERT嵌入、可解释语言及游戏特征、以及PU（Positive-Unlabeled）学习目标的模型PU-Lie，针对极少量标记欺骗数据和大量未标记数据，通过PU学习重点建模难得但关键的欺骗类别。

Result: 该模型在Diplomacy数据集上实现了新的宏F1分数0.60，且相比以往方法训练参数减少超过650倍。通过七类模型的系统评测和消融实验，展示了PU学习、语言可解释性和基于说话人特征的有效性。

Conclusion: PU-Lie模型在极度类别不均的战略对话中能够准确检测到欺骗，显示出PU学习对此类问题的优越性，强调应优先提升欺骗检测能力而非真实信息识别。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [175] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 本文提出了RAMA系统，通过多智能体架构和检索增强机制解决多模态虚假信息的自动核查问题，在多项基准数据集上表现优异，提升了对模糊或不确定陈述的验证能力。


<details>
  <summary>Details</summary>
Motivation: 随着多模态虚假信息快速增长，自动事实核查面临新挑战，尤其是在信息模糊或上下文不足时。作者旨在提升系统对多模态（文字+图像等）信息的核查能力。

Method: RAMA系统主要创新点包括：1）通过策略性查询，将多模态陈述转化为精准的网络检索查询；2）从多元权威来源聚合跨验证证据；3）采用多智能体集成架构，整合多种多模态大模型和提示变体的优势。

Result: 大量实验表明，RAMA在基准数据集上优于现有方法，特别是在处理模糊或不大可能的陈述时，通过基于检索证据提升了验证准确性。

Conclusion: 集成网络证据和多智能体推理对于可信多媒体核查至关重要。RAMA为更可靠、可扩展的事实核查提供了新思路，系统代码将公开发布。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [176] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 本文提出了一种细致微调方法，通过识别和剪除大型语言模型（LLM）中特定于数据集的神经元，以提升模型在新任务下的泛化能力，并在多项选择基准测试中取得了优于以往方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在特定数据集上会发展出依赖于数据集相关性的特定推理机制，在原任务上表现优秀，但在新任务或分布变化下易出现泛化能力不足的问题。因此，急需提升模型对新任务的迁移泛化能力。

Method: 作者提出了一种基于剪枝的微调方法，利用Integrated Gradients方法衡量神经元对高置信度预测的影响，并定位出对数据集特异机制贡献过大的神经元。通过剪除这些神经元，引导模型转而依赖更具泛化能力的表示。

Result: 在多个多项选择任务基准上评测，该剪枝微调方法显著提升了模型的泛化性能，效果优于现有其它非剪枝适应方法。

Conclusion: 基于识别与剪枝数据集特异机制神经元的微调方法，可显著强化LLM的泛化能力，提升对新任务的适应性，优于传统适应性方法。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [177] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文通过大规模数据收集和处理，打造了目前最大的藏语训练语料库，并在此基础上训练了多语种大模型Banzhida，显著提升了藏语任务的表现。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言，在现有大模型中支持不足，主要由于高质量训练语料稀缺。作者希望通过构建大规模藏语预训练语料和模型，改善藏语在生成式AI中的能力。

Method: 1. 聚合多种来源的数据，建立迄今为止最大的藏语预训练语料库。2. 针对藏语设计专门的数据清洗、处理流程。3. 在多语种大模型基础上，进行持续预训练/微调，形成Banzhida大模型。4. 构建新的高质量藏语评测基准，并结合已有公共基准评测模型性能。

Result: Banzhida大模型在多项任务上，无论面对同规模的开源多语种模型还是专为藏语设计的模型，均取得了显著且持续的性能提升。

Conclusion: Banzhida模型为藏语生成式AI设定了新基线，有效推动了低资源语言在大模型领域的发展。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [178] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 该论文介绍了一个新的气候变化视觉隐喻图像数据库，对比隐喻与字面图像在人类认知、审美和情感反应上的差异。


<details>
  <summary>Details</summary>
Motivation: 气候变化是极其复杂的环境问题，视觉隐喻有助于沟通与传播，但相关实证研究稀少，尤其缺乏系统数据库与对比分析。

Method: 构建气候变化视觉隐喻数据库（MetaClimage），包括隐喻与字面图像。通过人类受试者评价难度、有效性、艺术性与情感激发程度，并分析参与者生成的标签及其情感语义特征。

Result: 隐喻图像更难理解但更具美感，对传达效果与情感激发程度无显著提升。但高认知需求者对隐喻图像 arousal 更高。隐喻图像产生更多描述性标签，且标签更具积极意义和主导感。

Conclusion: 视觉隐喻尽管提升了认知负担，但促使更深层次的思考和抽象化，并带来正面审美体验；未来环境传播应权衡其认知消耗与潜在收益。本研究亦为学界提供了宝贵的视觉隐喻数据库。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [179] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: 本论文介绍了Swa-bhasha Resource Hub，这是一个面向罗马化僧伽罗语到僧伽罗语音译的资源和工具平台。该平台为研究和应用提供了支撑，并公开了这些数据集与工具。


<details>
  <summary>Details</summary>
Motivation: 面对罗马化僧伽罗语与僧伽罗语文字间的转换需求，缺乏系统的数据资源和工具限制了相关NLP研究与应用的发展。

Method: 作者收集、整理并公开了2020-2025年间开发的罗马化僧伽罗语到僧伽罗语音译资源和算法，建立了Swa-bhasha Resource Hub，同时对现有的音译应用进行了对比分析。

Result: 该平台汇集了现有数据集和工具，并将其公开，显著推动了僧伽罗语NLP特别是音译模型的训练与相关应用开发。

Conclusion: Swa-bhasha Resource Hub为罗马化僧伽罗语和僧伽罗语之间的转换提供了宝贵的开放资源，促进了该领域的研究与应用进步。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [180] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 该论文提出了一种基于心理学的幽默分解机制（HDM），结合了链式思维（CoT）和幽默理论，显著提升了大语言模型在幽默翻译任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大多数大语言模型虽然能够胜任一般翻译任务，但在幽默翻译领域存在语言干扰和幽默感缺失的问题。为此，研究者希望通过提升模型的幽默处理能力，推动跨文化交流。

Method: 作者提出了一种心理学启发的幽默分解机制（HDM），该机制利用链式思维（CoT）模拟人类思维过程，优化幽默文本的可读性，并融入幽默理论以增强译文的幽默元素。

Result: 在开源幽默数据集上的自动化实验表明，该方法显著提升了幽默翻译的质量，幽默提升7.75%、流畅度提升2.81%、连贯性提升6.13%。

Conclusion: 心理学启发的HDM机制能够有效提升大语言模型的幽默翻译能力，在幽默、流畅性和连贯性方面均有显著改进。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [181] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 该论文提出了ClaritySpeech系统，用于模糊痴呆症患者的语音信息，同时改善语音转录准确率并保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 痴呆症会导致患者语音特征异常，影响交流，也给隐私保护带来新挑战。而现有自动语音转写（ASR）技术难以处理这类非常规语音，影响病患的可访问性和隐私。

Method: 作者提出了ClaritySpeech框架，整合ASR、文本模糊和零样本TTS技术，无需特定数据微调，可在低数据环境下纠正痴呆症相关语音，同时尽可能保持说话者身份。

Result: 在ADReSS和ADReSSo数据集及不同敌对条件下，系统平均F1分数分别下降16%和10%，达到了50%的说话人相似性。此外，系统能大幅提升转写准确率（WER由0.73降至0.08）及语音质量评分（由1.65升至2.15）。

Conclusion: ClaritySpeech能够在保护说话人隐私与身份的前提下，有效提升痴呆症患者语音的可访问性和质量，解决了现有技术难以处理非常规语音的问题。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [182] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: 本文提出了DATE-LM，这是一个用于系统性评估数据归因方法的统一基准，适用于大语言模型（LLM）的实际应用，并通过大规模实验展示了现有归因方法的特点与局限。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法在LLM相关研究与应用中日益重要，但在LLM语境下的系统性评估仍有关键空白。缺乏一套统一、便于使用且适配多任务和架构的评估标准，制约了相关研究进展。

Method: 提出并实现了DATE-LM基准，涵盖三大评价任务：训练数据选择、有害/有偏见内容过滤、事实归因。该基准允许配置和运行大规模评测，并适配不同LLM架构。作者利用DATE-LM对多种主流数据归因方法开展系统测试与比较。

Result: 测试结果显示，没有一种归因方法能在所有任务上占优，各方法在对比简单基线方案时各有取舍，且性能对具体任务设计具有较大敏感性。

Conclusion: DATE-LM为LLM数据归因方法的标准化评测提供了基础，公开的排行榜有助于方法快速比较和社区参与，有望推动未来数据归因相关研究。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [183] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 研究针对医学文本二分类任务优化了DRAGON Longformer基础模型，通过结构化医学案例实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于医学文本包含丰富专业术语且结构复杂，现有通用预训练模型难以高效处理临床文本，因此需要通过领域适配优化技术提升模型在医疗场景下的表现。

Method: 使用500个结构化医学案例（400训练集、100验证集），对预训练的dragon-longformer模型进行超参数调整（如序列长度、学习率、训练轮数等）、领域特定预处理和结构更改，并加入医学专有术语提升模型医学文本理解力。

Result: 优化后模型精度由72.0%提升至85.2%，准确率、查准率、查全率和F1-score均明显提高，统计分析证明结果具有高度显著性（p<0.001）。

Conclusion: 优化后的Longformer模型能更好理解医学术语和结构化临床文本，在医疗文本分类任务中表现优越，具备广泛应用医疗NLP的潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [184] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文介绍了CoNLL-2013共享任务，该任务聚焦于语法错误纠正，内容涵盖任务定义、数据集、评估方法以及参赛团队的方法与结果。


<details>
  <summary>Details</summary>
Motivation: 近年来，由于第二语言学习者和自然语言处理需求增长，自动语法错误识别与纠正变得越来越重要。因此，作者组织了一个共享任务，推动该领域的研究和技术发展。

Method: 论文详细说明了共享任务如何界定语法错误纠正任务，构建和分发了数据集，选取并描述了用于任务自动评测的评估指标与计分器。同时，对各参赛队伍采用的方法进行了总结归纳。

Result: 汇总展示了各参赛队伍在语法错误纠正任务中的评测结果，比较了不同方法的表现。

Conclusion: 本共享任务为语法错误纠正领域提供了标准任务与数据集，有效促进各类技术的交流与对比评测，推动了该领域的发展。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [185] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文综述了RAG（检索增强生成）与推理方法的结合，总结了当前技术的进展与挑战，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: RAG方法能有效增强大模型的事实性，但不善于多步推理；以推理为导向的方法则容易出现事实错误。作者希望结合两者优势，推动更强大、可靠的知识推理系统发展。

Method: 作者分别梳理了推理增强RAG（在RAG各阶段引入更强推理能力）和RAG增强推理（用检索知识补足复杂推理所需信息），并聚焦于最近兴起的、推理与检索深度融合的系统架构。文章系统整理了相关方法、数据集与挑战，并提供了前沿方法的系统化对比。

Result: 综合分析表明，推理与检索的协同可以显著提升知识密集型任务表现，涌现了一系列新框架在基准测试上取得领先效果。文中还对各类方法、工具与挑战做了梳理和归纳，并整理了资源合集。

Conclusion: 推理与检索的进一步深度结合是未来趋势，将促使RAG系统在有效性、多模态、可信性及以人为本方面取得更大突破，本文为相关领域研究提供了系统框架与资源指引。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [186] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 本文提出了多模态讽刺生成数据集M2SaG，并基于此设计了结合PPO和对比学习的讽刺文本生成框架ViSP。ViSP在多项指标上优于现有的大模型，生成文本具有更强的讽刺性并更高事实不符程度。


<details>
  <summary>Details</summary>
Motivation: 讽刺作为复杂情感的一种，当前研究主要关注识别，自动生成方面尤其是基于图文的多模态生成研究严重不足，现有数据集图文内容与讽刺意图常常不匹配。基于此，该文致力于推动多模态讽刺文本自动生成。

Method: 构建了M2SaG多模态讽刺数据集（4970条：图片、讽刺文本和目标），并提出ViSP——融合PPO（基于DIP奖赏引导生成）和对比学习（促使偏好高奖赏输出）的生成系统。

Result: ViSP在五组主流指标下全面优于各类基线和大模型。生成文本在讽刺分数、事实不符程度等统计指标上显著提升于原始数据（如讽刺分数0.898 vs 0.770）。

Conclusion: 多模态信息能有效提升讽刺文本生成质量。ViSP框架结合新数据集为讽刺生成任务树立了新标杆，暴露了大模型对此任务的局限性。数据集和代码公开，有助于领域后续研究。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [187] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种结合LLM和数据增强的新方法，用于提升基于方面的情感分析（ABSA）任务的性能，特别是在标注数据稀缺和类别不均衡的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在ABSA任务中表现良好，但受限于短文本内容和标注数据稀缺（特别是标签不均衡，大多为正面），难以充分学习语境信息。因此，亟需通过高质量的数据增强技术提升ABSA模型表现。

Method: 本文利用LLM生成合成训练数据扩充原训练集，同时通过强化学习优化数据增强过程中LLM的表现，以实现数据规模扩大和标签均衡，并提升合成数据的质量。

Result: 在英文ABSA基准数据集上的实验结果及分析表明，所提方法明显优于强基线方法和大部分已有工作。

Conclusion: 论文方法可有效改进ABSA模型，尤其适用于训练数据稀缺和类别分布不均的场景，在实验中显示了优越性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [188] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax 是一个支持多智能体协作的端到端框架，通过协议驱动实现智能体间高效沟通、记忆复用和动态任务分解，从而提升多智能体系统的扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前企业场景对能够自主适应复杂与动态任务的智能系统需求高，但传统的单一AI系统缺乏协调、记忆复用和任务拆解能力，难以扩展。

Method: 提出GoalfyMax框架，核心包括基于Model Context Protocol（MCP）的智能体间通信层，以及可保留任务动机与执行轨迹的Experience Pack（XP）分层记忆系统，配合多轮对话、长短期记忆与安全验证，支持实时策略调整。

Result: 实验证明在复杂任务编排基准及案例分析中，GoalfyMax在适应性、协调能力和经验复用上均优于基线框架。

Conclusion: GoalfyMax为多智能体智能系统提供了易于扩展、可持续学习的基础，具备强大的实际应用前景。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [189] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了Ref-Long基准，用于系统评估长上下文语言模型（LCLMs）在长文本引用任务中的表现，发现目前主流模型在该任务上仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 虽然LCLMs在处理长文本任务方面表现出色，但特定的长文本引用能力尚未被系统研究。该能力对于模型理解和追踪文本中关键信息的上下文关系至关重要。为弥补这一研究空白，作者提出了专门用于评估长上下文引用任务的基准。

Method: 提出Ref-Long基准测试，要求模型在长上下文中根据给定的key识别和定位引用该key的具体文档索引。基准分为合成到真实三类数据集，全面考察LCLMs的引用能力。对13个主流LCLMs进行实验，并配合人工评测、任务格式调整和模型微调等多方面分析。

Result: 实验显示，包括GPT-4o等先进模型在内的13个LCLMs在长文本引用任务上表现都有明显不足。同时，通过多维度分析揭示了模型在此类任务上的典型弱点。

Conclusion: 尽管LCLMs强大，但在长上下文引用理解上依然面临挑战。Ref-Long为后续研究提供了统一测评基准，并揭示了改进方向。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [190] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 本文系统性分析了大语言模型（LLMs）在机器翻译及其评价任务中对提示信息中人为或合成错误的敏感性，结果发现提示质量对模型表现影响重大。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译任务中表现优异，但它们对提示（prompt）中的错误非常敏感。了解不同类型和程度的提示错误如何影响模型翻译及其评价，对提升模型可靠性和实际应用具有重要意义。

Method: 作者设计了包含各类人类常见和合成错误的提示，并在机器翻译与机器翻译评价两个任务中系统评测LLMs的表现，既给出了定量结果，也做了定性分析，研究不同噪声类型及其强度的具体影响。

Result: 提示中的高频或严重错误会显著降低模型翻译质量，有时甚至好于信息量少但无错误的提示。字符级及混合型噪声对性能打击最大，短语干扰影响较小。模型主要因为指令理解变差而非翻译能力本身受损，在人类难以理解的提示下模型仍能部分完成翻译。

Conclusion: 提示质量极大决定了LLM在翻译及相关任务上的表现，尤其是高噪声下，指令理解障碍成为主要瓶颈。实际应用中需充分关注和优化用户交互中的提示质量，以保证模型输出效果。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [191] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 本文探讨了如何将定义建模（definition modeling）技术应用到白俄罗斯语，并提出了一个包含43,150条定义的新数据集。实验显示，模型迁移所需数据量较少，但现有自动评价指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 词汇定义建模有助于词典编纂，可以支持更多方言和语言。然而，目前尚不清楚如何利用已有模型来支持尚未覆盖的语言。作者选择对白俄罗斯语进行尝试。

Method: 作者提出了一个包含43,150条白俄罗斯语定义的数据集，并对现有定义建模系统进行了微调和适配，用以生成白俄罗斯语词汇的定义。

Result: 实验结果表明，模型迁移适配对白俄罗斯语的定义建模需求数据量很小，但自动评价指标对生成定义的评估仍有不足。

Conclusion: 只需少量数据即可将现有定义建模模型迁移到新语言（如白俄罗斯语），但评价这些模型的自动指标还需要进一步改进。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [192] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出了NMIXX跨语种金融句子嵌入模型及韩语金融语义文本相似性测试集KorFinSTS，显著提升了韩语和英语金融语义建模能力。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型在金融领域、特别是低资源语言如韩语，难以很好地捕捉专业语义，主要因为行业术语、时序语义变化和双语词汇的不对齐。因此，亟需更有效的金融领域多语种语义表征方法和评测基准。

Method: 作者提出NMIXX跨语种嵌入模型，使用18800对高置信度三元组（包括领域内同义句、基于语义漂移的强负样本和中韩精确翻译）进行微调。同时发布KorFinSTS，覆盖新闻、公告、研报、规章等多类型的1921对韩文金融句语义相似性测试集。

Result: NMIXX（多语种bge-m3变体）在金融STS基准（英文、韩文）上，Spearman相关系数分别提升了0.10和0.22，显著优于其他开源基线模型，并对通用STS表现牺牲有限。此外，韩文tokens覆盖率高的模型适应性更强，显示tokenizer设计对低资源跨语种任务的重要作用。

Conclusion: 本文提出并公开了面向金融的强跨语种句子嵌入模型和评测基准，有效推动了低资源金融语义建模和多语言表征学习的进步。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [193] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: 论文介绍了Python库SpreadPy，用于在认知领域模拟扩散激活过程，支持对单层和多层网络的认知函数关系进行数值仿真。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏便捷工具来在认知和心理网络中模拟和分析扩散激活动态，限制了对结构与功能关系的系统性研究，尤其是在个体差异和临床表现方面。

Method: 作者实现了一个灵活的Python工具SpreadPy，能够在基于实证或理论构建的网络结构上进行模拟，通过对比模拟结果与认知理论和经验数据进行验证。

Result: 通过三项案例研究展示SpreadPy的应用：（1）高低数学焦虑学生的知识网络激活过程差异；（2）不同任务难度下创造力任务的激活轨迹及其对认知负荷的反映；（3）失语症患者词汇网络激活与命名任务中的错误类型相关。

Conclusion: SpreadPy为研究认知过程中的结构-功能关系、个体差异和临床障碍提供了机制层面的建模工具，促进了心理学、神经科学和教育领域的可重复性研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [194] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 本文首次系统研究了阿拉伯语知识编辑（KE），对多种方法在阿拉伯语下的表现和跨语言迁移性进行了评估，并公开了相关基准与数据。


<details>
  <summary>Details</summary>
Motivation: 虽然知识编辑在英文领域已有广泛研究，但其在形态丰富的语言（如阿拉伯语）上的表现和特性尚未被充分探索，存在研究空白。

Method: 评估了四种知识编辑方法（ROME、MEMIT、ICE 和 LTE）在阿拉伯语版ZsRE及Counterfact基准上的表现，涉及多语和跨语种设置。针对LTE方法，将其扩展至多语言环境，并采用阿-英联合训练。实验采用Llama-2-7B-chat模型。

Result: 参数方法在跨语通用性方面表现较弱，指令微调型方法更为稳健。对LTE方法进行阿-英联合训练后，阿拉伯语编辑能力及知识迁移效果均有提升。

Conclusion: 阿拉伯语知识编辑具备独特挑战和应用场景。联合多语训练可提升编辑和迁移能力。发布相关基准和多语训练数据，推动该领域后续研究。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [195] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 该论文提出利用Group-Relative Policy Optimization（GRPO）方法，结合BGE-M3嵌入作为语义奖励，有效提升泰国法律领域问答系统的推理能力与引用准确性，同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前检索增强生成（RAG）系统在泰国法律问答上的表现有限，特别是在复杂法律推理方面。传统微调方法和大型模型判别器成本高、效果有限，因此需要更高效且表现更佳的新方法。

Method: 作者提出GRPO方法，对LLM进行微调，使其对法律引用和答案质量表现更优。利用BGE-M3嵌入衡量语义相似性，替代消耗高的大模型判官，作为奖励信号来优化生成答案的质量和相关性。

Result: 在NitiBench基准测试上，该方法的法律引用F1分数比基础模型提升最高达90%，联合质量指标比传统指令微调提升31%。

Conclusion: GRPO方法在提高复杂法律推理能力和引用准确性的同时，显著降低了训练及推理成本，为泰国法律领域LLM的发展提供了高效且有效的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [196] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了MCEval，一个用于多语言、多文化情境下评估大语言模型文化偏见与理解能力的新框架。通过系统实验揭示了模型在不同语言和文化下的表现差异及公平性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在文化偏见和跨文化理解能力有限的问题。面对全球用户的多样化需求，缺乏能够系统性评估模型在不同文化、语言下表现的工具和框架。

Method: 本文提出了MCEval评估框架，利用动态文化问题构建，并通过反事实重述（Counterfactual Rephrasing）和混淆变量重述（Confounder Rephrasing）进行因果分析。评估覆盖13种文化和语言，设计了文化意识及文化偏见两类评估任务，分别采集了39897例文化意识和17940例文化偏见数据。

Result: 实验表明，不同语言与文化环境下大语言模型性能存在显著差异。最佳的文化表现不仅与训练数据分布相关，也与语言和文化的匹配度有关。此外，模型在英语情景下表现良好的方法，可能在其他语言文化环境下引发不公平问题。

Conclusion: MCEval为大语言模型多语言文化能力的综合评估提供了全新维度，有助于深入理解并提升模型的跨文化公平性与适用性。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [197] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 该论文系统分析了大语言模型在隐空间中的几何结构，发现高级语义信息集中在线性可分的低维子空间，并提出了利用这一特性的防御方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的行为解释和对齐改进依赖于对其隐空间结构的理解。目前尚不清楚语义信息在隐空间中的组织方式，理解这些结构有助于开发更高效的模型防御与对齐技术。

Method: 论文对11个decoder-only transformer模型隐藏状态开展大规模实证分析，涵盖6个科学主题、12层网络。研究分析了不同语义任务下，高层语义信息在隐空间的表示和分布，并通过干预实验探索其可操作性。进一步，作者以单层MLP分类器为例，测试了基于隐空间几何的攻击检测能力。

Result: 实验发现高级语义信息通常集中在隐空间中的低维子空间，并且在不同领域之间表现出良好的线性可分性。这种分离性在深层网络和结构化推理任务中更加明显。基于隐藏空间几何结构的简易干预可以高效捕捉推理模式，实现有针对性的防御。简单的MLP分类器即可高精度识别对抗攻击和恶意提示。

Conclusion: 大语言模型的隐空间具备可解释、可利用的几何分布特性，这些特性能支撑高效的模型防御与对齐解决方案。基于几何的工具和干预不仅可检测、缓解有害内容，还为未来模型安全和控制提供了新方向。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [198] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了一种自适应课程学习策略，利用预训练语言模型预测样本难度分数，动态调整微调样本的顺序，从而提升自然语言理解任务的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 目前主流课程学习方法采用手动设计的难度度量（如文本长度），但这些标准未必能准确反映模型视角下的真实难度，影响训练的有效性。

Method: 提出了一种基于自适应课程学习的方法，先利用预训练语言模型为样本打分，表示其难度。然后基于这些难度分数，尝试多种样本排序（如易到难、难到易、混合采样）进行微调训练。

Result: 在四个自然语言理解数据集（包括二分类和多分类任务）上的实验表明，该方法相比随机采样能显著加速模型收敛，并取得更优性能。

Conclusion: 让模型自身判断样本难度、据此调整训练顺序，可有效提升NLP任务中的训练效率和效果，优于传统手工难度指标的课程学习方法。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [199] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 本文提出了点击诱饵（clickbait）的新定义，强调好奇心缺口是其核心特征，并发布了首个西班牙语点击诱饵检测开源数据集TA1C。


<details>
  <summary>Details</summary>
Motivation: 现有的点击诱饵定义不统一，容易与轰动主义和承诺不符的标题混淆，缺乏明确区分标准，因此需要澄清其关键特征。

Method: 作者对点击诱饵的定义进行了理论梳理，明确划定其与相关概念的界限；参照新定义，手动标注并制作了3500条西班牙语推文组成的数据集TA1C，并用高效注释规范提升了标注一致性，达到了0.825的Fleiss' Kappa。随后实现并测试了多组基线模型。

Result: 发布了首个公开西班牙语点击诱饵检测数据集TA1C，基线模型在该数据集上取得了0.84的F1分数。

Conclusion: 观点清晰地区分了点击诱饵与相关概念，为学界对点击诱饵的研究提供了新的定义和资源，并客观量化了基线检测效果。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [200] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 本论文分析了大语言模型在未见任务中利用上下文学习的机制，特别研究了模型如何通过“off-by-one addition”任务实现泛化，并揭示了模型内部的函数诱导机制及其推广性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能够通过上下文学习完成从未遇到的任务，但其内部泛化机制尚不清楚。论文以“off-by-one加法”作为切入点，旨在揭示模型实现任务泛化的具体内部过程。

Method: 论文采用了电路风格的可解释性技术（如路径修补，path patching），对模型执行“off-by-one addition”任务时的内部计算过程进行分析，探索模型如何将标准加法泛化为“错位加法”。

Result: 1. 发现了一个可以解释模型如何将标准加法泛化到“off-by-one加法”的函数诱导机制，该机制比现有的归纳头机制（induction head）更具抽象性。
2. 诱导+1函数由多个注意力头并行协作完成，每个注意力头各自提供部分信息。
3. 该机制在更广泛的任务中被复用，包括合成任务和算法任务，如移位选择题和八进制加法。

Conclusion: 论文揭示了语言模型内部存在可复用、可组合的结构，这些结构支持了模型在任务层面的泛化能力，从而加深了我们对大语言模型泛化机制的理解。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [201] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种通过结合分层文本分割和聚类的新型RAG（检索增强生成）系统分块方法，用以生成更有语义连贯性的知识块，提升了检索相关信息的准确度和上下文相关性。实验表明，该方法在多个数据集上优于传统分块策略。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG分块方法没有很好地考虑文本的语义结构，导致检索到的信息碎片化、缺乏上下文语义。这阻碍了大语言模型对于外部知识的高效利用。因此，作者希望通过改进分块策略，提升检索到的信息相关性和有效性。

Method: 作者提出了一种结合分层文本分割与聚类的方法，根据文本的结构将内容分段，再通过聚类形成更具语义连贯性的知识块。推断阶段，检索不仅用段落层面的向量表征，还结合聚类层面的表征，从而提升了检索精度和上下文相关性。

Result: 在NarrativeQA、QuALITY和QASPER等数据集上的评测结果显示，本文提出的分块及检索方法相比传统分块方法效果更佳，检索到的信息更加精确和语义关联性更强。

Conclusion: 本文的方法在RAG系统中提升了检索块的语义结构与相关性，从而增强了LLMs对外部知识的利用能力，为后续RAG研究提供了更高效的分块和检索框架。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [202] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: 论文提出TinyRM，一种参数量极小但性能媲美主流大模型的小型 reward model（仅4亿参数），专用于人类反馈强化学习中的奖励建模，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型（上百亿参数）虽擅长进行人类反馈下的奖励建模和推理，但其在实际部署时高昂的推理消耗成为限制。研究需求是能否有体积小、成本低但效果强的奖励模型。

Method: 作者提出TinyRM系列小型号、双向Masked语言模型（MLM）。方法结合了几个关键技术：1）FLAN式提示；2）方向性低秩适应（DoRA）；3）部分网络层冻结。并通过领域特定的微调策略，尤其在推理任务上采用轻量化微调。TinyRM的最小模型仅有4亿参数。

Result: 实验表明，TinyRM在RewardBench评分集上表现优异，在推理和安全偏好建模任务上可以与参数量大175倍的主流大模型竞争。小模型通过领域定制微调策略后，效果尤其提升明显。

Conclusion: 轻量级双向架构通过结合适当微调和结构设计，能够作为高效、可扩展的人类偏好建模替代方案。虽然在通用性和偏好建模对话方面仍有挑战，但结果显示此方向非常有前景。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [203] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: 本文提出TextOmics数据集与ToDi生成框架，将组学表达与分子文本描述一一对应，极大提升了靶点相关新药分子生成的能力，并优于现有方法，尤其适合zero-shot分子生成场景。


<details>
  <summary>Details</summary>
Motivation: 当前新药发现中的分子生成缺乏异构数据与统一整合多样分子表示的方法，亟需能融合生物组学与分子语言信息的创新框架。

Method: 作者构建了TextOmics基准，建立组学表达与分子文本描述间的一一对应数据集；提出ToDi模型，联合利用组学信息与文本信息，通过双编码器（OmicsEn和TextEn）学习多层次生物与语义关联，再使用条件扩散（DiffGen）进行可控分子生成。

Result: TextOmics为分子生成提供了高质量异构数据，ToDi框架在大量实验中取得优于主流方法的表现，特别是在zero-shot（无监督新靶点）生成任务中展示出强大潜力。

Conclusion: TextOmics与ToDi为目标相关新药分子设计提供有效解决方案，推动了多模态、异构信息融合分子设计的发展，具有重要理论与实际应用价值。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [204] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 本研究提出了一种新的框架，通过同时学习风险因素和保护性因素对自杀风险变化的动态影响，实现对后续自杀风险的预测。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体自杀风险检测研究多关注即时风险，且只关注风险因素，忽略了对自杀有缓冲作用的保护性因素，无法充分反映用户心理状态的快速变化和多样影响。

Method: 1）构建了含有多达12年Reddit数据的保护性因素标注数据集；2）提出了动态因素影响学习方法（Dynamic Factors Influence Learning），用于建模风险和保护性因素对用户自杀风险变化的动态影响。

Result: 实验表明，所提模型在三个数据集上显著优于现有最新模型和大语言模型，并能为自杀风险变化分配可解释的权重。

Conclusion: 引入保护性因素并动态建模其与风险因素的影响，有助于更准确预测自杀风险变化，提升模型解释性，为临床提供更有针对性的干预建议。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [205] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: 本文提出了一种名为GeLaCo的新型大语言模型（LLM）压缩方法。使用进化算法和层级折叠来高效压缩模型，不仅提升了探索空间的效率，还超过了现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然效果强大，但实际部署和使用受限于其庞大的计算资源需求。因此，如何压缩模型体积并保持性能成为重要研究方向。传统的压缩方法（如结构化剪枝）存在搜索成本高且可能错失更优解等问题。

Method: 作者提出GeLaCo方法，利用进化算法，通过种群搜索与模块级相似性适应度函数，系统性地探索模型压缩解空间。该方法不仅支持对注意力、前馈与隐藏状态三类模块的压缩，还能够进行单目标或多目标优化，首次建立了压缩率与模型质量之间的帕累托前沿。

Result: 在基础模型和经过指令调优的模型上，GeLaCo方法通过困惑度和生成效果评测，不仅在压缩率和性能平衡上实现了更优的帕累托前沿，还优于目前最先进的压缩方法。

Conclusion: GeLaCo为LLM压缩提供了一种高效且性能优越的新方案，显著缓解了部署大型模型的计算瓶颈，并为模型压缩领域的进一步发展提供了新思路。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [206] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 当前主流大型语言模型（LLMs）虽然拥有强大的语言能力，但在跨文化道德多样性表达上存在重大缺陷，无法准确代表人类丰富的道德直觉，而是趋于道德平均化。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会科学研究中的广泛应用，尤其是作为“合成人口”（synthetic populations）来模拟人类行为和价值观，研究者担忧这些模型是否能够真实、准确地反映人类多样的道德与文化立场。

Method: 作者采用了Moral Foundations Questionnaire（道德基础问卷），在涵盖19种不同文化背景下，通过对比多种最先进的LLM生成结果与真实人类数据，用于检验LLMs对道德多样性的表达能力。

Result: 实验发现，当前最先进的LLMs无论模型规模大小，普遍存在“同质化”问题，即无法准确再现人类道德多样性，模型生成的道德选择往往表现为平均化，不能体现各地文化的独特性。

Conclusion: 作者认为，单靠提示微调等当前主流对齐方法无法令AI充分捕捉人类复杂且具文化特色的道德直觉。应当采用更具数据支撑的新目标和评估体系，以确保AI系统能够代表而非削平不同人类价值观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [207] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调方法——关键表示微调（CRFT），通过分析信息流识别并优化推理任务中关键表示，提高大模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统ReFT方法只在每层的固定位置微调表示，效果有限，且对模型输出影响不确定。复杂推理任务中，有些关键表示对最终输出有重要影响，精确微调这些关键表示可能显著提升推理能力。

Method: 提出CRFT方法，利用信息流分析自动识别最具影响力的关键表示，仅微调这些表示，保持大模型参数冻结，仅在低秩线性子空间进行优化，适用于监督学习流程。

Result: 在LLaMA和Mistral模型家族上，针对8个算术及常识推理任务测试，CRFT显著提升了模型性能。此外，在一枪学习环境下，准确率提升高达16.4%。

Conclusion: CRFT方法有效挖掘了表示层级优化潜力，是高效、轻量且性能强大的参数高效微调新方案，拓宽了传统微调范式。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [208] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）与传统Transformer的新型时序预测模型，通过融合两者的表示以提升预测精度，其方法在多个基准数据集上取得了优越的效果。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM应用于时间序列预测（TSF）的方法，通常通过提示或微调方式将LLM从文本任务中学到的知识迁移至时序任务，但LLM在处理连续数值型数据时表现有限，甚至不如为TSF直接训练的原生Transformer模型，后者又难以学到高层语义特征。为此，作者希望构建一种模型，既能充分利用LLM的高层语义能力，又能挖掘Transformer对历史信息的建模能力。

Method: 作者提出一种新颖的Transformer架构，该架构结合LLM与传统时序Transformer：先让LLM提取高层语义特征，再由Transformer提取时序特征，两者的表示通过融合得到混合表征，从而同时保留历史动态与语义变化信息。

Result: 在多个时序预测基准数据集上进行实验证明，该融合型模型在预测精度上优于单独使用LLM或Transformer的方案，证实了其有效性。

Conclusion: 融合LLM语义特征和传统Transformer时序特征的方法，可以显著提升时序预测的准确性，为复杂时序数据的智能建模提供了新路径。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [209] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种无需引入新参数的任务驱动特征蒸馏方法，实现了不同隐藏层维度下师生模型间的高效知识迁移，并在多个任务中显著优于传统线性映射基线。


<details>
  <summary>Details</summary>
Motivation: 解决传统特征蒸馏方法仅适用于师生模型隐藏层维度一致的问题，及现有线性映射方法需引入额外参数且下游任务表现较差的局限。

Method: 提出了一种新颖的任务相关特征蒸馏方法，通过识别教师模型对特定任务最相关的隐层单元，直接将其激活迁移给学生模型，无需新参数，并支持与其他蒸馏框架结合。

Result: 在分类、指令跟随和文本摘要等任务上，所提方法较线性投影基线取得了高达3%的性能提升，并在多项任务中取得了持续改进。

Conclusion: 该方法提高了师生模型特征蒸馏的灵活性和实际表现，是一种简单有效的知识迁移手段，适用于多种下游任务。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [210] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）在将辱骂性文本（如带有仇恨言论和脏话的推文和评论）转化为非辱骂性文本方面的表现。重点考察了GPT-4o、Gemini、DeepSeek和Groq四款模型。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在自然语言处理任务中取得了很大进步，但在识别并消除辱骂性内容、并保持原文本意图方面的能力尚未被充分探索。

Method: 使用GPT-4o、Gemini、DeepSeek和Groq等LLM对辱骂性文本进行识别，并将其转换为非辱骂性且保留原意图的文本，再通过情感和语义分析对原始和转换后的数据集进行评估。

Result: Groq模型的转换结果与其他LLM有很大不同。GPT-4o和DeepSeek-V3在表现上有较多相似性。

Conclusion: 不同的LLM在辱骂性文本转换任务上表现存在显著差异，部分模型在保持文本原意的同时实现去辱骂化方面更为有效。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [211] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 本论文提出了Absher基准，用于全面评估大型语言模型（LLM）在沙特主要方言上的理解能力，含18000+多项选择题，揭示了现有LLM在方言和文化理解上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语的多样性和地域方言丰富，现有LLM在处理阿拉伯语及其方言时存在性能不足。尤其是在以沙特为代表的具备多重方言和丰富文化内涵的环境下，缺乏有效的评测工具来衡量LLM对方言和文化细节的把控能力。为此，作者希望推动更接地气的训练和评测方法。

Method: 作者构建了Absher基准，包括18000多道多项选择题，覆盖沙特6种方言的词语、短语和谚语，类型涵盖语义判断、真伪判断、填空、上下文使用、文化理解和地理识别六类。随后选取多种主流多语言和阿拉伯语专用LLM进行实验评测，并详细分析它们的优劣。

Result: 所有评测的大型语言模型在沙特方言和需要文化推理、上下文理解的任务上表现出明显短板，能力差距尤为显著。

Conclusion: 当前的LLM尚未充分掌握沙特多方言及其文化细节，亟需针对方言的训练和文化相关的评测标准，以提升其在真实阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [212] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 本文提出了一种基于进化搜索的自动离散提示优化方法，有效提升了小型LLM在复杂任务中的表现，并在多个基准方法和任务上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动prompt优化方法多依赖大型模型，且评测任务较简单。但在复杂任务和对小模型场景下，提示设计难度增大、结果更敏感，迫切需要更有效的自动优化方法。

Method: 方法分两阶段：首先用语法引导的遗传编程，组合并搜索多种提示编辑操作，生成prompt设计方案；然后用局部搜索精细调整表现最好的设计。

Result: 在四个领域挑战性任务、三种小型通用LLM上，提出方法优于PromptWizard、OPRO和RL-Prompt，并能显著减缓现有方法的性能退化问题。

Conclusion: 论文证明了提出的进化搜索prompt优化策略在小模型和复杂任务下的实用性与优势，为自动化prompt工程提供了更稳健的方案。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [213] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出了一种基于增长界矩阵（GBM）的新型正则化技术，有效提升了LSTM、S4（State Space Models）、CNN等主流架构在自然语言处理任务中的对抗鲁棒性，实验显示对抗防御性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理模型在任务表现上取得了显著进展，但它们仍易受同义词替换等对抗攻击的影响。尤其是在LSTM和S4等重当前馈的序列建模结构上，鲁棒性的研究仍明显不足，亟需针对这些架构提出有效的防御方法。

Method: 作者提出使用增长界矩阵（GBM）作为正则化手段，通过分析和抑制输入扰动对模型输出的影响，从而提升模型鲁棒性。该方法被应用于LSTM、S4、CNN三种主流结构，并系统分析了S4（作为一种现代状态空间模型）的鲁棒性。

Result: 在多个架构和基准数据集上，实验结果表明该方法在对抗鲁棒性方面相较于已有方法有最高8.8%的提升，且在干净文本上的泛化能力也有所增强。

Conclusion: 提出的GBM正则化方法能有效提升NLP模型（包括S4等现代结构）应对对抗攻击的鲁棒性，优于目前的多种主流防御技术，对推动NLP模型安全性研究具有重要意义。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [214] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型（LLMs）在语言学研究中作为分析工具的可靠性，尤其聚焦于含有运动方式动词的时间表达中的情感意义的出现，并分别通过人类参与者和LLM进行了心理语言学实验，结果显示AI与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如GPT-4）在多种任务上展现出色表现，但它们能否模拟人类对语言细腻判断的能力仍存在疑问，需进一步检验其在心理语言学研究中的适用性。

Method: 进行了四项心理语言学实验，涉及新兴意义、情感转变、情感语境下的动词选择和句子与表情符号的关联。实验先招募人类被试，再用同样任务测试LLM（如GPT-4）。之后对人类与人工智能的结果进行统计相关性分析。

Result: 所有实验结果都显示，AI与人类的答案在评分模式和类别选择上高度一致（Spearman ρ = .73-.96），即便存在少量差异，也未影响总体解释结论。

Conclusion: 研究证据表明，LLM能够有效补充传统人工实验，不影响解释有效性的前提下实现规模化语言学研究，同时有助于新假说提出和数据扩展，是可信且有价值的语言学研究工具。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [215] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 作者提出了一个分层的隐喻处理模型，将隐喻意义视为多层洋葱结构，包括内容分析、概念融合和语用意图三层。该模型为计算系统理解复杂隐喻提供了理论和方法基础。


<details>
  <summary>Details</summary>
Motivation: 传统隐喻理解方法过于简单，将隐喻仅仅视为概念间的映射，无法捕捉其多层次、动态和语境相关的认知本质。为提升计算系统对隐喻的理解效果，需要更细致且认知基础更强的建模方法。

Method: 作者提出了一个三层结构的模型：（1）内容分析层，对隐喻进行基本概念元素注释；（2）概念融合层，建模不同概念成分的组合及其新意义的生成；（3）语用意图层，通过引入语用词汇，捕捉说话者意图、交流功能和语境影响。三层统一进一个形式化框架。

Result: 模型展示了如何逐层注释和建模隐喻意义，实现对隐喻深层、本体论及语用层面信息的表达，从而提升了计算系统处理隐喻时的语境敏感性和推理能力。

Conclusion: 该分层模型为隐喻意义表示和计算理解提供了新的理论基础，有助于实现超越表面关联的、更深入、语境相关的隐喻推理，为未来的计算隐喻理解研究奠定了重要基础。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [216] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文探究了解码器-only的大型语言模型（LLM）在处理图结构推理任务中的内部机制，提出了“诱导子结构过滤（ISF）”视角，揭示了其在识别和提取复杂图子结构中的能力，并进行了理论与实证分析。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在纯文本输入下能解决图推理问题，但其“解码器-only”结构为何能理解和操作底层图结构这一点尚不清楚，因此有必要深入剖析其内部机制。

Method: 作者以子结构提取为任务，设计理论分析与实验，提出并验证了ISF视角以揭示Transformer多层结构如何逐步筛选和识别图中的子结构，并分析不同输入查询对这一过程的影响。

Result: 实证结果和理论分析均验证了ISF过程的一致性，显示解码器-only Transformer能在各层展现稳定的内部动态，并能够从如分子图这类带属性的图中有效识别并提取组合子结构。

Conclusion: LLM及Transformer架构具备以“子结构思维”高效处理复杂图数据、执行子结构提取任务的能力，拓展了它们在图结构数据处理方面的理论基础和应用前景。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [217] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）在任务型对话中提出澄清性问题的能力，比较了LLM与人工提问的差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话系统中的应用增加，理解其在存在歧义时提出澄清性问题的能力很重要。作者希望探究LLM是否能像人一样，针对不同类型的不确定性（如指代歧义、任务不确定）发起澄清。

Method: 作者整合了Minecraft Dialogue Corpus中的两类标注（指代歧义和SDRT澄清），形成了一个新的可用于澄清研究的语料库。利用该语料库，系统对比分析了LLM与人类在面对歧义时的反应行为，并测试不同推理策略对LLM提问行为的影响。

Result: 结果发现：1）人类在面对指代歧义时很少发起澄清，但对任务不确定则更常提出；2）LLM的行为恰好相反，在指代歧义时提出较多澄清，对任务不确定则较少；3）人类与LLM提问的相关性较低；4）加入推理机制可提升LLM提出澄清问题的频率和相关性。

Conclusion: LLM在澄清性提问方面与人类存在明显差异，其对歧义和不确定性类型的感知与应对不是完全类人。推理机制的引入可改善部分表现，但与人类发问行为仍有差距。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [218] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 该论文对比分析了经典双向编码器与新一代超大规模自回归LLM在网络仇恨言论检测任务中的实际表现，验证超大模型是否带来实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 网络平台需要防止仇恨言论扩散，但又不能过度审查正常讨论。早期的变换器编码器在检测上取得了进展，但超大自回归LLM的实际效果尚未验证。该工作旨在评估当前LLM在现实仇恨言论检测中的表现。

Method: 作者设计了基准实验，分别用经典编码器和超大LLM，在网络交流语料上的仇恨言论检测任务进行比较，检测目标是 Hate or No Hate。

Result: 论文通过实验获得了两类模型在仇恨言论真实场景检测中的性能表现和相关数据。

Conclusion: 研究将揭示超大LLM是否能在实际仇恨言论检测场景中优于传统模型，提供对未来网络内容安全检测工具选择的参考。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [219] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文提出了一种创新的自动化招聘跟踪系统（ATS），通过新型的RPA框架（MLAR）集成大型语言模型（LLM），实现更高效的简历筛选和候选人匹配。性能评估显示MLAR在处理大量简历时快于主流RPA平台。


<details>
  <summary>Details</summary>
Motivation: 传统招聘流程在简历筛选和候选人初筛中因时间与资源受限而存在瓶颈。作者旨在通过自动化和智能技术缓解这些痛点，提高招聘效率和准确性。

Method: MLAR框架包括三层：第一层从职位描述中提取关键信息，第二层解析简历识别教育、经验、技能，第三层进行相似度匹配。各层输出通过高级语义算法自动匹配最合适候选人，并集成到现有RPA流程中，实现自动简历解析、职位匹配及候选人通知。

Result: 在高并发简历处理场景下，MLAR的每份简历平均处理时间为5.4秒，比Automation Anywhere快16.9%，比UiPath快17.1%。

Conclusion: MLAR显著提升了招聘环节的效率、准确性和可扩展性，有望变革现代招聘流程，成为适应当前用人需求的自动化解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [220] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 本文对比了扩散式语言模型（LLaDA）与自回归模型（LLaMA）在文本生成上的可检测性，发现现有检测方法对扩散模型无效，需开发新的检测手段。


<details>
  <summary>Details</summary>
Motivation: 虽然AI文本检测技术已相对成熟，但大多仅针对自回归式模型，对新兴扩散式模型的文本难以有效识别，为此作者系统研究二者区别和检测难题。

Method: 作者收集了2000个扩散式和自回归式文本样本，对比分析了困惑度（perplexity）、突发性（burstiness）、词汇多样性、可读性及BLEU/ROUGE得分，并测试现有检测器的表现。

Result: 扩散模型生成的文本在困惑度和突发性上接近人类文本，导致基于自回归方法的检测器出现高漏检率；而自回归模型困惑度明显较低，但词汇保真度下降。单一风格度量均无法有效区分扩散文本和人类文本。

Conclusion: 当前检测方法难以检测扩散式模型文本，亟需开发针对扩散模型的检测方法。文章建议可考虑混合检测模型、开发扩散特有风格学特征及更强的水印方案等方向。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [221] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: 本文提出了一种名为Mixture-of-Recursions (MoR)的新框架，可以在单个递归Transformer中同时实现参数共享和自适应计算，大幅提升大模型的计算与内存效率。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的扩大，模型的训练和部署成本高昂。目前已有方法通常只关注参数共享或自适应计算之一，而未能两者兼顾。论文旨在探索如何在保证高性能的同时，兼顾参数和计算效率。

Method: MoR在Transformer架构中通过递归多步复用共享层，实现参数缩减；通过轻量路由器按Token动态分配递归深度，引入自适应计算；在注意力机制上，仅在活跃Token间进行二次注意力计算，并选择性缓存其Key-Value对；同时提出Key-Value共享变体，进一步降低预填延迟和内存占用。

Result: MoR在135M至1.7B参数规模下，均表现出优于同等训练FLOPs和更小模型尺寸的较低验证困惑度、更优Few-shot准确率；相比基础和其他递归基线模型显著提升吞吐量。

Conclusion: MoR能够在不增加模型尺寸和计算代价的情况下，达到大模型的性能表现，是提升大语言模型性价比的有效途径。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [222] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 本文提出了CodeJudgeBench，这是一个专门评估大语言模型（LLM）在代码场景下充当“评委”能力的基准测试，用以衡量其在代码生成、修复和单元测试生成等任务中的判别表现。实验发现带推理能力的模型在多个任务上显著优于不带推理能力的模型，但所有模型在判断时仍存在较大随机性和不一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成等众多任务取得了领先成绩，但其作为“裁判”为不同模型代码输出打分的能力却缺乏系统评测，尤其在代码场景下缺少专用基准，导致这种评判机制的有效性及局限尚未充分探索。

Method: 作者构建了CodeJudgeBench基准，设置包括代码生成、代码修复和单元测试生成三类任务，并评测了26个LLM-as-a-Judge模型的评分能力，系统比较了带推理和不带推理的模型表现，探究了模型判定顺序变化、不同模型代码评分一致性及提示策略对结果的影响。

Result: 实验显示，具备“思考”能力的模型在所有任务上评判能力显著优于常规模型，且小参数量的带推理模型可超越体量较大的专用评分模型。但整体上模型评分结果波动性大，评判顺序等因素影响明显。此外，保留原始回复中的注释和推理过程有助于提升评分稳定性。

Conclusion: LLM-as-a-Judge在代码任务评分场景下尚不稳定，存在一定随机性和排序敏感性，需警惕其结果的可靠性。使用带推理的模型与优化提示策略可提升判别能力，但未来应进一步增强模型的一致性和稳健性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [223] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的多任务压力测试框架REST，对大模型的推理能力进行同时多题测试，发现模型在多任务下性能明显下降。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理评测方法大多只支持逐题单独测试，存在数据污染、高分趋同和缺乏场景压力的问题，难以真实反映模型在复杂现实环境下的推理能力。急需更能逼近真实使用场景、区分模型能力的评测方法。

Method: 作者提出REST框架，让模型同步面对多道题目，考核其分配注意力、抗干扰、动态认知负载管理等多任务关键能力。并对主流SOTA模型如DeepSeek-R1进行压力测试，并分析机制及对比训练方法影响。

Result: REST测试下，前沿大模型的推理表现大幅下降，暴露出在多上下文高负载下的显著弱点。“long2short”训练技巧能部分缓解性能下滑，且REST展现出比传统基准测试更强的模型区分力。

Conclusion: REST能更有效区分模型潜力、更贴合真实应用需求，同时能减少对人类人工标注和新题生成的依赖，是具前瞻性的推理能力评测新范式。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [224] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: 该论文提出了OTAS方法，用于提升户外环境下的开放词汇分割能力，弥补现有方法因语义模糊及分割边界不清导致的不足，能够无监督快速实现语义地图构建，并在多项数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉-语言分割方法主要依赖基于物体的分割先验，而在复杂、开放的户外场景中往往出现语义模糊和类别边界不清晰的问题，导致这些方法效果有限。因此，亟需新的分割方法适应开放世界语义，支持机器人在非结构化环境下的规划与控制。

Method: 作者提出了OTAS方法，它直接从预训练视觉模型的输出token中提取语义结构，通过单视图和多视角下的语义结构聚类，并基于语言进行语义锚定，最终重建一个几何一致性的特征场，实现了开放词汇的分割查询。OTAS支持零样本应用，无需针对场景微调，且能实时运行。

Result: 在Off-Road Freespace Detection数据集上，OTAS在IoU方面超过了现有微调和开放词汇2D分割方法；在TartanAir数据集的3D分割任务上，比开放词汇映射方法的IoU提升高达151%。此外，真实环境下的实验也证明了该方法在机器人应用中的可行性。

Conclusion: OTAS有效突破了现有开放词汇分割模型在户外复杂语义场景下的限制，可为机器人提供更强的环境理解能力，推动其在真实世界的自主应用。同时代码和ROS模块将在论文录用后公开。

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [225] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: 本文提出了AirScape，这是首个用于六自由度飞行器的世界模型，能够根据视觉输入和运动意图预测未来的视频观测序列。作者还构建了包括11000对视频与意图配对的数据集，并采用两阶段训练方法使模型具备空间想象能力。


<details>
  <summary>Details</summary>
Motivation: 机器人在三维空间中预测自身运动结果一直是体现具身智能的基础难题。现有方法缺乏对通用空间想象能力的探索，特别是对于具有六自由度运动的飞行体。

Method: 作者提出了AirScape世界模型，能够根据当前视觉输入和运动意图预测未来观测序列。方法上，作者构建了包含11000对无人机一视角视频与运动意图配对的数据集，涵盖多种场景与动作，并进行了超过1000小时的标注工作。模型训练采用两阶段方案，首先用无空间知识的基础模型训练，随后引入运动意图和物理时空约束进行微调。

Result: AirScape能够根据运动意图有效地预测无人机在三维空间中的视频序列表现，展现出较强的空间认知和想象能力。数据集的丰富性和标注质量为模型泛化提供了支持。

Conclusion: 本文首次为六自由度飞行体提出世界模型，并公布相关数据集。AirScape实现了以运动意图可控的空间想象，为具身智能领域提供了新的研究工具与方向。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [226] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP利用众包车辆数据，通过创新的Transformer架构，实现高精度、城市级矢量地图自动生成，显著提升精度与鲁棒性，并极大降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 目前高精度矢量地图对于自动驾驶至关重要，但基于LiDAR的传统方法成本高且慢，单车感知方法在精度及恶劣环境下鲁棒性不足，亟需更高效、精确、可扩展的建图方案。

Method: 提出EGC-VMAP端到端框架，通过创新的Trip-Aware Transformer架构，对众包车辆在不同时间和多车辆获得的地图要素进行融合，结合分层匹配和多目标损失，提升训练效率和建图质量。

Result: 在多城市大规模真实数据集上进行验证，EGC-VMAP地图精度和结构鲁棒性均显著优于单车基线方法。人工标注成本下降90%。

Conclusion: EGC-VMAP为城市级高精度地图构建提供了一种高效、可扩展且成本低廉的解决方案，具备产业应用前景。

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [227] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 提出融合摄像头和激光雷达的道路智能路侧单元（IRU）框架用于高精度语义地图构建，并发布RS-seq数据集，明显提升复杂交叉口地图精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于车辆的高精度交叉口语义建图方法受到遮挡和视角限制，无法满足复杂路口语义地图构建需求。亟需新方案解决此类问题，提高地图准确性和自动驾驶安全性。

Method: 提出一种新颖的摄像头-LiDAR融合框架，采用高位智能路侧单元（IRU）采集多模态数据。设计两阶段流程，包括模态特征提取和跨模态语义融合。并增强和注释现有V2X-Seq数据集，推出RS-seq，含精确标签的摄像头和LiDAR数据及丰富矢量化地图。

Result: 用RS-seq数据集定量评估，融合方法在语义分割任务中mIoU比单一图像提升4%，比单一点云提升18%。融合方案在实验中均显著优于单模态基线。

Conclusion: 建立了基于IRU的高精度语义建图基线方法并提供开放数据支撑，显著提升交叉口地图建构精度，为基础设施辅助自动驾驶系统研究提供坚实基础。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [228] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: 本论文聚焦于通过新方法提升多指机械手的灵巧操作能力，提出了一套加强型强化学习框架，并探索了模仿人类视觉-触觉演示的新范式。


<details>
  <summary>Details</summary>
Motivation: 实现类人机械手的高水平灵巧操作一直是机器人学的重要目标，因为它代表通用具身智能的重要里程碑。虽然机器人在手内物体转动等方面有进展，但受限于当前传感-运动学习的基础性瓶颈。本文致力于解决这些根本性限制，推动灵巧智能取得突破。

Method: 论文开发了一套针对多指机械手灵巧操作的机器人学习方法，通过结构化探索克服了随机探索的局限。进一步引入基于采样规划的直接探索并结合模仿学习，利用人类的视觉-触觉演示数据提升学习效率。

Result: 所提出的强化学习方法比传统随机探索表现出更高的学习效率和灵巧操作能力。引入人类演示的模仿学习方法，也为多指灵巧操作提供了新的路径和参考。

Conclusion: 论文证明了结构化探索及引入人类演示能有效提升多指机械手的强化学习能力，为实现更高级的机器人灵巧智能奠定了理论和方法基础。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [229] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: 该论文针对在线装箱问题（OBPP）提出了一种结合结构稳定性验证和启发式规划的装箱策略，旨在提升体积利用率的同时确保装箱的结构安全，以及在新物品无法直接放置时安全重排箱内已装物品。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习方法在OBPP中虽然优于传统启发式算法，但往往不能保证箱体的结构稳定性，也缺乏新物品无法直接安放时的安全重排机制，限制了实际应用。为解决这些问题，作者提出结合结构稳定性检测与自动重排的新策略。

Method: 作者提出“可承载凸多边形”（LBCP）概念，能高效判定物品装入后的稳定性避免箱体倒塌，并引入“稳定重排规划”（SRP）模块，在箱内合理重排已放物品以容纳新物品，并保证整体结构稳定性。

Result: 在标准OBPP基准数据集上的大量实验表明，LBCP稳定性验证具有高效性和良好适应性，而SRP模块在节省重排开销下也能有效找到合理的重排列方案。

Conclusion: 该方法为工业与物流领域的自动化装箱提供了一种兼顾体积利用和结构安全、切实可行且鲁棒性强的解决方案。

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [230] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的Vision-Language-Action（VLA）模型Tactile-VLA，通过深度融合视觉、语言、动作和触觉，实现机器人在接触丰富的物理场景中的精细物理交互。通过少量示范和触觉反馈，激活模型内在的物理常识，实现了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然现有VLA模型在理解‘做什么’方面表现优异，但在需要精细力控制的实际物理交互（特别是接触丰富的场景）中，尚缺乏对‘如何做’的精准落地。为让机器人成为通用智能体，需要将隐含知识更紧密地与物理互动结合，特别是融入触觉信息。

Method: 提出Tactile-VLA框架，将视觉、语言、动作与触觉传感器深度融合，并结合混合位置-力控制器，实现将模型的意图转化为精细的物理动作；新增推理模块，机器人可依据实时触觉反馈自适应策略。

Result: 实验表明，该方法在三方面具备有效性与泛化能力：（1）遵循触觉感知指令操作，（2）利用触觉相关常识，（3）协调自适应的触觉推理。关键发现：视觉-语言模型已有物理交互语义理解，通过与机器人的触觉传感器结合，少量示范即可在接触密集任务中实现零样本泛化。

Conclusion: Tactile-VLA框架通过引入触觉信息和模块化推理能力，显著提升了机器人在复杂物理环境下的适应性及泛化能力，为实现通用型自主机器人迈出重要一步。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [231] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: 本文提出了一种新的自动生成多步接触丰富的机器人操作任务的方法，通过输入用户定义的原子动作、物体和空间谓词，能够在选定机器人环境下输出可解的任务序列。


<details>
  <summary>Details</summary>
Motivation: 多步、接触丰富的机器人操作任务构建复杂且手动设计耗时，且对于强化学习等算法训练，需要大量高质量、多样化、可验证且带丰富信息的任务数据。本文旨在自动化地生成这类任务，降低人工成本，并提高任务多样性和质量。

Method: 所提生成器输入原子动作集、对象集及空间谓词，并通过符号（逻辑/操作一致性、对象-谓词适配）与物理（真实环境中可解性验证）双重验证筛选出可解的任务序列。所有通过验证的任务都可供训练或作为数据集存储。

Result: 生成器能够自动构建长度达15步、数量达百万级的独特高质量多步操作任务，且每个任务附带丰富的中间目标、奖励函数及初始-目标状态信息，适用于各种机器人训练框架。

Conclusion: 文中方法极大提升了复杂机器人操作任务生成的自动化程度，为强化学习等领域提供了丰富、多样且高质量的训练数据，并能更好地分析任务语义相似性。

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [232] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 该论文提出了一种无需重叠视场和初始参数估计的多激光雷达外参标定新框架。方法通过LBA优化联合迭代精炼，实现对外参的准确自动估计，实验结果优于现有方法，且代码已开源。


<details>
  <summary>Details</summary>
Motivation: 多激光雷达的高精度外参标定对于高质量3D地图重建系统至关重要，然而现有方法普遍依赖重叠视场、人工校准或专用标定板，应用受限。该研究旨在解决上述限制，实现无需外部特征和初值的自动化、多激光雷达高精度外参标定。

Method: 提出一种融合激光雷达束调整（LBA）和迭代精炼的新方法。首先通过目标雷达连续扫描和滑动窗口LBA构建基准点云地图，然后将激光雷达外参标定建模为联合LBA优化问题，引入自适应加权机制抑制异常值，实现鲁棒参数估计。

Result: 在CARLA仿真环境和真实场景中测试，所提方法在无重叠视场配置下，平均平移误差5mm，旋转误差0.2度，初始误差容忍高达0.4m/30度，精度和鲁棒性均优于现有主流方法。整个流程无需专用设施或人工参数调优。

Conclusion: 该方法极大简化了多激光雷达系统的外参标定过程，提升了标定的自动化、通用性和准确性，适用于各类实际应用场景，且已开源便于社区使用和创新。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [233] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: 本文提出了一种新的运动路径规划方法HZ-MP，针对非凸空间中的最优路径规划难题，利用混合zoneotope和启发式采样，有效提高了在高维、复杂障碍环境中的路径规划效率和精度。


<details>
  <summary>Details</summary>
Motivation: 非凸空间中的最优路径规划由于NP难性和MILP表示复杂性，现有方法在狭窄空间或有障碍目标区域效率低下，急需更高效、适应性强的样本探索方法。

Method: HZ-MP（Hybrid Zonotope-based Motion Planner）方法利用障碍空间分解，结合ellipsotope启发式，引导低维面采样，使搜索聚焦在有潜力的过渡区域，避免无谓的采样浪费。其结构化探索策略提升了狭窄区域和盒状目标中的性能。

Result: HZ-MP被证明具备概率完备性和渐近最优性，能在有限时间内收敛到近似最优的轨迹，且在高维复杂场景下具备良好的可扩展性。

Conclusion: 该方法有效解决了非凸自由空间中最优路径规划的高计算复杂性和采样效率问题，在狭窄或复杂目标环境中表现优越。

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [234] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 该论文提出了一种名为RMRP的高效地图表示方法，以及结合RMRP的感知感知规划框架RPATR，显著提升了大规模复杂环境下移动机器人自主导航的效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人自主导航方案在大规模复杂环境下，存在地图计算负担重、无人机传感易被遮挡、无人车难跨越复杂地形等挑战，这些都由于缺乏感知感知的策略而加剧。

Method: 作者提出RMRP（Random Mapping and Random Projection）方法，先将数据映射到高维空间，再用稀疏随机投影降维，形成轻量线性参数化地图，并由其理论提出Residual Energy Preservation Theorem确保关键几何特性得以保留。基于该地图，提出RPATR规划框架：无人机部分融合栅格与ESDF地图，前端用解析梯度提升路径安全与平滑，后端用闭式ESDF进行轨迹优化；通过训练的RMRP模型泛化能力，规划器还能预测未观测区域，实现前瞻性决策。对UGV，模型则快速表征地形，提供规划梯度，实现在线绕障。

Result: 该方法在不同环境下均获得验证，展现出优于现有方案的建图速度、内存消耗和精度。使得高速度无人机和无人车能高效安全地自主导航。

Conclusion: RMRP及RPATR框架能够有效缓解移动机器人在复杂环境中的建图与规划难题，并推动社区共享与合作。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [235] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种适用于空中航行器的受控零速度更新方法（C-ZUPT），该方法可在无需与地面接触的情况下，通过利用准静态平衡时刻进行高质量速度校正，显著降低惯性导航系统中的漂移。


<details>
  <summary>Details</summary>
Motivation: 惯性导航在缺乏卫星或摄像头等外部参考源时，精度会迅速下降，因此需要寻找新的方法有效校正累计误差，提升导航的稳定性和可靠性。

Method: 引入受控零速度更新（C-ZUPT），通过设置不确定性阈值，识别空中平台处于准静态（即接近静止）状态的时机，从而向状态估计算法提供高精度的速度更新。

Result: 实验结果表明，C-ZUPT能够大幅减少惯性漂移和控制消耗，减轻滤波器发散现象，并提升系统整体稳定性。

Conclusion: C-ZUPT方法有效提升了资源受限的空中导航系统的精度和能耗效率，有助于延长悬停及持续飞行能力。

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [236] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: 本论文提出了一种新的结合风格模仿和任务绩效的机器人学习方法，通过在约束马尔可夫决策过程（CMDP）框架下实现风格模仿，并采用自适应拉格朗日乘子来在保持任务绩效的前提下提升风格还原度，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人风格学习方法通常依赖于与任务目标高度一致的专家演示，但实际演示常常不完整或不现实，导致方法在风格和任务绩效之间权衡不足，往往牺牲任务绩效来追求风格。为了解决该矛盾，作者提出新的建模方法。

Method: 将风格模仿任务建模为约束马尔可夫决策过程（CMDP），以模仿风格为目标函数，并引入自适应调节的拉格朗日乘子，在保证任务绩效接近最优的前提下指导代理选择性吸取演示中的风格信息。

Result: 所提方法用于多种机器人平台和任务测试，结果显示能够同时保持强劲的任务执行和高保真的风格学习。在ANYmal-D硬件平台上，提升明显：机械能消耗降低14.5%、步态更加灵巧。

Conclusion: 该方法有效解决了实际演示下风格模仿和任务绩效难以兼得的问题，为机器人自然运动学习提供了强有力的工具，也展示了在真实硬件上的实用价值。

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [237] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 本文提出了一种结合能量扩散模型与人工势场的新型运动规划框架，实现了复杂环境下鲁棒且实时的轨迹生成，特别适用于追踪-规避问题。


<details>
  <summary>Details</summary>
Motivation: 追踪-规避问题在现实应用中具有重要意义，如机器人追击、逃逸等，但现有方法在动态、复杂环境中的实时轨迹规划和障碍规避仍有局限。作者旨在提升系统对动态障碍的感知与反应能力。

Method: 该方法无需完整几何建模，直接从点云处理障碍物信息。利用无分类器引导训练的能量扩散模型产生初始轨迹，并在采样环节集成局部人工势场以增强障碍避让能力。在动态场景下，系统还通过势场适应不断优化轨迹。

Result: 结果表明，在部分可观测性、动态变化的复杂环境追踪-规避场景中，该方法能够高效地产生并优化避障轨迹，展现了较强的实用性和鲁棒性。

Conclusion: 结合扩散模型与人工势场的策略，有效提升了在复杂、动态环境中的运动规划表现，证明了本方法在追踪-规避问题中的优越性和推广价值。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [238] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: 本文分析了多架四旋翼飞行器在靠近飞行时，因气流（下洗效应）相互作用带来的挑战，并提出了数据驱动的方法系统研究其影响。


<details>
  <summary>Details</summary>
Motivation: 多四旋翼系统在密集环境中运行时，由于气动相互作用（如下洗效应）容易导致车辆失稳和性能下降。以往多采用保守的避障区策略，导致空间利用率低，限制了应用范围。因此有必要深入理解和分析下洗效应，为多机协同与优化提供依据。

Method: 本研究通过测量力和力矩，分析不同多旋翼组合时的相互作用；同时利用粒子图像测速（PIV）技术，量化单个和成对四旋翼的下洗气流空间特性，获得对应数据。

Result: 研究获得了多旋翼间气动力相互作用的实验数据，以及单个与成对四旋翼的下洗气流空间分布，为更准确地描述和建模下洗效应提供了依据。

Conclusion: 本研究的数据和分析不仅有助于理解多四旋翼气动相互作用，还能为基于物理的协同、优化编队、新运行方式拓展及增强群控鲁棒性提供理论和数据支撑。

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [239] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: 本文提出了一种基于数据驱动的软件，利用IMU和GPS等低成本传感器，并结合数据滤波和传感器融合技术，实现了对无人机姿态和位置的高精度建模与可视化。


<details>
  <summary>Details</summary>
Motivation: 无人机在测试与开发中亟需精确建模，但硬件成本与数据质量常成限制，本文旨在通过廉价传感器及数据融合方法提升数据精度和建模可靠性。

Method: 采集IMU和GPS数据，通过数据滤波算法与传感器融合技术，IMU数据用于四元数姿态表示以避免欧拉角万向节锁问题，位置则融合了更新频率高但误差积累大的加速度计和更新频率低但坐标稳定的GPS。

Result: 结果表明软件能够高效、精准、流畅地渲染无人机的实时位姿。

Conclusion: 所提方案能以低成本手段实现无人机的高精度实时建模，适用于相关仿真与开发场景。

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [240] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种结合毫米波雷达与事件相机的高精度、低延迟无人机地面定位系统mmE-Loc，有效提升了无人机降落定位的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机采样频率低于毫米波雷达，成为多传感器无人机定位系统的数据瓶颈，影响了无人机精准高效降落的实现。

Method: 采用事件相机替换传统帧相机，提升与毫米波雷达的采样频率匹配度。提出两项创新模块：（1）时序一致性协同跟踪模块，利用无人机细微运动和结构特征提升测量精度；（2）图信息自适应联合优化模块，融合运动信息优化多传感器定位效果。

Result: 在真实无人机降落场景实测，mmE-Loc在定位精度和系统响应延迟方面，均显著优于当前主流方法。

Conclusion: 事件相机与毫米波雷达的协同应用及所提出的算法可有效提升无人机地面定位系统的性能，推动无人机自动降落领域的发展。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [241] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: 本文提出TruckV2X，这是首个以卡车为中心、具备多模态传感与多智能体合作的大规模协作感知数据集，旨在解决自动驾驶卡车在感知上的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶卡车在带来安全提升和成本降低的同时，由于体积较大和挂车移动动态复杂，存在感知上的盲区与遮挡问题，影响自身和周围道路使用者的感知能力。现有数据集多针对轻型车辆或缺乏重型车多智能体情景，缺少针对卡车实际应用的感知研究基础。

Method: 作者构建了TruckV2X数据集，涵盖LiDAR和相机等多模态传感信息，并设计了包含卡车头、挂车、联网自动车辆（CAVs）和路侧单元（RSUs）的多智能体协作情景，从而系统性描述和研究卡车在多智能体协作感知中的作用和需求。

Result: 实验建立了TruckV2X数据集的基准，评估了多智能体卡车协作感知性能，并指出了重型车辆感知领域未来需关注的研究重点。

Conclusion: TruckV2X为开发更强遮挡处理能力的协作感知系统奠定了基础，并有助于加速多智能体自动驾驶卡车系统的实际部署。数据集已公开可用。

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [242] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: 本文提出了Plan-MAE，一个基于掩码自编码器（MAE）的预测与规划统一预训练框架，并在大规模数据集上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶领域的规划与预测大多依赖模仿学习，以优化与真实轨迹的相符度，但往往忽略了场景理解对于生成更全面规划轨迹的重要作用。

Method: Plan-MAE对道路网络、周围交通体轨迹和导航路线分别进行掩码重建预训练，以增强模型的环境、社交和意图感知；此外，引入局部次级规划任务融合动力学和安全约束。预训练后模型在下游任务中被微调，联合生成预测与规划轨迹。

Result: 在大规模数据集上的实验表明，Plan-MAE在规划指标上远超现有方法，验证了该方法作为学习型运动规划器预训练步骤的有效性。

Conclusion: Plan-MAE为联合自动驾驶预测与规划提供了一种更具场景理解能力的预训练思路，可作为未来基于学习的方法的重要构件。

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [243] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 本文探讨了利用SNNs（脉冲神经网络）在神经形态计算硬件上直接处理LIDAR数据进行机器人导航和避障，其精度可与传统CNN相媲美，并首次分析了神经元膜泄漏参数对SNN性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在资源受限（如电池、载荷）的机器人场景下，传统神经网络难以兼顾高精度与低算力/内存需求。SNNs具备高效低延迟特性，但其在机器人导航特别是LIDAR场景下的表现及参数影响尚缺乏系统研究。

Method: 搭建自定义配备LIDAR的机器人平台，采集与人工操作相关的LIDAR数据及控制指令，构建有监督数据集。用SNNs处理LIDAR数据进行避障控制，重点研究LIF神经元膜电位泄漏常数对导航精度的影响，并与非脉冲CNN方案对比。

Result: 合理调整LIF神经元的膜泄漏参数后，SNN模型在机器人避障与导航任务上的控制精度可与非脉冲CNN相当。首次定量揭示膜泄漏对LIDAR数据处理精度的重要影响。

Conclusion: 经优化的SNNs可在机器人LIDAR导航任务中取得与主流CNN相当的性能，且更适合部署于资源受限的神经形态硬件。论文开放数据集，有助于促进后续相关研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [244] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: 该论文提出了一种用于无人驾驶赛车环境的统一规划-控制策略IteraOptiRacing，可实现高效避障和圈速优化。


<details>
  <summary>Details</summary>
Motivation: 当前无人驾驶赛车需要同时处理多车避障和竞速优化，现有方法难以兼顾实时性和轨迹的最优性，因此需要新的方法提升安全与性能。

Method: 基于i2LQR（为迭代任务设计的线性二次调节器）设计统一策略，利用历史数据迭代生成避障兼顾时间最优的轨迹，且算法易于并行计算，低计算负担，便于实时部署。

Result: 在高保真模拟器中进行多动态智能体的仿真，结果显示该方法在所有随机测试场景中均优于现有方法，显著提升赛车操控性能。

Conclusion: 提出的IteraOptiRacing策略能实现实时、多目标（避障+竞速）无人车轨迹规划与控制，适用于实际无人赛车应用。

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [245] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 本论文首次将蚂蚁启发的蘑菇体（MB）模型 lateralized 架构应用于视觉归巢任务，并在紧凑型自主机器人上实现，于有限感知和内存下达到了鲁棒、资源高效的归巢效果。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁以极少的感官输入和记忆就实现了鲁棒的视觉归巢，启发了自主导航的仿生研究。目前 MB 模型多用于路线跟随，尚未用于视觉归巢。作者希望通过仿生蚂蚁的策略，研发简单、高效又具生物学基础的机器人视觉归巢系统。

Method: 提出并实现 lateralized MB 架构，在真实机器人上用全景视觉输入、路径积分信号对学习时的视图进行左右分类存储。设计四步增量实验：1）仿真 attractor 动力学；2）现实中 decoupled 学习归巢与巢址搜索；3）随机游走+GPS-RTK 噪声 PI 后的归巢；4）增加第5个 MBON 输出神经元，实现精准的停靠点控制。

Result: 系统只用32x32像素全景图，内存<9 KB，树莓派4上8 Hz运行。无依赖激光/GPS，成功完成多种仿生归巢任务，包括拟蚂蚁式的精确停巢，实验覆盖仿真与真实自然室外环境。

Conclusion: 论文提出的视觉归巢系统为机器人自主导航提供了生物基础、资源高效的新方法，验证了 lateralized MB 架构的有效性与通用性，有助于低资源硬件实现鲁棒、仿蚁导航。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [246] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: 本文提出一个结合轨迹规划、多模态预测及主动探查的新框架，用于自主导航系统在动态环境下处理他方行为不确定性，并在仿真中验证了其效能和适用性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，自主系统需面对其他agent的不确定行为，传统方法难以有效结合不确定性预测与实际决策，因此需要设计更有效的推理和行动策略。

Method: 提出一个统一框架，将轨迹规划、多模态行为预测与主动探查结合；设计新型风险度量指标，能对混合高斯分布的多模态预测不确定性进行解析求解；主动探查机制能优化agent对环境中其他agent行为参数的估计，提高预测精度。

Result: 在MetaDrive仿真平台多个自动驾驶场景中测试，结果显示主动探查机制下系统能有效导航复杂交通情境，并对多种agent行为模式保持鲁棒。

Conclusion: 综合轨迹规划、多模态预测和主动探查的框架能显著提升在不确定动态环境下的自主导航能力，方法具备解析可行性和广泛应用潜力。

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [247] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为MRMEL（多残差专家混合学习）的新方法，用于提升自动驾驶车辆在多种交通情境下的流量控制表现，实现比现有基线更优的排放量减排效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆被用于流量控制时面临情境多样性和复杂目标的挑战，现有控制策略难以跨场景泛化且优化目标常常冲突，因此需要更鲁棒且通用的控制方法。

Method: 作者提出MRMEL框架，结合残差强化学习思想，将多个次优的AV控制策略作为专家模型，根据实时交通情景动态选择合适策略并加入学习得到的残差校正，实现面向多场景的自适应混合控制。

Result: 在亚特兰大、达拉斯沃斯堡以及盐湖城的真实数据下进行合作式节能驾驶实验，MRMEL在所有场景中，车辆总排放比最强基线方案额外降低4%–9%。

Conclusion: MRMEL框架能够稳定提升自动驾驶车辆在多样交通环境下的流量控制效果，表明通过多专家混合和残差优化可显著增强控制策略的适应性和鲁棒性。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [248] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: 本文提出了AdvGrasp框架，能通过物理视角系统性攻击并削弱机器人抓取系统的能力，主要通过改变物体形状来降低其可提取性和稳定性。实验和实际应用验证了方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前对机器人抓取系统的对抗性攻击多关注神经网络本身，忽略了抓取过程中的物理本质。本文意在提出从物理属性角度更实际有效的攻击方法，用于评估和提升抓取系统的鲁棒性。

Method: 提出AdvGrasp框架，分别针对提升能力（抗重力举升）和抓取稳定性（抗干扰能力）两个核心指标，通过形状变形来增加物体的重力力矩并减小wrench空间的稳定裕度，以系统性地生成对抗性物体削弱机器人抓取效果。

Result: 在多种场景下进行了充分实验，证明了AdvGrasp系统能有效降低机器人对物体的抓取成功率。实际物理实验也进一步验证其有效性和可应用性。

Conclusion: 通过物理视角的对抗性攻击，为评估与提升机器人抓取系统的安全性和鲁棒性开辟了新方向，对实际机器人应用具有重要意义。

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [249] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种能够自动发现可由谐波势场生成的不同同伦类路径的新方法，有效提升了机器人在复杂空间中的自主安全导航能力，允许对路径的拓扑属性进行定制。方法通过混合优化算法和微分同胚变换实现，并经大量仿真及硬件实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有的谐波势场方法虽能保证安全和全局收敛，但对势场及路径（尤其是路径的拓扑属性）缺乏定制能力，限制了在复杂环境中机器人路径规划的灵活性和多样性。这阻碍了复杂机器人任务的实现，因此亟需能够自定义同伦类路径的解决办法。

Method: 作者引入一种创新性方法，自动发现可由谐波势场生成的路径同伦类。具体做法包括：1）采用混合优化算法，在同伦类空间上开展搜索；2）决定每个star-obstacle树状结构的结构参数；3）对每个简化树的连续权重参数利用投影梯度下降进行优化；4）通过适当的微分同胚变换，将森林空间转化为无界点空间，进而简化路径的多方向D-signature设计。

Result: 仿真和硬件实验均表明，所提方法能针对不同场景的复杂工作空间，定制谐波势场，使路径具有期望的同伦属性，并始终保持导航的安全性和收敛性，超越了传统方法在灵活性和可定制性方面的局限。

Conclusion: 本文所提方法显著拓展了谐波势场在机器人路径规划领域的应用范围，实现了对路径同伦属性的自动发现和定制。通过空间变换与优化算法结合，兼具通用性、安全性和收敛性，为复杂环境下的自主机器人导航提供了更强大和灵活的工具。

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [250] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: 本文介绍了Octopi-1.5，这是一种最新版的视觉-触觉-语言多模态模型，能够处理来自多个物体部位的触觉信号，并通过增强检索生成模块提升任务表现，支持即学即用。用户可通过新型手持触觉界面TMI直观演示模型在物体识别和推断任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 触觉对于机器人操作、材料识别及视觉受限场景非常重要。目前多模态模型越来越关注底层触觉建模，但在多部位输入整合及新知识快速学习等方面仍有提升空间。

Method: 提出Octopi-1.5模型，能同时处理多个部位的触觉信号，并集成了检索增强生成（RAG）模块以提升模型在任务、推理及新物体学习中的表现。同时提供TMI手持装置，内置GelSight与TAC-02触觉传感器，实现无需机器人即可体验多模态触觉交互。

Result: Octopi-1.5能够结合触觉输入与常识知识，较前代把握多部位触觉信息，成功解决如物体识别等推断任务，并可实时通过RAG机制学习新物品。演示包括物体猜测、后续处理建议及新物品教学等。

Conclusion: Octopi-1.5展示了多模态视觉-触觉-语言模型在任务表现和即时学习能力方面的进步，通过直观互动也展示了当前模型的局限和发展潜力。代码与硬件设计亦已开源，推动该领域进一步研究。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [251] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的水下自主探索与检测系统，并在自主水下机器人Ariel上进行了集成与实地验证，展示了在恶劣视觉条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下环境因视觉障碍、光线不足、复杂地形等导致自动导航与检测变得非常困难。现有自主水下机器人在状态估计和路径规划方面存在鲁棒性和通用性不足的问题。为此，作者希望开发一种更为鲁棒且能够广泛推广的自主水下探索与检测方案。

Method: Ariel机器人装备了5个摄像头和IMU的传感系统，采用考虑折射误差的多目视觉-惯性状态估计算法，并引入了基于学习的机器人自身速度预测方法来增强在视觉退化环境下的鲁棒性。将既有的、自主探索与视觉检测算法集成到Ariel，实现了无人机级别的自主能力。

Result: 实验在特隆赫姆的潜艇干船坞、复杂及极端视觉环境下进行了实地测试。结果显示，该系统的状态估计算法鲁棒性强，路径规划方法可良好迁移到不同类型机器人上。

Conclusion: 文中提出的水下自主探索与检测系统在实际复杂环境中表现优异，提升了水下机器人在状态估计与任务执行方面的鲁棒性及通用性，对相关领域的应用具有实际意义和参考价值。

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [252] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: 本论文提出结合深度强化学习（RL）与进化策略（ES）的方法，用于提升欠驱动机器人控制的性能和鲁棒性。通过采用SAC和SNES双阶段训练，实验证明其在AI奥林匹克任务中大幅优于基线。


<details>
  <summary>Details</summary>
Motivation: 虽然深度强化学习能解决复杂的控制问题，但其初始策略可能在特定任务上的表现仍有提升空间，特别是在面对欠驱动机器人时。因此，需要方法对RL策略进行细致优化，以达到更高性能和任务鲁棒性。

Method: 先用软行动者-评论家（SAC）算法，借助代理奖励函数对RL智能体进行基础训练以近似复杂评分。随后，不直接用梯度，而是利用可分自然进化策略（SNES）就原始评分进行无梯度进化优化，实现对策略的精细微调。

Result: 在2024 IROS第二届AI奥林匹克RealAIGym环境的实验中，优化后的控制器在性能和鲁棒性两方面都有显著提升，相较于现有基线方法取得更高分数。

Conclusion: 结合深度强化学习与进化策略的方法能有效提升欠驱动机器人任务中的控制质量和鲁棒性，对高标准评分任务具有明显优势。

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [253] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: 提出了一种新方法MP-RBFN，利用径向基函数网络高效学习自主驾驶中的运动基元，兼具优化方法的精确性和采样方法的高性能，并显著提升生成运动基元的准确度。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的运动规划方法虽然精准，但计算量大，不适合实时应用。采样法虽然高效，但对轨迹几何形状有限制，难以涵盖复杂动态。该工作的动机是结合两者优点，实现既高效又能精准描述车辆运动的运动基元学习方法。

Method: 提出MP-RBFN方法，将径向基函数网络用于从最优控制问题中学习运动基元，兼顾轨迹的高保真生成与车辆动力学的精确建模。并将MP-RBFN集成到采样式轨迹规划器中，进行了与现有半解析方法的对比实验。

Result: MP-RBFN在生成优化运动基元的准确性上比现有半解析方法高7倍，同时推理速度快，在实践中能够实现对运动基元的精确刻画。

Conclusion: MP-RBFN结合采样法的高效与优化法的精度，显著提升了运动基元学习和运动规划的表现，为自动驾驶中的轨迹规划提供了实用并可扩展的解决方案。

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [254] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: 本文提出了一种极其轻量级的深度学习手势识别系统，实现了自然的人机协作且无需额外硬件。该系统模型仅22KB，识别8种手势，精度高达93.5%。结合量化与剪枝，模型可减小至7KB，并在实际机器人平台上实时验证。结果表明极小模型可用于边缘端自然交互。


<details>
  <summary>Details</summary>
Motivation: 现有人机协作多依赖外部设备如手柄、平板或可穿戴传感器，使用不便。本文旨在解决如何在设备资源受限的环境下，通过自然的手势交互实现高效准确的人机协作。

Method: 设计了一个含1,103参数、22KB大小的深度学习手势识别模型，实现8类手势检测。采用TensorFlow Lite进行量化与剪枝后，模型压缩至7KB，部署于UR5协作机器人，结合ROS2进行实时实验验证。

Result: 模型仅22KB且准确率达93.5%。模型优化后仅7KB，依旧表现出色。实验证明，极小模型可在通用协作机器人平台上实现准确且实时的手势交互控制。

Conclusion: 本文验证了极端轻量级深度学习模型在边缘设备上的可行性，为资源受限环境下的自然人机交互提供了新方向，有望拓宽协作机器人的应用场景。

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [255] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 本文提出了一种结合信任机制的博弈论自动驾驶车辆换道决策（TGLD）框架，使自动驾驶车辆能据人类驾驶车辆的动态信任水平进行更社会兼容的换道操作，并通过人机实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆换道方法通常忽略了与人类驾驶车辆之间的信任动态，无法准确预测人类驾驶员行为，限制了混合交通环境下的有效协作和社会兼容性。

Method: 1）构建多车辆联盟博弈模型，综合自动驾驶车辆的完全合作、以及基于实时信任评估的人类驾驶车辆的部分合作行为；2）开发在线信任评估方法，动态推断换道交互中的人类驾驶员信任水平，引导自动驾驶车辆采取合适程度的协作动作；3）设计以最小化对周围车辆干扰、提高自动驾驶行为可预测性为目标，保证换道策略的社会兼容性。

Result: 通过人机在环仿真实验（高速公路匝道并道场景），结果表明自动驾驶车辆能根据不同人类驾驶车辆的信任水平和驾驶风格灵活调整换道策略；加入信任机制后，显著提升了换道效率、保障了安全性，并增进了AV-HV交互的透明性和适应性。

Conclusion: 信任感知博弈换道框架能有效提升自动驾驶车辆在混合交通中的协作能力、安全性及社会融合度，有助于实现与人类驾驶员友好的自适应交互与换道决策。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [256] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出了一种改进的无迹卡尔曼滤波（UKF）方法，通过优化sigma点在导航误差状态向量非线性动态模型下的传播方式，显著提高滤波精度和导航性能。实验使用水下自主航行器的实际传感器数据验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有UKF在非线性模型下的均值和协方差预测容易导致滤波性能不稳定，作者希望通过改进sigma点的传播方法，提高滤波器的精度和导航系统的整体表现。

Method: 作者提出了一种针对导航误差状态向量非线性动态模型的sigma点传播新方法，在UKF预测步骤中应用该方法，优化了均值和协方差的更新流程。

Result: 通过多场景下的水下自主航行器实测数据，证明所提方法在滤波精度和导航性能方面优于传统方法。

Conclusion: 改进的UKF sigma点传播方法能有效提升滤波精度和导航系统性能，适用于实际复杂导航环境。

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [257] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: 本文综述了大模型（如LLM和VLM）在机器人领域的主要应用，并分析其优势与挑战，提出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 近年来基础模型（特别是LLM和VLM）的出现为机器人感知、规划、控制和人机交互带来了重大变革，急需系统性梳理现状与前沿。

Method: 采用批判性综述的方法，系统归类分析基础模型在机器人中仿真设计、现实执行、迁移学习和自适应等应用，并评估其实用性、总结关键趋势与瓶颈。

Result: 归纳了程序化场景生成、策略泛化和多模态推理等关键能力，识别了体现受限、多模态数据不足、安全风险和算力瓶颈等制约，并指出语义推理与物理智能结合的挑战。

Conclusion: 指出未来应聚焦于更健壮、可解释和具身的模型，实现机器人在现实环境中更强的语义理解与物理智能融合，提出相关研究路线图。

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [258] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文提出了针对无需关节力矩传感器的人形机器人全身力矩控制的新框架，结合物理引导神经网络（PINNs）与无迹卡尔曼滤波（UKF）以提升力矩估计和控制精度。方案在多种机器人上验证了良好的可扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人全身力矩控制依赖昂贵、复杂的关节力矩传感器，而许多机器人采用高减速比谐波驱动器和电机，并未配备这些传感器，因此亟需高精度、无传感器的力矩控制方法。此外，现有方法对摩擦建模和环境扰动的适应性有限。

Method: 该方法将物理引导神经网络（PINNs）用于基于关节及电机速度估算非线性静、动态摩擦，并能捕捉到如电机动作但无关节运动时的摩擦影响；将UKF无迹卡尔曼滤波器与PINN输出结合，用于实时估算关节力矩。整个流程集成至实时系统架构。

Result: 在ergoCub人形机器人上的实验显示，新方法相比传统RNEA（递归牛顿欧拉算法），力矩跟踪精度提高、能效更佳、对扰动的抵抗能力更强。方案在多种硬件摩擦特性不同的机器人上无需重新标定即可保持性能，展现出良好可扩展性。此外，与位置控制相比，力矩控制表现出显著优势。

Conclusion: 该框架为人形机器人提供了一种实用、可扩展且无需力矩传感器的力矩控制方案，能适应动态环境，具备优异的力矩跟踪、适应性和稳定性。

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [259] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: 本文开发了用于软体连续臂（SCAs）的模拟框架，并与视频跟踪系统集成，以便更好地进行实验测试和控制设计。


<details>
  <summary>Details</summary>
Motivation: 软体连续臂具备强大的机械柔顺性，在辅助设备、农业、搜索和手术等领域具有广泛应用前景，但由于其高度非线性的行为，实际应用和控制存在巨大挑战。

Method: 作者开发了一个可模块化组装的、基于纤维增强弹性体封装件（FREEs）的SCA仿真框架，并将其与视频追踪系统集成，能够对物理原型进行实验测试，并用于指导控制策略的设计。

Result: 已建立的仿真框架能够支持SCA的实验测试，并为控制算法开发和验证提供了工具，加快了软体机器人原型的开发迭代。

Conclusion: 该工作提升了SCAs的实验与控制能力，为其在各类实际应用中推广提供了基础支撑。

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [260] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了GUIDER，一个能够准确推断人类操作员意图的概率框架，实现了不干扰人类控制、人机协作的目标。通过双层信念建模，实现导航与操作双阶段意图识别，并在仿真中结果优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 实现人机协作需要机器人能够准确推测并理解人类操作员的实时意图，但传统方法常依赖预先设定目标，灵活性和泛化能力有限。作者旨在突破该局限，实现机器人在无人为限定目标情况下的高效意图识别，以提升协作流畅度与安全性。

Method: 提出了GUIDER双阶段意图估计算法，包含导航和操作两层信念网络。导航阶段将控制器输出与占据网格结合评估可能目标区域，操作阶段融合视觉显著性（U2Net、FastSAM）与几何抓取可行性测试，结合末端执行器运动学实时更新目标概率。全流程在仿真环境下与两个现有方法（BOIR、Trajectron）对比评测。

Result: 在25组测试中（五名参与者、五种任务），GUIDER在导航阶段稳定性达到93-100%（对比BOIR的60-100%），在操作阶段达到94-100%（对比Trajectron的69-100%），并在特定重定向场景分别提升39.5%和31.4%。在几何约束试验中，GUIDER对目标意图识别提速3倍。

Conclusion: GUIDER方法在移动操作任务中显著提升了人机协作时的意图推断准确性与稳定性，能够无须预设目标实现自动意图识别，为机器人灵活适应实际协作场景奠定基础。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [261] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: 提出了一种显式考虑闭环动力学的强化学习方法，大幅提升了双足机器人在多场景下的稳定行走能力，优于传统串联简化模型。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法通常将双足机器人复杂的平行机械臂结构简化为串联模型，忽略了关节耦合、摩擦动力学等关键问题，导致训练得到的控制策略难以迁移到真实机器人上。作者旨在解决这一sim-to-real转移难题。

Method: 提出了一个强化学习框架，在训练中显式建模闭环链动力学，并结合了对称性损失、对抗训练和有针对性的网络正则化。

Result: 在自研机器人TopA上进行了实验，结果显示该方法在复杂地形上实现了更稳定的行走表现，显著优于只考虑简化动力学的基线方法。

Conclusion: 显式建模闭环动力学和采用多种鲁棒性策略的RL方法能显著提升双足机器人真实环境中的行走稳定性，对仿真到现实的迁移具有积极促进作用。

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [262] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: 论文提出了REACT框架，能够在复杂水下结构中有效防止有缆水下机器人作业时缆线缠绕，实现更安全、更高效的覆盖式检测。


<details>
  <summary>Details</summary>
Motivation: 复杂水下结构的检测任务中，有缆水下机器人面临缆线缠绕的高风险，严重制约了作业效率和安全性。现有方法难以实时有效避免缆线缠绕，急需新方法。

Method: 提出REACT（实时防缠绕覆盖路径规划）框架，基于带符号距离场（SDF）的快速几何缆线建模，实时模拟3D缆线分布，通过限制缆线最大长度，实现有效的在线重规划与主动防缠绕，并集成到覆盖路径规划中。

Result: 在仿真管道检测场景中，REACT能无缠绕完成全覆盖检测，保持缆线约束，并将总任务时间缩短20%。尽管检测时间略长，但主动防缠绕避免了后续耗时解缆。真实环境测试也表明REACT能完成任务，而基线方法因缆线缠绕而失败。

Conclusion: REACT能有效提升有缆水下机器人检测复杂结构时的安全性和效率，避免缆线缠绕，实现任务顺利完成，优于传统路径规划方法。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [263] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: 本论文提出了一种结合大语言模型(LLM)语义反馈与强化学习的无人机视觉覆盖路径规划新方法（PIRL），在多个实验环境下实现了覆盖率提升与能效优化。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在无人机视觉覆盖任务中，奖励函数依赖于特定环境、缺乏语义适应性，导致泛化能力和实际部署有限。希望利用LLM的推理与理解能力，提升RL智能体的泛化和灵活性。

Method: 提出了Prompt-Informed Reinforcement Learning (PIRL)，将GPT-3.5生成的语义反馈动态整合进PPO强化学习的奖励函数，引导无人机在空间位置和摄像头角度上的策略调整。训练与评估在OpenAI Gym与Webots仿真环境中进行，考查方法对物理动态仿真的零样本泛化能力。

Result: PIRL在视觉覆盖率、电池效率和冗余度等关键指标上显著优于PPO静态奖励、探索性初始化、模仿学习与纯LLM控制等多种基线。在OpenAI Gym与Webots仿真中，最高分别提升覆盖率14%和27%、电池效率25%和冗余度降低18%。

Conclusion: LLM引导下的奖励塑造能有效提升RL在复杂空间任务中的表现，为自然语言先验与强化学习在机器人领域的结合提供了新的方向。

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [264] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于共识交替方向乘子法（CADMM）的新型轨迹优化框架，可将大规模长轨迹划分为多个片段并并行求解，实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理大规模长轨迹时效率低，难以应用于需要快速响应的运动规划场景。尽管并行计算在其他领域取得进展，但如何高效地将其用于轨迹优化仍是未决问题。

Method: 作者基于CADMM算法，将轨迹分割成多个片段，分别并行求解子问题，框架时间复杂度降到O(1)。同时引入支持线性和二次约束的闭式解，以及针对一般不等式约束的数值解。框架也能充分利用GPU等现代并行计算架构。

Result: 通过仿真和实验，方法在效率和轨迹平滑性上优于SOTA方法。特别是在处理一百个片段的大规模轨迹时，速度提升超过10倍。在GPU上运行，处理成千上万个片段也展现出高性能。

Conclusion: 本文提出的CADMM并行优化框架显著提升了大规模轨迹优化的效率和轨迹质量，极具工程应用潜力，特别适合利用GPU等现代并行硬件实现更大规模的快速轨迹优化。

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [265] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: 本文提出了一种将空间离散的占据栅格地图转化为可连续微分函数的方法，从而可作为约束集成进模型预测控制（MPC）以实现移动机器人在狭窄环境下的路径规划和避障。


<details>
  <summary>Details</summary>
Motivation: 目前移动机器人在实际狭窄环境中的导航多采用离散的代价地图，但MPC常依赖连续可微的代价和约束函数，两者兼容性差。因此需要方法能将离散障碍物信息有效嵌入到MPC约束中。

Method: 作者提出将占据栅格中的每个障碍物定义为多边形，并用半空间（线性不等式）描述每条边，通过AND和OR逻辑将所有障碍合成，再利用模糊逻辑将这些组合逻辑约束平滑化、转化为标准MPC可处理的不等式约束。

Result: 采用上述方法的基于MPC的轨迹规划器在仿真环境下成功进行了测试，展示了其有效性。

Conclusion: 该方法不仅解决了占据栅格地图与MPC约束公式的兼容问题，还为MPC在导航以外的逻辑/语言约束实现提供了基础和拓展思路。

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [266] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度学习的运动估计模型，将视觉、惯性和毫米波雷达数据进行融合，显著提升了自动驾驶系统在恶劣天气下的里程计估计精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统依赖多种传感器感知和定位，但摄像头等感知类传感器在恶劣天气和技术故障下表现不佳，现有技术无法很好应对动态环境变化。迫切需要提升在雨雪、光线变化等恶劣条件下的定位鲁棒性。

Method: 提出一种结合视觉、惯性和毫米波雷达的传感器融合深度学习方法，动态调整各传感器贡献度，通过先进的融合技术利用雷达在恶劣环境中的优势，弥补视觉传感器受限时的信息缺失。

Result: 在Boreas数据集上实验，模型在晴朗与恶劣环境下皆表现出较强鲁棒性和有效性，尤其在视觉信息受损时，模型表现尤为突出。

Conclusion: 通过动态融合多传感器信息，特别是有效利用毫米波雷达，提高了自动驾驶系统在各种环境下的里程计精度和可靠性，验证了该方法的实用价值。

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [267] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: 提出了一种场景感知型对话式高级驾驶辅助系统（SC-ADAS），结合大模型、视觉能力和函数调用，实现了实时自然语言多轮交互和驾驶辅助控制，在CARLA仿真环境中测试验证。


<details>
  <summary>Details</summary>
Motivation: 当前的高级驾驶辅助系统（ADAS）大多依赖固定逻辑，缺乏对场景环境的理解和自然语言交互能力，难以灵活应对复杂动态情况或驾驶员意图变化。

Method: 作者提出SC-ADAS框架，以模块化方式将大语言模型（LLM）、视觉转文本（vision-to-text）、结构化函数调用与ADAS集成，实现基于视觉和传感器信息的多轮对话，用户通过自然语言交流并确认后，控制ADAS执行响应操作。该系统无需对大模型微调，利用云端生成式AI并部署在CARLA仿真环境中。

Result: 系统在CARLA仿真环境下完成了基于场景理解的对话式多轮交互，成功实现了自然语言建议和驾驶辅助控制的流程。评估中分析了视觉感知带来的延迟和对话历史积累带来的Token增加等权衡问题。

Conclusion: 该工作展示了结合对话推理、场景感知与模块化ADAS控制的可行性，并为未来更智能的驾驶辅助系统提供了新方向。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [268] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: 该论文提出了MP1方法，结合3D点云输入和MeanFlow范式，实现了机器人操作中高效、精确的动作轨迹生成，在速度和准确率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型存在性能权衡：扩散模型采样慢、Flow-based方法虽快却受架构限制且需一致性损失。机器人学习场景对速度和精度要求高，尤其在少样本、大场景变化下更需良好泛化。

Method: MP1方法采用MeanFlow理论，通过一次网络前向实现动作轨迹生成（1-NFE），避免引入一致性约束和数值误差。引入CFG提高轨迹可控性，同时利用Dispersive Loss散射状态嵌入，提升泛化且不影响速度。

Result: 在Adroit和Meta-World基准及真实机器人场景中，MP1的任务平均成功率领先DP3（高10.2%）、FlowPolicy（高7.3%），且推理速度快19倍和近2倍。

Conclusion: MP1有效解决了机器人动作生成中的速度、精度和泛化问题，是现有扩散和Flow-based模型的优越替代方案，具有实际部署价值。

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>
