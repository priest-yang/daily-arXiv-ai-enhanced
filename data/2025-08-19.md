<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的实时吸烟检测系统，专用于火灾出口监控，通过改进YOLOv8模型实现高精度检测，并在多种场景和设备上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在火灾出口等关键安全区域监控吸烟行为对保障公共安全至关重要，因此需要开发能在实时、复杂环境下工作的高效检测系统。

Method: 采集了20种场景共8,124张图片并包含低光等难点样本，评估YOLOv8、YOLOv11、YOLOv12三种模型，并基于YOLOv8设计定制模型，增强其对复杂监控环境的适应性；在多种边缘设备上进行多线程推理测试。

Result: 定制模型在召回率(Recall)上达78.90%，mAP@0.5达83.70%，优于其他比较模型。Jetson Xavier NX设备推理延迟为52-97毫秒，适合实时检测。

Conclusion: 系统在复杂环境下具有优良的检测精度和实时性，为公共安全监控与自动合规监管提供了强大适应性和实用性的平台。

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 本文提出了只用程序生成数据进行表征学习，结合视觉记忆库，无需针对真实数据再训练，却能在多项视觉任务上实现接近甚至超越传统模型的效果。


<details>
  <summary>Details</summary>
Motivation: 获取大量带标注的真实世界图像昂贵且难以扩展，利用程序生成数据若能达到类似性能可大大减少成本和依赖。本文旨在探索只用程序数据训练图像表征模型的可行性以及性能极限。

Method: 作者仅用程序生成数据训练视觉表征模型，并建立视觉记忆库（收集嵌入向量用于检索），将这些模型直接应用于图像相似性、分类和语义分割等任务，无需针对标注真实图片再训练，通过与传统用真实数据训练的模型对比性能。

Result: 模型在多个数据集上表现优异：在NIGHTS视觉相似性任务上仅比Places模型低1%，CUB200和Flowers102分类任务分别高8%和15%，ImageNet-1K分类也在10%以内，COCO上的零样本分割R^2距离真实数据模型也只差10%。

Conclusion: 只用程序生成数据训练并结合视觉记忆，可在无需再训练的前提下获得大部分真实世界任务性能。当前与真实数据模型的性能差距主要因程序模型对同一物体不同部分表征不一致，导致检索误差，这为未来改进提供了方向。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 本文系统性评估了多种眼科基础模型（FM）的单独性能与融合表现，并提出了FusionFM评测体系，为模型选择和临床应用提供了有力依据。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析领域展现出较强泛化能力，特别是在眼科。但目前尚不清楚哪种FM最佳、它们在不同任务中表现如何，以及模型融合是否有优势。因此，亟需系统性评测和对比分析。

Method: 作者提出了FusionFM评测套件，并设计了两种模型融合方法，对四种主流眼科基础模型（RETFound、VisionFM、RetiZero、DINORET）在多国标准化数据集上进行单独和融合评测，任务涵盖眼科疾病（青光眼、糖尿病视网膜病变、年龄相关性黄斑变性）检测与系统性疾病（糖尿病、高血压）预测。评估指标包括AUC与F1分数。

Result: 结果显示DINORET与RetiZero在眼科与系统性疾病预测任务中表现突出，其中RetiZero对外部数据集具更强泛化能力；模型融合中的Gating策略在青光眼、AMD和高血压预测上有小幅提升。但高血压等系统性疾病在外部队列的预测仍然困难。

Conclusion: 本研究首次系统对比并融合多种主流眼科FM，总结了各自优势和模型融合带来的增益，为未来模型优化和临床应用提供了策略和方向。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: 本论文提出了UniDCF，一个能够融合点云和多视角影像的多模态牙颅面硬组织重建统一框架，实现了高效、精准的自动化重建，并显著提升了临床应用的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型局限于单一组织及单一成像模态，导致在牙颅面硬组织重建上泛化性差，难以兼顾解剖精准、计算效率与跨组织适应性，限制了其在临床上的广泛应用。

Method: 作者提出UniDCF框架，通过点云与多视角影像的多模态融合编码，并引入基于分数的去噪模块提升表面光滑度。该方法利用数据的互补信息，克服了单一模态方案的不足，并基于大规模多模态数据集（覆盖6609名患者，共54555条标注数据）进行训练与验证。

Result: 实验结果表明，UniDCF在几何精度、结构完整性和空间准确性等方面超越当前最先进方法，在临床仿真中设计时间缩短99%，且超过94%的案例获得临床医生认可。

Conclusion: UniDCF实现了牙颅面硬组织的快速、高保真自动化重建，促进个性化精准修复治疗，简化临床流程，并提升患者诊疗结局。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5 是 Ovis2 的升级版，对原生分辨率视觉感知和强多模态推理进行了改进。它包括原生分辨率视觉Transformer、思维模式推理、多阶段课程训练，取得了在多个基准测试的领先成绩。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型常见的输入图像固定分辨率处理方法会导致细节和全局结构损失，尤其影响复杂视觉内容（如复杂图表）的分析能力。同时，传统推理方式多为线性思维链，缺乏自我检查和修正机制，限制了推理准确性。因此，开发能原生处理可变分辨率图像、具备更强多模态推理能力的模型有重要意义。

Method: 1. 引入原生分辨率的视觉Transformer，避免因重采样或切块处理带来的信息损失；2. 模型支持“思考模式”，推理阶段可选择进行自我反思和修正以提升准确率；3. 采用五阶段课程式训练，包括基础视觉/多模态预训练、大规模指令微调和推理增强（如DPO、GRPO）；4. 利用多模态数据打包和并行混合训练提升效率；5. 推出9B和2B两个开源模型，适配不同资源场景。

Result: Ovis2.5-9B 在OpenCompass多模态排行榜上取得了78.3的平均分，超过前代（Ovis2-8B）并在40B参数以下范围内达到SOTA（同类开源MLLMs最佳）；Ovis2.5-2B达到了73.9分，也刷新了同体量模型的记录。模型在STEM、视频、图表分析等任务中表现突出。

Conclusion: Ovis2.5显著提升了原生分辨率感知及多模态推理能力，同时在有效性和高效率间取得平衡。其开源版本为资源受限设备部署优质模型提供了新选择，并为多模态理解和推理奠定了更坚实的基础。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: 该论文提出了VideoAVE数据集，这是首个公开可用的视频到文本的电商属性值抽取（AVE）数据集，并建立了基于该数据集的评测基线。


<details>
  <summary>Details</summary>
Motivation: 现有的AVE数据集多数仅涉及文本到文本或图像到文本的场景，缺乏对产品视频、属性种类多样性的支持，且鲜有公开数据集。因此亟需一个多领域视频到文本的AVE数据集，推动相关模型发展与评测。

Method: 作者构建了VideoAVE数据集，涵盖14个领域、172种属性，并采用基于CLIP的MoE过滤系统，提高数据的质量。最终获得了约22.4万训练样本和2.5万评测样本。此外，作者还搭建了基准测试平台，评估了多种主流视频视觉语言模型在不同AVE任务的表现。

Result: 实验结果表明，在属性值预测和开放属性值抽取任务上，各主流模型在视频到文本AVE场景下的表现仍有较大提升空间，现有模型对时序信息的利用能力有限。

Conclusion: VideoAVE数据集的公开为视频场景的电商属性值抽取任务提供了有力支持，也为视频视觉语言模型的后续研究和发展提供了重要的基准和资源。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 该研究通过手工特征（平面曲率、曲率符号和梯度方向）替代传统CNN，使用MLP实现手写字符识别，在MNIST和EMNIST数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前主流手写字符识别方法依赖于CNN等深度模型自动提取特征，但这样的方法可解释性较差且计算复杂。研究探索是否可以仅凭几何特征驱动深度学习模型，并提升模型可解释性。

Method: 作者构建了三个基于二阶几何特征的特征图，分别是曲率大小、曲率符号及梯度方向，并将这些手工特征输入多层感知机（MLP）进行分类，以手写数字和字母数据集MNIST和EMNIST作为实验对象。

Result: 基于上述三种手工几何特征，MLP在MNIST数字集上获得了97%的准确率，在EMNIST字母集上获得了89%的准确率。

Conclusion: 曲率等几何特征在手写字符图像中具备很强的判别能力，结合MLP可以取得媲美CNN的准确率，这显示了无需复杂深度特征工程也能获得优秀的分类效果，同时提升了模型可解释性。

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 本文提出了优化提示结构和多模态数据增强两种方法，有效提升了针对网络仇恨模因的检测能力。通过改进的提示设计和反事实中性的合成数据增强，显著提高了模型的健壮性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 网络中充斥着文本-图片混合的模因，仇恨信息较难检测，尤其是当其隐晦地隐藏在幽默或讽刺之中时。现有视觉-语言模型缺乏细粒度监督，难以应对隐性仇恨言论。

Method: 1. 提出提示优化框架，系统性探究提示结构、监督粒度和训练方式对模型表现的影响；2. 构建多模态数据增强流程，通过多智能体LLM-VLM体系生成2,479个反事实中性的合成模因，有选择性地修改仇恨成分。

Result: 结构化提示能提升小模型鲁棒性，InternVL2模型在二分类和多级分类场景下F1得分最高。数据增强流程减少了虚假相关性，提升了模型的泛化能力。

Conclusion: 提示结构和数据构成与模型体积同等重要。针对性的数据增强有助于构建更加可信和敏感的仇恨检测系统，并为生成合成数据的新研究方向提供思路。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 本文探讨了高斯曲率在三维表面建模中的作用，发现它可作为稀疏、紧凑的表面描述手段，有助于三维重建和作为无监督度量。


<details>
  <summary>Details</summary>
Motivation: 当前三维视觉主要依赖深度学习和大规模数据，但这些数据驱动方法缺乏可解析、可迁移和可控的显式三维几何模型。作者希望引入几何不变量——高斯曲率，来丰富三维重建的理论基础和实践工具。

Method: 作者分析了Middlebury stereo数据集三维表面中的高斯曲率特性，探讨了其描述能力，并比较了现有单目与双目方法中高斯曲率的隐含角色。此外，提出利用高斯曲率作为几何先验和无监督评价指标。

Result: 结果表明，高斯曲率不仅能够以稀疏、紧凑方式描述三维表面，且现有主流方法虽未显式应用高斯曲率，但隐式中加以利用。此外，高斯曲率作为几何先验可以提升三维重建效果，同时适用于无监督的评估。

Conclusion: 高斯曲率在三维表面建模和重建中具有独特优势，可作为几何先验和无监督指标，未来三维视觉方法可融合显式的高斯曲率机制，提升模型的可解释性与性能。

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [10] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 本论文系统性比较了多种图神经网络(GNN)中，图像到图结构的转换方案，致力于提升基于图级异常检测(GLAD)的效果，尤其在皮肤镜图像上并在完全无监督到全监督不同场景下进行对比实验。


<details>
  <summary>Details</summary>
Motivation: 此前已有研究通过将图像转为图结构后应用GNN于分类或异常检测任务，但尚缺乏对图像转图所用的多种分割、特征、连边策略在图级异常检测任务中的系统对比分析；该领域缺乏最佳实践指引。

Method: 作者综合评估了多种图像分割方法、节点连边策略和基于颜色、纹理及形状的节点特征组合，并在皮肤镜图像数据集上，比较了不同标注强度下（无监督、弱监督、全监督）的GLAD模型表现。

Result: 实验表明，颜色特征单独使用时表现最佳，加入形状和纹理特征可以进一步提升检测效果。最佳无监督配置（OCGTL方法）AUC-ROC达0.805，弱监督和全监督分别提升至0.872和0.914。

Conclusion: 不同图像到图的转换策略对GNN异常检测性能影响显著，合理组合特征和分割、连边方法能够在包括无监督场景下取得与经典方法接近甚至更优的性能，为后续相关应用和方法设计提供参考。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [11] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 本文系统性地综述了Transformer模型在无人机领域的最新进展，对各类Transformer架构及其应用进行了分类、评估和比较分析，并探讨了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的飞速发展，其强大的感知与决策能力为无人机系统的自主性和智能化带来了突破，但相关的系统性综述与应用框架尚不完善，因此该论文希望补全这一空白。

Method: 本论文采用系统综述的方法，对近年来Transformer在无人机领域的应用进行了整理和分类，涵盖注意力机制、CNN-Transformer混合、强化学习Transformer及大语言模型等不同类型，并通过结构化表格和性能基准进行对比。此外还梳理了关键数据集、仿真器与评估指标，对现有研究进行了综合比较和总结。

Result: 文章构建了针对无人机的Transformer模型统一分类法，综述了其在精密农业、自主导航等新兴应用领域的进展，给出了详细的比较分析，同时指出当前文献在计算效率、实时部署等方面存在不足。

Conclusion: 论文认为Transformer技术正逐步推动无人机智能化发展，但仍需解决多方面挑战，并提出了未来研究的方向。这对相关研究人员及从业者具有重要的参考意义。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [12] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: 本文提出了ComplicitSplat攻击方法，可以在3D Gaussian Splatting生成的场景中嵌入仅在特定视角下显现的对抗内容，攻击主流检测模型，揭示了该技术在自动导航等安全关键任务中的风险。


<details>
  <summary>Details</summary>
Motivation: 随着3D Gaussian Splatting（3DGS）技术广泛应用于从静态图像高效合成新视角图像，作者关心如果恶意攻击者篡改输入图像，是否会对安全关键应用造成威胁。现有大部分研究聚焦于模型白盒攻击或常规方法，缺乏对3DGS在下游检测任务上的黑盒攻击探索。

Method: 作者提出ComplicitSplat攻击，通过利用标准3DGS渲染方法，在物体表面嵌入仅于特定视角下可见的颜色和纹理伪装，实现对抗内容隐藏。这种方法无需访问模型结构或权重，属于黑盒攻击。作者在真实与合成场景、单阶段、多阶段、转换器等流行检测器上进行广泛实验。

Result: 实验表明，ComplicitSplat能够有效攻击多种类型（包括单阶段、多阶段和transformer等）目标检测器，在真实世界物理对象和合成场景中均表现出良好攻击效果，并能在看不见的视角隐藏对抗内容。

Conclusion: ComplicitSplat是首个针对3DGS的下游目标检测黑盒攻击方法，能在安全关键应用（如自动导航、机器人）中引入新的安全隐患。建议业界重视此类攻防漏洞，加强对3DGS及相关系统的安全防护。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [13] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 本论文探讨了基础模型在前列腺多参数MRI中的标注效率，强调影像质量分布对微调后性能的显著影响，建议在微调和部署阶段对影像质量进行严格评估和匹配。


<details>
  <summary>Details</summary>
Motivation: 基础模型能在使用更少标注数据的情况下实现高性能，但在医学影像领域，尤其是前列腺MRI中，影像质量的参差可能影响这种标注效率。因此，作者希望评估和理解影像质量分布如何影响基础模型的微调效果与泛化能力。

Method: 作者使用ProFound基础视觉模型，对大规模前列腺MRI预训练，再在微调和评估阶段系统性地调整高/低质量影像比例，观察不同质量分布以及微调-测试分布不匹配对下游任务（如自动报告、癌症检测）的影响。

Result: 实验表明，微调集与测试集高低质量影像分布的不同会显著影响下游性能。微调集高质量样本充足时，基础模型具有很高的标注效率；但缺乏高质量样本时，预训练优势明显降低。此外，不同下游任务对微调-测试分布匹配的需求各异。

Conclusion: 只有保证微调和部署阶段影像质量分布的一致，才能充分发挥基础模型的高效标注潜力。因此，在医学影像任务中应重视数据质量评估和质量标准建设，以优化模型性能和泛化能力。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [14] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: 本文提出了一种高效轻量的VLM参数适配方法AdaRing，通过跨层张量环分解同时提升适配器的表示能力与压缩率，实现显著减少训练参数同时保持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法通过在每一层网络中插入同质化适配器，提升容量，但忽视了层间冗余，导致模型压缩率不足、表示能力有限。需要一种新机制来兼顾参数高效性与表达多样性。

Method: 提出基于跨层张量环分解（TRD）的新型视觉-语言微调框架AdaRing。通过利用张量低秩性，将适配器建模为跨层共享的张量核和层特异的切片，大幅减少冗余参数。同时引入多样秩驱动和泛化感知微调，异质化适配器协同处理多样任务需求。

Result: AdaRing方法在多项任务上均取得了当前最佳性能，同时平均训练参数量减少90%。

Conclusion: AdaRing框架有效解决了适配器间冗余和容量受限问题，实现视觉-语言模型的极致轻量高效适应，具备广泛实际应用潜力。

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [15] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EVTP-IV的新颖视觉token剪枝方法，有效加速了基于自然语言指令的视觉分割任务，在保持精度的同时大幅提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在指令视觉分割任务中表现出色，但推理成本很高，特别是在视频场景下。作者注意到用较少但代表性更强的视觉token可以减少计算量，同时不会显著降低分割性能，因此提出改进的token剪枝方法。

Method: 基于对视觉token采样的经验分析，提出融合空间信息的k-center视觉token剪枝算法EVTP-IV。该方法选择空间上具有代表性的token子集，并进行信息论分析，证明其合理性。通过该方法，在仅使用20% token的情况下加快推理速度。

Result: 在标准IVS基准测试中，该方法在视频任务上实现了最高5倍、在图像任务上实现了3.5倍的速度提升，同时保持与基线方法相当的精度，并在不同剪枝比例下均优于现有最优剪枝方法。

Conclusion: EVTP-IV方法能够极大提高IVS任务的推理效率，在大幅减少token数量的前提下依然保持高分割准确度，是提升多模态大模型实际应用效率的有效手段。

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [16] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级纯CNN架构——Large Kernel Modulation Network (LKMN)，在资源受限场景下兼顾超分辨率图像重建的性能与速度。核心创新包括增强型部分大核块(EPLKB)和交叉门控前馈网络(CGFN)。实验显示该方法在提升画质的同时大幅加快推理速度，超越当前SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 在资源有限（如移动设备）场景下，现有CNN推理快但难以捕捉全局特征，Transformer虽可建模非局部信息但推理慢，两者难以兼顾速度与效果。该工作旨在提出一种兼具CNN低延迟和全局感知能力的高效SR方法。

Method: 设计了LKMN模型，主要包含两个新结构：1) EPLKB，通过通道洗牌、通道注意力和部分通道上的大核条状卷积，有效捕获非局部特征且保持低复杂度；2) CGFN，动态融合输入特征、局部特征和非局部特征，通过可学习缩放和交叉门控策略增强特征互补性。

Result: 大量实验表明，LKMN在多个数据集上均优于现有主流轻量级SR模型。例如LKMN-L在Manga109数据集上4倍放大时，PSNR比DAT-light高0.23 dB，推理速度几乎提升4.8倍。

Conclusion: LKMN在保证推理效率的前提下，有效改善了轻量级SR模型对全局信息的建模能力，达到更优质的超分辨率复原效果，是面向实际应用场景的有竞争力方案。

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 本文探讨了仅用Sobel算子提取的一阶边缘特征作为输入，训练全连接多层感知机(MLP)以进行手写字符识别是否可行。结果显示，MLP在MNIST和EMNIST_letters上的精度接近卷积神经网络(CNN)，并且模型更小、更透明。


<details>
  <summary>Details</summary>
Motivation: 近年来，卷积神经网络（CNNs）席卷计算机视觉领域，成为手写字符识别等任务的主流方案。然而，CNN结构复杂、参数多，且特征提取过程不透明。论文作者想重新审视传统的Sobel算子，探讨是否仅用Sobel算子生成的简单边缘图就足以驱动非卷积模型（如MLP）获得与CNN相近的性能，寻找更轻量、更可解释的替代方案。

Method: 作者首先将手写字符图片通过水平和垂直方向的Sobel算子处理，得到一阶导数的边缘图（水平和垂直梯度图），然后以这些边缘图作为输入，训练多层感知机（MLP）进行多类别分类，并分别在MNIST和EMNIST_letters数据集上进行测试。

Result: 仅使用Sobel一阶边缘特征的MLP模型在MNIST数据集上取得了98%的识别准确率，在EMNIST_letters数据集上达到92%。精度接近基于CNN的主流方法。与此同时，模型参数量更小、特征更直观透明。

Conclusion: 手写字符图片中大部分有判别力的特征已经被一阶边缘信息捕获。因此，用边缘特征驱动的MLP可作为HCR任务的轻量级、可解释性强的替代选择。

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频片段定位任务——在线混合模态检索（OVG-HQ），支持文本、图像、视频片段及其组合的查询，并提出了统一框架与新数据集，实现了高效准确的在线视频片段定位。


<details>
  <summary>Details</summary>
Motivation: 传统的视频定位任务主要依赖文本查询，无法满足流式视频场景或需要图像/视频片段作为查询的需求，因此亟需一种支持多模态混合查询并适用于在线场景的新方法。

Method: 提出OVG-HQ-Unify统一框架，包含参数化记忆块（PMB）用于保留历史知识增强当前决策，并设计跨模态蒸馏策略平衡强弱模态学习能力。此外，构建了支持多模态查询的新数据集（QVHighlights-Unify），并设计了适于在线场景的新评测指标。

Result: 在新构建的数据集上，OVG-HQ-Unify在准确率和推理效率上均优于现有方法，能高效处理多种模态混合的查询。

Conclusion: OVG-HQ-Unify为线上多模态视频片段定位提供了有力方案，支持线上高效处理和融合多种查询模态，推动了视频定位技术在更实际复杂场景下的应用。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SafeCtrl的插件，用于增强文本生成图像模型的安全性，能有效识别并抑制有害内容，同时保持生成图像的高保真度。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图片的模型可能会生成危险或不当内容。已有方法（如提示重写、模型微调）在提升安全性的同时影响了生成质量，且局部替换易导致语义问题。急需一种更平衡安全性和质量的新方法。

Method: SafeCtrl是一种非侵入式轻量级插件，采用“检测-抑制”新范式，首先精准定位有害内容区域，不采用简单的内容替换，而是抑制有害语义，引导模型自然生成安全、合理的内容。其训练采用直接偏好优化（DPO），利用现有的图像层面偏好数据、无需精细的像素级标注，学会在推理时对局部区域进行引导性抑制。

Result: 实验证明SafeCtrl在保障安全性和维持生成内容保真度方面明显优于现有主流方法。

Conclusion: SafeCtrl通过解耦和基于抑制的控制策略，为构建更安全、可扩展的生成模型提供了有效新方向。

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP通过结合单一像素的时序和光谱信息，实现了高效的遥感地物分类，无需依赖大面积图块或语义文本标签。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在遥感应用中依赖大面积图块和文本标签，导致计算成本高且训练数据难以获取，因此需要更高效、低依赖的新方法。

Method: 提出TimeSenCLIP框架，利用Sentinel-2卫星影像的单像素的时间序列和光谱信息以及与地面照片进行跨视角训练，减少对文本描述的依赖，并保持卫星图像与地面视角的语义一致性。

Result: 在LUCAS和Sen4Map数据集上针对地物类型、作物类型和生态系统类型的分类任务进行实验，结果表明单像素结合时间和光谱信息后可实现准确分类。

Conclusion: 单像素基于时序和光谱特征能够满足大规模遥感分类任务的需求，为相关领域提供了更高效、可扩展的技术路线。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 本文探讨了利用生成对抗网络（GAN）生成的合成MRI数据，辅助脑肿瘤分割模型训练的可行性。通过与单独真实数据集以及不同比例混合数据集对比，发现混合数据在某些方面能优化分割效果，但类别不平衡问题仍存。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤影像异质性强，标注数据稀缺，而且医学影像中普遍存在数据类别不平衡，手动分割十分困难。合成数据有望缓解数据量小和类别不平衡的问题，提升模型泛化能力。

Method: 使用BraTS 2020公开脑肿瘤MRI真实数据、medigan库生成的合成数据，以及多种比例（如40%真实+60%合成）的混合数据，训练U-Net分割网络；比较不同数据组合下的分割性能。

Result: 定量指标（Dice, IoU, 精度、召回率、准确率）上，混合数据组与仅真实数据组相当。但在定性评估中，混合组（尤其是40%真实+60%合成）提升了整个肿瘤边界分割效果。不过对肿瘤核心区和增强区分割仍然较差，类别不平衡问题未彻底解决。

Conclusion: 合成数据有助于脑肿瘤分割，是有前景的数据增广策略，但仍需进一步探索更大规模实验、更好体积一致性及如何减少类别不平衡带来的影响。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的点云去噪方法，分析其主要挑战、分类体系，并展望未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 实际环境采集到的点云数据通常伴随不同类型和强度的噪声，因此点云去噪（PCD）对于提升后续任务性能至关重要。尽管基于深度学习的PCD方法近年来取得了显著进展，但缺乏系统性的综述总结现有研究和挑战；该文旨在填补此空白。

Method: 作者对深度学习点云去噪领域进行全面梳理，提出面向去噪任务的专属分类体系，并将PCD过程分为离群点移除和表面噪声还原两个步骤。同时，对现有方法进行对比分析，归纳主要贡献。

Result: 本文总结现有基于深度学习的PCD方法的优点与不足，归纳主要挑战，比较各类方法的异同及优势，并系统性提出新的分类思路。

Conclusion: 深度学习在点云去噪领域展现出强大潜力，但还存在诸多局限。作者对未来的研究方向进行了展望，为后续相关研究提供了方向和参考。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: 本文提出DynamicPose，无需再训练即可实现快速运动情况下的鲁棒6D位姿跟踪，显著提升了动态场景中的追踪性能。


<details>
  <summary>Details</summary>
Motivation: 以往的6D位姿跟踪算法主要针对静态或准静态场景，在对象和摄像头均快速移动时表现大幅下降，因此需要一种能适应高动态场景的跟踪方法。

Method: DynamicPose包括三大关键模块：1）通过视觉-惯性里程计（VIO）补偿摄像机运动导致的兴趣区域偏移；2）利用深度信息的2D追踪器校正目标发生大平移时的ROI偏移；3）VIO引导的卡尔曼滤波器预测对象旋转，生成并筛选位姿候选，最终通过层级细化得到精确位姿。这一闭环系统不断利用6D位姿跟踪结果优化2D跟踪与滤波，提升整体同步与准确性。

Result: 在仿真和真实场景中，DynamicPose实现了实时、鲁棒的6D位姿跟踪，特别在快速运动物体及摄像头的情境下表现优异。

Conclusion: DynamicPose有效解决了快速运动场景下6D位姿跟踪的难题，为动态环境中的目标跟踪提供了实用、高性能的解决方案。

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 本文提出了一种利用知识蒸馏在单一邻域内近似多尺度特征的方法，并通过可迁移特征嵌入机制和中心加权IoU来提升点云目标检测的效率和效果，同时显著降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云目标检测至关重要，但现有方法通常计算量大，不利于轻量模型和计算资源受限的环境，亟需一种高效、资源友好的多尺度特征近似方法。

Method: 首先，采用知识蒸馏，在单邻域基础上近似点云的多尺度特征；其次，引入以类别相关统计为核心的可迁移特征嵌入，补偿单邻域导致的特征多样性损失；最后，通过中心加权IoU作为定位损失，缓解中心偏移带来的误差，并确保计算开销较低。

Result: 在公开数据集上进行广泛实验，结果显示该方法能有效提升点云目标检测性能，同时降低了计算资源消耗。

Conclusion: 所提出的方法能够在保证准确性的前提下，大幅降低计算复杂度，非常适用于计算资源有限或对速度有高要求的点云检测场景。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个首个用于3D模态统一理解与生成的框架UniUGG，能够提升3D场景的表征、理解和生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态统一架构在2D图像理解与生成方面已取得很大进展，但3D相关任务的整合仍旧存在困难，且研究相对空白。作者旨在填补这一空白，提出能同时进行3D理解与生成的统一框架。

Method: 提出UniUGG框架，结合大语言模型(LLM)对文本及3D表示进行统一理解和解码；核心为一种基于潜变量扩散模型的空间解码器，可实现高质量3D表示生成；支持基于参考图像和任意视角变换的3D场景生成与空间视觉问答；同时提出几何-语义联合预训练策略提升视觉编码器的语义和几何理解能力。

Result: 大量实验结果表明，所提方法在视觉表征、空间理解以及3D生成任务上均优于现有其他方法。

Conclusion: UniUGG实现了3D模态的统一理解与生成，增强了空间相关任务的表现，为多模态AI系统注入了新的能力。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 论文提出了一个用于参照视频对象分割的新框架SAMDWICH，并搭配手工标注的新数据集，通过更精确的时刻对齐文本和视频，实现更高效、精准的分割和跟踪任务。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法在训练时对于所有可见对象进行统一处理，与表达式真正指代的对象语义对齐度低，导致性能受限，需要更精细的文本-视频内容对齐机制。

Method: 提出SAMDWICH框架，基于手动标注的MeViS-M数据集，通过明确标记每个对象被表达式提及时的时间点，进行语义指向性更强的训练。包含Moment-guided Dual-path Propagation（MDP）和Object-level Selective Supervision（OSS）两部分，分别提升对象的时序跟踪和表达式一致性。

Result: 在复杂的MeViS数据集上，SAMDWICH显著优于现有方法，尤其是在处理多样表达和复杂场景下表现突出。

Conclusion: 引入时刻感知与对象级筛选的训练机制，提升了RVOS任务中的视频-文本对齐与表达一致性，为后续相关研究提供了新的思路和基准。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为PEdger++的协同学习框架，实现了在保持高精度的同时显著降低边缘检测模型的计算复杂度和模型体积，有效适用于不同计算资源的设备，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 边缘检测在众多计算机视觉任务中起着基础作用，但传统深度学习方法虽能提升精度却计算开销过高，不适合资源受限设备，因此需要兼顾高精度和低复杂度的高效边缘检测方案。

Method: 提出了PEdger++协同学习框架，通过融合异构架构、不同训练阶段、参数采样的跨信息，提升模型学习能力，从而在保持或提升精度的同时，显著降低了模型规模和计算开销；并提供了多种计算资源下的模型版本，以适应不同应用场景。

Result: 在BSDS500、NYUD和Multicue等多个主流数据集上，PEdger++在定量和定性结果中均明显优于现有方法，验证了其高效性和适应性。

Conclusion: PEdger++能够在低计算复杂度下实现高精度边缘检测，适用于各类终端设备，并为资源受限场景下的边缘检测提供了新的解决方案。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 本文提出了一个新的多分辨率、多模态微表情数据集，包含同步的RGB和事件相机数据，并在两个基线任务上验证了事件相机数据在微表情识别和重建方面的优势。


<details>
  <summary>Details</summary>
Motivation: 微表情分析对于人机交互和驾驶员监测等领域具有重要应用。但单靠RGB相机由于时间分辨率和运动模糊问题，难以有效捕捉快速、微弱的面部变化。事件相机具备微秒级精度和高动态范围，但缺乏相关公开数据集阻碍了其发展。

Method: 作者采集了首个同时包含多分辨率、RGB与事件相机数据的微表情数据集，并在两项基线任务上进行评估：（1）用脉冲神经网络进行Action Unit分类，（2）利用条件变分自编码器进行帧重建。

Result: 在Action Unit分类上，事件相机数据的识别准确率为51.23%，明显高于RGB数据的23.12%；在帧重建任务上，高分辨率事件数据输入可获得较高质量的重建结果（SSIM=0.8513，PSNR=26.89 dB）。

Conclusion: 事件相机数据在微表情识别与帧重建中表现优异，具备进一步研究和实际应用的潜力。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于生成式多模态大语言模型（MLLM）的产品表征学习方法MOON，并发布了大规模多模态基准集MBE，在多项产品理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的判别式双流结构难以处理产品多图文间的多对一对齐问题，且典型LLM缺少多模态和属性感知能力。需要新的方法提升产品统一表征学习能力。

Method: 提出了MOON模型：引入了引导式混合专家（MoE）模块，实现对多模态及属性信息的定向建模；核心语义区域检测以衰减图片噪声干扰；并设计了专用的负样本采样策略增强判别性。同时，构建并公开了多模态产品理解数据集MBE。

Result: MOON在自建基准MBE和公开数据集上零样本学习表现优异，具备强泛化能力，支持跨模态检索、产品分类、属性预测等下游任务。

Conclusion: MOON展现了生成式MLLM在产品理解上的巨大潜力，为后续多模态产品表征学习提供了新思路和基准。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为InstDrive的实例感知型3D高斯溅射重建框架，实现了动态驾驶场景的高效、灵活三维重建，并首次支持驾驶场景中的实例级分割。


<details>
  <summary>Details</summary>
Motivation: 以往工作将驾驶场景中的所有背景元素统一建模，限制了实例级理解和灵活编辑。同时，以往方法多依赖复杂流程或先验标注、且多面向视角丰富的室内场景，难以直接用于动态、开放的驾驶环境。

Method: InstDrive利用SAM生成的掩码作为伪标签，结合对比损失和伪监督机制指导2D特征学习；在3D层面，通过正则化和体素损失隐式编码实例ID，实现特征一致性。引入轻量级代码本，桥接连续特征与离散实例ID，去除预处理和复杂优化环节。

Result: 定量与定性实验均显示InstDrive优于先前方法，且无需复杂的数据处理流程，是首个解决驾驶场景实时3D实例分割的框架。

Conclusion: InstDrive为动态驾驶场景的三维重建与实例分割提供了高效实用的新方案，促进了自动驾驶和场景理解领域的发展。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 本论文提出了WiseLVAM，一种全自动且可手动调整的左心室线性测量方法，提高了临床超声心动图测量的自动化、准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法在B超图像上直接检测关键点，易因点位微小偏差导致明显测量误差，降低临床可信度。半自动方法虽能约束测量线，但需人工干预，亟需全自动且准确的解决方案。

Method: 提出基于弱监督的B超关键点检测先估算左心室轮廓，再依循临床指南自动推断左心室主轴和基底层位置并放置测量线，然后在运动模式（AMM）下自动完成线性测量。该方法结合了结构感知与运动感知，形成全自动WiseLVAM框架。

Result: WiseLVAM在左心室线性测量任务中展现出更高的鲁棒性和准确性，并具备全自动应用的能力，提升了测量的一致性和临床实际可操作性。

Conclusion: WiseLVAM方法能自动、准确地完成左心室线性测量，兼具手动调整灵活性，有望作为超声心动图测量的实用临床工具，满足实际工作流需要。

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: 本文提出了一种新型医疗视觉问答（VQA）模型Q-FSRU，将频谱特征融合方法与量子检索增强生成相结合，提升多模态复杂临床问题的解答能力，并在VQA-RAD数据集上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI在需要同时理解图像和文本的临床复杂任务上依然面临挑战。提升模型理解和解答高难度多模态问题的能力，对医生临床决策具有重要意义。

Method: 提出Q-FSRU模型，将来自医学图像和相关文本的特征通过快速傅里叶变换（FFT）转为频域，以过滤无用信息增强有效特征；同时引入量子启发的检索系统，通过量子相似性方法，从外部知识库检索医学事实，并融合至频域特征中；最终共同用于多模态推理和答案生成。

Result: 在VQA-RAD（真实放射影像与问答）数据集上实验，结果显示Q-FSRU在需要图文深度推理的复杂问题上明显优于先前模型。

Conclusion: 频域信息融合与量子检索增强方法的结合显著提升了模型的性能与可解释性，为构建智能且可用的临床AI工具提供了有效思路。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: 本文提出VimoRAG，一种结合视频检索增强的运动大模型生成框架，通过检索大规模视频数据库，提高3D动作生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有动作大模型因标注数据有限，存在严重的领域外/词汇外（OOD/OOV）问题，影响3D动作生成能力，需要借助更丰富的视频数据增强模型表现。

Method: VimoRAG框架融合了大规模野外视频数据库，检索相关2D人体动作信号以辅助3D运动生成。其要点包括：（1）研发了以运动为中心的视频检索模型，能有效区分人体姿态与动作；（2）提出Motion-centric Dual-alignment DPO训练机制，减少因检索不佳导致的错误传播。

Result: 实验结果表明，采用VimoRAG后，原本仅能用文本做输入的动作大模型性能得到了显著提升。

Conclusion: VimoRAG能显著提升动作大模型在生成任务中的表现，为利用视频数据弥补文本数据不足提供了有效途径。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注即可自动评估目标检测模型性能的方法，并展示其在现实应用场景中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管目标检测模型已取得显著进步，但在实际中仍需耗费大量人工成本进行性能评估。为减少这一成本，开发无需人工标注的自动评估方法成为迫切需求。

Method: 作者提出了Prediction Consistency and Reliability（PCR）方法，利用目标检测器在非极大值抑制（NMS）前产生的多个候选框，通过1）分析NMS前后候选框的空间一致性，以及2）保留候选框间置信度分数的可靠性，联合估计检测性能，无需依赖人工标注。同时，为了更真实、可扩展地评估，他们构建了一个通过不同程度图像扰动生成的meta-dataset。

Result: 实验结果表明，PCR方法在检测性能估计上比现有自动评估方法更准确，且作者提出的meta-dataset能够涵盖更广泛的检测性能范围。

Conclusion: PCR方法与新构建的meta-dataset为无标注环境下的目标检测自动评估提供了更高的准确性和适用性。

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新方法DiffGEBD，用于从生成式角度进行通用事件边界检测（GEBD），能够生成多样化且可信的事件边界，并在标准数据集上实现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有GEBD方法主要采用确定性预测，忽视了事件边界主观性带来的多样解空间，难以反映实际中事件界定的多样性。为解决这一问题，作者希望开发一种能生成多元有效结果的方法。

Method: 提出了DiffGEBD，将视频帧间的相关变化编码为时序自相似特征，并采用扩散生成模型，用随机噪声逐步解码为合理的事件边界预测。通过无分类器引导调整去噪扩散过程中的多样性。同时，作者设计了兼顾多样性和准确性的新评估指标。

Result: 在Kinetics-GEBD和TAPOS两个标准数据集上，DiffGEBD方法生成的事件边界具有多样性和可信度，且性能优越。

Conclusion: DiffGEBD模型不仅提升了GEBD任务预测多样性，还保证了预测边界的准确性，是事件边界检测任务的重要方法创新。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 提出了一种基于多阶段卷积神经网络（MSCNN）的综合方法，有效减少低端激光扫描仪在室内粗糙环境下的三维点位测量误差，提高空间测量精度，使低端设备性能接近高端设备。


<details>
  <summary>Details</summary>
Motivation: 室内激光扫描因设备性能和环境因素导致测量误差，高端和低端设备差距显著。传统方法难以充分校正低端设备的系统误差，限制了高精度模型的创建与室内空间的数字化改造需求。

Method: 将高端激光扫描仪的测量结果作为参考，配对同环境下的低端设备数据，统计分析并量化误差分布特性。结合传统几何处理与多阶段卷积神经网络深度学习校正框架，将误差矫正建模为监督学习问题，最大限度地修正系统性偏差且保持关键几何特征。

Result: 在粗糙室内数据集上，方法显著提升了低端设备的测量精度，均方误差（MSE）降低超过70%，峰值信噪比（PSNR）提升约6分贝。

Conclusion: 该方法无需硬件升级即可大幅提升低端激光扫描仪在室内测量任务中的精度和可靠性，为高精度三维建模和空间改造应用提供了新的技术途径。

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 本文针对扩散模型在低精度推理加速时因逐步量化误差积累导致输出质量下降的问题，提出了一种时间步感知的累积误差补偿方法，显著提升了现有后训练量化（PTQ）在低精度下的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因出色的图像生成能力受到关注，但其推理耗时、算力需求高，限制了大规模应用。后训练量化（PTQ）虽能加速模型，但因扩散过程多步迭代，每一步的量化误差会累加，最终影响合成质量。现有工作没有对误差传播机理给出系统分析，也缺乏有效补偿方案。

Method: 作者首先利用数学分析推导了扩散模型中逐步量化误差的传播方程，并首次给出了累积误差的闭式解。基于该理论，设计了时间步感知的累积误差补偿方案，针对每一步的量化误差动态调整，抑制误差的扩增。

Result: 在多个图像数据集上的实验证明，该补偿策略能有效缓解量化误差传播，极大提升低精度量化扩散模型的生成质量，并在主流后训练量化方法基础上达到当前最优表现。

Conclusion: 论文提出的理论分析和补偿方法有助于推动低精度扩散模型的实际部署，为低延迟、高效率的生成任务提供了新思路。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: 本文提出了一种名为VELVET-Med的新颖视觉-语言预训练框架，专为有限的三维医学影像（如CT）及文本数据设计，通过创新目标和架构在小规模配对数据上实现多项下游任务的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的视觉-语言模型（VLMs）应用受限于高质量大规模影像-文本配对数据的获取，尤其在三维影像（如CT）领域，数据收集极为困难，影响了模型在下游任务表现。该工作旨在解决有限医学影像数据情况下，如何有效进行跨模态预训练并提升模型泛化能力这一核心问题。

Method: 1）将单模态自监督学习方法纳入到视觉-语言预训练框架中，弥补了现有工作对这一方向的忽视；2）设计了名为TriBERT的创新语言编码器，能够学习多层次文本语义表达；3）提出层次化对比学习，捕捉三维影像与文本之间的多层次关联。

Result: 在仅使用38,875对扫描-报告配对数据的条件下，所提出的编码器在3D分割、跨模态检索、视觉问答、报告生成等多项下游任务取得了领域内最优（SOTA）性能，展现出较强的迁移能力和泛化能力。

Conclusion: VELVET-Med框架有效突破了三维医学影像与配套文本数据稀缺的限制，通过创新性自监督与多层次编码器设计，提升了视觉-语言模型在下游医学任务中的表现和泛化能力，对实际临床应用与医学AI发展具有重要意义。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 该论文提出了Simple o3框架，在多模态大模型（MLLMs）中实现了可扩展的动态工具交互，例如图像裁剪、放大和重用，推动了多模态场景下的链式思考能力。简单 o3在公开的TWI-Tools-146K数据集和多个基准测试表现优异，且具备较低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs尽管在视觉-语言任务表现优异，但在多模态链式推理方面研究不足，尤其是“拟人化思维＋图像迭代操作”方面。受到OpenAI o3模型启发，作者希望搭建更强、更经济的多模态推理框架。

Method: 作者设计了Simple o3框架，在监督微调(SFT)下，融入可执行的视觉操作（如裁剪、放大、图像重用），紧密交替于视觉-语言推理流程中。通过可扩展的数据合成流程，生成高质量的推理序列并构建TWI-Tools-146K数据集。

Result: 实验证明Simple o3在多个多模态推理基准任务上取得优于现有方法的成绩，展示了模型出色的推理力与细粒度视觉认知能力，并首次系统分析了不同视觉-语言交替推理策略的效果。

Conclusion: Simple o3不仅提升了多模态链式推理与精细认知能力，还兼顾了计算成本，具备良好的研究与应用潜力。多样化的推理策略、视觉token增强、区域裁剪等机制均对能力提升有积极作用。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出了DualFit，一种在虚拟试穿领域中通过双阶段流程提升衣物细节保真的方法，显著改善了细节保留与视觉真实感的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术虽然视觉效果提升明显，但在衣物细节（如标志和印花）保留方面存在不足。这些细节对品牌形象及顾客信任尤为重要，因此需要新的方法来提升细节还原能力。

Method: DualFit采用了两阶段的处理框架。第一阶段利用学习到的流场将目标衣物对齐到人物图像，重点保证细节的高保真。第二阶段通过拟合模块将已对齐的衣物与人体区域融合，并引入保留区域输入和局部修复掩码，实现重点区域的细致还原和最小化无关区域的改动。

Result: 实验结果表明DualFit在视觉上实现了更自然无缝的虚拟试穿，并且极大提升了衣物高频细节（如LOGO、印刷体）保留能力。

Conclusion: DualFit能够有效平衡重建精度与视觉真实感，为虚拟试穿中的衣物细节恢复提供了更优的解决方案，对线上时尚零售有实际应用价值。

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: 本文提出了一种名为TriQDef的防御框架，有效提升量化神经网络(QNNs)对于补丁攻击的鲁棒性，并显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 传统量化神经网络虽然能削弱像素级攻击，但对局部高显著性补丁攻击（patch-based adversarial attacks）的抵抗力有限，而且这些攻击能在不同比特宽度间迁移。现有防御方法要么过拟合于特定量化配置，要么难以应对这种跨比特宽漏洞。为解决这个问题，提出了新型的防御机制。

Method: TriQDef包括三部分：1) 特征错位惩罚(FDP)：通过对中间特征的感知相似性加以惩罚来强制不同语义错位；2) 梯度感知失调惩罚(GPDP)：利用边缘IoU和HOG余弦指标，通过最小化不同比特宽下输入梯度的一致性来显式破坏梯度对齐；3) 联合量化感知训练协议：在多量化等级共享权重的训练中统一应用上述两种惩罚。

Result: 在CIFAR-10和ImageNet上实验显示，TriQDef可使未见过的补丁与量化组合下的攻击成功率降低超过40%，同时保持原有的高准确率。

Conclusion: 同时干扰特征和梯度的语义及感知对齐有助于显著抑制补丁攻击在QNNs中的迁移性。TriQDef为提升量化网络安全性提供了有效且泛化性强的新思路。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 本论文提出了一种专门针对大规模视觉-语言模型（VLMs）微调的方法，能在细粒度领域适应和保留原有多模态知识间实现最佳平衡，显著避免灾难性遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs虽能生成强大的视觉和多模态表征，但在细粒度开放集视觉检索任务中效果仍不理想，需要对视觉编码器进行领域微调。传统微调方法会引发灾难性遗忘，模型的通用视觉和跨模态能力大幅下降。因此，亟需一种能兼顾领域适应和知识保留的微调方法。

Method: 受到持续学习领域的启发，作者系统分析了多种有助于知识保留的正则化方法，并提出融合这些技术的高效组合策略。同时，论文强调了验证集设计和超参数调优的重要性，以提升实验的复现性和泛化能力。所提方法无需在微调时使用文本数据或原始文本编码器，只针对视觉编码器进行微调。

Result: 在多个细粒度与粗粒度图像-图像和图像-文本检索基准评测中，所提方法均取得了强劲表现，同时成功保留了视觉-文本对齐能力。

Conclusion: 作者的方法能有效实现领域自适应增强，同时避免遗忘原有通用多模态知识，在实际应用中泛化能力更强，是提升VLMs检索能力的一项有价值技术。

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: 本论文提出了一种新的心脏动态磁共振重建方法KP-INR，通过融合空间坐标和局部k空间特征，实现了提升采集快速、低质量数据下的重建效果。实验证实方法优于主流基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有心脏动态磁共振（cine MRI）为了提速，采用欠采集技术但导致图像质量下降。隐式神经表示（INR）虽有进展，但仅利用坐标嵌入，未充分挖掘目标点及其邻域的特征信息。

Method: 提出KP-INR，构建双分支结构：一支处理k空间坐标的嵌入，另一支学习该坐标点多尺度局部特征，通过跨分支交互共同重建k空间，最终提升整体重建质量。

Result: 在CMRxRecon2024公开数据集上验证，KP-INR在苛刻的直行k空间采样任务中表现优于基线模型。

Conclusion: KP-INR有效利用空间信息和局部特征，显著提升了心脏动态MRI低质量采集条件下的重建效果，对该领域具有应用潜力。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，用以更精细地检测和量化扩散模型中的图片记忆现象，并揭示了现有检测和缓解手段的不足。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法只能识别完整图片的记忆，对图片局部和更复杂记忆模式无法有效衡量。因此，研究者希望提出更细致的判别和量化方法。

Method: 作者提出了基于分割的前景-背景记忆检测（FB-Mem）方法，能够对生成图片中被记忆的局部区域进行分类和量化。该方法还结合聚类分析，用于挖掘一对多的记忆对应关系。

Result: 新方法揭示了扩散模型的记忆现象比先前想象的更普遍，生成图片与多个类似训练图片群相关联。局部（特别是前景区域）的记忆难以通过现有神经元屏蔽与剪枝方法消除。

Conclusion: 提出的FB-Mem方法为扩散模型记忆现象的测量提供了有力工具。当前的缓解方法不足，需要结合聚类等更强的新策略加以改进。

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种名为RealTalk的新型框架，实现了高情感准确性、强情感可控性和稳健身份保真的情感说话人头部合成方法，在情感AI领域取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有说话人头部合成方法虽然能保证音唇同步和图像质量，但在表现准确、可控的情感和保持身份一致性方面表现有限。随着人工智能在社交场景的应用，对情感表达准确且可控的虚拟人有更高的需求。

Method: RealTalk框架包括：先用变分自编码器（VAE）从音频生成3D面部标志点；再通过ResNet基于的标志点变形模型（LDM）将标志点与情感标签嵌入拼接，生成具有情感特征的面部标志点；最后采用新型三平面注意力的神经辐射场（NeRF），结合面部混合形状系数，生成高度真实并具备目标情感的说话人头部图像。

Result: 实验结果表明，RealTalk在情感准确性、可控性及身份保持方面均优于现有方法，能够生成更为真实且具情感表达的虚拟说话人。

Conclusion: RealTalk推动了社会智能AI系统发展，为生成高质量、有情感表达并保持个人身份的虚拟人技术提供了新思路。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: 本文提出了WaveVerse框架，能够通过文本引导生成现实的室内场景及人体动作，并用相位一致的射线追踪方法模拟真实的RF信号，从而解决RF感知数据采集困难问题。实验证明该方法在高分辨率成像和人体行为识别任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 射频（RF）感知因其隐私保护优势成为室内感知的重要替代方法，但高质量RF数据在动态多样的室内环境下难以获取，限制了其发展。为此，作者希望引入低成本、安全并可扩展的数据生成方法。

Method: 提出了一种基于提示的WaveVerse框架。该框架包含：1）语言引导的4D世界生成器，通过带空间约束的因果Transformer生成符合描述的人体动作；2）相位一致的射线追踪模拟器，精准生成相干RF信号。

Result: 实验展示了框架可根据不同条件生成多样化的人体运动，并在波束赋形和呼吸监测等任务中成功利用相位一致性。此外，通过两个案例：高分辨率成像和行为识别，结果显示WaveVerse能高效生成数据并提升机器学习模型在数据稀缺和充足条件下的表现。

Conclusion: WaveVerse首次实现了RF成像数据可生成，并在多种RF感知任务中验证了生成数据的有效性，有助于推动基于射频的室内感知技术发展。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且统一的特征提升方法，将丰富的图像特征赋予3D表示，在多视角下实现稳健且高质量的特征挂载，显著提升了3D场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有特征提升方法在处理多视角图像时，常因特征不一致和噪声问题影响最终3D特征的质量和鲁棒性。因此，迫切需要一种理论上有保证、能在提升效率和质量之间取得平衡的特征提升方案。

Method: 作者将特征提升问题建模为稀疏线性逆问题，并给出了可闭式解法，提出了两种正则化策略：Tikhonov Guidance保证数值稳定性，Post-Lifting Aggregation通过聚类滤除特征噪声。这种方法对核和特征类型不依赖，具有良好的通用性和理论误差上界。

Result: 在开集词汇3D分割基准测试中，该方法优于现有的训练式、分组式和启发式特征提升方案，并且能在几分钟内完成高质量特征提升。

Conclusion: 本文方法具备理论最优性、鲁棒性和效率，有效解决了多视角下特征提升的不一致和噪声问题，为3D场景理解提供了高性能方案。

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 该研究针对棉花病害检测，优化了YOLOv11模型，通过改进模块和策略，大幅提升了检测准确率和实用性，并实现了移动端部署。


<details>
  <summary>Details</summary>
Motivation: 棉花病害早期检测准确率低、实际田间应用表现下降，多病害并发时误检率高，严重影响农作物健康和产量，有必要提升病害智能监测能力。

Method: 提出了C2PSA模块强化小目标特征提取，通过动态类别权重缓解数据样本不均衡，并结合Mosaic-MixUp数据增强方法提升模型泛化能力。

Result: 在4,078张图像数据集上，系统获得了mAP50: 0.820（提升8%），mAP50-95: 0.705（提升10.5%），推理速度达158 FPS，病害检测表现显著优于原模型。

Conclusion: 所提出的新检测系统显著提升了棉花病害的智能检测效果，支持移动端部署，为农业病害的实时监测和精准防治提供了有效工具。

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 本文提出了一种结合生成网络与物理神经模拟的新方法，实现了更快、更高保真度的超声CT（USCT）三维成像，显著提升了对肌骨系统组织结构的定量成像效果。


<details>
  <summary>Details</summary>
Motivation: 现有USCT在肌骨系统成像中受限，主要因为传统基于射线的重建方法忽视了超声波的强散射效应，导致成像准确度和分辨率不足。临床上急需一种兼具高分辨率与无辐射优势的技术来进行肌骨系统评估。

Method: 作者提出了一种“生成式神经物理”框架，将生成网络与嵌入物理约束的神经网络仿真相结合。该方法通过少量跨模态影像学习超声波三维传播的紧凑代理模型，兼顾了基于物理建模的准确性和深度学习的高效稳定，从而实现USCT高效高保真的三维组织成像。

Result: 在合成与实际数据（乳腺、手臂、腿部）上，该方法在十分钟内重建3D组织参数分布图，对肌肉和骨骼等的生物力学属性具备灵敏度，并且分辨率可与MRI相媲美。

Conclusion: 本方法通过解决强散射下的计算瓶颈问题，增强了USCT在肌骨系统疾病常规临床评估中的适用性，有望推动USCT在相关领域的常规应用。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 本文提出了视觉动作提示（visual action prompts），作为一种统一的动作表征方式，实现了高自由度复杂交互的动作驱动视频生成，并兼顾不同领域间视觉动态的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有的动作驱动视频生成方法在动作表达的精确性和通用性之间存在权衡：基于文本、粗糙动作、或者掩码的方法虽然通用，但精度低；以智能体为中心的方法精度高，但难以跨领域迁移。因此需要一种既精确又能跨领域迁移的动作表征。

Method: 作者提出利用可视化骨架（visual skeletons），将动作“渲染”为图像形式来作为领域无关的精确动作表征。设计了从人-物交互（HOI）和灵巧机器人操作两大富含交互的数据源中抽取骨架的流程，并通过在预训练视频生成模型中集成骨架驱动信号，采用轻量化微调实现精确动作控制和动态学习能力的结合。

Result: 在EgoVid、RT-1 和 DROID 三个视频生成相关数据集上进行了实验，结果显示所提出的方法在复杂交互、动作精度和跨领域动态迁移能力上均优于现有方法。

Conclusion: 视觉骨架驱动的动作提示为高自由度、复杂动作的精确控制和跨领域迁移提供了新的解决方案，证明在视频生成任务中可用性和有效性较高。

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [51] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 本文针对复杂环境下的显著性目标检测（SOD）引入了包含多种天气噪声的大规模新数据集WXSOD，并提出了具备天气感知能力的基线方法WFANet，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 目前SOD方法多聚焦于自然场景，对天气噪声下的鲁棒性研究不足，主要由于缺乏对应的像素级标注数据集。本文致力于填补该领域空白。

Method: 作者构建了WXSOD数据集，包含14945张含多样天气噪声的RGB图像、配套像素级标注和天气标签，具备合成和真实场景测试集。同时，提出了Weather-aware Feature Aggregation Network（WFANet）作为基线方法，采用双分支结构，分别抽取天气相关特征和显著性检测特征，最终融合以提升SOD鲁棒性。

Result: WFANet在WXSOD数据集上对比17种SOD方法，表现全面优于现有方法，验证了方法和数据集的有效性。

Conclusion: 针对天气噪声环境显著性检测，本文首次公开提出相关大规模数据集并给出有效基线算法，为该领域后续研究奠定了基础。

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [52] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: 本论文评估了包括GPT-5在内的多模态模型在空间智能任务上的表现，发现虽然取得了一定突破，但在许多方面仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在AI领域取得巨大进展，但在实现通用人工智能关键的空间理解和推理上仍有明显短板。随着GPT-5的发布，亟需系统评估其在空间智能上的能力。

Method: 作者提出了一个统一现有评测体系的空间任务分类法，并在8个核心基准上对主流闭源和开源多模态模型进行了对比，实验总消耗逾十亿token。此外，还进行了涵盖多种场景的定性评估。

Result: GPT-5在空间智能上表现出前所未有的实力，但在众多任务上仍未达到人类水平。部分高难度空间任务对所有多模态模型依旧具有挑战性，且闭源模型在此类问题上并未展现压倒性优势。

Conclusion: 尽管GPT-5等先进多模态模型在空间智能上取得了进步，但距离全面超越人类仍有差距。空间智能高难任务仍是未来AI发展的关键挑战。

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [53] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于超像素的连续低秩张量表示(SCTR)方法，有效突破了传统低秩张量方法在处理非均匀空间变异数据和仅适用于网格数据的限制，并显著提升了多维数据如多光谱图像和视频的处理效果。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量表示方法通常假定整体数据是低秩的，但在实际应用中数据常常存在显著的空间变化，导致整体低秩假设不成立。此外，这类方法通常只能处理离散网格数据，实际多维数据场景更加复杂和灵活，因此需要更加通用、灵活的模型。

Method: 本文提出SCTR框架：1) 利用语义一致的超像素作为基本建模单元，增强模型对空间变化的适应性；2) 提出一种新颖的非对称低秩张量分解(ALTF)方法，借助共享神经网络及专用分支对超像素专属因子矩阵进行参数化，使得模型在学习全局和本地特征时既高效又灵活。

Result: 在多光谱图像、视频及彩色图像等多个公开数据集上，SCTR方法较现有低秩张量表示法在PSNR指标上提升了3-5 dB，证明了其优越的表示力和泛化性。

Conclusion: SCTR方法通过超像素引导的连续低秩张量建模和创新性分解方式，实现了高效、灵活、表达力强的多维数据表示，极大扩展了低秩张量方法在实际复杂数据场景中的适用性。

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [54] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出并系统化了区域级上下文感知多模态理解（RCMU）任务，介绍了新的调优方法、数据集和评测基准，并开发出了表现优异的新多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型着重于一般视觉理解，忽视了将视觉对象与其文本上下文整合以实现更强上下文感知的需求。作者希望推动多模态模型在结合图像局部内容及其相关文本描述上的能力。

Method: 1）定义RCMU任务——要求模型结合图片内容与局部对象的文本信息响应指令。2）提出RCVIT方法，将对象信息（含位置信息）加入输入，使模型有效关联区域视觉内容与文本。3）构建RCMU大规模数据集。4）设计RC&P-Bench综合性评测基准及无参考评测指标，对区域级上下文描述进行细致评价。5）用RCMU数据集对Qwen2-VL系列模型进行RCVIT调优，得到RC-Qwen2-VL。

Result: RC-Qwen2-VL模型在多个RCMU任务表现优异，在多模态检索（RAG）和个性化多模态对话中有良好应用，综合评测指标也证实其区域级描述能力提升。

Conclusion: 本文提出的RCVIT方法和RCMU数据集及评测工具，有效提升了多模态大模型在区域级上下文感知理解的能力，为后续的多模态、个性化和上下文感知对话奠定了基础。

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [55] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的脉冲神经网络（SNN）架构——SNNSIR，用于双目图像复原任务，兼具高效能和低功耗，在多个复原任务上取得了有竞争力的表现，并大幅减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前SNN虽具高效低功耗优势，但在复杂图像复原任务中，由于表达能力和算子不兼容等问题，难以应用，且现有方法多为SNN-ANN混合架构，不具备完全硬件友好性。

Method: SNNSIR采用完全脉冲驱动架构，针对二值神经元表达能力弱的问题，提出Spike Residual Basic Block（SRBB）实现兼容脉冲残差学习；设计Spike Stereo Convolutional Modulation（SSCM）实现简化非线性和噪声区分；Spike Stereo Cross-Attention（SSCA）实现视图间特征高效交互，全流程兼容SNN。

Result: 在雨迹去除、雨滴去除、低照度增强和超分辨等多个典型双目图像复原任务上，SNNSIR在保证恢复效果的同时，显著降低了计算复杂度和能耗。

Conclusion: SNNSIR展示了SNN在复杂视觉任务中的应用潜力，实现了低功耗、实时性的复原处理，为SNN扩展至更多实际场景奠定了基础。

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [56] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对自动驾驶平台的语义分割网络自适应定制方法，能根据不同硬件与场景动态调整网络结构，实现性能与资源的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临多样化驾驶场景和不同的硬件资源与精度需求。嵌入式设备算力有限，需要在部署时充分考虑计算开销，提升硬件利用率与系统性能。

Method: 提出三层级控制机制（宽度因子、分类器深度、分类器卷积核），根据硬件约束和任务要求动态调整模型结构；利用贝叶斯优化和代理模型，在有限算力下高效搜索超参数，实现模型自动自适应；通过TSLA（任务特定学习自适应）调整MACs数量，获得多样化定制网络配置。

Result: 实现了针对不同自动驾驶硬件与场景的语义分割网络定制，模型可根据具体任务和硬件资源进行结构和计算资源分配优化。

Conclusion: 该方法提升了语义分割网络在自动驾驶平台上的适应性和性能，通过自动化定制优化了硬件利用率和模型准确率，适用于多元自动驾驶需求。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [57] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于弱监督的零样本跨域图像检索方法（CLAIR），利用CLIP等大型模型产生的伪标签，通过多种对比损失和跨域映射优化特征，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型（如CLIP）能够高效生成大量图片的伪标签，传统的无监督零样本跨域图像检索（UZS-CDIR）需求减少。因此，作者关注如何在弱监督（含噪伪标签）场景下进一步提升跨域图像检索的性能。

Method: 1）利用CLIP生成的图文特征相似性，为伪标签加上置信得分，提升标签质量；2）设计实例间、簇间、域间多种对比损失函数，编码更具类别感知的潜在空间，减轻域间差异；3）提出仅基于CLIP文本嵌入的显式跨域映射函数，实现跨域特征对齐；4）引入可学习的prompt增强模型对新类的零样本泛化能力。

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet等多个零样本跨域检索数据集上，CLAIR方法都取得了优于同类最新方法的表现。

Conclusion: CLAIR有效利用大型模型自动生成的弱标签数据，结合多种对比学习与跨域映射，显著提升了零样本跨域图像检索表现，并具备良好的泛化能力，对今后弱监督跨域检索研究有现实指导意义。

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [58] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 本文针对3D Gaussian Splatting（3DGS）在实时渲染中的重建质量问题，从多个角度系统性地改进其密度增强流程，并在保证高效性的同时提升了渲染效果。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽然在实时渲染方面表现优异，但其当前的点云密度增强（densification）策略常导致次优的重建质量，急需提升渲染保真度和算法效率。

Method: 本文从三个方面全面改进了3DGS的密度增强流程：（1）提出Edge-Aware Score，用于更有效地选择分裂候选高斯点；（2）引入Long-Axis Split策略，减少几何变形；（3）设计包括Recovery-Aware Pruning、Multi-step Update及Growth Control在内的一套防止过拟合的技术。

Result: 改进后的方法在不增加训练或推理开销的前提下，使用更少的高斯点实现了更高的渲染保真度，并在多个评测中达到了最新最优性能。

Conclusion: 本文提出的系统性改进显著提升了3DGS的重建和渲染质量，为高效、真实的3D实时渲染提供了更优的解决方案。

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [59] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 本文提出了一种利用神经元元胞自动机（NCA）实现白细胞弱监督分割的新方法，在无需重新训练分割标签的情况下即可生成分割掩码，并在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有白细胞检测和分割依赖大量人工标注数据，获取成本高、耗时。需要一种标签需求低但效果优良的新方法，以解决医学图像分析中的数据瓶颈。

Method: 提出了基于神经元元胞自动机（NCA）的弱监督分割方法（NCA-WSS），通过使用NCA分类产生的特征图来直接提取分割掩码，无需标注分割标签或重新训练。

Result: 在三个白细胞显微镜图像数据集上验证了该方法，结果显示NCA-WSS在弱监督分割任务中明显优于现有方法。

Conclusion: 本工作证明了NCA在弱监督医学图像分类和分割中的潜力，能高效扩展至实际应用，显著降低标注成本。

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [60] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 本文将注意力池化机制引入神经细胞自动机（NCA），提升了其在显微图像分类中的特征提取能力和分类准确率。提出的方法在8个多样的显微图像数据集上表现优越，且参数量低、可解释性强。


<details>
  <summary>Details</summary>
Motivation: 神经细胞自动机（NCA）具有鲁棒性和可解释性，是显微图像分析的有前途选择，但其分类性能尚不及更复杂的大型架构。为提升NCA性能，需改善其特征提取和分类准确率。

Method: 通过将注意力池化机制集成到NCA网络中，使其能够更有效地关注图像中的关键信息区域，从而提高特征提取能力和最终分类准确率。

Result: 在八个多样化的显微图像数据集上评估表明，该方法显著优于现有的NCA方法。また相比于传统的轻量级卷积神经网络和视觉Transformer架构，提出方法在保持较低参数量的前提下取得了更优性能。

Conclusion: 集成注意力池化机制后的NCA模型，可作为可解释、参数高效的图像分类方法，尤其适合对可解释性要求较高的显微图像分析领域。

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [61] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: 本文提出了一种名为DoppDrive的新颖雷达点云时序聚合方法，通过利用多普勒信息提升点云密度，并显著提高自动驾驶中的雷达目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有雷达点云目标检测方法受限于点云稀疏性，尤其在远距离处更为明显。为提升准确率，常用时序聚合进行点密度提升，但会因动态目标带来的散射影响，导致检测性能下降。

Method: DoppDrive方法在时序聚合过程中，首先依据多普勒分量将历史帧点云进行径向移动，消除径向散射；同时，基于每个点的多普勒值和角度分配独特的聚合时长，减少切向散射，从而提升点云质量。该过程独立于具体检测器，可用作通用点云增强步骤。

Result: 实验表明，DoppDrive能广泛兼容各类检测模型和数据集，在提升点云密度的同时，带来检测性能的大幅提升。

Conclusion: DoppDrive作为前置点云密度增强模块，能够有效缓解动态目标时序聚合中的散射问题，提升检测精度，并且适用于主流雷达检测模型。

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [62] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 头戴式显示器（HMD）会遮挡用户面部，影响虚拟现实社交体验。本文提出结合视频修复和三维人脸重建的学习框架，实现去除HMD后的高质量3D人脸重建。


<details>
  <summary>Details</summary>
Motivation: HMD虽是扩展现实必备，但会遮住上半张脸，影响视频会议等社交XR应用中表情与视线交流。缺乏去除HMD遮挡并还原完整3D人脸的方法，尤其是能保持真实感和身份一致性的方法。

Method: 提出基于几何建模的学习框架：先用GAN引导的视频修复网络（融合稠密人脸关键点和一张无遮挡参考帧）填补HMD遮挡区域，并保持身份一致；再用SynergyNet网络对修复后帧回归3DMM参数重建3D人脸。整个流程通过关键点优化提升修复和重建质量。

Result: 实验证明该方法能有效去除HMD遮挡，恢复面部细节且重建三维几何真实；消融实验显示方法对关键点密度鲁棒性强，稀疏情况下质量退化有限。

Conclusion: 提出的联合去遮挡和高保真3D重建框架为HMD场景下人脸社交交互提供了解决方案，在保持人脸真实性和身份的同时，实现了高质量3D几何复原，对社交XR应用具有实际意义。

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [63] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出了一种新的语义差异感知检测器（SDD），通过对比和对齐语义概念与伪造空间来提升图像伪造检测性能，在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的发展，如何有效检测图像伪造以确保数字媒体的可信度变得非常重要。已有研究表明，预训练模型中的语义概念对于识别伪造图像至关重要，但伪造空间和语义空间的不对齐影响了检测效果。

Method: 作者提出了一种新的语义差异感知检测器（SDD），其核心包括三个模块：语义token采样模块（减少与伪造或语义无关的特征）、基于可视化重建的概念级伪造差异学习模块（增强语义与伪造痕迹的交互）、低层伪造特征增强模块（整合语义伪造差异，减少冗余特征）。整个方法依赖于预训练视觉语言模型的知识。

Result: 在两个标准的图像伪造检测数据集上进行了实验，所提SDD方法表现优异，性能超过了当前主流的伪造检测方法。

Conclusion: 所提的SDD方法能有效对齐语义空间和伪造空间，提高了图像伪造的检测能力，为可信数字媒体提供了有力工具。

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [64] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 本文提出了AquaFeat模块，通过任务驱动的特征增强提高了水下目标检测的性能，优于传统增强方法，兼具高精度与高效率。


<details>
  <summary>Details</summary>
Motivation: 传统的水下图像增强方法没有针对下游的目标检测任务进行优化，导致检测模型在严重退化的图像上表现不佳。为解决水下环境下目标检测精度低的问题，亟需一种与检测任务紧密结合的特征增强方法。

Method: 提出了AquaFeat模块，这是一种可插拔的任务驱动特征增强模块。该方法将多尺度特征增强网络与检测器的损失函数端到端联合训练，使得增强直接针对检测任务相关特征进行优化，以提升目标检测表现。

Result: 将AquaFeat集成到YOLOv8m后，在复杂水下数据集上取得了先进的精度（Precision 0.877）、召回率（Recall 0.624）和竞争力的mAP（mAP@0.5为0.677，mAP@[0.5:0.95]为0.421），并保持了46.5 FPS的推理速度。

Conclusion: AquaFeat模块显著提升了水下目标检测的精度和召回率，同时保持了高效的计算速度，可满足水下生态监测和基础设施检测等实际应用需求。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [65] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于Mamba架构的图像去模糊网络，通过引入记忆缓冲机制和Ising正则化损失，在不增加额外复杂度的情况下，有效提升了图像结构信息的保留和去模糊性能。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba架构在图像去模糊任务上的flatten-and-scan策略容易造成像素信息丢失和通道冗余，且现有改进方法常常导致计算复杂度增加，影响实时性。作者希望设计一种在保持Mamba架构高效性的同时解决这些问题的方法。

Method: 本文提出了结构感知的图像去模糊网络：1) 设计了一种记忆缓冲机制，在扫描过程中保留历史信息以更好融合特征，2) 引入受Ising模型启发的正则化损失，模拟像素间的“互吸”关系，提升结构一致性。构建了MBMamba网络，并未改变Mamba原有架构。

Result: 在多个主流基准测试上，MBMamba优于当前最新方法，实现了更优秀的图像去模糊表现。

Conclusion: 提出的方法能够在不增加Mamba架构复杂度的前提下，提升图像去模糊时的结构保留和特征聚合能力，有望为实时高效的图像去模糊应用提供更优选择。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [66] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需事先标注、能够在第一人称视频中精准定位手与物体接触与分离时刻的新方法EgoLoc，极大提升了虚拟现实/增强现实和人机交互领域的应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注交互方式本身（如“如何交互”），而对手与物体“何时接触或分离”这一更细粒度、难度更高且对混合现实和机器人运动规划至关重要的问题研究甚少。

Method: 作者提出将手物体的时序交互节点定位问题（TIL）作为任务，开发了EgoLoc方法。EgoLoc运用基于手部动态的采样，借助视觉-语言模型甄别接触/分离属性，精准本地化关键时刻并自带闭环反馈，无需依赖物体掩码和动作分类标注，实现零样本泛化。

Result: 在公开数据集和自建基准上，大量实验证明EgoLoc能够有效识别第一人称视角下手与物体的接触及分离时刻，并成功支持多种下游应用，如机器人操作任务。

Conclusion: EgoLoc实现了无需标注与先验知识的泛化时序交互定位，为AR/VR和机器人交互等前沿应用提供了关键技术支持。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [67] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 该论文提出一种通过生成额外合成训练数据来提升视觉类offline RL（离线强化学习）泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习依赖于预采集数据，导致政策泛化能力有限，特别是在视觉类任务中容易过拟合，难以应对噪声与分布外环境。为提升视觉offline RL泛化能力，亟需增加数据多样性。

Method: 作者提出了两步法：首先对原始离线数据进行增强，引入多样性以提升零样本泛化能力；其次利用扩散模型在潜在空间中生成额外数据，进一步丰富训练集。该方法适用于连续动作空间（Visual D4RL）和离散动作空间（Procgen），且无需修改现有离线RL算法。

Result: 实验结果表明，该方法显著提升了泛化能力，减小了测试时的泛化差距，同时保证了计算效率。

Conclusion: 本方法能有效增加训练数据多样性，提升视觉offline RL的泛化和鲁棒性，对未来利用合成数据训练通用智能体具有重要推动作用。

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [68] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为IPGPhormer的新方法，用于提升病理图像在癌症预后分析中的预测效能及可解释性。该方法结合图网络与Transformer结构，能自动捕捉组织及细胞层面的空间关系，同时实现端到端的可解释性。实验证明该方法在多个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有WSI（全视野病理切片）分析方法难以兼顾长距离空间关系的建模和局部上下文依赖，同时缺乏内在可解释性，限制了其临床应用价值。提高模型对肿瘤微环境特征的理解并增强可解释性，对改善癌症预后分析具有重要意义。

Method: 提出IPGPhormer框架，将病理图像转化为图结构，对肿瘤微环境的空间依赖性进行建模。框架融合了图神经网络与Transformer，能够同时实现长距离空间关系与局部上下文的结合。不同于以往需要额外人工注释的方法，IPGPhormer能在组织及细胞层面进行自动化可解释性分析。

Result: 在四个公开基准病理数据集上进行实验，IPGPhormer在预测准确性和可解释性方面，均显著优于当前主流方法，展示了其优越的泛化能力和实用性。

Conclusion: IPGPhormer为病理图像癌症预后分析提供了一种更为可靠且可解释的工具，具有推动临床辅助决策系统发展的潜力。

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [69] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 本文提出了一种针对Vision Transformer（ViT）的新型集成式对抗攻击方法ViT-EnsembleAttack，显著提升了对抗样本的迁移性，在同类方法中表现优异。


<details>
  <summary>Details</summary>
Motivation: 以往集成式对抗攻击大多侧重于模型权重或路径优化，忽视了通过对集成模型自身进行改进来增强对抗样本迁移性的可能性，尤其是在ViT架构中关注较少。

Method: 提出对代理ViT模型进行对抗式增强，采用多头丢弃、注意力分数缩放和MLP特征混合三种策略，并结合贝叶斯优化选择参数。将增强后的模型集成生成对抗样本，进一步引入自动重加权和步长放大模块提升迁移性。

Result: 大量实验表明，所提ViT-EnsembleAttack方法在ViT上的对抗样本迁移性显著优于现有集成攻击方法。

Conclusion: 通过对ViT代理模型的对抗式增强并结合新型集成方法，可极大提升对抗攻击的迁移性，为ViT体系下的对抗研究提供了新的思路和有效工具。

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [70] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: 当前的文本生成图像（T2I）模型难以处理复杂、长篇的文本指令，DeCoT框架通过引入大语言模型来对复杂指令进行分解和增强，显著提升了T2I模型在细节、空间关系和约束处理上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在面对复杂指令时，常出现细节遗漏、空间关系混乱等问题，尤其是在LongBench-T2I等基准测试中暴露明显缺陷，因此需要新方法提升模型对复杂文本指令的理解与执行能力。

Method: 提出DeCoT（Decomposition-CoT）框架，分为两个核心阶段：1）利用LLM对原始复杂指令进行语义分解和增强，明确具体指令单元并消除歧义；2）将这些结构化语义单元转化为多阶段或优化后的单一提示词，再输入现有的T2I模型生成图像。

Result: 在LongBench-T2I数据集的多项测试中，DeCoT集成Infinity-8B后平均分数达3.52，优于基线Infinity-8B模型的3.44。多个评估器验证了模型在‘文本’和‘构图’等方面的显著提升，消融实验和人工评测也确定了各组件的重要作用。

Conclusion: DeCoT能够提升T2I系统解释和执行复杂文本指令的能力，生成图像的感知质量和指令忠实度均明显优于传统方法，有效弥合用户意图与T2I模型输入需求之间的鸿沟。

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [71] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: 本论文提出FedCSAP，一种面向联邦学习的跨模态风格感知提示生成方法，能融合多尺度视觉特征与风格，提升泛化能力，在图像分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（如CLIP）已广泛应用于联邦学习，由于其计算高效，适合边缘设备。然而，传统只用最后一层特征的方法无法捕捉多尺度视觉信息与客户端数据的风格多样性，导致泛化能力受限。

Method: FedCSAP方法结合CLIP视觉编码器的低、中、高层特征以及利用批量统计提取的客户端风格指示符，将精细视觉特征与文本上下文结合，生成有区分性与无冗余的上下文相关提示符。在联邦学习架构下，实现本地训练、全局聚合，兼顾数据隐私和非IID分布。

Result: 在多个图像分类数据集上的实验表明，FedCSAP在准确率和泛化能力上均超越现有的联邦提示学习方法。

Conclusion: 整合多尺度特征及风格信息，FedCSAP有效提升了联邦视觉-语言模型的泛化能力，并能应对实际中的非IID分布和风格多变性，具有广阔应用前景。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [72] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: 本文提出了一种提升大型视觉语言模型（LVLM）复杂视觉推理能力的新方法：多视角上下文增强推理（MPCAR），通过在推理时自动生成并整合多元视角信息，显著提升了VQA等任务表现，无需对模型参数进行微调。


<details>
  <summary>Details</summary>
Motivation: LVLM虽在视觉-语言任务有突破，但在需深入语境、多角度分析、细致识别等复杂推理任务上表现有限。现有方法通常只做一次图片编码和单一提示，难以捕捉图像的复杂细节和多样推理路径。作者希望弥补这方面的不足。

Method: MPCAR方法包含三步：1）LVLM先从多个视角自动生成N个多样且互补的图像描述或初步推理路径；2）将这些描述与原始问题智能整合，生成内容丰富的上下文增强型提示（prompt）；3）用增强提示再次引导LVLM进行更深入推理和答案输出。该流程无需模型微调。

Result: 在GQA、VQA-CP v2和ScienceQA等VQA挑战数据集上，MPCAR相较主流基线获得了显著准确率提升，尤其是在强依赖语境理解的任务上效果更佳。人类评测也表明该方法能输出更加连贯、完整的答案。消融实验进一步验证了多样模板和视角数量的重要性。

Conclusion: MPCAR证明了通过LVLM生成式能力在推理时丰富输入语境，无需微调即可极大激发其复杂多模态推理潜力，为VQA等任务带来新突破。

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [73] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型适用于自动驾驶的视觉-语言模型框架LMAD，显著提升了场景推理任务的表现，并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言大模型虽然在场景理解和与用户交互上有所进展，但通常对复杂场景的整体与细致识别、空间感知能力有限，难以满足自动驾驶的高要求。作者希望弥补自动驾驶场景下这些不足。

Method: 作者设计了LMAD框架，融合了自动驾驶端到端流程，加入了场景交互机制和专家适配器，使VLM更好适应专门的自动驾驶场景。框架既与现有VLM兼容，也能集成到规划型驾驶系统中。

Result: 在DriveLM和nuScenes-QA等数据集上，大量实验证明LMAD显著提升了现有VLM在驾驶推理任务中的表现。

Conclusion: LMAD为可解释自动驾驶树立了新标杆，有效提升了VLM用于自动驾驶领域的场景理解和推理能力。

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [74] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 本论文提出了S5框架，这是首个面向遥感影像的可扩展半监督语义分割方法。通过数据挑选和大规模预训练，极大提升了模型在多任务、多个基准上的实际表现。


<details>
  <summary>Details</summary>
Motivation: 现有半监督遥感语义分割研究多聚焦于小规模数据与模型，难以满足真实应用对大规模无标注数据的需求，同时像素级标注成本高昂。该论文旨在解决大规模遥感场景下利用无标注数据有效提升语义分割性能的问题。

Method: 提出了S5框架，通过熵值筛选与多样性扩张选取大规模高质量无标注遥感数据，得到RS4P-1M数据集。在大规模数据上预训练不同规模的遥感基础模型（RSFM），并在下游任务微调阶段引入MoE多数据集微调策略，实现跨基准的高效学习和泛化。

Result: 通过在多个遥感基准上的实验证明，所提出的RSFM模型在土地覆盖分割及目标检测等任务上均取得了最新的优秀性能，显著优于现有方法。

Conclusion: 本文创新性地扩展了遥感领域半监督语义分割的规模和能力，提出的S5框架和方法可以推动实际遥感应用的落地。所有数据、代码与模型将对外公开，促进领域发展。

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [75] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: 本文提出了SRMA-Mamba网络，有效提升了MRI肝硬化病变的三维分割精度。


<details>
  <summary>Details</summary>
Motivation: 肝硬化对慢性肝病的预后至关重要，但由于肝脏结构复杂和病变多样，现有方法很难精准分割MRI中的肝脏病变区域，且空间解剖信息利用不足，影响了其临床可用性和可解释性。

Method: 提出基于Mamba的新型网络SRMA-Mamba，核心包括空间解剖Mamba模块（SABMamba），在肝硬化部位选择性扫描并整合多平面（矢状、冠状、轴状）解剖信息，构建全局空间上下文，实现高效三维分割。同时引入空间反向注意模块（SRMA），通过粗分割与分层编码细化病变边界。

Result: SRMA-Mamba在肝脏病理分割三维任务中，分割精度显著优于当前最先进方法（SOTA）。

Conclusion: SRMA-Mamba能有效捕捉和融合MRI中肝脏复杂的空间解剖特征，提升了肝硬化分割的精度，具有良好临床应用前景。代码已开源。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [76] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出TiP4GEN框架，实现了高质量、支持细粒度内容控制的动态全景场景生成，突破了现有静态或局部动态场景的限制。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR技术的迅速发展，对高质量、沉浸式动态场景的需求日益增长。传统方法多聚焦于静态或视角有限的动态场景，难以满足360度沉浸体验的要求，因此亟需新的生成方法。

Method: 作者提出TiP4GEN框架，结合了全景视频生成与动态场景重建。视频生成方面引入双分支结构（全景分支与透视分支），通过双向交叉注意力促进全局与局部信息融合。场景重建方面，基于3D高斯斑点法，利用度量深度图对时空点云进行空间-时间对齐，并通过姿态估计初始化相机，确保几何一致性和时序连贯性。

Result: 大量实验结果表明，TiP4GEN在生成视觉效果逼真、运动连贯的动态全景场景方面相较现有方法具有明显优势。

Conclusion: TiP4GEN实现了可控且几何一致的高质量动态全景场景生成，在沉浸式虚拟环境构建领域具有显著价值。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [77] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 本文通过比较生物和人工感知系统在视觉错觉上的反应，揭示了它们在构建视觉现实时的关键差异，并探讨了这些发现对更健壮、人类一致性更强的AI视觉系统设计的启示。


<details>
  <summary>Details</summary>
Motivation: 随着人工视觉系统在越来越多具有人类色彩的任务中崭露头角，理解其是否像人类一样也会“感知”错觉，并探寻AI系统是否存在其特有错觉现象，对于提高AI的可解释性与对齐性至关重要。

Method: 作者系统性地对比了AI视觉模型和人类在经典视觉错觉（涉及颜色、尺寸、形状、运动）上的响应，并分析AI的错觉表现是训练特定任务还是模式识别副产品。此外，还探讨了AI特有的像素敏感性和幻觉现象。

Result: 研究发现，部分人类视觉错觉现象可以在AI模型中通过有针对性的训练或模式识别自发出现。但AI也存在独有的错觉现象（如像素级敏感和幻觉），这些现象在人类感知体系中并不具备类似物。

Conclusion: 该研究揭示了AI与人类在视觉错觉反应上的差异和AI特有感知脆弱性，对设计兼具人类有益感知偏好及更高安全性的AI视觉系统具有重要启示。

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [78] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 这篇论文分析了视觉问答自然语言解释（VQA-NLE）系统在生成解释时存在不一致和缺乏理解的问题。作者通过提问扰动及新提出的图像微调策略，揭示了这些系统的脆弱性，并提出利用外部知识增强模型鲁棒性的办法。实验显示现有系统安全性仍需提升。


<details>
  <summary>Details</summary>
Motivation: VQA-NLE系统被广泛用于提升模型可解释性，但目前它们的解释容易出现前后矛盾或对上下文理解不足，影响了模型的可信度和可用性。

Method: 作者采用了现有的对抗性提问扰动策略，并创新性地提出对图像进行最小化修改的新攻击方法。同时，提出通过引入外部知识来缓解模型解释不一致问题。然后在两个标准基准数据集和两种主流VQA-NLE模型上进行了实验。

Result: 实验证明，提出的对抗性攻击策略能够显著影响VQA-NLE模型的输出，而引入外部知识可有效缓和模型表现出的脆弱性，提升鲁棒性。

Conclusion: 当前VQA-NLE系统在安全性和可靠性方面存在重要隐患，作者的方法能有效揭示并缓解这些问题。未来需要进一步提高相关系统的稳健性以适应实际应用。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [79] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: 论文提出了X-Ray-CoT，一个结合视觉-语言大模型（LVLMs）与Chain-of-Thought推理机制的胸部X光智能诊断及可解释报告生成框架，有效提升诊断精度及报告可解释性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片在肺部和心脏疾病诊断中至关重要，但人工解读依赖丰富经验并存在较大主观差异，传统深度学习模型虽然准确率高，但缺乏可解释性，限制其在临床高风险场景的应用。

Method: X-Ray-CoT框架首先提取多模态特征和视觉概念，然后引入结构化Chain-of-Thought提示方式，引导大语言模型进行逐步推理和详细诊断报告生成，并与现有黑盒模型对比性能。

Result: X-Ray-CoT在CORDA数据集上实现了80.52%的Balanced Accuracy和78.65%的F1分数，略优于现有黑盒模型。更重要的是，该方法能生成高质量、可解释的诊断报告，并通过初步人类评估验证了报告解释效果。消融实验显示多模态融合及CoT推理对性能提升有决定作用。

Conclusion: X-Ray-CoT实现了胸部X光影像智能诊断和解释性报告的有机结合，显著推动了医学影像AI可用性和可信度，为临床实际应用迈出关键一步。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [80] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: 提出了Inverse-LLaVA模型，颠覆传统多模态学习对齐预训练的必要性，将文本嵌入映射到视觉空间，显著降低计算需求，并在复杂推理任务上取得优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法依赖耗费巨大的视觉-文本对齐预训练，将视觉特征映射到离散文本空间。这不仅成本高，还限制了模型效率与创新。因此，论文旨在探索无需对齐预训练的新范式。

Method: 提出Inverse-LLaVA模型，创新性地将文本嵌入映射至连续视觉表征空间，并在transformer中间层实现模态融合。引入选择性加法注意力机制，无需大量对齐数据即可动态融合多模态信息。

Result: 在九个多模态基准上，Inverse-LLaVA在需要复杂推理和认知能力的任务上表现突出（如MM-VET、ScienceQA等），在对记忆视觉-文本关联要求高的感知任务上有所下降；整体计算资源需求降低45%。

Conclusion: 实验证明多模态学习在复杂推理任务中无需对齐预训练，促进了保存模态特性的高效架构发展，挑战了传统模态融合观念，开辟了新研究方向。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [81] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 本论文提出了一种结合微调视觉-语言模型（VLM）联盟与推理大语言模型（LLM）的自动化H-反射波形解读与诊断系统，有效提升了神经肌肉诊断的准确性与标准化水平。


<details>
  <summary>Details</summary>
Motivation: 传统H-反射肌电信号分析存在解读偏差和一致性差等问题，难以实现高效标准化。针对这一挑战，研究旨在通过AI手段，提升波形分析的自动化与客观性。

Method: 作者构建了多种针对H-反射波形图片及临床注释数据微调的VLM模型群，自动提取关键电生理特征及状态，并将多模型结果以共识法则聚合后，交由专门设计的推理型LLM进一步解释和决策。整合流程结合了提示工程和LLM Agent自动化推理。

Result: 实验结果显示，该系统能够实现高准确性、一致性和可解释性的H-反射评估，明显提升了诊断的自动化与标准化水平。

Conclusion: 该研究首次实现了VLM联盟与推理型LLM结合用于H-反射图像分析，为新一代AI驱动的神经肌肉评估和运动员监测平台奠定了基础。

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [82] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 本研究提出了结合CNN与Transformer结构的混合模型，并引入了CKAN网络，用于提升皮肤癌医学图像分类的准确性和鲁棒性。模型在多个公开数据集上均取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期诊断依赖于高效准确的图像分类，但恶性与非恶性病变很难区分，现有方法在空间与上下文特征捕捉上存在局限。因此，迫切需要开发能够有效利用多维特征信息的分类模型。

Method: 本文提出了序列和并行混合的CNN-Transformer结构，CNN负责提取局部空间特征，Transformer建模全局依赖关系，再通过CKAN进行非线性特征融合。此外，采用迁移学习和广泛的数据增强以提升模型泛化能力。

Result: 混合模型在HAM10000、BCN20000和PAD-UFES这三个公共皮肤病图像数据集上分别取得了92.81%、91.17%、97.83%的准确率和相近的F1分数，显示出模型对多样数据分布和类别不均衡情况的良好适应性。

Conclusion: 结合CNN、Transformer及CKAN的混合模型能有效提升医学图像分类的判别能力和泛化能力，对未来皮肤癌等疾病的计算机辅助诊断具有实用价值。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [83] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: 本文提出了一种负责任的人工智能系统RAIS-DR，用于糖尿病视网膜病变（DR）早期筛查，比现有FDA认证系统在多项指标上表现更好，并且兼顾公平性。


<details>
  <summary>Details</summary>
Motivation: DR是劳动年龄人群失明的重要原因，早期筛查可大幅降低失明风险，但受限于专业医生短缺和筛查难度。AI技术虽已应用，但在现实中受到数据质量和偏见问题影响，难以大规模采用。作者旨在开发兼具高准确性和伦理原则的DR筛查AI系统。

Method: 开发了RAIS-DR系统，涵盖数据预处理、质量评估和三种专用DR分类模型，融入AI生命周期的伦理原则。并与FDA认证的EyeArt系统在1046例未见过的本地数据集上进行了性能对比。

Result: RAIS-DR系统在F1分数、准确率和特异度上均比EyeArt系统提高5-20%；在公平性指标（Disparate Impact和Equal Opportunity Difference）上各亚群体表现均衡，有望减少医疗不平等。

Conclusion: RAIS-DR是一个稳健且伦理一致的DR筛查AI系统，在临床环境具有优异表现和公平性，适合推广应用。相关代码和模型参数已开源。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [84] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: 该论文提出了一种结合神经架构搜索（NAS）与LoRA的多模态模型微调框架LangVision-LoRA-NAS，能够为不同任务自动优化LoRA秩配置，实现性能提升及成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs（视觉语言模型）普遍采用固定秩LoRA进行高效微调，但固定秩限制了其在不同任务下的灵活性和效率。因此，研究者希望探索能否根据任务需求动态调整LoRA秩，从而提升性能同时减少计算资源消耗。

Method: 作者提出LangVision-LoRA-NAS框架，将NAS（神经架构搜索）与LoRA结合，对LoRA中的秩参数进行动态优化。该方法针对不同多模态任务，通过自动搜索找到最优的低秩更新策略，兼顾性能与效率。实验基于LLaMA-3.2-11B模型和多个数据集验证了方法有效性。

Result: 实验表明，LangVision-LoRA-NAS在LLaMA-3.2-11B模型上的多模态任务中，能够显著提升模型性能，并降低微调所需的计算成本。

Conclusion: LangVision-LoRA-NAS为多模态大模型微调提供了高效灵活的解决方案，在性能和成本之间取得良好平衡，具有良好实际应用前景。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [85] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 本文利用Cross-View Transformers（CVT）从摄像头图像生成鸟瞰图（BEV），以提高自动驾驶感知能力，并通过模拟器评估其在城市场景下的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，准确的BEV鸟瞰图对于环境感知与安全驾驶至关重要。传统方法通常难以从摄像头图像直接推断出准确的BEV特征，因此需要开发更高效的图像到BEV的映射方法。

Method: 作者采用Cross-View Transformer（CVT）模型来实现从多摄像头图像到BEV地图（包含道路、车道标记和规划轨迹三个通道）的学习映射，并在高真实感的城市驾驶模拟器中进行训练测试。研究还探讨了不同摄像布局和两种损失函数（focal loss与L1 loss）对性能的影响。

Result: 实验显示，在仅用一个城镇的训练数据基础上，采用四个摄像头并结合L1损失函数的CVT模型，在新的未见过的城镇测试中表现最为稳健。

Conclusion: CVT方法能有效实现摄像头输入到BEV鸟瞰图的合理准确映射，展现出较强的泛化能力和应用潜力，有望推动自动驾驶感知系统的发展。

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [86] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态、个体化情感识别方法MuSACo，有效提升了情感识别的准确性和鲁棒性，并在两个挑战性数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多源域自适应（MSDA）方法在情感识别中往往忽视了多模态信息，或者将所有源域简单混合，导致无法充分刻画个体差异，难以满足个体化识别的需求，特别是在健康等需要细颗粒度个性分析的场景。

Method: MuSACo方法基于协同训练，结合多域（每个数据域对应不同被试）和多模态信息，通过选择和组合与目标个体最相关的源个体，利用优势模态生成伪标签、进行类别相关学习，同时结合类别无关损失应对低置信度样本。最后，对各模态的源特征对齐，仅对置信度高的目标特征进行融合学习。

Result: 在BioVid和StressID两个具有挑战性的多模态情感识别数据集上，MuSACo方法超越了融合法（UDA）和当前最先进的多源域自适应方法，表现出更高的性能。

Conclusion: MuSACo不仅更好地利用了多模态和个体化信息，提升了情感识别的精度，而且在数字健康等关注个体差异的实际应用中具有广泛前景。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [87] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: 本文提出了一个基于大规模视觉-语言模型的图像伪造检测新框架REVEAL，能够自动检测、定位伪造区域，并提供推理解释，兼具泛化性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的发展，视觉伪造变得越来越普遍，现有方法在跨领域泛化性和可解释性方面存在不足，因此需要更健壮、泛化性好且能够解释推理过程的检测方法。

Method: 将伪造检测视为基于提示（prompt）的视觉推理任务，利用大规模视觉-语言模型（Vision-Language Models, VLMs）的语义对齐能力，提出REVEAL框架，包括：（1）整体场景级评估——关注图像整体的物理、语义、透视和真实感；（2）区域级异常检测——将图像分割为多个区域并逐一分析。

Result: 在来自多个领域（如Photoshop、DeepFake和AIGC编辑）的数据集上进行实验，VLMs与主流基线方法进行了对比，并详细分析了VLMs给出的推理解释。

Conclusion: REVEAL框架展示了在不同类型伪造检测任务中的泛化能力，并能输出可解释的推理，有助于增强图像伪造检测系统的实用性和可信度。

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [88] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 本文针对多领域视觉推理VLM训练数据单一及泛化能力有限的问题，提出并构建了一个跨域多任务视觉推理数据集，以及高质量样本筛选和多轮强化学习训练的方法，最终训练出了性能领先的新模型Vision-G1。


<details>
  <summary>Details</summary>
Motivation: 当前VLM主要专注于数学和逻辑推理任务，数据集领域狭窄，导致模型难以泛化到更多推理场景，且跨领域数据兼容性问题突出。

Method: 1) 从46种数据源、8大任务维度构建涵盖广泛领域的视觉推理数据集；2) 通过影响函数+难度过滤策略筛选高质量训练样本；3) 基于数据课程与多轮强化学习对VLM（Vision-G1）进行训练，不断提升其推理能力。

Result: Vision-G1在多个视觉推理基准测试上取得了最新最优结果，超越同等规模VLM，以及专有模型GPT-4o和Gemini-1.5 Flash。

Conclusion: 本工作突破了VLM推理训练的领域局限与数据瓶颈，提出的多域数据集和训练方案有效提升了模型的泛化与推理能力，推动了VLM在更广泛推理任务上的发展。

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [89] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种针对老照片上色的新方法，能在极少数据的情况下实现高质量的参考图像上色，并有效克服了自然灰度图像与老照片之间的领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习上色方法大多依赖大规模数据集和自然图片，对缺乏真实上色标注且风格与现代图像差异较大的老照片表现不佳，因此亟需一种解决领域差异、数据稀缺的更通用方法。

Method: 作者提出了一种新颖的卷积神经网络算法SFAC，仅需两张图片即可训练并完成老照片参考上色。该方法通过特征分布对齐损失建立语义对应关系，保持语义相关区域颜色一致。同时，引入结构保持机制（特征层感知约束+像素层多层冻结金字塔），减少由于颜色迁移带来的结构扭曲。

Result: 实验结果显示，SFAC在多个定性和定量指标上均优于现有方法，能够在老照片场景下实现更自然、结构保真的上色效果。

Conclusion: SFAC打破了对大数据的依赖，实现了直接基于老照片自身特征的高效上色，为领域特殊图像的上色提供了全新思路，具有良好应用前景。

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [90] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的骨架密集表征学习（USDRL）框架，作为骨架基础模型，显著提升了多种人体动作理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 骨架为人体建模提供了与设备和模态无关的表示，广泛应用于机器人控制和交互。然而，目前缺乏具备可扩展性和泛化性的骨架基础模型，难以胜任多样化的动作理解任务。

Method: USDRL框架包含三大模块：1）基于Transformer的密集时空编码器（DSTE），采用双流结构分别学习时序动态和空间结构特征；2）多粒度特征去相关（MG-FD），在时间、空间和实例域协同去除相关性，增强信息提取能力；3）多视角一致性训练（MPCT），结合多视角和多模态自监督，提升高层语义学习与多模态特征表达。

Result: 在25个基准数据集和9种骨架动作理解任务上，涵盖粗粒度、密集预测及迁移预测，USDRL框架显著优于当前最优方法。

Conclusion: USDRL作为首个统一骨架基础模型，推动了密集预测等骨架动作理解任务的发展，有望成为该领域研究的重要方向。

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [91] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 现有的大型多模态模型常借用文本链式思维(CoT)推理方法，但在多模态任务(如音频、视觉、文本)中效果有限。本文提出Multimodal Chain of Continuous Thought (MCOUT) 方法，以连续潜在空间的推理状态替代自然语言，显著提升模型多模态推理能力，并在多个权威基准数据集上有较大进步。


<details>
  <summary>Details</summary>
Motivation: 针对当前多模态推理采用CoT等语言表达方法时存在的信息对齐效率低、泛化能力有限等问题，作者希望探索无需局限于自然语言的新型“隐式连续推理”机制。

Method: 提出MCOUT方法，将推理状态设计为潜在空间中的连续向量，并通过多次迭代优化，与视觉和文本特征进行动态对齐。包括两个变体：1) MCOUT-Base，利用语言模型最后隐藏状态作为连续思维表达迭代推理；2) MCOUT-Multi，将多模态隐式注意力机制整合进去，加强跨模态关联。

Result: 在MMMU、ScienceQA、MMStar等主流多模态基准集上，MCOUT方法在多项选择题和开放性任务中，对比领先基线最多提升8.23%的准确率、8.27%的BLEU分数。

Conclusion: MCOUT展示了潜在连续推理在提升多模态推理中的潜力，突破了仅依赖语言表达的瓶颈，是迈向类似人类反思式多模态推理的重要新方向。

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [92] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一种基于扩散模型的新型视觉语言大模型自动驾驶方法，在推理速度和决策准确性上超过了自回归VLM方法，并能在真实车辆上稳定运行。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视觉语言大模型大多采用自回归结构，但这种逐token生成方式导致推理延迟高，且只能进行单向推理，难以满足动态和安全关键场景的需求。

Method: 提出ViLaD框架，将大规模视觉语言扩散模型（LVLD）用于自动驾驶。通过掩码扩散模型可并行生成完整的驾驶决策序列，支持双向推理与递进式easy-first决策优化。模型在nuScenes数据集和真实车辆场景分别进行测试和部署。

Result: 在nuScenes上，ViLaD在规划精度和推理速度上均优于主流自回归VLM，达到几乎零失误率。在真实车辆的交互式泊车中也表现出良好实际效果。

Conclusion: ViLaD框架显著提升了端到端自动驾驶的推理效率与决策质量，有望推动VLM自动驾驶模型在实际环境中的应用落地。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [93] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 该论文提出了首个面向用户生成内容（UGC）的大规模视觉失真评估指令微调数据集ViDA-UGC，并通过构建新的失真评估基准（ViDA-UGC-Bench），显著提升了多模态大模型对UGC图像质量的可解释性分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）虽然推动了图像质量评估（IQA）从不可解释转向可解释，但对于UGC和AI生成内容（AIGC）图像的失真评价未加区分，且缺乏细致的质量分析与修复指导。因此，亟需面向UGC图像的细粒度失真分析数据集与评测体系，助力更实用、可解释的IQA。

Method: 作者构建了包含1.1万张带有细致质量标注的UGC图像数据集ViDA-UGC，采用失真导向的流程，结合人工标注和“思维链（CoT）”框架，引导GPT-4o识别并分析UGC失真，生成高质量描述；同时严选476张图像及6149组QA由专家校对，打造ViDA-UGC-Bench基准。

Result: 实验表明，ViDA-UGC数据集及CoT框架能够持续提升多种基础MLLMs在ViDA-UGC-Bench和Q-Bench等评测上的图像质量分析能力，部分能力甚至超过了GPT-4o。

Conclusion: ViDA-UGC和ViDA-UGC-Bench首次实现了针对UGC图像的高质量失真评估指令微调数据集与基准评测，有效增强了MLLM对于UGC失真类型的理解及生成可解释质量分析的能力，有望促进实际应用中的图像质量监控与优化指导。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [94] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 本文提出了OpenMoCap模型和CMU-Occlu数据集，专门用于解决大规模标记遮挡下的光学动作捕捉问题，并展示出显著的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 当前光学动作捕捉在虚拟现实和影视制作等领域应用广泛，但在现实环境中的大规模标记遮挡情况下表现极差。现有方法主要存在缺乏贴近真实遮挡情况的数据集，以及训练策略无法有效捕捉标记之间的长距离依赖关系这两大问题。

Method: 1) 构建了CMU-Occlu数据集，利用光线追踪技术逼真地模拟标记遮挡；2) 提出了OpenMoCap新模型，通过标记-关节链推理机制，实现标记与关节间深层约束的同时优化和构建。

Result: 广泛的对比实验表明，OpenMoCap在各种场景下都优于现有方法，在有严重遮挡条件下表现最为突出。CMU-Occlu数据集也为后续研究提供了基础。

Conclusion: OpenMoCap不仅提升了遮挡环境下的动作捕捉鲁棒性，还已集成到MoSen MoCap系统中，具备实际应用潜力，代码已开源，有望促进后续研究。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [95] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波的新型视觉原语WIPES，可实现多维视觉信号的高效表示，在保证更高视觉质量的同时，实现了更快的渲染速度。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉与图形表示方法，在灵活的频率调制和快速渲染之间存在权衡，许多方法依赖频率引导或复杂的神经网络解码，导致频谱损失或渲染速度慢。因此，亟需一种既高效又能兼顾高低频信息，并能快速渲染的视觉表示方法。

Method: 作者提出了WIPES，这是一种通用的小波基视觉原语，利用小波的空间-频率局部化特性，以更好捕捉低频和高频视觉信息。同时设计了基于小波的可微分光栅化器，实现快速的视觉渲染。

Result: 在2D图像表示、5D静态和6D动态新视图合成等多个视觉任务上，WIPES在渲染质量和推理速度上优于基于INR的方法，并且在渲染质量上超过基于高斯的表示方法。

Conclusion: WIPES作为一种新的视觉原语，兼具高效性和高质量渲染，优于现有INR或高斯表示法，在多维视觉任务中具有广泛前景。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [96] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的广告创意图片评估与选择新范式，并构建相关数据集与选择器模型，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC技术的发展，广告主能够低成本大批量生成创意图片，但缺乏评估创意质量并给出可解释选择的方法。现有方法主要侧重于排序，无法满足创意选择中的可解释性需求。

Method: 作者提出基于多模态大语言模型(MLLM)的方法，将创意图片评估与选择整合为自然语言生成任务，并构建了用于比较推理的创意图片对数据集CreativePair（包含8k标注对）。此外，文中提出了Creative4U选择器，通过CoT-SFT与GRPO强化学习联合训练，可以结合用户兴趣做出可解释选择。

Result: 无论在离线还是在线的实验中，所提出的方法（Creative4U）在创意图片选择准确率与可解释性上均明显优于以往方法。

Conclusion: 本文方法有效提升了创意图片的自动选择与解释能力，为电商广告平台创意推荐带来进步。相关数据与代码开放，有助于进一步推动领域研究与产业应用。

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [97] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 提出了一种新的视觉-语言模型云边协同范式——Context Transfer，实现更高效的延迟感知推理。


<details>
  <summary>Details</summary>
Motivation: 当前云-边协同的视觉-语言系统无法充分利用云端大模型延迟输出的准确信息，且对云端延迟波动适应性差。实际实时应用如自动驾驶等领域对快速、可靠和准确的感知能力有迫切需求。

Method: 提出Context Transfer范式，将云端大模型延迟产生的高质量决策作为历史上下文，为边端小模型的实时推理提供指导。基于此，设计SpotVLM系统，引入上下文替换和视觉焦点模块，改进历史文本输入并强化视觉定位一致性。

Result: 在三个实时视觉任务、四个数据集上开展大量实验，结果表明该框架大幅提升了协同推理的有效性和一致性。

Conclusion: 新提出的协同范式为未来视觉-语言模型系统的高效、延迟感知协作策略奠定了基础。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [98] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 该论文提出了一种两阶段的Posterior-Mean Rectified Flow (PMRF)流程，实现仅用无对比增强MRI合成对比增强T1加权脑MRI影像，减少对钆剂的依赖。


<details>
  <summary>Details</summary>
Motivation: 对比增强MRI在神经肿瘤学诊断中很重要，但需要使用钆剂，带来额外成本、扫描时间、环境污染及患者潜在风险。因此需要无钆剂条件下复刻对比增强MRI的影像。

Method: 提出了一种两阶段的PMRF流程：第一阶段利用基于patch的3D U-Net预测体素级后验均值以最小化MSE；第二阶段用time-conditioned 3D rectified flow进一步修正，增强真实纹理同时保持结构精度。模型训练数据来自多机构的成对pre/post对比增强T1w影像（BraTS 2023-2025数据集）。

Result: 在360例多样化测试集中，改进后的输出在轴向FID为12.46、KID为0.007，较初始均值输出FID下降~68.7%；均方误差为0.057，较均值略增~27%。定性评估表明方法能够真实还原病变边缘和血管细节。

Conclusion: 该方法有效平衡了感知与失真，能够在临床部署中模拟对比增强MRI，减少钆剂依赖，具有重要现实应用价值。

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [99] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 本文提出了一种在测试时持续适应（CTTA）新领域的方法，能够更快适应新领域、并避免遗忘旧领域知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CTTA 目标是在推理期间让模型在不断变化的目标域下持续适应。现有方法适应速度慢且容易遗忘已学领域知识，导致在未来遇到相似领域时表现不佳，亟需探索和利用两者兼顾的新机制。

Method: 作者提出一种平衡探索和利用（BEE）的 mean teacher 框架。其中，多层一致性正则化（MCR）损失用来对齐学生模型和教师模型的中间特征，加速新领域适应；互补锚点回放（CAR）机制通过重用历史检查点，恢复和补充多样领域的知识，防止遗忘。

Result: 实验表明，本文方法在多个主流基准数据集上明显优于现有最先进的 CTTA 方法。

Conclusion: 该方法能更快自适应新领域，且更好利用历史知识应对未来领域，综合上实现了在 CTTA 任务中的有效提升。

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [100] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: 本研究提出了DyCrowd，这是首个可以实现大场景视频中数百个人体三维时空一致重建的框架。创新地通过群组引导的优化策略和VAE运动先验应对严重遮挡和时序不一致，并构建了虚拟数据集VirtualCrowd。实验显示其性能领先同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模人群三维重建工作大多只用静态图片，缺乏对时间一致性的处理且难以解决遮挡问题。因此亟需一个既具时序信息又能抵抗遮挡的三维人群重建方法。

Method: 提出DyCrowd框架，采用粗到细的群组引导运动优化策略提升遮挡下三维重建效果，并引入VAE人体运动先验和分段级群组引导优化以增强时序稳定性。还设计了异步运动一致性损失（AMC loss），用无遮挡人体群体的运动恢复被遮挡者的动作。为此还建立了大型虚拟人群视频数据集VirtualCrowd。

Result: 在大场景动态人群三维重建实验上，DyCrowd取得了最新的性能，优于同类方法。

Conclusion: DyCrowd能够实现大场景视频中大规模人群的高质量、时空一致三维重建，尤其在面对严重遮挡和时序不一致问题时表现出色。数据集和代码计划对外开放以促进研究领域发展。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [101] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段的人体去遮挡方法，通过先恢复人体结构掩膜再完成RGB外观重建，实现了对被遮挡人体的准确还原，提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 去遮挡（de-occlusion）任务对于实现更准确的人体分析十分重要，但当前深度学习在人体遮挡部分的恢复方面仍面临较大挑战，尤其是在结构和外观重建上。

Method: 方法分为两个阶段：首先采用基于扩散的先验及关节点热力图对人体结构掩膜进行补全，然后以补全的掩膜为条件；结合从VQA模型和CLIP编码器获得的人体描述文本特征，利用Stable Diffusion重建RGB图像。同时针对可见区域的像素级退化，进行了解码器微调。

Result: 该方法即使在严重遮挡情况下也能有效还原人体结构和外观，在掩膜与RGB补全方面均优于现有方法，并提升了人体姿态估计和三维重建等下游任务的性能。

Conclusion: 本文创新性地将人体先验、热力图与文本特征整合进去遮挡流程，双阶段设计可显著提升遮挡恢复效果，对相关视觉任务应用具有实用价值。

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [102] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出通过微调CLIP模型（称为WP-CLIP），以自动预测和分析Wölfflin提出的五项艺术风格原则，实现视觉艺术自动化分析。


<details>
  <summary>Details</summary>
Motivation: 现有度量方法无法有效地在视觉艺术中预测Wölfflin的五项风格原则，而对画作进行形式分析需要能够解读颜色、构图等视觉关键元素的自动化工具。

Method: 作者检验了预训练的CLIP模型是否能理解并预测Wölfflin的五项艺术风格原则。在发现CLIP原生难以捕捉这些细致风格后，作者对其进行了有标注真实艺术图像的数据集微调，使其能给出每一原则的评分，然后在GAN生成图像和Pandora-18K艺术数据集上测试模型泛化能力。

Result: 微调后的WP-CLIP模型能在不同艺术风格和类别下有效预测五项原则分数，泛化性良好。

Conclusion: VLMs（视觉-语言模型）经适当微调后，在自动化艺术分析领域具有广阔应用前景。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [103] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: 提出AdaBEV框架，通过自适应实例感知BEV表示提升多无人机三维检测的性能，实现高精度且具备良好计算效率的协同感知。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同3D检测可获得多视角感知优势，但受限于无人机算力有限，如何高效融合多源信息、提升精度成为难题。

Method: 本文提出AdaBEV方法，包含Box-Guided Refinement Module（BG-RM）和Instance-Background Contrastive Learning（IBCL）两大模块。BG-RM仅细化前景实例相关BEV网格，结合2D监督和空间细分，提升表示效率。IBCL则通过对比学习增强前后景特征分离能力，提升特征判别性和语义表达能力。

Result: 在Air-Co-Pred数据集上，AdaBEV在模型不同规模下实现了优越的精度-计算量权衡，特别是在低分辨率输入情况下显著优于现有方法，并接近性能上界，同时系统额外计算开销很小。

Conclusion: AdaBEV框架有效提升多无人机3D检测准确性和效率，为资源受限平台的3D感知设计带来了新的可能。

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [104] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了TTA-DAME方法，通过源域数据增强、域判别器和域检测器应对驾驶场景下频繁的天气域转移，实现模型在测试时动态适应和性能提升。


<details>
  <summary>Details</summary>
Motivation: 实际驾驶场景中，天气等域频繁变化，导致模型表现下降；如何在测试时自适应不同目标域，成为亟需解决的问题。

Method: 提出TTA-DAME方法：1）利用源域数据增强目标域；2）引入域判别器和专用域检测器，缓解如白天到夜晚的剧烈域转移；3）训练多个检测器，通过NMS整合预测结果。

Result: 在SHIFT Benchmark上实验证明该方法能显著提升模型性能。

Conclusion: TTA-DAME能够有效应对动态域转移，提升自动驾驶场景下模型在不稳定环境下的鲁棒性和适应性。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [105] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文针对更加实际的类增量重复（CIR）场景，提出了多层次知识蒸馏（MLKD）和动态自监督损失（SSL）两种有效利用无标签数据的新方法，显著提升了模型在该任务下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的类增量学习任务假设每个任务包含的都是未见过的新类别，但现实中许多场景会反复遇到已经学过的类别；同时，现实中常有丰富的无标签数据可用。为提升模型适应此类更实际场景（CIR）的能力，需要新方法。

Method: 提出了两个关键组件：1）多层次知识蒸馏（MLKD），从多个历史模型和不同层级（特征、logits等）蒸馏知识，以更好保留旧知识；2）动态自监督损失（SSL），充分利用无标签数据加快新类别学习，同时动态调整权重，确保训练重心仍在主要任务上。

Result: 上述方法在CIR设定下显著提升了模型稳定性和适应性，在CVPR第五届CLVISION挑战赛中获得第二名，验证了方法的有效性。

Conclusion: 通过更合理地利用无标签数据和丰富历史知识，所提出方法能够有效提升CIR任务下的性能，对实际类增量学习具有重要意义。

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [106] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 该论文研究了自动驾驶车辆中不同摄像头传感器配置导致的跨传感器领域差距，并提出用数据驱动的神经渲染方法来缓解这一差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆由于车型不同，摄像头传感器安装不可避免地存在差异。这种差异对感知模型的准确性产生了负面影响，亟需研究其影响并寻找应对方案。

Method: 作者搭建了CamShift数据集，模拟小型车和SUV之间的传感器差异，用于研究主流3D目标检测器的性能变化。同时，提出基于神经渲染的数据集适配方法，把一个传感器配置下的数据转换到另外一个配置。

Result: 实验显示，基于密集鸟瞰图BEV（如BEVFormer）的模型对摄像头配置更具鲁棒性；而提出的神经渲染适配方案能显著提升所有检测模型在跨传感器情况下的性能表现。

Conclusion: 论文有效揭示了跨传感器领域差距带来的问题，并通过数据驱动的适配方法大幅缓解了该问题，提高了模型的泛化能力和数据的复用效率，为实际多车型、跨设备自动驾驶感知系统部署提供了解决方案。

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [107] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: 本文揭示大型视觉-语言模型（LVLM）在应对由生成式AI（GenAI）带来的新闻内容多样性时，抵御多模态虚假信息能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着多模态虚假信息日益泛滥，如何利用大模型准确检测此类信息成为学术与实际应用的热点。然而，生成式AI工具的发展极大提升了新闻内容的多样性和复杂度，这对现有的检测系统提出了新的挑战。本文旨在系统性评估和揭示LVLM类检测器在该场景下面临的脆弱性。

Method: 作者提出了DriftBench基准数据集，包含16,000条新闻实例，涵盖六种内容多样化类型，还设计了三类评测任务：多层次漂移下的真伪验证、对抗性证据污染影响、推理一致性分析。用六个主流LVLM检测器进行全面实验对比。

Result: 实验结果显示，所有检测器在多层次漂移情况下性能平均降低14.8%（F1分数），推理逻辑不稳定，对抗性证据干扰下失败更严重，暴露出现有多模态虚假信息检测系统的显著脆弱性。

Conclusion: 在生成式AI推动内容多样化背景下，现有LVLM检测系统存在根本性缺陷，急需开发更具鲁棒性的检测方法，以有效应对未来日益复杂的多模态虚假信息挑战。

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [108] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的实时手语翻译辅助技术，能将手势转换为文本和语音，提升听障和语障人士的交流能力。


<details>
  <summary>Details</summary>
Motivation: 听障和语障人士在日常生活中面临交流障碍，亟需高效的辅助工具以促进他们的社会融合和自主交流。

Method: 利用卷积神经网络（CNN）对Sign Language MNIST数据集进行训练，通过摄像头实时捕捉手势，系统对手势进行分类，并将识别结果通过文本-语音合成（TTS）转化为语音输出。

Result: 实验结果显示该系统模型准确率高、实时性能好，尽管存在一定延迟，但整体表现稳定可靠。

Conclusion: 该系统在提升手语用户自主性和社会参与度方面具有很高的实际应用价值，是一种便捷且具有广泛适用性的辅助工具。

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [109] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 文章提出了Dual Contrastive Denoising Score框架，实现在无需进一步训练的情况下，基于大规模文本-图像扩散模型实现真实图像的灵活编辑，同时保持结构的一致性和高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有的文本-图像生成模型虽然可生成高质量图片，但难以直接高效编辑现实图片，原因包括难以精确描述细节，以及容易导致输入内容结构被大幅改变。为解决这些实际需求，作者提出创新方法以提升编辑的精准性和结构保持能力。

Method: 借鉴无配对图像转化领域的对比学习思想，在原有扩散模型中引入Dual Contrastive Loss，并直接利用自注意力层的中间特征，无需其他辅助网络，提升传统diffusion方法的实用性和灵活性。

Result: 实验证明该方法能够高效灵活地编辑现实图片，在显著提升结构和内容一致性的同时，优于现有方法，并可直接调用预训练的文本-图像扩散模型，无需重新训练。

Conclusion: Dual Contrastive Denoising Score方法为现实图像编辑提供了更强的灵活性和保真性，拓展了大规模扩散模型的应用空间，为无需文本精确描述和结构保持的编辑带来实用解决方案。

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [110] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文分析了3D高斯散射（3DGS）在稀疏视角下出现外观伪影的问题，提出了共适应分数（CA）用于度量高斯之间的缠结，并提出了两种减弱方法以提升新视角渲染质量。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS在密集视角下能实现高质量新视角合成，但在训练视角稀疏时遇到新视角伪影问题。为解决这一实际应用的痛点，有必要分析其产生机理并提出解决方案。

Method: 作者提出用共适应分数（CA），通过多次随机选取高斯集合渲染同一视角、计算像素方差，衡量高斯之间的缠结程度；并基于此，提出随机高斯丢弃与不透明度扰动两种轻量化方法，减少高斯共适应现象。

Result: 实验表明，作者提出的两种策略能够缓解稀疏视角下的高斯共适应现象，减少新视角中的外观伪影，在多个基准和方法上均验证了有效性。

Conclusion: 论文揭示了稀疏视角下3DGS高斯共适应导致新视角伪影的本质问题，并提出了简单有效的缓解策略，为该领域后续改进提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [111] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种用于单幅图像去焦模糊的新方法，能够有效处理空间变化模糊核的问题，从而恢复出全焦点图像。实验结果证明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统去焦去模糊方法在严重模糊区域因缺乏高频细节，内核估计表现不佳；因此，亟需更鲁棒的模糊核建模方法。

Method: 提出了频域驱动的逆核预测网络（FDIKP），利用频域信息增强结构可辨识性。设计了双分支逆核预测（DIKP）以提高核估计的精度和稳定性。引入位置自适应卷积（PAC）以加强去卷积的适应性，并提出了双域尺度递归模块（DSRM）用于多阶段融合去卷积结果。

Result: 大量实验验证表明，所提方法在恢复质量方面优于当前主流方法。

Conclusion: 结合空间与频域的核建模和去卷积，有效提升了单幅图像去焦去模糊的效果，是图像修复领域的重要进展。

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [112] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种结合传统与深度学习方法的新颖小样本图像集分类方法DCSCR，有效提升了图像集特征学习和相似性度量能力，在多个数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有传统ISC方法忽略特征学习，仅用原始像素特征分类；而深度学习方法即便可提取深度特征，却无法自适应地调整特征以更好地度量图像集间距离，尤其在小样本任务中表现有限。因此，急需一种新方法同时提升特征表达和相似性度量能力。

Method: 作者提出Deep Class-specific Collaborative Representation (DCSCR) 网络，将全卷积深度特征提取、全局特征学习与基于类别的协作表示度量学习模块结合。通过深度特征提取器和全局特征学习模块学习局部与全局帧级特征，再通过类特定协作表示模块自适应地学习图像集的概念级特征，并用新的CSCR对比损失函数优化图像集间距离。

Result: 在多个主流小样本ISC数据集上的大量实验表明，所提方法在图像集分类任务上相较主流方法表现更好，验证了该方法的有效性。

Conclusion: DCSCR方法能够同时提升图像集特征表达与集间相似性度量能力，在小样本图像集分类场景下取得更优性能，为ISC领域提供了新思路。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [113] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的基于Mamba结构的网络，用于高效地去除图像中的阴影，其在多个主流数据集上的性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阴影去除任务中，阴影区域的退化具有空间局部性和非均匀性，现有方法难以利用未受阴影影响区域的信息，且不同区域需要不同的变换策略，统一处理效果不佳。

Method: 提出了一种包含双尺度融合和双路径扫描的Mamba网络。通过Dual-Scale Fusion Mamba Block（DFMB）融合原始特征和低分辨率特征以增强多尺度表达并减少边界伪影；Dual-Path Mamba Group（DPMG）采用水平扫描获得全局特征，同时引入mask感知的自适应扫描策略，实现结构连续性和精细区域建模。

Result: 通过在阴影去除标准基准数据集上的实验，提出的方法显著优于现有最先进的算法，取得了更高的去阴影效果和图像质量。

Conclusion: 该Mamba网络通过充分利用非局部上下文和区域特异性变换，结合创新的双尺度和双路径机制，大幅提升了阴影去除性能，为图像恢复相关任务提供了新思路。

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [114] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: 本论文提出了CLAIRE-DSA，一个基于深度学习的系统，用于自动识别和分类急性缺血性脑卒中机械取栓术中获取的血管造影影像的关键属性，以实现自动质控和流程优化。


<details>
  <summary>Details</summary>
Motivation: 在急性缺血性脑卒中机械取栓的数字减影血管造影（DSA）流程中，图像质量参差不齐经常影响下游人工智能模型的准确性和性能，缺乏高效的自动化图像质量评价和筛查工具，成为临床及科研应用发展的瓶颈。

Method: 研究者基于预训练ResNet主干网络，针对1,758例标注的血管造影最小强度投影（MinIP）图像进行微调，分别训练了9个图像属性分类器（如对比剂存在、投影角度、运动伪影等级等），对每个要素进行独立预测。

Result: CLAIRE-DSA在全部9个标签上的评价指标都表现优异，ROC-AUC在0.91-0.98，精度在0.70-1.00；在血管分割任务中，利用该模型过滤掉低质量图像后，分割成功率由42%提升至69%（p<0.001）。

Conclusion: CLAIRE-DSA作为自动化DSA图像属性分类工具表现出色，有望为急性缺血性脑卒中患者的临床影像标注和质量控制带来积极作用，辅助临床与科研工作流程，代码已公开。

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [115] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 本论文针对CdZnTe半导体图像标注难题，提出基于组内一致性的半监督分割方法ICAF，并显著提升了低标注数据下的分割精度。


<details>
  <summary>Details</summary>
Motivation: CdZnTe材料的缺陷边界对比度低，标注者需同时参考多视图才能标注，而现有半监督分割方法只能处理一对一的图片-标签关系，无法利用多视图的组内一致性，易产生误差累积和确认性偏差。

Method: 提出了一种组内一致性增强框架（ICAF），包括组内视图采样（IVS）建立组内一致性基线，并设计了伪标签修正网络（PCN），其中视图增强模块（VAM）通过聚合多视图动态合成边界敏感视图，视图修正模块（VCM）则实现组内视图的信息交互，提高分割精度。

Result: 在仅使用2组（5‰）标注数据的条件下，以DeepLabV3+（ResNet101）为基线，在CdZnTe数据集上取得了70.6% mIoU，实验证明方法有效。

Conclusion: ICAF能充分挖掘和利用CdZnTe多视图组的内在一致性，有效提升低标注场景下的分割效果，为半监督语义分割任务提供了新思路。

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [116] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack是一种针对复杂城市交通环境中的小目标，提升多目标跟踪准确性和鲁棒性的无人机跟踪新方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂城市交通中的多目标跟踪面对小目标尺寸变化、遮挡、非线性交叉运动和运动模糊等挑战，现有方法稳定性不足。

Method: 提出SocialTrack，将多尺度特征增强用于小目标检测，利用速度自适应CKF（VACKF）提升轨迹预测，采用群体运动补偿策略（GMCS）与时空记忆预测（STMP）修正低质量轨迹及历史轨迹信息，提升目标关联和防止身份切换。

Result: 在UAVDT与MOT17数据集上，SocialTrack在MOTA、IDF1等关键指标上都优于现有最优方法，表现出更好的鲁棒性与适应性。

Conclusion: SocialTrack方法有效提升了无人机复杂交通下小目标跟踪的性能，并具备模块化、兼容性，可与其他跟踪器集成进一步增强检测表现。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [117] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 本文提出了一种改进的基于潜在扩散模型的图像风格迁移方法，能够更好地匹配多种风格并防止内容与风格混淆，取得了领先的风格化效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于潜在扩散模型的图像风格迁移虽然有进展，但依然存在风格匹配不准确、可用风格图像数量有限及内容与风格特征耦合的问题。

Method: 作者方法采用多风格图像输入，通过结合图像提示适配器(propt adapters)和特征统计对齐的方法，在去噪UNet的交叉注意力层和自注意力层介入，利用聚类从风格样本中提取代表性注意力特征，实现更精准的风格表达和内容保护。

Result: 实验显示，该方法达到了图像风格迁移领域的最新最好水平（state-of-the-art）。

Conclusion: 多风格图像、特征对齐与注意力机制的结合极大提升了扩散模型在图像风格迁移上的表现，有效解决了内容泄露和风格匹配不准的问题。

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [118] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 本研究利用谷歌街景图像和深度学习算法，创新性地估算全球范围内城市的自行车和摩托车出行比例，并与传统调查数据进行了对比验证，取得较高精度预测成果。


<details>
  <summary>Details</summary>
Motivation: 全球范围内缺少比较性强的自行车和摩托车出行行为数据，传统数据采集成本高、覆盖有限，难以支持健康和交通相关政策研究。作者希望通过自动化方式高效获取、分析交通出行方式数据。

Method: 使用YOLOv4深度学习模型在谷歌街景(GSV)图像中检测自行车和摩托车，并利用来自185个城市8000张图片/城市的数据，与出行调查和人口密度等变量，开发beta回归模型预测出行方式占比。

Result: YOLOv4模型检测自行车和摩托车的平均精确度为89%；GSV检测数与实际出行占比相关性良好（摩托车为0.78，自行车为0.51）。回归模型能较好预测出行占比，$R^2$分别为0.614和0.612，绝对中位误差约为1.3%-1.4%。

Conclusion: 结合计算机视觉和街景图像可以高效、准确地捕捉城市出行方式，与传统调查数据互为补充，适用于没有近期调查数据的城市，为健康与交通政策提供支持。

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [119] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: 本论文利用计算机视觉方法，通过卷积神经网络（ResNet50）和视觉Transformer（ViT）模型，对食双星的光变曲线进行自动分类，并提出了一种新颖的极坐标六边形可视化方法提升模型泛化能力。在主要的二分类任务中取得了高准确率，但对于更复杂的斑点检测任务仍表现有限。


<details>
  <summary>Details</summary>
Motivation: 随着大规模天文巡天数据的积累，需要高效、自动化的方法来对食双星系统进行形态分类，辅助天文学研究。传统方法效率较低，深度学习和计算机视觉提供了新的自动化手段。

Method: 作者使用了ResNet50和ViT两种预训练深度学习模型，针对合成数据集中的光变曲线图像进行微调。为提升泛化能力并减少过拟合，提出将相位折叠光变曲线转成极坐标结合六边形可视化的图像表示。采用分阶段层次化方法，先二分类筛选不同类型食双星，再检测是否存在斑点。

Result: 在主任务食双星类型二分类上，模型在多个光谱段验证集上准确率高于96%，在真实观测数据集（OGLE, DEBCat, WUMaCat）中表现同样优异（准确率高于94%，TESS数据上最高达100%）。但在自动检测光变斑点（二分类）这个子任务中，模型表现较差。

Conclusion: 计算机视觉方法在食双星系统形态分类中极具潜力，特别适合大规模巡天数据的自动处理。但目前模型在识别复杂微弱光变特征（如斑点）方面存在显著局限性，需要进一步研究和改进。

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [120] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种创新的图像生成方法，将图像分解为一个结构化序列，通过不同粒度的可视化层次逐步细化生成，展现了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在细粒度和全局布局的平衡上存在局限，难以实现多层次、结构化控制。作者希望提出一种方法，能够分层生成图像，同时提升结果质量和生成灵活性。

Method: 作者提出了Next Visual Granularity (NVG)框架，将图像表示为一系列共享空间分辨率但令牌数不同的结构化序列。图像生成从空白开始，通过多层级、逐步细化的方式，自粗到细进行视觉粒度的分解和合成。方法采用分层训练，每一层对上一层结果进一步细化，最终实现细粒度控制和生成。

Result: 在ImageNet数据集上的实验表明，NVG在条件图像生成任务中，相比VAR系列，在FID分数上有小幅度但稳定的提升（3.30降至3.03，2.57降至2.44，2.09降至2.06），且展现出良好的可扩展性。同时，作者进行了大量分析验证了NVG的能力和潜力。

Conclusion: NVG框架通过分层视觉粒度建模，实现了多层次、可控性的图像生成效果且优于主流方法，为高质量、层次化的图像生成提供了新途径。

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [121] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: 本论文介绍了在CVPR 2025事件视觉研讨会举办的时空实例分割挑战赛（SIS）的任务、数据集、竞赛详细信息及结果，并总结了前五名团队的主要方法。


<details>
  <summary>Details</summary>
Motivation: 为促进事件相机与灰度相机数据融合下的像素级时空实例分割技术进步，主办了本次挑战赛，推动相关研究发展。

Method: 竞赛任务要求参赛者从时空配准的事件相机和灰度相机数据中分割出给定类别目标的准确像素级掩膜。论文重点介绍了前五名队伍所采用的主要解决方案及技术手段。

Result: 论文总结了挑战赛结果，列出了各队成绩，并对前五名队伍的方法进行了详细描述。

Conclusion: 此次SIS挑战推动了基于多模态数据的时空实例分割算法发展，为相关方法和资源的进一步研究提供了平台和参考。

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [122] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的深度学习水下图像修复模型DEEP-SEA，有效改善水下图像清晰度与色彩失真，提升了水下监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 水下监测对了解海洋环境和生态变化非常重要，但受到光散射、吸收和浑浊影响，水下图像质量普遍较差，导致监测准确性和自动识别能力受限。现有方法对这种复杂退化情况修复效果有限。因此，亟需开发能进一步提升水下图像质量的方法。

Method: 提出DEEP-SEA模型，其核心是“双频增强自注意力空间与频率调制器”，能够在频域自适应地优化特征表达，同时增强空间结构信息，从而同时提升图像细节和结构一致性。作者在EUVP和LSUI数据集上进行充分实验评估。

Result: 在恢复精细图像细节和结构一致性方面，DEEP-SEA在EUVP和LSUI数据集上的表现优于当前主流方法。

Conclusion: DEEP-SEA有效缓解了水下图像的视觉退化问题，提升了水下监测平台在生态观测、物种识别和自主导航等应用中的可靠性和准确性。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [123] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 本文提出了MMPDA方法，在多模态欺骗检测竞赛中表现优异, 有效解决了域迁移问题，在多个指标上超过其他参赛队伍。


<details>
  <summary>Details</summary>
Motivation: 在多模态欺骗检测任务中，不同数据集（源域与目标域）之间存在显著的域分布差异，影响模型泛化能力。解决跨域迁移问题，提升模型在新领域的表现，是当前该领域的核心挑战。

Method: 提出了多源多模态逐步域适应（MMPDA）框架，通过在特征层与决策层逐步对齐源域和目标域，实现音视频知识从多个源域向目标域的有效迁移。方法综合利用多模态信息，减小分布间的偏差。

Result: 在MMDD挑战赛第二阶段中，准确率达到60.43%，F1值为56.99%，F1分数比第一名高5.59%，准确率比第三名高6.75%，整体排名前二。

Conclusion: 所提出的MMPDA方法能够有效缓解多源多模态跨域问题，在多模态欺骗检测领域具有广泛适用性和领先性能。

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [124] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出了一种用于视觉-语言模型的新型微调策略CoMuCo，使其在跨域的少样本图像识别任务中表现更佳，并建立了新的基准来验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在自然图像领域内表现优异，但当应用于与自然图像差异较大的跨域任务时效果显著下降，因此需要新的方法提升其跨域能力。

Method: 提出了Consistency-guided Multi-view Collaborative Optimization (CoMuCo)微调策略。该策略包括两个具有互补功能的专家模块提取多视角特征，结合基于先验知识的一致性约束和基于信息几何的共识机制来增强特征鲁棒性。此外，建立了一个新的跨域少样本基准。

Result: 在现有和新提出的跨域基准上，CoMuCo在少样本学习任务中持续优于当前方法。

Conclusion: CoMuCo显著提升了视觉-语言模型在跨域少样本任务中的能力，为该领域方法和评测基准的进步提供了有力支撑。

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [125] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言模型微调方法MPS-Tuning，能在提升模型性能的同时有效保护语义流形结构。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM微调策略虽可防止过拟合和实现领域自适应，但常忽略了数据分布在特征空间中的几何结构，从而可能造成整体语义表示的扭曲。

Method: MPS-Tuning将特征空间中的数据分布视为语义流形，在微调过程中通过对齐微调前后的特征Gram矩阵，显式地约束流形的几何结构，并刻意优化类间可分性。此外，通过优化图文特征对的相似度，进一步增强流形的判别能力。理论上，这种约束近似于控制Gromov-Wasserstein距离的上界。

Result: 大量实验表明，MPS-Tuning不仅能显著提升模型性能，同时更好地保持了原有流形的结构。

Conclusion: MPS-Tuning为VLM提供了一种同时兼顾性能提升和语义结构保护的微调新范式，对下游任务具有较强实用性和推广意义。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [126] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型引导方法S^2-Guidance，通过在扩散模型的前向过程随机丢弃网络模块，提升了生成内容的质量，并优于当前主流的无分类器引导（CFG）方法。


<details>
  <summary>Details</summary>
Motivation: 无分类器引导（CFG）虽然提升了扩散模型的生成质量和对提示的响应，但存在生成结果次优、语义不连贯和质量下降等问题。作者发现原因在于模型过度依赖次优预测。

Method: 作者通过实证分析，首先证明模型内部的子网络能有效改善次优预测表现；基于此，提出S^2-Guidance：在前向过程随机丢弃部分网络模块，以构建变化的随机子网络，引导模型远离低质量输出，提升其生成能力。

Result: 在文本到图像和文本到视频的生成任务上进行了大量定性和定量实验，S^2-Guidance表现优于CFG和其他先进的引导策略，生成内容质量显著提升。

Conclusion: S^2-Guidance能有效克服CFG的局限，利用模型自身子网络结构增强生成模型的表现，有望广泛应用于高质量文本到图像/视频生成任务。代码即将开源。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [127] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: 本文提出了一种基于非负矩阵分解（NMF）的全新神经网络稀疏化方法ONG，实现一次性高效剪枝并通过梯度掩码精准保持稀疏率，提升模型训练与部署效率且效果优异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽性能出众，但庞大的模型规模带来部署困难。现有剪枝手段多为复杂迭代且稀疏性保持不理想，亟需更简便高效、能严格控制稀疏率的新方法。

Method: 提出ONG（One-shot NMF-based Gradient Masking）方法，训练开始前利用NMF分解判别重要权重结构一次性剪枝；训练阶段采用精确梯度掩蔽机制，仅更新保留权重，严格保持目标稀疏度。将ONG整合至BIMP评测框架，并与主流稀疏化方法在CIFAR-10/100与ResNet系列模型上比较。

Result: 实验证明，ONG在不同稀疏率下能取得与最优方法相当或更佳的性能；剪枝后结构完整且可精准控制稀疏度。

Conclusion: ONG方法实现了高效、结构友好及稀疏率可控的神经网络稀疏化方案，为剪枝与部署带来新思路。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [128] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出了一种基于临床报告条件生成CT全卷积图像的新模型CTFlow，并在多个指标上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前CT影像生成面临数据量不足、隐私保护和合规等问题，而临床报告配对大规模CT数据集（如CT-RATE）的出现，使文本条件生成模型成为可能。该领域亟需高效、可扩展并能与临床文本高度配准的生成方法。

Method: 作者提出CTFlow模型，利用0.5B参数量的latent flow matching Transformer结构。在方法上，使用FLUX中的A-VAE定义潜在空间，并结合CT-CLIP文本编码器将临床报告编码为条件信息。创新性地采用自回归生成策略，先由文本生成首批CT切片序列，再利用前序生成结果和文本生成后续切片，通过分片保持整体体积生成的一致性及降低内存消耗。

Result: CTFlow在时序一致性、影像多样性及文本-影像配准度等方面，利用FID、FVD、IS分数和CLIP分数进行评测，结果均优于现有业界领先的CT生成模型。

Conclusion: CTFlow有效提升了大规模临床文本引导下的CT体积生成质量，具备推广至医学数据增强和隐私保护等多种应用场景的潜力。

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [129] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合相机与激光雷达（LiDAR）传感器的多阶段跨模态融合3D检测框架CMF-IOU，通过多层次的信息融合显著提升了三维目标检测的性能，在KITTI、nuScenes和Waymo等主流数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D检测方法大多只在单一或部分阶段进行信息融合，导致特征提取不充分、性能受限。作者旨在解决3D空间信息与2D语义信息对齐困难，从而提升整体检测效果。

Method: 1. 利用深度补全部网络将相机像素投影到3D空间生成伪点，实现与LiDAR点的统一表示；2. 设计双向跨视角增强3D主干网络（包括S2D和ResVC分支），分别强化稀疏LiDAR点和缓解伪点误差；3. 提出迭代体素-点细粒度池化模块，在提案细化阶段融合两类信息；4. 结合IoU联合预测分支和新颖提案生成方案，保留高IoU及高分类分数的框。

Result: 在KITTI、nuScenes和Waymo大规模标准数据集上，该方法全面优于现有主流多模态检测方法，展示了显著的性能提升和跨数据集的泛化能力。

Conclusion: 多阶段、深度跨模态融合能够有效对齐、提升LiDAR与摄像头数据互补性，使3D检测更为精准。CMF-IOU为多模态目标检测提供了新的融合思路，也为后续研究奠定技术基础。

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [130] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 本文提出了7Bench，这是首个用于评估布局引导文本生成图像模型中语义和空间对齐能力的基准测试集，弥补了当前只关注文本对齐而忽视布局对齐的评测空白。


<details>
  <summary>Details</summary>
Motivation: 当前布局引导的文本到图像生成模型广泛应用于内容创作和合成数据生成，并通过显式的元素空间排列条件提升图像生成的可控性。但现有评测仅关注文本与图像的语义对齐，未对图像与布局间的空间对齐进行系统评价，这限制了模型空间忠实性的评估，尤其在生成合成数据时会影响数据质量。

Method: 作者设计了7Bench基准，包含涵盖七种具有挑战性的场景的文本与布局对，并从对象生成、颜色忠实性、属性识别、目标间关系和空间控制等多个维度考察模型表现。此外，提出了一套评测协议，将布局对齐评分结合到现有体系中，以全面量化空间对齐能力。

Result: 利用7Bench，作者对多种主流扩散模型进行了系统评价，发现了不同模型在各类对齐任务中的优势和局限性，为后续模型优化提供了参考。

Conclusion: 7Bench填补了文本到图像生成领域语义与空间对齐联合评测的空白，有助于推动模型空间忠实性和实际应用效果的进一步提升。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [131] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为HiAD的新框架，用于高分辨率图像异常检测，显著提升了检测的精度和效率，并在多个高分辨率基准数据集上取得了优异的实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法多聚焦于低分辨率场景，而高分辨率图像通过传统降采样会丢失细粒度判别信息，导致异常区域难以检测。现有通过轻量网络或图像切块等方法仍难以兼顾精度和效率，不能满足工业实际需求。

Method: 提出HiAD高分辨率异常检测通用框架，采用双分支结构融合不同尺度的信息，通过多分辨率特征融合策略解决纹理细粒度变化问题。创新性地引入detector池和多种detector分配策略，实现基于patch特征的自适应分配，在保证检测性能的同时有效控制计算资源消耗。

Result: 在自构建的高分辨率异常检测基准数据集（MVTec-HD、VisA-HD、RealIAD-HD）上进行大量实验，HiAD方法取得了优于现有方法的性能表现。

Conclusion: HiAD框架能够在有限资源下，对高分辨率图像中不同尺度的异常区域实现高效、精准地检测，具有较强实用价值。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [132] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: 提出了一种名为SEDEG的两阶段增量学习框架，旨在同时提升编码器和解码器的泛化能力，有效缓解灾难性遗忘，并在多个视觉基准数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法大多只关注提升编码器或解码器单一部件的泛化能力，导致在面对新数据时原有知识易丢失（灾难性遗忘），特别是在存储历史样本有限的小内存场景下效果更差。

Method: SEDEG框架采用两阶段训练：第一阶段通过特征提升训练集成编码器，获取更具泛化能力的特征并促进解码器的平衡和泛化；第二阶段利用知识蒸馏（包括均衡KD和特征KD）将集成编码器能力压缩到单一编码器，实现更有效的知识转移和泛化。

Result: 在三个主流视觉增量学习基准上实验表明SEDEG获得了优于其他方法的性能；消融实验进一步验证了框架各组件的有效性。

Conclusion: SEDEG能够系统性提升增量学习中ViT编码器和解码器的泛化能力，减缓灾难性遗忘问题，适应小样本记忆场景，方法具有良好的实用前景。

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [133] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net的新型全自动纤维束分割框架，大幅提升猕猴解剖追踪数据分析的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 手动标注组织切片中的神经纤维束十分耗时，且现有自动方法对稀疏纤维束检测能力有限，需要复杂的后处理，阻碍了大规模分析与泛化能力。

Method: 基于U-Net架构，结合大尺寸patch、针对前景的采样策略及半监督预训练，实现了对猕猴追踪数据（组织切片）纤维束的全自动分割，并支持分析独立切片。

Result: 与最新方法相比，本方法稀疏纤维束检测能力提升20%，误发现率（FDR）减少40%，消除了误标末端为束等常见错误。

Conclusion: 新方法可促进解剖追踪数据的大规模自动分析，生成更多用于验证和优化dMRI束成像的高质量真实数据。

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [134] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: 本论文提出了一个名为Lumen的视频重光照系统，能够根据文本描述灵活调整视频中的光照和背景，并实现前景光照与背景替换的自然融合。


<details>
  <summary>Details</summary>
Motivation: 视频重光照在影视制作、虚拟现实等领域非常重要，但很难获得在不同光照条件下拥有相同前景的高质量配对视频，且传统方法难以平衡光照一致性、视频连贯性与前景属性保持。

Method: 1. 提出Lumen端到端视频重光照框架，基于大规模视频生成模型，支持文本控制；2. 构建包含真实和合成视频的大规模数据集，通过3D渲染和HDR模拟各类光照；3. 联合训练并引入domain-aware adapter，实现重光照与不同域分布的解耦学习。

Result: Lumen在构建的测试基准和现有方法对比中，能有效实现具有电影感的连贯重光照效果，同时严格保存前景属性，视频时序一致性表现突出。

Conclusion: Lumen可以高质量地实现基于文本描述的视频重光照与背景替换，同时保证前景属性和时序上的一致性，技术具有广泛实用价值。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [135] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出了一种新的人体动作识别方法MaskSem，通过语义引导的掩码机制和高阶运动特征的重建，提升了基于骨架的动作识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有自监督骨架动作识别方法只关注少数关节和低阶运动特征，难以捕捉复杂运动模式，限制了模型在机器人协作等实际场景中的理解能力。

Method: 提出MaskSem框架，利用基于相对运动的Grad-CAM对骨骼时空区域进行语义引导的掩码，通过挖掘关节中富含语义的时序信息提升特征表达力。同时，将低阶（速度）和高阶（加速度）运动作为重建目标，让模型学习多阶运动特征，获得更全面的动态过程描述。

Result: 在NTU60、NTU120和PKU-MMD数据集上，结合基础Transformer结构，MaskSem显著提升了基于骨架的动作识别准确率。

Conclusion: MaskSem能够有效促进模型学习复杂且具有判别力的运动特征，丰富动作识别表达，提升了在人机协作等实际应用场景中的实用性。

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [136] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种适用于开放式医学问答的强化学习框架ARMed，通过引入自适应语义奖励提升推理能力，在多项医学问答基准上显著提高了模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前基于规则的强化学习已在多模态推理中显示出潜力，但医学影像领域应用较少，且主要针对封闭式VQA，难以满足临床实际需求。开放式医学VQA更贴合实际，却研究不足。现有方法的语义奖励存在区分度差的问题，限制了模型进一步提升。为此，作者希望设计一种更有效的奖励机制，提升医学多模态推理能力。

Method: 本文提出的ARMed方法，先利用链式思维数据进行有监督微调注入医学领域知识，再结合基于文本正确性和自适应语义的奖励进行强化学习。这样既提升了推理水平，又缓解了传统语义奖励难以区分不同回答的弊端。

Result: 在六个具有挑战性的医学VQA基准上，ARMed显著提升了准确率和泛化能力：域内任务提升32.64%，跨域任务提升11.65%。

Conclusion: 奖励区分度对于医学强化学习非常关键，ARMed展示了自适应语义奖励在提升临床多模态推理上的潜力，有助于实现更鲁棒、具有实际临床价值的医学AI模型。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [137] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的牙齿多类别分割方法，用于CBCT影像，实现了良好的分割效果。


<details>
  <summary>Details</summary>
Motivation: 自动化牙科结构分割有助于病灶识别（如髓腔或根尖病变）以及放射治疗方案制定，尤其在头颈部癌症患者中具有重要价值。

Method: 作者团队基于MONAI Auto3DSeg框架和3D SegResNet模型，采用63份CBCT扫描进行5折交叉验证训练，预处理包括像素重采样和强度截断。先对下颌骨进行初步分割，并基于该结果紧缩区域后进一步分割更小的神经结构，同时在5折预测中融合多标签STAPLE方法。

Result: 在ToothFairy3挑战赛的外部验证集上，平均Dice系数达0.87，表明方法具有较高的准确率。

Conclusion: 提出的深度学习分割方法在牙科CBCT图像上表现出良好效果，有助于提升放射肿瘤学等领域的临床自动化影像分析水平，有望改善患者护理。

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [138] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: 提出了一种新的注视检测模型GazeDETR，通过解耦的人头定位与注视点预测，提高了注视目标检测的准确性，并在多个数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端注视目标检测模型使用单一解码器同时进行人头定位和注视点预测，导致两者特征表征相互纠缠，影响任务表现。亟需一种能有效区分两任务特征的模型结构。

Method: 提出GazeDETR架构，采用两个彼此解耦的独立解码器，分别针对人头定位与注视点预测，利用专属注意力机制学习各自最有用的信息。人头定位解码器聚焦局部特征，注视点解码器结合局部与全局特征。

Result: GazeDETR在GazeFollow、VideoAttentionTarget和ChildPlay三个数据集上均取得了最新的最优结果，明显优于现有的端到端方法。

Conclusion: 解耦的人头定位和注视目标检测方法能提升注视检测性能，GazeDETR为相关领域带来显著突破，有助于推动人机交互与数字表型分析相关应用的发展。

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [139] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 本文提出一种名为Compact Attention的新型稀疏注意力机制，有效提升了长序列视频生成Transformer的效率，在单GPU上实现1.6~2.5倍加速，且保持与全注意力方法接近的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 目前Transformer用于视频生成时自注意力计算复杂度极高，现有稀疏注意力方法（例如因式分解注意力、固定稀疏模式）未能充分利用视频的时空冗余，并且要么限制灵活性，要么带来额外开销，限制了长序列视频的高效生成能力。

Method: 作者系统分析了视频扩散Transformer的注意力结构，发现注意力头具有动态且结构化的异质稀疏性。为此，提出Compact Attention包括：（1）自适应分块策略，实现多样空间交互模式的动态分组；（2）基于帧距离的时间动态窗口策略，自适应调整稀疏度；（3）自动配置搜索算法，优化稀疏模式及关键注意力路径。

Result: 在单GPU平台下，Compact Attention在保持良好视觉质量的条件下，实现了1.6~2.5倍的注意力计算加速，超过了现有稀疏注意力方法。

Conclusion: 本文通过结构化稀疏性挖掘，为高效长序列视频生成提供了新范式，在加速和质量间实现了优异平衡，具备实际应用和硬件友好性。

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [140] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本论文提出了一种无需标签数据的零代价(Zero-cost)神经架构搜索代理方法，可更高效且准确地预测神经网络结构性能。


<details>
  <summary>Details</summary>
Motivation: 以往的零代价代理往往依赖有标签数据，且多聚焦于收敛性、泛化性或表现力中的某一方面，限制了其实际应用范围。本研究动机是同时解决依赖有标签数据和偏重单一属性这两大痛点。

Method: 作者首先剖析了通道共线性对网络收敛与泛化的影响，随后整合收敛性、泛化性与表现力，提出基于神经网络层特征SVD（奇异值分解）和网络输出外部曲率的新型零代价代理，无需标签，仅取一条无标签数据即可计算。代理为特征状态数条件数逆和及输出曲率对数简化调和平均。

Result: 通过6组实验（涵盖CNN和Transformer空间）和多个NAS基准（NAS-Bench-101/201、TransNAS-Bench-101-micro，以及DARTS和AutoFormer搜索空间），新代理与现有方法对比表现优越，并且计算效率突出。

Conclusion: 该方法有效弥补了依赖标签和指标单一的缺点，在多个主流NAS任务与基准中验证了其实用性和泛用性，可极大简化和加速实际零代价NAS流程。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [141] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文综述了智慧城市中多模态视觉目标跟踪（MMVOT）的最新进展与挑战，系统梳理其关键环节、主流方法及评测标准。


<details>
  <summary>Details</summary>
Motivation: 智慧城市发展产生了海量多模态数据，如何高效利用多模态数据实现对城市基础设施和服务的全方位监控，其核心任务之一即为多模态视觉目标跟踪，需要系统梳理现状与问题。

Method: 对MMVOT进行全面的多模态分析，涵盖数据收集、模态对齐与标注、模型设计和评价四大方面；对现有方法进行分类分析，系统比较RGB与其它六类模态（热红外、深度、事件、近红外、语言、声纳）的辅助分支设计实验配置；首次分析MMVOT数据集中目标类别分布，关注长尾分布和动物类别缺失问题。

Result: 本文梳理了六大MMVOT任务、主流方法、338篇相关文献，归纳了不同模态融合方式对跟踪性能的影响，并揭示MMVOT数据集目标类别存在明显长尾分布和动物类目标不足。

Conclusion: 作者认为，多模态跟踪并非在所有场景下都优于单模态，需要具体情况具体分析。未来应加强长尾类别处理、丰富动物等特殊类别数据。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [142] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 本文通过实验证明，提升特征多样性有助于开放集识别和持续学习任务。


<details>
  <summary>Details</summary>
Motivation: 开放集识别和持续学习是机器学习中的重要难点，现有方法多数只在提升特征多样性上采用启发式手段，缺乏深入探讨特征多样性的实际作用。

Method: 作者通过实验，系统性地分析了提升特征多样性对开放集样本识别和持续学习（包括旧知识保持与新知识融入）的影响。

Result: 实验证明，提高特征多样性能够提升开放集样本的识别能力，同时有助于持续学习中已学知识的保持及新知识的引入。

Conclusion: 增强特征多样性是提升开放集识别与持续学习性能的关键因素，未来应加大对此领域的理论和方法研究。

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [143] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: 本文提出了SlimComm，一种高效通信的协同感知框架，通过集成4D雷达和稀疏查询方法，大幅降低自动驾驶车辆间的感知信息传输带宽，同时保证或提升感知准确度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在感知过程中，由于传感器范围有限和遮挡，单车难以获得完整信息。现有的协同感知方法需要大规模传输BEV特征，带宽消耗高，限制了实际部署。

Method: SlimComm结合4D雷达Doppler数据，构建以运动为中心的动态地图，区分动态与静态目标，并发起两类查询：一类针对高可信度的动态区域，一类探索被遮挡区域。仅传递与查询相关的BEV特征，通过多尺度门控可变形注意力进行特征融合与通信，从而实现稀疏高效的数据交换。

Result: SlimComm在自建的OPV2V-R和Adver-City-R数据集上测试，通信带宽相比全图特征传输降低高达90%，在不同交通密度和遮挡情况下，感知结果与或优于现有主流方法。

Conclusion: SlimComm有效解决了协同感知通信负载大的问题，在保证感知准确的同时大幅节省带宽，为自动驾驶协同感知的实际落地提供了新方案。

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [144] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: 本论文提出了Matrix-Game 2.0，这是一个能够实时生成长视频的交互式世界模型，采用少步自回归扩散方法，实现高效互动环境仿真。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式世界模型依赖于双向注意力和长时间推理步骤，导致实时性能差，难以满足实际物理动态和即时响应的需求。

Method: Matrix-Game 2.0包含三大部分：1）基于Unreal Engine和GTA5的大规模数据生产流水线，生成约1200小时多样且含交互注释的视频数据；2）动作注入模块，支持逐帧级别的鼠标和键盘动作作为交互条件；3）基于因果架构的少步骤蒸馏方法，实现实时、流式视频生成。

Result: Matrix-Game 2.0 能以25 FPS的速度，在多场景下生成高质量、分钟级别的交互视频，性能优异且速度极快。

Conclusion: 提出的方法实现在交互世界建模任务中的高效实时视频生成，推动了相关领域研究，并已开源模型权重与代码库。

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [145] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出EgoTwin框架，首创性地实现了结合第一视角视频与人体运动的生成，解决了视角对齐和因果关联两大挑战，并在大规模数据集和新指标下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然外部视角的视频生成已有较大发展，但第一视角（egocentric）视频生成仍很少被研究。生成第一视角视频需要同时建模佩戴者的视角内容和由身体运动引起的摄像机运动模式，目前尚无有效联合解决方案。

Method: 提出EgoTwin框架，基于扩散Transformer架构。创新点包括：用以头部为中心的运动表示，将人体运动锚定至头部关节；并引入受控制论启发的交互机制，在注意力操作中建模视频与运动之间的因果关系。作者还采集了大规模真实世界的文本-视频-动作三联同步数据集，并设计新指标评估视频与运动一致性。

Result: 在大规模数据集上开展了大量实验，结果显示EgoTwin框架在生成的视频-运动对齐一致性和生成质量方面优于现有方法。

Conclusion: EgoTwin有效解决了联合第一视角视频和人体运动生成中面临的关键挑战，可为虚拟现实、动作捕捉等领域提供更自然、协调的生成内容。

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [146] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HierAdaptMR的分层特征自适应框架，有效提升了深度学习心脏MRI重建模型在多中心、异构环境下的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习心脏MRI重建模型在不同医院、不同扫描仪和成像协议下常因域偏移问题导致泛化能力差，影响实际临床应用。因此，需要开发能适应多中心、异构数据的重建方案。

Method: HierAdaptMR采用分层自适应结构：包括针对不同成像协议的Protocol-Level Adapter、针对不同扫描中心的Center-Level Adapter，并以变分展开重建网络为主干。同时引入Universal Adapter，通过中心无关的随机训练实现对全新中心的泛化。在损失函数方面，采用多尺度SSIM损失，融合频域增强和对比度自适应加权，以提升优化鲁棒性。

Result: 在CMRxRecon2025大规模多中心数据集（涵盖5+中心、10+扫描仪、9种模态）上，HierAdaptMR在跨中心泛化和重建质量方面均优于现有深度学习方法。

Conclusion: HierAdaptMR为多领域MRI重建提供了参数高效、泛化性强的解决方案，有望促进深度学习模型在真实临床多中心场景下的落地应用。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [147] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 本论文提出了一种新的人机交互采集指导方法，通过语义分割和视觉-语言模型，为多尺度场景扫描识别并突出显示关键物体，以优化3D高斯扩散等新颖视图合成算法的输入采集质量。实验结果显示在真实场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高质量视图合成（例如3D高斯扩散）需要均匀密集的视角图片输入，但人为采集往往效率低且覆盖不均，尤其对重要具有视角依赖性的物体捉襟见肘。现有采集指导方案要么偏重单一物体，要么忽略了材料的视角依赖特性，因此亟需更智能的采集辅助技术。

Method: 作者提出了一种多尺度场景扫描的在场可视化指导方法。通过语义分割和类目识别，结合视觉-语言模型对场景中各物体重要性排序，并在关键物体周围生成球体引导摄像机运动，保障这些区域得到更充分的拍摄，从而提升视角依赖性表现的完整度。

Result: 在真实场景实验中，新方法在提升视图合成质量和拍摄引导效果方面均优于传统均匀取样与单一物体导向策略。

Conclusion: 论文验证了基于语义和视觉-语言排序的多尺度采集指导能够有效提升3D新颖视角合成质量，尤其在场景复杂且包含多个重要对象时表现更加突出。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [148] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 本文提出了首个专为人形体编辑设计的大规模数据集，并基于此开发了Odo，一种通过扩散模型实现真实、直观人体形体编辑的方法，性能全面优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前人形体编辑领域受限于数据集稀缺和现有方法（如3D可变形模型或图像变形）带来的比例失真、纹理损坏及背景不一致等问题，阻碍了其发展。

Method: 作者创建了包含18573张、1523人、涵盖胖、肌肉、瘦等多样体型、背景衣着一致的大规模数据集，并提出Odo方法：利用冻结的UNet保持细节和背景，结合ControlNet以SMPL深度图引导体型变换，实现端到端扩散式人形体编辑。

Result: 实验表明，Odo方法的逐顶点重建误差低至7.5mm，远优于基线方法的13.6mm，并能真实精确地编辑人体形体，极大减少不自然失真。

Conclusion: Odo及其大型数据集推动了人形体编辑领域发展，提供高质量且可控的体型编辑能力，为后续研究和实际应用奠定坚实基础。

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [149] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 本文提出了一个分两步的多模态方法，结合胸部X光片和MIMIC-Eye数据集，通过引入眼动追踪信号来提升疾病分类和区域相关的放射学报告生成效果。


<details>
  <summary>Details</summary>
Motivation: 病灶分类和医学影像报告生成是医学人工智能的关键任务，但现有方法难以有效融合医生的关注区域信息。作者希望通过引入眼动追踪数据，提高模型准确性及报告解释性。

Method: 方法分两阶段：第一阶段提出一个注视引导的对比学习结构，将视觉特征、临床标签、绑定框和眼动信号融入联合训练，设计了包含MSE、KL散度、相关性和质心对齐的新型多项注视损失函数；第二阶段构建一个模块化报告生成流程，抽取诊断关键词，通过先验构建的字典映射至解剖区域，并利用结构化提示生成区域相关句子。

Result: 引入眼动数据后，疾病分类的F1分数从0.597提高到0.631，AUC从0.821提升到0.849，精度和召回率同步提升。报告生成部分在临床关键词召回和ROUGE指标上也有提升。

Conclusion: 整合医生注视信息能够显著提升疾病分类性能，并提高医学报告的可解释性和区域相关性，证明了注视信息指导下的多模态方法的有效性。

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [150] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 本论文提出利用Stable Diffusion生成ID卡真样本合成图像，以解决训练用真样本数量不足问题，并验证这些合成样本提升了检测系统的泛化能力，有助于提升演示攻击检测（PAD）系统性能。


<details>
  <summary>Details</summary>
Motivation: 目前PAD系统由于真样本（bona fide images）稀缺，加上攻击方式多样，增加了训练高效检测系统的难度。多数算法只关注生成攻击样本，忽视了真样本数量的不足。

Method: 创新性地使用Stable Diffusion生成逼真的ID卡真样本图像，并将合成样本用于PAD系统的训练与评估。同时，通过从零训练的检测系统和商用解决方案分别进行实验验证。

Result: 实验结果显示，检测系统能将生成的合成样本识别为真样本，且对检测系统的泛化能力和性能有积极作用。

Conclusion: 生成高质量真样本合成图像有助于缓解数据受限，提高PAD系统在实际应用中的表现和鲁棒性。

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [151] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 本文针对遥感视觉问答（RSVQA）领域模型的可解释性和数据偏差问题，提出了Chessboard数据集及可解释模型Checkmate，实现更透明、可信的推理。


<details>
  <summary>Details</summary>
Motivation: 现有RSVQA模型普遍缺乏可解释性，且数据集存在分布偏差，导致模型利用捷径做决策，难以实现真正可信的视觉推理。因此，亟需解决模型解释性及数据集偏差问题。

Method: 作者构建了Chessboard数据集，包含3,123,253个问题，提供均衡的答案分布，并将每个答案与图像具体单元格关联，实现细粒度视觉推理。基于该数据集，提出了Checkmate模型，通过识别最相关的图像单元格，提高模型决策的可解释性。

Result: 通过在多种模型结构下的实验，作者证明Checkmate模型在提升决策透明度和可信度方面有明显优势，优于现有RSVQA方法。

Conclusion: 提出的数据集和模型显著增强了RSVQA系统的可解释性与决策透明性，为遥感视觉问答模型的可靠应用奠定基础。

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [152] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 提出DMS方法，利用扩散模型生成新视角，解决无监督双目/单目深度估计中遮挡和重建难题，无需标注数据，实验效果优异。


<details>
  <summary>Details</summary>
Motivation: 虽然基于监督的双目匹配与单目深度估计取得进展，但自监督方法在处理遮挡等重建模糊区域上仍有痛点；因此，论文旨在提升自监督深度估计的效果，特别是面对遮挡、视野外区域时的歧义问题。

Method: 提出一个通用的、模型无关(colloquial: plug-and-play)的方法DMS，将扩散模型引入深度估计算法。具体做法是微调Stable Diffusion模型，基于双目相机的极线几何合成新的视角（如左移/右移视角和两相机间新视角），用来补充原始图像中遮挡或缺失像素，实现更准确的光度重建，从而提升自监督训练表现。

Result: 通过基准数据集大量实验，DMS方法可降低35%的outlier比例，并在若干评测指标上达到了最新最好（state-of-the-art）性能。

Conclusion: DMS方法能够高效、无成本地集成到现有自监督深度估计流程中，无需人工标注，显著缓解了遮挡区重建难题，赋予深度估计更强的泛化和适应能力。

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [153] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: 本研究评估了基于Transformer的实时检测模型RT-DETR在海滩垃圾自动检测与计数任务中的表现，对比了RT-DETR-L与RT-DETR-X两个变体。尽管RT-DETR-X的准确率略高，但RT-DETR-L推理速度更快，更适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 海岸污染日益严重，需要高效、自动化的监测与管理方法。目前缺乏可扩展、自动化且适用于实际环境的垃圾检测方案。

Method: 采用最先进的Transformer检测模型RT-DETR，分别对其大型（L）和超大型（X）两个模型在公开海岸垃圾数据集上进行训练与严谨对比分析，通过mAP@50和mAP@50-95评测准确率，并比较两者的推理速度。

Result: RT-DETR-X模型mAP@50为0.816，mAP@50-95为0.612，略高于RT-DETR-L模型的0.810和0.606，但RT-DETR-L模型推理速度快（20.1 ms vs 34.5 ms）。

Conclusion: 在实时野外应用中，RT-DETR-L因推理速度快且检测精度高，性价比更优，是更实用的环境保护检测方案。本研究突出了模型复杂度和实际可用性之间的权衡。

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [154] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的动画迁移方法Motion2Motion，能够在骨骼结构差异较大的不同角色之间实现运动迁移，无需大规模数据集，仅依赖少量骨骼对应关系和目标动画示例即可高效完成任务。


<details>
  <summary>Details</summary>
Motivation: 现有的动画迁移方法多数针对骨骼结构相似的角色，对于骨骼拓扑差异较大的情况缺乏有效手段，主要受制于骨骼对应关系难以建立以及缺乏跨结构的大规模配对动作数据集。

Method: 提出Motion2Motion框架，无需训练，利用极少量目标骨骼动画实例及稀疏的骨骼对应关系，实现运动从源到目标不同骨骼拓扑的高效迁移。通过定性和定量评测，分析与现有技术的优劣。

Result: Motion2Motion在相似骨骼和跨物种骨骼结构的动画迁移实验中，均表现出高效且可靠的效果；方法易于集成到实际应用和用户界面中，验证了其实用性。

Conclusion: Motion2Motion有效突破了不同骨骼拓扑间动画迁移的瓶颈，为工业应用提供了切实可行的方案，并已在实用案例中获得验证。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的3D场景重建方法IGFuse，通过融合多次扫描数据，利用物体自然重排揭示先前被遮挡的区域，实现高质量的可交互高斯场景重建。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉与机器人领域，完整且可交互的3D场景重建面临遮挡与传感器覆盖有限的长期难题。现有方法多需多阶段流程或高密度扫描，且精度有限，难以扩展。为此，作者希望提出一种能自动融合多次扫描，减少遮挡影响并简化流程的新方法。

Method: 提出IGFuse框架，利用多次场景扫描（扫码过程存在物体自然而然的位移和遮挡变化），融合各次观测，生成分割感知的高斯场，并施加双向光度与语义一致性约束。通过引入伪中间场景状态来统一配准，以及协同约剪机制优化几何结构。

Result: IGFuse无需密集观测或复杂流程，实现了高保真度的渲染和对象级场景操作。实验表明，其对新场景配置具备很强泛化性，3D重建与仿真迁移能力显著优于现有方法。

Conclusion: IGFuse创新地将多次扫描中的信息有效融合，突破了现有方法的局限，实现了实时、鲁棒、高精度和可交互的三维场景重建，为实际应用（如机器人、虚拟/增强现实等）提供了有力工具。

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是一种能够从单张图片生成动态三维（4D）场景表示的高效端到端模型，性能超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有4D场景生成方法依赖多帧视频输入或高计算量优化，缺乏单帧高效生成能力，且优质4D数据稀缺，限制了4D模型的发展。

Method: 1）构建了大规模高质量4D数据集4DNeX-10M；2）提出统一的6D视频表示，联合建模RGB和空间XYZ序列；3）设计了一套简单有效的迁移策略，使预训练视频扩散模型适配4D建模任务。

Result: 4DNeX生成的高质量动态点云支持新视角视频合成，实验表明其在效率和泛化性上优于现有4D生成方法。

Conclusion: 4DNeX为单图像到4D建模提供了高效可扩展的方案，为可模拟动态场景变化的生成式4D世界模型奠定基础。

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [157] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）内部权重激活信息来自动构建语言空间的新框架，实现了无需手工特征提取即可刻画语言间关系，并在多样数据集与多语LLM上验证了有效性，揭示了语言之间的新联系。


<details>
  <summary>Details</summary>
Motivation: 传统的语言相似性度量多依赖于手工设计的语言学特征，不仅耗时且受专家主观影响，且难以利用现代LLM蕴含的丰富表示能力。作者希望探索能否利用LLM自身的内部权重信息自动刻画和度量语言关联。

Method: 作者提出通过改编后的剪枝算法，计算LLM各语言输入时模型内部权重的重要性得分，进而自动生成高维的语言向量。这些向量用于构建语言的度量空间，取代了传统依赖手动特征的做法。

Result: 该方法在覆盖106种语言的多语大模型、多样数据集上进行了实证。实验结果显示，自动得出的语言间关系与传统语言学家划分的语系高度吻合，同时也揭示了一些潜在的、未被传统语系归类的语言间联系，暗示历史接触或语言进化特征。

Conclusion: 利用LLM内部权重作为语言表征维度具备可行性和解释性，能够自动反映语言学本质并发现新的语言联系，为语言之间的关联测度提供了新的自动化、高效且客观的技术路径。

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [158] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: 本研究评估使用大模型生成的合成问答数据，能否作为无真人标注数据时的有效评测基准。结果发现，在变化检索器参数（检索器RAG）时，合成数据可以可靠评估，但在比较生成器结构时则不可靠。


<details>
  <summary>Details</summary>
Motivation: 高质量的人类标注数据稀缺，收集成本高。研究者希望通过LLMs生成合成数据，代替人工数据用于检验不同问答系统方案的优劣。

Method: 在四个数据集上（两个开放、两个专有），分别设计两个实验：一是改变检索器设置、固定生成器，二是相反。用合成基准与人工标注数据对比，观察评测排名是否一致。

Result: 当仅对比不同检索器配置时，合成问答基准的评测结果与真人数据高度一致。而对比不同生成器架构时，合成数据无法保持评测的一致性。

Conclusion: 合成问答数据可以部分替代人工标注数据，在评测检索器配置方面有效，但比较生成器结构时则不可靠。其不一致或来源于任务不匹配和风格偏差。

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [159] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: 本文将模仿学习方法应用于对话任务，通过专家演示训练对话模型，并开发了能够区分专家与生成对话的判别器，从而揭示了当前对话模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 在没有奖励信号时，构建有效对话策略存在困难，作者尝试利用模仿学习和专家示例来克服该难题。

Method: 采用模仿学习利用专家对话数据训练政策（对话生成模型），并引入判别器对专家与人工生成的对话进行判别，分析模型性能和局限。

Result: 训练出的对话策略效果良好，但判别器的表现揭示了现有对话模型存在的不足和局限性。

Conclusion: 这种结合模仿学习与判别器的方法不仅可用于提升对话策略，也能帮助发现普通对话模型中易出现的问题，对改进对话任务有指导意义。

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [160] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: 作者分析了Faetar语音识别任务中的转写不一致性对ASR性能的影响，发现其不是主要挑战；提出有限词典有助提升识别效果，但整体任务依旧非常困难。


<details>
  <summary>Details</summary>
Motivation: Faetar语音识别基准是一个低资源、难度较高的ASR任务，业界普遍关注转写不一致性对模型表现的干扰，作者希望通过实证分析，明确不一致性所带来的具体影响。

Method: 作者利用一个小型手工构建的词典，仔细分析了转写不一致性的情况，并测试了基于词的bigram语言模型和有限词典约束对识别性能的影响。

Result: 结果显示，转写中确实存在不一致，但主导挑战在任务本身；Bigram词语言模型无助于提升识别效果，而解码时约束在有限词典内则能带来一定好处。

Conclusion: 转写不一致并非任务的主要难点，适当利用有限词典能提升效果，但Faetar ASR任务总体依然极具挑战性。

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [161] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: 本文系统评估了当前主流大语言模型（如Google Gemini）在学术文本处理及同行评议辅助中的能力，发现其在复杂理解和高层次分析任务上表现有限，且目前不适于无监督、直接用于学术同行评议工作。


<details>
  <summary>Details</summary>
Motivation: 近来，关于大型语言模型（LLM）是否能够有效辅助科学发现和学术同行评审的讨论愈发激烈。本文旨在通过系统流程和严格评估，明确LLM在学术文本处理中的实际表现及其局限性。

Method: 作者将计算机科学研究中分散的任务有机整合为结构化评估流程，包括内容复述、文本对比、文本评分和深度反思四大任务，分别考查LLM在不同角色（如知识源、判断者、评价者、协作者）的表现。以三大顶级信息系统期刊文章为样本，通过多维文本指标，设计详细提示词，对Google Gemini等主流LLM进行系统测试。

Result: LLM在摘要和释义上的表现尚可；在文本对比和排名时扩展性较弱；进行学术评分时区分能力差；对深度反思的回答虽自洽但缺乏创新性见解。所有评估结果在内部（语言学）、外部（与基准答案比对）、及人工评价中都一致，且呈现出对提示词等变量的鲁棒性。

Conclusion: 现有大语言模型在学术文本处理和评审辅助中存在明显短板，尤其在高阶认知和判别型任务上难以胜任。作者不建议在缺乏人工监督的情况下将LLM直接用于构建学术同行评议。

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [162] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 论文提出了一种用于科学文本简化的新方法，结合大语言模型（LLMs）和两阶段流程，在句子和文档层面同时进行简化。


<details>
  <summary>Details</summary>
Motivation: 科学文本复杂晦涩，不利于非专业用户理解。现有简化方法在上下文连贯性和忠实性方面存在问题，因此需要提升科学文本简化的效果。

Method: 方法分为句子级和文档级：句子级先由LLM生成结构化计划，再按计划简化句子；文档级用LLM生成简明摘要，用摘要引导后续文档简化。融合两阶段流程，确保简化过程中内容连贯、忠实。

Result: 采用该方法能生成更加连贯、忠实于原文的科学文本简化结果，同时在简明性和内容保留度方面表现优异。

Conclusion: 所提两阶段LLM驱动的简化框架，有效提升了科学文本在句子及文档层面的简化质量。该方法具有良好的上下文一致性和内容忠实度，对科学文本简化任务具有参考价值。

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [163] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 本文提出了一种集成多种方法来检测和评估科学文本简化中的创造性生成与信息失真。


<details>
  <summary>Details</summary>
Motivation: 科学文本简化中可能出现信息失真或不准确。现有检测方法存在局限，需要更稳健的解决方案。

Method: 方法上，作者集成了BERT分类器、语义相似度度量、自然语言推断模型和大语言模型推理，并用元分类器整合多信号，提升检测的鲁棒性。另外，用LLM后编辑系统对简化文本进行校正。

Result: 多方法集成显著提升了对虚假及失真内容的检测能力。LLM后编辑策略有效提高了简化文本的可靠性和准确性。

Conclusion: 集成多种模型和后编辑方法能更全面、准确地检测和修正科学文本简化中的信息失真，提升结果质量。

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [164] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: 本文系统性回顾了与成语相关的心理语言学和计算语言学数据集，梳理其内容、形式和应用任务，并分析了数据集构建和注释的最新趋势。


<details>
  <summary>Details</summary>
Motivation: 成语的表意性和不可直译性增加了其计算处理和心理实验研究的难度。针对大量分布于不同领域的成语数据集，缺乏系统性综述以及两领域之间的联系亟待厘清。

Method: 作者收集并分析了53个用于成语研究的数据集，分别来自心理语言学和计算语言学领域，着重比较其注释维度、覆盖语言、任务类型和使用目的，总结当前数据集在注释和任务建模中的发展趋势。

Result: 心理语言学数据集主要以熟悉度、透明度等主观评分为主，计算语言学数据集则聚焦于成语性检测、释义生成、多语言建模等任务。近年来数据集的语言覆盖度提升，任务内容更加多样化，但两个领域的数据集仍相互独立、缺乏融合。

Conclusion: 虽然数据集在内容和建模任务方面都有进展，但心理语言学和计算语言学关于成语研究尚未形成协同发展，未来应推动两领域间的交流与资源共享。

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [165] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: 本论文提出通过模拟生物激素周期（如月经周期、昼夜节律）的方法，将生物节律嵌入到大语言模型中，以改善AI在上下文相关信息筛选上的表现，并发现模型表现随周期性激素变化而产生有意义的波动。


<details>
  <summary>Details</summary>
Motivation: AI系统在面对极大可能空间时，难以高效判断哪些信息与上下文相关，即“框架难题”。作者受到生物节律特别是激素周期作为相关性过滤机制的启发，尝试用生物学原理提升AI模型的上下文感知能力。

Method: 构建了一个框架，把基于周期函数建模的激素变化（包括雌激素、睾酮、皮质醇）的模拟周期，通过系统提示嵌入大语言模型。随后在多个主流大语言模型上分析这种周期性变化对语言生成情感和风格的影响，并使用SQuAD、MMLU等基准测试评估模型性能随周期的变化。

Result: 实验显示模型输出的情感与风格随“激素阶段”发生有意义波动，如月经期悲伤峰值、排卵期快乐主导、昼夜节律带来清晨乐观和夜间内省。不同激素水平对应模型在特定任务上的表现波动，最佳表现出现在激素水平中等而非极端的阶段。

Conclusion: 该方法为AI提供了一种新的上下文感知方式，同时揭示了社会性别和生物学偏见已内嵌于语言模型之中。

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [166] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 该论文探讨跨多语言迁移学习对委婉语检测的影响，提出顺序微调能显著提升低资源语言的检测效果。


<details>
  <summary>Details</summary>
Motivation: 委婉语受文化影响大且语义模糊，在低资源语言环境中，现有的语言模型在检测委婉语时存在较大挑战。如何提升多语言模型，尤其在低资源语言的委婉语检测能力，是亟需研究的问题。

Method: 作者对XLM-R和mBERT两个多语言模型进行了单语、同步和顺序微调三种策略的对比实验，涵盖英语、西班牙语、中文、土耳其语和约鲁巴语五种语言；分析了不同语言对的搭配、语言类型学特征及预训练覆盖度对模型性能的影响。

Result: 结果表明：用高资源语言做顺序微调可以有效提升在低资源语言上的委婉语检测性能，XLM-R模型提升幅度更大但对预训练覆盖和遗忘现象更敏感；mBERT提升较小但结果更稳健。

Conclusion: 顺序微调是一种简单、有效的方式，有助于提升多语言模型在低资源语言上的委婉语检测能力。

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [167] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的分词架构SupraTok，实现了显著提升的分词效率和下游NLP任务性能。通过引入跨边界多词单元、熵驱动数据选择和多阶段课程学习，SupraTok在多语言测试中优于现有分词器，并提升了GPT-2规模模型在推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP模型架构取得了巨大进步，分词策略却长期停滞不前，成为影响效率和性能的瓶颈。需解决当前分词器难以捕捉多词语义单元、训练数据质量不高和训练稳定性等问题。

Method: SupraTok在分词架构上有三大创新：1) 跨边界模式学习，自动发掘多词表达；2) 熵驱动的数据优化，筛选高质量训练语料；3) 多阶段课程学习，提升训练的稳定性。该方法在BPE基础上扩展，学习压缩率高且语义统一的“超级词”token。

Result: 在英文分词效率上，SupraTok相比OpenAI的o200k分词器提升了31%，相比Google Gemma 3提升30%，并在38种语言上保持竞争力。在GPT-2级别（1.24亿参数）模型中无改动集成后，SupraTok在HellaSWAG和MMLU基准测试上分别带来8.4%和9.5%的性能提升。

Conclusion: SupraTok展示了更高效的分词成为提升大模型性能的新路径，配合架构创新可进一步提升NLP模型。但作者强调需在更大规模模型验证其能力。

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [168] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: 文章提出了一种新的对话情感识别（ERC）方法——InitERC，这是一个一阶段的上下文指导微调框架，显著提高了情感识别的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前多轮对话中的情感识别依赖多阶段的指令微调，大多分别关注说话人特征和上下文情境，难以实现三者（说话人、上下文、情感）间的动态交互和紧密对齐。对此，作者提出联合建模的新范式以提升识别效果。

Method: 作者提出InitERC方法，通过一阶段的上下文指导指令微调实现说话人、上下文与情感的统一对齐。方法包括演示样例池构建、上下文样例选择、提示模板设计和上下文指导调优四大模块，并实证分析了检索策略、样例顺序和样例数量等关键因素的影响。

Result: 在三个公开数据集上的实验表明，InitERC方法在对话情感识别任务上显著优于最新的多阶段微调基线方法，取得了更好的性能。

Conclusion: 一阶段上下文指导微调方法（InitERC）有效增强了大语言模型对说话人、上下文、情感三者交互的建模能力，为对话情感识别任务带来了更准确的结果，显示出广阔的应用前景。

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [169] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出了CORE指标来量化多智能体大语言模型对话中的语言多样性和稳健性，并分析不同博弈环境下的语言使用差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型多智能体互动场景中涌现出多种能力，但对这些互动中的语言多样性缺乏量化方法。

Method: 提出了CORE（Conversational Robustness Evaluation Score）指标，结合了聚类熵、词汇重复率和语义相似度，系统量化对话质量。并在合作、竞争、中性三种博弈环境下，结合Zipf定律和Heaps定律分析对话词频分布和词汇增长。

Result: 在合作环境下，展现出更陡的Zipf分布和更高的Heaps指数，表明有更多词汇拓展伴随更多重复。而竞争环境下则表现为较低指数，表明词汇受限且重复较少。

Conclusion: 社会激励显著影响语言适应性，CORE指标能有效评估多智能体大模型系统的语言稳健性。

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [170] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文构建了关于汉语和日语完成体完美方面的自然语言推断(NLI)数据集，揭示了当前大型语言模型（LLMs）在处理时间推断上的不足。


<details>
  <summary>Details</summary>
Motivation: 英文中通过不同的语法形式区分完成体时态，但汉语和日语在完成体没有专门形式，这导致时态推断变得复杂。为了研究这一问题，作者希望评估现有NLI系统在不同语言下处理时间语义的能力。

Method: 作者基于语言学动机，设计了模板化的NLI数据集，每种语言1350对句子，并通过实验测试先进的LLMs在该任务上的表现，聚焦模型如何在完成体相关的时态和参考时间推断上表现。

Result: 实验结果显示，即使是当前最先进的LLMs也难以准确处理细微的时态及参考时间变化，尤其是在汉语和日语完成体方面。

Conclusion: 文章指出现有模型在时间语义推断上的局限性，强调跨语言的评测对于改进和完善NLI系统理解时间语义至关重要。

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [171] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的多代理协作对抗框架（CAMF），用于提升当前大型语言模型（LLM）生成文本的检测性能，通过多维度、多阶段的深度分析，实现对机器生成文本（MGT）的精确检测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的流行，机器生成文本（MGT）在传播虚假信息和学术不端等方面带来了风险。目前零样本检测方法虽实用，但分析较浅，常局限于少数文本特征，且缺乏跨维度一致性分析（如风格、语义、逻辑等）。因此亟需一种更全面、更深入的检测方法。

Method: 作者提出协作对抗多代理框架（CAMF），利用多个基于LLM的代理，在三个阶段协作：1）多维度语言特征提取；2）对抗性一致性探测；3）合成判断聚合。通过协作与对抗机制，深度挖掘文本中的细微、跨维度的不一致性特征，提升检测准确率。

Result: 实验结果显示，CAMF方法在零样本的MGT检测任务上，在准确率等指标上均显著优于现有的主流方法。

Conclusion: CAMF框架通过协作和对抗的多代理分析，能够有效克服现有检测方法的不足，大幅提升对机器生成文本的辨识能力。

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [172] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: 该论文提出了一种基于指令的大语言模型持续对比调优方法，以提升持续关系抽取（CRE）任务中的性能，并显著缓解灾难性遗忘。实验结果在TACRED和FewRel数据集上达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有持续关系抽取方法主要通过记忆回放和对比学习减少灾难性遗忘，但忽视了对模型认知偏差更敏感的错误样本。如何更有效利用这些错误案例，提升LLM的持续学习能力，是亟待解决的问题。

Method: 作者提出了一种新的指令引导式持续对比调优方法：首先，将每个任务的训练集和记忆集按照初次模型响应的正确与否进行划分，并以双任务微调的方式分别处理。《利用LLM的指令跟随能力，设计了基于指令的对比调优策略，让模型不断在指令提示下纠正已有认知偏差，缩小新旧知识之间的差距。

Result: 在TACRED与FewRel两个主流持续关系抽取数据集上进行实验，所提模型取得了显著优于现有方法的效果，创下新SOTA。

Conclusion: 通过重视和有效利用模型的错误案例，并结合LLM的指令感知能力，能够更好地缓解持续学习过程中的灾难性遗忘问题，并显著提升CRE任务表现，为未来CRE方向提供了新思路。

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [173] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE是一种新提出的LLM置信度估计方法，可在生成文本过程中提供高精度的细粒度置信分数，并通过多项创新方法显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种任务上表现优异，但本质上缺乏自我感知，经常对错误预测给出过高置信度，影响了生成内容的可靠性和可信度。因此，提升模型置信度估计的准确度成为亟需解决的问题。

Method: 作者提出了FineCE方法，包括：1）构建能有效反映LLM输出概率分布的训练数据流程，并以监督方式训练预测任意文本序列置信度的模型；2）创新性地提出Backward Confidence Integration（BCI）策略，推理时利用后文信息反向优化当前序列的置信度估计；3）设计了三种置信度估计位置的选择策略。

Result: 在多个公开基准数据集上的大量实验证明，FineCE在置信度估计的准确性和细粒度表现上，都稳定且明显优于现有主流方法。

Conclusion: FineCE有效提升了LLM生成内容的置信度估计水平，提高了模型输出的可解释性和可靠性。相关代码和基线方法均已开源，便于后续研究和实践应用。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [174] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于Jacobian结构的多目标优化方法（J6）以提升大型语言模型（LLM）适应性，能够在诸如提升事实性和增强置信度等不同目标间动态权衡，实现更高效和透明的参数调整。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型适应过程中需兼顾多目标（如事实性提升与置信度增强），但现有方法忽略了目标与参数间的复杂几何关联，难以高效解决参数冲突与协同。

Method: 提出J6方法，将梯度交互矩阵分解为六个可解释成分，可以实现硬性和软性参数更新，如通过argmax选择主导方向或通过softmax加权，形成动态的、结构化且具有几何解释的更新机制。

Result: J6方法不仅提升了参数归因和任务干扰分析的能力，还能根据局部冲突或协同情况动态调整模型，优化了多目标下的模型表现。

Conclusion: J6为多目标神经网络调优提供了可扩展的、结构化的雅可比推理机制，在冲突感知的prompt优化和神经模型调优中具有广泛应用前景。

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [175] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型评价方法STEM，可高效、可解释地区分不同模型的能力水平，解决了传统基准测试逐渐失效和算力消耗巨大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在公开基准上表现提升，但不必然代表推理能力真正增强，且由于过度拟合与评测成本高，很难有效区分模型优劣。

Method: 提出了STEM评价框架，核心思想是通过分析同一架构不同参数规模下的表现变化，找到‘显著转变样本’（STS），用这些样本对未知模型的能力进行高效估计。实验用Qwen3系列和六种代表性基准集构建STS池，并测试泛化性。

Result: 实验表明，STEM评价方法能可靠地反映模型能力的真实变化趋势，排名与真实能力高度一致。

Conclusion: STEM能为不同架构的大模型提供可扩展、细粒度且计算高效的能力评估，是一种实用的评测工具。

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [176] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: 该研究首次系统评估了“思维预算”机制在医学推理任务中的作用，揭示了计算资源与推理质量之间的基本缩放规律。通过大量实验，结果显示“思维预算”可显著优化不同大小大模型在医疗场景下的应用表现。


<details>
  <summary>Details</summary>
Motivation: 随着多参数大模型在医学领域的应用，如何在保证推理质量的同时优化计算资源分配变得极为重要。现有研究尚未全面揭示“思维预算”对模型推理性能的影响及其扩展规律。

Method: 作者系统地对Qwen3（1.7B-235B参数）和DeepSeek-R1（1.5B-70B参数）两大主流模型，在15个涵盖不同专科与难度的医疗数据集上，采用控制实验设定不同“思维预算”（从零到无限token），分析推理结果随预算与模型规模变化的规律。

Result: （1）推理准确率随“思维预算”和模型规模增加呈对数缩放规律；（2）划分了三类效率区间，分别适用于实时应用、常规临床支持和高要求诊断场景；（3）小模型扩展“思维预算”带来更高比例的性能提升；（4）不同专科对推理深度需求呈现明显差异；（5）相关机制对不同模型架构表现出良好的可迁移性。

Conclusion: “思维预算”是优化医学AI推理系统的关键机制，实现计算资源与实际医疗需求动态匹配，并提升了医疗AI部署时所需的透明度与效率。

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [177] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: 本文探讨了利用大语言模型（LLM）作为文本隐私评估工具的可行性，并与人类对于隐私的主观判断进行了系统性对比和分析。


<details>
  <summary>Details</summary>
Motivation: 虽然隐私保护在自然语言处理领域已有多项进展，但衡量文本隐私性的准确手段仍旧缺失。鉴于LLM在NLP多个评估任务中与人工标注高度一致，作者希望探索LLM-as-a-Judge能否用作隐私敏感性评估工具，进而解决隐私度量主观性强、难以标准化的问题。

Method: 作者采用了10个数据集、13种主流LLM及多达677位人类参与者作为评测对象，通过对比LLM和人类在文本隐私敏感性判断上的一致性来评估LLM的表现，并系统性分析了两者的推理过程。

Result: 结果显示，隐私敏感性确实是难以准确衡量的概念，人类之间的一致性本身也很低。但实验发现：LLM可以有效地模拟全局人类隐私视角，在整体评估上与人类表现较为接近。

Conclusion: LLM-as-a-Judge在文本隐私评估方面展现出一定潜力，能够作为补充或替代部分人工评估，推动了自动化、标准化隐私敏感性测评工具的发展进程。同时，作者也指出了LLM在此应用中的局限性与改进空间。

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [178] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: 本文对阿拉伯语多模态机器学习（MML）的现有研究进行了系统综述，并提出了新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态机器学习在全球范围内的快速发展，阿拉伯语相关的多模态研究也取得了基础性进展，因此有必要对该领域进行全面梳理与总结。

Method: 作者通过文献调研，按照数据集、应用、方法和挑战四个主题，对阿拉伯语MML相关研究进行了归纳和分析，构建了新的研究分类法。

Result: 本文呈现了阿拉伯语MML领域在数据集、实际应用、研究方法和面临的主要挑战方面的系统性总结，指出了目前仍未充分研究的领域和存在的关键空白。

Conclusion: 综述为研究人员提供了阿拉伯语MML现状的全景视角，明确了未来可拓展的研究方向和亟待解决的问题，有助于推动该领域继续发展。

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [179] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文提出了SEA-BED，这是首个涵盖东南亚（SEA）10种语言、9类任务、169个数据集的大规模句子嵌入基准，重在补全该区域缺乏高质量本地数据的空白。


<details>
  <summary>Details</summary>
Motivation: 东南亚地区拥有近7亿人口，但却没有区域性、原生语料的句子嵌入基准。现有多语种基准覆盖少且多靠机器翻译，忽略了本地语言的独特性，因此有必要构建专门面向SEA的、由人工整理的数据基准。

Method: 作者构建了SEA-BED，涵盖10种SEA语言、9类NLP任务、169个数据集（其中71%为人工整理）。研究设置三大问题：1）哪些语言和任务在SEA地区具挑战性，2）SEA语言的表现差异是否具有全球独特性，3）人工语料与机器翻译在评测中的差别。对6类任务、17个嵌入模型进行实验，开展跨基准间比较及翻译效应分析。

Result: 实验发现，不同SEA语言的模型表现有显著差异，6项研究结果中排名出现大幅变动。人工语料比机器翻译语料对低资源语言（如缅甸语）更加重要。

Conclusion: SEA-BED成功补全了东南亚NLP在本地嵌入评测上的短板，揭示了SEA语言任务的模型瓶颈及机器翻译对评测的局限性，为后续低资源语言的NLP模型发展提供了新工具和分析视角。

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [180] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: 本论文分析了语音基础模型（SFM）的内部知识，提出轻量级分析方法，并扩展了口语理解任务的数据集和评测，发现SFM在端到端任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前SFM在多种语音任务中表现突出，但对其内部所学知识及对复杂口语理解任务的能力尚不清楚，且相关评测数据集较少。

Method: 提出无需训练的统计分析方法，比较不同SFM在声学和语言知识层面的编码效果。针对口语命名实体识别（NER）和实体定位（NEL），构建评测任务和数据集，并用端到端SFM方法与传统级联方法进行对比。

Result: 分析发现不同SFM的层含有不同程度的声学和语言信息。构建的SFM端到端系统在NER和NEL任务上优于传统方法。端到端模型在不同SFM和适应策略下均表现良好。

Conclusion: 本论文提出的分析工具和新任务为SFM的理解和发展提供了基础，端到端SFM系统在口语理解等复杂任务上具有潜力，为今后模型设计提供了参考。

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [181] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文系统综述了将文本转换为结构化数据（如表格、知识图谱、图表）的方法、数据集和评测标准，并提出了通用评价框架，强调了其对下一代AI系统基础性作用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向自主操作和上下文感知检索发展，需要将非结构化文本高效转换为结构化数据，以便支持总结、数据挖掘等关键应用。目前相关研究方法、数据集和评测标准尚缺乏系统总结与整合。

Method: 采用系统性文献综述，对现有文本转结构（text-to-structure）技术、数据集和评测体系进行分类、梳理与评价，并提出了一个通用的结构化输出评价框架。

Result: 本文总结了当前主流的文本转结构方法、相关数据集与评测标准，分析了技术难点，并提出了未来研究的可能方向和统一评价体系。

Conclusion: 将文本转为结构格式是支撑下一代AI系统的重要基础设施。本文为该领域提供了系统的综述和统一评测框架，为后续研究和应用指明了方向。

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [182] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: 本文提出了一个新的大型语言模型（LLM）推理策略分类体系，并系统梳理了相关研究，旨在推动LLM实现更自适应和高效的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在推理上取得了重大进展，但现实任务中要求模型能够根据不同需求采用不同的推理策略，包括快速的直觉反应、缜密的逐步推理，以及结合外部工具的推理方式。因此亟需对这些推理策略有系统化的分类和理解。

Method: 作者借鉴认知心理学的理论，将LLM推理策略从两个维度分类：一是快速/慢速，用以区分直觉式与深思熟虑式的处理方式；二是内部/外部，用以区分依托模型参数自身知识，还是结合外部工具进行推理。同时，系统综述并分类了近年来自适应推理相关的研究方法及其决策要素。

Result: 通过梳理与归类，作者明确了当前LLM适应性推理领域的研究现状和主流方法，细化并揭示了不同推理策略的适用场景和关键瓶颈。

Conclusion: 面对LLMs推理能力的多样化需求，未来应着力解决适应性、自主性、效率和可靠性等方面的挑战，以推动LLMs在更广泛、复杂的实际场景中高效推理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [183] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估方式，即评估大语言模型（LLM）能否预测自身回答的特性。通过Self-Execution Benchmark发现，现有模型在该方面表现较差，且模型变大不一定提升其自我预判能力。


<details>
  <summary>Details</summary>
Motivation: 以往LLM评估主要关注知识和推理，但很少关注模型对自身输出的预测能力。作者希望探索模型能否了解和预测自身的表现和倾向。

Method: 提出了Self-Execution Benchmark，包含模型是否能预判某问题对自己来说是否困难、是否会拒答、常见联想等。使用该基准对不同LLM进行实验评测。

Result: 实验显示，主流LLM在该基准任务中的表现普遍较差，同时模型尺寸和能力的提升也未带来明显改进。

Conclusion: LLM在自我表现预判上存在基本缺陷，这表明当前模型在自我行为表征与推理能力上有根本性局限。

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [184] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出了LegalΔ，一种通过链式思维信息增益提升法律推理能力的强化学习框架，显著提高了法律大模型推理的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型（LLMs）在司法决策自动化方面取得一定进展，但还难以生成可靠且可解释的推理过程。具体表现为模型倾向于直接给出答案，缺乏细致的多步推理，这在需要严密论证的复杂法律场景下效果有限。

Method: LegalΔ采用强化学习框架，在训练时使用直接答案和推理增强两种输入模式，并最大化两者之间的信息增益。第一阶段，从强大的推理大模型DeepSeek-R1中提取潜在推理能力；第二阶段，通过差分比较和多维奖励机制（考虑结构连贯性及法律专业性）进一步提升推理质量。

Result: 在多项法律推理任务上，LegalΔ在准确性和可解释性方面均优于强基线，不需要人工偏好标注数据即可得到更为健壮、可信的判决结果。

Conclusion: LegalΔ能够系统性提升法律大模型的推理质量和可解释性，为法律AI实际应用带来更大价值，其代码与数据开源，便于后续研究和应用。

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [185] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了ChronoQA，这是一个评估中文检索增强问答（RAG）系统时间推理能力的大规模基准数据集。该数据集包含逾30万篇新闻和5176个高质量时间类问题，支持单文档和多文档场景，并经过多重验证以保证数据质量，旨在推动时间敏感型问答系统的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统基准往往不重视时间推理，尤其是在中文和检索增强场景下，缺乏专门针对时间信息理解与推理能力的评价资源。因此，作者希望填补这一空白，推动相关技术进步。

Method: 构建了ChronoQA数据集，从2019至2024年间的30多万篇新闻中抽取样本，并设计了5176个覆盖绝对、汇总和相对时间型问题，涵盖显式与隐式时间表达。数据集同时支持单、多文档任务，并采用规则、LLM和人工多阶段质量验证。

Result: ChronoQA数据集具备完善结构标注和高数据质量，能够支持多种时间推理任务的评测。实验结果表明它能有效区分不同系统在时间推理能力上的表现。

Conclusion: ChronoQA为中文时序推理的检索增强问答系统提供了动态、可扩展且高质量的评测工具，有助于推动该领域研究与系统发展。

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [186] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: 本文提出了一种结合法律逻辑与深度学习的缓刑预测方法，并证实该方法优于传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有智能司法辅助系统（IJAS）在缓刑预测方面缺乏专用方法，且对缓刑适用因素研究有限，仅依赖数据驱动方法，忽视司法判决的法律逻辑。

Method: 1. 构建包含案件事实与缓刑法律要素（PLEs）的专用缓刑数据集；2. 基于缓刑法律逻辑与双轨刑罚理论，设计新的多任务双理论缓刑预测模型（MT-DT）；3. 通过实验验证模型效果。

Result: MT-DT模型在缓刑预测数据集上的表现优于基线模型，同时对法律逻辑的分析进一步验证了方法有效性。

Conclusion: 结合法律逻辑的深度学习方法能显著提升缓刑预测的准确性和实际适用性，为智能司法提供更合理的决策辅助。

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [187] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: 本文提出了一种将现有的Transformer编解码器ASR模型改造为低延迟流式识别系统的方法，通过因果化编码器和精细微调，使其具备实时转录能力，实验结果优于现有流式方法。


<details>
  <summary>Details</summary>
Motivation: 目前SOTA离线ASR模型如Whisper和Canary虽然性能优异，但架构和训练方式不适用于流式（实时）语音转写。因此，有必要探索如何将高性能的离线ASR系统转化为低延迟的流式系统。

Method: 作者分析了将传统编码器-解码器Transformer转化为低延迟流式模型的难点，通过采用LoRA低秩微调方法，将原本非因果编码器变为因果编码器，并对编码器和解码器进行联合微调，利用弱对齐数据集。同时，提出了新的推理机制，支持贪婪和束搜索译码，其效果经过理论分析为局部最优。

Result: 在低延迟（chunk size<300ms）实验下，微调后的模型在大多数场景下优于现有未微调的流式ASR方案，并且模型复杂度更低。此外，训练过程带来了更好的对齐效果，可方便实现词级时间戳提取。

Conclusion: 通过对Transformer架构和训练流程的合理改造，离线高性能ASR模型可有效应用于实时流式转写场景，且表现优异。相关代码和模型已开源，有助于推动流式ASR领域的发展。

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [188] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文关注于多答案问答（MAQA）任务，提出了一种利用事实核查数据集构建冲突感知MAQA基准（NATCONFQA）的方法，并发现当前主流大语言模型在处理答案冲突时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的多答案问答数据集构建成本高，且常依赖合成数据或局限于是/否问题，缺乏真实世界中包含冲突答案的高质量数据。因此，迫切需要推动包含真实冲突情境的MAQA研究。

Method: 作者提出了基于事实核查数据集的新方法，自动构建包含详细答案冲突标签的NATCONFQA数据集。任务要求模型不仅识别所有有效答案，还要检测出具体冲突的答案对。随后，作者评估了八个先进LLM在该数据集上的表现。

Result: 结果显示，现有LLM在区分和处理答案冲突方面较为脆弱，并采用了不理想的冲突解决策略，难以胜任包含多样化冲突的问答场景。

Conclusion: NATCONFQA为冲突感知的MAQA研究提供了高效、真实且具挑战性的新基准。当前LLM尚未能有效应对冲突答案的识别与处理，该领域仍有较大发展空间。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [189] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReaLM的强化学习框架，提升小型语言模型（SLM）在推理、自治和泛化等多方面的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管SLM在费用和资源消耗上优于大型模型，但在多步复杂推理中容易出错且表现不佳。已有方法往往不能同时兼顾推理能力、自主性和泛化能力。该研究旨在全面提升SLM在这些核心维度的表现。

Method: 提出ReaLM框架，包括：（1）MRPV策略，通过对比正反推理路径提升推理能力；（2）EAAI训练方法，通过逐步减少外部指导信号提升模型自治性；（3）引入有指导的Chain-of-Thought蒸馏，帮助模型学习领域知识，提升泛化能力。

Result: 在多个垂直领域和通用推理任务上进行了大量实验，ReaLM在推理能力、自主性和泛化三方面显著提升了SLM性能。

Conclusion: ReaLM框架为SLM带来了更强推理能力、更好自治性和更佳泛化能力，为小模型高效应用于复杂推理任务提供了新解决方案。

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [190] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedKGent的LLM代理框架，能高效构建随时间动态演化的医学知识图谱，并在大规模PubMed医学文献上取得优异效果，在多项医学问答任务中显著提升了大模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前医学知识图谱构建大多依赖监督流程，泛化能力有限，或简单利用大模型结果，忽略知识时间动态和不确定性，难以适应医学知识的快速演进和复杂性。

Method: 作者提出MedKGent框架，借助Qwen2.5-32B-Instruct驱动的Extractor与Constructor两个智能体，在1975-2023间的千万级PubMed摘要上逐天模拟知识演化。Extractor以采样法抽取实体关系三元组并赋置信度；Constructor通过时间戳和置信度将高置信数据集成到知识图谱，并处理重复与矛盾。

Result: 最终知识图谱含156,275个实体和2,971,384条三元组。经两款先进LLM及三位领域专家评估，准确率近90%，多项医学问答基准（RAG）均显著优于未加知识增强的基线，且药物再定位等应用场景具备实际价值。

Conclusion: MedKGent可高效构建动态、可信的医学知识图谱，不仅提升了医学领域大模型问答能力，还为知识驱动的医学研究提供了可扩展、实用的基础设施。

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [191] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: 本文提出了一种结合规则和BERT模型的自然语言处理（NLP）方法，用于从电子病历中高效、准确地提取和识别新冠后遗症（PASC）症状。该模型在多中心大规模数据上表现良好，有助于提高PASC的诊断效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 新冠后遗症（PASC）症状多样且随时间变化，给临床诊断带来很大挑战。目前缺乏高效且准确的工具从临床文本中自动提取和识别相关症状，亟需开发智能化手段提升PASC诊断水平。

Method: 研究团队构建了包含PASC相关专业术语的词表，并与临床专家协作；设计了基于规则的命名实体识别与BERT断言检测相结合的混合NLP流程，对RECOVER项目的多中心临床记录进行症状提取和断言判别；采用多中心医生记录进行模型开发和评估。

Result: 在内部和外部多中心数据集上的症状断言检测平均F1分数分别达0.82和0.76，处理单份文本平均2.4秒左右。症状提及与人工标注结果的相关性极高（ρ>0.83，P<0.0001），验证了模型的有效性和效率。

Conclusion: 所提出的混合NLP流程能够高效、准确地从临床病历中识别新冠后遗症症状，具有实际应用潜力，可为PASC的诊断提供强有力支持。

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [192] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: 本文提出了一种名为ZigzagAttention的方法，通过对大语言模型（LLM）中的注意力头重新分层，从而优化长文本上下文处理时KV缓存对内存和延迟的消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文信息时，KV缓存的内存占用极大，影响模型部署和推理效率。近期工作关注减少这一消耗，特别是通过区分注意力头类型（retrieval 与 streaming）部分裁剪KV缓存。但原有方法会导致额外的延迟。

Method: 作者提出了一种新的判别准则，将retrieval heads 与 streaming heads分别聚合在不同的层中，实现分层处理。这样可以有效减少KV缓存的开销并消除由头类型混合带来的额外延迟。该方法被称为ZigzagAttention。

Result: 采用这种分层方法后，在可接受的性能损失下，有效降低了KV缓存的内存消耗和延迟，优于采用混合注意力头的现有优化方法。

Conclusion: ZigzagAttention方法能进一步优化大语言模型的部署效率，降低内存和延迟消耗，在不显著影响模型性能的前提下，提升了实际应用的可用性。

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [193] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: 本研究提出“文化基因”概念，比较了GPT-4（偏西方）与ERNIE Bot（偏东方）在两大经典文化维度上的表现，发现两者在价值观取向上有显著差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的全球部署使其文化和伦理假设问题重要，但相关探讨不足。本文希望揭示LLMs如何继承其训练语料的文化基因，从而引发对文化敏感部署的重视。

Method: 作者引入了文化探针数据集（CPD），包含200个关于个人主义-集体主义（IDV）和权力距离（PDI）的探针题。采用标准化zero-shot方法，对GPT-4（西方语料）和ERNIE Bot（东方语料）回答进行人类标注和文化得分分析，并与霍夫斯泰德国家文化分数进行对比。

Result: GPT-4表现为个人主义、低权力距离；ERNIE Bot体现集体主义和较高权力距离。两者文化倾向与各自代表国家（美国和中国）高度一致。所有差异均有显著统计意义（p<0.001）。

Conclusion: LLMs会映射训练语料的文化价值观。因此，在评估和部署时要考虑文化因素，防止算法带来的文化霸权。

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [194] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 本文探究大型语言模型（LLM）在物理系统动力学预测任务中的上下文学习（ICL）能力，并揭示其内部机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多类型任务中展现出强大的ICL能力，但它们如何在不同任务类别（尤其是高度结构化的物理任务）中成功运作，其内部机制仍未明确，物理任务能作为检测和解析这类机制的理想测试平台。

Method: 作者使用物理系统的动力学预测任务测试LLM的ICL能力，通过不同长度的输入上下文检验其预测性能，同时利用稀疏自编码器（SAEs）分析模型激活，探查模型内部特征与物理变量的关联。

Result: 实验表明，LLM在上下文长度增加时其动力学预测表现提升。通过激活分析，发现模型学到的特征与能量等关键物理变量密切相关。

Conclusion: LLM在ICL过程中能编码和提取有意义的物理概念，表明其具有跨任务学习物理规律的潜力，研究为理解LLM在真实结构化场景下的学习机制提供了新视角。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [195] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: 本文提出了一种名为M3PO的新型方法，通过智能选择模型生成的高价值偏好样本对，提高大型视觉语言模型（LVLMs）在视觉指令跟随任务中的能力，减少对高成本人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调（SFT）与偏好优化方法（如RLHF、DPO）在高效利用模型自身生成空间、挖掘有挑战性的负样本上效果有限，且人工标注成本高、质量难以保证。因而需要一种更高效、数据利用率更高的方法提升LVLMs性能。

Method: 提出M3PO方法，从模型生成的多样候选样本池中，通过结合多模态对齐分数（MAS）和模型自信分数（对数概率），计算创新型M3P-Score，自动筛选出具有高学习价值、尤其是困难反例的偏好样本对，用于直接偏好优化（DPO）微调，实现了高效的数据利用。

Result: 在多个主流多模态指令跟随基准（如MME-Bench、POPE、IFT和Human Pref. Score）上，M3PO方法在LLaVA-1.5 (7B/13B)等基础模型上，一致优于强基线（SFT、模拟RLHF、传统DPO、RM-DPO等）。

Conclusion: M3PO方法有效缓解了LVLMs依赖高成本人工偏好标注的问题，通过合理挖掘模型生成空间内的高价值偏好样本，提升了多模态指令跟随能力，具有数据高效、泛化能力强的特点。

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [196] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: 该论文提出了LoraxBench，这是一个专为印尼低资源语言开发的NLP基准测试，涵盖六种多样化任务，评估了20种语言和不同的语言模型，发现低资源语言的NLP仍面临巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管印尼人口众多、语言丰富（有700种语言），但其在NLP领域的发展却相对滞后，尤其低资源语言缺乏有效的评价基准，限制了相关技术的进步。本研究旨在弥补这一差距，促进多语种和低资源NLP的发展。

Method: 作者建立了LoraxBench基准，覆盖印尼20种低资源语言以及三种语言的两种文体（正式与非正式），设置六项NLP任务（阅读理解、开放域问答、语言推断、因果推理、翻译、文化QA），并用多种多语种及区域性大模型进行性能评测。

Result: 基准测试结果表明，该任务集对现有模型构成较大挑战。在印尼语和其他低资源语言间表现存在明显差异，区域模型相比通用多语模型未形成绝对优势。此外，更高阶、罕见的文体（如爪哇语敬语Krama）显著降低了模型表现。

Conclusion: LoraxBench有效填补了印尼低资源语言NLP基准的空白，实验显示当前主流LLM在这些低资源语言、特殊文体处理上仍有明显不足。跨语言、跨文体的建模是今后改进的关键方向。

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [197] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI发布了两款开源大模型GPT-OSS（20B和120B参数），并系统对比评测了市面多种大模型，发现小模型（20B）在某些基准上表现优于大模型（120B），突显稀疏架构规模扩展未必带来线性收益。


<details>
  <summary>Details</summary>
Motivation: OpenAI自2019年GPT-2以来首次发布大规模开源权重模型，希望通过性能评测，为开源社区大模型选择和优化提供实证参考。

Method: 作者发布了两款基于MoE（专家混合）架构的模型（20B和120B参数），与当前主流6款开源大模型（参数量14.7B-235B，包含稠密和稀疏结构）在10项基准上对比评测。评测涵盖常识、数学推理、代码生成、多语言和对话能力，全部以未量化模型统一推理配置，并用McNemar检验和效应量进行统计分析。

Result: gpt-oss-20B在多项评测（如HumanEval、MMLU）表现优于gpt-oss-120B，而且推理资源消耗显著更少。两者在代码生成表现较强，但多语言能力较弱，总体处于开源大模型的中等水平。

Conclusion: 当前稀疏大模型参数简单扩展并未带来预期性能提升，未来需关注进一步优化策略，为开源大模型部署和选择提供效率与性能的实证指导。

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [198] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: 本文通过对RoBERTa和GPT-2在语法信息缺失的训练数据上的表现分析，发现大模型在缺乏语法提示时，其动词表征能力下降明显，特别对于心理动词。这与人类儿童语言学习中的语法助推假说一致。


<details>
  <summary>Details</summary>
Motivation: 想探究大规模语言模型在动词意义学习中，是否像人类儿童一样依赖句法信息（语法助推），以及这种机制在不同类型词汇（如心理动词、实体动词、名词）中的体现。

Method: 通过对RoBERTa和GPT-2训练数据进行操控，分别去除语法信息或共现信息，分析模型在不同训练环境下对动词及名词的表征能力变化。重点关注心理动词和实体动词的表现差异。

Result: 去除语法信息后，模型对动词，尤其是心理动词的表征能力下降更大。相比之下，去除共现信息对名词的影响更大。

Conclusion: 大模型对动词的学习与人类存在相似的语法助推效应，心理动词对此机制尤为依赖。操控语言模型的训练环境有助于大规模验证语言发展假说。

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [199] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的因果推理框架CDCR-SFT，通过让大语言模型显式构建变量级的因果有向无环图（DAG），显著提升了模型的因果推理能力，并减少了逻辑不一致的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常出现逻辑上不一致但表面连贯的幻觉，而现有的推理方法如CoT及其变体只能在语言层面运作，无法真实建模变量间的因果关系，因此需要一种能明确表达因果结构的新方法。

Method: 作者提出了CDCR-SFT框架，让LLM在训练时显式地构建变量级的因果有向无环图，并在图上进行推理。该方法基于新构建的CausalDR数据集进行监督微调，数据集中每个样本都包含输入问题、显式因果DAG、基于图的推理过程以及经验证的答案。

Result: 在四个大语言模型和八项任务上的实验表明，CDCR-SFT显著提升了因果推理能力，在CLADDER任务上达到95.33%的最高准确率，首次超过人类（94.8%），在HaluEval上模型幻觉率降低了10%。

Conclusion: 通过对LLM引入显式的因果结构建模，可以有效缓解模型输出中的逻辑不一致问题，显著提升了因果推理的可靠性和准确性。

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [200] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 本文提出了一种名为CorrSteer的新方法，通过推理时的激活与样本正确性相关性，自动选择和利用SAE特征，提升下游任务表现，特别在Gemma 2 2B和LLaMA 3.1 8B上效果显著。


<details>
  <summary>Details</summary>
Motivation: 原有的稀疏自编码器（SAEs）在无监督特征抽取和LLM可解释性方面有优势，但在实际下游应用中存在受限：一是对比数据集需求高，二是需大量存储激活值，导致实用性差。

Method: 提出CorrSteer方法，核心做法是推理过程中收集激活值，统计其与样本正确性的相关性，从中自动筛选出高相关性特征，并以平均激活为依据追踪操控系数，完全无需额外对比集或存储负担。

Result: 在QA、偏见抑制、安全防御和推理等多项任务上，CorrSteer在Gemma 2 2B和LLaMA 3.1 8B模型上都较传统方法有明显提升。例如MMLU基准提升了+4.1%，HarmBench提升了+22.9%（仅用4000样本）。筛选出的特征也表现出对不同任务语义需求的适应性。

Conclusion: 基于相关性的CorrSteer特征选择和自动控制方法为SAE在大语言模型智能操控中的实用落地提供了有效且可扩展的路径。

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [201] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: 该论文系统性研究了多模态大语言模型（MLLM）用于自动口语评测（ASA），展示其在内容和语言使用方面的卓越性能，并提出了Speech-First Multimodal Training（SFMT）方法，有效提升了口语传递方面的评测效果。


<details>
  <summary>Details</summary>
Motivation: 传统的自动口语评测系统存在局限：仅基于文本的方法缺乏声学信息，仅基于音频的方法又缺乏语义信息。如何结合多模态信息，实现更全面、准确的自动口语评测成为重要问题。

Method: 作者首次系统性地研究了多模态大语言模型（MLLM）在自动口语评测中的应用，对比分析了MLLM在内容、语言和传递等方面的表现。为克服MLLM在传递方面的挑战，提出了Speech-First Multimodal Training（SFMT）的方法，采用课程学习策略，先加强语音建模基础，再进行跨模态融合训练。

Result: 实验表明，基于MLLM的系统在整体口语评测性能上显著超越传统方法，PCC值从0.783提升至0.846。针对传递方面，SFMT方案比传统训练方法的准确率提高了4%。

Conclusion: 多模态大语言模型为自动口语评测提供了全面高效的新方案。SFMT方法特别提升了在口语传递维度上的评测效果，为ASA领域开辟了新路径。

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [202] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Semantic Anchoring的混合记忆架构，通过结合显式语言学线索（如句法依存、话语关系、指代消解）以提升大语言模型在多轮、长期对话中的记忆和召回能力。实验结果表明，该方法显著提升了事实召回率和对话连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统虽然能够通过稠密向量存储对话历史，但往往无法捕捉更细致的语言结构特征，导致模型在长期对话中的记忆和语境把握能力受限。为了解决这一问题，作者希望增强对话历史表示的结构化和丰富度。

Method: 作者提出Semantic Anchoring架构，在稠密向量基础上，融合句法依存分析、话语关系标注和指代消解技术，生成结构化的记忆条目。这种混合式方法既保留了语义相似性，又引入了显式的语言学结构。两者结合用于长期对话历史的存储和检索。

Result: 在改编后的长期对话数据集上，Semantic Anchoring相比于传统RAG系统，事实召回率和对话连贯性提升高达18%。此外，作者还进行了消融实验、人类评估及错误分析，验证了方法的稳健性和可解释性。

Conclusion: 将结构化语言学知识融入对话历史存储显著提升了多轮对话中的记忆召回和语境理解能力，为LLMs增强长期互动能力提供了有效的新范式。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [203] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro是一种集成不同容量与效率的大语言模型（LLM）的推理路由框架，通过对查询进行嵌入与聚类，将其动态分配到最合适的模型，实现性能与效率的灵活权衡，在多个基准测试中获得最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型发展面临性能与效率的权衡难题，现有方案很难兼顾推理表现与计算成本，亟需一种通用且灵活的解决办法。

Method: Avengers-Pro首先将输入查询进行嵌入与聚类，然后根据预设的性能-效率得分，将每个查询路由到最匹配的大语言模型（如高效模型或高容量模型），实现了多模型集成下的性能与成本调节。

Result: 在6项基准和8种主流LLM上，Avengers-Pro可通过调整参数，实现比单一最强模型（GPT-5-medium）高7%的平均准确率；还能以低27%的成本达到与最强模型同等准确率，或以低63%的成本实现90%的性能表现，并在准确率与成本之间取得最优的平衡。

Conclusion: Avengers-Pro为大语言模型的推理效率与性能权衡提供了统一且可调控的解决方案，在准确率和成本方面均达到了当前最优水平，拓展了LLM实际应用的灵活性和性价比。

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [204] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的识别大语言模型（LLM）生成的假新闻的方法——LIFE，通过识别语言模型生成文本中的统计指纹，有效提升了假新闻检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，伪造新闻变得异常容易并对社会造成威胁，现有主要通过文本内容检测假新闻，但由于内容往往流畅且看似真实，传统方法难以发现伪造痕迹。

Method: 作者通过分布性差异分析，发现通过提示词生成的LLM新闻在概率分布上具有独特语言指纹。基于该发现，提出LIFE方法，重建单词级概率分布以提取判别性特征，同时采用关键片段技术加强差异，从而提升检测性能。

Result: 实验表明LIFE方法在检测LLM生成的假新闻上取得了领先的性能，并且在人类编写的假新闻检测中依然表现良好。

Conclusion: LIFE方法通过语言指纹检测显著提升了对LLM生成假新闻的识别能力，是假新闻检测的有效工具。

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [205] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 针对大语言模型（LLMs）在低资源语言（LRLs）常识推理能力较差的问题，本文提出使用合成混杂语料微调模型的方法，并取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高资源语言如英语上的表现远优于低资源语言，导致不同语言使用者的体验不公平。提升低资源语言的模型表现对于公平性和多语种应用非常关键。

Method: 通过受控的语言混合技术，生成合成的混杂语言文本（包含高资源和低资源语言内容），再用该文本微调LLM。同时构建并发布了基于CommonSenseQA的多种语言比例的混杂语料集。

Result: 实验证明，用混杂语料微调后的LLM在低资源语言常识推理任务上的表现显著提升，并且对高资源语言的性能无负面影响，甚至有所提升。

Conclusion: 基于混杂语料微调是一种有效提升低资源语言表现的方法，有助于推动多语种公平性，并为后续相关研究提供了数据集和技术参考。

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [206] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: 本文探索了大型语言模型（LLM）根据文本预测人类感知的痛苦程度分数的能力，并提出了融入游戏化机制的评估方法。


<details>
  <summary>Details</summary>
Motivation: 情感和痛苦的感知对用户体验评估、人机交互等诸多领域至关重要。利用LLM从自然语言描述中自动预测痛苦分数，可以拓展情感推理任务在实际场景中的应用。

Method: 将任务设置为回归问题，对文本描述分配0-100的痛苦分数，评估了零样本、固定上下文少样本，以及基于句子嵌入的检索式少样本等多种提示方法，并提出“痛苦游戏秀”游戏化测评框架，通过若干轮次考察LLM的预测和适应反馈能力。

Result: 固定和检索式少样本方法均优于零样本，显示上下文例子的优势。游戏化机制揭示LLM在动态情感推理中超越静态回归的潜力。

Conclusion: 少样本提示能显著提升痛苦感知预测效果，LLM具有情感推理和反馈自适应能力，游戏化框架为该方向研究提供了新思路。

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [207] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 提出了一种名为ToolACE-MT的新框架，用于高效生成高质量、多轮多步的大模型智能体对话数据。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体LLM任务的数据生成方法大多依赖于多轮自回归交互，计算昂贵且效率低，限制了实际应用，因此需要一种更高效的数据生成方法。

Method: 提出ToolACE-MT框架，分为三阶段：粗粒度初始化（生成对话结构骨架）、迭代细化（掩码-填充操作丰富细节）、离线验证（通过规则和模型审查对话的正确性和连贯性）。

Result: 实验表明，ToolACE-MT可以高效、有效并具通用性地生成用于工具增强大模型智能体的多轮对话数据。

Conclusion: ToolACE-MT为高质量多智能体数据构建提供了新范式，有助于提升工具增强类大模型的应用性能。

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [208] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的多学科推理数据合成管道DESIGNER，通过设计逻辑抽象生成更具挑战性的推理题目，扩展了数据集的多样性和难度，有效提升了大模型的跨学科推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在多步复杂推理任务中表现不佳，且公开推理数据集往往缺乏学科广度和题目结构深度，难以充分训练和评估模型的真实推理能力。因此，亟需一种能够生成高质量、多样化多学科推理题目的合成方法，为推动大模型推理能力提升提供数据支撑。

Method: 作者提出了DESIGNER管道，核心是引入‘设计逻辑’概念，模仿人类教师命题流程。具体做法是：用大模型逆向工程提取并抽象出12万余条不同学科现有题目的设计逻辑，然后将这些逻辑与丰富的学科原始材料（如书籍/网页语料）匹配，自动生成具有高度难度和多样性的推理题目。最终合成了两个大规模数据集：DLR-Book（304万题，来自书籍语料）和DLR-Web（166万题，来自网页语料），覆盖75个学科。

Result: 通过与现有基线数据集对比分析，新合成数据集在难度和多样性方面均大幅领先。基于Qwen3-8B/4B-Base模型的指令微调实验证明：新数据集训练下的模型在多学科推理任务上远超同体量的公开数据集，并且在整体推理能力上优于官方Qwen3-8B和Qwen3-4B模型。

Conclusion: DESIGNER数据合成管道能自动生成更难、更有广度的多学科推理题目，为大模型推理能力训练与评估带来更强动力。基于该方法合成的DLR-Book和DLR-Web数据集，显著提升了主流大模型的多学科推理水准，成为推动AI推理研究的有力工具。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [209] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: 本文提出并公开了LinguaSafe，这是一个覆盖12种语言、包含4.5万条数据的多语言大模型安全测试数据集，并展示了不同语言和领域间安全性评估结果可能存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型（LLM）的多语言安全性评测数据和评价不足，特别是在各种语言和文化情境下，缺乏全面性、多样性的对齐基准，严重制约了多语言模型的安全对齐发展。

Method: 作者构建了LinguaSafe多语言安全基准，采用翻译、创译及本地原创数据三种方式筛选获取数据，覆盖从匈牙利语到马来语等12种语言。该基准框架具备多维度细粒度评价能力，支持直、间接安全性检测及过度敏感性测试。

Result: 基于LinguaSafe的评测显示，不同语言及细分领域的模型安全性显著不同，即使是资源相近的语言也表现不同。该基准为深入多语言安全性评估提供了全面的指标体系。

Conclusion: 论文强调对LLM多语言安全性的系统、全面评估对于实现安全对齐至关重要，LinguaSafe的数据集和代码已开源，旨在推动多语言大模型安全领域的进一步研究。

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [210] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL通过集群检索和执行描述语言（EDL）两步，将自然语言问题更有效地转化为SQL查询，明显提升了大型数据库中的Text-to-SQL任务表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型提升了Text-to-SQL的准确性，但自然语言与SQL语义不匹配的问题依然严重，尤其是在大型数据库中，属性语义相似导致模型难以正确关联表结构并生成准确SQL，亟需更有效的解决方案。

Method: CRED-SQL包括两大步骤：首先利用集群检索对大型数据库的schema进行筛选，找出与自然语言问题最相关的表和字段；其次创新性地引入执行描述语言EDL，作为NLQ与SQL间的中间自然语言层，把转换过程拆解为Text-to-EDL和EDL-to-SQL两步，从而减少语义偏移并利用LLM的推理能力。

Result: 在SpiderUnion和BirdUnion两个大规模跨领域基准集上，CRED-SQL表现出新的SOTA（最优）性能，验证了其有效性和可扩展性。

Conclusion: CRED-SQL显著缓解了Text-to-SQL中的语义不匹配和schema关联难题，在大规模数据库环境下有效提升了自动SQL生成的准确性和通用性。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [211] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: 本文提出了SALAMANDRATA系列模型，属于跨38种欧洲语言的翻译任务优化大模型，并介绍了其训练方法、在WMT25中的应用及公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在多语言翻译（尤其是欧洲语言）上的性能还有改进空间，需要专门设计和训练以取得更好的翻译表现。

Method: SALAMANDRATA模型包含2B和7B两个版本，训练流程包括：1) 基于平行语料的持续预训练，2) 利用高质量指令的监督微调。在参赛WMT25时，针对挑战中新增的非欧洲语言扩展模型词表，并重新进行持续预训练和微调。解码阶段采用了两种基于质量的技术：最小贝叶斯风险解码和基于COMET/COMET-KIWI的重排序。

Result: SALAMANDRATA（特别是7B版本）被用作BSC团队参加WMT25翻译任务的主力模型，优化了所有翻译方向的表现，并成功适应了新增的非欧洲语言需求。

Conclusion: SALAMANDRATA模型在多语言翻译任务上展示了优异性能，已在Hugging Face上公开发布（包括最新版V2），对多语言NLP社区具有应用和研究价值。

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [212] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 该论文提出HeteroRAG框架，通过融合异质知识源大幅提升医疗大规模视觉-语言模型（Med-LVLM）的事实准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大视觉-语言模型在临床应用中表现出色，但存在事实错误和输出不可靠的问题，影响实际诊断的安全性。传统的检索增强生成（RAG）技术难以高效利用多个异质（多模态、多来源）知识库，因而限制了模型的知识广度与深度。该研究旨在解决多模态、多知识库高效检索与整合的问题，从而提升Med-LVLM的临床表现。

Method: 作者构建了包含大量多模态报告和丰富文本语料的MedAtlas知识库，提出HeteroRAG框架。具体方法包括：1）设计Modality-specific CLIPs提升跨模态报告检索效果；2）引入Multi-corpora Query Generator动态为不同语料库生成检索查询；3）通过异质知识偏好调优，对Med-LVLM进行训练，实现跨模态、多源知识的融合与对齐。

Result: 在12个数据集、3种模态上进行的大量实验表明，HeteroRAG在绝大多数医疗视觉语言任务中取得了当前最优表现，显著提升了模型的事实准确性与可靠性。

Conclusion: HeteroRAG有效解决了医疗多模态大型模型跨知识库检索与知识整合的核心难题，使Med-LVLM的输出更加准确和可靠，为实际临床应用奠定了更坚实的基础。

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [213] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: 本文提出了Atomic Thought和Atom-Searcher框架，通过细粒度思维单元和奖励机制提升大语言模型在复杂推理任务中的表现，实现了更高效、更可解释的多步推理与信息整合。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽然具备强大的问题解决能力，但由于内在知识静态，处理复杂任务受限。RAG虽可扩展外部信息，但在多步推理和主动检索上仍有不足。当前基于结果的强化学习方法存在奖励稀疏、梯度冲突等问题，进一步限制了性能与效率提升。

Method: 作者提出Atomic Thought范式，将推理分解为可监控的细粒度功能单元，并用RRMs为每步提供Atomic Thought奖励（ATR），结合课程式奖励策略，提出新的RL算法Atom-Searcher，实现推理路径快速收敛，并逐步由过程奖励过渡到结果奖励。

Result: 在七个基准任务上，Atom-Searcher在性能和解释性上均超过现有方法，推理过程更接近人类思维，推理路径快速收敛。测试时可灵活扩展算力以应对更高复杂度问题。

Conclusion: Atomic Thought与Atom-Searcher框架有效提升了LLM在复杂推理与深度研究任务中的表现，解决了现有RL方法的奖励稀疏等核心问题，实现了更高效、更人性化的推理过程，为大模型智能体的发展提供了新范式。

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [214] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: 本文挑战了让低资源语言与高资源标准语言（如阿拉伯语标准语）对齐可以提升建模表现的常规假设，发现过度对齐实际上会阻碍生成式建模能力。作者提出了一种新的在线变分探测方法，有效减少了标准语对方言生成能力的干扰，对25种阿拉伯方言的实验表明生成质量显著提升。


<details>
  <summary>Details</summary>
Motivation: 许多多语言或方言NLP任务中，高资源标准语言与低资源相关方言被假定为共享表示，认为对齐有利于提升低资源方言的表现。作者怀疑这种过度表示融合会不利于低资源方言的生成建模，因此希望通过实验证明并缓解该问题。

Method: 作者首次综合性地用因果分析方法研究标准语与方言之间的表示纠缠现象。他们提出了在线变分探测框架，在微调过程中持续估计标准语子空间，并通过投影实现方言表征与标准语的主动解耦。主实验以阿拉伯语25种方言为例，基于平行语料验证方法有效性。

Result: 对25种阿拉伯方言的生成建模实验表明，与传统微调比，采用子空间解耦方法可提升chrF++分数最高+4.9、平均+2.0。在生成能力提升的同时，标准语表现略有折中。实验提供了高资源标准语子空间主导会限制相关方言生成能力的因果证据。

Conclusion: 本文证实了高资源标准语在多语言模型中可能主导表征，损害低资源方言的生成能力。所提出的子空间层面干预方法能有效改善该问题，对多语/多域/多方言建模具有广泛意义，并提供了新的模型表示分配控制框架。相关代码将会开源。

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [215] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: 本文旨在构建一个法语语义语料库，通过对法语对话文本进行AMR语义标注，并对AMR框架进行了扩展以适应自发口语和法语特有结构。作者公开了语料资源，并训练了相关解析器。


<details>
  <summary>Details</summary>
Motivation: 法语对话尤其是自发口语语料在语义层面的资源稀缺，现有AMR框架无法充分覆盖口语动态及法语特性，因此需要扩展和规范化标注体系，推动法语对话语言研究。

Method: 1）选择法语自发对话语料（DinG语料库，来自桌游Catan的对话）；2）对语料进行AMR语义标注，并扩展AMR以适应法语自发口语特征；3）制定细致的标注指南，保证标注一致性；4）公开语料，并基于该数据训练并评估AMR解析器模型，用于辅助后续人工标注。

Result: 构建完成带有AMR标注的法语对话语料库，提出并实施了针对口语特性的AMR扩展，编写了详细指南，并开源所有资源；训练的AMR解析器模型在该语料库上获得评估结果，可辅助自动标注。

Conclusion: 该研究为法语对话构建了新的高质量语义资源，丰富了法语对话AMR语料，为后续法语语义分析和相关工具的开发提供了数据和方法支持。

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [216] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 这篇论文研究了如何利用社交媒体对话上下文（如父推文）改善辱骂性语言检测的准确性。结果显示，结合父推文的内容特征，显著优于只关注回复内容。


<details>
  <summary>Details</summary>
Motivation: 现有辱骂性言论检测系统多只分析单个社交媒体帖子，忽略了对话上下文，比如相关的父推文。本研究动机是利用对话结构提升检测性能，解决实际应用中的不足。

Method: 作者构建了一个包含父推—回复（parent-reply）对的推文数据集，其中回复已标注是否为辱骂性言论。设计并对比了基于内容和基于账户的多种特征，仅用回复特征与融合父推文上下文特征分别训练四种分类模型，进行实验对比。

Result: 实验表明，结合父推文的上下文特征能大幅提升模型表现。内容型特征（即推文内容本身）比账户特征（发帖者信息）对分类提升贡献更大。多种内容特征协同使用效果最好。

Conclusion: 充分利用对话上下文，特别是内容特征，是提升辱骂性言论检测的有效途径。论文为社交媒体实际对话场景中的有害内容检测模型设计提供了参考。

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [217] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: 本文旨在利用风格计量学（stylometry）等计算方法，对中世纪大学的口头讲授记录整理出的文献进行作者归属分析，以验证编辑层次和文献形成假说。


<details>
  <summary>Details</summary>
Motivation: 现有间接证据表明，早期经院时期已有基于口头教学记录的文学创作（reportationes），但关于这一实践的直接评论文献极少，学界对其编辑与成型过程认识有限。作者旨在通过计算方法揭示文献形成的细节，推进对经院传统文献生成机制的理解。

Method: 以Stephen Langton的《神学问题集》（Quaestiones Theologiae）为例，设计实验，应用手写文本识别（HTR）和风格计量学方法。具体分析词频、词性标注和伪词缀等语言特征，比较手工与自动数据的效果，并测试transformer模型驱动的OCR与自动转录对拉丁语文献的适用性。

Result: 目前主要是研究设想和实验设计，还未给出具体结果。预期结果包括对编辑层次的新发现以及方法的可行性验证。

Conclusion: 如果实验成功，将为经院拉丁文献的协作性文本分析提供一种可复用范式，并对计算人文方法在该领域的进一步推广带来理论与工具层面的推动。

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [218] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: 该论文研究了transformer语言模型中词语意义的表示机制，发现其词嵌入空间包含丰富的语义信息，反驳了“意义消解论”假设。


<details>
  <summary>Details</summary>
Motivation: 当前对transformer大模型中词语含义的内部表示机制尚不明确，尤其是其是否类似于心理语言学中的词汇存储单元。为理解模型是否以单独的语义词条形式存储词义，作者展开本研究。

Method: 作者提取了RoBERTa-base模型的token嵌入空间，并用k-means算法将其聚类为200类。其后，手动检查各类以观察是否反映语义信息，并进一步测量聚类与心理语言学五项指标的相关性：效价（valence）、具体性（concreteness）、象征性（iconicity）、禁忌性（taboo）及习得年龄（age of acquisition）。

Result: 聚类结果表现出对语义信息高度敏感；且与心理语言学五项指标均有相关性，说明token嵌入空间编码了丰富且多样的语义信息。

Conclusion: transformer模型的词向量嵌入空间确实能够承载丰富的语义知识，因此否定了部分认为transformer模型无法表示词语意义的理论。

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [219] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）的智能体方法，用于提升复杂表格的语义标注任务（STA）准确性，并在效率和成本上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂表格在进行列类型注释（CTA）和单元格实体注释（CEA）时由于语义丢失、同形异义词、拼写错误、缩写及本体层次要求等问题，导致现有语义标注准确率低。因此，需要更加智能且高效的注释方法。

Method: 作者基于ReAct框架，设计并实现了五种外部工具结合专属提示，赋能LLM智能体根据表格特征动态选择合适的注释策略，并通过引入Levenshtein距离减少冗余注释。

Result: 新方法在SemTab挑战赛的Tough Tables和BiodivTab数据集上，多个指标均超越了现有方法。同时，利用Levenshtein距离去除冗余注释，实现了标注时间成本降低70%、LLM Token用量降低60%。

Conclusion: 基于LLM的STA智能体方法在复杂场景下具备更高的准确性和显著的效率提升，为大规模自动化语义表格标注提供了可行且经济的解决方案。

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [220] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种新型的自我优化方法——ProActive Self-Refinement (PASR)，能够使大语言模型在生成过程中动态优化输出，提升了准确率并减少了计算消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的自我优化方法多为被动式，并采用固定迭代次数，难以根据上下文变化动态优化，不能充分发挥自我优化的潜力。作者受到人类思考过程中动态修正思想的启发，提出更主动的自我优化方法。

Method: PASR方法让模型能够在生成过程中，主动根据内部状态和当前上下文判断是否需要优化输出、在何时以及如何优化，而不是简单地整体重生成，而是细粒度地选择性优化。

Result: 在十个不同任务上广泛实验，PASR在Qwen3-8B模型上将平均token消耗降低了41.6%，同时准确率提高了8.2%。

Conclusion: PASR方法显著提升了大语言模型的输出质量及效率，具有推广价值。相关代码与基线方法已开源。

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [221] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: 本研究开发了一种基于大语言模型（LLM）的多智能体系统（MAS），用于解决多约束、长时序的旅行规划任务，通过引入笔记本和协调者提升任务完成率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体系统在处理长时序、有复杂约束的任务时，面临细节依赖和多约束协调的难题，亟需有效的信息共享和智能体协作机制。本文以旅行规划为代表性任务，探索解决方案。

Method: 作者构建了一个基于LLM的MAS，设计了两个关键机制：一是笔记本，用于智能体间信息共享；二是协调者，在自由对话中指导协调以提升系统一致性。

Result: 实验结果显示，笔记本机制使幻觉细节导致的错误减少18%，协调者在子任务内进一步减少错误13.5%。两者结合后，在TravelPlanner基准上通过率达到25%，比单智能体7.5%基线提升了17.5%。

Conclusion: 结构化信息共享与反思式编排机制对于提升LLM多智能体系统在长时序规划任务中的表现具有重要作用。

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [222] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: 本文提出了WebMall，一个用于评估基于大模型的网页代理的多店在线购物基准，涵盖91种跨店购物任务，用以推进智能体在复杂电商场景下的有效性与效率研究。


<details>
  <summary>Details</summary>
Motivation: 现有的基准如WebShop和ShoppingBench只能涵盖单一商店或有限的购物场景，无法很好地反映实际生活中用户在多个线上商店比价和购物的复杂流程。作者希望通过引入更加真实、多样化且涉及跨店比较的任务，推动大模型网站代理在电商领域的应用和发展。

Method: 作者设计了WebMall，包含了四家模拟网上商店，这些商店的数据来自于Common Crawl中真实网店的商品信息。基准涵盖91项跨店任务（如商品检索、比价、加购物车及结算），既有基础购物操作也包括模糊条件检索、替代商品匹配、兼容商品查找等高级任务。作者用不同体系的八种基线代理在WebMall上进行性能测试。

Result: 在WebMall上，最佳基线代理在基础任务（如检索、比价等）上的完成率为75%、F1分数为87%；在高级任务（如模糊检索、推荐等）上的完成率为53%、F1分数为63%。

Conclusion: WebMall作为多商店、异构商品电商基准填补了现有评测工具不足，推动了网页代理在电商导航、推理和效率等实际复杂任务中的发展。数据及基准已公开，有望促进相关研究。

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [223] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文提出一种新方法，提高合成语音中表现讽刺语气的能力。方法结合了来自双模态讽刺检测模型的反馈损失，并通过两阶段迁移学习提升讽刺语音合成效果。实验表明该方法在自然度和讽刺感知上都取得了提升。


<details>
  <summary>Details</summary>
Motivation: 讽刺语音能增强娱乐和人机交互的自然性，但其特有的细致韵律与带标签数据的稀缺，使得讽刺语音合成具有较大挑战性。本文旨在有效捕捉和表达讽刺语气，以提升虚拟语音在实际应用中的表现。

Method: 1）将双模态讽刺检测模型的反馈损失集成到TTS训练中，促进模型捕捉并表达讽刺特征。2）利用迁移学习，首先在多风格语音数据集上微调预训练模型，再用专注于讽刺语音的小数据集进行精调，从而增强讽刺语音的表达能力。

Result: 客观与主观实验均显示，提出的方法显著提升了合成语音在质量、自然度以及讽刺感知方面的表现，优于传统TTS方法。

Conclusion: 结合反馈损失和迁移学习的两阶段微调方法，可有效提升讽刺语音合成的质感和讽刺表达能力，对自然人机交互等领域具有积极意义。

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [224] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法LoRID，通过模拟人类心理学的双系统思维，有效提升小型语言模型（SLM）在数学推理任务中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在数学推理上表现出色，但由于参数巨大，部署和训练成本过高。相比之下，小型语言模型（SLM）推理能力较差，现有提升方法依赖大量由LLM生成的数据，效率和泛化效果有限。作者希望找到针对SLM更高效、具心理学启发的新训练范式。

Method: 受心理学的System 1（直觉思维）和System 2（深度思维）启发，作者提出多LoRA交互的推理蒸馏方法（LoRID）：首先用LLM生成知识增强数据集；然后训练SLM的LoRA模块，分为Intuitive Reasoner (IR) 负责直觉推理、Knowledge Generator (KG) 负责抽取知识、Deep Reasoner (DR) 负责利用知识深度推理；最后互相检验输出一致性，不一致时迭代推理，提升推理准确性。

Result: 实验表明，LoRID方法在数学推理任务（特别是GSM8K数据集）取得了新的最佳表现，在五个基线模型上的准确率分别超越第二优秀方法2.3%、16.1%、2.4%、12.3%和1.8%。

Conclusion: LoRID方法通过模拟人类双系统逻辑思维，有效提升了SLM在数学推理能力，证明了该思路的有效性和可扩展性，为小模型推理能力提升提供了新的方向。

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [225] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: 本文提出了TR-MMLU，这是专为评估大语言模型在土耳其语理解和生成能力而设计的基准测试集，并用其对现有主流模型进行了评测。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在多种自然语言任务上表现优异，但对资源有限语言（如土耳其语）的评测不足，缺乏系统的评估工具，限制了这些模型在相关领域的发展与优化。

Method: 研究者构建了TR-MMLU基准，包含6200道多项选择题，覆盖土耳其教育体系中的62个领域，用以全面考察大语言模型的土耳其语语言和概念能力。随后，使用该基准对多种先进大语言模型进行了实证评估。

Result: 评测结果展现了主流大语言模型在处理土耳其语文本方面的能力，同时揭示了模型在土耳其语理解、知识掌握等方面的不足之处。

Conclusion: TR-MMLU为土耳其语NLP研究提供了标准化评测工具，推动了相关领域的研究进步，并为后续改进模型和开展创新研究提供了重要参考。

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [226] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: 本文提出了一个新的评价框架，用于分析和优化土耳其语等形态复杂、资源稀缺语言的分词方法，从多维度量化分词性能并强调分词在下游自然语言处理任务中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有的分词方法大多围绕英语等高资源语言设计，忽视了形态丰富和资源稀缺语言（如土耳其语）的特殊性，导致大语言模型在处理这类语言时效果受限。为此，作者希望提出一套标准化且能反映语言特点的分词评估体系。

Method: 作者基于土耳其教育系统的TR-MMLU（6,200道选择题）数据集，从词表规模、分词后token数量、处理时间、语言专属token比例（%TR）和token纯度（%Pure）五个维度定量评估各分词工具，并通过下游任务（MMLU分数）相关性分析各指标。

Result: 实验证明：语言专属token比例（%TR）与下游NLP表现的相关性高于token纯度；仅依靠扩大模型规模，并不会显著提升语言表现能力，分词适配性更为关键。

Conclusion: 论文提出了一套面向形态复杂、资源稀缺语言的分词评测新框架和新标准，对推动此类语言的NLP技术发展和应用具有重要指导意义。

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [227] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED数据库收集了大量带有系统注释的英语口语错误，可用于测试和评估自动语音识别（ASR）模型，通过对WhisperX模型在5300个真实口语错误样本上的表现分析，证明了该数据库对ASR诊断的价值。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别系统在现实、自然对话中的表现还存在挑战，特别是在处理真实的言语错误时缺乏有效的公开数据集。针对这一不足，作者希望提供一个具有丰富注释和分类体系的口语错误数据库，以便深入评估和改进ASR模型的弱点。

Method: 作者整理并公开了SFUSED数据库，内容为英语自发语音中的口语错误，并按照多个语言学维度（如层级、上下文敏感性、词语降级、纠错等）进行了详细注释。作者进一步用WhisperX ASR模型在数据库中的5300个带注释的口语错误样本上进行评测，分析ASR系统在处理不同类型错误时的表现。

Result: 评测显示，SFUSED数据库能够有效区分并诊断ASR系统在不同层面的表现，尤其是在处理真实口语错误时。通过对WhisperX的测试，表明注释维度对模型的性能评估有重要作用。

Conclusion: SFUSED数据库为研究和改进语音识别系统提供了有力工具，其系统化的注释不仅促使更精细的模型诊断，也能促进语言学、心理语言学及ASR研究领域的发展。

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [228] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: 现有的因果语言模型和离散扩散模型大多采用固定或随机的生成顺序，但这可能不符合原始的逻辑顺序。本文提出了一种基于强化学习的自适应生成顺序方法ReCOR，实现了推理和规划任务上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前主流模型在需要自适应生成顺序才能高效解决的问题上表现不佳。生成顺序与实际逻辑顺序不符，限制了模型的性能。

Method: 提出ReCOR（Reinforced Context Order Recovery），利用无监督方式，通过强化学习根据预测难度自适应地选择下一个待生成token，实现顺序的自适应。该方法无需标注信息，依赖自监督的token预测统计信息。

Result: 在推理和规划等复杂数据集上，ReCOR在多数指标上优于现有基线模型，在部分场景下甚至超过了使用真实顺序的oracle模型。

Conclusion: 自适应、数据依赖的token生成顺序能有效提升模型性能，ReCOR为解决相关生成任务提供了一种新方案，具有较强的实用性和泛化潜力。

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [229] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文提出了DocHPLT，这是迄今为止最大规模的公开文档级机器翻译数据集，包含50种语言与英语配对的1.24亿对文档，以推动多语种、长文本机器翻译研究。DocHPLT中还保留了文档的完整性，并显著提升了大模型在低资源语言上的翻译表现。数据集已开源，便于学术和工业界使用。


<details>
  <summary>Details</summary>
Motivation: 现有文档级机器翻译数据集语言数量有限，且主要面向高资源语言，严重制约了多语种、长上下文机器翻译模型的研究和应用。

Method: 改造了现有网页数据抓取流程，在保持原始文档完整性的同时生成多语种文档配对，从而构建了涵盖50种语言、1.24亿对文档（42.6亿句子）的数据集。通过实验探索了最佳训练上下文策略，对大模型进行微调，并与主流基线进行对比。

Result: 用DocHPLT微调的大语言模型，在文档级翻译任务上明显优于未经微调的指令调优基线模型，尤其在低资源语言上的提升更为显著。

Conclusion: DocHPLT数据集的发布为多语种文档级机器翻译提供了重要资源和基准，有助于推动全球范围内多语言、长上下文模型的发展。

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [230] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一套端到端的RAG（检索增强生成）流程，专为法律领域设计，对现有基线进行了三方面增强：上下文感知的查询翻译器、开源的高效检索策略与综合性评测体系，实现了更高检索与生成质量，可与闭源系统媲美。


<details>
  <summary>Details</summary>
Motivation: 法律领域对生成内容的真实性和可追溯性有极高需求，传统大模型存在幻觉问题，需通过引用真实法律依据加以缓解。因此，提升RAG在法律场景下的表现具有重要价值。

Method: 针对原有LegalBenchRAG基线，进行了三方面提升：（1）构建基于上下文的查询翻译器，实现文献引用与自然问题的分离，并根据问题专业度和具体性自适应检索深度与回复风格；（2）基于SBERT和GTE嵌入的开源检索策略，大幅提升检索性能，保障成本效益；（3）联合多种评测指标（RAGAS、BERTScore-F1、ROUGE-Recall）建立全面的评测与生成对齐体系。

Result: 新流程在检索质量上大幅超越基线，召回率提升30-95%，准确率提升约2.5倍（K>4），开源系统可媲美甚至优于闭源系统。定制法律指向型Prompt较通用Prompt产出更为真实、契合情境的答案。

Conclusion: 任务感知与组件级调优能够为法律研究提供真实、可复现、成本可控的RAG解决方案，有望推动法律智能助手的实际落地和应用。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [231] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: 本文提出了AutoBnB-RAG系统，将检索增强生成（RAG）集成到多智能体网络安全事件响应模拟中。结果表明，这种方法能提升决策质量，更好地重建复杂攻击，适用于多种组织结构。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在自动化事件响应中表现出潜力，但其推理常受限于缺乏外部知识，致使决策效果受影响。作者旨在弥补该短板，提升多智能体事件响应的智能决策能力。

Method: 该研究基于Backdoors & Breaches桌面推演环境，提出AutoBnB-RAG，允许智能体发起检索请求并融合外部证据。研究设置了两种检索场景（基于技术文档与叙述式事件报告），并在八种团队结构下对系统进行评估，包括新引入的具备争论机制的团队。通过模拟现实网络攻击事件来验证其实用性。

Result: 实验结果显示，检索增强可以提升多智能体系统的决策质量和响应成功率，能够准确还原复杂、多阶段的攻击过程，并在不同组织模型下均表现出显著提升。

Conclusion: 集成检索机制能有效提升LLM多智能体在网络安全决策中的表现，为真实环境下的事件响应提供了更有力的智能辅助手段。

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [232] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种名为BlindSpot的框架，用于检测和量化大型语言模型在呼叫中心对话摘要生成过程中的操作性偏差。研究发现，现有各种主流LLM在此场景下均存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在呼叫中心生成对话摘要的应用极为广泛，但目前尚不清楚模型在生成摘要时是否系统性强化或忽略某些内容，导致特定操作性偏差，这关系到生产应用的公正性与准确性。因此需要一种系统方法来识别和量化这种操作性偏差。

Method: 作者提出了基于15个操作性偏差维度的分类法，并开发了BlindSpot框架，利用LLM实现零样本分类，对每个话单及其摘要在各维度上生成类别分布。通过Fidelity Gap（JS散度）和Coverage（标签遗漏率）两个指标，量化偏差强度。随后在2500份真实通话数据及20种不同LLM生成的摘要上进行了实证分析。

Result: 实验结果显示，无论模型的规模和家族（包括GPT、Llama、Claude等），所有主流LLM均表现出系统性的操作性偏差。

Conclusion: LLM在呼叫中心对话摘要任务中普遍存在明显的操作性偏差，BlindSpot框架可以有效识别和量化这些偏差，为实际部署和模型优化提供了必要工具。

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [233] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文提出了阿拉伯语多方言常识验证数据集MuDRiC，并基于图卷积网络（GCN）的方法，有效提升了阿拉伯语常识验证任务的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语常识验证领域尚不发达，尤其是多方言的资源严重不足，而日常交流中各类方言广泛存在，现有研究主要聚焦在现代标准阿拉伯语（MSA），方言的常识验证能力成为制约阿拉伯语自然语言理解发展的瓶颈。

Method: 本文构建了包含多种阿拉伯语方言的常识数据集（MuDRiC），并创新性地将图卷积网络（GCN）应用于阿拉伯语常识推理，通过强化语义关系建模提升验证性能。

Result: 实验证明，该方法在阿拉伯语常识验证任务上表现优越，超越了现有方法。

Conclusion: 本文首次发布阿拉伯语多方言常识推理数据集，并提出新的常识验证方法，为处理阿拉伯语复杂变体和提升自然语言理解能力提供了坚实基础。

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [234] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: 论文提出将水印检测器与非水印检测器结合起来，用于提升大语言模型生成内容检测的性能，并验证了混合检测方案优于单一检测方法。


<details>
  <summary>Details</summary>
Motivation: 单独依赖水印技术进行LLM生成内容的检测存在局限性，尤其是在部分微调后模型（如RLHF后）熵值降低、水印嵌入能力下降的场景下，检测难度增大。为解决这一问题，需要寻找更鲁棒的检测方式。

Method: 作者研究了将水印检测器和非水印检测器（如统计方法、机器学习模型等）进行组合的混合检测方案，设计并评估了多种混合策略，比较了不同策略在多种实验条件下的检测性能表现。

Result: 实验表明，混合检测方案在多种实验条件下相较于单独使用水印或非水印检测器均表现出更优的检测性能，提升了对LLM生成内容的检测准确率和鲁棒性。

Conclusion: 将水印检测与非水印检测相结合是提升LLM生成内容检测有效性的一种可行途径，为提升生成内容溯源及真实性验证提供了新的解决思路。

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [235] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 该论文提出了一种新的评测基准OptimalThinkingBench，用于共同评估大语言模型（LLM）的过度思考和思考不足问题，并推动模型在性能与效率间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有思考型LLM在复杂任务上表现优越但计算成本高且在简单任务上容易过度思考，非思考型LLM虽然高效但难以应对复杂推理任务。这导致用户需自行在不同模型间选择，缺乏统一评测与指导。

Method: 作者设计了OptimalThinkingBench，包括两个子基准：OverthinkingBench（覆盖72个领域的简单查询）和UnderthinkingBench（包含11个难推理任务）。引入了新的思考调整准确率指标，对33种思考型和非思考型模型进行了全面评估，并尝试了多种促进最优思考的方法。

Result: 评测结果表明，目前无模型能在所有任务上实现最优思考。思考型模型在简单任务上往往严重过度思考却无性能提升，非思考型大型模型则易于思考不足甚至不如较小的思考型模型。优化方法常在提升一类任务表现的同时牺牲另一类。

Conclusion: 当前的LLM尚无法兼顾性能与效率，未来亟需开发能在多任务场景下实现思考最优平衡的统一模型。

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [236] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: 本文分析了当前大模型评测基准存在信号与噪声等问题，并提出提升评测可靠性的干预方法，实证表明优化信噪比显著提升评测结果，最终发布了大规模公开评测数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型研发成本高昂，模型优劣的决策依赖于多任务评测基准，但当前基准的可靠性有所欠缺。因此，提升评测基准可靠性对于模型优化和资源有效利用至关重要。

Method: 作者系统分析了30个评测基准，并在375个参数规模从60M到32B的大语言模型上进行测试，总计产生90万组评测结果。提出'信号'（区分好坏模型的能力）和'噪声'（对训练步变化的敏感性）两个评价指标，并通过更优评测方式（如采用困惑度代替准确率、过滤噪声任务、利用中间checkpoint输出平均），验证提升信噪比的干预方法。

Result: 高信噪比的基准能更好区分优劣模型、预测模型放大时的误差更小。提出的三项干预措施（改用更优指标、任务筛选、结果平均）均有效提升了评测可靠性。

Conclusion: 高信号、低噪声的评测基准更适合用于大模型开发与选型，未来设计/选择基准时应优先考虑信噪比。本工作公开了大规模评测数据集，供学术与工业界参考。

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [237] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文提出了一种新方法——RepreGuard，能更有效、更鲁棒地检测大语言模型生成内容，在ID和OOD场景下都优于现有检测方法。


<details>
  <summary>Details</summary>
Motivation: 生成式大语言模型（LLM）内容检测对于防止滥用、构建可信AI至关重要。现有方法在分布外（OOD）场景下鲁棒性不足，因此需要探索更全面和本质的区分特征。

Method: 作者假设LLM的内部表征比传统特征更好地区分LLM生成文本（LGT）和人类文本（HWT），通过分析不同LLM对两类文本的神经激活模式提出RepreGuard方法。利用代理模型提取文本表征，计算文本在显著区分特征方向上的投影分数，并与阈值比较实现分类。

Result: RepreGuard在ID和OOD场景下，相较所有基线方法取得平均94.92% AUROC，且在不同文本长度与主流攻击下表现出较强鲁棒性。

Conclusion: 实验表明，基于LLM内部表征的激活特征能更好地区分AI生成内容与人写内容，RepreGuard方法有效且鲁棒，对推进内容检测领域有重要意义。

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [238] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文展望了未来自主机器人能够与人类协作并以自然语言交流的愿景，提出利用大语言模型（LLMs）提升机器人理解力，并探索机器人、LLM与人类协作的方案。通过实验展示其可行性，并讨论实现集成系统的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有交互式任务学习（ITL）系统在语言理解方面能力有限，不能很好地与人类进行自然语言交流。大语言模型的出现为提升机器人的语言理解带来新机遇，但将LLM集成到现实物理环境下的机器人中仍存在诸多挑战。

Method: 首先回顾了目前市场上一些与人类密切协作的商用机器人，并分析其语言能力的不足。随后，提出一种以认知智能体为核心、结合物理机器人、LLM与人类互动，并通过经验积累情境知识的AI系统方案。重点关注三个自然语言理解相关挑战，并针对每一个挑战设计了使用ChatGPT的概念验证实验。

Result: 实验表明，借助LLM（如ChatGPT）可以在一定程度上提升机器人对自然语言的理解能力，实现了初步的场景验证。

Conclusion: 将LLM辅助的语言理解系统与机器人深度集成有望显著提升人机协作水平，但要实现实际可用的综合机器人助手系统仍需进一步技术研发和集成。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [239] [Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots](https://arxiv.org/abs/2508.11802)
*Luigi Penco,Beomyeong Park,Stefan Fasano,Nehar Poddar,Stephen McCrory,Nicholas Kitchel,Tomasz Bialek,Dexton Anderson,Duncan Calvert,Robert Griffin*

Main category: cs.RO

TL;DR: 本文提出了一种能够在机器人远程操作中，将用户的步伐实时转移给机器人，同时兼顾高效性与稳定性的同步步态传递方法。


<details>
  <summary>Details</summary>
Motivation: 用户与机器人在远程操作中进行高速任务时难以实现同步，特别是在步态操作下，传统方法难以兼顾效率、稳定性与对环境的适应。

Method: 创新性地不是直接复制用户的足部姿态，而是将用户的步伐重新映射为机器人的足步落点，结合机器人自身动态，提升其行走过程中的平衡性和稳定性。方法支持对用户步伐的预测以减少延迟，并可以根据外部环境（如地面不平）自动调整步幅。

Result: 实验在类人机器人Nadia上进行了验证，结果证明该系统能够有效提升远程操作中的机器人步态同步与环境适应性。

Conclusion: 该方法为远程控制场景下，实现高效、稳定且环境自适应的机器人步态传递提供了新的解决思路，具有良好的实际应用前景。

Abstract: Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.

</details>


### [240] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: LocoMamba是一种基于Mamba、能够高效建模长序列依赖、用于跨模态深度强化学习的视觉驱动框架，能有效提升机器人在复杂环境下的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前用于机器人视觉驱动决策的深度强化学习难以同时兼顾效率、对长距离依赖的建模能力、运算速度以及对不同模态（如视觉与状态）的高效整合。且长序列的训练往往面对严重的内存和速度瓶颈。本文致力于提升在复杂环境下机器人的运动学习表现，并提高其泛化能力。

Method: 方法上，首先用多层感知机对本体状态嵌入、用轻量级卷积神经网络将深度图像变成紧凑Token，然后用堆叠的Mamba层融合这些Token，利用其近线性扫描方式，减少延迟和内存占用，提升对不同 token 数量和分辨率的鲁棒性，且有助于避免过拟合。最后用PPO算法、随机地形与外观、障碍物密度课表以及紧凑的状态中心奖励进行端到端训练。

Result: 实验在包含静止与移动障碍、复杂地形的仿真环境中评估了方法。与主流方法对比，LocoMamba在获得更高回报与成功率、碰撞更少、对新地形与不同障碍密度的泛化性更强、在同一计算预算下训练更快等方面表现突出。

Conclusion: LocoMamba显著增强了机器人在多变环境中的运动能力、泛化与效率，验证了Mamba等选择性空间状态模型在跨模态强化学习中的优势。

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [241] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 本文针对自动驾驶中的目标检测任务，系统分析和解决了因环境变化引发的数据分布漂移问题。通过结合数据漂移检测、数据集平衡和CycleGAN数据增强，提升了YOLOv5框架下检测模型的性能，并在BDD100K数据集上超过了基线模型。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶领域，复杂多变的环境导致训练数据与实际数据分布不一致（数据分布漂移），现有机器学习模型在这种情况下性能下降，亟需有效识别和缓解此类漂移，提升模型鲁棒性和泛化能力。

Method: 系统综述了数据漂移检测方法，通过数据漂移检测和分析对数据集进行分类和平衡，并构建目标检测模型。进一步将CycleGAN为基础的数据增强方法集成至YOLOv5模型中进行性能优化。

Result: 经实验验证，所提出方法在BDD100K数据集上取得了优于基线模型的表现，说明方法有效提升了检测性能。

Conclusion: 整合数据漂移检测、数据集平衡和基于CycleGAN的数据增强，有效缓解了自动驾驶目标检测中的数据分布漂移问题，提升了模型的鲁棒性和实际应用价值。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [242] [Bioinspired underwater soft robots: from biology to robotics and back](https://arxiv.org/abs/2508.11883)
*Lei Li,Boyang Qin,Wenzhuo Gao,Yanyu Li,Yiyuan Zhang,Bo Wang,Shihan Kong,Jian Wang,Dekui He,Junzhi Yu*

Main category: cs.RO

TL;DR: 本文探讨了以海洋生物为灵感的水下软体机器人，认为软体机器人不仅模仿生物，还可以反向为生物学研究提供工具，并提出了超越物种仿生的新范式。


<details>
  <summary>Details</summary>
Motivation: 海洋中未开发区域广阔，且软体生物多样，激发了以软体海洋生物为灵感开发水下机器人的兴趣。生物启发的软体机器人有助于提升水下运动、感知与环境交互能力。

Method: 提出了一个融合生物学原理、机器人实现和生物学验证的双向研究框架，主张让软体机器人用于探究生物功能、检验进化假说，并提出跨物种、结合趋同演化原理的“生物普适启发机器人”概念。

Result: 软体机器人因其柔顺性，在复杂环境下优于刚性系统，可用于海洋探索、操作和医疗；机器人也反哺生物学、可作为研究工具。

Conclusion: 尽管取得进展，但在材料、驱动效率、自主性和智能方面仍有挑战。生物与工程结合将推进海洋探索与科学发现。

Abstract: The ocean vast unexplored regions and diverse soft-bodied marine organisms
have spurred interest in bio-inspired underwater soft robotics. Recent advances
have enabled new capabilities in underwater movement, sensing, and interaction.
However, these efforts are largely unidirectional, with biology guiding
robotics while insights from robotics rarely feed back into biology. Here we
propose a holistic, bidirectional framework that integrates biological
principles, robotic implementation, and biological validation. We show that
soft robots can serve as experimental tools to probe biological functions and
even test evolutionary hypotheses. Their inherent compliance also allows them
to outperform rigid systems in unstructured environments, supporting
applications in marine exploration, manipulation, and medicine. Looking
forward, we introduce bio-universal-inspired robotics, a paradigm that
transcends species-specific mimicry by identifying convergent principles across
species to inspire more adaptable designs. Despite rapid progress, challenges
persist in material robustness, actuation efficiency, autonomy, and
intelligence. By uniting biology and engineering, soft robots can advance ocean
exploration and deepen scientific discovery.

</details>


### [243] [From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics](https://arxiv.org/abs/2508.11884)
*Havel Liu,Mingzhang Zhu,Arturo Moises Flores Alvarez,Yuan Hung Lo,Conrad Ku,Federico Parres,Justin Quan,Colin Togashi,Aditya Navghare,Quanyou Wang,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文介绍了一款取名为Kid Cosmo的娱乐型仿人机器人，聚焦于角色化外形和流畅动作的结合，展示了机器人在娱乐行业的新潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数仿人机器人注重功能性，外观设计较为实用，缺乏娱乐领域所需的视觉冲击与角色表现力。因此，设计能够灵活运动并具有生动外观的娱乐仿人机器人，成为一个具有挑战性和创新性的方向。

Method: 设计并实现了一台具备角色外观及拟人动作表现力的仿人机器人Kid Cosmo。该机器人高度1.45米，重量25千克，拥有28自由度，采用本体感知驱动器实现力矩控制步行和流畅动作。系统架构和技术实现确保机器人能够进行上半身和下半身的协调复杂动作。

Result: Kid Cosmo在全球的电影宣传活动中公开展示，实现了真实的角色动作和流畅步行。通过实验，团队掌握了功能型娱乐机器人在运动稳定性和动作协调性上的关键挑战与初步解决方法。

Conclusion: 该研究展示了娱乐取向仿人机器人的可行性，证明了角色化外形与高水平技术性能的结合能够满足娱乐领域的特殊需求，并为未来仿人机器人在娱乐行业的应用奠定了基础。

Abstract: Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.

</details>


### [244] [Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System](https://arxiv.org/abs/2508.11885)
*Haixin Gong,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 本文提出了一种新的人脚接触丰富且可变形的肌肉骨骼模型，能更精确模拟步态，与人类实验数据接近，并优于传统刚性模型。


<details>
  <summary>Details</summary>
Motivation: 现有的人脚肌肉骨骼模型在地面接触部分过于简化，难以准确再现步态和脚部与地面的复杂力学作用。为了解决这一问题，需要开发更真实的脚与地面交互模型。

Method: 作者开发了一个全新的、能表达多点接触和变形的完整人脚肌肉骨骼系统模型。为了解决建模时控制多点接触和材料变形的挑战，提出了两阶段策略训练行走控制策略，并进行了与传统模型的对比分析和实验数据验证。

Result: 新模型在运动学、动力学和步态稳定性等多个指标上均优于传统刚性模型。并通过与真实人体步行实验数据对比，验证了该模型的准确性和现实性。

Conclusion: 本文推动了人脚肌肉骨骼系统中包含丰富接触的信息建模工作，为需要精细控制脚部与地面交互的人形机器人等应用奠定了坚实的基础。

Abstract: The human foot serves as the critical interface between the body and
environment during locomotion. Existing musculoskeletal models typically
oversimplify foot-ground contact mechanics, limiting their ability to
accurately simulate human gait dynamics. We developed a novel contact-rich and
deformable model of the human foot integrated within a complete musculoskeletal
system that captures the complex biomechanical interactions during walking. To
overcome the control challenges inherent in modeling multi-point contacts and
deformable material, we developed a two-stage policy training strategy to learn
natural walking patterns for this interface-enhanced model. Comparative
analysis between our approach and conventional rigid musculoskeletal models
demonstrated improvements in kinematic, kinetic, and gait stability metrics.
Validation against human subject data confirmed that our simulation closely
reproduced real-world biomechanical measurements. This work advances
contact-rich interface modeling for human musculoskeletal systems and
establishes a robust framework that can be extended to humanoid robotics
applications requiring precise foot-ground interaction control.

</details>


### [245] [Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards](https://arxiv.org/abs/2508.11887)
*Yousra Shleibik,Jordan Sinclair,Kerstin Haring*

Main category: cs.RO

TL;DR: 本文提出了一种结合实时凝视追踪、情境感知显著性分析和同步视觉及听觉提醒的概念框架，以增强半自动驾驶场景下驾驶员的情境感知和接管安全性。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统的发展，人类仍需在特定场景下参与决策，尤其是车辆无法独立识别环境元素时。因此，确保在车辆接管过程中驾驶员具备充分的情境感知能力对于安全尤为关键。

Method: 提出采用基于注视（凝视）操控、针对性视觉和听觉提示的注意力重定向方法。整个框架结合了实时视线追踪、场景显著性分析以及多模态（视觉+听觉）提醒，以促进驾驶员关注新出现的潜在危险，并减少目标固视现象。

Result: 尽管摘要未给出实验或实际结果，但概念框架旨在提升驾驶员在半自动驾驶中的风险预警能力和接管效率。

Conclusion: 该研究为半自动驾驶车辆中人机协作与安全接管流程提供了新方向，提出的技术有望显著提升驾驶安全性和效率。

Abstract: The advent of autonomous driving systems promises to transform transportation
by enhancing safety, efficiency, and comfort. As these technologies evolve
toward higher levels of autonomy, the need for integrated systems that
seamlessly support human involvement in decision-making becomes increasingly
critical. Certain scenarios necessitate human involvement, including those
where the vehicle is unable to identify an object or element in the scene, and
as such cannot take independent action. Therefore, situational awareness is
essential to mitigate potential risks during a takeover, where a driver must
assume control and autonomy from the vehicle. The need for driver attention is
important to avoid collisions with external agents and ensure a smooth
transition during takeover operations. This paper explores the integration of
attention redirection techniques, such as gaze manipulation through targeted
visual and auditory cues, to help drivers maintain focus on emerging hazards
and reduce target fixation in semi-autonomous driving scenarios. We propose a
conceptual framework that combines real-time gaze tracking, context-aware
saliency analysis, and synchronized visual and auditory alerts to enhance
situational awareness, proactively address potential hazards, and foster
effective collaboration between humans and autonomous systems.

</details>


### [246] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: 本论文提出了AMAD-SRL框架，将符号强化学习（SRL）与无人机自主任务规划融合，有效提升了动态环境下无人机的决策安全性和任务效率。实验结果表明其在仿真环境中可稳定运行，任务效率较传统方法提升约75%。


<details>
  <summary>Details</summary>
Motivation: 现有无人机自主系统多依赖规则基础的结构化推理，但遇到动态复杂的操作环境时适应性不足。符号强化学习（SRL）能够结合领域知识和约束，提升无人机自主任务的鲁棒性和智能决策水平，因此迫切需要将其与传统规划框架融合。

Method: 提出了AMAD-SRL框架，将AMAD认知多智能体架构与基于PDDL的符号强化学习结合，实现任务的动态规划与执行，并在与硬件一致的软件仿真环境（SIL）中进行验证。

Result: 实验在目标获取场景下验证了该系统，展现出BDI与SRL模块的稳定集成和切换。相比覆盖式基线，AMAD-SRL方案在任务路径距离上优化约75%。

Conclusion: AMAD-SRL为复杂无人机任务提供了可靠的规划与执行基础，具备良好的实用性和可扩展性，并为未来实际硬件验证与进一步优化指明了方向。

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [247] [OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation](https://arxiv.org/abs/2508.11898)
*Jilei Mao,Jiarui Guan,Yingjuan Tang,Qirui Hu,Zhihang Li,Junjie Yu,Yongjie Mao,Yunzhe Sun,Shuang Liu,Xiaozhu Ju*

Main category: cs.RO

TL;DR: 本文提出了一种新的多视角融合框架OmniD，通过将多视角图像合成为统一的鸟瞰图（BEV）表征，显著提升了视觉运动策略在不同环境下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动策略容易在训练集上过拟合（例如固定相机位置和背景），导致在分布外任务上表现差。此外，当前方法难以有效融合多视角信息生成有效的三维表征。

Method: 提出了Omni-Vision Diffusion Policy（OmniD）框架，该框架利用变形注意力机制的Omni-Feature Generator（OFG）对多视图图像进行特征抽象，去除视图特定噪声和干扰，将其统一为鸟瞰图表征用于下游任务。

Result: OmniD在三类实验中分别比最优基线提升了11%、17%和84%（分别对应分布内、分布外和小样本实验）。

Conclusion: OmniD显著提高了视觉运动策略的泛化能力和多视角信息融合能力，为实际机器人运动控制等任务提供了更鲁棒的解决方案。

Abstract: The visuomotor policy can easily overfit to its training datasets, such as
fixed camera positions and backgrounds. This overfitting makes the policy
perform well in the in-distribution scenarios but underperform in the
out-of-distribution generalization. Additionally, the existing methods also
have difficulty fusing multi-view information to generate an effective 3D
representation. To tackle these issues, we propose Omni-Vision Diffusion Policy
(OmniD), a multi-view fusion framework that synthesizes image observations into
a unified bird's-eye view (BEV) representation. We introduce a deformable
attention-based Omni-Feature Generator (OFG) to selectively abstract
task-relevant features while suppressing view-specific noise and background
distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the
best baseline model for in-distribution, out-of-distribution, and few-shot
experiments, respectively. Training code and simulation benchmark are
available: https://github.com/1mather/omnid.git

</details>


### [248] [Control of Legged Robots using Model Predictive Optimized Path Integral](https://arxiv.org/abs/2508.11917)
*Hossein Keshavarz,Alejandro Ramirez-Serrano,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出了一种结合MPPI、CE和CMA的新型采样式预测控制器MPOPI，用于提升足式机器人的运动能力，展现出更高采样效率和更优运动性能。


<details>
  <summary>Details</summary>
Motivation: 足式机器人虽然擅长应对复杂环境，但性能仍不如自然系统，因此需要提高其在真实场景中的自主运动与控制水平。近期采样式预测控制方法取得了良好成效，但仍有提升空间，尤其在采样效率和运动结果方面。

Method: 将模型预测路径积分（MPPI）与交叉熵（CE）策略和协方差矩阵自适应（CMA）方法相结合，形成MPOPI方法，用于实时全身运动生成。通过在四足机器人多个复杂仿真场景中进行实验验证。

Result: MPOPI方法兼具三种方法优点，展现出更高的采样效率；在相同样本数量下，足式机器人获得更优的运动能力。大量仿真结果表明MPOPI作为anytime控制策略，在每次迭代下运动能力均有所提升。

Conclusion: MPOPI显著提升了足式机器人在复杂环境下的运动性能，为实现高效自主控制提供了新方案，具有重要的应用价值。

Abstract: Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.

</details>


### [249] [ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models](https://arxiv.org/abs/2508.11918)
*Zhichen Lou,Kechun Xu,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 论文提出了ExploreVLM框架，融合视觉-语言模型，实现机器人对于动态环境下任务的实时自适应规划与探索，并在探索性任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的机器人方法在交互探索、准确感知和实时规划调整方面仍有明显不足，限制了机器人高效融入日常生活的能力。

Method: 提出ExploreVLM框架，通过基于VLMs的逐步反馈机制实现闭环任务规划，核心包括双阶段带自反思的任务规划器、以物体为中心的空间关系图提升场景感知、及动作执行验证器确保每一步的正确性，触发必要的重规划。

Result: 在大量真实世界实验中，ExploreVLM在探索驱动任务上显著超过现有主流方法。消融实验表明，反思性规划器和结构化感知对于提高系统稳健性和效率起到了关键作用。

Conclusion: ExploreVLM借助闭环反馈和结构化场景认知，极大增强了机器人在复杂环境中的适应能力和执行效率，为机器人智能任务规划提供了有力支撑。

Abstract: The advancement of embodied intelligence is accelerating the integration of
robots into daily life as human assistants. This evolution requires robots to
not only interpret high-level instructions and plan tasks but also perceive and
adapt within dynamic environments. Vision-Language Models (VLMs) present a
promising solution by combining visual understanding and language reasoning.
However, existing VLM-based methods struggle with interactive exploration,
accurate perception, and real-time plan adaptation. To address these
challenges, we propose ExploreVLM, a novel closed-loop task planning framework
powered by Vision-Language Models (VLMs). The framework is built around a
step-wise feedback mechanism that enables real-time plan adjustment and
supports interactive exploration. At its core is a dual-stage task planner with
self-reflection, enhanced by an object-centric spatial relation graph that
provides structured, language-grounded scene representations to guide
perception and planning. An execution validator supports the closed loop by
verifying each action and triggering re-planning. Extensive real-world
experiments demonstrate that ExploreVLM significantly outperforms
state-of-the-art baselines, particularly in exploration-centric tasks. Ablation
studies further validate the critical role of the reflective planner and
structured perception in achieving robust and efficient task execution.

</details>


### [250] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: 该论文提出了一种基于视觉的全向双足机器人步行学习框架，利用深度图像实现无缝的全向运动，并通过创新的数据增强技术提升训练效率和实际鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双足机器人在动态环境下实现全向、敏捷运动存在感知和控制的双重挑战，特别是利用视觉信息进行全向地形感知时，模拟中的计算代价极高，导致强化学习难以应用于真实场景。

Method: 结合强健的无视觉（blind）控制器与教师-学生策略，教师利用视觉信息指导学生技能，通过噪声增强的地形数据进行训练，避免RL过程中的大规模渲染开销；提出高效的数据增强方法以提升监督学习训练速度。

Result: 仿真与真实世界实验均显示该方法能高效实现视觉驱动的全向步行，训练速度较常规方法加快10倍，并大幅减少渲染成本。

Conclusion: 首次实现了视觉驱动的全向双足步行机器人，展现出对复杂地形的良好适应性，并为未来机器人在复杂环境中的运动控制提供了新方法。

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [251] [Toward General Physical Intelligence for Resilient Agile Manufacturing Automation](https://arxiv.org/abs/2508.11960)
*Sandeep Kanta,Mehrdad Tavassoli,Varun Teja Chirkuri,Venkata Akhil Kumar,Santhi Bharath Punati,Praveen Damacharla,Sunny Katyara*

Main category: cs.RO

TL;DR: 本文综述了面向通用物理智能（GPI）的视觉-语言-行动（VLA）基础模型在制造业中的应用进展，并对其工业部署能力进行了评估和分析。


<details>
  <summary>Details</summary>
Motivation: 当前制造业要求机器人具备弹性与情境推理能力，但VLA等基础模型在实际制造业中的应用和作用尚未充分探讨。本文旨在系统梳理GPI范式下VLA模型在制造业的研究进展和应用状况，填补这一研究空白。

Method: 作者通过文献调研，总结归纳了GPI领域内VLA模型的最新研究，将其归纳为多感知学习、虚实迁移、规划与控制、不确定性与安全性、标准化评测等五大主题，并对主流实现进行对比分析与消融实验，评测其工业部署的可行性。

Result: 文章系统对比了不同VLA模型的代表性实现，全面分析其在GPI范式下各项能力，展示了当前技术在工业应用中的准备度及潜在局限，并归纳出现有研究面临的主要挑战。

Conclusion: GPI范式下的VLA模型已表现出面向工业的巨大潜力，但在多模态理解、虚实泛化、安全保障等方面仍存在诸多挑战。文章提出了未来将GPI更好融合进工业5.0生态的研究方向和建议。

Abstract: Agile and human-centric manufacturing stipulates resilient robotic solutions
capable of contextual reasoning and safe interaction in unstructured
environments. Foundation models particularly the Vision Language Action (VLA)
models have emerged to fuse multimodal perception, reasoning and physically
grounded action across varied embodiments into unified representation, termed
as General Physical Intelligence (GPI). While GPI has already been described in
the literature but its practical application and evolving role in contemporary
agile manufacturing processes have yet to be duly explored. To bridge this gap,
this practical review systematically surveys recent advancements in VLA models
within GPI context, performs comprehensive comparative analysis of leading
implementations and evaluates their readiness for industrial deployment through
structured ablation study. Our analysis has organized state-of-the-art into
five thematic pillars including multisensory representation learning, sim2real
transfer, planning and control, uncertainty and safety measures and
benchmarking. Finally, we articulate open research challenges and propose
directions to better integrate GPI into next-generation industrial ecosystems
in line with Industry 5.0.

</details>


### [252] [Fully Spiking Actor-Critic Neural Network for Robotic Manipulation](https://arxiv.org/abs/2508.12038)
*Liwen Zhang,Heng Deng,Guanghui Sun*

Main category: cs.RO

TL;DR: 本研究提出了一种基于全脉冲神经网络（SNN）的混合课程强化学习（CRL）框架，用于9自由度机器人手臂的目标到达与抓取任务。通过简化SNN结构并结合课程学习策略，实现了资源受限环境下的高效、低能耗控制。实验结果显示，该方法在物理约束下表现优越，且能耗远低于传统人工神经网络（ANN）方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制方法在资源受限环境下仍面临网络结构复杂、推理延迟高和能耗大的挑战。通过利用SNN的高推理速度、低能耗和生物可解释性，有望实现更高效的机器人控制。同时，实际任务中的学习效率和策略精度也需进一步提升。

Method: 作者提出将SNN与分阶段课程学习策略相结合，采用Proximal Policy Optimization (PPO)算法，并通过动态两阶段奖励调整机制和优化观测空间提高学习效率。此外，提出能耗建模框架，定量比较SNN和ANN理论能耗。实验采用Isaac Gym模拟平台进行验证。

Result: 新方法在目标到达和抓取任务中具有更高的表现，尤其在物理约束和动态操作场景下展现更强的可扩展性与能效优越性。与传统PPO与ANN基线方法对比，实验显示SNN方法学习效率更高，能耗更低。

Conclusion: 结合SNN与课程学习的机器人控制框架可以在满足能耗和响应速度需求的同时，显著提升学习和操作的效果。该方法在资源受限与动态操控等实际应用场景中具有良好推广性和应用前景。

Abstract: This study proposes a hybrid curriculum reinforcement learning (CRL)
framework based on a fully spiking neural network (SNN) for 9-degree-of-freedom
robotic arms performing target reaching and grasping tasks. To reduce network
complexity and inference latency, the SNN architecture is simplified to include
only an input and an output layer, which shows strong potential for
resource-constrained environments. Building on the advantages of SNNs-high
inference speed, low energy consumption, and spike-based biological
plausibility, a temporal progress-partitioned curriculum strategy is integrated
with the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy
consumption modeling framework is introduced to quantitatively compare the
theoretical energy consumption between SNNs and conventional Artificial Neural
Networks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized
observation space further improve learning efficiency and policy accuracy.
Experiments on the Isaac Gym simulation platform demonstrate that the proposed
method achieves superior performance under realistic physical constraints.
Comparative evaluations with conventional PPO and ANN baselines validate the
scalability and energy efficiency of the proposed approach in dynamic robotic
manipulation tasks.

</details>


### [253] [Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs](https://arxiv.org/abs/2508.12043)
*Fei Lin,Tengchao Zhang,Qinghua Ni,Jun Huang,Siji Ma,Yonglin Tian,Yisheng Lv,Naiqi Wu*

Main category: cs.RO

TL;DR: 本文研究了利用大语言模型（LLM）驱动的无人机集群在带宽受限环境下，通过语义压缩实现高效协作通信的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛应用于无人系统，无人机集群在语义理解和自主任务执行上有显著提升，但有限的通信带宽和高频次交互对队内语义传递带来巨大挑战。

Method: 构建了四种环境复杂度不同的二维仿真场景，设计了结合系统提示与任务指令提示的通信执行流程，系统评测了九种主流LLM在不同情境下的语义压缩表现，并通过消融实验分析不同环境复杂度和编队规模对适应性和稳定性的影响。

Result: 实验结果表明，基于LLM的无人机集群在带宽受限和多跳链路条件下，有望实现高效的协作通信。

Conclusion: LLM驱动的无人机集群具备通过语义压缩实现高效、稳定协作通信的潜力，对提升无人系统自主协作能力有重要意义。

Abstract: The rapid adoption of Large Language Models (LLMs) in unmanned systems has
significantly enhanced the semantic understanding and autonomous task execution
capabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited
communication bandwidth and the need for high-frequency interactions pose
severe challenges to semantic information transmission within the swarm. This
paper explores the feasibility of LLM-driven UAV swarms for autonomous semantic
compression communication, aiming to reduce communication load while preserving
critical task semantics. To this end, we construct four types of 2D simulation
scenarios with different levels of environmental complexity and design a
communication-execution pipeline that integrates system prompts with task
instruction prompts. On this basis, we systematically evaluate the semantic
compression performance of nine mainstream LLMs in different scenarios and
analyze their adaptability and stability through ablation studies on
environmental complexity and swarm size. Experimental results demonstrate that
LLM-based UAV swarms have the potential to achieve efficient collaborative
communication under bandwidth-constrained and multi-hop link conditions.

</details>


### [254] [OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments](https://arxiv.org/abs/2508.12071)
*Amy Phung,Richard Camilli*

Main category: cs.RO

TL;DR: 本文提出了一种实时的水下三维场景重建方法OASIS，结合声学和光学传感器，以提升复杂水下环境的重建精度和操作效率。


<details>
  <summary>Details</summary>
Motivation: 高分辨率的水下三维重建对于建设、维修、监测、探索等领域至关重要。现有方法大多只关注离线重建，缺乏对自主或遥操作水下机器人实时空间感知的支持。

Method: OASIS方法融合了成像声呐与光学相机的数据，基于体素切割技术实现实时的三维重建。采用“手持视角”配置，利用机械臂的灵活性，从不同视角捕获工作空间的多视图图像，提升重建的完整性和准确性。

Result: 通过水池实验验证OASIS的有效性，展示了其在水下操作任务中的定性和定量优势。相比现有单一传感器方法，OASIS在重建精度和实时性方面表现更佳。

Conclusion: OASIS实现了对无结构水下环境的实时、高精度三维重建，有助于提升水下机器人在操作、探索等任务中的空间感知和作业能力。

Abstract: High resolution underwater 3D scene reconstruction is crucial for various
applications, including construction, infrastructure maintenance, monitoring,
exploration, and scientific investigation. Prior work has leveraged the
complementary sensing modalities of imaging sonars and optical cameras for
opti-acoustic 3D scene reconstruction, demonstrating improved results over
methods which rely solely on either sensor. However, while most existing
approaches focus on offline reconstruction, real-time spatial awareness is
essential for both autonomous and piloted underwater vehicle operations. This
paper presents OASIS, an opti-acoustic fusion method that integrates data from
optical images with voxel carving techniques to achieve real-time 3D
reconstruction unstructured underwater workspaces. Our approach utilizes an
"eye-in-hand" configuration, which leverages the dexterity of robotic
manipulator arms to capture multiple workspace views across a short baseline.
We validate OASIS through tank-based experiments and present qualitative and
quantitative results that highlight its utility for underwater manipulation
tasks.

</details>


### [255] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: 本文介绍了在公共场所部署社交机器人时遇到的困难和解决过程，重点在于如何获得相关利益方的信任。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人在公共空间的使用越来越普遍，作者关注到除技术挑战外，人的接受度和利益相关方的反对也是实际部署中的主要障碍。

Method: 作者在两个真实场景（学生服务中心和难民/寻求庇护者服务点）中实际部署社交机器人，记录并分析与相关工作人员互动和获取信任的过程。

Result: 虽然遇到了较多阻力，作者最终通过持续沟通和合作获得了工作人员的信任，顺利部署了机器人并开展了研究。

Conclusion: 取得利益相关方信任对于社交机器人在公共空间成功部署至关重要，技术开发之外需重视与人的互动与关系建立。

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [256] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: 提出了一种新方法，可以在保证任务完成精度的同时，动态选择最少的传感器开启，从而减少无人系统的能耗。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在部分可观测环境下需要大量传感器以确保定位，但始终开启所有传感器浪费能量且实际不可行。因此，需要研究如何在完成任务的前提下，动态决定每个位置最少开启哪些传感器以控制状态不确定性。

Method: 作者将扩散模型（diffusion planner）与位姿-置信度信息和传感器选择掩码联合建模，将模型输出的轨迹分散度作为定位误差的代理指标。提出B-COD方法，实现了在一次前向计算中输出短期轨迹、每步点的不确定性以及本地定位误差估计，无需外部协方差传播。基于该代理指标，利用soft-actor-critic方法在线优化传感器选择，实现能耗与定位精度的权衡。

Result: 在无人水面艇的实测中，B-COD方法能有效减少传感器能耗，同时其任务完成性能与始终全部开启传感器的基线方法相当。

Conclusion: B-COD方法验证了可通过扩散模型与强化学习在线协作，既降低能耗又保证定位精度，为部分可观测环境下机器人感知子系统的高效协同提供了新思路。

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [257] [Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)](https://arxiv.org/abs/2508.12170)
*Aryan Gupta*

Main category: cs.RO

TL;DR: 本文对2020-2024年发布的、聚焦机器人能效的软件级方法进行了系统性综述，分析了79篇同行评议文献，梳理了其方法、评估指标和主要研究方向，并提出了报告规范和未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在工业、探索等领域的广泛应用，能效问题日益突出。以往关于机器人能效的软件层面综述多停留在2020年前，且分类不全面，难以指导后续能效提升技术的发展。

Method: 采用自动化加人工审核的文献筛选流程，包括Google Scholar检索、正反向追溯、LLM辅助数据抽取和人工抽样校验，最终筛选出79篇相关论文。分析重点涵盖应用领域、评价指标、能耗模型、主耗电部件、主要软件方法及能耗与性能权衡等。

Result: 工业类应用居多，占比31.6%；运动/执行器被认定为主要能耗部件(68.4%)，计算/控制器其次(13.9%)。评估以仿真为主(51.9%)，混合评估也不少(25.3%)。主流能耗建模为基于物理表示模型(87.3%)。运动轨迹优化是最常见方法(69.6%)，常结合学习与调度；但数据通讯与待机节能关注度较低。能效指标报告不统一，影响可比性。

Conclusion: 作者建议未来报告能效应规范化（如总能耗、平均功率、任务归一化指标等），并呼吁关注跨层设计、非性能指标权衡等方向。文末附带复现代码与数据集，便于后续跟踪和研究。

Abstract: This study presents a systematic literature review of software-level
approaches to energy efficiency in robotics published from 2020 through 2024,
updating and extending pre-2020 evidence. An automated-but-audited pipeline
combined Google Scholar seeding, backward/forward snowballing, and
large-language-model (LLM) assistance for screening and data extraction, with
~10% human audits at each automated step and consensus-with-tie-breaks for
full-text decisions. The final corpus comprises 79 peer-reviewed studies
analyzed across application domain, metrics, evaluation type, energy models,
major energy consumers, software technique families, and energy-quality
trade-offs. Industrial settings dominate (31.6%) followed by exploration
(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of
studies, with computing/controllers a distant second (13.9%). Simulation-only
evaluations remain most common (51.9%), though hybrid evaluations are frequent
(25.3%). Representational (physics-grounded) energy models predominate (87.3%).
Motion and trajectory optimization is the leading technique family (69.6%),
often paired with learning/prediction (40.5%) and computation
allocation/scheduling (26.6%); power management/idle control (11.4%) and
communication/data efficiency (3.8%) are comparatively underexplored. Reporting
is heterogeneous: composite objectives that include energy are most common,
while task-normalized and performance-per-energy metrics appear less often,
limiting cross-paper comparability. The review offers a minimal reporting
checklist (e.g., total energy and average power plus a task-normalized metric
and clear baselines) and highlights opportunities in cross-layer designs and in
quantifying non-performance trade-offs (accuracy, stability). A replication
package with code, prompts, and frozen datasets accompanies the review.

</details>


### [258] [Humanoid Motion Scripting with Postural Synergies](https://arxiv.org/abs/2508.12184)
*Rhea Malhotra,William Chong,Catie Cuan,Oussama Khatib*

Main category: cs.RO

TL;DR: 该论文提出了一种名为SynSculptor的系统，通过分析与提取人体动作协同，实现不依赖训练的人形机器人类人动作生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 生成仿人机器人的类人动作序列存在参考动作采集、动作合成、动作映射等难题。如何高效、自然地让机器人生成与执行类人动作，是机器人研究的重要挑战。

Method: 1) 收集3小时以上的真人动作捕捉数据，控制模拟机器人实时模仿；2) 对速度轨迹用PCA提取主要的姿态协同（synergy），以动量变化分段，构建带风格标签的协同库；3) 设计足部滑动比、动量与动能变化等指标评估合成动作，并和参考动作对比；4) 将协同与动作-语言transformer结合，使机器人在任务执行时能基于选定协同自适应姿态。

Result: 建立了丰富的姿态协同库，实现了无需训练即可基于协同脚本生成各种类人动作。通过对足部滑动、动作平滑性等多指标评估，验证了生成动作与真人参考动作高度相似。系统还可搭配动作-语言模型用于实际控制。

Conclusion: 提出的SynSculptor系统能有效解决仿人机器人类人动作采集、生成和映射的难题，以姿态协同为核心，为训练无关的动作合成与控制提供了新思路，评估指标和实验效果表明其实用性与优越性。

Abstract: Generating sequences of human-like motions for humanoid robots presents
challenges in collecting and analyzing reference human motions, synthesizing
new motions based on these reference motions, and mapping the generated motion
onto humanoid robots. To address these issues, we introduce SynSculptor, a
humanoid motion analysis and editing framework that leverages postural
synergies for training-free human-like motion scripting. To analyze human
motion, we collect 3+ hours of motion capture data across 20 individuals where
a real-time operational space controller mimics human motion on a simulated
humanoid robot. The major postural synergies are extracted using principal
component analysis (PCA) for velocity trajectories segmented by changes in
robot momentum, constructing a style-conditioned synergy library for free-space
motion generation. To evaluate generated motions using the synergy library, the
foot-sliding ratio and proposed metrics for motion smoothness involving total
momentum and kinetic energy deviations are computed for each generated motion,
and compared with reference motions. Finally, we leverage the synergies with a
motion-language transformer, where the humanoid, during execution of motion
tasks with its end-effectors, adapts its posture based on the chosen synergy.
Supplementary material, code, and videos are available at
https://rhea-mal.github.io/humanoidsynergies.io.

</details>


### [259] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出了一种高效的自引导动作扩散方法，能在推理时显著提升生成机器人策略的表现，尤其在采样预算有限的情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式机器人策略中，为提升推理时的行动一致性与响应性，双向解码等复杂推理时间搜索方法被提出。然而这些方法随着动作采样多样性的增加，计算代价高昂。作者希望提升效率同时不损失性能。

Method: 作者提出了自引导动作扩散（self-guided action diffusion），在每一个扩散步骤中，根据先前的决策引导建议分布，从而提高推理效率和动作一致性。该方法是针对扩散策略优化的双向解码的高效替代方案。

Result: 在仿真任务实验中，该自引导方法几乎无需额外推理代价即可实现接近最优的表现。在采样预算紧张时，成功率比现有方法高出最多70%。

Conclusion: 自引导动作扩散方法能在高效率的同时提升动作一致性和反应性，尤其适合动态、采样受限的场景，具有实用推广价值。

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [260] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: 本文提出了一个新的框架VLAPS，将模型规划（如MCTS）与预训练视觉-语言-动作（VLA）模型结合，显著提升了机器人在大类别任务下的表现，尤其是在语言指令下的复杂任务。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练的视觉-语言-动作模型在机器人泛化任务中显示出潜力，但实际零样本部署时往往出现脆弱或不安全的行为，尤其是在分布外场景下效果不理想，因此需要提升其泛化和安全性。

Method: 作者提出VLAPS框架，通过在目标环境的模型上运行修改版MCTS，将VLA模型的动作先验作为搜索的引导。这种结合既利用了VLA策略的高层抽象和先验，又引入了模型规划的精细搜索，从而控制计算量并提升任务完成率。

Result: 实验显示，VLAPS在多项基于语言指令的机器人任务中，显著优于仅使用VLA模型的基线方法，在复杂任务上的成功率最多可提升67个百分点。

Conclusion: VLAPS为机器人泛化任务提供了更稳健和高效的推理方法，通过结合预训练知识和模型规划，为VLA模型的实际应用奠定了更坚实的基础。

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [261] [Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids](https://arxiv.org/abs/2508.12252)
*Kaizhe Hu,Haochen Shi,Yao He,Weizhuo Wang,C. Karen Liu,Shuran Song*

Main category: cs.RO

TL;DR: 本论文提出了Robot-Trains-Robot (RTR) 框架，由机械臂教师辅助人形机器人学生进行强化学习，实现了高效、低人工干预的现实世界机器人训练。文中还提出一种新RL管道，优化动力学潜变量以提升仿真到现实的迁移。方法在两项具挑战性的人形机器人任务中验证，展示了现实世界人形机器人学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于仿真的强化学习在人形机器人运动方面取得进展，但现实世界直接训练或自预训练策略适应仍罕见，主要受限于安全、奖励设计和学习效率问题。现实环境的直接学习对于突破仿真与现实的差距（sim-to-real gap）非常关键。

Method: 提出了Robot-Trains-Robot（RTR）系统，由机械臂在训练过程中主动辅助人形机器人，承担保护、学习进度安排、奖励分配、扰动、失败检测和自动复位等任务，降低对人工干预的需求。此外，提出优化单一动力学潜变量的强化学习流程，提升仿真到现实的策略迁移稳定性与效率。

Result: 在两个有挑战性的现实世界任务上进行了验证：（1）微调人形机器人行走策略以实现精确速度跟踪；（2）从零开始学习人形机器人站起任务。这些实验均显示该方法可以高效稳定地推进现实世界中的人形机器人学习。

Conclusion: Robot-Trains-Robot框架极大简化了现实世界中人形机器人强化学习的流程，提高了训练效率并减少人工干预，展示了该方法对推进现实世界人形机器人智能水平的巨大潜力。

Abstract: Simulation-based reinforcement learning (RL) has significantly advanced
humanoid locomotion tasks, yet direct real-world RL from scratch or adapting
from pretrained policies remains rare, limiting the full potential of humanoid
robots. Real-world learning, despite being crucial for overcoming the
sim-to-real gap, faces substantial challenges related to safety, reward design,
and learning efficiency. To address these limitations, we propose
Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
actively supports and guides a humanoid robot student. The RTR system provides
protection, learning schedule, reward, perturbation, failure detection, and
automatic resets. It enables efficient long-term real-world humanoid training
with minimal human intervention. Furthermore, we propose a novel RL pipeline
that facilitates and stabilizes sim-to-real transfer by optimizing a single
dynamics-encoded latent variable in the real world. We validate our method
through two challenging real-world humanoid tasks: fine-tuning a walking policy
for precise speed tracking and learning a humanoid swing-up task from scratch,
illustrating the promising capabilities of real-world humanoid learning
realized by RTR-style systems. See https://robot-trains-robot.github.io/ for
more info.

</details>


### [262] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: 本文针对机器人协助穿衣的难题，提出了一种适用于紧身衣物的双臂穿衣策略，并以新的球坐标系对人体不同姿势实现了轨迹学习与适应。


<details>
  <summary>Details</summary>
Motivation: 现有机器人协助穿衣的研究多集中于宽松衣物，单臂即可完成，针对紧身衣物往往失败。本研究旨在解决机器人为行动受限人群穿戴紧身衣物时，因袖口狭窄和衣物柔性导致穿衣失败的问题。

Method: 本文建立了基于球坐标系的穿衣表示方式，用方位角作为双臂协作重要特征；采用高斯混合模型（GMM）与高斯混合回归（GMR）、模仿学习的方法生成适应不同手臂姿势的穿衣轨迹。

Result: 通过多组实验验证了该策略在适应不同受试者手臂姿态下穿戴紧身衣物的有效性。

Conclusion: 提出的双臂协作穿衣方法能有效应对紧身衣物的穿戴任务，提高了机器人辅助穿衣的适应性和成功率。

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [263] [A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts](https://arxiv.org/abs/2508.12296)
*Bin Wang,Jiwen Zhang,Song Wang,Dan Wu*

Main category: cs.RO

TL;DR: 本文提出了一种针对批量高精度装配中，因加工误差导致配合类型和间隙不确定性的机器人高鲁棒性装配控制方法。通过分解任务、融合力与视觉信息，并采用多任务强化学习与策略蒸馏，实现了高效率、高成功率的自动装配。


<details>
  <summary>Details</summary>
Motivation: 工业高精度批量装配中，零件因加工误差导致配合类型（过盈/间隙）和配合量具有不确定性，常规控制策略难以兼顾所有情况，鲁棒性不足。

Method: 将批量装配任务分解为多个确定性子任务，分别采用融合力、视觉信息的强化学习（FVFC-MTRL）学习各子任务的柔顺控制策略。最后用多教师策略蒸馏把多种子任务策略整合进一个统一学生网络，形成终极鲁棒控制策略。

Result: 实验证明该方法能够针对多种配合类型和配合量建立鲁棒自动装配控制策略。与现有方法相比，训练效率提升明显，最终策略在力柔顺性和任务成功率上表现更优。

Conclusion: 本文方法能有效应对装配过程中配合类型和配合量不确定性的挑战，实现更强鲁棒性和更高效率的自动装配控制，有望应用于高精度工业批量制造。

Abstract: In some high-precision industrial applications, robots are deployed to
perform precision assembly tasks on mass batches of manufactured pegs and
holes. If the peg and hole are designed with transition fit, machining errors
may lead to either a clearance or an interference fit for a specific pair of
components, with uncertain fit amounts. This paper focuses on the robotic batch
precision assembly task involving components with uncertain fit types and fit
amounts, and proposes an efficient methodology to construct the robust and
compliant assembly control strategy. Specifically, the batch precision assembly
task is decomposed into multiple deterministic subtasks, and a force-vision
fusion controller-driven reinforcement learning method and a multi-task
reinforcement learning training method (FVFC-MTRL) are proposed to jointly
learn multiple compliance control strategies for these subtasks. Subsequently,
the multi-teacher policy distillation approach is designed to integrate
multiple trained strategies into a unified student network, thereby
establishing a robust control strategy. Real-world experiments demonstrate that
the proposed method successfully constructs the robust control strategy for
high-precision assembly task with different fit types and fit amounts.
Moreover, the MTRL framework significantly improves training efficiency, and
the final developed control strategy achieves superior force compliance and
higher success rate compared with many existing methods.

</details>


### [264] [Implementation and evaluation of a prediction algorithm for an autonomous vehicle](https://arxiv.org/abs/2508.12312)
*Marco Leon Rapp*

Main category: cs.RO

TL;DR: 本文提出了一种每5毫秒预测一次自动驾驶汽车轨迹的算法，通过比较动力学和运动学自行车模型，动力学模型在高速下精度更高，最终实现了每米轨迹仅1.25厘米的偏差。


<details>
  <summary>Details</summary>
Motivation: 实现自动驾驶车辆在高速和复杂工况下对自身轨迹更高精度的实时预测，提升自动驾驶系统安全性与稳定性。

Method: 实验测定车辆参数（质量、重心、转动惯量、轮胎侧偏刚度），通过一种新颖的光学定位测量方法获得侧偏刚度数据，将动力学模型集成至扩展卡尔曼滤波器，并使用ROS和C++实现实时运行，比较动力学和运动学模型效果。

Result: 动力学模型在高速度下的轨迹预测明显优于运动学模型，实现了每米1.25厘米的轨迹偏差，精度相比运动学模型提升高达82.6%。

Conclusion: 动力学自行车模型结合扩展卡尔曼滤波器能够高精度实时预测自动驾驶汽车轨迹，尤其适用于高速工况，优于传统运动学模型。

Abstract: This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.

</details>


### [265] [Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control](https://arxiv.org/abs/2508.12335)
*Yunfan Gao,Florian Messerer,Niels van Duijkeren,Rashmi Dabir,Moritz Diehl*

Main category: cs.RO

TL;DR: 本文提出了一种结合局部降维与外部主动集方法的新型碰撞避免方法，有效解决了具状态不确定性环境下半无限约束的最优控制问题，并在高频率的机器人实测和三维仿真中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的优化与模型预测控制方法在面对大量障碍点及机器人的复杂多边形形状时，碰撞避免约束会导致优化问题变得极为高维和复杂，尤其是在考虑状态不确定性（如位置和姿态误差）时。因此，寻找一种高效的碰撞避免建模与求解方法，提升安全性与实时性，是机器人导航领域的重大挑战和研究动机。

Method: 作者将环境中的障碍物表示为大量点，机器人建模为多个带padding的多边形，并将碰撞避免约束建模为障碍点与机器人轮廓间的半无限约束。为高效求解该半无限规划最优控制问题，提出结合局部降维（local reduction）和外部主动集（external active-set）方法：迭代筛选“最危险”障碍点，仅对相关有限子集求解有限约束问题。同时，针对考虑状态不确定性（椭球体）导致的无限约束维度扩展，将平移不确定性通过局部降维与形状参数化处理，旋转不确定性用backoff重构法。

Result: 所提方法实现了高效的实时碰撞避免控制器，在真实机器人上以20Hz运行，实现了狭窄空间下的快速无碰导航。同时在仿真中验证了其对三维避碰问题的有效性。

Conclusion: 本文提出的结合局部降维和主动集的半无限规划碰撞避免方法能够高效处理大规模障碍物和机器人状态不确定性，在实际和仿真中表现出色，显著提高了机器人安全导航的实时性和适用性。

Abstract: This paper presents a novel approach for collision avoidance in optimal and
model predictive control, in which the environment is represented by a large
number of points and the robot as a union of padded polygons. The conditions
that none of the points shall collide with the robot can be written in terms of
an infinite number of constraints per obstacle point. We show that the
resulting semi-infinite programming (SIP) optimal control problem (OCP) can be
efficiently tackled through a combination of two methods: local reduction and
an external active-set method. Specifically, this involves iteratively
identifying the closest point obstacles, determining the lower-level distance
minimizer among all feasible robot shape parameters, and solving the
upper-level finitely-constrained subproblems.
  In addition, this paper addresses robust collision avoidance in the presence
of ellipsoidal state uncertainties. Enforcing constraint satisfaction over all
possible uncertainty realizations extends the dimension of constraint
infiniteness. The infinitely many constraints arising from translational
uncertainty are handled by local reduction together with the robot shape
parameterization, while rotational uncertainty is addressed via a backoff
reformulation.
  A controller implemented based on the proposed method is demonstrated on a
real-world robot running at 20Hz, enabling fast and collision-free navigation
in tight spaces. An application to 3D collision avoidance is also demonstrated
in simulation.

</details>


### [266] [SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning](https://arxiv.org/abs/2508.12394)
*Zichen Yan,Rui Huang,Lei He,Shao Guo,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于视觉强化学习的无人机自主导航方法，实现了无需外部定位的端到端图像目标导航，并结合深度安全模块实现实时避障。


<details>
  <summary>Details</summary>
Motivation: 目前大多数图像目标导航研究集中在地面机器人上，而无人机由于飞行稳定性和高频控制等需求，实现更为困难。现有无人机方法多局限于路径跟踪或避障，尚缺乏能全面自主探索并实现视觉目标定位的方案。

Method: 作者提出了一个模拟到现实（sim-to-real）框架，利用视觉强化学习训练无人机进行图像目标导航。创新点包括用辅助任务（图像扰动、未来过渡预测）提升视觉骨干网络能力，实现无需依赖外部定位的直接速度控制；同时融合基于深度的安全模块以增强避障能力。

Result: 该方法在无人机导航中实现了全面的自主探索、目标追踪与避障，并且省略了显式的全局地图构建。

Conclusion: 本文方法为无人机图像目标导航提供了新的解决思路，并证明了其端到端决策与多任务能力，提升了无人机在复杂环境中的自主导航水平。

Abstract: Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an
unknown environment and reaching a location that visually matches a given
target image. While prior works primarily study ImageNav for ground robots,
enabling this capability for autonomous drones is substantially more
challenging due to their need for high-frequency feedback control and global
localization for stable flight. In this paper, we propose a novel sim-to-real
framework that leverages visual reinforcement learning (RL) to achieve ImageNav
for drones. To enhance visual representation ability, our approach trains the
vision backbone with auxiliary tasks, including image perturbations and future
transition prediction, which results in more effective policy training. The
proposed algorithm enables end-to-end ImageNav with direct velocity control,
eliminating the need for external localization. Furthermore, we integrate a
depth-based safety module for real-time obstacle avoidance, allowing the drone
to safely navigate in cluttered environments. Unlike most existing drone
navigation methods that focus solely on reference tracking or obstacle
avoidance, our framework supports comprehensive navigation
behaviors--autonomous exploration, obstacle avoidance, and image-goal
seeking--without requiring explicit global mapping. Code and model checkpoints
will be released upon acceptance.

</details>


### [267] [PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting](https://arxiv.org/abs/2508.12395)
*Zihan Wang*

Main category: cs.RO

TL;DR: 本研究提出了一种利用等离子体推进的超静音飞艇（PUB），通过等离子体矢量推进实现了无机械螺旋桨的超静音飞行，并在实际飞行中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 传统的飞艇和无人机通常依赖机械螺旋桨推进，这带来了噪音大、结构复杂等问题，不适合于对噪音敏感或空间受限的环境。因此作者希望开发一种结构简单、安静度高的新型飞行平台。

Method: 采用氦气浮升体提供升力，利用四层环形非对称电容器产生等离子体风推力；通过模块化推进单元和具有两自由度的机头实现推进和推力矢量控制，并设计了闭环滑移控制系统以实现稳定飞行。通过飞行实验验证其推力控制和飞行动作能力。

Result: 飞行实验验证了等离子体矢量推进的可行性、两自由度矢量控制的有效性以及控制系统的稳定性。飞艇可实现全阶段（起飞、爬升、悬停、下降、平滑着陆）飞行。

Conclusion: PUB具有低噪音、结构简单、高机动性优势，适用于噪音敏感、封闭或近空环境下的应用，展示了无螺旋桨等离子体推进技术的实际潜力。

Abstract: This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.

</details>


### [268] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 本论文提出了一种仅依赖机器人内置关节传感器（无需外部传感器）进行手势识别的方法，并通过深度学习模型取得了高精度的识别效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于视觉或机器人皮肤的手势识别在HRC中较为常见，但依赖外部传感器，提升了成本和复杂性。作者希望探索不用外部传感器，只用机器人自身传感器是否也能实现高效手势识别。

Method: 作者设计并评估了多种卷积神经网络（CNN）架构，对数据进行不同表示（如时频谱图Spectrogram），采集了两组数据集，通过比较数据表现方式和模型架构对识别准确率的影响。在Frank Emika机器人上实现并测试，包括STFT2DCNN和STT3DCNN两种方法。

Result: 实验证明，基于时频谱图的数据表现方式显著提升了识别精度，模型架构影响较小。尤其在机器人姿态变化情况下，spectrogram方法泛化性更好。最终两种方法都达到了95%以上的手势识别准确率。

Conclusion: 研究首次验证了无需外部传感器，仅靠内置关节传感器即可高效完成手势/触觉识别，为未来更低成本、更易实现的HRC系统提供了方向。

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [269] [Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2508.12439)
*Sunyu Wang,Arjun S. Lakshmipathy,Jean Oh,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 本文提出了一种基于测地线追踪的积分方法，在离散网格（manifold mesh）上实现滚滑接触（roll-slide contact）的建模，从而允许仿真高精度物体几何的灵巧操作机器人动作规划。实验表明，相较于使用简单碰撞检测和基本形状建模的基线方法，该方法在准确率和精度上都表现最好。


<details>
  <summary>Details</summary>
Motivation: 目前大多数滚滑接触模型仅适用于可微的连续形状，但实际操作对象多为复杂、不规则的离散网格。为实现高精度灵巧操作任务，迫切需要能直接在三维网格上建模和推理滚滑接触的方法。

Method: 作者提出了一种结合测地线追踪的积分方法，能够在网格上对滚滑接触进行一阶时间积分。并基于该方法，结合最小二乘优化器规划多指手部对多个物体的原地灵巧操纵动作，优化目标是最小化接触点滑动与旋转，保持抓取稳定性。

Result: 在仿真环境下，本文方法对多种对象进行了灵巧操纵任务，并与基于碰撞检测和原始几何建模的基线方法对比，实验结果显示即使网格较粗，本文方法依然在精度和准确率上优于其它方案。

Conclusion: 该方法可直接在三维网格上进行精确的滚滑接触建模并规划高保真的机器人精细操作。未来工作将探索引入多点接触和接触力建模，以提升表面接触的准确性和鲁棒性。

Abstract: Reasoning about rolling and sliding contact, or roll-slide contact for short,
is critical for dexterous manipulation tasks that involve intricate geometries.
But existing works on roll-slide contact mostly focus on continuous shapes with
differentiable parametrizations. This work extends roll-slide contact modeling
to manifold meshes. Specifically, we present an integration scheme based on
geodesic tracing to first-order time-integrate roll-slide contact directly on
meshes, enabling dexterous manipulation to reason over high-fidelity discrete
representations of an object's true geometry. Using our method, we planned
dexterous motions of a multi-finger robotic hand manipulating five objects
in-hand in simulation. The planning was achieved with a least-squares optimizer
that strives to maintain the most stable instantaneous grasp by minimizing
contact sliding and spinning. Then, we evaluated our method against a baseline
using collision detection and a baseline using primitive shapes. The results
show that our method performed the best in accuracy and precision, even for
coarse meshes. We conclude with a future work discussion on incorporating
multiple contacts and contact forces to achieve accurate and robust mesh-based
surface contact modeling.

</details>


### [270] [Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics](https://arxiv.org/abs/2508.12456)
*Hadas C. Kuzmenko,David Ehevich,Oren Gal*

Main category: cs.RO

TL;DR: 该论文提出了一种结合多智能体群体机器人系统和液体时常神经网络（LTCN）的框架，有效提升了海洋溢油预测与应急能力。


<details>
  <summary>Details</summary>
Motivation: 现有海洋溢油轨迹预测因受到多种物理、化学和环境因素影响而复杂，造成及时、有效应对溢油事件困难。提升实时预测精度和协调处置效率，对环境和经济影响减缓至关重要。

Method: 框架基于MOOS-IvP平台，集成了多智能体群体机器人系统与LTCN神经网络。利用LTCN擅长处理复杂时序过程的特点，机器人群体能够协同实现溢油的实时预测、动态追踪与快速应对，融合分布式智能和自适应机器学习方法。

Result: 在Deepwater Horizon溢油数据集上，所提出的LTC-RK4模型实现了0.96的空间精度，比LSTM模型高出23%。实验验证了系统在预测准确性和群体机器人协作能力上的显著优势。

Conclusion: 本研究显著提升了海洋溢油轨迹预测精度及机器人群体应急协调能力，推动了溢油自动化、可持续管理技术的发展，对环境保护具有积极意义。

Abstract: Marine oil spills pose grave environmental and economic risks, threatening
marine ecosystems, coastlines, and dependent industries. Predicting and
managing oil spill trajectories is highly complex, due to the interplay of
physical, chemical, and environmental factors such as wind, currents, and
temperature, which makes timely and effective response challenging. Accurate
real-time trajectory forecasting and coordinated mitigation are vital for
minimizing the impact of these disasters. This study introduces an integrated
framework combining a multi-agent swarm robotics system built on the MOOS-IvP
platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system
fuses adaptive machine learning with autonomous marine robotics, enabling
real-time prediction, dynamic tracking, and rapid response to evolving oil
spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent
processes--the framework achieves real-time, high-accuracy forecasts of spill
movement. Swarm intelligence enables decentralized, scalable, and resilient
decision-making among robot agents, enhancing collective monitoring and
containment efforts. Our approach was validated using data from the Deepwater
Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,
surpassing LSTM approaches by 23%. The integration of advanced neural modeling
with autonomous, coordinated robotics demonstrates substantial improvements in
prediction precision, flexibility, and operational scalability. Ultimately,
this research advances the state-of-the-art for sustainable, autonomous oil
spill management and environmental protection by enhancing both trajectory
prediction and response coordination.

</details>


### [271] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 该论文介绍了一个利用YOLOv8模型和Kociemba算法，通过三步进电机自动还原魔方的系统，具备虚拟界面，平均解魔方时间约为2.2分钟。


<details>
  <summary>Details</summary>
Motivation: 动机在于开发一个能够自动检测、虚拟显示并快速物理还原魔方的系统，为自动化与人机交互领域提供技术支持。

Method: 系统包括三台步进电机用于魔方操作，一个微控制器进行硬件管理，一个摄像头和YOLO检测模型进行实时魔方状态识别。采用Unity开发了用户友好的GUI，将检测到的魔方状态虚拟化显示。魔方求解采用Kociemba算法，物理操作则通过多步进电机单自由度联动实现。

Result: YOLOv8检测模型表现优异（精度0.98443，召回率0.98419），整体系统平均实现了约2.2分钟的自动魔方还原。

Conclusion: 该系统实现了自动化、实时魔方识别、虚拟交互和物理还原，融合了计算机视觉、智能控制和人机界面设计，为相关自动化系统开发提供了参考。

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [272] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法PROD，利用力控探测融合弹性静态距离函数，重建可变形物体的形状和力学属性。与传统视觉几何方法相比，PROD能同时估计材料形状和刚度，在不同应用中表现出较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的可变形物体重建方法多依赖几何或视觉信息，难以精确获取物体机械属性（如刚度）。在机器人操作、医学成像等领域，对材料物理属性的需求日益增强，因此需要更全面的方法结合物理交互信息。

Method: 该方法通过表面力控探测获取位姿和受力数据，将物体变形建模为弹性静态过程，推导出泊松方程，从稀疏测量中估算其弹性静态距离函数（SDF）。并假设稳态弹性动力学，可由变形观测数据恢复初始的SDF，同时分析位移响应推断材料刚度。

Result: PROD在仿真软体交互中，对位姿误差、非标准受力及曲率误差表现出良好鲁棒性，能够准确重建可变形体的形状和机械性质。

Conclusion: PROD方法为可变形物体的形状和材料属性重建提供了强有力工具，适用于机器人操控、医学成像及触觉反馈系统等多领域，尤其在非理想测量条件下表现优越。

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [273] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种基于运动的事件相机多传感器系统时空旋转外参自动标定方法，无需特定标定板，具有高精度与高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时空分辨率和低延迟等优势，被用于多传感器融合，但事件相机相关的外参标定问题研究不多，传统方法大多依赖帧图像或标定靶标，局限性明显。因此需要一种适用于事件相机，并可无靶标自动标定的高效方法。

Method: 该方法利用事件相机和其他异构传感器各自的旋转运动估计数据，首先采用事件流的法向流从原始事件流中估算角速度，然后以CCA（典型相关分析）为思想，初步估计时序偏移和旋转外参。最后，采用连续时间SO(3)参数化，进行联合非线性优化进一步精化参数。整个流程不需专用标定板，完全基于运动信息。

Result: 在公开和自采多数据集上，与基于标定靶标方法相比，本方法的外参标定精度相当；相较于仅依赖CCA的方法，稳定性更强，具备更高的精度、鲁棒性与灵活性。

Conclusion: 提出的非依赖标定靶标、基于运动的事件相机多传感器外参精确标定方法，适用于实际应用且表现优越，代码将开源，有助于后续相关研究发展。

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [274] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: 本文提出了一种适用于软体连续体机器人（SCRs）的实时非线性模型预测控制（MPC）方法，基于领域解耦的物理信息神经网络（DD-PINN），大幅提升了模型推理速度，并在仿真和现实实验中实现了高精度和高加速度控制。


<details>
  <summary>Details</summary>
Motivation: 由于软体连续体机器人的动力学模型计算开销大，难以实现高效的动态控制。现有数据驱动方法虽然可以部分缓解问题，但在适应性和形态全捕获方面存在局限，限制了实际应用场景。

Method: 提出一种领域解耦的物理信息神经网络（DD-PINN），作为Cosserat杆动力学模型的高效近似，速度提升44000倍。将该神经网络集成到非线性进化型MPC中，并结合无迹卡尔曼滤波自适应估计姿态和柔顺性。整个系统架构实现了70Hz的实时控制。

Result: 在仿真和实际实验中，该MPC框架能实现SCR末端轨迹的高精度跟踪和定点控制，位置误差低于3mm（占执行器长度2.3%），最大加速度达3.55 m/s2。

Conclusion: 所提出的DD-PINN+MPC框架兼具高精度、实时性和适应性，有效提升了SCR动态控制能力，为其实际应用拓展提供了理论及技术基础。

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [275] [MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA](https://arxiv.org/abs/2508.12729)
*Junhao Ye,Cheng Hu,Yiqin Wang,Weizhan Huang,Nicolas Baumann,Jie He,Meixun Qu,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: 本文提出了MCTR算法，通过曲率修正滑动平均方法提升自主赛车中轨迹的平滑度，并在CARLA中通过数字孪生和3D LiDAR感知验证其性能。实验结果显示，MCTR优于现有方法，表现出更高的鲁棒性和实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于反应式控制的自主赛车方法（如FTG和DTR）对轨迹平滑性支持不足，影响性能；同时常用模拟器缺少3D LiDAR感知，难以进行真实环境下的有效测试。作者希望解决轨迹平滑和高真实性验证两个核心瓶颈。

Method: MCTR算法在轨迹生成时采用曲率修正滑动平均，改善生成路径的平滑性。此外，作者在支持3D LiDAR感知的CARLA模拟器中搭建数字孪生系统，对算法进行仿真和实际车辆实验双重验证。

Result: 在仿真和真实车辆实验中，MCTR算法表现出比传统DTR及FTG方法更优的轨迹平滑性和鲁棒性，提升了自主赛车的控制效果和可靠性。

Conclusion: MCTR算法有效解决了路径平滑不足的问题，并通过高保真仿真环境和实车测试证明了其实用性，对提升自主赛车的性能具有重要意义。

Abstract: In autonomous racing, reactive controllers eliminate the computational burden
of the full See-Think-Act autonomy stack by directly mapping sensor inputs to
control actions. This bypasses the need for explicit localization and
trajectory planning. A widely adopted baseline in this category is the
Follow-The-Gap method, which performs trajectory planning using LiDAR data.
Building on FTG, the Delaunay Triangulation-based Racing algorithm introduces
further enhancements. However, DTR's use of circumcircles for trajectory
generation often results in insufficiently smooth paths, ultimately degrading
performance. Additionally, the commonly used F1TENTH-simulator for autonomous
racing competitions lacks support for 3D LiDAR perception, limiting its
effectiveness in realistic testing. To address these challenges, this work
proposes the MCTR algorithm. MCTR improves trajectory smoothness through the
use of Curvature Corrected Moving Average and implements a digital twin system
within the CARLA simulator to validate the algorithm's robustness under 3D
LiDAR perception. The proposed algorithm has been thoroughly validated through
both simulation and real-world vehicle experiments.

</details>


### [276] [RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph](https://arxiv.org/abs/2508.12916)
*Hecheng Wang,Jiankun Ren,Jia Yu,Lizhe Qi,Yunquan Sun*

Main category: cs.RO

TL;DR: RoboRetriever是一个只依赖单个手腕安装的RGB-D摄像头，通过自然语言指令进行物体检索的机器人系统，能够在杂乱、部分可见的环境中表现出强适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人系统普遍依赖多个固定摄像头，导致硬件成本高、适应性差，而人类通过主动视觉、实体交互等能力，仅凭单双眼就能高效在复杂环境下检索目标。作者希望开发一种低成本、高适应性的单摄像头机器人系统。

Method: 提出RoboRetriever框架，利用手腕安装的单个RGB-D相机采集环境信息，将视觉观测通过动态分层场景图表示，结合自然语言任务指令推理目标物体与操作行动。引入新的视觉提示方案，用大规模视觉-语言模型预测与任务语义相关的最佳相机位姿，实现任务感知的主动视觉和交互感知联动的操控。

Result: RoboRetriever在多种真实世界的物体检索任务中进行了测试，包括有人工干预的场景。实验表明系统在仅用一个摄像头的条件下，在杂乱环境中展现了很强的适应性和鲁棒性。

Conclusion: RoboRetriever证明了使用单一手腕RGB-D摄像头和大模型推理，可以显著简化硬件配置，同时实现复杂场景下物体检索任务的高效和可靠。

Abstract: Humans effortlessly retrieve objects in cluttered, partially observable
environments by combining visual reasoning, active viewpoint adjustment, and
physical interaction-with only a single pair of eyes. In contrast, most
existing robotic systems rely on carefully positioned fixed or multi-camera
setups with complete scene visibility, which limits adaptability and incurs
high hardware costs. We present \textbf{RoboRetriever}, a novel framework for
real-world object retrieval that operates using only a \textbf{single}
wrist-mounted RGB-D camera and free-form natural language instructions.
RoboRetriever grounds visual observations to build and update a \textbf{dynamic
hierarchical scene graph} that encodes object semantics, geometry, and
inter-object relations over time. The supervisor module reasons over this
memory and task instruction to infer the target object and coordinate an
integrated action module combining \textbf{active perception},
\textbf{interactive perception}, and \textbf{manipulation}. To enable
task-aware scene-grounded active perception, we introduce a novel visual
prompting scheme that leverages large reasoning vision-language models to
determine 6-DoF camera poses aligned with the semantic task goal and geometry
scene context. We evaluate RoboRetriever on diverse real-world object retrieval
tasks, including scenarios with human intervention, demonstrating strong
adaptability and robustness in cluttered scenes with only one RGB-D camera.

</details>


### [277] [Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users](https://arxiv.org/abs/2508.12925)
*Eetu Laukka,Evan G. Center,Timo Ojala,Steven M. LaValle,Matti Pouke*

Main category: cs.RO

TL;DR: 论文探讨了在高延迟远程移动机器人系统中，通过光流技术为用户创造自我运动的错觉，以缓解360°摄像头回传画面延迟带来的控制障碍。实验发现该方法对任务表现和精度无显著提升，且有增加虚拟现实晕动症风险的迹象。


<details>
  <summary>Details</summary>
Motivation: 随着移动远程呈现机器人和360°沉浸式摄像技术的发展，用户操控受制于网络和图像处理延迟导致的控制不便。迫切需要辅助方法，提升用户实时操控的体验。

Method: 提出并实现了一种利用光流为用户在机器人的运动指令与360°摄像头图像反馈之间的延迟期内，创造自我运动错觉的技术，并通过实验测试其对控制表现（如完成任务时长、碰撞率）和晕动症的影响。

Result: 结果显示，在500毫秒控制延迟下，引入自我运动错觉对机器人控制的效率和精确性没有显著改善，并且可能加重用户的VR晕动症状。

Conclusion: 所提光流自我运动错觉方法，目前难以显著提升高延迟下的远程机器人操控表现，还需进一步优化调整，以减小副作用并提高其实用性。

Abstract: Mobile telepresence robots allow users to feel present and explore remote
environments using technology. Traditionally, these systems are implemented
using a camera onboard a mobile robot that can be controlled. Although
high-immersion technologies, such as 360-degree cameras, can increase
situational awareness and presence, they also introduce significant challenges.
Additional processing and bandwidth requirements often result in latencies of
up to seconds. The current delay with a 360-degree camera streaming over the
internet makes real-time control of these systems difficult. Working with
high-latency systems requires some form of assistance to the users.
  This study presents a novel way to utilize optical flow to create an illusion
of self-motion to the user during the latency period between user sending
motion commands to the robot and seeing the actual motion through the
360-camera stream. We find no significant benefit of using the self-motion
illusion to performance or accuracy of controlling a telepresence robot with a
latency of 500 ms, as measured by the task completion time and collisions into
objects. Some evidence is shown that the method might increase virtual reality
(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We
conclude that further adjustments are necessary in order to render the method
viable.

</details>


### [278] [Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion](https://arxiv.org/abs/2508.12928)
*Victor Dhédin,Haizhou Zhao,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出了一个结合蒙特卡洛树搜索（MCTS）与全身轨迹优化（TO）的运动规划框架，能够在复杂环境下为腿足机器人同时优化接触序列和接触点选择，实验验证具有较强的通用性和现实转移能力。


<details>
  <summary>Details</summary>
Motivation: 腿足机器人能适应复杂和受限环境，但其运动规划涉及连续和离散变量的复杂优化，目前缺乏高效同时处理步态、接触点选择的通用方法。

Method: 作者提出了一套基于MCTS与全身轨迹优化相结合的规划流程，能同步优化机器人在多变环境下的接触序列（什么时候落脚）和接触点选择（具体落在哪里），从而高效生成可行的多样化运动方案。

Result: 仿真实验表明该方法能快速产出丰富且动态一致性的运动方案，并能将仿真中的结果转移至实际四足机器人。同时，该框架也适用于拟人机器人复杂无周期动作的规划。

Conclusion: 首次实现了基于全身动力学的四足机器人同步接触序列及接触点选择的有效运动规划，为复杂多接触机器人运动提供了可行的解决方案。

Abstract: Legged robots have the potential to traverse highly constrained environments
with agile maneuvers. However, planning such motions requires solving a highly
challenging optimization problem with a mixture of continuous and discrete
decision variables. In this paper, we present a full pipeline based on
Monte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to
perform simultaneous contact sequence and patch selection on highly challenging
environments. Through extensive simulation experiments, we show that our
framework can quickly find a diverse set of dynamically consistent plans. We
experimentally show that these plans are transferable to a real quadruped
robot. We further show that the same framework can find highly complex acyclic
humanoid maneuvers. To the best of our knowledge, this is the first
demonstration of simultaneous contact sequence and patch selection for acyclic
multi-contact locomotion using the whole-body dynamics of a quadruped.

</details>


### [279] [Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade](https://arxiv.org/abs/2508.12946)
*Ann-Sophie Schenk,Stefan Schiffer,Heqiu Song*

Main category: cs.RO

TL;DR: 本论文通过访谈教师和学生，探讨了社会机器人在六年级计算机科学课堂中的应用初步见解，强调了师生对机器人的需求和期望存在差异，带来了复杂的设计挑战。


<details>
  <summary>Details</summary>
Motivation: 目前社会机器人在教育场景中应用日益增多，但关于其在K12课堂中的具体需求和潜在用途缺乏细致了解。作者希望通过调查一线教师与学生对机器人教学的看法，指导未来更有效的机器人教育方案设计。

Method: 采用访谈法，分别对六年级计算机科学课的教师和学生进行深度交流，收集他们对社会机器人进入课堂的期待、功能需求及担忧点。

Result: 教师和学生普遍对在教室引进机器人持开放态度，但两组人群在具体需求和功能期望上存在显著差异与多样性。

Conclusion: 尽管理论上师生对社会机器人的教育应用持积极态度，但需求的异质性导致机器人设计面临多重挑战，需在未来研究和产品开发中进一步细化和权衡。

Abstract: In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.

</details>


### [280] [Scaling Whole-body Multi-contact Manipulation with Contact Optimization](https://arxiv.org/abs/2508.12980)
*Victor Levé,João Moura,Sachiya Fujita,Tamon Miyake,Steve Tonneau,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本论文提出一种新方法，使仿人机器人能够自主完成全身操作任务，突破现有离散采样规划方法的局限，在全身操作规划中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的全身操作任务规划方法大多依赖于离散采样，面对机器人和物体表面的连续接触可能性时，存在可扩展性差和效率低下的问题，难以适用于复杂任务。因此，需要新的方法克服表面接触连续性带来的挑战。

Method: 作者提出了一种新的机器人与物体表面表示方法，可以实现接近点的闭式计算；同时设计了新的代价函数，有效引导全身操作的运动规划。方法采用基于梯度的优化，替代传统的离散采样。

Result: 实验结果表明，该方法能解决现有方法无法处理的问题，并在规划时间上比最先进的方法提升了77%。此外，方法也在实际仿人机器人操控箱体任务上成功验证。

Conclusion: 提出的方法有效提升了仿人机器人全身操作任务的自主规划效率和适用性，对实际机器人应用具有重要意义。

Abstract: Daily tasks require us to use our whole body to manipulate objects, for
instance when our hands are unavailable. We consider the issue of providing
humanoid robots with the ability to autonomously perform similar whole-body
manipulation tasks. In this context, the infinite possibilities for where and
how contact can occur on the robot and object surfaces hinder the scalability
of existing planning methods, which predominantly rely on discrete sampling.
Given the continuous nature of contact surfaces, gradient-based optimization
offers a more suitable approach for finding solutions. However, a key remaining
challenge is the lack of an efficient representation of robot surfaces. In this
work, we propose (i) a representation of robot and object surfaces that enables
closed-form computation of proximity points, and (ii) a cost design that
effectively guides whole-body manipulation planning. Our experiments
demonstrate that the proposed framework can solve problems unaddressed by
existing methods, and achieves a 77% improvement in planning time over the
state of the art. We also validate the suitability of our approach on real
hardware through the whole-body manipulation of boxes by a humanoid robot.

</details>


### [281] [BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments](https://arxiv.org/abs/2508.13052)
*Sourav Raxit,Abdullah Al Redwan Newaz,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: BOW Planner是一种新型可扩展运动规划算法，利用约束贝叶斯优化（CBO）在复杂环境下高效、安全地为机器人生成运动轨迹，相比传统方法在计算速度和路径质量上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人运动规划方法在遇到速度、加速度等运动学约束时效率不高，尤其是在高维环境和严格安全要求下采样消耗大，难以快速生成安全路径。

Method: BOW Planner关注在可达速度规划窗口内进行运动规划，通过约束贝叶斯优化（CBO）高效采样控制输入，能够应对高维目标函数和安全约束，以极少采样实现高效路径搜索。

Result: 理论分析证明了BOW Planner具有渐近收敛至近最优解的能力，实验评估（含仿真和实机器人）显示其在复杂环境下的计算时间、轨迹长度和解算时间相比现有技术有显著提升。

Conclusion: BOW Planner在采样效率、安全优化和规划速度方面表现突出，为机器人复杂环境导航提供了高效可靠的新工具，且已开源并在多种真实机器人系统上成功应用，具有广泛应用前景。

Abstract: This paper introduces the BOW Planner, a scalable motion planning algorithm
designed to navigate robots through complex environments using constrained
Bayesian optimization (CBO). Unlike traditional methods, which often struggle
with kinodynamic constraints such as velocity and acceleration limits, the BOW
Planner excels by concentrating on a planning window of reachable velocities
and employing CBO to sample control inputs efficiently. This approach enables
the planner to manage high-dimensional objective functions and stringent safety
constraints with minimal sampling, ensuring rapid and secure trajectory
generation. Theoretical analysis confirms the algorithm's asymptotic
convergence to near-optimal solutions, while extensive evaluations in cluttered
and constrained settings reveal substantial improvements in computation times,
trajectory lengths, and solution times compared to existing techniques.
Successfully deployed across various real-world robotic systems, the BOW
Planner demonstrates its practical significance through exceptional sample
efficiency, safety-aware optimization, and rapid planning capabilities, making
it a valuable tool for advancing robotic applications. The BOW Planner is
released as an open-source package and videos of real-world and simulated
experiments are available at https://bow-web.github.io.

</details>


### [282] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 本文综述了基于大规模视觉-语言模型（VLM）的视觉-语言-动作（VLA）模型在机器人操作领域的最新进展，系统梳理了相关的技术体系、架构范式、交叉领域集成及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人操作方法（如基于规则的算法）难以适应开放、复杂和新颖的环境，因此需要更具通用性和扩展性的智能模型。随着大规模VLM的出现，为多模态理解和控制提供了新途径，但其在机器人操作领域的系统梳理尚不充分。

Method: 本文首先界定了“大规模VLM驱动的VLA模型”，并提出双重架构范式：单体模型（单/双系统）与层次化模型（对计划与执行解耦）。然后，深入分析了这些模型在与高级领域集成（如强化学习、类人视频学习、世界模型等）、架构特性、数据集与基准、以及未来发展方向（如记忆机制、4D感知、高效自适应、多代理协作等）方面的研究进展。

Result: 综述总结了该领域的架构体系、主要技术特点以及支持发展的数据集与评价基准，同时归纳了研究中的不一致点，并为细分领域提供了统一分类和方向建议。

Conclusion: 本文整体系统整合了大规模VLM与机器人操作交叉领域的最新进展，帮助缓解了研究碎片化和分类混乱问题，为相关学者提供了清晰的研究架构和未来发展路线。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.

</details>


### [283] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: 该论文提出了Observation-Centric VLA (OC-VLA) 框架，通过将动作预测基于相机观测坐标系，从而提高视觉-语言-动作（VLA）模型在不同视角下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型由于观测与动作空间的不一致，无法很好地适应现实世界中多样化的相机视角，因此其泛化能力有限，任务成功率受到影响。

Method: OC-VLA方法直接利用相机的外参，将末端执行器的位姿预测从机器人基坐标系转换到相机坐标系，使来自不同视角的数据能在统一坐标下进行动作预测，无需结构性修改即可直接应用于现有的VLA模型。

Result: 实验表明，OC-VLA在仿真与真实操作任务中加速了训练收敛，提高了任务成功率，并且显著改善了模型在跨视角下的泛化能力。

Conclusion: OC-VLA是一种轻量级、兼容性强的方法，能够增强VLA模型对于多视角相机设置的适应性，将促进机器人在实际环境中的应用。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


### [284] [Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors](https://arxiv.org/abs/2508.13151)
*Yuying Zhang,Joni Pajarinen*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的方法，使移动机器人在动态环境中能够通过操纵清除障碍物来完成导航任务。


<details>
  <summary>Details</summary>
Motivation: 移动操作机器人在动态环境中经常被可移动障碍物阻挡。传统方法通常将导航和操纵分离，遇到需要优先清障才能导航的场景时效果不佳。

Method: 方法结合操纵能力先验，聚焦于高操控能力的位置，结合affordance map选择高质量操作动作，利用强化学习高效学习合适的操控策略。设定了两个仿真任务：Reach（有效定位机械臂末端使底盘前进）和Door（先推开门再通过）。

Result: 在仿真环境下，该方法使机器人有效完成了操控后导航的任务。将所学策略迁移到真实的Boston Dynamics Spot机器人，在Reach任务上取得了成功。

Conclusion: 该方法提升了机器人在需边操作边导航环境中的表现，具备较好的现实可迁移性。

Abstract: Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.

</details>
