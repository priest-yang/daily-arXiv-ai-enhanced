<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 213]
- [cs.CL](#cs.CL) [Total: 140]
- [cs.RO](#cs.RO) [Total: 74]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement](https://arxiv.org/abs/2509.16221)
*Martin Preiß*

Main category: cs.CV

TL;DR: 本研究探讨了将集成学习应用于OCR识别历史医学手写文档，以提高识别准确率。结果显示，集成学习确实能提升OCR准确率，并且数据集大小影响不大。


<details>
  <summary>Details</summary>
Motivation: 医学手写病历数字化对未来数据利用至关重要，但高准确率至关重要。单一OCR方法受限，故尝试集成多模型，提升手写识别效能。

Method: 采用多种OCR模型进行集成学习，对比单一模型与集成方法在医学手写病历识别中的表现，评估集成效果及训练数据量对结果的影响。

Result: 集成学习提高了OCR识别的准确率。具体有效的集成方法得以确定，同时发现训练数据集大小对提升效果没有明显影响。

Conclusion: 集成学习适用于医疗手写文档数字化，可有效提升OCR准确率，并且在样本量有限时同样有效。

Abstract: For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.

</details>


### [2] [Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute](https://arxiv.org/abs/2509.16343)
*Chung-En,Yu,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练即可提升视觉系统鲁棒性的推理方法，对现有视觉和多模态模型进行包装，显著提升高难视觉推理任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 在遥感、医疗等高风险领域中，智能视觉系统需具备广泛鲁棒性，但对模型进行频繁且昂贵的再训练并不可行，因此亟需能提升鲁棒性的无训练框架。

Method: 提出Visual Reasoning Agent (VRA)框架，通过'思考—评判—行动'循环结构，将现有的视觉-语言模型和纯视觉系统包裹起来，实现无训练的智能推理能力。

Result: 在众多具挑战性的视觉推理基准测试上，VRA框架可带来最高达40%的绝对准确率提升，但推理时计算代价显著增加。

Conclusion: VRA能够大幅提升视觉任务的可靠性，后续工作将探索降低推理开销的方法，如优化查询分流和早停机制，同时保持能力优势。

Abstract: Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.

</details>


### [3] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

TL;DR: 本论文提出ForestGen3D，一种利用航空激光雷达（ALS）数据生成高保真3D森林结构的生成模型，极大提升了大尺度生态系统三维结构测量的可行性和精度。


<details>
  <summary>Details</summary>
Motivation: 生态系统中生物及非生物组分的三维结构是理解生态过程与反馈（如火灾、干旱、病害等）关键，但真实三维结构的宽范围测量昂贵且难以实现，因此需开发高效低成本的3D结构重建方法。

Method: 提出了ForestGen3D，这是基于条件去噪扩散概率模型（DDPM），通过训练与配准的ALS/TLS数据，利用稀疏ALS数据生成接近TLS高精度的3D点云。引入凸包几何约束确保生成结果在空间上的生态合理性，并通过多尺度（树、样地、景观）真实数据验证。

Result: ForestGen3D能在不同尺度生成与真实TLS数据高度相似的3D结构，并能准确推算树高、胸径、冠幅、冠体积等生物物理指标。所提‘几何包含性’指标在无TLS真值情况下也能作为质量评估代理。

Conclusion: ForestGen3D为基于ALS数据的生态3D建模、火灾模拟及燃料结构刻画提供了可扩展高效工具，降低了大范围生态三维结构测量的门槛。

Abstract: The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.

</details>


### [4] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种关于合成图像数据生成的新优化问题——可缩放锚定区域装箱（RARP）问题，并提出了相应的启发式算法用于高效解决该问题。


<details>
  <summary>Details</summary>
Motivation: 传统的图像数据生成（如在画布上合理放置合适大小和位置的物体）是一个比判别性任务更难的问题，且主流生成方法（基于图形和生成模型）都遇到优化难题。作者希望通过引入新问题与算法方法，提升合成图像生成的效果与效率。

Method: 作者将该数据生成问题形式化为RARP（可缩放锚定区域装箱）问题，并证明其为NP-hard。针对该问题，提出了一种通用的贪心启发式算法，通过迭代方式成对打包形状和位置任意的区域，满足优化约束。该算法被用于生成大规模的合成异常检测数据集。

Result: 通过实现该算法并用于合成数据集，作者以可视化和解的正确性检查验证了算法的有效性。每个样本都呈现出装箱参数的高度变化，显示了方法的灵活与实用性。

Conclusion: 本文提出的RARP问题对影像科学社区具有实际意义，新颖算法对合成数据生成和深度学习生成模型研究有重要推动作用。

Abstract: The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.

</details>


### [5] [Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor](https://arxiv.org/abs/2509.16382)
*Saurabh Saini,Kapil Ahuja,Marc C. Steinbach,Thomas Wick*

Main category: cs.CV

TL;DR: 本研究提出一种新的基于特征提取的甲状腺癌计算机辅助诊断系统，通过结合改进的局部离散余弦变换（LDCT）与改进局部二值模式（ILBP），实现甲状腺超声图像的高精度分类，最终在两个公开数据集（TDID和AUITD）上取得了接近100%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 甲状腺超声图像由于周围复杂解剖结构和组织密度变化，导致图像纹理难以准确描述，并且存在噪声等问题。以往研究证明纹理特征对分辨甲状腺肿瘤类型很重要，因此该研究试图开发一种优越的特征提取方法，提高分类性能。

Method: 作者首先采用局部离散余弦变换（LDCT）提取局部纹理特征，并引入对噪声具有鲁棒性的改进局部二值模式（ILBP），将二者结合形成新的特征描述子BPD-LDCT，然后用非线性SVM进行分类。在两个阶段上进行评估：第一阶段区分良性与恶性结节，第二阶段对恶性结节进一步分级（TI-RADS 4/5）。

Result: 该系统在公开数据集TDID和AUITD上的分类准确率极高：第一阶段在TDID上近100%、AUITD上97%；第二阶段在TDID上近100%、AUITD上99%。

Conclusion: 结合LDCT和ILBP特征的新型BPD-LDCT特征描述子可以显著提升甲状腺癌超声影像的分类准确率，具有极高的应用潜力。

Abstract: In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.

</details>


### [6] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为StereoAdapter的参数高效自监督水下立体视觉深度估计算法，通过结合LoRA适配的大型单目视觉基础模型和循环立体匹配细化模块，实现了在有限标注数据下更准确、鲁棒的三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 水下机器人如导航、检测和地图构建等任务需要高精度的三维几何信息。目前主流的水下深度估计算法面临两大挑战：一是大规模视觉基础编码器难以在水下领域进行高效参数和无监督适配，二是如何融合全局一致但尺度模糊的单目深度先验和局部精度高但容易受光照影响的双目深度信息。

Method: 作者提出了StereoAdapter框架，采用基于LoRA的单目基础模型高效自适应，并设计了循环的双目细化模块。同时引入动态LoRA用于高效秩选择，并以合成大规模水下数据集UW-StereoDepth-40K先行预训练以提升网络的泛化鲁棒性。整个框架为自监督学习，无需大规模标注。

Result: 在TartanAir和SQUID这两个基准测试集上，StereoAdapter相较最新方法分别提升了6.11%和5.12%。在实际BlueROV2水下机器人平台上部署也能呈现出一致的性能与鲁棒性提升。

Conclusion: 该方法实现了水下深度估计网络的参数高效自适应，在有限标注和复杂环境下展现了优越性能，为实际水下机器人任务提供了高性价比的三维视觉解决方案。

Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [7] [AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](https://arxiv.org/abs/2509.16421)
*Aiden Chang,Celso De Melo,Stephanie M. Lukin*

Main category: cs.CV

TL;DR: 提出Aha框架，在不访问未来视频帧的条件下实现基于自然语言任务的实时视频高光片段检测，性能优于主流离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解与高光检测方法大多假设可获得完整视频，不适用于流式、在线等实时场景，而实时环境（如自动驾驶、无人机监控、灾难救援等）对逐帧推理提出了新需求。

Method: Aha为自回归高光检测框架，使用多模态视觉-语言模型与精简、解耦的头部，输入连续视频帧和自然语言任务描述，无需未来帧。通过Dynamic SinkCache机制，实现无限长度流的常量内存消耗，保障可扩展性。

Result: Aha在TVSum和Mr.Hisum高光检测基准上分别比最佳离线模型提升5.9%和8.3%的mAP（平均精度），且 surpassing 了全上下文的视频-语言模型。

Conclusion: Aha能实现配置自然语言任务条件下的实时、逐帧视频理解和推理，适用于机器人等实际高实时性场景，在下游规划与长时理解任务中展现高应用潜力。

Abstract: Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).
We explore Aha's potential for real-world robotics applications given a
task-oriented natural language input and a continuous, robot-centric video.
Both experiments demonstrate Aha's potential effectiveness as a real-time
reasoning module for downstream planning and long-horizon understanding.

</details>


### [8] [3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction](https://arxiv.org/abs/2509.16423)
*Maria Taktasheva,Lily Goli,Alessandro Fiorini,Zhen,Li,Daniel Rebain,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 该论文提出了一种结合2D/3D高斯的新型混合表征方法，实现了对室内场景高质量重建，尤其解决了现有神经辐射场方法在平坦无纹理区域表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经辐射场的重建方法在生成真实感数字孪生方面取得了进展，但面对平坦、无纹理的表面时，生成的重建往往不均匀且半透明，影响视觉和几何准确性。传统表面重建虽能解决问题，但牺牲了视觉质量，因此亟需兼顾两者优势的新方法。

Method: 作者提出了一种2D/3D高斯混合表示：对于平坦表面，动态检测并用约束的平面2D高斯建模，其他区域则用自由形态的3D高斯表示。通过端到端联合优化，提升平面与非平面区域的重建质量。

Result: 该方法在ScanNet++和ScanNetv2数据集上取得了最优的深度估计性能，在不依赖特定相机模型的情况下实现了优越的网格提取效果，显著提升了室内场景重建质量。

Conclusion: 本文方法有效兼顾了视觉与几何重建质量，尤其改善了平坦无纹理区域的表现，显示出在高质量室内场景重建中的广泛适用性。

Abstract: Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.

</details>


### [9] [TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks](https://arxiv.org/abs/2509.16429)
*Itzik Waizman,Yakov Gusakov,Itay Benou,Tammy Riklin Raviv*

Main category: cs.CV

TL;DR: 本文提出了一种结合Transformer和CNN的全新白质束成像（tractography）方法，用于更精准地重建大脑内神经纤维路径。


<details>
  <summary>Details</summary>
Motivation: 白质束成像在重建大脑神经纤维时，常受噪声和复杂结构（如纤维交叉、分叉等）影响，现有方法在精度和路径完整性上存在限制。

Method: 利用Transformer模型捕捉神经纤维流的序列特征，并结合CNN提取局部体素的微结构信息，两者融合后进行神经纤维方向的预测。

Result: 在Tractometer工具包上，该方法展现出与现有最先进方法相当乃至更优的性能，并在TractoInferno真实数据集上证明了良好的泛化能力。

Conclusion: 融合Transformer与CNN的模型能有效提升白质神经通路的成像精度与完整性，对脑连接图谱的构建具有重要意义。

Abstract: White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.

</details>


### [10] [Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation](https://arxiv.org/abs/2509.16436)
*Zhejia Zhang,Junjie Wang,Le Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于mmFormer的新型多模态MRI分类模型，通过自适应模块应对现实场景中常见的模态缺失问题，在LiFS任务上取得了较好的准确率和AUC表现。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI在实际临床中常因设备差异或患者配合问题导致模态缺失，严重影响下游模型的表现。因此需要设计可适应任意模态缺失的鲁棒模型，以提升医疗AI在真实场景中的应用价值。

Method: 该方法以mmFormer为基础，保留其多模态特征提取结构，增加缺失模态补偿模块（包含零填充、可用性掩码和可训练统计参数的Delta函数），实现对缺失模态的信息动态重建。此外，采用交叉验证融合策略，集成多模型预测来提升最终性能。

Result: 在CARE 2025挑战赛LiFS任务的测试集上，该模型在同分布设备上的肝硬化检测和显著纤维化检测准确率分别为66.67%、74.17%，AUC分别为71.73%、68.48%。

Conclusion: 本研究证明了增强mmFormer结构和集成补偿策略，使MRI多模态分类模型更具鲁棒性和实用性，有望推动其在多模态不全的临床现实世界的推广应用。

Abstract: In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.

</details>


### [11] [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
*Mohamed Eltahir,Osamah Sarraj,Abdulrahman Alfrihidi,Taha Alshatiri,Mohammed Khurd,Mohammed Bremoo,Tanveer Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种三阶段的自动化框架AutoArabic，利用大语言模型将主流英文视频-文本检索数据集自动本地化为现代标准阿拉伯语，并显著降低人工修改量，提升阿拉伯语评测便利性。


<details>
  <summary>Details</summary>
Motivation: 当前视频-文本互检领域主要以英文数据集为主，阿拉伯语资源缺乏，评测和本地化难以规模化。该领域急需自动化、高效的阿拉伯语本地化工具，以促进多语种研究和公平评估。

Method: 作者设计了三阶段流程：1）用SOTA大语言模型将英文检索数据集自动翻译为阿拉伯语，2）引入自动错误检测模块，高精度地标记潜在翻译问题，3）可选人工后编辑以进一步优化文本描述。框架在DiDeMo数据集上进行了实证应用。

Result: 框架应用于DiDeMo数据集后生成了40,144条流畅阿拉伯语描述（DiDeMo-AR）。错误检测准确率达97%；人工检查量减少近四倍。基于CLIP架构的模型实验证明阿拉伯语本地化后的难度与英文版本相当（R@1性能仅低约3%）。后编辑越充分，检索效果越好，完全依赖LLM输出仍具可用性。

Conclusion: AutoArabic有效推动了视频检索数据的自动阿拉伯语本地化，为后续阿拉伯语及多语言数据集构建提供了可复现工具和经验，同时通过性能分析证明自动本地化的实际效用与局限。代码已开源，便于他人拓展至其他语言。

Abstract: Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.

</details>


### [12] [KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models](https://arxiv.org/abs/2509.16452)
*Son Hai Nguyen,Diwei Wang,Jinhyeok Jang,Hyewon Seo*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉-语言模型（VLMs）与领域知识的方法，实现了对室内日常动作的高精度视频识别。通过可学习的文本提示嵌入，显著提升了动作识别性能，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在复杂真实环境中安全可靠地识别动作面临挑战，尤其是仅依靠视频信号时。提升基于视频的动作识别准确率对于机器人应用至关重要。

Method: 作者将类别级动作的文本描述作为可学习的prompt，嵌入到冻结的预训练视觉-语言模型中，并探索了多种文本描述的结构化和编码方式，仅用RGB视频作为测试输入。

Result: 在ETRI-Activity3D数据集上，新方法识别准确率超过95%，优于当前最优的动作识别模型。

Conclusion: 将领域知识丰富的文本prompt与VLM结合能在极少监督的条件下显著增强机器人对复杂日常动作的感知与识别能力。

Abstract: Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.

</details>


### [13] [Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models](https://arxiv.org/abs/2509.16472)
*Parth Agarwal,Sangaa Chatterjee,Md Faisal Kabir,Suman Saha*

Main category: cs.CV

TL;DR: 本论文提出了一种结合1D和3D分支的CNN-LSTM框架，集成多数据集，实现了高准确率与出色可解释性，用于步态分析。


<details>
  <summary>Details</summary>
Motivation: 目前主流的步态分析模型大多缺乏可解释性，且通常只依赖单一数据集，难以同时兼顾通用性与解释性，限制了其在临床和生物识别等多领域的应用。

Method: 作者提出了一种双分支的CNN-LSTM框架。其中1D分支基于GAVD数据集的关节特征，3D分支基于OU-MVLP数据集的人体轮廓。同时，模型采用SHAP分析时间归因信息，和Grad-CAM实现空间定位，提高模型可解释性。

Result: 在保留测试集上的实验结果显示，系统达到了98.6%的准确率，并且在召回率和F1分数上均表现优异。

Conclusion: 本文方法在兼顾识别准确率的同时，增强了模型的可解释性，推动了步态分析在临床诊断与生物识别领域的进步。

Abstract: Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.

</details>


### [14] [Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion](https://arxiv.org/abs/2509.16474)
*Gabrielle Chavez,Laureano Moro-Velazquez,Ankur Butala,Najim Dehak,Thomas Thebaud*

Main category: cs.CV

TL;DR: 本文提出了一种联合利用时间序列与手写图像信息的神经网络分类框架，有效提升了神经障碍（如帕金森和阿尔茨海默病）诊断中手写分析的准确性及泛化性，在多个数据集及任务（如画钟和螺旋任务）上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征工程或计算机视觉的手写分析方法，难以在不同数据集间泛化，且无法同时充分利用手写数据的时序和图像特征，因此需要设计能够融汇这两类信息的新方法，以更精准地辅助神经障碍疾病检测。

Method: 基于ResNet50（在ImageNet-1k预训练）模型，提出了一种联合分类框架，将手写的时间序列数据和图像数据协同输入，进行二分类实验，评估其在现有时间序列数据集和图像数据集上的表现。

Result: 实验表明，该模型在NeuroLogical Signals（NLS）数据集的特定手写任务（如Draw Clock和Spiral任务）表现出显著提升，并在跨数据集、多数据集实验中均取得F1 score高达98的优异成绩，尤其是在帕金森病检测任务上。

Conclusion: 该模型能够充分挖掘不同形态手写信号的特征，显著提升神经障碍运动障碍检测的准确性与泛化能力，展现了其在神经障碍辅助诊断领域的应用潜力。

Abstract: Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.

</details>


### [15] [Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs](https://arxiv.org/abs/2509.16476)
*Qinyu Chen,Jiawen Qi*

Main category: cs.CV

TL;DR: 本论文提出了GazeVLM，一种利用人眼视线信息提升视觉-语言模型（VLM）推理效率的方法，在大幅减少推理计算的同时仍保持甚至提升了问答任务的质量。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在执行复杂语言-视觉任务时，视觉token存在大量冗余，导致推理效率低，特别限制了其在AR/VR等边缘设备上的实时应用。当前主流的视觉token精简方法通常需要修改架构或增加额外推理模块，计算和内存开销大，并且易出现关注区域和提示词不对齐问题，特别在场景或提示变化时容易漏掉细粒度细节。针对这些问题，作者希望找到一种无需复杂模块即可高效推理的方法，并且能准确覆盖用户真正关注的图像内容。

Method: 提出GazeVLM框架，在不需额外训练的情况下，直接利用人类视线信息作为自然监督信号，自动确定图像中真正关心的ROI（兴趣区域），并可选结合低分辨率全局视图，模仿人眼的中心-外围感知机制，从而大幅减少不相关视觉token输入，保留关键细节。该方法无需模型结构修改或中间激活访问，便于直接部署。作者在Qwen2.5-VL-3B/7B模型和VOILA-COCO gaze benchmark上进行了视觉问答实验。

Result: GazeVLM最多可减少93.1%的视觉token、59.6%的总token、50%的FLOPs，同时回答质量优于或持平于全分辨率基线。问答质量通过GPT-4o进行pairwise判分，包含覆盖度、准确性、细节和流畅性等多维度综合考量。实验证明在大幅提升效率的前提下，精度和细节均无明显降低。

Conclusion: 通过将人工视线引入VLM推理过程，无需训练与架构修改，GazeVLM可显著提升效率并保持任务相关细节，为边缘设备上的高效VLM推理提供了一条简便可行的路径，在实际消费级应用中具有广泛前景。

Abstract: Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.

</details>


### [16] [Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture](https://arxiv.org/abs/2509.16479)
*Christopher Silver,Thangarajah Akilan*

Main category: cs.CV

TL;DR: 本研究提出一种基于热成像和BiConvLSTM模型的摔倒检测方法，大幅提升了检测准确率并兼顾用户隐私，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 传统摔倒检测方法（如可穿戴传感器、环境传感器和RGB视觉系统）在可靠性、用户依从性和隐私方面存在不足。照护机构和老年用户更偏好无需配戴、被动型且保护隐私的实时检测系统。

Method: 采用增强了空间、时间、特征、self及general注意力机制的双向卷积LSTM（BiConvLSTM）模型，系统实验证明了不同注意力机制、循环模块和运动流集成后对检测性能的影响，并筛选出表现最佳的模型结构。

Result: 在TSF数据集上，该方法达到99.7%的ROC-AUC分数，并在新出现且多样化且注重隐私保护的TF-66基准数据集上显示出良好的泛化能力。

Conclusion: 该方法在热成像摔倒检测领域达到了最新的性能标准，具备实际部署潜力，有望成为高性能、隐私友好的摔倒检测解决方案。

Abstract: Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.

</details>


### [17] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的统一框架Octree Latent Semantic Diffusion，实现了3D语义场景的补全、扩展和生成，能够高效处理室内外场景，并在实际机器人感知任务中展现出强大的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景补全、扩展和生成方法通常针对单一任务、单一场景类型（如室内或室外），模型之间割裂且难以泛化，无法满足机器人自主导航和探索对于多功能、多场景兼容性的需求。

Method: 提出了一种直接在双八叉树图隐空间操作的新方法：首先通过结构扩散预测八叉树分割，构建稀疏高效的场景结构；随后进行语义扩散，在构建的八叉树结构上生成隐语义嵌入，并经图VAE解码为体素级的语义标签。对于场景补全和扩展，采用推理时的掩码或外扩生成技术，利用局部激光雷达数据作为条件，无需重新训练或微调。

Result: 方法能够输出高质量的场景结构和连贯的语义信息，实现了从单次激光扫描数据的鲁棒补全，并且在面对分布外数据时展示了零样本泛化能力。

Conclusion: 在双八叉树图隐空间进行生成式场景补全和扩展，是替代传统回归管线、面向真实机器人认知任务的高效可扩展方案。

Abstract: The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.

</details>


### [18] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 该论文发现当前最新的视频生成模型在视觉上虽然逼真，但存在微妙的几何畸变，影响自动驾驶感知任务的效果。提出了RLGF方法，通过集合几何反馈强化学习优化生成模型，大幅提升了合成视频的几何准确性和下游3D检测效果。


<details>
  <summary>Details</summary>
Motivation: 合成数据对于自动驾驶系统至关重要，但目前合成视频存在几何畸变，导致感知模型精度大幅下降。作者发现使用合成数据和真实数据在3D检测等感知任务上有明显性能差距，亟需解决其几何真实性不足的问题。

Method: 提出了带有几何反馈的强化学习（RLGF）方法，利用下游自动驾驶感知模型在潜空间中的反馈作为奖励来优化扩散视频生成模型。方法核心包括：高效的潜空间窗口优化技术，针对性反馈，层次化几何奖励机制，从点、线、面和场景占据一致性多个层次提供奖励。此外，提出了评估几何畸变的新指标GeoScores。

Result: 在DiVE模型和nuScenes数据集上应用，RLGF减少了21%的消失点误差和57%的深度误差，3D目标检测mAP提升了12.7%，显著缩小了合成数据与真实数据之间的性能差距。

Conclusion: RLGF作为一种即插即用的优化方案，极大提升了合成视频的几何真实性和实际可用性，为自动驾驶开发提供了更可靠的训练素材。

Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.

</details>


### [19] [CommonForms: A Large, Diverse Dataset for Form Field Detection](https://arxiv.org/abs/2509.16506)
*Joe Barrow*

Main category: cs.CV

TL;DR: 本文提出了CommonForms，一个面向网页级表单字段检测的大规模数据集，并提出了表现出色的新模型FFDNet系列。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、高质量、公开可用的表单字段检测数据集和相关高效模型，且现有商业方案不支持所有字段类型，如复选框。

Method: 作者通过对Common Crawl中的PDF进行过滤，构建了涵盖多语言、多领域、超过45万页的CommonForms数据集。将表单字段检测任务转为图像中的目标检测问题，并开发了FFDNet-Small与FFDNet-Large两种模型进行实验与评估。

Result: FFDNet-Small和FFDNet-Large在CommonForms测试集上取得了很高的检测精度，训练成本低于500美元。消融实验验证了高分辨率输入和数据清洗的重要性。实验中，该方案优于流行商业PDF表单工具，尤其能够检测复选框等字段。

Conclusion: CommonForms是首个面向表单字段检测的大规模开放数据集，FFDNet系列是首批开源的针对该任务的高效模型。数据集和代码将对推动该领域研究具有重要价值。

Abstract: This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms

</details>


### [20] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出了一种用于实际视频超分辨率（VSR）的单步扩散模型（OS-DiffVSR），通过邻帧对抗训练和多帧融合机制，在提升推理效率的同时，保证了视频质量和时序一致性，其性能在多个基准数据集上超过了现有的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频超分中取得良好质量，但推理速度慢、帧处理慢且音画一致性难以兼顾，亟需提高推理效率且不损失质量。

Method: 提出OS-DiffVSR模型，通过邻帧对抗训练提升帧间一致性和视效质量，同时引入多帧融合机制减少视频闪烁，只需一步采样即可还原高质量视频。

Result: 大量实验表明，OS-DiffVSR在多个VSR热门基准数据集上，质量超过了需要多步采样的现有扩散方法，同时提升了推理速度。

Conclusion: OS-DiffVSR有效解决了扩散模型在实际VSR中的效率与质量难以兼得的问题，为高效高质的视频超分辨率提供了新的方向。

Abstract: Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.

</details>


### [21] [SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2509.16509)
*Haijin Zeng,Xuan Lu,Yurong Zhang,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双速学习框架（SlowFast-SCI），用于高效、可自适应的光谱压缩成像（SCI）重建，不仅提升了泛化能力，还显著减少了计算量和参数。


<details>
  <summary>Details</summary>
Motivation: 传统的深度展开方法只能缓慢累积知识，缺乏对新光学配置的快速适应性，导致在训练时未见的相机或者特定光谱场景下表现不佳，同时计算和推理开销大。作者希望通过引入快速学习机制来同时获得鲁棒性和自适应性。

Method: 提出SlowFast-SCI双速框架，包括慢速阶段（预训练或复用基干网络，蒸馏成紧凑快速模型）和快速阶段（每个块嵌入轻量自适应模块，测试时自监督学习，无需回传重训练主骨干）。

Result: SlowFast-SCI相比原有方法，参数与FLOPs降低70%以上、OOD（分布外）数据PSNR提升最高达5.79dB，跨域适应性得以保持，适应速度提升4倍。

Conclusion: 该方法首次在深度展开框架中引入测试时快速自适应机制，兼容任意深度展开网络，为自适应光谱重建和现场可部署的计算成像提供了新范式。

Abstract: Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.

</details>


### [22] [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
*Burak Satar,Zhixin Ma,Patrick A. Irawan,Wilfried A. Mulyawan,Jing Jiang,Ee-Peng Lim,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 本文提出了一个新基准——Seeing Culture Benchmark (SCB)，用于考察多模态视觉-语言模型在文化推理任务中的表现，特别关注东南亚多元文化场景。SCB数据集包含1,065张覆盖138种文化物件、5大类别、7个东南亚国家的图片，并配有3,178道问题。评测显示当前VLM在文化推理和空间定位结合方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着多模态视觉-语言模型和文化相关数据集的进步，当前数据集在文化推理深度和多样性上仍有显著不足，尤其是对许多被低估的文化的表现有限。因此，作者希望推动模型跨文化推理能力的发展，并为东南亚等多文化地区打造衡量标准。

Method: SCB设置为两阶段任务。第一阶段是多选视觉问答，要求模型在相同类别但国家来源不同的选项中选出正确图片；第二阶段，要求模型在选对图片后，对相关文化物品进行分割以证明推理。所有选项设计均保证来自单一类别但区分国家来源。数据集由人工精心注释。

Result: 实验评测了多个主流多模态模型，发现它们在处理文化推理和空间定位（如分割证据）时表现不佳，特别是在复杂或富含文化信息的场景中。实验明确揭示了当前模型的不足。

Conclusion: SCB为多模态模型的文化推理能力提供了新的评测基准，揭示了模型当前在视觉推理与空间定位结合方面的短板，对未来文化推理领域的模型改进与研究具有指导意义。

Abstract: Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture

</details>


### [23] [FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers](https://arxiv.org/abs/2509.16518)
*Sankeerth Durvasula,Kavya Sreedhar,Zain Moustafa,Suraj Kothawade,Ashish Gondimalla,Suvinay Subramanian,Narges Shahidi,Nandita Vijaykumar*

Main category: cs.CV

TL;DR: 本文提出了一种用于长序列Diffusion Transformer的视频生成任务的细粒度（fine-grained）稀疏注意力机制FG-Attn，有效提升了计算效率，显著加快了视频生成速度。


<details>
  <summary>Details</summary>
Motivation: 在基于Diffusion Transformer的视频生成任务中，注意力机制需要处理极长序列（上万embedding），导致计算量巨大，尤其在短视频生成中成为主要延迟瓶颈。现有方法大多使用块状稀疏注意力，仅减少部分计算量，未能充分利用注意力矩阵的稀疏特性。

Method: 作者提出细粒度稀疏注意力（FG-Attn），对注意力矩阵按Mx1纵向切片而非传统的MxM区块进行稀疏过滤，只计算与查询最相关的key。为此，设计了异步收集合并（asynchronous-gather load）批量读取机制，可高效从显存中收集与查询相关的key-value信息，并在GPU共享内存中紧凑排列。

Result: 在视频Diffusion模型上测试，该方法在单张H100 GPU上，对5秒480P视频平均提速1.55倍（最高1.65倍）；对5秒720P视频平均提速1.41倍（最高1.49倍）。

Conclusion: 本文的细粒度稀疏注意力机制较现有块状稀疏方法更高效地剔除了无效计算，显著提升了视频生成速度，对长序列Diffusion Transformer模型具有实际应用价值。

Abstract: Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.

</details>


### [24] [PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality](https://arxiv.org/abs/2509.16519)
*Yang Han*

Main category: cs.CV

TL;DR: 本文推出了PM25Vision（PM25V）数据集，这是迄今为止规模最大、最全面的街景图像对应PM2.5空气质量数据集，为相关研究提供了全新的资源。


<details>
  <summary>Details</summary>
Motivation: 空气质量监测（尤其是PM2.5浓度）对公众健康至关重要，但传统的监测站覆盖有限，且现有数据集规模和空间精度都存在不足，制约了基于视觉的空气质量估计模型的发展。

Method: 收集并整理了覆盖3261个空气质量监测站和11年的街景图像及其匹配的PM2.5读数，总计11114张图片。通过精确的数据同步和清洗流程，实现了5公里的空间精度，并以CNN与Transformer为基线模型，测试了视觉估算效果。

Result: 与已有数据集相比，PM25V在样本数、时间跨度和空间精度等方面显著提升，能够更好地支持模型在空气质量估算任务上的训练与评估。基线模型的实验结果验证了数据集的可用性。

Conclusion: PM25V数据集为街景图像预测PM2.5浓度提供了高质量、公开可用的基础资源，有望推动视觉空气质量估算领域的研究进步。

Abstract: We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.

</details>


### [25] [Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity](https://arxiv.org/abs/2509.16527)
*Guangze Zheng,Shijie Lin,Haobo Zuo,Si Si,Ming-Shan Wang,Changhong Fu,Jia Pan*

Main category: cs.CV

TL;DR: 本文提出了用于视觉跟踪的格子玻尔兹曼模型（LBM），能够有效地分解和预测像素的动态变化，并通过多项基准测试验证其实用性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉跟踪任务中，如何实时高效地追踪对象的像素动态，特别是在复杂真实世界场景下，仍然是难点。传统方法在时空建模和像素层级动态刻画上存在瓶颈，难以适应在线和大规模应用需求。

Method: 该方法利用格子玻尔兹曼模型（LBM）将视觉表示分解为动态像素格点网络，并通过多层预测-更新网络估计目标像素位置及可见性。在预测阶段，设计空间邻域像素的格点碰撞与时序上下文中的格点流动；更新阶段结合在线视觉特征修正像素分布。

Result: 在TAP-Vid、RoboTAP点追踪基准及TAO、BFT、OVT-B等大规模开放世界目标跟踪基准上，LBM展现了优异的效率和实时实用性。

Conclusion: LBM不仅能高效地建模像素级动态，还兼具在线和实时处理能力，对实际视觉跟踪任务具备很强适应性和推广性。

Abstract: This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.

</details>


### [26] [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
*Shubhashis Roy Dipta,Tz-Ying Wu,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种全新的、无需参考（reference-free）的多模态视频字幕评估方法VC-Inspector，通过利用大模型自动生成伪字幕训练评估器，不依赖人工标注，实现高效准确的视频字幕质量评估。该方法在人类一致性和跨领域泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 已有视频字幕评估方法高度依赖人工标注的参考字幕，导致在新领域和真实环境下难以进行准确评估。而获取地面真实字幕开销巨大甚至不可行，造成现有监督方法和协议不适用于大规模、多样化视频场景。

Method: 作者提出了一套基于事实准确性、无需参考字幕的新评估框架VC-Inspector：利用大模型生成高低质量伪字幕，监督训练多模态评估模型（举例Qwen2.5-VL）；该方法聚焦事实检验，不依赖真实字幕。评估时，模型只需输入视频和待评字幕即可判断字幕质量。

Result: VC-Inspector在VATEX-Eval数据集上的评测结果与人工评价的相关性更高，优于当前最优方法，并且在Flickr8K等图像字幕数据集上也具有较好泛化能力。

Conclusion: VC-Inspector为视频字幕自动化、可扩展和客观评估提供了新方案，减少人工依赖，提升了跨领域和实际应用中的评测有效性与普适性，为多样化视频内容的自动评估奠定了基础。

Abstract: Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.

</details>


### [27] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于Rectified Flow的高效单步扩散模型RFfusion，用于图像融合，极大提升了推理速度且保持高质量融合。还设计了面向融合的VAE架构和双阶段训练策略，使模型兼顾细节与效率。实验显示RFfusion在速度和质量上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在图像融合上取得进展，但推理复杂、速度慢，影响了实际应用。因此需要开发高效且效果优良的新模型。

Method: 1）将Rectified Flow引入扩散模型，实现无额外训练的单步采样；2）设计了针对图像融合的VAE架构，将融合操作嵌入潜空间以进一步降低计算复杂度；3）创新提出双阶段训练策略提升模型融合能力与结构细节保留。

Result: RFfusion在多个实验中，在推理速度和融合质量上都超过了现有主流方法。

Conclusion: RFfusion为图像融合任务带来了新的高效解决方案，为实用化扩散模型提供了方向，其高融合质量及速度优势明显。

Abstract: Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.

</details>


### [28] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文提出了一种叫做时空高斯喷溅（ST-GS）的新框架，显著提升了3D占据预测的空间与时间一致性，在nuScenes数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 3D占据预测对于自动驾驶场景理解至关重要。已有高斯方法虽然能降低计算开销，但在多视图空间交互和多帧时间一致性上存在不足，需要进一步提升效果来满足实际需求。

Method: 提出ST-GS框架，核心包括两个创新点：一是引入指导性空间聚合策略，结合双模注意力机制以增强高斯表示中的空间交互；二是设计了几何感知的时间融合方案，有效结合历史数据、提升场景补全的时序连续性。

Result: 在大规模nuScenes占据预测基准测试中，ST-GS不仅取得了当前最优的性能，还明显提升了时间一致性，超过了以往所有基于高斯的方法。

Conclusion: ST-GS通过创新性的空间—时间建模，兼顾效率与表现，显著推进了基于视觉的自动驾驶3D占据预测技术的发展。

Abstract: 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.

</details>


### [29] [Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose](https://arxiv.org/abs/2509.16557)
*Muhammad Hamza,Danish Hamid,Muhammad Tahir Akram*

Main category: cs.CV

TL;DR: 本文提出了I2S (Interact2Sign)多阶段框架，通过分析3D手部姿态，实现对人与物体交互的识别，以及基于此的用户身份识别，适用于AR安全等高风险环境。该方法在多个数据集上获得了较高的识别准确率，并且模型小巧，适合实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 在航空航天维修、手术等对安全和个性化有极高需求的AR应用场景中，如何准确地识别人及其与物体的交互，是实现个性化辅助与安全认证的关键。现有方法面临精度、实时性和设备资源的瓶颈，因此需要一种高效、准确、轻量级的用户身份识别方法。

Method: I2S框架通过分析第一视角（egocentric）视频中3D手势，分阶段完成（1）物体类别识别、（2）人-物交互识别，以及（3）身份识别。其核心在于精心设计并提取空间、频域、运动学、方向性以及新颖的“手间空间包络（IHSE）”等特征，并通过特征增强与消融实验优化特征组合。最后在公开数据集上验证方法性能。

Result: I2S框架在ARCTIC和H2O等双手对象操作数据集上实现了用户识别平均F1值97.52%。模型体积小于4MB，单次推理耗时仅0.1秒，在工业级实时应用中表现出色。消融实验进一步验证了各特征类别和一体化策略的效用。

Conclusion: I2S框架在保持高识别率和实时性、小模型体积的同时，极大提升了在AR安全及个性化应用场景的实用价值，为在安全关键、资源受限环境下实现高效用户身份认证提供了一种可行的解决方案。

Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.

</details>


### [30] [Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization](https://arxiv.org/abs/2509.16560)
*Ji Soo Lee,Byungoh Ko,Jaewon Cho,Howoong Lee,Jaewoon Byun,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出了CaRe-DPO，一个优化文本-视频检索的检索驱动生成方法，利用偏好优化策略提升辅助字幕的判别能力，显著增强了检索表现。


<details>
  <summary>Details</summary>
Motivation: 用在文本-视频检索中提升辅助字幕判别力。传统生成的字幕过于泛化，难以区分相似视频，且现有评估指标与检索目标不符，限制了实际检索效果。

Method: 提出CaRe-DPO框架，采用Dual-Group Direct Preference Optimization（DG-DPO）策略，以检索相关性分数直接优化字幕生成。并提出带role-embedding的多模态大模型（MLLM）检索器，区分辅助字幕与检索查询等不同文本类型。

Result: 实验显示，CaRe-DPO能够生成更具区分性的辅助字幕，并显著提升文本-视频检索的性能。

Conclusion: 检索驱动的字幕生成方法能更有效促进文本-视频检索，并为今后字幕生成和多模态检索方法提供新的思路。

Abstract: In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.

</details>


### [31] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的黑盒反事实生成框架，无需训练即可根据理论保证逐步生成最优编辑，实现了更具人类水平的解释性反事实生成。方法利用预训练扩散模型编辑图片，无需分类器内部信息，并通过包括CNN、ViT和LVLM的分类器以及人的评价，突出模型与人类推理间的解释性差距。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒反事实生成框架忽视了编辑的语义内容，且依赖大量训练。作者希望实现不依赖训练、能理解语义、结果更具人类解释性的反事实生成方法。

Method: 提出一种即插即用的黑盒反事实生成框架。方法利用理论上的最优编辑策略，结合预训练的图像编辑扩散模型，在无需训练和不访问分类器内部信息的情况下，逐步建议编辑步骤，生成具有解释性的反事实样本。

Result: 实验证明该框架在CNN、ViT及大规模视觉语言模型（LVLM）等多种分类器下均有效，通过全面的人类评测，验证了其生成的反事实更加接近人类理解。同时，实验展示了神经网络模型行为与人类推理之间仍存在明显解释性差距。

Conclusion: 提出的黑盒反事实生成框架无需训练即可实现具有人类水平解释性的结果。方法不仅提升了反事实解释的可用性和合理性，还帮助揭示了深度模型与人类推理之间的本质差异。

Abstract: Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.

</details>


### [32] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文提出了一种新型自监督指标DeepSSIM，用于衡量深度生成模型中的记忆现象，有效识别医疗影像生成中的患者信息泄漏风险。通过在MRI生成任务上实验证明，DeepSSIM的检测性能明显优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在医疗影像中的应用广泛，但其对训练数据的记忆可能导致敏感患者信息泄漏，引发隐私安全风险。目前缺乏可扩展、高效的工具来检测和量化生成模型的记忆程度，尤其是在处理大规模数据时。

Method: 提出自监督指标DeepSSIM：将图像投影到学习得到的嵌入空间，再通过优化嵌入之间的余弦相似度以拟合真实空间中SSIM分数（结构相似性指数），并结合结构保留增强，增强对医学图像结构的敏感性，无需空间精确对齐即可判别记忆。

Result: 在两个公开脑部MRI数据集（IXI和CoRR）上，使用易记忆设置下的Latent Diffusion Model生成合成影像，DeepSSIM相较最佳现有方法，F1分数提升52.03%。

Conclusion: DeepSSIM能有效、高效地检测和量化生成模型的记忆（隐私泄漏）现象，尤其适合医学影像场景，优于现有方法，具有实际应用和研究价值。

Abstract: Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [33] [SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving](https://arxiv.org/abs/2509.16588)
*Haiming Zhang,Yiyao Zhu,Wending Zhou,Xu Yan,Yingjie Cai,Bingbing Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SQS的查询驱动式splatting预训练方法，显著提升了稀疏感知模型在自动驾驶中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏感知模型（SPMs）因不依赖显式高密度空间建构，具有高效计算优势，但如何提升其感知精度和表达力依然是挑战，尤其是在自动驾驶等高要求场景。

Method: SQS方法在预训练阶段引入一个插件模块，对稀疏查询预测3D高斯表示，通过自监督splatting，实现多视角图像与深度图重建，从而学习细粒度上下文特征。微调阶段，预训练得到的高斯查询通过查询交互机制与下游任务特定查询显式结合，兼容3D占用预测与物体检测等任务。

Result: 在自动驾驶相关的标准数据集上，SQS方法在多项基于查询的3D感知任务（如占用预测和三维物体检测）取得显著性能提升，相较现有的顶尖预训练方法，分别在占用预测mIoU和3D检测NDS上有+1.3和+1.0的提升。

Conclusion: SQS为稀疏感知模型领域引入了一种高效且通用的预训练范式，有效提升了其在自动驾驶多任务中的表现，具有重要应用前景。

Abstract: Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).

</details>


### [34] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

TL;DR: 本文提出了FakeChain基准，研究多步骤或混合Deepfake伪造对检测模型的挑战，发现现有检测器无法有效应对复杂的多步骤伪造。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测模型主要训练在单一步骤伪造上，但随着伪造技术的发展，多步骤（混合不同伪造手段）Deepfake层出不穷，现有检测模型对此行为的表现几乎未知，亟需更符合现实情况的评测体系。

Method: 作者构建了FakeChain基准，包含1、2、3步多种主流Deepfake生成器组合产生的大量混合伪造样本，通过系统分析检测模型在不同混合操作步数、生成器组合和质量下的检测表现及谱特性。

Result: 检测性能受最后一步伪造类型影响极大，如果测试分布与训练分布不同，F1分数最高下降可达58.83%。这表明现有检测器更多依赖最后一步的伪造痕迹，而非累计的操作痕迹，泛化能力有限。

Conclusion: 检测模型应明确考虑伪造的操作历史和序列，不能只依赖最后步骤；FakeChain基准有助于推动更具现实代表性与挑战性的检测研究。

Abstract: Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [35] [Describe-to-Score: Text-Guided Efficient Image Complexity Assessment](https://arxiv.org/abs/2509.16609)
*Shipeng Liu,Zhonglin Zhang,Dengfeng Chen,Liang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种融合视觉与文本语义信息的新方法来评估图像复杂度，有效提高了准确性和泛化能力。所提方法在多个数据集上优于现有主流方案。


<details>
  <summary>Details</summary>
Motivation: 现有图像复杂度评估方法大多仅依赖视觉特征，忽略了高层次语义信息，导致精度和泛化能力受限。

Method: 提出了D2S（Describe-to-Score）框架，通过预训练的视觉-语言模型为图像生成描述性文本，并设计了特征对齐和熵分布对齐机制，实现视觉与文本语义的融合。训练阶段用多模态信息，推理时仅用视觉分支，避免了多模态计算开销。

Result: 在IC9600数据集上，D2S的评估结果优于现有方法，并在无参考图像质量评价（NR-IQA）基准上也具备竞争力。

Conclusion: 多模态信息融合能显著提升图像复杂度相关任务的评估效果，所提出的D2S框架兼顾效率与说服力，展示了多模态融合方法的优越性。

Abstract: Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S

</details>


### [36] [Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model](https://arxiv.org/abs/2509.16617)
*David Kreismann*

Main category: cs.CV

TL;DR: 本研究针对城市热岛效应，利用地理空间基础模型预测城市地表温度，对未来气候情景和植被策略变化进行了模拟，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着城市化和气候变化的发展，城市热岛效应问题加剧，但传统机器学习方法由于数据有限预测不准，尤其在服务欠缺区域。

Method: 作者微调了基于全球非结构化数据训练的地理空间基础模型，用于预测未来情景下的城市地表温度，并模拟分析了不同植被策略带来的影响。

Result: 微调后的模型在像素级的温度下采样误差低于1.74°C，可与实际数据模式吻合，外推能力温度误差最高为3.62°C。

Conclusion: 地理空间基础模型具备优良的泛化和外推能力，在应对传统模型难以应用的场景中，可为城市热岛缓解措施的制订提供准确可靠的数据支持。

Abstract: As urbanization and climate change progress, urban heat island effects are
becoming more frequent and severe. To formulate effective mitigation plans,
cities require detailed air temperature data. However, predictive analytics
methods based on conventional machine learning models and limited data
infrastructure often provide inaccurate predictions, especially in underserved
areas. In this context, geospatial foundation models trained on unstructured
global data demonstrate strong generalization and require minimal fine-tuning,
offering an alternative for predictions where traditional approaches are
limited. This study fine-tunes a geospatial foundation model to predict urban
land surface temperatures under future climate scenarios and explores its
response to land cover changes using simulated vegetation strategies. The
fine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C and
aligned with ground truth patterns, demonstrating an extrapolation capacity up
to 3.62 {\deg}C.

</details>


### [37] [Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery](https://arxiv.org/abs/2509.16618)
*Pengfei Hao,Hongqiu Wang,Shuaibo Li,Zhaohu Xing,Guang Yang,Kaishun Wu,Lei Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法Surgical-MambaLLM，将Mamba2与大语言模型(LLM)结合，用于提升手术场景视觉-问题定位回答任务（Surgical-VQLA）效果，并在主流数据集上获得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景的视觉-语言回答方法难以处理文本与视觉细节间的复杂依赖，且对手术场景的空间信息感知不足，这限制了其在医疗学习和助手中的应用效果。

Method: 作者提出Surgical-MambaLLM模型，首次将Mamba2与LLM结合。核心包括跨模态双向Mamba2集成（CBMI）模块，用于更好地融合视觉与文本信息，以及专为手术场景设计的器械感知扫描（SIP）模式，以增强模型对手术场景空间信息的感知。

Result: 实验结果表明，Surgical-MambaLLM在EndoVis17-VQLA和EndoVis18-VQLA两个数据集上均优于最先进的同类方法，在Surgical-VQLA任务表现出显著提升。

Conclusion: Surgical-MambaLLM通过创新的跨模态集成与空间感知方式，显著提升了大语言模型在手术视觉场景问题定位和回答任务中的能力，推动了该领域的发展。

Abstract: In recent years, Visual Question Localized-Answering in robotic surgery
(Surgical-VQLA) has gained significant attention for its potential to assist
medical students and junior doctors in understanding surgical scenes. Recently,
the rapid development of Large Language Models (LLMs) has provided more
promising solutions for this task. However, current methods struggle to
establish complex dependencies between text and visual details, and have
difficulty perceiving the spatial information of surgical scenes. To address
these challenges, we propose a novel method, Surgical-MambaLLM, which is the
first to combine Mamba2 with LLM in the surgical domain, that leverages
Mamba2's ability to effectively capture cross-modal dependencies and perceive
spatial information in surgical scenes, thereby enhancing the LLMs'
understanding of surgical images. Specifically, we propose the Cross-modal
Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective
multimodal fusion, with its cross-modal integration capabilities. Additionally,
tailored to the geometric characteristics of surgical scenes, we design the
Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the
surgical images, enhancing the model's spatial understanding of the surgical
scene. Extensive experiments demonstrate that our Surgical-MambaLLM model
outperforms the state-of-the-art methods on the EndoVis17-VQLA and
EndoVis18-VQLA datasets, significantly improving the performance of the
Surgical-VQLA task.

</details>


### [38] [CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition](https://arxiv.org/abs/2509.16623)
*Junjie Zhou,Haijun Xiong,Junhao Lu,Ziyu Lin,Bin Feng*

Main category: cs.CV

TL;DR: 本文提出了一种结合图卷积和Transformer的骨架步态情感识别新框架CGTGait，通过创新结构增强了对全局时空特征的提取，并显著降低了计算复杂度，实现了新的性能标杆。


<details>
  <summary>Details</summary>
Motivation: 当前骨架步态情感识别方法主要关注空间和局部时序信息，未能充分挖掘长距离时序依赖，影响了识别精度和特征表达能力。因此需要一种能够同时高效提取空间拓扑和全局时序特征的方法。

Method: 提出CGTGait新架构，将图卷积用于帧级空间拓扑建模、Transformer用于全局时序依赖建模，并通过双向交叉流融合（BCSF）模块整合姿态与动作特征，实现多层特征互补。

Result: 在Emotion-Gait和ELMD两个公开数据集上实验，CGTGait取得了新的SOTA或具有竞争力的表现，推理计算复杂度比现有方法低约82.2%（仅需0.34G FLOPs）。

Conclusion: CGTGait有效结合了空间与时序特征提取，提升了步态情感识别精度，且极大减少计算量，有望推动该领域实际应用落地。

Abstract: Skeleton-based gait emotion recognition has received significant attention
due to its wide-ranging applications. However, existing methods primarily focus
on extracting spatial and local temporal motion information, failing to capture
long-range temporal representations. In this paper, we propose
\textbf{CGTGait}, a novel framework that collaboratively integrates graph
convolution and transformers to extract discriminative spatiotemporal features
for gait emotion recognition. Specifically, CGTGait consists of multiple CGT
blocks, where each block employs graph convolution to capture frame-level
spatial topology and the transformer to model global temporal dependencies.
Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to
effectively aggregate posture and motion spatiotemporal features, facilitating
the exchange of complementary information between the two streams. We evaluate
our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating
that our CGTGait achieves state-of-the-art or at least competitive performance
while reducing computational complexity by approximately \textbf{82.2\%} (only
requiring 0.34G FLOPs) during testing. Code is available at
\small{https://github.com/githubzjj1/CGTGait.}

</details>


### [39] [Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning](https://arxiv.org/abs/2509.16628)
*Janak Kapuriya,Anwar Shaikh,Arnav Goel,Medha Hira,Apoorv Singh,Jay Saraf,Sanjana,Vaibhav Nauriyal,Avinash Anand,Zhengkui Wang,Rajiv Ratn Shah*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-文本模型微调方法VCASFT，显著提升了小型视觉语言模型在科学视觉问答任务中的性能，并针对低资源语言发布了高质量印地语多模态问答数据集HiSciVQA。


<details>
  <summary>Details</summary>
Motivation: 现有小型视觉语言模型在科学视觉问答任务中表现有限，尤其在低资源语言环境下，缺乏有效的训练方法和数据集。本文试图通过新范式提升VQA表现，并补充低资源语言的训练数据短板。

Method: 提出VCASFT方法，结合图像描述、问答对和指令微调模型，提升模型效果。方法在大规模涵盖不同语言与学科的ScienceQA数据集上验证，并专为低资源场景构建HiSciVQA印地语多模态问答数据集。另外，设计基于大模型的创新自动评测方案，超越传统的n-gram准确率评估。

Result: VCASFT在ScienceQA基准上取得显著效果提升，表现出良好的可迁移性和多语种、多学科适应性。HiSciVQA数据集的引入极大促进了低资源语言VQA模型评测，自动评估体系也获得更深入有效的效果分析。

Conclusion: VCASFT为提升小型视觉语言模型在科学视觉问答任务（特别是低资源语言环境）上的表现提供了有效新途径。HiSciVQA数据集为社区扩展低资源多模态研究基础，开源所有资源助力进一步学术研究。

Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning
(VCASFT), a novel learning paradigm designed to enhance the performance of
smaller Vision Language Models(VLMs) on scientific visual question
answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts
alongside question-answer pairs and instruction-tunes models to yield
significant performance improvements. To comprehensively evaluate VCASFT, we
benchmark it on ScienceQA, which consists of questions across diverse
languages, subjects, and fields, demonstrating its adaptability and
effectiveness in a variety of educational contexts. Additionally, to further
demonstrate the effectiveness of this technique on lowresource languages, we
developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated
Hindi multimodal Q&A pairs. This dataset addresses the critical need for
low-resource language Q&A datasets and serves as a foundation for testing
VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to
evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness
surpassing traditional n-gram matching accuracy metrics. We are committed to
advancing the field by open-sourcing all code files and the HiSciVQA dataset
for the research community.

</details>


### [40] [Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation](https://arxiv.org/abs/2509.16630)
*Yue Ma,Zexuan Yan,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Zhifeng Li,Wei Liu,Linfeng Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的人像动画生成方法Follow-Your-Emoji-Faster，利用扩散模型和表情标记，显著提升了身份保留、表情迁移准确性和生成效率，并支持多种风格肖像（包括真人、卡通、雕塑和动物）。


<details>
  <summary>Details</summary>
Motivation: 人像动画生成领域面临的核心问题是如何在保持肖像身份的前提下，实现目标表情的精准转移、长时序一致性以及较高的生成效率。尤其在表情外观多样、风格丰富的肖像动画中，这些问题尤为突出。

Method: 该方法在Stable Diffusion的基础上做了两项关键改进：一是引入“表情感知的面部标记”作为运动显式信号，从而实现更好的动作对齐、表达夸张表情并减少身份混淆；二是提出基于表情和面部掩蔽的细粒度损失函数，以更好捕捉细微表情并忠实还原创意肖像。此外，为提升扩散生成的速度和长期连续性，设计了渐进式生成策略与泰勒插值缓存，实现2.6倍无损加速。

Result: 该方法适用于多种类型肖像（真人、卡通、雕塑、动物），支持可控且富有表现力的动画生成。实验证明，在新提出的大型综合评测集EmojiBench++上，Follow-Your-Emoji-Faster在动画质量和可控性方面均优于现有方法。

Conclusion: Follow-Your-Emoji-Faster在肖像动画领域有效解决了身份保留、表情迁移准确性及生成效率的难题，输出高质量动画，方法高效、用户友好，具有广泛适用性。代码与数据集已开源，为后续研究提供了良好基础。

Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework
for freestyle portrait animation driven by facial landmarks. The main
challenges in this task are preserving the identity of the reference portrait,
accurately transferring target expressions, and maintaining long-term temporal
consistency while ensuring generation efficiency. To address identity
preservation and accurate expression retargeting, we enhance Stable Diffusion
with two key components: a expression-aware landmarks as explicit motion
signals, which improve motion alignment, support exaggerated expressions, and
reduce identity leakage; and a fine-grained facial loss that leverages both
expression and facial masks to better capture subtle expressions and faithfully
preserve the reference appearance. With these components, our model supports
controllable and expressive animation across diverse portrait types, including
real faces, cartoons, sculptures, and animals. However, diffusion-based
frameworks typically struggle to efficiently generate long-term stable
animation results, which remains a core challenge in this task. To address
this, we propose a progressive generation strategy for stable long-term
animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless
acceleration. These two strategies ensure that our method produces high-quality
results efficiently, making it user-friendly and accessible. Finally, we
introduce EmojiBench++, a more comprehensive benchmark comprising diverse
portraits, driving videos, and landmark sequences. Extensive evaluations on
EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior
performance in both animation quality and controllability. The code, training
dataset and benchmark will be found in https://follow-your-emoji.github.io/.

</details>


### [41] [DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration](https://arxiv.org/abs/2509.16632)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文提出DA-Font框架，通过双重注意力机制提升少样本字体生成的质量，在结构和细节上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前少样本字体生成方法在复杂多变的字体风格下常出现笔画错误、伪影和模糊等问题，影响生成字体的实用性和美观，需要更好地建模结构与风格细节。

Method: 提出DA-Font框架，核心为Dual-Attention Hybrid Module（DAHM），包含两个协同注意力模块：组件注意力块利用内容图像的部件信息进行风格迁移，关系注意力块进一步结合原始和风格化特征来优化空间关系。还设计了角一致性损失和弹性网格特征损失提升几何对齐。

Result: 在多种风格和字符集上进行实验，DA-Font在结构完整性和局部细节保真度上优于现有主流方法。

Conclusion: DA-Font在生成高质量少样本字体方面表现优异，能提升字体生成的结构正确性和细节表现，实现更实用的字体自动生成。

Abstract: Few-shot font generation aims to create new fonts with a limited number of
glyph references. It can be used to significantly reduce the labor cost of
manual font design. However, due to the variety and complexity of font styles,
the results generated by existing methods often suffer from visible defects,
such as stroke errors, artifacts and blurriness. To address these issues, we
propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid
Module (DAHM). Specifically, we introduce two synergistic attention blocks: the
component attention block that leverages component information from content
images to guide the style transfer process, and the relation attention block
that further refines spatial relationships through interacting the content
feature with both original and stylized component-wise representations. These
two blocks collaborate to preserve accurate character shapes and stylistic
textures. Moreover, we also design a corner consistency loss and an elastic
mesh feature loss to better improve geometric alignment. Extensive experiments
show that our DA-Font outperforms the state-of-the-art methods across diverse
font styles and characters, demonstrating its effectiveness in enhancing
structural integrity and local fidelity. The source code can be found at
\href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.

</details>


### [42] [When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs](https://arxiv.org/abs/2509.16633)
*Abhirama Subramanyam Penamakuri,Navlika Singh,Piyush Arora,Anand Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种名为MPA的新方法，通过发现并弥合大模型和小模型在无标注图像上的知识鸿沟，显著提升了小型视觉-语言模型（S-VLM）的性能，缩小了其与大型模型（L-VLM）之间的差距，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型虽然精度高但计算开销大，难以在资源受限或推理密集情境中应用。而小模型虽高效却与大模型表现有明显差距，因此亟需找到提升小模型性能的新方法。

Method: 提出Model Parity Aligner（MPA）框架，不依赖标注数据，通过对比小大模型在无标注图像上的输出，精准定位其知识差异，并有针对性地优化小模型，让知识迁移更高效。

Result: 在TextVQA、ST-VQA、ChartQA和OKVQA四个需要不同推理能力的VQA基准集上，MPA显著提升了小模型表现，缩小了与大模型的性能差距，同时保持了较低的计算消耗。

Conclusion: MPA能够高效、系统地用大模型提升小模型的能力，非常适合在资源有限或对性能效率有要求的情境中应用。代码已开源。

Abstract: Large Vision-Language Models (L-VLMs) have demonstrated remarkable
performance in various vision and language tasks, including visual question
answering (VQA). However, their high computational cost makes them impractical
for resource-constrained settings and inference-heavy applications. In
contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer
from a significant performance gap compared to their larger counterparts. In
this work, we introduce the Model Parity Aligner (MPA), a novel framework
designed to systematically improve S-VLMs by leveraging unlabeled images and
effective knowledge transfer from L-VLMs. Instead of traditional knowledge
distillation methods that rely on labeled training data, MPA employs a
strategic parity-based approach that precisely identifies the knowledge
disparities between S-VLMs and L-VLMs, and optimizes training by targeting only
these disparities. We conduct extensive experiments on four diverse VQA
benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires
specialized reasoning capabilities such as text recognition, chart
interpretation, and commonsense and factual understanding. Our results
demonstrate that MPA consistently enhances the performance of S-VLMs on all
benchmarks, reducing the performance gap while maintaining computational
efficiency. We make our code publicly available.

</details>


### [43] [Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification](https://arxiv.org/abs/2509.16635)
*Xulin Li,Yan Lu,Bin Liu,Jiaze Li,Qinhong Yang,Tao Gong,Qi Chu,Mang Ye,Nenghai Yu*

Main category: cs.CV

TL;DR: 本论文提出了一个可适应于任意时间和多种场景的人体再识别（ReID）新任务——Anytime Person Re-identification (AT-ReID)，并构建了首个大规模AT-ReID数据集AT-USTC，提出统一模型Uni-AT，有效提高了多场景下的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体再识别（ReID）任务和数据集只适应限定时间和特定场景，无法满足实际应用中全天候、所有场景下的目标人物检索需求。

Method: 作者构建了一个包含 RGB 和 IR 摄像头拍摄、跨21个月的403k图像、涉及270名志愿者的AT-USTC数据集，并提出统一模型Uni-AT，包括多场景ReID特征学习框架（MS-ReID）、属性专家混合模块（MoAE）和分层动态加权策略（HDW），以增强模型的多场景泛化能力与均衡表现。

Result: 实验表明，提出的Uni-AT模型在多种场景和时间段下的检索任务中均取得了出色结果，且在泛化性方面优于现有方法。

Conclusion: 论文首次提出AT-ReID新任务并构建了相应数据集与统一模型，有效推动了人体再识别技术向真实应用需求的发展，提升了全天候多场景下的实际表现。

Abstract: In real applications, person re-identification (ReID) is expected to retrieve
the target person at any time, including both daytime and nighttime, ranging
from short-term to long-term. However, existing ReID tasks and datasets can not
meet this requirement, as they are constrained by available time and only
provide training and evaluation for specific scenarios. Therefore, we
investigate a new task called Anytime Person Re-identification (AT-ReID), which
aims to achieve effective retrieval in multiple scenarios based on variations
in time. To address the AT-ReID problem, we collect the first large-scale
dataset, AT-USTC, which contains 403k images of individuals wearing multiple
clothes captured by RGB and IR cameras. Our data collection spans 21 months,
and 270 volunteers were photographed on average 29.1 times across different
dates or scenes, 4-15 times more than current datasets, providing conditions
for follow-up investigations in AT-ReID. Further, to tackle the new challenge
of multi-scenario retrieval, we propose a unified model named Uni-AT, which
comprises a multi-scenario ReID (MS-ReID) framework for scenario-specific
features learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate
inter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)
strategy to ensure balanced training across all scenarios. Extensive
experiments show that our model leads to satisfactory results and exhibits
excellent generalization to all scenarios.

</details>


### [44] [Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination](https://arxiv.org/abs/2509.16639)
*Shangzhuo Xie,Qianqian Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的点云处理模块GF-Core，通过优化已有网络结构中的分组和特征提取环节，大幅提升了点云任务表现。此外，搭配自监督预训练策略，模型在多个数据集上表现优异，达到了先进框架的水平。


<details>
  <summary>Details</summary>
Motivation: 当前点云处理方法大多关注提出新结构，但传统点云架构（基于采样、分组、特征提取）尚有潜力未被充分挖掘。作者认为通过优化模块组合而非大幅修改结构同样能显著提升性能。

Method: 提出GF-Core模块，实现对分组层和特征提取层的协同调控，促进更细致的特征聚合。此外，设计了适用于点云输入的自监督预训练策略。整个方法注重轻量和易于集成。

Result: 在ModelNet40数据集上方法将基线网络准确率提升至94.0%，达到当前先进架构的成绩；在ScanObjectNN三种变体上分别取得2.96%、6.34%、6.32%的准确率提升。

Conclusion: 通过GF-Core模块和自监督预训练，传统点云架构可以充分释放潜力，实现与复杂方法相当的表现，兼具高性能与结构简洁，具备实际应用价值。

Abstract: Point cloud analysis has evolved with diverse network architectures, while
existing works predominantly focus on introducing novel structural designs.
However, conventional point-based architectures - processing raw points through
sequential sampling, grouping, and feature extraction layers - demonstrate
underutilized potential. We notice that substantial performance gains can be
unlocked through strategic module integration rather than structural
modifications. In this paper, we propose the Grouping-Feature Coordination
Module (GF-Core), a lightweight separable component that simultaneously
regulates both grouping layer and feature extraction layer to enable more
nuanced feature aggregation. Besides, we introduce a self-supervised
pretraining strategy specifically tailored for point-based inputs to enhance
model robustness in complex point cloud analysis scenarios. On ModelNet40
dataset, our method elevates baseline networks to 94.0% accuracy, matching
advanced frameworks' performance while preserving architectural simplicity. On
three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,
6.34%, and 6.32% respectively.

</details>


### [45] [ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents](https://arxiv.org/abs/2509.16645)
*Yichen Wang,Hangtao Zhang,Hewen Pan,Ziqi Zhou,Xianlong Wang,Peijin Guo,Lulu Xue,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种细粒度对抗攻击框架ADVEDM，能够有选择性地干扰视觉-语言模型（VLM）对关键物体的感知，使其在不破坏整体语义的情况下输出有效但错误的决策，加强了在真实环境中的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 当前针对VLM的对抗攻击方法，要么假设攻击者拥有完整模型信息（不现实），要么效果有限（因破坏大量语义导致输出无效，无法影响实际决策）。研究动机在于设计更精细、更具现实威胁的攻击方式。

Method: 提出细粒度对抗攻击框架ADVEDM，局部地更改VLM对少数关键物体的感知，保持其余区域语义不变。并实现两个变体：ADVEDM-R（移除特定物体语义）和ADVEDM-A（添加新物体语义）。

Result: 在通用场景和具体的EDM任务上，ADVEDM及其变体都展现出细致可控、高效攻击的性能，能够诱导VLM做出表面上合理但实际上错误的决策。

Conclusion: 该方法能更真实地威胁基于VLM的实体智能体，显示现有防护措施存在漏洞，需要进一步提升VLM在真实物理世界下的安全性。

Abstract: Vision-Language Models (VLMs), with their strong reasoning and planning
capabilities, are widely used in embodied decision-making (EDM) tasks in
embodied agents, such as autonomous driving and robotic manipulation. Recent
research has increasingly explored adversarial attacks on VLMs to reveal their
vulnerabilities. However, these attacks either rely on overly strong
assumptions, requiring full knowledge of the victim VLM, which is impractical
for attacking VLM-based agents, or exhibit limited effectiveness. The latter
stems from disrupting most semantic information in the image, which leads to a
misalignment between the perception and the task context defined by system
prompts. This inconsistency interrupts the VLM's reasoning process, resulting
in invalid outputs that fail to affect interactions in the physical world. To
this end, we propose a fine-grained adversarial attack framework, ADVEDM, which
modifies the VLM's perception of only a few key objects while preserving the
semantics of the remaining regions. This attack effectively reduces conflicts
with the task context, making VLMs output valid but incorrect decisions and
affecting the actions of agents, thus posing a more substantial safety threat
in the physical world. We design two variants of based on this framework,
ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific
object from the image and add the semantics of a new object into the image. The
experimental results in both general scenarios and EDM tasks demonstrate
fine-grained control and excellent attack performance.

</details>


### [46] [Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?](https://arxiv.org/abs/2509.16654)
*Xin Chen,Jia He,Maozheng Li,Dongliang Xu,Tianyu Wang,Yixiao Chen,Zhixin Lin,Yue Yao*

Main category: cs.CV

TL;DR: 本论文系统评估了视觉-语言模型（VLM）在理解路网拓扑结构上的能力，发现即便是最先进的模型在空间和时序推理方面也存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态推理任务中取得进展，但在自动驾驶关键能力——道路拓扑理解上的应用仍然有限。此前工作对VLMs在该领域的研究较少，尤其在安全导航所需的空间推理能力上的表现亟待提升。

Method: 作者将多视角图像统一投影到地面坐标系并融合为鸟瞰图车道线（BEV lanes），然后设计了四个与拓扑相关的VQA诊断任务，从不同角度评估VLMs的空间拓扑推理能力。

Result: 实验发现，闭源大模型（如GPT-4o）在部分任务上能达到较高准确率，但在某些时序问题上表现仍不及人类（如GPT-4o在向量二分类任务仅为67.8%），而开源大VLM（即便30B参数量）表现更差，表明空间推理能力仍为VLM的瓶颈。此外，模型能力与模型规模、推理token长度及示例数量正相关。

Conclusion: 当前VLM在道路拓扑推理上难以胜任自动驾驶需求，空间推理瓶颈明显。模型能力随规模和推理过程增强而提升，未来研究可针对空间推理能力和更大规模训练继续努力。

Abstract: Vision-Language Models (VLMs) have recently shown remarkable progress in
multimodal reasoning, yet their applications in autonomous driving remain
limited. In particular, the ability to understand road topology, a key
requirement for safe navigation, has received relatively little attention.
While some recent works have begun to explore VLMs in driving contexts, their
performance on topology reasoning is far from satisfactory. In this work, we
systematically evaluate VLMs' capabilities in road topology understanding.
Specifically, multi-view images are projected into unified ground-plane
coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these
BEV lanes, we formulate four topology-related diagnostic VQA tasks, which
together capture essential components of spatial topology reasoning. Through
extensive evaluation, we find that while frontier closed-source models (e.g.,
GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some
temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in
vector, a two-class classification problem). Furthermore, we find open-source
VLMs, even at 30B scale, struggle significantly. These results indicate that
spatial reasoning remains a fundamental bottleneck for current VLMs. We also
find that the model's capability is positively correlated with model size,
length of reasoning tokens and shots provided as examples, showing direction
for future research.

</details>


### [47] [MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness](https://arxiv.org/abs/2509.16673)
*Sinuo Wang,Yutong Xie,Yuyuan Liu,Qi Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态医学数据增强方法MedCutMix，在四个放射学下游任务上提升了性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于医学领域图文数据集的隐私和配对标注成本高，现有数据增强方法多样性受限，难以充分提升医学视觉-语言预训练（VLP）表现。

Method: 提出MedCutMix方法：在医学报告中进行诊断句子级的CutMix，并结合诊断句与医学图像间的跨模态注意力机制，引导影像模态内的显著区域混合，从而实现更具疾病相关性的多模态增强。

Result: 在四个放射学诊断数据集上，MedCutMix均超过了以往方法，在性能和泛化性能方面表现优越。

Conclusion: MedCutMix能够针对医学VLP任务，有效缓解数据标注和隐私问题，提升模型在相关下游任务中的表现和泛化能力。

Abstract: Vision-Language Pre-training (VLP) is drawing increasing interest for its
ability to minimize manual annotation requirements while enhancing semantic
understanding in downstream tasks. However, its reliance on image-text datasets
poses challenges due to privacy concerns and the high cost of obtaining paired
annotations. Data augmentation emerges as a viable strategy to address this
issue, yet existing methods often fall short of capturing the subtle and
complex variations in medical data due to limited diversity. To this end, we
propose MedCutMix, a novel multi-modal disease-centric data augmentation
method. MedCutMix performs diagnostic sentence CutMix within medical reports
and establishes the cross-attention between the diagnostic sentence and medical
image to guide attentive manifold mix within the imaging modality. Our approach
surpasses previous methods across four downstream radiology diagnosis datasets,
highlighting its effectiveness in enhancing performance and generalizability in
radiology VLP.

</details>


### [48] [FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World](https://arxiv.org/abs/2509.16674)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li*

Main category: cs.CV

TL;DR: FitPro是一个针对开放世界、零样本、交互式行人检索任务提出的新方法，能更好地理解语义并适应不同场景。它结合了对比解码、增量语义挖掘和分层检索三大模块，有效提升了检索系统的泛化能力和语义建模能力。实验结果显示，FitPro在多个数据集和协议下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本行人检索方法在受限制环境下虽有进展，但在开放世界中泛化能力弱、语义理解不足，导致实际交互检索效果不佳。因此有必要设计更能适应复杂应用场景，且具备更强语义理解和泛化能力的行人检索框架。

Method: 提出FitPro框架，包含三大创新模块：1) 特征对比解码（FCD）通过提示引导对比学习，从去噪图像生成高质量结构化行人描述，缓解语义漂移。2) 增量语义挖掘（ISM）融汇多视角观测，构建全局行人表示，在多轮交互中提升模型对视角切换及描述细粒度变化的鲁棒性。3) 查询感知分层检索（QHR）结合查询类型动态优化检索流程，实现对多模态多视角输入的自适应处理。

Result: 在五个公开数据集和两种评测协议上的大量实验表明，FitPro能显著克服现有方法的泛化和语义建模局限性，在交互式行人检索任务上取得优异成绩。

Conclusion: FitPro有效提升了开放世界下交互式行人检索的实际应用价值，明显改善了模型的泛化和语义理解能力，为该领域的实际部署铺平了道路，相关代码和数据也即将公开。

Abstract: Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target
pedestrians in visual scenes according to natural language descriptions.
Although existing methods have achieved progress under constrained settings,
interactive retrieval in the open-world scenario still suffers from limited
model generalization and insufficient semantic understanding. To address these
challenges, we propose FitPro, an open-world interactive zero-shot TPR
framework with enhanced semantic comprehension and cross-scene adaptability.
FitPro has three innovative components: Feature Contrastive Decoding (FCD),
Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval
(QHR). The FCD integrates prompt-guided contrastive decoding to generate
high-quality structured pedestrian descriptions from denoised images,
effectively alleviating semantic drift in zero-shot scenarios. The ISM
constructs holistic pedestrian representations from multi-view observations to
achieve global semantic modeling in multi-turn interactions,thereby improving
robustness against viewpoint shifts and fine-grained variations in
descriptions. The QHR dynamically optimizes the retrieval pipeline according to
query types, enabling efficient adaptation to multi-modal and multi-view
inputs. Extensive experiments on five public datasets and two evaluation
protocols demonstrate that FitPro significantly overcomes the generalization
limitations and semantic modeling constraints of existing methods in
interactive retrieval, paving the way for practical deployment. The code and
data will be released at https://github.com/
lilo4096/FitPro-Interactive-Person-Retrieval.

</details>


### [49] [Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence](https://arxiv.org/abs/2509.16677)
*Wenxin Li,Kunyu Peng,Di Wen,Ruiping Liu,Mengfei Duan,Kai Luo,Kailun Yang*

Main category: cs.CV

TL;DR: 本文关注于行动相关视频对象分割过程中标签噪声（包括文本提示噪声和掩码注释噪声）的问题，并首次构建了相关基准ActiSeg-NL以及系统性分析了不同噪声学习策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的行动相关视频对象分割依赖大规模标注和prompt，既昂贵又易引入多模态噪声（如不精确掩码或提示歧义），但此前关于标签噪声的影响很少有研究。本研究动机在于系统性探索标签噪声如何影响分割表现，并提升模型鲁棒性。

Method: 作者引入了两类标签噪声：文本提示噪声（类别互换和类别内同义词替换）与掩码注释噪声（边界扰动）。建立了ActiSeg-NL基准，借助六种标签噪声下的学习策略，分析它们在三类噪声环境下的表现。同时提出了并行掩码头机制（PMHM）缓解掩码注释噪声。

Result: 定性和定量评估展示了各种鲁棒性表现及失败模式（如边界泄漏、定位错误和身份替换），不同学习策略之间存在前景-背景权衡，有的更均衡，有的则注重前景。所提PMHM方法提升了对掩码注释噪声的鲁棒性。

Conclusion: 本研究首次对行动相关视频对象分割的标签噪声展开系统研究，建立了评测基准，并为后续鲁棒性提升和高效低成本标注环境下的分割研究提供了坚实基础。

Abstract: Embodied intelligence relies on accurately segmenting objects actively
involved in interactions. Action-based video object segmentation addresses this
by linking segmentation with action semantics, but it depends on large-scale
annotations and prompts that are costly, inconsistent, and prone to multimodal
noise such as imprecise masks and referential ambiguity. To date, this
challenge remains unexplored. In this work, we take the first step by studying
action-based video object segmentation under label noise, focusing on two
sources: textual prompt noise (category flips and within-category noun
substitutions) and mask annotation noise (perturbed object boundaries to mimic
imprecise supervision). Our contributions are threefold. First, we introduce
two types of label noises for the action-based video object segmentation task.
Second, we build up the first action-based video object segmentation under a
label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies
to this setting, and establish protocols for evaluating them under textual,
boundary, and mixed noise. Third, we provide a comprehensive analysis linking
noise types to failure modes and robustness gains, and we introduce a Parallel
Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative
evaluations further reveal characteristic failure modes, including boundary
leakage and mislocalization under boundary perturbations, as well as occasional
identity substitutions under textual flips. Our comparative analysis reveals
that different learning strategies exhibit distinct robustness profiles,
governed by a foreground-background trade-off where some achieve balanced
performance while others prioritize foreground accuracy at the cost of
background precision. The established benchmark and source code will be made
publicly available at https://github.com/mylwx/ActiSeg-NL.

</details>


### [50] [IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation](https://arxiv.org/abs/2509.16678)
*Suorong Yang,Hongchao Yang,Suhan Guo,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: 本文提出了IPF-RDA信息保留框架，有效增强了数据增强方法的鲁棒性，并在多个主流数据集和方法上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 数据增强虽然能提升深度模型泛化能力，但会引入分布偏移和噪声，影响模型性能。本文旨在解决这一痛点，提高数据增强的鲁棒性。

Method: IPF-RDA框架包括两项核心创新：（1）新颖的类别判别信息估计算法，用于识别易受数据增强扰动的点及其重要性；（2）新的信息保留方案，能自适应地在增强样本中保留关键信息，并确保增强数据多样性。框架还根据操作类型将数据增强方法分类，并整合入整体方案。

Result: IPF-RDA在CIFAR-10、CIFAR-100、Tiny-ImageNet、CUHK03、Market1501、Oxford Flower和MNIST等数据集、常用深度模型和多种SOTA数据增强方法下，均带来一致且显著的性能提升。

Conclusion: IPF-RDA框架简单有效，可提升多种数据增强方法的鲁棒性和性能，验证了其实用性与可扩展性。

Abstract: Data augmentation is widely utilized as an effective technique to enhance the
generalization performance of deep models. However, data augmentation may
inevitably introduce distribution shifts and noises, which significantly
constrain the potential and deteriorate the performance of deep networks. To
this end, we propose a novel information-preserving framework, namely IPF-RDA,
to enhance the robustness of data augmentations in this paper. IPF-RDA combines
the proposal of (i) a new class-discriminative information estimation algorithm
that identifies the points most vulnerable to data augmentation operations and
corresponding importance scores; And (ii) a new information-preserving scheme
that preserves the critical information in the augmented samples and ensures
the diversity of augmented data adaptively. We divide data augmentation methods
into three categories according to the operation types and integrate these
approaches into our framework accordingly. After being integrated into our
framework, the robustness of data augmentation methods can be enhanced and
their full potential can be unleashed. Extensive experiments demonstrate that
although being simple, IPF-RDA consistently improves the performance of
numerous commonly used state-of-the-art data augmentation methods with popular
deep models on a variety of datasets, including CIFAR-10, CIFAR-100,
Tiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its
performance and scalability are stressed. The implementation is available at
https://github.com/Jackbrocp/IPF-RDA.

</details>


### [51] [ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering](https://arxiv.org/abs/2509.16680)
*Xingjian Diao,Weiyi Wu,Keyi Kong,Peijun Qing,Xinwen Xu,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: 本文提出了ProtoVQA，一个为视觉问答（VQA）设计的原型学习框架，实现了高解释性和竞争性准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统虽然能给出答案，但缺乏让人信服且便于验证的解释，尤其是在安全关键领域尤为重要。已有的基于原型的可解释模型主要用于纯视觉任务，尚未充分应用到VQA中。

Method: 提出了ProtoVQA框架：（1）学习与问题相关的原型，用于将答案关联到判别性的图像区域；（2）通过空间约束匹配保证选择证据的连贯性和语义相关性；（3）使用共享原型骨干同时支持问答和证据定位。还提出了视觉-语言对齐分数（VLAS）来量化模型关注区域与真实证据的对齐程度。

Result: 在Visual7W数据集上，ProtoVQA在保持竞争性准确率的同时，能够输出更加可信和细粒度的解释。

Conclusion: ProtoVQA推动了VQA系统向透明、可解释和更值得信赖的方向发展。

Abstract: Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.

</details>


### [52] [Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels](https://arxiv.org/abs/2509.16684)
*Qi Zhang,Bin Li,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了面向场景级多视角人群计数与定位的问题，通过主动选择和利用最优视角，提高了跨场景和低标注需求下的效果，提出的AVS方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往多视角人群统计主要关注输入视角中出现的人群，忽视了如何选择能更好覆盖场景所有人群的最佳镜头；且现有方法需大量标注，难以跨场景应用。因此需研究低标注、泛化能力强的视角选择方案。

Method: 首先提出了独立视角选择（IVS）方法，基于视角及场景几何独立进行视角选择、标注和下游任务。进一步提出主动视角选择（AVS）方法，将视角选择、标注和下游任务联合优化，综合考虑几何信息和模型预测，主动选择需标注的视角。

Result: 在多视角的人群计数与定位实验中，AVS方法在跨场景能力和低标注需求下均优于传统方法，提升了结果精度并扩展了应用场景。

Conclusion: 主动视角选择方法（AVS）能高效利用有限标注，提升跨场景多视角人群统计的准确性，优于现有方法，具有更广泛的实际应用前景。

Abstract: Multi-view crowd counting and localization fuse the input multi-views for
estimating the crowd number or locations on the ground. Existing methods mainly
focus on accurately predicting on the crowd shown in the input views, which
neglects the problem of choosing the `best' camera views to perceive all crowds
well in the scene. Besides, existing view selection methods require massive
labeled views and images, and lack the ability for cross-scene settings,
reducing their application scenarios. Thus, in this paper, we study the view
selection issue for better scene-level multi-view crowd counting and
localization results with cross-scene ability and limited label demand, instead
of input-view-level results. We first propose an independent view selection
method (IVS) that considers view and scene geometries in the view selection
strategy and conducts the view selection, labeling, and downstream tasks
independently. Based on IVS, we also put forward an active view selection
method (AVS) that jointly optimizes the view selection, labeling, and
downstream tasks. In AVS, we actively select the labeled views and consider
both the view/scene geometries and the predictions of the downstream task
models in the view selection process. Experiments on multi-view counting and
localization tasks demonstrate the cross-scene and the limited label demand
advantages of the proposed active view selection method (AVS), outperforming
existing methods and with wider application scenarios.

</details>


### [53] [Towards a Transparent and Interpretable AI Model for Medical Image Classifications](https://arxiv.org/abs/2509.16685)
*Binbin Wen,Yihang Wu,Tareef Daqqaq,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文探讨了可解释人工智能（XAI）在医疗领域的应用，通过分析不同医疗数据集上的仿真，展示XAI提升AI决策透明度和可解释性的作用，并讨论了该领域面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 复杂AI模型的不透明性阻碍了其在医疗实际中的应用，亟需提升AI决策的透明度和可解释性，增强医疗人员对AI结果的信任。

Method: 采用多种医疗数据集，实现在这些数据集上的XAI模型仿真，详细分析和对比主流XAI方法，并就其如何提升AI结果的可解释性展开阐述。

Result: 仿真结果表明，XAI能够有效解释AI模型的预测结果，显著提升医疗决策的透明度和可解释性，帮助医疗人员更好地理解和应用AI辅助的诊疗建议。

Conclusion: 持续发展和深入探索XAI方法，特别是结合多样化的医疗数据集，有助于推动XAI在医疗领域的广泛应用，提高其实际效能，并促进AI技术的临床普及。

Abstract: The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.

</details>


### [54] [Spectral Compressive Imaging via Chromaticity-Intensity Decomposition](https://arxiv.org/abs/2509.16690)
*Xiaodong Wang,Zijun He,Ping Wang,Lishun Wang,Yanan Hu,Xin Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的分解框架和深度网络，实现了高质量、高保真度的CASSI高光谱图像重建，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CASSI系统下的高光谱重建存在两个难题：1）空间和光谱信息纠缠导致逆问题严重欠定，2）成像结果受到场景照明影响，难以恢复与光照无关的固有谱反射特性。

Method: 作者提出了chromaticity-intensity decomposition（色度-强度分解）框架，将高光谱图像分解为空间光滑的强度图和包含丰富空间细节与局部谱稀疏性的色度立方。基于该分解，设计了CIDNet深度网络，融合了混合空间-光谱Transformer用于高质量色度重建，并引入退化感知、空间自适应的噪声估计模块以处理各向异性噪声；此外，该方法依托双摄像头CASSI系统。

Result: 在合成与真实CASSI数据集上进行了大量实验，结果显示该方法在谱与色度保真度方面均优于已有方法。

Conclusion: 分解策略和CIDNet能高效且高质量地克服CASSI高光谱重建中的照明干扰和欠定性问题，为照明不变的HSI重建提供了新的思路和工具。

Abstract: In coded aperture snapshot spectral imaging (CASSI), the captured measurement
entangles spatial and spectral information, posing a severely ill-posed inverse
problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured
radiance inherently depends on scene illumination, making it difficult to
recover the intrinsic spectral reflectance that remains invariant to lighting
conditions. To address these challenges, we propose a chromaticity-intensity
decomposition framework, which disentangles an HSI into a spatially smooth
intensity map and a spectrally variant chromaticity cube. The chromaticity
encodes lighting-invariant reflectance, enriched with high-frequency spatial
details and local spectral sparsity. Building on this decomposition, we develop
CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a
dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral
Transformer tailored to reconstruct fine-grained and sparse spectral
chromaticity and a degradation-aware, spatially-adaptive noise estimation
module that captures anisotropic noise across iterative stages. Extensive
experiments on both synthetic and real-world CASSI datasets demonstrate that
our method achieves superior performance in both spectral and chromaticity
fidelity. Code and models will be publicly available.

</details>


### [55] [InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention](https://arxiv.org/abs/2509.16691)
*Qiang Xiang,Shuang Sun,Binglei Li,Dejia Song,Huaxia Li,Nemo Chen,Xu Tang,Yao Hu,Junping Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为InstanceAssemble的新型布局到图像（L2I）生成架构，实现了高精度、高可控性的图像合成。该方法在复杂布局条件下取得了当前最优性能，并兼容多种风格的LoRA模块。


<details>
  <summary>Details</summary>
Motivation: 现有Layout-to-Image方法在图像位置与内容的精准可控生成上仍有不足，急需提升复杂布局下的生成质量和灵活性。

Method: 提出InstanceAssemble架构，通过实例组装注意力机制，将布局条件、文本、视觉信息灵活融合进扩散模型，并利用轻量级LoRA模块适配现有DiT-base的T2I模型。同时引入新基准数据集Denselayout和评价指标Layout Grounding Score（LGS）。

Result: 实验表明，InstanceAssemble方法在复杂布局的L2I任务中性能优越，并兼容不同风格的LoRA模块，达到当前最优水平。

Conclusion: InstanceAssemble有效提升了布局到图像生成的精准性与多模态可控性，为后续相关研究和实际应用打下基础。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules.

</details>


### [56] [Animalbooth: multimodal feature enhancement for animal subject personalization](https://arxiv.org/abs/2509.16702)
*Chen Liu,Haitao Wu,Kafeng Wang,Xiaowang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AnimalBooth的个性化动物图像生成框架，在保持动物身份特征与提升图像质量方面取得了优越表现。


<details>
  <summary>Details</summary>
Motivation: 当前个性化动物图像生成存在因特征错位导致身份漂移的问题，急需提升身份保持能力以实现高质量的动物形象定制。

Method: AnimalBooth框架引入了Animal Net与自适应注意力模块，有效缓解域间对齐误差；提出了基于离散余弦变换（DCT）的频率控制特征整合模块，指导扩散过程从结构到细节进行逐步优化，并构建了高分辨率动物数据集AnimalBench。

Result: 实验结果表明AnimalBooth方法在多个指标与基线算法对比下均表现突出，在身份一致性和感知质量方面实现了提升。

Conclusion: AnimalBooth框架成功增强了身份保持能力和图像视觉质量，为动物图像个性化生成奠定了新的基准，并促进了相关领域的进一步研究。

Abstract: Personalized animal image generation is challenging due to rich appearance
cues and large morphological variability. Existing approaches often exhibit
feature misalignment across domains, which leads to identity drift. We present
AnimalBooth, a framework that strengthens identity preservation with an Animal
Net and an adaptive attention module, mitigating cross domain alignment errors.
We further introduce a frequency controlled feature integration module that
applies Discrete Cosine Transform filtering in the latent space to guide the
diffusion process, enabling a coarse to fine progression from global structure
to detailed texture. To advance research in this area, we curate AnimalBench, a
high resolution dataset for animal personalization. Extensive experiments show
that AnimalBooth consistently outperforms strong baselines on multiple
benchmarks and improves both identity fidelity and perceptual quality.

</details>


### [57] [When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2509.16704)
*Pan Liu,Jinshi Liu*

Main category: cs.CV

TL;DR: 本论文关注半监督语义分割中的伪标签筛选问题，提出了一种新的置信区分学习方法（CSL），通过在置信分布空间中优化伪标签选择，并通过随机遮蔽提升模型对低置信样本的上下文理解，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前伪标签生成已经取得较大进展，但伪标签筛选常用的固定阈值方法无法有效区分正确和错误预测，容易造成模型过拟合并丢失关键上下文信息，亟需更加智能和鲁棒的伪标签筛选方式。

Method: 提出Confidence Separable Learning（CSL）方法，将伪标签筛选建模为置信分布空间中的凸优化问题，为每个样本自适应设定可靠/不可靠预测的决策边界；同时对高置信像素进行随机遮蔽，引导模型学习低置信区的上下文关系，以缓解丢弃不确定预测的负面影响。

Result: 在Pascal、Cityscapes和COCO等语义分割主流数据集上，CSL方法在准确率等多项指标上优于当前主流方法，展现出更强的泛化能力和鲁棒性。

Conclusion: CSL显著提升了伪标签筛选质量和语义分割性能，并有效弥补了现有固定阈值方法带来的过拟合和上下文信息丢失等问题。该方法具有很好的实用价值和推广前景。

Abstract: While significant advances exist in pseudo-label generation for
semi-supervised semantic segmentation, pseudo-label selection remains
understudied. Existing methods typically use fixed confidence thresholds to
retain high-confidence predictions as pseudo-labels. However, these methods
cannot cope with network overconfidence tendency, where correct and incorrect
predictions overlap significantly in high-confidence regions, making separation
challenging and amplifying model cognitive bias. Meanwhile, the direct
discarding of low-confidence predictions disrupts spatial-semantic continuity,
causing critical context loss. We propose Confidence Separable Learning (CSL)
to address these limitations. CSL formulates pseudo-label selection as a convex
optimization problem within the confidence distribution feature space,
establishing sample-specific decision boundaries to distinguish reliable from
unreliable predictions. Additionally, CSL introduces random masking of reliable
pixels to guide the network in learning contextual relationships from
low-reliability regions, thereby mitigating the adverse effects of discarding
uncertain predictions. Extensive experimental results on the Pascal,
Cityscapes, and COCO benchmarks show that CSL performs favorably against
state-of-the-art methods. Code and model weights are available at
https://github.com/PanLiuCSU/CSL.

</details>


### [58] [Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding](https://arxiv.org/abs/2509.16721)
*Haoyuan Li,Rui Liu,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Text-Scene的新方法，能够自动将复杂3D场景解析为可理解的文本描述，不需要人为参与，同时提出了一个3D任务规划基准InPlan3D。实验表明，生成的文本解析能准确表达3D场景内容，并为下游任务提供帮助。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在2D图像理解方面取得了显著进展，但扩展到3D场景理解存在挑战：3D场景包含更丰富的空间关系、物理属性等，而缺乏大规模3D视觉-语言数据集也是主要障碍。因此，急需新方法实现3D场景的语言理解。

Method: 提出了Text-Scene框架，利用几何分析与MLLMs，将3D场景中的对象属性、空间关系等转化为结构化、连贯的文本描述。该流程无需人工干预，同时能生成详实且可被人类理解的场景摘要。此外，构建了InPlan3D基准数据集，用以评估3D场景下的长期任务规划能力。

Result: Text-Scene生成的文本能准确、详细地表达3D场景的对象信息和全局环境。实验结果表明，基于该方法得到的文本解析可有效提升3D任务推理等下游任务的性能。InPlan3D基准涵盖636个室内场景、3174个长期规划任务，验证了本方法的有效性。

Conclusion: Text-Scene方法实现了3D场景到自然语言的高质量转化，推动了3D场景内容的可理解性，使得下游AI系统可以更好地进行场景理解与推理。方法通用且不依赖人工标注，代码与数据集将公开，有望促进该领域发展。

Abstract: Enabling agents to understand and interact with complex 3D scenes is a
fundamental challenge for embodied artificial intelligence systems. While
Multimodal Large Language Models (MLLMs) have achieved significant progress in
2D image understanding, extending such capabilities to 3D scenes remains
difficult: 1) 3D environment involves richer concepts such as spatial
relationships, affordances, physics, layout, and so on, 2) the absence of
large-scale 3D vision-language datasets has posed a significant obstacle. In
this paper, we introduce Text-Scene, a framework that automatically parses 3D
scenes into textual descriptions for scene understanding. Given a 3D scene, our
model identifies object attributes and spatial relationships, and then
generates a coherent summary of the whole scene, bridging the gap between 3D
observation and language without requiring human-in-the-loop intervention. By
leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions
that are accurate, detailed, and human-interpretable, capturing object-level
details and global-level context. Experimental results on benchmarks
demonstrate that our textual parses can faithfully represent 3D scenes and
benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we
present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of
3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity
and accessibility in our approach, aiming to make 3D scene content
understandable through language. Code and datasets will be released.

</details>


### [59] [Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment](https://arxiv.org/abs/2509.16727)
*Xin Lei Lin,Soroush Mehraban,Abhishek Moturu,Babak Taati*

Main category: cs.CV

TL;DR: 本论文提出了3DPain合成数据集和ViTPain模型，旨在解决现有疼痛自动评估中样本不平衡和无法精确控制面部表情问题。该工作提升了数据多样性和临床适用性，为非沟通性病人提供更准确可靠的痛苦评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有自动面部疼痛评估受限于数据集人口和标签失衡，以及生成模型对面部动作单元和结构的控制力不足，影响了模型泛化和临床可靠性。特别是在痴呆症等无法自我表达的患者中，精确评估疼痛尤为重要。

Method: 论文提出三阶段框架：生成多样3D面部网格，利用扩散模型贴图，并通过动作单元（AUs）驱动的面部绑定，合成带有中性与痛苦配对图像、多视角、AU配置、PSPI分数及痛区热力图的图像。3DPain包含82500份样本，涵盖2500个数字身份，人口分布均衡。提出ViTPain模型，采用Vision Transformer基于热力图教师-学生蒸馏框架，提升对RGB图像的疼痛识别准确性与可解释性。

Result: 成功构建了高质量、多样化、注释丰富（包括首个痛区热力图）的3DPain数据集；ViTPain模型利用热力图注释引导RGB评估，实现更精准和具有解释性的自动疼痛识别。

Conclusion: 3DPain数据集和ViTPain方法为自动化疼痛评估领域提供了人口多样性强、标签丰富并可控性高的研究基础，有助于提高在非交流患者中的泛化能力和临床实用性。

Abstract: Automated pain assessment from facial expressions is crucial for
non-communicative patients, such as those with dementia. Progress has been
limited by two challenges: (i) existing datasets exhibit severe demographic and
label imbalance due to ethical constraints, and (ii) current generative models
cannot precisely control facial action units (AUs), facial structure, or
clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for
automated pain assessment, featuring unprecedented annotation richness and
demographic diversity. Our three-stage framework generates diverse 3D meshes,
textures them with diffusion models, and applies AU-driven face rigging to
synthesize multi-view faces with paired neutral and pain images, AU
configurations, PSPI scores, and the first dataset-level annotations of
pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain
expression heatmaps and 2,500 synthetic identities balanced by age, gender, and
ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal
distillation framework in which a heatmap-trained teacher guides a student
trained on RGB images, enhancing accuracy, interpretability, and clinical
reliability. Together, 3DPain and ViTPain establish a controllable, diverse,
and clinically grounded foundation for generalizable automated pain assessment.

</details>


### [60] [Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2509.16738)
*Kai Jiang,Zhengyan Shi,Dell Zhang,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本论文提出了一种利用有益噪声促进类增量学习（CIL）的方法——Mixture of Noise (Min)，在多个数据集和场景下取得了前沿性能。


<details>
  <summary>Details</summary>
Motivation: CIL的挑战在于持续学习新类别时仍需保留旧知识，但微调预训练模型容易导致参数漂移，降低泛化能力。部分研究表明噪声在一定情况下能抑制作业特定的低相关特征，有助于后续任务。因此，作者希望设计方法将噪声机制变为增量学习的优势。

Method: 提出用信息论指导学习任务相关的有益噪声，即Mixture of Noise (Min)。具体做法是从新任务的高维特征中学习任务特定噪声，再动态调整不同任务噪声的混合权重，最后将有益噪声嵌入中间特征层，从而屏蔽部分无效特征的响应。

Result: 在六个基准数据集上进行了大量实验，证明Min方法在大多数增量学习设置下，特别是50步增量学习中取得了最先进的性能。

Conclusion: 事实表明，有益噪声能够显著提升连续学习中的模型表现，为未来增量学习研究提供了新思路和可能性。

Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories
while retaining the knowledge of old ones. Pre-trained models (PTMs) show
promising capabilities in CIL. However, existing approaches that apply
lightweight fine-tuning to backbones still induce parameter drift, thereby
compromising the generalization capability of pre-trained models. Parameter
drift can be conceptualized as a form of noise that obscures critical patterns
learned for previous tasks. However, recent researches have shown that noise is
not always harmful. For example, the large number of visual patterns learned
from pre-training can be easily abused by a single task, and introducing
appropriate noise can suppress some low-correlation features, thus leaving a
margin for future tasks. To this end, we propose learning beneficial noise for
CIL guided by information theory and propose Mixture of Noise (Min), aiming to
mitigate the degradation of backbone generalization from adapting new tasks.
Specifically, task-specific noise is learned from high-dimension features of
new tasks. Then, a set of weights is adjusted dynamically for optimal mixture
of different task noise. Finally, Min embeds the beneficial noise into the
intermediate features to mask the response of inefficient patterns. Extensive
experiments on six benchmark datasets demonstrate that Min achieves
state-of-the-art performance in most incremental settings, with particularly
outstanding results in 50-steps incremental settings. This shows the
significant potential for beneficial noise in continual learning.

</details>


### [61] [CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding](https://arxiv.org/abs/2509.16745)
*Ritabrata Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 提出了用于评估视觉解释结构感知能力的全新基准数据集CAMBench-QR，并通过精确控制的二维码几何结构检验主流CAM方法的解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉解释（如CAM方法）虽给出合理的热力图，但往往无法忠实反映目标图像的结构性区域。缺乏能够直接检验视觉解释是否准确聚焦于关键结构（而非背景）的系统性基准和量化指标。

Method: 提出了CAMBench-QR基准：合成包含精确掩码和受控失真的二维码/非二维码数据，基于二维码固定几何结构（定位图案、计时线、模块网格），制定结构感知评价指标（如结构区域质量比、背景泄漏、AUC、与结构距离等），并结合因果遮挡法、插入/删除信度、鲁棒性及延时等传统指标，全面评测主流高效CAM方法（LayerCAM、EigenGrad-CAM、XGrad-CAM）在零样本和最后层微调两种实际应用情境下的解释能力。

Result: 结构感知指标能有效区分主流CAM方法在聚焦结构要素与规避背景方面的能力。通过系统测试发现，不同方法在不同调优方案下，其结构聚焦表现存在明显差异。基准和指标均可简便复现。

Conclusion: CAMBench-QR为视觉解释方法提供了首个结构感知定量评价标准，有助于批量筛选和优化更忠实于目标结构信息的视觉解释方法，是未来结构感知可解释性研究的重要工具。

Abstract: Visual explanations are often plausible but not structurally faithful. We
introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical
geometry of QR codes (finder patterns, timing lines, module grid) to test
whether CAM methods place saliency on requisite substructures while avoiding
background. CAMBench-QR synthesizes QR/non-QR data with exact masks and
controlled distortions, and reports structure-aware metrics (Finder/Timing Mass
Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside
causal occlusion, insertion/deletion faithfulness, robustness, and latency. We
benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)
under two practical regimes of zero-shot and last-block fine-tuning. The
benchmark, metrics, and training recipes provide a simple, reproducible
yardstick for structure-aware evaluation of visual explanations. Hence we
propose that CAMBENCH-QR can be used as a litmus test of whether visual
explanations are truly structure-aware.

</details>


### [62] [HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis](https://arxiv.org/abs/2509.16748)
*Heyuan Li,Kenkun Liu,Lingteng Qiu,Qi Zuo,Keru Zheng,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文针对三平面(tri-plane)表示在3D感知生成对抗网络（GAN）中用于头像合成时出现的特征纠缠与渲染低效等问题，提出了一种创新的hy-plane混合平面表示，结合了平面与球面表示的优点，并通过近等面积坐标映射及单通道特征生成，有效提升了合成头像的效果和细节表现。


<details>
  <summary>Details</summary>
Motivation: 三平面(a.k.a. tri-plane)表示虽然在3D-aware GAN高效，但笛卡尔坐标映射导致特征纠缠和图像镜像瑕疵。SphereHead虽然通过球形坐标有所改进，但带来了球面与方形特征映射不均等新问题。同时，多通道特征的相互干扰没有被彻底解决，这些问题共同阻碍了三平面方法的进一步提升。作者动机在于系统性分析上述瓶颈，并提出创新解决方案。

Method: 作者提出hy-plane混合平面表示，融合了传统平面和球面平面的优点；球面平面采用新颖的近等面积映射(near-equal-area warping)，提升方形特征图的利用率。此外，生成器生成单通道统一特征图，避免多通道特征渗透和干扰。

Result: 通过一系列技术改进（hy-plane、近等面积warping、单通道特征），新方法HyPlaneHead在全头部图像合成任务上取得了业界最佳（state-of-the-art）表现。

Conclusion: 混合平面hy-plane表示与相关技术提升，有效解决了三平面及其变体在3D感知GAN生成头像过程中的长期难题，推动了三平面架构性能的极限。

Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for
head image synthesis and other 3D object/scene modeling tasks due to their
efficiency. However, querying features via Cartesian coordinate projection
often leads to feature entanglement, which results in mirroring artifacts. A
recent work, SphereHead, attempted to address this issue by introducing
spherical tri-planes based on a spherical coordinate system. While it
successfully mitigates feature entanglement, SphereHead suffers from uneven
mapping between the square feature maps and the spherical planes, leading to
inefficient feature map utilization during rendering and difficulties in
generating fine image details. Moreover, both tri-plane and spherical tri-plane
representations share a subtle yet persistent issue: feature penetration across
convolutional channels can cause interference between planes, particularly when
one plane dominates the others. These challenges collectively prevent
tri-plane-based methods from reaching their full potential. In this paper, we
systematically analyze these problems for the first time and propose innovative
solutions to address them. Specifically, we introduce a novel hybrid-plane
(hy-plane for short) representation that combines the strengths of both planar
and spherical planes while avoiding their respective drawbacks. We further
enhance the spherical plane by replacing the conventional theta-phi warping
with a novel near-equal-area warping strategy, which maximizes the effective
utilization of the square feature map. In addition, our generator synthesizes a
single-channel unified feature map instead of multiple feature maps in separate
channels, thereby effectively eliminating feature penetration. With a series of
technical improvements, our hy-plane representation enables our method,
HyPlaneHead, to achieve state-of-the-art performance in full-head image
synthesis.

</details>


### [63] [DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images](https://arxiv.org/abs/2509.16767)
*Ozgur Kara,Harris Nisar,James M. Rehg*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffEye的扩散模型框架，可以更真实和多样地预测人类在自然图片自由浏览时的眼动轨迹，弥补了以往只用离散注视点、忽略原始眼动轨迹信息的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的注视路径和显著性预测方法通常只利用离散注视点序列（scanpath），忽略了原始眼动轨迹的丰富信息，而且大多只能生成唯一、长度固定的路径，无法反映实际人类视觉注意力的多样性和随机性。

Method: 提出DiffEye——一个基于扩散模型的训练框架，输入为自然图片和原始眼动轨迹，结合新颖的对应位置嵌入（CPE），将注视位置信息与视觉特征对齐；通过扩散模型生成多样、连续的眼动轨迹。

Result: DiffEye即使在小型数据集上训练，也能生成高质量、真实、多样的眼动轨迹，并且可以转换为scanpath和显著性图，输出结果更贴合人类视觉注意分布。性能在scanpath生成任务上达到SOTA水准，并首次实现了连续眼动轨迹的生成。

Conclusion: DiffEye是首个在自然图片上利用扩散模型并充分利用原始眼动数据的框架，为真实的视觉注意力建模提供了新的解决方案，对视觉和眼动相关研究具有重要意义。

Abstract: Numerous models have been developed for scanpath and saliency prediction,
which are typically trained on scanpaths, which model eye movement as a
sequence of discrete fixation points connected by saccades, while the rich
information contained in the raw trajectories is often discarded. Moreover,
most existing approaches fail to capture the variability observed among human
subjects viewing the same image. They generally predict a single scanpath of
fixed, pre-defined length, which conflicts with the inherent diversity and
stochastic nature of real-world visual attention. To address these challenges,
we propose DiffEye, a diffusion-based training framework designed to model
continuous and diverse eye movement trajectories during free viewing of natural
images. Our method builds on a diffusion model conditioned on visual stimuli
and introduces a novel component, namely Corresponding Positional Embedding
(CPE), which aligns spatial gaze information with the patch-based semantic
features of the visual input. By leveraging raw eye-tracking trajectories
rather than relying on scanpaths, DiffEye captures the inherent variability in
human gaze behavior and generates high-quality, realistic eye movement
patterns, despite being trained on a comparatively small dataset. The generated
trajectories can also be converted into scanpaths and saliency maps, resulting
in outputs that more accurately reflect the distribution of human visual
attention. DiffEye is the first method to tackle this task on natural images
using a diffusion model while fully leveraging the richness of raw eye-tracking
data. Our extensive evaluation shows that DiffEye not only achieves
state-of-the-art performance in scanpath generation but also enables, for the
first time, the generation of continuous eye movement trajectories. Project
webpage: https://diff-eye.github.io/

</details>


### [64] [MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation](https://arxiv.org/abs/2509.16768)
*Omid Bonakdar,Nasser Mozayani*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMPart的新型框架，实现了基于单张图片的结构感知3D模型生成。该方法允许用户对部件拆分过程进行控制，并能更好推断遮挡部分结构。最终输出可编辑和理解性更高的三维模型。


<details>
  <summary>Details</summary>
Motivation: 传统3D生成方法大多只产生无结构信息的封闭网格，导致编辑、动画制作和语义理解受限。尽管结构感知3D生成（如按部件分解）有所改进，但现有方案中用户无法控制分割细节，也难以还原被遮挡区域，实际应用受阻。

Method: 论文首先采用视觉语言模型（VLM）结合输入图片和用户描述，生成提示信息。接着，通过生成模型在上述提示指导下，为每个部件生成独立的图像（尤其对遮挡部分有更好推断）。随后，进入多视角生成阶段，为每个部件生成一组一致的视角图像。最后，重建模型将这些多视角图像转换为3D模型。

Result: 所提方法可从单张图片生成结构化且部件分明的3D模型，且用户可对拆分和遮挡部分推断过程进行有效控制，在编辑性、动画和语义理解方面优于现有方法。

Conclusion: MMPart框架大幅提升了单图3D结构感知生成中的用户控制性和遮挡推断能力，对于3D编辑、动画制作及语义应用有显著意义。

Abstract: Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,
metaverse, and robotics. However, most methods represent the target object as a
closed mesh devoid of any structural information, limiting editing, animation,
and semantic understanding. Part-aware 3D generation addresses this problem by
decomposing objects into meaningful components, but existing pipelines face
challenges: in existing methods, the user has no control over which objects are
separated and how model imagine the occluded parts in isolation phase. In this
paper, we introduce MMPart, an innovative framework for generating part-aware
3D models from a single image. We first use a VLM to generate a set of prompts
based on the input image and user descriptions. In the next step, a generative
model generates isolated images of each object based on the initial image and
the previous step's prompts as supervisor (which control the pose and guide
model how imagine previously occluded areas). Each of those images then enters
the multi-view generation stage, where a number of consistent images from
different views are generated. Finally, a reconstruction model converts each of
these multi-view images into a 3D model.

</details>


### [65] [Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm](https://arxiv.org/abs/2509.16771)
*Xiaohan Chen,Hongrui Gu,Cunshi Wang,Haiyang Mu,Jie Zheng,Junju Du,Jing Ren,Zhou Fan,Jing Li*

Main category: cs.CV

TL;DR: 本文提出了一种结合U-Net神经网络和LSD算法的新模型，有效检测天文图像中的人造卫星轨迹。模型在模拟和真实数据上均表现优异，能够显著减少卫星造成的光度干扰。


<details>
  <summary>Details</summary>
Motivation: 随着人造卫星数量激增，卫星反光导致天文图像产生条带伪影，干扰天文观测。准确检测并定位这些伪影对确保天文数据准确性变得至关重要。

Method: 提出结合U-Net深度学习分割网络与Line Segment Detector（LSD）算法的卫星轨迹检测模型。使用Mini-SiTian Array数据生成的375幅模拟卫星轨迹图像对模型进行训练，并在真实观测数据上测试性能。

Result: 对信噪比大于3的卫星轨迹检测率超过99%，在真实Mini-SiTian阵列观测数据上的召回率为79.57%，精确率为74.56%。

Conclusion: 该模型能高效、准确地检测天文图像中的卫星轨迹，为缓解卫星伪影带来的天文观测干扰提供了一种有效工具。

Abstract: With the rapid increase in the number of artificial satellites, astronomical
imaging is experiencing growing interference. When these satellites reflect
sunlight, they produce streak-like artifacts in photometry images. Such
satellite trails can introduce false sources and cause significant photometric
errors. As a result, accurately identifying the positions of satellite trails
in observational data has become essential. In this work, we propose a
satellite trail detection model that combines the U-Net deep neural network for
image segmentation with the Line Segment Detector (LSD) algorithm. The model is
trained on 375 simulated images of satellite trails, generated using data from
the Mini-SiTian Array. Experimental results show that for trails with a
signal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.
Additionally, when applied to real observational data from the Mini-SiTian
Array, the model achieves a recall of 79.57 and a precision of 74.56.

</details>


### [66] [Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models](https://arxiv.org/abs/2509.16805)
*Md. Atabuzzaman,Ali Asgarov,Chris Thomas*

Main category: cs.CV

TL;DR: 论文揭示大规模视觉-语言模型（LVLMs）在多项选择视觉问答（MCQA）任务中存在选项偏置问题，并提出了一种无需重新训练的推理时去偏方法来提升模型的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在视觉-语言任务中表现突出，现有工作多关注单模态偏置，而MCQA任务中针对特定选项或位置（如“A”）的选择偏置问题尚未被系统研究。深入理解并缓解这一偏置，对提升模型在细粒度视觉推理任务中的鲁棒性具有重要意义。

Method: 作者构建了不同难度级别（基于选项语义相似度区分）的细粒度MCQA基准来分析LVLMs的选择偏置问题。针对这一问题，提出基于推理时模型logit输出的去偏方法：通过一般和上下文提示估算集成偏置向量，并对模型输出进行置信度自适应校正，无需重新训练，兼容冻结参数的模型。

Result: 实验证明，多种先进LVLMs在MCQA任务中普遍存在选项偏置，并且该偏置在任务难度提高时加剧。所提去偏方法显著降低了选项偏置，并在高难度任务下显著提升了准确率。

Conclusion: LVLMs在MCQA任务中存在被忽视的选择偏置问题，影响其精细推理能力。作者提出的无须训练的推理时去偏方法，显著改善模型的性能与鲁棒性，为未来LVLMs的公平性和可靠性研究提供了新思路。

Abstract: Large Vision-Language Models (LVLMs) have achieved strong performance on
vision-language tasks, particularly Visual Question Answering (VQA). While
prior work has explored unimodal biases in VQA, the problem of selection bias
in Multiple-Choice Question Answering (MCQA), where models may favor specific
option tokens (e.g., "A") or positions, remains underexplored. In this paper,
we investigate both the presence and nature of selection bias in LVLMs through
fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,
defined by the semantic similarity of the options. We further propose an
inference-time logit-level debiasing method that estimates an ensemble bias
vector from general and contextual prompts and applies confidence-adaptive
corrections to the model's output. Our method mitigates bias without retraining
and is compatible with frozen LVLMs. Extensive experiments across several
state-of-the-art models reveal consistent selection biases that intensify with
task difficulty, and show that our mitigation approach significantly reduces
bias while improving accuracy in challenging settings. This work offers new
insights into the limitations of LVLMs in MCQA and presents a practical
approach to improve their robustness in fine-grained visual reasoning. Datasets
and code are available at:
https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs

</details>


### [67] [MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging](https://arxiv.org/abs/2509.16806)
*Kacper Marzol,Ignacy Kolton,Weronika Smolak-Dyżewska,Joanna Kaleta,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了MedGS，一种基于高斯喷洒(GS)机制的半监督神经隐式曲面重建框架，用于提升医学多模态三维成像（如超声、MRI、CT）的表面重建和帧间插值质量，增强噪声鲁棒性并降低伪影。


<details>
  <summary>Details</summary>
Motivation: 传统的三维医学成像表面重建和插值方法易受到图像噪声和帧间信息不全的影响，影响解剖结构的准确建模和可视化。

Method: 提出了一种基于GS的插值机制，将医学影像表示为嵌入三维空间的连续二维帧，并以高斯分布进行建模，结合半监督学习方式进行表面重建。

Result: MedGS相比以往神经隐式方法训练效率更高，能更好地抑制噪声、减少伪影，实现对复杂解剖结构的高保真建模和灵活编辑。

Conclusion: MedGS同时兼顾速度、噪声鲁棒性和建模能力，适用于大规模、实际的医学成像三维建模与可视化应用。

Abstract: Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.

</details>


### [68] [Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models](https://arxiv.org/abs/2509.16822)
*Townim Faisal Chowdhury,Vu Minh Hieu Phan,Kewen Liao,Nanyu Dong,Minh-Son To,Anton Hengel,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 该论文提出了一种无需借助额外生成模型，直接在分类器特征空间中生成反事实解释的新方法Mirror-CFE，结合镜像映射实现平滑的输入变换，提升了解释的有效性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像分类器的反事实解释方法大多依赖于外部生成模型和附加编码器，忽略对分类器本身特征空间和决策边界的解释，导致解释不够贴合模型本身。本研究旨在设计一种更直接、忠实反映分类器特征空间结构的反事实解释方法。

Method: 提出Mirror-CFE方法，在分类器特征空间中，将决策边界视为“镜子”，利用镜面反射的思想构建特征变换，同时通过学习特征空间到图像空间的映射函数进行反事实图像生成，并保留距离关系以获得平滑变换。

Result: 在四个图像数据集上进行实验，Mirror-CFE方法在生成的反事实解释的有效性和输入相似性上均优于当前主流的解释方法。此外，生成的特征演化序列可视化提升了对分类决策过程的可解释性。

Conclusion: Mirror-CFE改进了基于反事实的图像分类器解释方法，增强了对分类器特征空间与决策边界的可解释性，并能生成高有效性、相似性的反事实图像，为模型调试和透明化提供了有价值的工具。

Abstract: Counterfactual explanations (CFE) for deep image classifiers aim to reveal
how minimal input changes lead to different model decisions, providing critical
insights for model interpretation and improvement. However, existing CFE
methods often rely on additional image encoders and generative models to create
plausible images, neglecting the classifier's own feature space and decision
boundaries. As such, they do not explain the intrinsic feature space and
decision boundaries learned by the classifier. To address this limitation, we
propose Mirror-CFE, a novel method that generates faithful counterfactual
explanations by operating directly in the classifier's feature space, treating
decision boundaries as mirrors that ``reflect'' feature representations in the
mirror. Mirror-CFE learns a mapping function from feature space to image space
while preserving distance relationships, enabling smooth transitions between
source images and their counterfactuals. Through extensive experiments on four
image datasets, we demonstrate that Mirror-CFE achieves superior performance in
validity while maintaining input resemblance compared to state-of-the-art
explanation methods. Finally, mirror-CFE provides interpretable visualization
of the classifier's decision process by generating step-wise transitions that
reveal how features evolve as classification confidence changes.

</details>


### [69] [L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models](https://arxiv.org/abs/2509.16832)
*Ziyang Xu,Benedikt Schwab,Yihui Yang,Thomas H. Kolbe,Christoph Holst*

Main category: cs.CV

TL;DR: 提出了一种新的建筑级激光雷达点云与三维城市语义模型配准方法L2M-Reg，在模型存在不确定性的情况下可实现更准确高效的配准。


<details>
  <summary>Details</summary>
Motivation: 现有LoD2级别的三维城市语义模型存在泛化不确定性，使得激光雷达点云与模型之间的高精度配准尤其是在单体建筑层面变得十分困难。为实现数字孪生城市中的诸多后续应用，有必要解决这一配准难题。

Method: 提出L2M-Reg方法，包括：建立可靠的平面对应关系、构建伪平面约束的高斯-Helmert模型以及自适应估算垂直方向上的平移量。该方法显式地考虑了模型不确定性。

Result: 在三个现实世界数据集上进行实验，结果表明L2M-Reg在准确性和计算效率上均优于现有的基于ICP和基于平面的方法。

Conclusion: L2M-Reg为激光雷达点云与存在不确定性的建筑级三维语义城市模型之间的配准提供了一种新颖且有效的解决方案。

Abstract: Accurate registration between LiDAR (Light Detection and Ranging) point
clouds and semantic 3D city models is a fundamental topic in urban digital
twinning and a prerequisite for downstream tasks, such as digital construction,
change detection and model refinement. However, achieving accurate
LiDAR-to-Model registration at individual building level remains challenging,
particularly due to the generalization uncertainty in semantic 3D city models
at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing
L2M-Reg, a plane-based fine registration method that explicitly accounts for
model uncertainty. L2M-Reg consists of three key steps: establishing reliable
plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,
and adaptively estimating vertical translation. Experiments on three real-world
datasets demonstrate that L2M-Reg is both more accurate and computationally
efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg
provides a novel building-level solution regarding LiDAR-to-Model registration
when model uncertainty is present.

</details>


### [70] [ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression](https://arxiv.org/abs/2509.16853)
*Jinhao Wang,Cihan Ruan,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种无需数据集特定处理、可泛化的方法，通过分析VAE编解码模型的内在参数（如权重方差、偏置幅值及通道间相关性）识别并组织图像压缩潜在通道的重要性，有效提升压缩率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 以往的学习式图像压缩研究发现，编码时仅有少量潜在通道对重建至关重要，其他大多信息量有限。如果利用好这种通道间不平衡性，能优化编码效率及计算开销。但现有方法多依赖数据集相关的消融实验且通常独立分析通道，未关注通道间的相关性和依赖性。

Method: 作者提出了一种面向已训练VAE模型的、数据集无关的通道重要性识别组织方法，通过分析模型参数统计量（权重方差、偏置大小、通道对相关性）来估算各通道重要性。进而发现了“固定显著通道空间”（ISCS）结构，将通道划分为核心与辅助两类，并提出了基于ISCS的确定性通道排序和分组方案，实现通道切片并行解码，降低冗余，提高比特率效率。

Result: 实验验证表明，在多种主流学习式图像压缩框架中，该方法既能有效降低比特率和计算量，又能保持良好的重建质量。

Conclusion: 该方法为现有学习式图像压缩模型提供了简单、模块化、有效的通道优化方案，可促进压缩效率与解码速度的提升。

Abstract: Prior studies in learned image compression (LIC) consistently show that only
a small subset of latent channels is critical for reconstruction, while many
others carry limited information. Exploiting this imbalance could improve both
coding and computational efficiency, yet existing approaches often rely on
costly, dataset-specific ablation tests and typically analyze channels in
isolation, ignoring their interdependencies.
  We propose a generalizable, dataset-agnostic method to identify and organize
important channels in pretrained VAE-based LIC models. Instead of brute-force
empirical evaluations, our approach leverages intrinsic parameter
statistics-weight variances, bias magnitudes, and pairwise correlations-to
estimate channel importance. This analysis reveals a consistent organizational
structure, termed the Invariant Salient Channel Space (ISCS), where
Salient-Core channels capture dominant structures and Salient-Auxiliary
channels provide complementary details. Building on ISCS, we introduce a
deterministic channel ordering and grouping strategy that enables
slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.
  Experiments across multiple LIC architectures demonstrate that our method
effectively reduces bitrate and computation while maintaining reconstruction
quality, providing a practical and modular enhancement to existing learned
compression frameworks.

</details>


### [71] [ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM](https://arxiv.org/abs/2509.16863)
*Amanuel T. Dufera,Yuan-Li Cai*

Main category: cs.CV

TL;DR: ConfidentSplat是一种基于3D高斯投影的SLAM系统，通过引入置信度加权的深度融合机制，实现了更鲁棒且高保真的仅RGB重建，显著提升了重建精度与新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 目前的RGB-only 3DGS SLAM方法因深度估计不可靠，导致几何重建不准确。本研究旨在解决该痛点，实现适用于真实复杂场景且鲁棒性更高的高保真3D重建。

Method: 提出一种创新的置信度加权融合机制，自适应整合多视角几何深度与单目先验（Omnidata ViT生成），依据多视角一致性等显式置信指标加权，生成高质量代理深度以优化3DGS地图。系统还结合DROID-SLAM的前端与后端优化（如回环检测与全局捆绑调整），实现全局一致性和在线自适应。

Result: 在TUM-RGBD、ScanNet等标准数据集和定制移动数据集上验证，系统在重建精度（L1深度误差）和新视角合成质量（PSNR、SSIM、LPIPS）等方面较主流方法取得显著提升，尤其在困难场景表现突出。

Conclusion: 基于置信度的感知融合是提升RGB-Only视觉SLAM精度与鲁棒性的有效范式，为密集视觉SLAM技术的进一步发展提供了新思路。

Abstract: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.

</details>


### [72] [$\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation](https://arxiv.org/abs/2509.16873)
*Yuanzhi Li,Lebin Zhou,Nam Ling,Zhenghao Chen,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一个新的大规模、多模态、多视角的游戏内容数据集（M^3VIR），为生成式AI在游戏与娱乐领域的研究提供更高质量的数据支持。数据集采用Unreal Engine 5渲染，覆盖80个场景和8个类别，支持超分辨率、视角合成和可控视频生成等任务。


<details>
  <summary>Details</summary>
Motivation: 现有游戏类数据集规模小、内容单一，缺乏高保真和多样化数据，且没有适用于可控视频生成的基准，限制了生成式AI技术在游戏和娱乐产业的应用和发展。

Method: 作者构建了M^3VIR数据集，包含高质量的低分辨-高分辨真实配对帧和多视角数据，设计了支持超分、视角合成和其组合，以及首个面向多风格、对象级可控视频生成的子集。并用现有SOTA方法进行了基准测试，为以后的相关研究提供了基线。

Result: M^3VIR数据集为AI驱动的游戏内容研究带来了高质量、真实、多样的数据，基准实验显示现有方法尚不能直接处理可控视频生成，凸显本数据集的挑战性与前瞻性。

Conclusion: M^3VIR数据集的发布为下一代云游戏和娱乐领域的AI研究提供了坚实的数据基础，有助于推动生成式AI在内容还原、压缩以及可控内容生成等方面的发展。

Abstract: The gaming and entertainment industry is rapidly evolving, driven by
immersive experiences and the integration of generative AI (GAI) technologies.
Training such models effectively requires large-scale datasets that capture the
diversity and context of gaming environments. However, existing datasets are
often limited to specific domains or rely on artificial degradations, which do
not accurately capture the unique characteristics of gaming content. Moreover,
benchmarks for controllable video generation remain absent.
  To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale,
multi-modal, multi-view dataset specifically designed to overcome the
shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$
provides diverse, high-fidelity gaming content rendered with Unreal Engine 5,
offering authentic ground-truth LR-HR paired and multi-view frames across 80
scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution
(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and
$\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set
enabling research on controlled video generation. Additionally, we benchmark
several state-of-the-art SR and NVS methods to establish performance baselines.
While no existing approaches directly handle controlled video generation,
$\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing
the dataset, we aim to facilitate research in AI-powered restoration,
compression, and controllable content generation for next-generation cloud
gaming and entertainment.

</details>


### [73] [SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation](https://arxiv.org/abs/2509.16886)
*Yingzhen Hu,Yiheng Zhong,Ruobing Li,Yingxue Su,Jiabao An,Feilong Tang,Jionglong Su,Imran Razzak*

Main category: cs.CV

TL;DR: SAM模型在医学图像分割中表现受限，本文提出SAM-DCE方法，通过平衡局部和全局特征、缓解token一致性问题，提高分割精度，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SAM模型虽具零样本分割能力，但用于医学图像时因领域差异、解剖结构多样及依赖用户提示等问题表现不佳。近期无提示方案虽减少专家干预，但仍有鲁棒性和适应性不足、语义过度平滑与token一致性等问题，亟需改进。

Method: 提出SAM-DCE方法，兼顾局部判别性和全局语义信息，缓解token一致性，提升类别区分和掩码解码效果，使分割结果更加细致且一致。

Result: 在多个医学领域基准数据集上进行大量实验，显示该方法有效提升了分割性能。

Conclusion: SAM-DCE能够克服现有方法在医学图像分割中的缺陷，具有更强的鲁棒性和适应性，对医疗图像分割领域有实际应用价值。

Abstract: The Segment Anything Model (SAM) demonstrates impressive zero-shot
segmentation ability on natural images but encounters difficulties in medical
imaging due to domain shifts, anatomical variability, and its reliance on
user-provided prompts. Recent prompt-free adaptations alleviate the need for
expert intervention, yet still suffer from limited robustness and adaptability,
often overlooking the issues of semantic over-smoothing and token uniformity.
We propose SAM-DCE, which balances local discrimination and global semantics
while mitigating token uniformity, enhancing inter-class separability, and
enriching mask decoding with fine-grained, consistent representations.
Extensive experiments on diverse medical benchmarks validate its effectiveness.

</details>


### [74] [Rethinking Evaluation of Infrared Small Target Detection](https://arxiv.org/abs/2509.16888)
*Youwei Pang,Xiaoqi Zhao,Lihe Zhang,Huchuan Lu,Georges El Fakhri,Xiaofeng Liu,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一个新的评估框架和混合层次评价指标，从像素级和目标级综合衡量红外小目标检测(IRSTD)模型性能，并且引入系统性误差分析方法和跨数据集评测，推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测评测存在三大不足：1）评价指标分散且只关注像素或目标层面，缺乏全局性；2）过度关注总分掩盖了误差细节，不利于分析模型失效原因；3）大多采用数据集内评测，忽视了模型鲁棒性和泛化能力的验证。为此，亟需更全面、系统的评测方法促进模型改进。

Method: 本文提出：1）设计混合层次的评价指标，同时反映像素级和目标级性能；2）建立系统性的误差分析方式，揭示模型失效类型，更细致地改进模型；3）强调跨数据集评估，推动对模型泛化能力的考量。此外，作者还发布了开源评测工具集，便于标准化对比与复现。

Result: 提出的评价框架更能全面、细致地反映模型优缺点，并推动了模型鲁棒性和泛化性能的提升。开源工具推动了社区标准化。

Conclusion: 该研究为红外小目标检测领域提供了更合理、更有效的模型评价体系，有助于挖掘现有方法的潜在问题和提升发展，促进了模型在实际应用中的可靠性。

Abstract: As an essential vision task, infrared small target detection (IRSTD) has seen
significant advancements through deep learning. However, critical limitations
in current evaluation protocols impede further progress. First, existing
methods rely on fragmented pixel- and target-level specific metrics, which
fails to provide a comprehensive view of model capabilities. Second, an
excessive emphasis on overall performance scores obscures crucial error
analysis, which is vital for identifying failure modes and improving real-world
system performance. Third, the field predominantly adopts dataset-specific
training-testing paradigms, hindering the understanding of model robustness and
generalization across diverse infrared scenarios. This paper addresses these
issues by introducing a hybrid-level metric incorporating pixel- and
target-level performance, proposing a systematic error analysis method, and
emphasizing the importance of cross-dataset evaluation. These aim to offer a
more thorough and rational hierarchical analysis framework, ultimately
fostering the development of more effective and robust IRSTD models. An
open-source toolkit has be released to facilitate standardized benchmarking.

</details>


### [75] [Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning](https://arxiv.org/abs/2509.16892)
*Jiahe Qian,Yaoyu Fang,Ziqiao Weng,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新型的空间转录组大规模预训练方法CoMTIP，能够同时利用图像、基因名称及其表达量进行联合学习，获得通用且可推广的多模态表示，在多个任务上超越现有方法，并实现了零样本基因表达预测。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组跨模态预训练方法一般只利用基因名称或表达值其中之一，导致基因信息割裂且不能保持基因与其表达量的关联，同时仅关注图像与文本对齐，忽略了学习鲁棒图像特征所需的重要视觉线索。本研究旨在解决这些问题，提升联合表示的泛化能力和下游表现。

Method: 提出了CoMTIP，首次联合利用图像、基因名称与表达量进行对比遮罩式文本-图像预训练。图像分支用遮罩特征建模重建部分图像补丁，学习上下文感知的特征表示。文本分支采用可扩展的基因文本编码器，针对每个基因和其数值构建特定嵌入，并通过对偶对抗训练保持基因-表达量正确关联。最后通过InfoNCE损失在共享空间对齐图像与文本表示。

Result: 在公开空间转录组数据集上，CoMTIP在多项下游任务（如基因表达预测）上均优于现有方法，且首次实现了零样本基因表达预测能力。

Conclusion: CoMTIP实现了多模态、更细粒度的空间转录组联合建模，显著提升了模型表示能力和泛化能力，为跨平台、跨实验室的空间转录组分析提供了强大工具。

Abstract: Spatial transcriptomics aims to connect high-resolution histology images with
spatially resolved gene expression. To achieve better performance on downstream
tasks such as gene expression prediction, large-scale pre-training is required
to obtain generalisable representations that can bridge histology and
transcriptomics across tissues, protocols, and laboratories. Existing
cross-modal pre-training approaches for spatial transcriptomics rely on either
gene names or expression values in isolation, which strips the gene branch of
essential semantics and breaks the association between each gene and its
quantitative magnitude. In addition, by restricting supervision to image-text
alignment, these methods ignore intrinsic visual cues that are critical for
learning robust image features. We present CoMTIP, the first Contrastive Masked
Text-Image Pretraining framework that jointly learns from images, gene names,
and expression values while capturing fine-grained visual context for spatial
transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct
occluded patches and learn context-aware image embeddings. The text branch
applies a scalable Gene-Text Encoder that processes all gene sentences in
parallel, enriches each gene and its numerical value with dedicated embeddings,
and employs Pair-aware Adversarial Training (PAAT) to preserve correct
gene-value associations. Image and text representations are aligned in a shared
InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets
show that CoMTIP not only surpasses previous methods on diverse downstream
tasks but also achieves zero-shot gene expression prediction, a capability that
existing approaches do not provide.

</details>


### [76] [PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion](https://arxiv.org/abs/2509.16897)
*Xuewan He,Jielei Wang,Zihan Cheng,Yuchen Su,Shiyue Huang,Guoming Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的数据无关知识蒸馏（DFKD）方法PRISM，通过提升合成大规模图像数据的稳定性和多样性，从而更有效地实现知识迁移，解决现有方法在大规模场景下的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有DFKD方法在小规模图像上表现良好，但在大规模图像上由于模式崩溃而导致知识迁移效果有限。同时，直接用扩散模型生成数据会面临精度-召回的矛盾：合成数据是否贴合真实分布，以及能否覆盖真实分布的多样性。

Method: 作者提出PRISM方法，包含两大创新：1）能量引导的分布对齐（Energy-guided Distribution Alignment）用于避免生成分布外（OOD）样本；2）多样化提示工程（Diversified Prompt Engineering）提升合成数据对真实数据流形的覆盖度。

Result: 在多种大规模图像数据集上，PRISM表现优于现有DFKD方法。实验显示合成数据的质量和多样性均得到了提升，为学生模型带来了更高的性能。

Conclusion: PRISM有效解决了DFKD在大规模图像上的模式崩溃和精度-召回难题，提升了无数据知识蒸馏的实际应用价值，并增强了模型的领域泛化能力。

Abstract: Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to
a student without access to the real in-distribution (ID) data. While existing
methods perform well on small-scale images, they suffer from mode collapse when
synthesizing large-scale images, resulting in limited knowledge transfer.
Recently, leveraging advanced generative models to synthesize photorealistic
images has emerged as a promising alternative. Nevertheless, directly using
off-the-shelf diffusion to generate datasets faces the precision-recall
challenges: 1) ensuring synthetic data aligns with the real distribution, and
2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a
precision-recall informed synthesis method. Specifically, we introduce
Energy-guided Distribution Alignment to avoid the generation of
out-of-distribution samples, and design the Diversified Prompt Engineering to
enhance coverage of the real ID manifold. Extensive experiments on various
large-scale image datasets demonstrate the superiority of PRISM. Moreover, we
demonstrate that models trained with PRISM exhibit strong domain
generalization.

</details>


### [77] [ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis](https://arxiv.org/abs/2509.16900)
*Chengsheng Zhang,Linhao Qu,Xiaoyu Liu,Zhijian Song*

Main category: cs.CV

TL;DR: 本文提出了一种名为ME-Mamba的多专家系统，通过高效融合病理全切片图像（WSIs）和基因组学数据，为癌症生存分析提供了新的多模态方法。该方法在TCGA五个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 癌症生存分析依赖于大量病理图像，但通常只有切片级标签，难以学习判别性特征；而基因组数据的兴起为多模态分析带来可能，因此需要一种有效利用并融合两种模态信息的方法，提高分析精准度。

Method: 1. 设计了病理专家和基因组专家，分别处理单模态数据，并利用Mamba结构结合常规扫描与注意机制提取重要特征。2. 设计协同专家通过最优传输方法进行模态间局部对应学习，并用最大均值差异（MMD）全局损失提升分布一致性。3. 融合特征再由Mamba骨干进一步整合，最终完善生存分析。

Result: 该方法在Cancer Genome Atlas（TCGA）五个公开数据集上进行了大量实验，均取得了当前最先进的性能，稳定性与效率俱佳。

Conclusion: ME-Mamba系统通过多专家协同和高效特征融合，实现了低计算复杂度下的精准癌症生存分析，为多模态医学人工智能应用提供了新思路。

Abstract: Survival analysis using whole-slide images (WSIs) is crucial in cancer
research. Despite significant successes, pathology images typically only
provide slide-level labels, which hinders the learning of discriminative
representations from gigapixel WSIs. With the rapid advancement of
high-throughput sequencing technologies, multimodal survival analysis
integrating pathology images and genomics data has emerged as a promising
approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures
discriminative pathological and genomic features while enabling efficient
integration of both modalities. This approach achieves complementary
information fusion without losing critical information from individual
modalities, thereby facilitating accurate cancer survival analysis.
Specifically, we first introduce a Pathology Expert and a Genomics Expert to
process unimodal data separately. Both experts are designed with Mamba
architectures that incorporate conventional scanning and attention-based
scanning mechanisms, allowing them to extract discriminative features from long
instance sequences containing substantial redundant or irrelevant information.
Second, we design a Synergistic Expert responsible for modality fusion. It
explicitly learns token-level local correspondences between the two modalities
via Optimal Transport, and implicitly enhances distribution consistency through
a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused
feature representations are then passed to a mamba backbone for further
integration. Through the collaboration of the Pathology Expert, Genomics
Expert, and Synergistic Expert, our method achieves stable and accurate
survival analysis with relatively low computational complexity. Extensive
experimental results on five datasets in The Cancer Genome Atlas (TCGA)
demonstrate our state-of-the-art performance.

</details>


### [78] [SLAM-Former: Putting SLAM into One Transformer](https://arxiv.org/abs/2509.16909)
*Yijun Yuan,Zhuoguang Chen,Kenan Li,Weibang Wang,Hang Zhao*

Main category: cs.CV

TL;DR: SLAM-Former是一种将完整SLAM能力整合到单一Transformer中的新型神经方法，能高效处理实时建图和追踪，并在性能上优于或媲美最新SLAM方法。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统主要依赖传统算法，而神经方法难以集成完整SLAM流程。本文提出SLAM-Former，旨在探索如何将前端追踪与后端优化全流程高效集成入深度神经网络架构中。

Method: SLAM-Former的架构借鉴传统SLAM，采用Transformer模块分为前端和后端。前端实时处理单目图像，实现增量建图与追踪，后端则进行全局优化以保证几何一致性，并通过前后端交替，彼此促进提升性能。

Result: 实验结果显示，SLAM-Former在多个数据集上性能优异，超越或与最新的密集型SLAM方法持平。

Conclusion: SLAM-Former验证了神经SLAM端到端融合的可行性和高效性，为SLAM全流程深度网络设计提供了新思路。

Abstract: We present SLAM-Former, a novel neural approach that integrates full SLAM
capabilities into a single transformer. Similar to traditional SLAM systems,
SLAM-Former comprises both a frontend and a backend that operate in tandem. The
frontend processes sequential monocular images in real-time for incremental
mapping and tracking, while the backend performs global refinement to ensure a
geometrically consistent result. This alternating execution allows the frontend
and backend to mutually promote one another, enhancing overall system
performance. Comprehensive experimental results demonstrate that SLAM-Former
achieves superior or highly competitive performance compared to
state-of-the-art dense SLAM methods.

</details>


### [79] [Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](https://arxiv.org/abs/2509.16935)
*Lavish Ramchandani,Gunjan Deotale,Dev Kumar Das*

Main category: cs.CV

TL;DR: 本文利用大型视觉基础模型和低秩适应(LoRA)微调方法，在MIDOG 2025挑战中对非典型有丝分裂的深度学习分类进行了系统评估，取得了88.37%的平衡准确率，展示了基础模型在该任务中的潜力但仍需提升泛化性和特异性。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂与肿瘤侵袭性和预后不良相关，但因形态特征细微且样本比例失衡，病理学家的判读具有主观性，检测极具挑战性。因此，期望借助先进深度学习技术提升其检测性能。

Method: 作者应用大规模视觉基础模型（Virchow、Virchow2、UNI）及LoRA技术进行参数高效微调，针对不同LoRA秩和多样的数据划分（随机/分组）进行了详尽实验，并采用三折交叉验证集成，系统评估方法的健壮性。

Result: 最佳方案（Virchow结合LoRA秩8与三折交叉验证集成）在初步测试集上获得了88.37%的平衡准确率，在挑战排名中并列第9。

Conclusion: 视觉基础模型结合高效微调策略在非典型有丝分裂分类任务中展现出良好前景，但在提高领域泛化性和分类特异性方面仍有优化空间。

Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated
with tumor aggressiveness and poor prognosis. Their detection remains a
significant challenge due to subtle morphological cues, class imbalance, and
inter-observer variability among pathologists. The MIDOG 2025 challenge
introduced a dedicated track for atypical mitosis classification, enabling
systematic evaluation of deep learning methods. In this study, we investigated
the use of large vision foundation models, including Virchow, Virchow2, and
UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We
conducted extensive experiments with different LoRA ranks, as well as random
and group-based data splits, to analyze robustness under varied conditions. Our
best approach, Virchow with LoRA rank 8 and ensemble of three-fold
cross-validation, achieved a balanced accuracy of 88.37% on the preliminary
test set, ranking joint 9th in the challenge leaderboard. These results
highlight the promise of foundation models with efficient adaptation strategies
for the classification of atypical mitosis, while underscoring the need for
improvements in specificity and domain generalization.

</details>


### [80] [Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.16942)
*Bin Wang,Fei Deng,Zeyu Chen,Zhicheng Yu,Yiguang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProSFDA的原型引导型无源域自适应（SFDA）方法，用于遥感影像的语义分割。方法重点通过原型加权的伪标签和原型对比策略，提高模型在无标签目标域下自训练的稳健性和判别性。实验结果显示，ProSFDA在应对伪标签噪声和减少领域差异方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法在没有目标域标签的情况下，需使用伪标签来自训练模型。但生成的伪标签常常包含噪声，影响了领域自适应的效果。如何在目标域无标签数据下，提升伪标签的可靠性和模型对领域差异的适应能力，是本文关注的核心问题。

Method: 提出ProSFDA框架，包含两项创新策略：1）原型加权伪标签，通过计算类别原型与特征的相似度，为伪标签赋予权重，过滤噪声伪标签，提升自训练效果；2）原型对比策略，通过聚合同类别特征，强化类别区分性，从而在无监督条件下学习更判别的表征。

Result: 在多个典型遥感影像数据集上，ProSFDA相比现有SFDA方法取得了显著更好的语义分割准确率。特别是在目标域标注极其稀缺的条件下，对领域间分布差异的适应能力更强。

Conclusion: ProSFDA能够有效缓解无标签目标域中的伪标签噪声问题，通过原型加权和原型对比策略，提升模型在SFDA场景下的表现，对遥感图像分割等相关领域具有明显的应用前景。

Abstract: Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic
segmentation of Remote Sensing Images (RSIs) using only a well-trained source
model and unlabeled target domain data. However, the lack of ground-truth
labels in the target domain often leads to the generation of noisy
pseudo-labels. Such noise impedes the effective mitigation of domain shift
(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA
framework. It employs prototype-weighted pseudo-labels to facilitate reliable
self-training (ST) under pseudo-labels noise. We, in addition, introduce a
prototype-contrast strategy that encourages the aggregation of features
belonging to the same class, enabling the model to learn discriminative target
domain representations without relying on ground-truth supervision. Extensive
experiments show that our approach substantially outperforms existing methods.

</details>


### [81] [CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception](https://arxiv.org/abs/2509.17107)
*Lingzhao Kong,Jiacheng Lin,Siyu Li,Kai Luo,Zhiyong Li,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多智能体协同感知框架CoBEVMoE，通过动态专家混合机制提升融合的效果，在多个数据集上表现领先。


<details>
  <summary>Details</summary>
Motivation: 协同感知中，不同智能体面对视角和空间位置差异，采集到的信息具有异质性，现有方法主要对齐相似特征，忽视了这种感知多样性，导致感知融合效果受限。

Method: 提出CoBEVMoE框架，基于鸟瞰图空间（BEV），引入动态Mixture-of-Experts（DMoE）架构。每个专家根据特定智能体的输入特征动态生成，挖掘独特且可靠的感知信息，同时关注共享语义。提出Dynamic Expert Metric Loss（DEML），增强专家间的多样性，提升融合特征区分性。

Result: 在OPV2V和DAIR-V2X-C数据集上，方法分别在基于摄像头的BEV分割任务IoU提升1.5%，在基于激光雷达的三维目标检测AP@50提升3.0%。

Conclusion: 动态专家机制能有效建模多智能体之间的异质特征，提升协同感知性能，实现当前最优效果。

Abstract: Collaborative perception aims to extend sensing coverage and improve
perception accuracy by sharing information among multiple agents. However, due
to differences in viewpoints and spatial positions, agents often acquire
heterogeneous observations. Existing intermediate fusion methods primarily
focus on aligning similar features, often overlooking the perceptual diversity
among agents. To address this limitation, we propose CoBEVMoE, a novel
collaborative perception framework that operates in the Bird's Eye View (BEV)
space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In
DMoE, each expert is dynamically generated based on the input features of a
specific agent, enabling it to extract distinctive and reliable cues while
attending to shared semantics. This design allows the fusion process to
explicitly model both feature similarity and heterogeneity across agents.
Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance
inter-expert diversity and improve the discriminability of the fused
representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets
demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,
it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the
AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the
effectiveness of expert-based heterogeneous feature modeling in multi-agent
collaborative perception. The source code will be made publicly available at
https://github.com/godk0509/CoBEVMoE.

</details>


### [82] [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://arxiv.org/abs/2509.16944)
*Yuheng Shi,Xiaohuan Pei,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效且无需标注的自蒸馏候选区域网络（SD-RPN），通过改进多模态大语言模型（MLLMs）的细粒度感知能力，实现高分辨率区域定位与数据效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM面临实用难题：需要高分辨率视觉信息以实现细粒度感知，但处理完整高分辨率图像计算消耗极大。已有区域关注机制受限于数据集规模或计算效率，存在明显权衡矛盾。

Method: 作者提出SD-RPN方法，将MLLM中间层的噪声注意力图转化为高质量的伪RoI标签，通过显式去噪和消除歧义，训练轻量级RPN以进行精确定位。该RPN可在一次前向传递内预测RoI，不需要多轮预填或慢速自回归过程。

Result: SD-RPN集成到LLaVA-1.5架构中，在只用极少数据（如1万对QA数据）训练的情况下，能在TextVQA、DocVQA、V-Star等未见基准上取得超10%绝对准确率提升，展现极高的数据效率和泛化能力。

Conclusion: 无需昂贵人工标注或大规模微调，所提SD-RPN为MLLMs实现细粒度视觉感知与计算高效性提供了一种实用且可扩展的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual
information to perform fine-grained perception, yet processing entire
high-resolution images is computationally prohibitive. While recent methods
leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they
typically present a difficult trade-off: training-based approaches depend on
large-scale annotated datasets, while training-free methods that utilize the
model's internal attention are computationally inefficient and less accurate,
requiring either multi-pass prefill stages or reliance on the slow
auto-regressive decoding process. In this paper, we propose an efficient,
annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves
this trade-off. The SD-RPN is built around a pipeline that transforms the noisy
attention maps from the MLLM's middle layers into high-quality pseudo-RoI
labels by explicitly denoising the signal and resolving ambiguity. We use these
labels to train a lightweight Region Proposal Network (RPN) that learns a more
precise localization. This RPN is also highly efficient, predicting the RoI in
a single forward pass using features from the MLLM's middle layers, decoupling
RoI identification from the auto-regressive generation and avoiding costly
multi-pass operations.To validate our approach, we integrate the framework into
the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)
question-answer pairs, our method demonstrates exceptional data efficiency and
generalization, achieving over a 10% absolute accuracy improvement on unseen
benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a
practical and scalable solution for enhancing the fine-grained perception of
MLLMs without requiring costly supervision or full model fine-tuning. Code is
available at https://github.com/YuHengsss/SD-RPN.

</details>


### [83] [DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking](https://arxiv.org/abs/2509.17323)
*Buyin Deng,Lingxin Huang,Kai Luo,Fei Teng,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了DepTR-MOT，一种融合实例级深度信息的DETR架构多目标跟踪方法，显著提升了在遮挡和近距离场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于检测的多目标跟踪方法（TBD）主要依赖二维线索，如边界框和运动建模，但这些在出现遮挡或目标密集时往往效果不佳。在机器人环境下，这些问题更加突出。而深度信息有望缓解上述困境，但目前公开数据集多数缺乏深度标注，导致深度线索在MOT领域未被充分利用。

Method: 提出DepTR-MOT，以DETR为基础，创新性地引入了（1）基础模型驱动的实例级软深度标签监督，用于优化深度预测；（2）致密深度图蒸馏算法，用于保持全局深度一致性。这些策略使得模型在推理阶段能输出实例级深度，无需额外计算开销，也不依赖基础模型。

Result: 在QuadTrack和DanceTrack两个数据集上的实验结果显示，所提方法分别达到了27.59和44.47的HOTA分数。尤其在QuadTrack（机器人平台MOT数据集）测试中，提出方法在遮挡和近距离跟踪下展现了明显优势。

Conclusion: DepTR-MOT通过融合深度感知，有效解决了TBD方法在遮挡和密集环境下的不足，显著提升了多目标跟踪在机器人应用中的鲁棒性。源码即将开源。

Abstract: Visual Multi-Object Tracking (MOT) is a crucial component of robotic
perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D
cues, such as bounding boxes and motion modeling, which struggle under
occlusions and close-proximity interactions. Trackers relying on these 2D cues
are particularly unreliable in robotic environments, where dense targets and
frequent occlusions are common. While depth information has the potential to
alleviate these issues, most existing MOT datasets lack depth annotations,
leading to its underexploited role in the domain. To unveil the potential of
depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based
detector enhanced with instance-level depth information. Specifically, we
propose two key innovations: (i) foundation model-based instance-level soft
depth label supervision, which refines depth prediction, and (ii) the
distillation of dense depth maps to maintain global depth consistency. These
strategies enable DepTR-MOT to output instance-level depth during inference,
without requiring foundation models and without additional computational cost.
By incorporating depth cues, our method enhances the robustness of the TBD
paradigm, effectively resolving occlusion and close-proximity challenges.
Experiments on both the QuadTrack and DanceTrack datasets demonstrate the
effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,
respectively. In particular, results on QuadTrack, a robotic platform MOT
dataset, highlight the advantages of our method in handling occlusion and
close-proximity challenges in robotic tracking. The source code will be made
publicly available at https://github.com/warriordby/DepTR-MOT.

</details>


### [84] [Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation](https://arxiv.org/abs/2509.16949)
*Ruicong Liu,Takehiko Ohkawa,Tze Ho Elden Tse,Mingfang Zhang,Angela Yao,Yoichi Sato*

Main category: cs.CV

TL;DR: 本论文提出了RPEP，这是首个利用带标签的RGB图像和无配对标签的事件数据进行预训练，用于事件驱动的三维手势估计的方法。通过新颖的事件数据生成及运动分解技术，解决了事件数据标注稀缺的问题，大幅提升了估计效果。


<details>
  <summary>Details</summary>
Motivation: 事件相机在三维手势估计方面具有高时域分辨率和低延迟等优势，但因缺乏标注数据，实际应用受限。该工作旨在利用丰富的RGB带标签数据帮助事件手势估计器的训练，从而突破事件数据标注不足的瓶颈。

Method: 方法核心为构建伪事件-RGB对：先用现有带标签的RGB手势数据，通过新提出的事件数据生成策略，分解手部连续动作，产生更真实符合动态手势的事件数据，并引入运动逆转约束进行正则化，从而进行事件手势估计模型的预训练。

Result: 在真实事件手势数据集EvRealHands上获得了最高24%的性能提升，相较当前主流方法取得显著领先，并且在仅少量标注样本下模型仍有优秀表现。

Conclusion: RPEP有效缓解了事件手势估计中的标注瓶颈，大幅改善了模型性能，有望推动事件相机在三维手势估计实际场景的落地应用。

Abstract: This paper presents RPEP, the first pre-training method for event-based 3D
hand pose estimation using labeled RGB images and unpaired, unlabeled event
data. Event data offer significant benefits such as high temporal resolution
and low latency, but their application to hand pose estimation is still limited
by the scarcity of labeled training data. To address this, we repurpose real
RGB datasets to train event-based estimators. This is done by constructing
pseudo-event-RGB pairs, where event data is generated and aligned with the
ground-truth poses of RGB images. Unfortunately, existing pseudo-event
generation techniques assume stationary objects, thus struggling to handle
non-stationary, dynamically moving hands. To overcome this, RPEP introduces a
novel generation strategy that decomposes hand movements into smaller,
step-by-step motions. This decomposition allows our method to capture temporal
changes in articulation, constructing more realistic event data for a moving
hand. Additionally, RPEP imposes a motion reversal constraint, regularizing
event generation using reversed motion. Extensive experiments show that our
pre-trained model significantly outperforms state-of-the-art methods on real
event data, achieving up to 24% improvement on EvRealHands. Moreover, it
delivers strong performance with minimal labeled samples for fine-tuning,
making it well-suited for practical deployment.

</details>


### [85] [EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](https://arxiv.org/abs/2509.17430)
*Gunjan Chhablani,Xiaomeng Ye,Muhammad Zubair Irshad,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯散点重建（GS）和Habitat-Sim仿真器的新方法，能够用普通手机快速重建实际部署环境，从而高效地对AI智能体进行环境自适应训练，极大提升了虚拟到现实（sim-to-real）任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有Embodied AI大多依赖合成仿真环境训练，但这些环境缺乏真实感或需高成本硬件获取，导致虚拟到现实迁移存在很大性能损失。

Method: 利用手机拍摄获取目标环境数据，通过3D Gaussian Splatting重建高保真场景，在Habitat-Sim中进行个性化的策略微调训练。系统地评估了不同训练策略、预训练数据集、重建技术对迁移性能的影响。

Result: 通过EmbodiedSplat微调的智能体在真实Image Navigation任务上胜过在大规模现实或合成数据集预训练的基线模型，分别提升20%和40%成功率，且重建场景的仿真与真实表现高度相关（相关系数0.87-0.97）。

Conclusion: EmbodiedSplat有效缩小了仿真-现实差距，使得AI智能体能低成本适应多样化真实环境，显著提升迁移效果。

Abstract: The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20\% and 40\% on real-world Image Navigation task. Moreover,
our approach yields a high sim-vs-real correlation (0.87--0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat

</details>


### [86] [VidCLearn: A Continual Learning Approach for Text-to-Video Generation](https://arxiv.org/abs/2509.16956)
*Luca Zanchetta,Lorenzo Papa,Luca Maiano,Irene Amerini*

Main category: cs.CV

TL;DR: 本文提出了VidCLearn，一个用于文本生成视频的持续学习框架，有效解决了现有模型难以增量学习的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成视频模型难以持续学习新数据，需要从头训练，效率低下，限制了实际应用。

Method: VidCLearn采用学生-教师架构，学生模型持续增量学习新数据，教师模型通过生成式重放保持已学知识；引入新颖的时序一致性损失提升运动平滑性，并加入视频检索模块在推理时提供结构引导。整体设计更加高效，减少计算资源消耗。

Result: 实验结果表明，VidCLearn在视觉质量、语义一致性和时间连贯性上均优于基线方法。

Conclusion: VidCLearn有效实现了高效、可持续的文本到视频生成，推动该领域的发展，兼顾生成性能和计算资源消耗，可为实际应用提供支持。

Abstract: Text-to-video generation is an emerging field in generative AI, enabling the
creation of realistic, semantically accurate videos from text prompts. While
current models achieve impressive visual quality and alignment with input text,
they typically rely on static knowledge, making it difficult to incorporate new
data without retraining from scratch. To address this limitation, we propose
VidCLearn, a continual learning framework for diffusion-based text-to-video
generation. VidCLearn features a student-teacher architecture where the student
model is incrementally updated with new text-video pairs, and the teacher model
helps preserve previously learned knowledge through generative replay.
Additionally, we introduce a novel temporal consistency loss to enhance motion
smoothness and a video retrieval module to provide structural guidance at
inference. Our architecture is also designed to be more computationally
efficient than existing models while retaining satisfactory generation
performance. Experimental results show VidCLearn's superiority over baseline
methods in terms of visual quality, semantic alignment, and temporal coherence.

</details>


### [87] [VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video](https://arxiv.org/abs/2509.17647)
*Yu Liu,Baoxiong Jia,Ruijie Lu,Chuyue Gan,Huayu Chen,Junfeng Ni,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法VideoArtGS，可从单目视频高精度重建带关节物体的数字孪生，实现了几何、分割和运动参数的同步估计。该方法在现有技术上大幅提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有从单目视频重建带关节物体的技术存在几何与动态解耦困难、监督信号不足等问题，严重影响重建质量。作者希望解决这些在仅用视频输入重建高质量数字孪生面临的关键挑战。

Method: 提出VideoArtGS方法，引入运动先验引导流程，通过3D轨迹分析与降噪，提供可靠的运动参数初始化，并设计混合的center-grid零件分配模块，用于提升构建关节变形场时的部件运动精确度。

Result: VideoArtGS在运动分解和网格重建方面实现了业界领先的表现，将重建误差降低了约两个数量级，显著优于现有方法。

Conclusion: VideoArtGS显著推进了基于视频的带关节物体数字孪生重建，为实际场景数字孪生创建提供了新的基线和工具。

Abstract: Building digital twins of articulated objects from monocular video presents
an essential challenge in computer vision, which requires simultaneous
reconstruction of object geometry, part segmentation, and articulation
parameters from limited viewpoint inputs. Monocular video offers an attractive
input format due to its simplicity and scalability; however, it's challenging
to disentangle the object geometry and part dynamics with visual supervision
alone, as the joint movement of the camera and parts leads to ill-posed
estimation. While motion priors from pre-trained tracking models can alleviate
the issue, how to effectively integrate them for articulation learning remains
largely unexplored. To address this problem, we introduce VideoArtGS, a novel
approach that reconstructs high-fidelity digital twins of articulated objects
from monocular video. We propose a motion prior guidance pipeline that analyzes
3D tracks, filters noise, and provides reliable initialization of articulation
parameters. We also design a hybrid center-grid part assignment module for
articulation-based deformation fields that captures accurate part motion.
VideoArtGS demonstrates state-of-the-art performance in articulation and mesh
reconstruction, reducing the reconstruction error by about two orders of
magnitude compared to existing methods. VideoArtGS enables practical digital
twin creation from monocular video, establishing a new benchmark for
video-based articulated object reconstruction. Our work is made publicly
available at: https://videoartgs.github.io.

</details>


### [88] [MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image](https://arxiv.org/abs/2509.16957)
*Leiyu Wang,Biao Jin,Feng Huang,Liqiong Chen,Zhengyong Wang,Xiaohai He,Honggang Chen*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为MO R-CNN的高效多光谱定向目标检测框架，有效提升了检测精度且降低了计算和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像的目标检测存在模态内与模态间的差异，当前提升检测准确率的方法大多依赖复杂网络，导致高计算和存储成本，限制了实际应用。

Method: 作者提出了轻量级MO R-CNN，包含异构特征提取网络(HFEN)、单模态监督(SMS)及基于条件的多模态标签融合(CMLF)：HFEN自适应对齐和融合多模态特征，SMS监督多尺度特征以促进跨模态学习，CMLF按规则融合多模态标签，提升模型监督一致性。

Result: 在DroneVehicle、VEDAI和OGSOD数据集上的实验表明，MO R-CNN在检测精度和效率方面均优于现有方法。

Conclusion: MO R-CNN为多光谱定向目标检测提供了高效、低复杂度的新思路，有助于实际应用推广。源码已公开。

Abstract: Oriented object detection for multi-spectral imagery faces significant
challenges due to differences both within and between modalities. Although
existing methods have improved detection accuracy through complex network
architectures, their high computational complexity and memory consumption
severely restrict their performance. Motivated by the success of large kernel
convolutions in remote sensing, we propose MO R-CNN, a lightweight framework
for multi-spectral oriented detection featuring heterogeneous feature
extraction network (HFEN), single modality supervision (SMS), and
condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal
differences to adaptively align, merge, and enhance multi-modal features. SMS
constrains multi-scale features and enables the model to learn from multiple
modalities. CMLF fuses multimodal labels based on specific rules, providing the
model with a more robust and consistent supervisory signal. Experiments on the
DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The
source code is available at:https://github.com/Iwill-github/MORCNN.

</details>


### [89] [DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2509.17684)
*ThankGod Egbe,Peng Wang,Zhihao Guo,Zidong Chen*

Main category: cs.CV

TL;DR: 本文评估了DINOv3这一最新自监督视觉骨干网络，在机器人操作任务中的表现，并将其与传统的ImageNet预训练（如ResNet-18）进行比较，发现DINOv3在多个任务上具有更好或相当的性能，尤其在样本效率和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉编码器通常依赖于有监督的ImageNet预训练，但近年来自监督大模型的能力迅速提升。作者希望探索，大规模自监督视觉模型能否取代有监督预训练作为机器人操作中的感知前端，尤其在无需标签的场景下是否更具通用性和实用性。

Method: 作者采用了DINOv3作为视觉特征编码器，并在四个机器人操作基准任务（Push-T、Lift、Can、Square）上进行对比实验。比较对象为ResNet-18（ImageNet有监督预训练），分别考察三种训练方式：从头训练、参数冻结以及微调。所有实验均基于FiLM调制的扩散策略进行。

Result: 微调下的DINOv3在多个任务中达到或超过了ResNet-18的表现；即使仅参数冻结，DINOv3也具有较强的可迁移性。自监督特征还带来了更好的样本效率和鲁棒性。在最具挑战性的Can任务上，DINOv3背后的方法测试成功率比ResNet18高出10%，其他任务表现相当。

Conclusion: 大规模自监督视觉模型可以作为机器操作扩散策略的高效、可泛化的感知前端。该工作显示，通过可扩展的无标签预训练，能提升机器人操作策略的性能，值得进一步研究和推广。

Abstract: This paper evaluates DINOv3, a recent large-scale self-supervised vision
backbone, for visuomotor diffusion policy learning in robotic manipulation. We
investigate whether a purely self-supervised encoder can match or surpass
conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under
three regimes: training from scratch, frozen, and finetuned. Across four
benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned
diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds
ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating
strong transferable priors, and (iii) self-supervised features improve sample
efficiency and robustness. These results support self-supervised large visual
models as effective, generalizable perceptual front-ends for action diffusion
policies, motivating further exploration of scalable label-free pretraining in
robotic manipulation. Compared to using ResNet18 as a backbone, our approach
with DINOv3 achieves up to a 10% absolute increase in test-time success rates
on challenging tasks such as Can, and on-the-par performance in tasks like
Lift, PushT, and Square.

</details>


### [90] [Penalizing Boundary Activation for Object Completeness in Diffusion Models](https://arxiv.org/abs/2509.16968)
*Haoyang Xu,Tianhao Zhao,Sibei Yang,Yutian Li*

Main category: cs.CV

TL;DR: 本文针对扩散模型在文本生成图像任务中易生成不完整物体的问题，提出了一种无需重新训练、可直接应用于预训练模型的训练外解决方案，显著提升了图像完整性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在文本到图像生成时，经常会出现物体不完整、残缺的现象，这影响了后续应用效果。通过分析发现，这主要由于训练时常用的数据增强方式RandomCrop破坏了物体的连贯性。

Method: 作者提出了一种训练外的方法：在去噪初期对图像边界的激活值进行惩罚，可以在无需重新训练的情况下，直接应用于预训练的Stable Diffusion等扩散模型，并且只需极小的计算开销。

Result: 实验结果表明，该方法显著提升了扩散模型生成图像的物体完整性，同时提升了生成图像的整体质量。

Conclusion: 通过在图像生成过程中对边界激活值施加惩罚，可有效缓解因RandomCrop带来的物体不完整问题，方法简单高效，具有良好实用价值。

Abstract: Diffusion models have emerged as a powerful technique for text-to-image (T2I)
generation, creating high-quality, diverse images across various domains.
However, a common limitation in these models is the incomplete display of
objects, where fragments or missing parts undermine the model's performance in
downstream applications. In this study, we conduct an in-depth analysis of the
incompleteness issue and reveal that the primary factor behind incomplete
object generation is the usage of RandomCrop during model training. This widely
used data augmentation method, though enhances model generalization ability,
disrupts object continuity during training. To address this, we propose a
training-free solution that penalizes activation values at image boundaries
during the early denoising steps. Our method is easily applicable to
pre-trained Stable Diffusion models with minimal modifications and negligible
computational overhead. Extensive experiments demonstrate the effectiveness of
our method, showing substantial improvements in object integrity and image
quality.

</details>


### [91] [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](https://arxiv.org/abs/2509.16970)
*Wei Liao,Chunyan Xu,Chenxu Wang,Zhen Cui*

Main category: cs.CV

TL;DR: 本论文提出了一种结合大语言模型（LLM）语义引导的框架，以解决遥感目标检测中的稀疏标注问题，并显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中目标分布密集且类别不平衡，导致稀疏标注下目标检测效果受限。现有的Dense Pseudo-Label方法在伪标签生成方面潜力大，但存在选择歧义和置信度估计不一致的问题，因此亟需改进。

Method: 提出了一种LLM辅助的语义引导框架，利用LLM的高阶语义推理能力，生成高置信度伪标签；引入Class-Aware Dense Pseudo-Label Assignment机制，针对未标注和稀疏标注样本适应性分配伪标签；并设计了自适应难负样本重加权模块，以稳定监督分支并弱化背景干扰。

Result: 在DOTA和HRSC2016这两个遥感目标检测数据集上的实验结果表明，所提方法优于现有的单阶段检测器框架，在稀疏标注情形下检测性能有显著提升。

Conclusion: 融合LLM语义能力的伪标签分配和难负样本处理能够有效缓解稀疏标注带来的监督信息不足问题，为遥感目标检测的实际应用提供了更鲁棒的解决方案。

Abstract: Sparse annotation in remote sensing object detection poses significant
challenges due to dense object distributions and category imbalances. Although
existing Dense Pseudo-Label methods have demonstrated substantial potential in
pseudo-labeling tasks, they remain constrained by selection ambiguities and
inconsistencies in confidence estimation.In this paper, we introduce an
LLM-assisted semantic guidance framework tailored for sparsely annotated remote
sensing object detection, exploiting the advanced semantic reasoning
capabilities of large language models (LLMs) to distill high-confidence
pseudo-labels.By integrating LLM-generated semantic priors, we propose a
Class-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns
pseudo-labels for both unlabeled and sparsely labeled data, ensuring robust
supervision across varying data distributions. Additionally, we develop an
Adaptive Hard-Negative Reweighting Module to stabilize the supervised learning
branch by mitigating the influence of confounding background information.
Extensive experiments on DOTA and HRSC2016 demonstrate that the proposed method
outperforms existing single-stage detector-based frameworks, significantly
improving detection performance under sparse annotations.

</details>


### [92] [The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA](https://arxiv.org/abs/2509.16972)
*Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji*

Main category: cs.CV

TL;DR: 本论文提出了SaSaSa2VA方法，解决了基于多模态大语言模型的指向性视频目标分割中的关键瓶颈，在第七届LSVOS大赛中取得第一。


<details>
  <summary>Details</summary>
Motivation: 现有指向性视频目标分割方法（如Sa2VA）在分割性能上受限于稀疏的帧采样和对整段视频只用单一[SEG] token，导致无法充分利用时空信息。

Method: 提出了Segmentation Augmented和Selective Averaged方法，结合多模态大语言模型与视频分割模型SAM2，通过增强分割和测试时集成，有效提升了目标分割的精度和稳定性。

Result: 在第七届LSVOS Challenge (RVOS track) 中，SaSaSa2VA取得了67.45的J&F分数，领先亚军2.80分，并通过消融实验验证了方法的有效性。

Conclusion: 高效的分割增强和测试时集成策略能显著提升基于大语言模型的指向性视频目标分割效果。相关代码已开源。

Abstract: Referring video object segmentation (RVOS) requires segmenting and tracking
objects in videos conditioned on natural-language expressions, demanding
fine-grained understanding of both appearance and motion. Building on Sa2VA,
which couples a Multi-modal Large Language Model (MLLM) with the video
segmentation model SAM2, we identify two key bottlenecks that limit
segmentation performance: sparse frame sampling and reliance on a single [SEG]
token for an entire video. We propose Segmentation Augmented and Selective
Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge
(RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and
surpassing the runner-up by 2.80 points. This result and ablation studies
demonstrate that efficient segmentation augmentation and test-time ensembling
substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA
repository: https://github.com/magic-research/Sa2VA.

</details>


### [93] [Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime](https://arxiv.org/abs/2509.16977)
*Petros Georgoulas Wraight,Giorgos Sfikas,Ioannis Kordonis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的手写文本识别（HTR）方法，利用了有限标注数据，通过视觉特征与语义词表示的对齐和迭代自举策略，在低资源场景下显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有HTR方法依赖大量标注数据，而历史档案或小型数据集普遍缺乏这些资源，现有方法不可行，需要能在少量标注下有效训练的新方法。

Method: 提出使用最优传输（OT）方法，将无标注图像的视觉特征与语义词嵌入对齐。以极少量标注样本为起点，迭代进行词图像和标签的配对，对高置信度的配对自动生成伪标签，并用不断扩充的数据集自举训练识别器。

Result: 通过数值实验验证，该视觉-语义迭代对齐方法在低资源HTR基准测试中显著提升了识别准确率。

Conclusion: 所提方法能有效利用有限标注和可用的先验词典知识，为低资源领域提供了实用的HTR解决方案。

Abstract: Handwritten Text Recognition (HTR) is a task of central importance in the
field of document image understanding. State-of-the-art methods for HTR require
the use of extensive annotated sets for training, making them impractical for
low-resource domains like historical archives or limited-size modern
collections. This paper introduces a novel framework that, unlike the standard
HTR model paradigm, can leverage mild prior knowledge of lexical
characteristics; this is ideal for scenarios where labeled data are scarce. We
propose an iterative bootstrapping approach that aligns visual features
extracted from unlabeled images with semantic word representations using
Optimal Transport (OT). Starting with a minimal set of labeled examples, the
framework iteratively matches word images to text labels, generates
pseudo-labels for high-confidence alignments, and retrains the recognizer on
the growing dataset. Numerical experiments demonstrate that our iterative
visual-semantic alignment scheme significantly improves recognition accuracy on
low-resource HTR benchmarks.

</details>


### [94] [VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation](https://arxiv.org/abs/2509.16986)
*Feng Han,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种专为自回归图像生成模型（如GPT-4o、LlamaGen）设计的安全防护方法，能够有效擦除不安全或侵权概念而不影响其他内容。


<details>
  <summary>Details</summary>
Motivation: 随着自回归文本到图像模型生成高质量、逼真图像，其生成不安全内容（如NSFW）以及侵犯艺术风格版权的风险日增。因此，迫切需要专门针对自回归模型的安全机制，但相关方法研究不足。现有的概念擦除大多针对扩散模型，不适用于逐token生成的自回归模型。

Method: 提出“视觉对比挖掘（VCE）”框架，包括创新性的对比图像对构建方法，能够精确区分不安全概念与内容语义，并采用DPO（Direct Preference Optimization）训练提升模型对比特征识别能力，从而实现精准概念擦除。

Result: 在艺术风格擦除、显式内容擦除和对象移除3个任务上，VCE表现出色，实现了精准的概念安全擦除，并保持对安全内容的完整性，达到了当前最佳表现。

Conclusion: VCE为自回归图像生成模型的安全防护提供了有效手段，在不牺牲模型生成质量和安全内容的前提下，实现了高效的敏感或侵权概念擦除，对实际应用具有重要意义。

Abstract: Recently, autoregressive image generation models have wowed audiences with
their remarkable capability in creating surprisingly realistic images. Models
such as GPT-4o and LlamaGen can not only produce images that faithfully mimic
renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also
potentially generate Not-Safe-For-Work (NSFW) content, raising significant
concerns regarding copyright infringement and ethical use. Despite these
concerns, methods to safeguard autoregressive text-to-image models remain
underexplored. Previous concept erasure methods, primarily designed for
diffusion models that operate in denoising latent space, are not directly
applicable to autoregressive models that generate images token by token. To
address this critical gap, we propose Visual Contrast Exploitation (VCE), a
novel framework comprising: (1) an innovative contrastive image pair
construction paradigm that precisely decouples unsafe concepts from their
associated content semantics, and (2) a sophisticated DPO-based training
approach that enhances the model's ability to identify and leverage visual
contrastive features from image pairs, enabling precise concept erasure. Our
comprehensive experiments across three challenging tasks-artist style erasure,
explicit content erasure, and object removal-demonstrate that our method
effectively secures the model, achieving state-of-the-art results while erasing
unsafe concepts and maintaining the integrity of unrelated safe concepts. The
code and models are available at https://github.com/Maplebb/VCE.

</details>


### [95] [A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection](https://arxiv.org/abs/2509.16988)
*Mingshuai Sheng,Bhatti Uzair Aslam,Junfeng Zhang,Siling Feng,Yonis Gulzar*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跨层多特征融合网络（CHMFFN），用于高光谱图像变化检测，基于多尺度编码-解码结构，结合多种注意力和自适应融合模块，在四个数据集上取得超过现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱变化检测方法在多尺度特征利用和差分特征融合效率方面存在不足，影响检测精度和效率。为提升多尺度特征表达和高效差分特征融合，需设计更有效的网络结构。

Method: 作者设计了基于编码-解码框架的CHMFFN网络，包含多尺度特征提取子网（用不同感受野的卷积与残差模块），引入DCCSA注意力模块提取光谱-空间-时间特征；STCFL模块进行跨时相差异特征学习；AFAF模块实现层次差分特征的自适应动态融合。

Result: 在四个公开高光谱数据集上，CHMFFN方法的变化检测性能全面优于现有主流方法，实验验证了其有效性和优越性。

Conclusion: CHMFFN通过多尺度特征提取、注意力和自适应融合模块高效提升了高光谱变化检测能力，是变化检测领域有效且先进的方法。

Abstract: Hyperspectral change detection (HCD) aims to accurately identify land-cover
changes in hyperspectral images of the same area acquired at different times,
with key applications in environmental monitoring and disaster assessment. To
address limitations of existing methods, such as insufficient use of multiscale
features and low efficiency in differential feature fusion, this paper proposes
a cross-hierarchical multi-feature fusion network (CHMFFN) based on a
multiscale encoder-decoder architecture. The front-end adopts a multiscale
feature extraction subnetwork, built on an encoder-decoder backbone with
residual connections and a dual-core channel-spatial attention (DCCSA) module
to extract spectral-spatial-temporal features (SSTF). The encoder captures
multiscale features from shallow details to deep semantics via residual blocks
and convolutional kernels with varying receptive fields. The decoder restores
spatial resolution and suppresses noise information through skip connections
integrating encoder features. Additionally, a spectral-temporal change feature
learning (STCFL) module learns cross-temporal change features at different
levels, strengthening inter-temporal difference capture. An adaptive fusion of
advanced features (AFAF) module dynamically balances hierarchical differential
features via adaptive weights, enhancing representation of complex changes.
Experiments on four public hyperspectral datasets show CHMFFN outperforms
state-of-the-art methods, verifying its effectiveness.

</details>


### [96] [DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment](https://arxiv.org/abs/2509.17012)
*Zhichao Ma,Fan Huang,Lu Zhao,Fengjun Guo,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了一个包含5000张带主观评分的文档图像质量评测数据集（DIQA-5000），并基于该数据集开发了一种新的无需参考的文档图像质量评估模型，在多个维度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文档图像在OCR、文档修复等应用中普遍存在，但现有的图像质量评测方法多针对自然图像，缺少针对文档图像的专业数据集与评测方法。因此，亟需建立高质量的数据集和专用的评测模型。

Method: 收集并增强500个真实文档图像，生成5000张具有丰富失真的文档图像，并邀请15位受试者从整体质量、锐度、色彩保真度三个维度进行主观评分。模型方面，结合文档布局特征、设计多级特征融合模块并为各评分维度设置独立评分头，实现多维度质量分数预测。

Result: 在DIQA-5000和OCR准确率相关的文档图像数据集上，所提出模型在各维度上均显著优于现有主流无需参考的图像质量评测方法。

Conclusion: DIQA-5000为文档图像领域的质量评测提供了标准数据集，提出的新型评测模型在多维度质量预测上效果显著，促进了文档图像相关应用的技术发展。

Abstract: Document image quality assessment (DIQA) is an important component for
various applications, including optical character recognition (OCR), document
restoration, and the evaluation of document image processing systems. In this
paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset
comprises 5,000 document images, generated by applying multiple document
enhancement techniques to 500 real-world images with diverse distortions. Each
enhanced image was rated by 15 subjects across three rating dimensions: overall
quality, sharpness, and color fidelity. Furthermore, we propose a specialized
no-reference DIQA model that exploits document layout features to maintain
quality perception at reduced resolutions to lower computational cost.
Recognizing that image quality is influenced by both low-level and high-level
visual features, we designed a feature fusion module to extract and integrate
multi-level features from document images. To generate multi-dimensional
scores, our model employs independent quality heads for each dimension to
predict score distributions, allowing it to learn distinct aspects of document
image quality. Experimental results demonstrate that our method outperforms
current state-of-the-art general-purpose IQA models on both DIQA-5000 and an
additional document image dataset focused on OCR accuracy.

</details>


### [97] [When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration](https://arxiv.org/abs/2509.17024)
*Wenxuan Fang,Jili Fan,Chao Wang,Xiantao Hu,Jiangwei Weng,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种新框架LCDiff，通过分离亮度和色度信息，并结合扩散模型，实现了对恶劣天气下图像更优质的还原，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气下图像因不可预测和动态的降质，传统方法泛化性差，新兴的提示学习法又依赖视觉语言模型导致结果不一致，因此需要更鲁棒且无需额外提示的方法提升图像恢复效果。

Method: 提出LCDiff框架，包括两个主要部分：1）Lumina-Chroma Decomposition Network（LCDN）在YCbCr色彩空间分开处理与退化相关的亮度与与退化无关的色度信息；2）Lumina-Guided Diffusion Model（LGDM）用退化相关的亮度信息作为扩散模型引导，省去人工降质提示，并引入动态时间步损失函数优化去噪，有效兼顾图像低频和高频特征的恢复。同时，作者还构建了DriveWeather全天气驾驶数据集用于评测。

Result: 大量实验表明，该方法在图像还原任务上优于目前所有主流方法，建立了AWIR领域新标杆。

Conclusion: LCDiff通过亮度-色度分解与扩散模型引导，提升了恶劣天气图像恢复效果，为AWIR提供了新思路。同时，公开的全新数据集促进了后续研究。

Abstract: Adverse Weather Image Restoration (AWIR) is a highly challenging task due to
the unpredictable and dynamic nature of weather-related degradations.
Traditional task-specific methods often fail to generalize to unseen or complex
degradation types, while recent prompt-learning approaches depend heavily on
the degradation estimation capabilities of vision-language models, resulting in
inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel
framework comprising two key components: \textit{Lumina-Chroma Decomposition
Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN
processes degraded images in the YCbCr color space, separately handling
degradation-related luminance and degradation-invariant chrominance components.
This decomposition effectively mitigates weather-induced degradation while
preserving color fidelity. To further enhance restoration quality, LGDM
leverages degradation-related luminance information as a guiding condition,
eliminating the need for explicit degradation prompts. Additionally, LGDM
incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising
network, ensuring a balanced recovery of both low- and high-frequency features
in the image. Finally, we present DriveWeather, a comprehensive all-weather
driving dataset designed to enable robust evaluation. Extensive experiments
demonstrate that our approach surpasses state-of-the-art methods, setting a new
benchmark in AWIR. The dataset and code are available at:
https://github.com/fiwy0527/LCDiff.

</details>


### [98] [Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views](https://arxiv.org/abs/2509.17027)
*Zhenya Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯喷溅（Gaussian Splatting）的框架，可以直接从内窥镜数据高效构建交互式外科手术仿真场景。该方法能实现高质量、真实感渲染，并可在稀疏视角下进行实时、物理合理的手术场景变形仿真。


<details>
  <summary>Details</summary>
Motivation: 传统手术仿真环境的搭建方式繁琐、耗时且难以扩展，且常常导致细节欠缺和仿真不真实。希望通过数据驱动的方法提高效率、细节与仿真质量，并能有效处理内窥镜数据视角受限的难题。

Method: 提出了基于Gaussian Splatting的场景重建框架，并引入基于虚拟相机的正则化方法，通过自适应采样虚拟视角，缓解因视角单一导致的过拟合和几何失真。同时，结合基于稀疏控制节点的物质点法，高效实现场景中的物理变形仿真。

Result: 该方法可在几分钟之内从稀疏内窥镜视角重建手术场景，并实现用户自定义交互下的实时、物理合理变形，显著优于传统或现有方法。

Conclusion: 提出的方法在渲染质量、重建效率和真实性方面均表现突出，能够极大提升医疗仿真训练的体验和效率，在实际代表性手术数据上验证了其实用性和有效性。

Abstract: Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.

</details>


### [99] [From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning](https://arxiv.org/abs/2509.17040)
*Hang Du,Jiayang Zhang,Guoshun Nan,Wendi Deng,Zhenyan Chen,Chenyang Zhang,Wang Xiao,Shan Huang,Yuqi Pan,Tao Qi,Sicong Leng*

Main category: cs.CV

TL;DR: 本文提出了一个新的基准MIR，用于测试多模态大模型在多图片交错推理场景下的能力，并提出了分阶段课程学习策略显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多图片基准忽略了图片与文本的交错关系，缺乏对图片与对应文本深层次关联和场景理解的考察，因此需要一个新的基准和推理方法以提升大模型的复杂跨模态理解能力。

Method: 1. 构建一个包含多个图像与交错文本的新基准任务MIR，需要模型联合推理图片与其相关文本，建立图片区域与文字的准确对应。
2. 为每个实例设定分步骤推理要求，并提出“由易到难”的分阶段课程式学习方法，从简单到复杂逐渐训练模型。

Result: 大量实验证明，所提分阶段课程学习大幅提升了多模态大模型在MIR以及其它相关基准上的推理表现。

Conclusion: MIR基准及方法推动了多图片交错推理相关研究，有望促进多模态大模型在复杂跨模态任务中的能力提升。

Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language
Models (MLLMs) ability to jointly comprehend and reason across multiple images
and their associated textual contexts, introducing unique challenges beyond
single-image or non-interleaved multi-image tasks. While current multi-image
benchmarks overlook interleaved textual contexts and neglect distinct
relationships between individual images and their associated texts, enabling
models to reason over multi-image interleaved data may significantly enhance
their comprehension of complex scenes and better capture cross-modal
correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring
joint reasoning over multiple images accompanied by interleaved textual
contexts to accurately associate image regions with corresponding texts and
logically connect information across images. To enhance MLLMs ability to
comprehend multi-image interleaved data, we introduce reasoning steps for each
instance within the benchmark and propose a stage-wise curriculum learning
strategy. This strategy follows an "easy to hard" approach, progressively
guiding models from simple to complex scenarios, thereby enhancing their
ability to handle challenging tasks. Extensive experiments benchmarking
multiple MLLMs demonstrate that our method significantly enhances models
reasoning performance on MIR and other established benchmarks. We believe that
MIR will encourage further research into multi-image interleaved reasoning,
facilitating advancements in MLLMs capability to handle complex inter-modal
tasks.Our code and dataset are available at
https://github.com/Shelly-coder239/MIRBench.

</details>


### [100] [Towards Generalized Synapse Detection Across Invertebrate Species](https://arxiv.org/abs/2509.17041)
*Samia Mohinta,Daniel Franco-Barranco,Shi Yan Lee,Albert Cardona*

Main category: cs.CV

TL;DR: 本论文提出了一种高效、简单但性能强大的突触检测方法SimpSyn，并在多个体积电镜（EM）数据集上验证，表现优于现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 理解神经回路结构对个体行为差异至关重要，而突触是神经回路结构的核心。目前体积电镜提供了足够的分辨率，但自动化突触检测因数据标注稀疏、形态多样和跨数据集特征差异而面临挑战，亟需高效、通用的自动检测方法。

Method: 1）整理了涵盖果蝇和微黄蜂两个无脊椎物种、四个公开数据集的多样化EM基准；2）提出SimpSyn，一种单阶段残差U-Net模型，对突触前后位置进行球形掩码预测，优化速度与标注效率；3）与当前多任务主流模型Synful进行系统对比，并通过消融实验探讨后处理策略。

Result: SimpSyn在所有数据集上的F1分数均超越了Synful，且通过简单后处理手段（如局部峰值检测、距离筛选）可获得高性能。尽管跨数据集泛化仍有挑战，混合训练时SimpSyn表现依然较好。

Conclusion: 面向大规模神经联系组学的突触检测任务，结构简洁、贴合实际任务需求的轻量模型具备极佳的实用性与可扩展性，是复杂方法的有效替代方案。

Abstract: Behavioural differences across organisms, whether healthy or pathological,
are closely tied to the structure of their neural circuits. Yet, the fine-scale
synaptic changes that give rise to these variations remain poorly understood,
in part due to persistent challenges in detecting synapses reliably and at
scale. Volume electron microscopy (EM) offers the resolution required to
capture synaptic architecture, but automated detection remains difficult due to
sparse annotations, morphological variability, and cross-dataset domain shifts.
To address this, we make three key contributions. First, we curate a diverse EM
benchmark spanning four datasets across two invertebrate species: adult and
larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,
we propose SimpSyn, a single-stage Residual U-Net trained to predict
dual-channel spherical masks around pre- and post-synaptic sites, designed to
prioritize training and inference speeds and annotation efficiency over
architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s
Synful [1], a state-of-the-art multi-task model that jointly infers synaptic
pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in
F1-score across all volumes for synaptic site detection. While generalization
across datasets remains limited, SimpSyn achieves competitive performance when
trained on the combined cohort. Finally, ablations reveal that simple
post-processing strategies - such as local peak detection and distance-based
filtering - yield strong performance without complex test-time heuristics.
Taken together, our results suggest that lightweight models, when aligned with
task structure, offer a practical and scalable solution for synapse detection
in large-scale connectomic pipelines.

</details>


### [101] [AgriDoctor: A Multimodal Intelligent Assistant for Agriculture](https://arxiv.org/abs/2509.17044)
*Mingqing Zhang,Zhuoning Xu,Peijie Wang,Rongji Li,Liang Wang,Qiang Liu,Jian Xu,Xuyao Zhang,Shu Wu,Liang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AgriDoctor的多模态诊断系统，有效提升了作物病害诊断的准确性，并基于新构建的大型农作物病害数据集表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有作物病害诊断方法多依赖于单一图像模型，难以结合农业领域的知识，且缺乏支持语言交互与理解的能力。大型语言模型和视觉-语言模型虽潜力巨大，但因缺乏领域数据和适应性，其在农业场景下表现有限。

Method: 该工作开发了AgriDoctor，一个模块化、可扩展的多模态框架，集成路由器、分类器、检测器、知识检索器和大语言模型，首次将多智能体多模态推理引入农业领域。同时，构建了包含40万标注图片、831条知识、30万多语种工具选择指令的数据集AgriMM用于训练与评测。

Result: 在AgriMM数据集上训练后，AgriDoctor在作物病害精细诊断等任务上，显著优于现有最先进的大型视觉-语言模型。

Conclusion: AgriDoctor系统为智能化、可持续农业提供了新的范式，有效提升了作物健康诊断的交互性和专业性。

Abstract: Accurate crop disease diagnosis is essential for sustainable agriculture and
global food security. Existing methods, which primarily rely on unimodal models
such as image-based classifiers and object detectors, are limited in their
ability to incorporate domain-specific agricultural knowledge and lack support
for interactive, language-based understanding. Recent advances in large
language models (LLMs) and large vision-language models (LVLMs) have opened new
avenues for multimodal reasoning. However, their performance in agricultural
contexts remains limited due to the absence of specialized datasets and
insufficient domain adaptation. In this work, we propose AgriDoctor, a modular
and extensible multimodal framework designed for intelligent crop disease
diagnosis and agricultural knowledge interaction. As a pioneering effort to
introduce agent-based multimodal reasoning into the agricultural domain,
AgriDoctor offers a novel paradigm for building interactive and domain-adaptive
crop health solutions. It integrates five core components: a router,
classifier, detector, knowledge retriever and LLMs. To facilitate effective
training and evaluation, we construct AgriMM, a comprehensive benchmark
comprising 400000 annotated disease images, 831 expert-curated knowledge
entries, and 300000 bilingual prompts for intent-driven tool selection.
Extensive experiments demonstrate that AgriDoctor, trained on AgriMM,
significantly outperforms state-of-the-art LVLMs on fine-grained agricultural
tasks, establishing a new paradigm for intelligent and sustainable farming
applications.

</details>


### [102] [Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization](https://arxiv.org/abs/2509.17049)
*Peng Wang,Yong Li,Lin Zhao,Xiu-Shen Wei*

Main category: cs.CV

TL;DR: 提出了一种基于可学习查询的新颖精细化哈希方法，实现了更高效且具可解释性的图像检索，尤其是在低比特哈希和细粒度类别中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有精细化哈希方法在需要区分外观极为相似类别时，对图像细粒度属性的挖掘能力有限，且传统哈希位通常缺乏具体语义和可解释性，导致检索性能受限。因此，亟需一种方法，使哈希位能与特定视觉属性相关联，从而提升细粒度检索效果和哈希代码的鲁棒性。

Method: 提出了一种基于可学习查询的属性感知哈希编码方法：通过一组定制查询，在哈希过程中捕获并表示精细的属性级信息，提升各哈希位的解释性和相关性。进一步设计了辅助分支，用于建模高阶属性交互，缓解低位哈希下优化困难的问题，增强哈希码的鲁棒性和区分性。

Result: 在多个标准数据集上实验表明，该方法能生成具有属性感知的哈希码，在检索准确性和鲁棒性上均优于最新方法，特别在低比特哈希条件下优势更加明显。

Conclusion: 该方法有效提升了精细化图像检索的性能，实现了哈希位与视觉属性的关联，增加了可解释性，并在实际数据集上显著优于传统方法，有望成为细粒度哈希任务的新工具。

Abstract: Fine-grained hashing has become a powerful solution for rapid and efficient
image retrieval, particularly in scenarios requiring high discrimination
between visually similar categories. To enable each hash bit to correspond to
specific visual attributes, we propoe a novel method that harnesses learnable
queries for attribute-aware hash codes learning. This method deploys a tailored
set of queries to capture and represent nuanced attribute-level information
within the hashing process, thereby enhancing both the interpretability and
relevance of each hash bit. Building on this query-based optimization
framework, we incorporate an auxiliary branch to help alleviate the challenges
of complex landscape optimization often encountered with low-bit hash codes.
This auxiliary branch models high-order attribute interactions, reinforcing the
robustness and specificity of the generated hash codes. Experimental results on
benchmark datasets demonstrate that our method generates attribute-aware hash
codes and consistently outperforms state-of-the-art techniques in retrieval
accuracy and robustness, especially for low-bit hash codes, underscoring its
potential in fine-grained image hashing tasks.

</details>


### [103] [Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition](https://arxiv.org/abs/2509.17050)
*Junhao Jia,Yunyou Liu,Yifei Sun,Huangwei Chen,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 本文提出了GeoProto框架，通过在深度特征空间中引入内在几何结构，提升原型网络在细粒度识别任务中的性能，远超传统欧氏原型网络。


<details>
  <summary>Details</summary>
Motivation: 深度视觉特征通常分布在非线性流形上，欧氏距离难以真实反映相似性。这对于需要解释性和细微语义区分的原型网络细粒度识别尤其严重，需要更适合的相似性度量方法。

Method: 提出将每个类别深度特征的流形结构蒸馏至扩散空间，并使用可微的Nyström插值，使新样本和可学习原型都能访问几何结构。通过设置紧凑的类内锚点并定期更新，使嵌入与骨干网络同步，保证推理效率和可扩展性。

Result: 在CUB-200-2011和Stanford Cars数据集上，GeoProto框架生成的原型更聚焦于语义对齐的细节部分，整体性能明显优于传统欧氏原型网络。

Conclusion: 利用几何结构提升了原型网络细粒度识别的效果，证明了GeoProto在解释性和性能上的优势，适合未来可解释视觉任务推广。

Abstract: Nonlinear manifolds are widespread in deep visual features, where Euclidean
distances often fail to capture true similarity. This limitation becomes
particularly severe in prototype-based interpretable fine-grained recognition,
where subtle semantic distinctions are essential. To address this challenge, we
propose a novel paradigm for prototype-based recognition that anchors
similarity within the intrinsic geometry of deep features. Specifically, we
distill the latent manifold structure of each class into a diffusion space and
introduce a differentiable Nystr\"om interpolation, making the geometry
accessible to both unseen samples and learnable prototypes. To ensure
efficiency, we employ compact per-class landmark sets with periodic updates.
This design keeps the embedding aligned with the evolving backbone, enabling
fast and scalable inference. Extensive experiments on the CUB-200-2011 and
Stanford Cars datasets show that our GeoProto framework produces prototypes
focusing on semantically aligned parts, significantly outperforming Euclidean
prototype networks.

</details>


### [104] [CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner](https://arxiv.org/abs/2509.17065)
*Yao Du,Jiarong Guo,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本研究提出了CardiacCLIP，一种基于视频的心脏超声分析框架，显著提升了左心室射血分数（LVEF）的预测准确性，特别在小样本（few-shot）设定下效果优异。


<details>
  <summary>Details</summary>
Motivation: 传统LVEF预测方法依赖大规模标注数据，成本高且难以适应不同临床环境；而现有的视觉-语言模型（如EchoCLIP）未能有效捕获心脏的时序动态和局部结构信息，从而影响诊断准确性。

Method: 提出CardiacCLIP框架，包括多帧学习（MFL）注意力机制用于选择性融合信息帧，以及EchoZoom多尺度特征提取策略，用于精细化捕捉心脏结构，整体上以CLIP为基础适配视频小样本心脏超声分析。

Result: 在EchoNet-Dynamic数据集1-shot（小样本）设定下，CardiacCLIP将平均绝对误差（MAE）降低了2.07，验证了该方法在诊断准确性上的显著提升。

Conclusion: CardiacCLIP通过创新的视频特征融合和多尺度表示，提升了小样本场景下心脏超声的诊断准确性，为各类临床环境下的LVEF自动估计提供了高效、通用的解决方案。

Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment,
with left ventricular ejection fraction (LVEF) serving as a key indicator of
heart function. Existing LVEF estimation methods depend on large-scale
annotated video datasets, which are costly and limit adaptability across
various clinical settings. Recent vision-language models for echocardiography,
such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial
temporal dynamics and localized cardiac structures essential for accurate
diagnosis. To address these challenges, we propose CardiacCLIP, a video-based
framework that enhances LVEF prediction through attention-based frame
aggregation and multi-resolution input scaling. Specifically, we introduce MFL
(Multi Frame Learning), a novel attention-based mechanism for selectively
fusing informative frames, and EchoZoom, a multi-scale feature extraction
strategy that refines spatial representations of cardiac structures. As a novel
adaptation of CLIP models for few-shot echocardiogram video analysis, our
approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on
the EchoNet-Dynamic dataset under 1-shot setting. The code is available at
https://github.com/xmed-lab/CardiacCLIP.

</details>


### [105] [Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models](https://arxiv.org/abs/2509.17074)
*Qian Zhang,Lin Zhang,Xing Fang,Mingxin Zhang,Zhiyuan Wei,Ran Song,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于机器人视觉可供性（affordance）学习的新框架，通过信息约束来强化视觉与文本特征对齐，实现更优的基于文本引导的可供性区域识别，并在AGD20K数据集上一举提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有采用视觉-语言预训练模型的可供性学习方法忽视了视觉图像与语言描述在特征层面对齐的重要性，导致文本引导的可供性区域识别结果不理想。为此，作者尝试引入新的约束机制以促进特征对齐，从而提升可供性学习的效果。

Method: 提出了一个包含互信息约束的框架：1）可供性互信息约束，通过最大化可供性区域特征与对应文本提示的互信息，同时优化文本提示和视觉特征的学习；2）对象级信息约束，通过最大化对象视觉特征与其类别文本特征的互信息，获得更高质量的对象表示，从而增强对可供性区域的语义判断。

Result: 在AGD20K数据集上的实验表明，所提方法在单样本可供性学习任务中超越了现有方法，取得了新的最优效果。

Conclusion: 通过在视觉-语言特征对齐中引入信息驱动约束，有效提升了文本引导的视觉可供性学习表现。该方法为机器人理解与交互物理世界提供了新的思路和更可靠的技术基础。

Abstract: Visual affordance learning is crucial for robots to understand and interact
effectively with the physical world. Recent advances in this field attempt to
leverage pre-trained knowledge of vision-language foundation models to learn
affordance properties with limited training data, providing a novel paradigm
for visual affordance learning. However, these methods overlook the
significance of maintaining feature alignment between visual images and
language descriptions for identifying affordance areas with textual guidance,
and thus may lead to suboptimal results. In this paper, we present an
informative framework for text-guided affordance learning, which involves
information-based constraints to achieve text-image alignment at feature level.
Specifically, we design an affordance mutual information constraint that helps
learn appropriate textual prompts and task-oriented visual features
simultaneously by maximizing the mutual information between the features of the
affordance areas in the input images and the corresponding textual prompts. In
addition, we propose an object-level information constraint that maximizes the
mutual information between the visual features of a given object and the text
features of the category it belongs to. This enables the model to capture
high-quality representations for the object, providing more reliable semantic
priors for identifying affordance regions. Experimental results on the AGD20K
dataset show that the proposed method outperforms existing approaches and
achieves the new state-of-the-art in one-shot affordance learning.

</details>


### [106] [Enhanced Detection of Tiny Objects in Aerial Images](https://arxiv.org/abs/2509.17078)
*Kihyun Kim,Michalis Lazarou,Tania Stathaki*

Main category: cs.CV

TL;DR: 本论文针对YOLOv8等单阶段检测器在航拍图像微小目标检测上表现不足的问题，提出了图像分辨率调整、数据增强和注意力机制三大改进策略，并设计了基于注意力增强CNN的MoonNet框架，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 单阶段目标检测器如YOLOv8虽训练速度快，但在处理低分辨率、背景复杂的航拍微小目标检测任务时，经常检测性能不足，亟需提升微小目标检测精度。

Method: 提出三种简单可集成到YOLOv8的提升策略：输入图像分辨率提升、数据增强和引入注意力机制。具体地，在YOLOv8主干网络中引入了SE Block和CBAM两种注意力模块，并提升通道数量，形成新型MoonNet骨干结构。此外，调整输入图像大小及恰当的数据增强方式。

Result: MoonNet主干网络相较原YOLOv8在微小目标检测准确率上取得提升。在集成到YOLC模型后，在微小目标检测基准上实现了当前最优性能。

Conclusion: 本文证明通过输入尺寸调整、数据增强与注意力机制增强，单阶段检测器能大幅提升微小目标检测表现；集成注意力模块的MoonNet具有良好泛化能力和提升空间。

Abstract: While one-stage detectors like YOLOv8 offer fast training speed, they often
under-perform on detecting small objects as a trade-off. This becomes even more
critical when detecting tiny objects in aerial imagery due to low-resolution
targets and cluttered backgrounds. To address this, we introduce three
enhancement strategies -- input image resolution adjustment, data augmentation,
and attention mechanisms -- that can be easily implemented on YOLOv8. We
demonstrate that image size enlargement and the proper use of augmentation can
lead to enhancement. Additionally, we designed a Mixture of Orthogonal
Neural-modules Network (MoonNet) pipeline which consists of attention-augmented
CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE
Block) and the Convolutional Block Attention Module (CBAM), were integrated
into the backbone of YOLOv8 with an increased number of channels, and the
MoonNet backbone obtained improved detection accuracy compared to the original
YOLOv8. MoonNet further proved its adaptability and potential by achieving
state-of-the-art performance on a tiny-object benchmark when integrated with
the YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet

</details>


### [107] [A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion](https://arxiv.org/abs/2509.17079)
*Yuhong Feng,Hongtao Chen,Qi Zhang,Jie Chen,Zhaoxi He,Mingzhe Liu,Jianghai Liao*

Main category: cs.CV

TL;DR: 本文提出了一种用于RGB-热成像下人群计数的新方法，在现有方法的基础上提升了极端条件下的检测精度。该方法通过创新的空间调制机制与自适应融合策略，实现了更好的目标定位与多模态数据整合。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的人群计数方法虽然能够捕捉场景的全局信息，但由于缺乏空间归纳偏置，容易将注意力扩散到无关的背景区域，导致定位精度下降。此外，有效整合RGB与热成像这两种跨模态信息依然是一大挑战。解决这些难题对于提升复杂场景下人群计数的鲁棒性与精度具有重要意义。

Method: 作者提出了双调制框架（Dual Modulation Framework），包括两个新模块：1）空间调制注意力（Spatially Modulated Attention, SMA），通过可学习的空间衰减掩码，降低远距离令牌之间的注意力分配，抑制注意力外扩到背景区域；2）自适应融合调制（Adaptive Fusion Modulation, AFM），利用动态门控机制，在跨模态融合时动态选择最可靠的信息通道，实现更优的数据互补。

Result: 在多个RGB-热成像人群计数数据集上进行了广泛的实验验证，结果显示所提方法在人群定位和总数估算上明显优于以往的同类方法，取得了更高的精度和鲁棒性。

Conclusion: 本文方法通过引入空间调制和自适应融合机制，显著提升了跨模态人群计数的性能和实用性，对实际公共安全和复杂环境具有较好应用前景。

Abstract: Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in
challenging conditions. While recent Transformer-based methods excel at
capturing global context, their inherent lack of spatial inductive bias causes
attention to spread to irrelevant background regions, compromising crowd
localization precision. Furthermore, effectively bridging the gap between these
distinct modalities remains a major hurdle. To tackle this, we propose the Dual
Modulation Framework, comprising two modules: Spatially Modulated Attention
(SMA), which improves crowd localization by using a learnable Spatial Decay
Mask to penalize attention between distant tokens and prevent focus from
spreading to the background; and Adaptive Fusion Modulation (AFM), which
implements a dynamic gating mechanism to prioritize the most reliable modality
for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting
datasets demonstrate the superior performance of our method compared to
previous works. Code available at
https://github.com/Cht2924/RGBT-Crowd-Counting.

</details>


### [108] [HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis](https://arxiv.org/abs/2509.17083)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出Hybrid Radiance Fields (HyRF)，结合了显式3D高斯和神经场的优点，实现了高质量、实时的新视图合成，同时显著减少了内存开销。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）虽可替换NeRF、实现高质量和实时渲染，但需要为每个高斯分布单独存储参数，造成较高的内存消耗，同时现有压缩方法难以保留高频空间细节。为此，作者亟需找到兼顾细节表现与内存高效的新表示方法。

Method: HyRF将场景分解为两部分：（1）一组仅存储高频关键参数的紧凑显式高斯；（2）基于网格的神经场预测其余属性。同时，引入解耦神经网络模型，分别建模几何（如尺度、不透明度、旋转）和视角相关颜色，并提出混合渲染方案，把高斯投影与神经场预测的背景组合，改进远景表现。

Result: 实验表明，HyRF在保持实时性能的前提下，模型体积比原3DGS减少超过20倍，同时达到当前最优的渲染质量。

Conclusion: HyRF有效兼顾了显式与隐式场景表示的优点，大幅提升渲染效率并保持高保真度，适合高效新视图合成任务。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.

</details>


### [109] [MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors](https://arxiv.org/abs/2509.17084)
*Binhua Huang,Nan Wang,Arjun Parakash,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了MoCLIP-Lite，一种结合CLIP图像编码器与运动矢量特征、通过两个分支后期融合的高效视频动作识别框架。该方法效率高、效果好，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法通常计算量大且依赖大量视频预训练，而大规模视觉-语言模型（如CLIP）在静态图像零样本任务表现优异，同时运动矢量可高效提供视频时序信息，因此作者希望将两者优势结合，提升视频识别效率和准确性。

Method: 提出了MoCLIP-Lite框架：使用冻结的CLIP图像编码器提取静态特征，搭配在原始运动矢量上训练的轻量有监督网络提取动态特征；融合阶段，两分支均被冻结，仅训练一个极小的MLP头实现高效后期特征融合。

Result: 在UCF101数据集上，MoCLIP-Lite取得了89.2%的Top-1准确率，远超零样本基线（65.0%）和仅靠运动矢量的基线（66.5%）。

Conclusion: MoCLIP-Lite为高效视频理解提供了新基线，将大型静态模型的表现与低成本运动信息有效结合，有望拓展实际应用场景。

Abstract: Video action recognition is a fundamental task in computer vision, but
state-of-the-art models are often computationally expensive and rely on
extensive video pre-training. In parallel, large-scale vision-language models
like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot
capabilities on static images, while motion vectors (MV) provide highly
efficient temporal information directly from compressed video streams. To
synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple
yet powerful two-stream late fusion framework for efficient video recognition.
Our approach combines features from a frozen CLIP image encoder with features
from a lightweight, supervised network trained on raw MV. During fusion, both
backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is
trained, ensuring extreme efficiency. Through comprehensive experiments on the
UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,
significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)
baselines. Our work provides a new, highly efficient baseline for video
understanding that effectively bridges the gap between large static models and
dynamic, low-cost motion cues. Our code and models are available at
https://github.com/microa/MoCLIP-Lite.

</details>


### [110] [SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks](https://arxiv.org/abs/2509.17086)
*Jie Chen,Yuhong Feng,Tao Dai,Mingzhe Liu,Hongtao Chen,Zhaoxi He,Jiancong Bai*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFN-YOLO的家禽检测方法，通过尺度感知融合技术提升在复杂环境下的检测表现，并发布了一个新数据集M-SCOPE，实验显示模型精度高且参数量小，适合智慧养殖应用。


<details>
  <summary>Details</summary>
Motivation: 尽管检测方法取得进展，但在自由放养环境下，由于目标尺度多样、遮挡和背景复杂，现有方法效果不佳。亟需针对复杂环境、能兼顾检测精度与效率的新方法。

Method: 提出SFN-YOLO模型，采用尺度感知融合策略，将本地细节和全局上下文信息结合，提升对不同尺度和复杂背景家禽的检测。同时，构建丰富且贴合实际的M-SCOPE数据集用于模型训练和验证。

Result: SFN-YOLO模型在M-SCOPE数据集上取得了80.7%的mAP，参数量仅为7.2M，比主流方法减少35.1%，并在跨域测试中表现出良好的泛化能力。

Conclusion: SFN-YOLO在保证高精度的同时大幅降低模型复杂度，具备实时性和高效性，能有效推动智慧养殖自动化应用。

Abstract: Detecting and localizing poultry is essential for advancing smart poultry
farming. Despite the progress of detection-centric methods, challenges persist
in free-range settings due to multiscale targets, obstructions, and complex or
dynamic backgrounds. To tackle these challenges, we introduce an innovative
poultry detection approach named SFN-YOLO that utilizes scale-aware fusion.
This approach combines detailed local features with broader global context to
improve detection in intricate environments. Furthermore, we have developed a
new expansive dataset (M-SCOPE) tailored for varied free-range conditions.
Comprehensive experiments demonstrate our model achieves an mAP of 80.7% with
just 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining
strong generalization capability across different domains. The efficient and
real-time detection capabilities of SFN-YOLO support automated smart poultry
farming. The code and dataset can be accessed at
https://github.com/chenjessiee/SFN-YOLO.

</details>


### [111] [AlignedGen: Aligning Style Across Generated Images](https://arxiv.org/abs/2509.17088)
*Jiexuan Zhang,Yiheng Du,Qian Wang,Weiqi Li,Yu Gu,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了AlignedGen方法，有效提升了Diffusion Transformer（DiT）生成模型在多图像风格一致性问题上的表现。通过改进注意力共享机制与位置编码，极大增强了不同图片间的风格统一性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成能力强，但在相同风格条件下生成的多张图片缺乏风格一致性，对创意类工作流程造成困扰。现有的一些方法简单地采用U-Net架构，效果差且与先进的DiT模型不兼容，亟需新方法在提升风格一致性的同时保持高图像质量和更强的兼容性。

Method: 作者提出了AlignedGen无训练框架，核心为两个创新点：一是Shifted Position Embedding (ShiftPE)，即为每张图片分配非重叠位置编码，有效解决注意力共享时的位置信号冲突；二是Advanced Attention Sharing (AAS)，三个精心设计的注意力共享提升技术。此外，作者还设计了一种高效的特征提取算法，支持用外部图片作为风格参考。

Result: 丰富的实验表明，AlignedGen能大幅提高DiT模型生成图片间的风格一致性，并保持文本条件与图像内容的精确对应，同时兼容外部风格输入。

Conclusion: AlignedGen突破了以往扩散模型风格一致性差、架构受限的问题，为DiT等先进扩散模型的实际应用提供了有力工具，为定制和创意图像生成流程带来了显著提升。

Abstract: Despite their generative power, diffusion models struggle to maintain style
consistency across images conditioned on the same style prompt, hindering their
practical deployment in creative workflows. While several training-free methods
attempt to solve this, they are constrained to the U-Net architecture, which
not only leads to low-quality results and artifacts like object repetition but
also renders them incompatible with superior Diffusion Transformer (DiT). To
address these issues, we introduce AlignedGen, a novel training-free framework
that enhances style consistency across images generated by DiT models. Our work
first reveals a critical insight: naive attention sharing fails in DiT due to
conflicting positional signals from improper position embeddings. We introduce
Shifted Position Embedding (ShiftPE), an effective solution that resolves this
conflict by allocating a non-overlapping set of positional indices to each
image. Building on this foundation, we develop Advanced Attention Sharing
(AAS), a suite of three techniques meticulously designed to fully unleash the
potential of attention sharing within the DiT. Furthermore, to broaden the
applicability of our method, we present an efficient query, key, and value
feature extraction algorithm, enabling our method to seamlessly incorporate
external images as style references. Extensive experimental results validate
that our method effectively enhances style consistency across generated images
while maintaining precise text-to-image alignment.

</details>


### [112] [Uncertainty-Supervised Interpretable and Robust Evidential Segmentation](https://arxiv.org/abs/2509.17098)
*Yuzhu Li,An Sui,Fuping Wu,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种自监督的不确定性估计方法，通过引入关于边界和噪声区域图像梯度与不确定性关系的三项原则，设计了两种新的不确定性监督损失，大幅提升了医学图像分割中预测不确定性的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割中的深度学习不确定性估计方法缺乏有效监督，导致模型预测不确定性的可解释性差和鲁棒性不足。如何让模型的不确定性输出更符合人类直观解释，成为亟需解决的问题。

Method: 作者提出结合自监督学习，引入三条图像梯度与不确定性关系原则，并据此设计两种不确定性监督损失函数。此外，还提出了新的定量指标来评估不确定性的可解释性与鲁棒性。

Result: 实验结果表明，该方法在常规和分布外（OOD）场景下，医学图像分割性能优良，相比现有方法，模型的不确定性输出在解释性及鲁棒性方面表现更佳。

Conclusion: 该自监督不确定性估计方法显著提高了医学图像分割模型中不确定性的可解释性和鲁棒性，对实际医学应用更具参考价值。

Abstract: Uncertainty estimation has been widely studied in medical image segmentation
as a tool to provide reliability, particularly in deep learning approaches.
However, previous methods generally lack effective supervision in uncertainty
estimation, leading to low interpretability and robustness of the predictions.
In this work, we propose a self-supervised approach to guide the learning of
uncertainty. Specifically, we introduce three principles about the
relationships between the uncertainty and the image gradients around boundaries
and noise. Based on these principles, two uncertainty supervision losses are
designed. These losses enhance the alignment between model predictions and
human interpretation. Accordingly, we introduce novel quantitative metrics for
evaluating the interpretability and robustness of uncertainty. Experimental
results demonstrate that compared to state-of-the-art approaches, the proposed
method can achieve competitive segmentation performance and superior results in
out-of-distribution (OOD) scenarios while significantly improving the
interpretability and robustness of uncertainty estimation. Code is available
via https://github.com/suiannaius/SURE.

</details>


### [113] [The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment](https://arxiv.org/abs/2509.17100)
*Deepak Alapatt,Jennifer Eckhoff,Zhiliang Lyu,Yutong Ban,Jean-Paul Mazellier,Sarah Choksi,Kunyi Yang,2024 CVS Challenge Consortium,Quanzheng Li,Filippo Filicori,Xiang Li,Pietro Mascagni,Daniel A. Hashimoto,Guy Rosman,Ozanan Meireles,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本文介绍了SAGES Critical View of Safety（CVS）Challenge，这是外科协会首次举办的AI竞赛，致力于腹腔镜胆囊切除手术中CVS步骤的自动评估，推动了手术质量评估领域AI技术的进步。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术中的CVS步骤虽然重要，却存在执行不一致的问题，而AI能够标准化评估和培训过程，实现更广泛的质量提升。

Method: 全球54家机构、24个国家的数百名临床和工程专家联合收集和注释了1000段手术视频，采用了经过共识验证的标准协议，并开发了EndoGlacier系统来管理大规模、多标注者的视频数据。13支国际团队参与竞赛，针对AI模型的性能、不确定性捕捉与临床稳健性进行评估。

Result: 顶尖团队在评估性能上实现了17%的相对提升，校准误差降低超过80%，稳健性提升17%。结果分析还揭示了影响模型性能的方法学趋势。

Conclusion: 本研究推进了AI在手术质量评估领域的现实应用，并为今后开发更加稳健和可临床部署的AI系统提供了有益指导。

Abstract: Advances in artificial intelligence (AI) for surgical quality assessment
promise to democratize access to expertise, with applications in training,
guidance, and accreditation. This study presents the SAGES Critical View of
Safety (CVS) Challenge, the first AI competition organized by a surgical
society, using the CVS in laparoscopic cholecystectomy, a universally
recommended yet inconsistently performed safety step, as an exemplar of
surgical quality assessment. A global collaboration across 54 institutions in
24 countries engaged hundreds of clinicians and engineers to curate 1,000
videos annotated by 20 surgical experts according to a consensus-validated
protocol. The challenge addressed key barriers to real-world deployment in
surgery, including achieving high performance, capturing uncertainty in
subjective assessment, and ensuring robustness to clinical variability. To
enable this scale of effort, we developed EndoGlacier, a framework for managing
large, heterogeneous surgical video and multi-annotator workflows. Thirteen
international teams participated, achieving up to a 17\% relative gain in
assessment performance, over 80\% reduction in calibration error, and a 17\%
relative improvement in robustness over the state-of-the-art. Analysis of
results highlighted methodological trends linked to model performance,
providing guidance for future research toward robust, clinically deployable AI
for surgical quality assessment.

</details>


### [114] [Stencil: Subject-Driven Generation with Context Guidance](https://arxiv.org/abs/2509.17120)
*Gordon Chen,Ziqi Huang,Cheston Tan,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Stencil的新框架，通过联合使用两个扩散模型，在高效保持主体一致性的同时提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文生图扩散模型在生成过程中难以保持主体一致性，且在模型微调时存在效率与质量之间的权衡。此外，直接对大模型微调代价高昂，而只微调轻量模型又损失了图像保真度，微调还可能破坏模型固有的先验知识。

Method: Stencil框架在推理时联合使用两个扩散模型：一个在指定主体图像上高效微调的轻量级模型，以及一个冻结的大型预训练模型。轻量模型保证效率，而大模型在推理时注入丰富的先验知识，以提升生成图像的保真度。

Result: Stencil能够在不到一分钟内，高效地生成高保真、创新的主体图像，取得了当前同类方法中的最佳性能。

Conclusion: Stencil框架成功解决了主体一致性、效率与质量的平衡问题，提升了主体驱动生成任务的表现，树立了新的业界标杆。

Abstract: Recent text-to-image diffusion models can generate striking visuals from text
prompts, but they often fail to maintain subject consistency across generations
and contexts. One major limitation of current fine-tuning approaches is the
inherent trade-off between quality and efficiency. Fine-tuning large models
improves fidelity but is computationally expensive, while fine-tuning
lightweight models improves efficiency but compromises image fidelity.
Moreover, fine-tuning pre-trained models on a small set of images of the
subject can damage the existing priors, resulting in suboptimal results. To
this end, we present Stencil, a novel framework that jointly employs two
diffusion models during inference. Stencil efficiently fine-tunes a lightweight
model on images of the subject, while a large frozen pre-trained model provides
contextual guidance during inference, injecting rich priors to enhance
generation with minimal overhead. Stencil excels at generating high-fidelity,
novel renditions of the subject in less than a minute, delivering
state-of-the-art performance and setting a new benchmark in subject-driven
generation.

</details>


### [115] [SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM](https://arxiv.org/abs/2509.17136)
*Yuhao Tian,Zheming Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的场景感知增强边缘-云协同工业视觉检测框架（SAEC），有效结合多模态大模型与轻量化策略，兼顾高精度与低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 工业视觉检测需要兼具高精度和低资源消耗，但目前多模态大模型推理强却计算消耗大，轻量模型难以处理复杂案例。

Method: 提出SAEC框架，包含高效MLLM微调、轻量级多尺度场景复杂度估计和自适应边缘-云调度三大组件，有效匹配场景复杂度与推理策略，动态分配边缘和云端计算资源。

Result: 在MVTec AD和KSDD2数据集上，SAEC分别达到85.11%和82.72%准确率，分别超越Qwen 22.1%/20.8%，LLaVA 33.3%/31.6%。运行时间缩短22.4%，且能耗降低40%-74%。

Conclusion: SAEC框架在保证高精度的同时大幅减少了计算资源和能耗，为工业视觉检测的实际部署提供了更优解。

Abstract: Industrial vision inspection requires high accuracy under stringent resource
constraints, yet existing approaches face a fundamental trade-off. Multimodal
LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive
computational costs, while lightweight edge models often fail on complex cases.
In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative
industrial vision inspection framework with MLLM. The framework is composed of
three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect
Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)
Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect
detection by tailoring multimodal reasoning to scene complexity and dynamically
balancing computation between edge and cloud resources. Experimental results on
MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%
accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It
also reduces runtime by up to 22.4% and cuts energy per correct decision by
40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.

</details>


### [116] [SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction](https://arxiv.org/abs/2509.17172)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出一种新的人脸美学自动评估方法MD-Net，将生成式与时序建模高效结合，显著提升了评测效果，在SCUT-FBP5500数据集上达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN或ViT的人脸美预测方法存在局限：CNN善于提取局部特征，难以高效建模全局依赖；ViT虽能处理全局关系，但计算开销大。需要结合二者优点，高效且准确地评估人脸的美感。

Method: 提出了MD-Net双流架构：一条流用冻住的预训练扩散模型U-Net编码器，获取细粒度美学信息；另一条利用Vision Mamba（Vim）高效捕获全局结构，两者通过交叉注意力机制融合，形成更全面的人脸表征用于美学预测。

Result: 在SCUT-FBP5500数据集上，MD-Net取得了0.9235的Pearson相关性，优于现有技术，显示了混合架构的强大潜力。

Conclusion: 通过融合生成式先验和高效全局建模，MD-Net为复杂视觉评价任务（如美感预测）提供了更有效的解决方案，对后续相关研究具有推动意义。

Abstract: The automated prediction of facial beauty is a benchmark task in affective
computing that requires a sophisticated understanding of both local aesthetic
details (e.g., skin texture) and global facial harmony (e.g., symmetry,
proportions). Existing models, based on either Convolutional Neural Networks
(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases
that limit their performance; CNNs excel at local feature extraction but
struggle with long-range dependencies, while ViTs model global relationships at
a significant computational cost. This paper introduces the
\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture
that resolves this trade-off by delegating specialized roles to
state-of-the-art models. The first stream leverages a frozen U-Net encoder from
a pre-trained latent diffusion model, providing a powerful generative prior for
fine-grained aesthetic qualities. The second stream employs a Vision Mamba
(Vim), a modern state-space model, to efficiently capture global facial
structure with linear-time complexity. By synergistically integrating these
complementary representations through a cross-attention mechanism, MD-Net
creates a holistic and nuanced feature space for prediction. Evaluated on the
SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson
Correlation of \textbf{0.9235} and demonstrating the significant potential of
hybrid architectures that fuse generative and sequential modeling paradigms for
complex visual assessment tasks.

</details>


### [117] [Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge](https://arxiv.org/abs/2509.17187)
*Lalith Bharadwaj Baru,Kamalaker Dadi,Tapabrata Chakraborti,Raju S. Bapi*

Main category: cs.CV

TL;DR: 本文提出了SSB方法，首次将薛定谔桥理论应用于医学图像分割，通过建模图像与分割掩膜的动态关系，有效应对边界不清和分割多样性问题，并在多个数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割经常遇到病灶边界不清晰和标注结果差异大的问题，传统算法难以同时保证结构完整性与多样性。本文为了解决这一痛点，提出新方法提升分割准确性和鲁棒性。

Method: 提出Segmentation Schrödinger Bridge (SSB)方法，建模图像-掩膜联合动态，结合创新损失函数以增强结构性和多样性。同时，提出Diversity Divergence Index (DDI)用于量化分割多样性和标注者间一致性。

Result: SSB方法在LIDC-IDRI、COCA和RACER（自有）等数据集上实现了领先的分割性能，优于现有主流方法，并能够更好地还原不确定和多样的分割边界。

Conclusion: SSB为曖昧场景下的医学图像分割提供了新思路，不依赖额外指导，实现边界更清晰、结果多样且结构完整，并用DDI指标有效度量分割的多样性和一致性。

Abstract: Accurate segmentation of medical images is challenging due to unclear lesion
boundaries and mask variability. We introduce \emph{Segmentation Sch\"{o}dinger
Bridge (SSB)}, the first application of Sch\"{o}dinger Bridge for ambiguous
medical image segmentation, modelling joint image-mask dynamics to enhance
performance. SSB preserves structural integrity, delineates unclear boundaries
without additional guidance, and maintains diversity using a novel loss
function. We further propose the \emph{Diversity Divergence Index} ($D_{DDI}$)
to quantify inter-rater variability, capturing both diversity and consensus.
SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER
(in-house) datasets.

</details>


### [118] [Echo-Path: Pathology-Conditioned Echo Video Generation](https://arxiv.org/abs/2509.17190)
*Kabir Hamzah Muhammad,Marawan Elbatel,Yi Qin,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的生成框架Echo-Path，可根据特定心脏病理状态合成超声心动图视频，用以提高自动化诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要致死原因，而超声心动图对其诊断至关重要。但针对某些心脏病理的超声数据稀缺，阻碍了自动诊断模型的发展。为解决此问题，作者希望通过生成合成病理超声视频以补充数据集。

Method: 作者提出了Echo-Path方法，在先进的超声视频生成器基础上引入病理条件机制，使模型能学习并控制疾病特异的心脏结构和运动模式。方法专门针对心房间隔缺损（ASD）及肺动脉高压（PAH）两类病理进行视频生成和验证。

Result: 实验结果显示，合成视频在分布距离度量上具有很高的视觉保真度，且展示了合理的临床病理特征。此外，基于合成数据训练的分类器在真实数据上表现良好，并能提升ASD和PAH的后续诊断准确率7%和8%。

Conclusion: Echo-Path可生成高质量、特定病理的超声心动图视频，能够有效扩充心脏病数据集，提升自动诊断模型性能，对临床辅助决策和人工智能研究具有重要意义。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
globally, and echocardiography is critical for diagnosis of both common and
congenital cardiac conditions. However, echocardiographic data for certain
pathologies are scarce, hindering the development of robust automated diagnosis
models. In this work, we propose Echo-Path, a novel generative framework to
produce echocardiogram videos conditioned on specific cardiac pathologies.
Echo-Path can synthesize realistic ultrasound video sequences that exhibit
targeted abnormalities, focusing here on atrial septal defect (ASD) and
pulmonary arterial hypertension (PAH). Our approach introduces a
pathology-conditioning mechanism into a state-of-the-art echo video generator,
allowing the model to learn and control disease-specific structural and motion
patterns in the heart. Quantitative evaluation demonstrates that the synthetic
videos achieve low distribution distances, indicating high visual fidelity.
Clinically, the generated echoes exhibit plausible pathology markers.
Furthermore, classifiers trained on our synthetic data generalize well to real
data and, when used to augment real training sets, it improves downstream
diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset
are available here https://github.com/Marshall-mk/EchoPathv1

</details>


### [119] [VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery](https://arxiv.org/abs/2509.17191)
*Jinchao Ge,Tengfei Cheng,Biao Wu,Zeyu Zhang,Shiya Huang,Judith Bishop,Gillian Shepherd,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: 本文针对MLLMs（多模态大型语言模型）分析文化遗产器物领域的能力不足，提出了VaseVL系统与数据集，显著提升了对古希腊陶器的风格分类和历史归属推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前通用MLLMs在文化遗产领域（如古希腊陶器分析）缺乏专家知识，且微调过拟合易导致推理脆弱，难满足实用需求。因此，亟需提升模型在该领域的稳健推理和专家级表现。

Method: 提出SFT（监督微调）后接RL（强化学习）的VaseVL方法：1）构建细致的问题类型分类；2）基于分类针对性诊断SFT模型性能弱点；3）采用针对性奖励优化方法，有针对性地提升模型在弱项上的表现。同时，发布了VaseVQA大型图像问答数据集。

Result: 实验证明VaseVL在陶器风格分类和历史归属推理任务上取得了SOTA（最新）结果。尤其在组合推理鲁棒性方面相较单独SFT有明显提升，体现了奖惩设计和任务拆解的有效性。

Conclusion: 诊断引导和类型条件的奖励工程能显著提升MLLMs在文化遗产领域的推理能力，VaseVQA数据集也为后续相关研究提供了高质量可复用资源。

Abstract: Analyzing cultural-heritage artifacts remains challenging for MLLMs: general
models lack domain expertise, and SFT often overfits superficial patterns,
yielding brittle reasoning for authentication and historical attribution. This
raises the question of how to equip MLLMs with robust, expert-level reasoning
for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns
evaluation into supervision: we construct a taxonomy of question types, probe
the SFT model to localize type-specific performance gaps, and optimize with
type-conditioned, compositionality-oriented rewards targeting those gaps. We
also release VaseVQA, a comprehensive benchmark of 31,773 images designed to
probe deep understanding. Experiments show state-of-the-art results on style
classification and historical attribution with marked gains in compositional
robustness over SFT-only baselines, validating diagnosis-guided,
taxonomy-conditioned reward engineering and providing a reusable resource for
future research. Code and dataset will be available at
https://github.com/AIGeeksGroup/VaseVQA.

</details>


### [120] [Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation](https://arxiv.org/abs/2509.17206)
*Gunner Stone,Sushmita Sarker,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 本文提出了一种将语义条件嵌入到点云生成过程中的扩散模型，实现了结构与语义的共同建模，使得生成的3D点云具备更好的结构一致性和语义分割能力。


<details>
  <summary>Details</summary>
Motivation: 3D点云生成广泛应用于遥感、机器人和数字对象建模，但现有方法大多只关注几何结构，将语义信息作为后处理步骤单独添加，导致生成结果在结构和语义上的耦合不足。亟需一种方法在生成过程中直接融合语义信息。

Method: 作者提出了一种基于扩散模型的生成框架，在生成的每个点上直接关联条件变量（即语义标签），该条件变量在扩散过程中引导点云生成，实现几何信息与语义的联合合成。通过有引导和无引导扩散的对比分析，研究了条件变量对生成质量的影响。

Result: 实验验证了该方法能生成结构连贯并具有明确语义分割能力的点云，生成结果在细节和准确性上优于传统方法，能针对具体物体部件和特征生成高质量点云。

Conclusion: 将语义条件纳入点云生成的扩散模型，有效提升了点云的结构与语义一致性，证明了该方法在结构与语义的联合建模上的优越性。

Abstract: Generating realistic 3D point clouds is a fundamental problem in computer
vision with applications in remote sensing, robotics, and digital object
modeling. Existing generative approaches primarily capture geometry, and when
semantics are considered, they are typically imposed post hoc through external
segmentation or clustering rather than integrated into the generative process
itself. We propose a diffusion-based framework that embeds per-point semantic
conditioning directly within generation. Each point is associated with a
conditional variable corresponding to its semantic label, which guides the
diffusion dynamics and enables the joint synthesis of geometry and semantics.
This design produces point clouds that are both structurally coherent and
segmentation-aware, with object parts explicitly represented during synthesis.
Through a comparative analysis of guided and unguided diffusion processes, we
demonstrate the significant impact of conditional variables on diffusion
dynamics and generation quality. Extensive experiments validate the efficacy of
our approach, producing detailed and accurate 3D point clouds tailored to
specific parts and features.

</details>


### [121] [Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds](https://arxiv.org/abs/2509.17207)
*Gunner Stone,Youngsook Choi,Alireza Tavakkoli,Ankita Shukla*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云Transformer预训练策略Point-RTD，通过去噪方式增强点云token的鲁棒性，在多个数据集上显著优于现有方法（如PointMAE）。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的点云预训练方法（如PointMAE）主要采用掩码重建方式，难以充分学习结构先验，提升模型的token鲁棒性和下游表现，亟需新型预训练策略。

Method: 提出Point-RTD方法，将点云token进行扰动（corruption），并利用生成器-判别器结构进行去噪重建，区别于传统mask-reconstruction任务，采用corruption-reconstruction框架提升token鲁棒性和结构感知。

Result: 在ShapeNet数据集上，Point-RTD的重建误差比PointMAE降低93%以上，测试集Chamfer Distance降低14倍以上，并且在ShapeNet、ModelNet10及ModelNet40分类基准上更快收敛、准确率更高，全面超越Point-MAE。

Conclusion: Point-RTD通过corruption-reconstruction预训练策略和判别器-生成器结构，极大提升了点云Transformer模型的表征能力和下游任务性能，具有明显优势。

Abstract: Pre-training strategies play a critical role in advancing the performance of
transformer-based models for 3D point cloud tasks. In this paper, we introduce
Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to
improve token robustness through a corruption-reconstruction framework. Unlike
traditional mask-based reconstruction tasks that hide data segments for later
prediction, Point-RTD corrupts point cloud tokens and leverages a
discriminator-generator architecture for denoising. This shift enables more
effective learning of structural priors and significantly enhances model
performance and efficiency. On the ShapeNet dataset, Point-RTD reduces
reconstruction error by over 93% compared to PointMAE, and achieves more than
14x lower Chamfer Distance on the test set. Our method also converges faster
and yields higher classification accuracy on ShapeNet, ModelNet10, and
ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework
in every case.

</details>


### [122] [MirrorSAM2: Segment Mirror in Videos with Depth Perception](https://arxiv.org/abs/2509.17220)
*Mingchen Xu,Yukun Lai,Ze Ji,Jing Wu*

Main category: cs.CV

TL;DR: MirrorSAM2是首个将Segment Anything Model 2（SAM2）用于RGB-D视频镜面分割的框架，通过结合RGB和深度信息，实现无提示下的自动镜面分割，并在多个基准上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 镜面检测面临反射模糊和纹理混淆等难题。传统的分割方法难以在视频中自动、精准地识别镜面目标。作者希望通过充分利用RGB与深度信息互补性，扩展SAM2，解决现有方法局限，提升镜面分割的自动化和准确性。

Method: 提出MirrorSAM2框架，包含四个关键模块：1）深度变换模块实现RGB与深度数据对齐，2）深度引导多尺度点提示生成器自动生成分割提示，3）频率细节注意力融合模块提升结构边界，4）带有可学习镜面标记的解码器优化镜面分割效果。整体系统能够实现无需人工提示的自动镜面分割。

Result: 在VMD与DVMD公开数据集上，MirrorSAM2在小尺寸镜面、弱边界和强反射等复杂场景下均取得了当前最优（SOTA）的分割表现，优于以往方法。

Conclusion: MirrorSAM2首次实现了SAM2在RGB-D视频镜面分割中的自动化应用，并有效解决了反射混淆等难题，显著提升了分割的准确性和鲁棒性。

Abstract: This paper presents MirrorSAM2, the first framework that adapts Segment
Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.
MirrorSAM2 addresses key challenges in mirror detection, such as reflection
ambiguity and texture confusion, by introducing four tailored modules: a Depth
Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point
Prompt Generator for automatic prompt generation, a Frequency Detail Attention
Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with
a learnable mirror token for refined segmentation. By fully leveraging the
complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities
to the prompt-free setting. To our knowledge, this is the first work to enable
SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD
benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under
challenging conditions such as small mirrors, weak boundaries, and strong
reflections.

</details>


### [123] [DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction](https://arxiv.org/abs/2509.17232)
*Bo Liu,Runlong Li,Li Zhou,Yan Zhou*

Main category: cs.CV

TL;DR: DT-NeRF通过融合扩散模型和Transformer，大幅提升了3D场景重建的细节还原和多视角一致性，在主流数据集上显著优于传统NeRF及当前领先方法。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF及类似方法在稀疏视角或复杂几何场景下面临细节恢复不足和多视角一致性差的问题。本研究为解决这些核心难点，提出引入扩散模型和Transformer，以提升3D重建效果。

Method: 提出DT-NeRF，将扩散模型与Transformer模块结合，用于稀疏视角下的高质量信息生成与全局特征建模。方法在Matterport3D和ShapeNet数据集上进行实验，并通过消融实验验证各模块作用。

Result: 在PSNR、SSIM、Chamfer Distance和Fidelity等主流衡量指标上，DT-NeRF明显优于传统NeRF和SOTA方法。消融实验显示扩散与Transformer模块对性能提升均至关重要。

Conclusion: DT-NeRF模块间协同带来高效、精准的3D场景重建能力。未来可通过进一步优化模型、探索更先进生成模型和新型网络结构，持续提升在大规模动态场景中的表现。

Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.

</details>


### [124] [SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2509.17246)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 本文提出SPFSplatV2，一种无需真实位姿监督即可完成高效3D Gaussian点云建模和新视图合成的前馈框架。该方法能在输入未配准的稀疏多视图图像下，端到端同时估计3D重建与相机位姿，并在无地面真值情况下达到当前最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前3D新视图合成方法普遍依赖于人工标注的相机位姿作为监督，而实际中大规模数据集缺乏精确标注，这极大限制了方法的应用范围和可扩展性。如何无需真实位姿信息即可实现高质量的3D重建，成为亟需解决的问题。

Method: SPFSplatV2通过一个共享特征提取骨干，将未配准输入同时映射为3D Gaussian primitive和相机位姿，采用masked attention机制有效估算目标姿态，并用重投影损失强化几何约束。该训练框架兼容不同3D重建架构，并推出两种变体模型。

Result: 在没有位姿监督的情况下，SPFSplatV2在相机姿态估计和3D新视图合成（无论同域、异域、极端视角变化、图像重叠少）的任务上，均取得了超越带有几何监督方法的表现，达到了当前最优结果。

Conclusion: SPFSplatV2消除了对真实相机位姿标注的依赖，极大提升了3D新视图合成技术的可扩展性，为大规模、泛化性强的3D建模提供了有效解决方案。

Abstract: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.

</details>


### [125] [Optimized Learned Image Compression for Facial Expression Recognition](https://arxiv.org/abs/2509.17262)
*Xiumei Li,Marc Windsheimer,Misha Sadeghi,Björn Eskofier,André Kaup*

Main category: cs.CV

TL;DR: 本文提出了一种端到端模型，在面部表情识别（FER）任务中提升了图像压缩率和识别准确率。该模型兼顾特征保存与高效压缩，通过定制损失函数实现压缩与识别的优化平衡，并实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的有损压缩会损失对FER任务关键的图像特征，导致识别准确率下降。现有方法难以兼顾压缩效率与识别性能，因此需要新的压缩框架提升这一平衡。

Method: 提出了一种端到端训练的压缩与识别联合优化模型，设计了自定义损失函数，对压缩损失与识别损失加权。实验探索了不同损失权重下的性能表现，并比较了仅调优压缩模型与联合优化的效果。

Result: 单独调优压缩模型使分类准确率提升0.71%，压缩效率提升49.32%；而压缩与识别的联合优化使准确率提升4.04%，效率提升89.12%。联合优化模型可在高压缩率下依然可靠保留图像细节，并适应于压缩和未压缩数据。

Conclusion: 联合优化压缩与分类模型显著提升了FER准确率和图像压缩效率，尤其在高压缩率下仍可保持细节和识别性能，优于单独优化。自定义损失权重对于性能平衡具有重要作用。

Abstract: Efficient data compression is crucial for the storage and transmission of
visual data. However, in facial expression recognition (FER) tasks, lossy
compression often leads to feature degradation and reduced accuracy. To address
these challenges, this study proposes an end-to-end model designed to preserve
critical features and enhance both compression and recognition performance. A
custom loss function is introduced to optimize the model, tailored to balance
compression and recognition performance effectively. This study also examines
the influence of varying loss term weights on this balance. Experimental
results indicate that fine-tuning the compression model alone improves
classification accuracy by 0.71% and compression efficiency by 49.32%, while
joint optimization achieves significant gains of 4.04% in accuracy and 89.12%
in efficiency. Moreover, the findings demonstrate that the jointly optimized
classification model maintains high accuracy on both compressed and
uncompressed data, while the compression model reliably preserves image
details, even at high compression rates.

</details>


### [126] [Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity](https://arxiv.org/abs/2509.17282)
*Xiangmin Xu,Zhen Meng,Kan Chen,Jiaming Yang,Emma Li,Philip G. Zhao,David Flynn*

Main category: cs.CV

TL;DR: 本文提出一种基于上下文的策略优化框架，在无线网络多机器人场景下，实现三维场景实时表达时，有效平衡数据新鲜度与表达质量。


<details>
  <summary>Details</summary>
Motivation: 当前实时三维场景表达在沉浸式应用（如数字制造、元宇宙、VR/AR/MR）中需求旺盛。但在保障通信实时性的同时，如何兼顾表达的高保真度，仍然是个技术难点。

Method: 作者采用无线网络多移动机器人采集环境图像，上传到边缘服务器进行三维重建。创新地引入同时考虑信息时效性（AoI）与语义信息的上下文bandit PPO算法，提出两个决策策略（ω-threshold与ω-wait），并与两种基线方案对比，评估其在标准数据集和三维重建模型上的表现。

Result: 实验表明提出的方法在兼顾低延迟的同时，提升了三维表示的保真度，并能较好地解释模型的决策行为。

Conclusion: 该方法为动态环境下三维场景的实时表达提供了新的技术思路，实现了时效性与保真度间有效权衡，有助于推动相关应用的发展。

Abstract: Real-time Three-dimensional (3D) scene representation is a foundational
element that supports a broad spectrum of cutting-edge applications, including
digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and
the emerging metaverse. Despite advancements in real-time communication and
computing, achieving a balance between timeliness and fidelity in 3D scene
representation remains a challenge. This work investigates a wireless network
where multiple homogeneous mobile robots, equipped with cameras, capture an
environment and transmit images to an edge server over channels for 3D
representation. We propose a contextual-bandit Proximal Policy Optimization
(PPO) framework incorporating both Age of Information (AoI) and semantic
information to optimize image selection for representation, balancing data
freshness and representation quality. Two policies -- the $\omega$-threshold
and $\omega$-wait policies -- together with two benchmark methods are
evaluated, timeliness embedding and weighted sum, on standard datasets and
baseline 3D scene representation models. Experimental results demonstrate
improved representation fidelity while maintaining low latency, offering
insight into the model's decision-making process. This work advances real-time
3D scene representation by optimizing the trade-off between timeliness and
fidelity in dynamic environments.

</details>


### [127] [Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models](https://arxiv.org/abs/2509.17283)
*Licheng Zhan,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种结合门检测与大语言模型（LLM）推理的新方法，实现对建筑设施类型及其空间分布的自动枚举，并首次引入Chain-of-Thought（CoT）提升LLM在建筑合规检查中的表现，结果显示在多数据集上具有良好的泛化性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 建筑合规检查（BCC）需要精确统计不同设施类型及其空间分布，人工操作费时耗力，且现有流程对此研究不足，影响了合规检查的自动化与效率。

Method: 作者提出一种创新方法，将视觉中的门检测与具备推理能力的大语言模型（LLM）结合，并利用Chain-of-Thought（CoT）机制提升推理准确性，从而实现设施类型数量的自动验证。

Result: 所提方法在真实与合成平面图数据集上进行了实验，结果表明其在不同数据集、不同设施类型上都表现出了良好的有效性和鲁棒性。

Conclusion: 结合门检测与LLM推理的自动设施枚举方法有效提升了建筑合规检查的自动化水平，首次应用CoT管道提升了LLM在该任务中的性能，为行业流程自动化带来新的解决思路。

Abstract: Building compliance checking (BCC) is a critical process for ensuring that
constructed facilities meet regulatory standards. A core component of BCC is
the accurate enumeration of facility types and their spatial distribution.
Despite its importance, this problem has been largely overlooked in the
literature, posing a significant challenge for BCC and leaving a critical gap
in existing workflows. Performing this task manually is time-consuming and
labor-intensive. Recent advances in large language models (LLMs) offer new
opportunities to enhance automation by combining visual recognition with
reasoning capabilities. In this paper, we introduce a new task for BCC:
automated facility enumeration, which involves validating the quantity of each
facility type against statutory requirements. To address it, we propose a novel
method that integrates door detection with LLM-based reasoning. We are the
first to apply LLMs to this task and further enhance their performance through
a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse
datasets and facility types. Experiments on both real-world and synthetic floor
plan data demonstrate the effectiveness and robustness of our method.

</details>


### [128] [UIPro: Unleashing Superior Interaction Capability For GUI Agents](https://arxiv.org/abs/2509.17328)
*Hongxin Li,Jingran Su,Jingfan Chen,Zheng Ju,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了UIPro，一个能够通用操作多平台、多任务GUI界面的全能型智能体，并在多个基准测试上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体受限于场景、规模和动作空间的异构性，阻碍了通用型智能体的发展。该研究旨在突破这些限制，推进更强大、更普适的GUI智能体实现。

Method: 作者首先构建了包含2060万个GUI理解任务的大规模多平台多任务数据集，用于预训练UIPro以增强其GUI理解能力。然后，通过建立统一的动作空间，将不同任务数据集融合，并继续微调，提升UIPro的动作预测能力。

Result: UIPro在多个平台的多类GUI任务基准测试中，表现出优于以往方法的性能，证明了其通用性和有效性。

Conclusion: UIPro通过大数据预训练和动作空间统一，显著提升了GUI智能体的通用性和任务表现，为通用型GUI操作智能体的发展提供了有效路径。

Abstract: Building autonomous agents that perceive and operate graphical user
interfaces (GUIs) like humans has long been a vision in the field of artificial
intelligence. Central to these agents is the capability for GUI interaction,
which involves GUI understanding and planning capabilities. Existing methods
have tried developing GUI agents based on the multi-modal comprehension ability
of vision-language models (VLMs). However, the limited scenario, insufficient
size, and heterogeneous action spaces hinder the progress of building
generalist GUI agents. To resolve these issues, this paper proposes
\textbf{UIPro}, a novel generalist GUI agent trained with extensive
multi-platform and multi-task GUI interaction data, coupled with a unified
action space. We first curate a comprehensive dataset encompassing 20.6 million
GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding
capability, which is key to downstream GUI agent tasks. Subsequently, we
establish a unified action space to harmonize heterogeneous GUI agent task
datasets and produce a merged dataset to foster the action prediction ability
of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's
superior performance across multiple GUI task benchmarks on various platforms,
highlighting the effectiveness of our approach.

</details>


### [129] [SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction](https://arxiv.org/abs/2509.17329)
*Neham Jain,Andrew Jong,Sebastian Scherer,Ioannis Gkioulekas*

Main category: cs.CV

TL;DR: 该论文提出了SmokeSeer方法，结合热成像与RGB图像，通过3D高斯分布融合，实现多视角视频下的三维场景重建和烟雾去除，对不同密度与动态烟雾均有效，并开源了相关数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的烟雾极大降低了图像质量，妨碍了可见性。目前方法易出现幻觉内容，或仅适用于静态、低密度烟雾。因此，急需一种能应对真实场景各种烟雾情况并高质量还原图像的方法。

Method: 提出SmokeSeer，利用热成像对烟雾散射影响较小的特点，结合多视角RGB与热成像图像，基于3D高斯分布法将两种模态信息融合，并显式分离场景中的烟雾与非烟雾部分，从而实现三维重建与烟雾去除。

Result: 在合成数据和自建的真实多视角RGB+热成像数据集上进行验证，SmokeSeer展示了对不同密度与动态变化烟雾场景的优越处理能力。

Conclusion: SmokeSeer显著提升了烟雾遮挡下的场景可视化能力，可处理不同密度和时变烟雾，实验效果优于已有方法。相关数据与代码均已开源，推动该领域进一步发展。

Abstract: Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.

</details>


### [130] [Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model](https://arxiv.org/abs/2509.17365)
*Amanuel Tafese Dufera*

Main category: cs.CV

TL;DR: 本文介绍了一种基于Transformer模型的自动图像描述系统，用于克服传统卷积神经网络（CNN）和长短期记忆网络（LSTM）在训练和推断效率上的不足。该方法将在EfficientNetB0基础上结合Transformer自注意力机制，并通过Flickr30k数据集进行实验。


<details>
  <summary>Details</summary>
Motivation: 当前主流的CNN和LSTM方法在图像描述任务上虽然取得了进展，但由于RNN的顺序性质，训练和推断速度较慢，LSTM在处理超长序列时记忆能力有限。因此，需要探索更高效、适合并行化的模型来提升图像描述系统的性能。

Method: 本文提出利用Transformer模型的自注意力机制，有效捕捉图像数据中的长短距离依赖，实现高效的并行训练与推断。具体方法包括以EfficientNetB0提取图像特征，搭建Transformer架构用于序列建模，在Flickr30k数据集上经过专业的数据预处理后进行训练。

Result: 通过在Flickr30k数据集上的实验，展示了该Transformer模型在图像描述任务中的高效训练、推断及描述生成能力。具体结果未详细说明，但强调了并行化优势。

Conclusion: 基于Transformer的图像描述系统能够克服传统RNN方法在训练和推断方面的局限，通过自注意力机制实现高效的并行化计算，具备良好的应用前景。

Abstract: Automatic image captioning, a multifaceted task bridging computer vision and
natural lan- guage processing, aims to generate descriptive textual content
from visual input. While Convolutional Neural Networks (CNNs) and Long
Short-Term Memory (LSTM) networks have achieved significant advancements, they
present limitations. The inherent sequential nature of RNNs leads to sluggish
training and inference times. LSTMs further struggle with retaining information
from earlier sequence elements when dealing with very long se- quences. This
project presents a comprehensive guide to constructing and comprehending
transformer models for image captioning. Transformers employ self-attention
mechanisms, capturing both short- and long-range dependencies within the data.
This facilitates efficient parallelization during both training and inference
phases. We leverage the well-established Transformer architecture, recognized
for its effectiveness in managing sequential data, and present a meticulous
methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,
construct a model architecture that integrates an EfficientNetB0 CNN for fea-
ture extraction, and train the model with attention mechanisms incorporated.
Our approach exemplifies the utilization of parallelization for efficient
training and inference. You can find the project on GitHub.

</details>


### [131] [Revisiting Vision Language Foundations for No-Reference Image Quality Assessment](https://arxiv.org/abs/2509.17374)
*Ankit Yadav,Ta Duc Huy,Lingqiao Liu*

Main category: cs.CV

TL;DR: 本文系统评估了六种主流预训练视觉骨干网络在无参考图像质量评价（NR-IQA）任务中的表现，并发现激活函数的选择和SigLIP2架构对性能有显著影响。作者提出了一种可学习的激活选择机制，打破人工设计限制，实现了多项基准数据集的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言预训练模型在无参考图像质量评价方面表现突出，但不同视觉主干网络架构的优劣势尚不明确。本研究旨在系统比较这些主干结构，并探索影响NR-IQA性能的关键因素。

Method: 评估了CLIP、SigLIP2、DINOv2、DINOv3、Perception以及ResNet六种预训练骨干网络在NR-IQA中的表现，每种骨干均结合一个轻量MLP头进行微调。实验重点分析了主干性能及激活函数（如Sigmoid、ReLU、GELU）的影响，并提出一种可学习的通道级激活函数选择机制。

Result: 实验发现SigLIP2主干表现最为稳定，并且激活函数的选择（尤其是Sigmoid）对模型泛化能力影响显著。提出的可学习激活机制在CLIVE、KADID10K和AGIQA3K等数据集上取得了新的SOTA表现。消融实验验证了这些发现在不同架构和设置下的普适性。

Conclusion: 激活函数以及主干网络的合理选择对NR-IQA的性能提升至关重要。本文提出的可学习激活机制能自动适应最佳非线性形式，无需人工调参，显著提升了NR-IQA模型的性能，建立了资源高效的强大基线。

Abstract: Large-scale vision language pre-training has recently shown promise for
no-reference image-quality assessment (NR-IQA), yet the relative merits of
modern Vision Transformer foundations remain poorly understood. In this work,
we present the first systematic evaluation of six prominent pretrained
backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task
of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an
identical lightweight MLP head. Our study uncovers two previously overlooked
factors: (1) SigLIP2 consistently achieves strong performance; and (2) the
choice of activation function plays a surprisingly crucial role, particularly
for enhancing the generalization ability of image quality assessment models.
Notably, we find that simple sigmoid activations outperform commonly used ReLU
and GELU on several benchmarks. Motivated by this finding, we introduce a
learnable activation selection mechanism that adaptively determines the
nonlinearity for each channel, eliminating the need for manual activation
design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and
AGIQA3K. Extensive ablations confirm the benefits across architectures and
regimes, establishing strong, resource-efficient NR-IQA baselines.

</details>


### [132] [Diff-GNSS: Diffusion-based Pseudorange Error Estimation](https://arxiv.org/abs/2509.17397)
*Jiaqi Zhu,Shouyi Lu,Ziyao Li,Guirong Zhuo,Lu Xiong*

Main category: cs.CV

TL;DR: 提出了一种名为Diff-GNSS的新方法，利用扩散模型精准估计并修正城市环境下GNSS伪距误差，显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS定位常因多路径和非视距信号导致测量误差增大，现有基于学习的方法难以处理复杂的误差分布，亟需更精细的误差建模方法来提升定位系统可靠性。

Method: 该方法采用两阶段结构：首先用基于Mamba的模块实现粗略误差估计，随后引入有条件的去噪扩散模型，对伪距误差进行细粒度建模与修正。通过引入三种GNSS测量质量相关的特征作为条件，引导扩散过程，提升误差建模的可控性和准确性，同时在扩散阶段还对每颗卫星的误差不确定性进行评估。

Result: 在公开和自建数据集上，Diff-GNSS在多项指标上均显著优于现有主流方法，并公布了涵盖多种场景的真实数据集。

Conclusion: Diff-GNSS是首次将扩散模型应用于GNSS伪距误差估计，所提出的可插拔扩散细化模块可集成到现有网络中，显著提升误差估计能力，为GNSS定位发展提供了新思路。

Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban
positioning. However, multipath and non-line-of-sight reception often introduce
large measurement errors that degrade accuracy. Learning-based methods for
predicting and compensating pseudorange errors have gained traction, but their
performance is limited by complex error distributions. To address this
challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement
(pseudorange) error estimation framework that leverages a conditional diffusion
model to capture such complex distributions. Firstly, a Mamba-based module
performs coarse estimation to provide an initial prediction with appropriate
scale and trend. Then, a conditional denoising diffusion layer refines the
estimate, enabling fine-grained modeling of pseudorange errors. To suppress
uncontrolled generative diversity and achieve controllable synthesis, three key
features related to GNSS measurement quality are used as conditions to
precisely guide the reverse denoising process. We further incorporate
per-satellite uncertainty modeling within the diffusion stage to assess the
reliability of the predicted errors. We have collected and publicly released a
real-world dataset covering various scenes. Experiments on public and
self-collected datasets show that DiffGNSS consistently outperforms
state-of-the-art baselines across multiple metrics. To the best of our
knowledge, this is the first application of diffusion models to pseudorange
error estimation. The proposed diffusion-based refinement module is
plug-and-play and can be readily integrated into existing networks to markedly
improve estimation accuracy.

</details>


### [133] [Interpreting vision transformers via residual replacement model](https://arxiv.org/abs/2509.17401)
*Jinyeong Kim,Junhyeok Kim,Yumin Shim,Joohyeok Kim,Sunyoung Jung,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文提出了通过分析ViT（视觉Transformer）中各层特征及其计算方法，系统揭示了ViT如何表征和处理信息，并给出了提升可解释性的具体方法。


<details>
  <summary>Details</summary>
Motivation: 长期以来，ViT内部机制较难被解释和理解，阻碍了模型的进一步优化和应用。本研究旨在填补这一空白，通过细致分析ViT特征和计算流程，提升模型可解释性。

Method: 作者提取了ViT所有层的6.6K特征（借由稀疏自编码器），并提出了残差替换模型（residual replacement model），用可解释特征代替残差流中的原始计算，从而生成更简单且便于人类理解的计算电路。

Result: 分析显示ViT的特征从低层的模式到高层语义逐步演化，同时模型还以特定方式编码了曲线和空间位置。残差替换方法成功简化了ViT的原始计算，实现了更高效的人类可解释性。此外，框架也展示了在去除带偏相关性任务中的价值。

Conclusion: 通过整套框架，能更直观理解ViT工作机制，对于可解释AI和减少模型偏见具有重要意义。

Abstract: How do vision transformers (ViTs) represent and process the world? This paper
addresses this long-standing question through the first systematic analysis of
6.6K features across all layers, extracted via sparse autoencoders, and by
introducing the residual replacement model, which replaces ViT computations
with interpretable features in the residual stream. Our analysis reveals not
only a feature evolution from low-level patterns to high-level semantics, but
also how ViTs encode curves and spatial positions through specialized feature
types. The residual replacement model scalably produces a faithful yet
parsimonious circuit for human-scale interpretability by significantly
simplifying the original computations. As a result, this framework enables
intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility
of our framework in debiasing spurious correlations.

</details>


### [134] [Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture](https://arxiv.org/abs/2509.17406)
*Jonathan Wuntu,Muhamad Dwisnanto Putro,Rendy Syahputra*

Main category: cs.CV

TL;DR: 本文提出采用最新的YOLOv10-nano模型，在印度尼西亚海洋生态系统进行实时海洋鱼类检测，取得高精度和高速度的优异表现，展现了模型在生态保护中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 印度尼西亚海域拥有极高的生物多样性，但传统鱼类检测方法费时且需专家参与，难以满足大规模、实时生态监测需求，亟需自动化、高效的技术手段助力海洋保护。

Method: 本研究使用YOLOv10-nano深度学习模型，利用Bunaken国家海洋公园的试验数据，以及DeepFish和OpenImages V7-Fish数据集，对模型的检测能力进行评估。YOLOv10-nano通过CSPNet骨干网络、PAN特征融合和金字塔空间注意块等技术，实现高效、精准的目标检测。

Result: YOLOv10-nano在DeepFish和OpenImages V7-Fish上分别取得了mAP50为0.966、mAP50:95为0.606的高检测精度，同时参数量低（2.7M）、计算量少（8.4 GFLOPs）、推理速度快（CPU上达29.29 FPS），能够满足实时应用需求。虽然单独用OpenImages V7-Fish精度较低，但与DeepFish结合提升了模型鲁棒性。

Conclusion: YOLOv10-nano模型能够为资源受限环境下的海洋鱼类监测和保护提供高效、可扩展的技术方案，具备实际部署价值，有望推动海洋生态保护工作的自动化与科学化。

Abstract: Indonesia's marine ecosystems, part of the globally recognized Coral
Triangle, are among the richest in biodiversity, requiring efficient monitoring
tools to support conservation. Traditional fish detection methods are
time-consuming and demand expert knowledge, prompting the need for automated
solutions. This study explores the implementation of YOLOv10-nano, a
state-of-the-art deep learning model, for real-time marine fish detection in
Indonesian waters, using test data from Bunaken National Marine Park. YOLOv10's
architecture, featuring improvements like the CSPNet backbone, PAN for feature
fusion, and Pyramid Spatial Attention Block, enables efficient and accurate
object detection even in complex environments. The model was evaluated on the
DeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano
achieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606
while maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It
also delivered an average inference speed of 29.29 FPS on the CPU, making it
suitable for real-time deployment. Although OpenImages V7-Fish alone provided
lower accuracy, it complemented DeepFish in enhancing model robustness.
Overall, this study demonstrates YOLOv10-nano's potential for efficient,
scalable marine fish monitoring and conservation applications in data-limited
environments.

</details>


### [135] [Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling](https://arxiv.org/abs/2509.17427)
*Hodaka Kawachi,Jose Reinaldo Cunha Santos A. V. Silva Neto,Yasushi Yagi,Hajime Nagahara,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种结合可微分前向模型与扩散先验的单帧深度估计方法，在编码孔径成像下无须手工先验和配对训练数据，获得了高精度、鲁棒性的RGBD重建，效果优于U-Net和传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于散焦的深度估计（DFD）方法常依赖手工设计的先验，限制了重建质量，并且U-Net类方法需要成对的训练数据和特定硬件配置，实用性受限。因此亟需一种无需配对数据、先验泛化性强的DFD方案。

Method: 作者提出了一种新颖的优化框架：用可微分前向模型保证重建与观测值一致，同时引入扩散模型学习得来的图像先验，在去噪图像域作为正则化项。该方法无需与RGBD双目数据配对训练，也不依赖特定的相机配置。

Result: 在大量仿真和原型机实验上，该方法在不同噪声水平下均能实现高质量且稳定的RGBD重建。相比U-Net基线法和经典DFD算法，本法均取得了更优表现。

Conclusion: 利用扩散先验正则化和可微分前向模型的一体化优化，摆脱了传统与深度网络在训练数据和硬件上的依赖，实现了鲁棒高效的编码孔径下单帧DFD重建。

Abstract: We propose a single-snapshot depth-from-defocus (DFD) reconstruction method
for coded-aperture imaging that replaces hand-crafted priors with a learned
diffusion prior used purely as regularization. Our optimization framework
enforces measurement consistency via a differentiable forward model while
guiding solutions with the diffusion prior in the denoised image domain,
yielding higher accuracy and stability than clas- sical optimization. Unlike
U-Net-style regressors, our approach requires no paired defocus-RGBD training
data and does not tie training to a specific camera configuration. Experiments
on comprehensive simulations and a prototype camera demonstrate consistently
strong RGBD reconstructions across noise levels, outperforming both U-Net
baselines and a classical coded- aperture DFD method.

</details>


### [136] [Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration](https://arxiv.org/abs/2509.17429)
*Zhitao Zeng,Guojian Yuan,Junyuan Mao,Yuxuan Wang,Xiaoshuang Jia,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了多尺度时序预测（MSTP）任务，旨在让视觉-语言模型能够在多个时间尺度和状态层级下准确预测场景的细粒度状态，并针对该任务发布了首个基准数据集，以及一种“增量生成与多智能体协作（IG-MC）”的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型难以同时处理多个时间尺度与细粒度状态的预测，特别是在对复杂的如外科手术等场景的精准理解和推理所需，因此需要建立统一框架与方法来提升多尺度时序预测能力。

Method: （1）提出MSTP任务，将多尺度划分为：时间尺度（不同预测提前时长）和状态尺度（不同层级的状态）；（2）建立覆盖多状态和多时间尺度注释的MSTP基准数据集；（3）提出IG-MC方法，包括可扩展的增量生成模块（持续生成未来预览，避免长时预测性能下降）和基于多智能体协作的决策框架（包括生成、发起和评估多个状态预测），以实现全局一致性和局部准确性平衡。

Result: IG-MC方法在MSTP基准数据集上展示出优于传统时序预测方法的表现，能够有效同步视觉生成与决策过程，并显著提升长时、细粒度状态的预测准确率。

Conclusion: 通过提出MSTP任务、基准数据集以及IG-MC创新方法，本文为多尺度时序预测和场景理解提供了新的研究方向和工具，对包括外科手术等复杂应用场景的智能决策具有广泛意义。

Abstract: Accurate temporal prediction is the bridge between comprehensive scene
understanding and embodied artificial intelligence. However, predicting
multiple fine-grained states of a scene at multiple temporal scales is
difficult for vision-language models. We formalize the Multi-Scale Temporal
Prediction (MSTP) task in general and surgical scenes by decomposing
multi-scale into two orthogonal dimensions: the temporal scale, forecasting
states of humans and surgery at varying look-ahead intervals, and the state
scale, modeling a hierarchy of states in general and surgical scenes. For
example, in general scenes, states of contact relationships are finer-grained
than states of spatial relationships. In surgical scenes, medium-level steps
are finer-grained than high-level phases yet remain constrained by their
encompassing phase. To support this unified task, we introduce the first MSTP
Benchmark, featuring synchronized annotations across multiple state scales and
temporal scales. We further propose a method, Incremental Generation and
Multi-agent Collaboration (IG-MC), which integrates two key innovations. First,
we present a plug-and-play incremental generation module that continuously
synthesizes up-to-date visual previews at expanding temporal scales to inform
multiple decision-making agents, keeping decisions and generated visuals
synchronized and preventing performance degradation as look-ahead intervals
lengthen. Second, we present a decision-driven multi-agent collaboration
framework for multi-state prediction, comprising generation, initiation, and
multi-state assessment agents that dynamically trigger and evaluate prediction
cycles to balance global coherence and local fidelity.

</details>


### [137] [Emergent 3D Correspondence from Neural Shape Representation](https://arxiv.org/abs/2509.17431)
*Keyu Du,Jingyu Hu,Haipeng Li,Hao Xu,Haibing Huang,Chi-Wing Fu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于分层神经语义表示的新方法，实现了准确且鲁棒的3D语义对应。其方法无需训练，兼容多种预训练3D生成模型，并在多个应用任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D语义对应任务受限于精度和泛化能力，尤其在形状多样性和无监督/训练自由情境下，现有方法难以取得理想效果。因此需要新的方法提升3D对象之间的语义对应稳定性和准确性。

Method: 提出分层神经语义表示（HNSR），融合全局语义特征和多分辨率的局部几何特征。利用预训练3D生成模型，设计逐步的全局到局部匹配策略，先用全局特征进行粗匹配，再通过局部特征细化映射。整个框架无需再训练，可灵活结合不同类型的3D生成骨干网络。

Result: 在形状共分割、关键点匹配、纹理转移等任务上泛化性好，即使跨类别也能取得优异结果。定性及定量结果显示，该方法明显优于以往SOTA方法。

Conclusion: 分层神经语义表征与全局到局部渐进式匹配思路，使得该方法具备高鲁棒性与高准确性。其训练自由和广泛兼容性使其具备现实落地和理论研究上的推广价值。

Abstract: This paper presents a new approach to estimate accurate and robust 3D
semantic correspondence with the hierarchical neural semantic representation.
Our work has three key contributions. First, we design the hierarchical neural
semantic representation (HNSR), which consists of a global semantic feature to
capture high-level structure and multi-resolution local geometric features to
preserve fine details, by carefully harnessing 3D priors from pre-trained 3D
generative models. Second, we design a progressive global-to-local matching
strategy, which establishes coarse semantic correspondence using the global
semantic feature, then iteratively refines it with local geometric features,
yielding accurate and semantically-consistent mappings. Third, our framework is
training-free and broadly compatible with various pre-trained 3D generative
backbones, demonstrating strong generalization across diverse shape categories.
Our method also supports various applications, such as shape co-segmentation,
keypoint matching, and texture transfer, and generalizes well to structurally
diverse shapes, with promising results even in cross-category scenarios. Both
qualitative and quantitative evaluations show that our method outperforms
previous state-of-the-art techniques.

</details>


### [138] [Training-Free Label Space Alignment for Universal Domain Adaptation](https://arxiv.org/abs/2509.17452)
*Dujin Lee,Sojung An,Jungmyung Wi,Kuniaki Saito,Donghyun Kim*

Main category: cs.CV

TL;DR: 提出了一种结合大规模视觉-语言模型（如CLIP）的无训练标签空间对齐方法，用于通用领域自适应（UniDA），能在标签空间差异和目标特有类别存在下持续提升表现。


<details>
  <summary>Details</summary>
Motivation: 传统的UniDA方法主要聚焦于视觉空间对齐，但易受视觉内容的歧义干扰，不够健壮和泛化。近年来VLMs（如CLIP）在零样本能力上表现卓越，但其如何适应未知标签空间仍存挑战。动机在于充分利用VLMs的强大表征能力，仅对齐标签空间来增强领域适应性。

Method: 首先利用生成式视觉-语言模型发现目标域未知类别。针对新发现标签存在噪声及语义歧义的问题，提出了一种无训练的标签空间对齐方法，通过过滤与精炼噪声标签对齐源域和目标域标签空间；然后构建融合了共享知识与目标私有类别信息的通用分类器，实现优质迁移。

Result: 该方法在DomainBed等关键基准上显著优于当前主流UniDA方法，H-score提升7.9%，H^3-score提升6.1%。引入自训练机制后，两项指标还能额外提升1.6%。

Conclusion: 通过标签空间对齐和通用分类器设计，有效应对标签空间歧义及目标私有类别的问题，显著提升了UniDA任务的泛化与适应能力，验证了CLIP等VLMs在领域自适应新场景中的巨大潜力。

Abstract: Universal domain adaptation (UniDA) transfers knowledge from a labeled source
domain to an unlabeled target domain, where label spaces may differ and the
target domain may contain private classes. Previous UniDA methods primarily
focused on visual space alignment but often struggled with visual ambiguities
due to content differences, which limited their robustness and
generalizability. To overcome this, we introduce a novel approach that
leverages the strong \textit{zero-shot capabilities} of recent vision-language
foundation models (VLMs) like CLIP, concentrating solely on label space
alignment to enhance adaptation stability. CLIP can generate task-specific
classifiers based only on label names. However, adapting CLIP to UniDA is
challenging because the label space is not fully known in advance. In this
study, we first utilize generative vision-language models to identify unknown
categories in the target domain. Noise and semantic ambiguities in the
discovered labels -- such as those similar to source labels (e.g., synonyms,
hypernyms, hyponyms) -- complicate label alignment. To address this, we propose
a training-free label-space alignment method for UniDA (\ours). Our method
aligns label spaces instead of visual spaces by filtering and refining noisy
labels between the domains. We then construct a \textit{universal classifier}
that integrates both shared knowledge and target-private class information,
thereby improving generalizability under domain shifts. The results reveal that
the proposed method considerably outperforms existing UniDA techniques across
key DomainBed benchmarks, delivering an average improvement of
\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.
Furthermore, incorporating self-training further enhances performance and
achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and
H$^3$-scores.

</details>


### [139] [Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks](https://arxiv.org/abs/2509.17457)
*Paweł Jakub Borsukiewicz,Jordan Samhi,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为LEAM的新方法，用于分析人脸识别模型在识别个体时主要关注的面部区域。该方法可以为后续个性化隐私保护措施提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别系统的普及，隐私风险增加，现有的对抗技术不能针对个体面部特征进行调整，导致效果有限。因此作者希望通过更细致的分析方法，提升隐私保护的有效性。

Method: 作者提出了Layer Embedding Activation Mapping（LEAM）技术，通过结合人脸分割工具，分析1000个人、9个预训练人脸识别模型的激活区域，识别每个人被模型用于识别的最关键面部区域。作者还用遮挡实验验证了LEAM筛选区域的合理性，并测试了这些区域跨模型的可迁移性。

Result: 实验发现，不同模型关注面部区域存在差异，但总体上主要关注面中部（如鼻子区域占18.9%-29.7%），且同一人的激活模式相似性远高于不同个体。LEAM选出的1%关键像素跨模型可迁移，并确认这些区域对识别效果影响显著。

Conclusion: LEAM揭示了人脸识别模型的个体识别模式，为未来基于个体特征实现更有效的隐私保护措施（如个性化干扰）奠定了基础。

Abstract: The proliferation of facial recognition systems presents major privacy risks,
driving the need for effective countermeasures. Current adversarial techniques
apply generalized methods rather than adapting to individual facial
characteristics, limiting their effectiveness and inconspicuousness. In this
work, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique
that identifies which facial areas contribute most to recognition at an
individual level. Unlike adversarial attack methods that aim to fool
recognition systems, LEAM is an explainability technique designed to understand
how these systems work, providing insights that could inform future privacy
protection research. We integrate LEAM with a face parser to analyze data from
1000 individuals across 9 pre-trained facial recognition models.
  Our analysis reveals that while different layers within facial recognition
models vary significantly in their focus areas, these models generally
prioritize similar facial regions across architectures when considering their
overall activation patterns, which show significantly higher similarity between
images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.
different individuals (0.04-0.13), validating the existence of person-specific
recognition patterns. Our results show that facial recognition models
prioritize the central region of face images (with nose areas accounting for
18.9-29.7% of critical recognition regions), while still distributing attention
across multiple facial fragments. Proper selection of relevant facial areas was
confirmed using validation occlusions, based on just 1% of the most relevant,
LEAM-identified, image pixels, which proved to be transferable across different
models. Our findings establish the foundation for future individually tailored
privacy protection systems centered around LEAM's choice of areas to be
perturbed.

</details>


### [140] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 提出了一种名为CARINOX的框架，通过结合噪声优化和探索，以及基于与人类判断相关的奖励选择策略，显著提升了文本到图像扩散模型在复杂组合描述下的生成结果与文本的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在遇到描述复杂对象关系、属性或空间布局的提示语时，经常无法生成与文本组合一致的高质量图片。现有推理时优化初始噪声的方法各有优劣，单独使用时效果有限，且奖励指标本身也难以全面捕捉组合性，导致生成指导不可靠。

Method: 提出CARINOX，一种结合噪声优化与探索的统一推理框架。该方法通过系统性选择与人类判断高度相关的奖励指标，优化与探索初始噪声空间，以提升模型对文本复杂组合描述的对齐度。

Result: 在T2I-CompBench++和HRS两个包含多样组合难题的基准上，CARINOX分别提高了16%和11%的平均对齐分数，并且在所有主要类别上均超越了现有的优化和探索类方法，图像质量和多样性得到保持。

Conclusion: CARINOX通过理论和实验证明了统一优化与探索及奖励选择策略能够更好地提升文本-图像组合一致性。该方法在不损失图像质量与多样性的前提下，为复杂文本提示的图像生成任务提供了更有效的解决方案。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [141] [CSDformer: A Conversion Method for Fully Spike-Driven Transformer](https://arxiv.org/abs/2509.17461)
*Yuhao Zhang,Chengjun Zhang,Di Wu,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出了一种新的方法CSDformer，可高效地将transformer模型转换为全脉冲神经网络（SNN），实现更优的性能和更低的训练/计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前基于spike的transformer模型要么训练开销高，要么转换时不易实现硬件友好操作，需要新的、高效的转换方法。

Method: 1）设计了面向转换的transformer架构，用新激活函数NReLU替换attention部分的softmax；2）对模型进行量化和训练；3）用时间分解技术实现模型向全spike驱动的SNN转换；4）提出延迟型整合-脉冲神经元减少误差。

Result: 在ImageNet、CIFAR-10和CIFAR-100上测试，ImageNet上在7步延时下top-1准确率达76.36%。训练计算资源减75%，训练加速2-3倍，并优于现有模型。

Conclusion: 首次实现了通过转换方法得到的全spike驱动transformer模型，在高性能、极低时延、低计算成本和低训练开销之间取得平衡，推动了可用的SNN模型发展。

Abstract: Spike-based transformer is a novel architecture aiming to enhance the
performance of spiking neural networks while mitigating the energy overhead
inherent to transformers. However, methods for generating these models suffer
from critical limitations: excessive training costs introduced by direct
training methods, or unavoidably hardware-unfriendly operations in existing
conversion methods. In this paper, we propose CSDformer, a novel conversion
method for fully spike-driven transformers. We tailor a conversion-oriented
transformer-based architecture and propose a new function NReLU to replace
softmax in self-attention. Subsequently, this model is quantized and trained,
and converted into a fully spike-driven model with temporal decomposition
technique. Also, we propose delayed Integrate-andFire neurons to reduce
conversion errors and improve the performance of spiking models. We evaluate
CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1
accuracy under 7 time-steps on ImageNet, demonstrating superiority over
state-of-the-art models. Furthermore, CSDformer eliminates the need for
training SNNs, thereby reducing training costs (reducing computational resource
by 75% and accelerating training speed by 2-3$\times$). To the best of our
knowledge, this is the first fully spike-driven transformer-based model
developed via conversion method, achieving high performance under ultra-low
latency, while dramatically reducing both computational complexity and training
overhead.

</details>


### [142] [MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception](https://arxiv.org/abs/2509.17462)
*Changwon Kang,Jisong Kim,Hongjae Shin,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出了MAESTRO框架，有效缓解多任务3D感知中的特征干扰，显著提升了3D目标检测、BEV地图分割和3D占用预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，多个任务的联合优化可能导致任务冲突，使模型性能下降。如何解决任务间特征干扰问题，是推动多任务3D感知发展的关键。

Method: MAESTRO框架包含三个模块：1) 类别原型生成器（CPG），对类别进行前景/背景分组，为不同任务分配原型特征；2) 任务特有特征生成器（TSFG），利用原型组聚焦于任务相关特征，抑制无关特征；3) 场景原型聚合器（SPA），借助检测和分割头的信息优化占用预测的原型组。

Result: 在nuScenes和Occ3D数据集上，MAESTRO在3D目标检测、BEV地图分割及3D占用预测各任务上均优于现有方法，展示了其有效性和优越性。

Conclusion: MAESTRO通过结构化地生成任务特有特征并减少特征干扰，为多任务3D感知提供了有效解决方案，能够提升多任务模型的整体性能。

Abstract: The goal of multi-task learning is to learn to conduct multiple tasks
simultaneously based on a shared data representation. While this approach can
improve learning efficiency, it may also cause performance degradation due to
task conflicts that arise when optimizing the model for different objectives.
To address this challenge, we introduce MAESTRO, a structured framework
designed to generate task-specific features and mitigate feature interference
in multi-task 3D perception, including 3D object detection, bird's-eye view
(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three
components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature
Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class
categories into foreground and background groups and generates group-wise
prototypes. The foreground and background prototypes are assigned to the 3D
object detection task and the map segmentation task, respectively, while both
are assigned to the 3D occupancy prediction task. TSFG leverages these
prototype groups to retain task-relevant features while suppressing irrelevant
features, thereby enhancing the performance for each task. SPA enhances the
prototype groups assigned for 3D occupancy prediction by utilizing the
information produced by the 3D object detection head and the map segmentation
head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate
that MAESTRO consistently outperforms existing methods across 3D object
detection, BEV map segmentation, and 3D occupancy prediction tasks.

</details>


### [143] [Stable Video-Driven Portraits](https://arxiv.org/abs/2509.17476)
*Mallikarjun B. R.,Fei Yin,Vikram Voleti,Nikita Drobyshev,Maksim Lapin,Aaryaman Vasishta,Varun Jampani*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型与面部关键区域控制的新型人像动画生成框架，提升了表情控制和时序一致性，能从单张图片生成效果逼真的高质量动画。


<details>
  <summary>Details</summary>
Motivation: 以往人像动画方法普遍存在表情表达受限、时序一致性差以及对新身份和大姿态变化泛化能力弱等问题。最新的扩散模型虽然提升了质量，但在控制能力和架构设计上存在不足。因此需要一种新方法解决上述局限。

Method: 作者提出以驱动视频中的面部关键区域（眼、鼻、口）作为强控制信号，结合跨身份监督防止外观泄漏，在架构设计上只引入极少新参数，通过空间-时间注意力机制和历史帧增强时序一致性，最后引入信号融合策略平衡动作还原与身份保持。

Result: 新方法在表情控制精度、时序一致性与生成质量方面，实现了优于现有方法的表现，并适用于实际场景的人像动画需求。

Conclusion: 该扩散模型框架以更强的控制能力和泛化能力，实现高质量、可控的人像动画，推动了该领域在实际应用中的落地。

Abstract: Portrait animation aims to generate photo-realistic videos from a single
source image by reenacting the expression and pose from a driving video. While
early methods relied on 3D morphable models or feature warping techniques, they
often suffered from limited expressivity, temporal inconsistency, and poor
generalization to unseen identities or large pose variations. Recent advances
using diffusion models have demonstrated improved quality but remain
constrained by weak control signals and architectural limitations. In this
work, we propose a novel diffusion based framework that leverages masked facial
regions specifically the eyes, nose, and mouth from the driving video as strong
motion control cues. To enable robust training without appearance leakage, we
adopt cross identity supervision. To leverage the strong prior from the
pretrained diffusion model, our novel architecture introduces minimal new
parameters that converge faster and help in better generalization. We introduce
spatial temporal attention mechanisms that allow inter frame and intra frame
interactions, effectively capturing subtle motions and reducing temporal
artifacts. Our model uses history frames to ensure continuity across segments.
At inference, we propose a novel signal fusion strategy that balances motion
fidelity with identity preservation. Our approach achieves superior temporal
consistency and accurate expression control, enabling high-quality,
controllable portrait animation suitable for real-world applications.

</details>


### [144] [ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](https://arxiv.org/abs/2509.17481)
*Xingqi Wang,Yiming Cui,Xin Yao,Shijin Wang,Guoping Hu,Xiaoyu Qin*

Main category: cs.CV

TL;DR: 本文提出了ChartHal基准，专注于大视觉-语言模型（LVLMs）在图表理解任务中的幻觉问题，评估发现主流模型幻觉严重。


<details>
  <summary>Details</summary>
Motivation: LVLMs在图表理解中频繁出现幻觉，但相关研究尚未系统分析幻觉类型和产生原因，尤其是与图表理解任务结合场景依然空白。

Method: 构建了一个包含精细化幻觉分类体系的基准ChartHal，并收集了1062个人工标注样本，系统评估了多种主流LVLMs模型的图表理解幻觉表现。

Result: 结果显示，多数主流LVLM（包括GPT-5和o4-mini）在该基准上的准确率仅为34.46%和22.79%，表明幻觉现象极其普遍，模型在面对与图表无关或矛盾信息的问题时更易出错。

Conclusion: 当前LVLMs在图表理解场景下的幻觉问题十分严重，亟需更强的缓解措施和改进方法。

Abstract: Large Vision-Language Models (LVLMs) have recently demonstrated remarkable
progress, yet hallucination remains a critical barrier, particularly in chart
understanding, which requires sophisticated perceptual and cognitive abilities
as well as rigorous factual accuracy. While prior work has investigated
hallucinations and chart comprehension independently, their intersection
remains largely unexplored. To address this gap, we present ChartHal, a
benchmark that features a fine-grained taxonomy of hallucination scenarios in
chart understanding, along with a human-validated dataset of 1,062 samples. Our
evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations
on ChartHal, including proprietary models such as GPT-5 and o4-mini, which
achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals
that questions involving information absent from or contradictory to charts are
especially likely to trigger hallucinations, underscoring the urgent need for
more robust mitigation strategies. Code and data are available at
https://github.com/ymcui/ChartHal .

</details>


### [145] [Multimodal Medical Image Classification via Synergistic Learning Pre-training](https://arxiv.org/abs/2509.17492)
*Qinghua Lin,Guang-Hai Liu,Zuoyong Li,Yang Li,Yuting Jiang,Xiang Wu*

Main category: cs.CV

TL;DR: 本文提出了一种结合预训练和微调的多模态半监督医学图像分类框架，在标注样本稀缺的情况下，实现了多模态病理图像的有效特征融合与分类。


<details>
  <summary>Details</summary>
Motivation: 多模态医学图像能提供更丰富的诊断信息，但因缺乏专家标注数据，基于计算机视觉的多模态诊断融合仍面临挑战，尤其是如何实现少标签情况下的有效特征融合。

Method: 作者提出“预训练+微调”框架。预训练阶段采用一致性、重构与对齐的协同自监督学习方法，将一种模态视为另一种模态的数据增强，提升了模型的特征表示能力。微调阶段，针对不同模态分别设计编码器，并设立多模态融合编码器，同时提出分布偏移校正方法，减轻因标签稀缺带来的预测不确定性及过拟合。

Result: 在Kvasir和Kvasirv2公胃镜数据集上，方法取得了优于现有主流分类方法的定量与定性效果。

Conclusion: 新提出的多模态半监督分类方法在标签不足背景下，有效提升了医学图像诊断的准确性和鲁棒性，为实际临床应用提供了有力支持。

Abstract: Multimodal pathological images are usually in clinical diagnosis, but
computer vision-based multimodal image-assisted diagnosis faces challenges with
modality fusion, especially in the absence of expert-annotated data. To achieve
the modality fusion in multimodal images with label scarcity, we propose a
novel ``pretraining + fine-tuning" framework for multimodal semi-supervised
medical image classification. Specifically, we propose a synergistic learning
pretraining framework of consistency, reconstructive, and aligned learning. By
treating one modality as an augmented sample of another modality, we implement
a self-supervised learning pre-train, enhancing the baseline model's feature
representation capability. Then, we design a fine-tuning method for multimodal
fusion. During the fine-tuning stage, we set different encoders to extract
features from the original modalities and provide a multimodal fusion encoder
for fusion modality. In addition, we propose a distribution shift method for
multimodal fusion features, which alleviates the prediction uncertainty and
overfitting risks caused by the lack of labeled samples. We conduct extensive
experiments on the publicly available gastroscopy image datasets Kvasir and
Kvasirv2. Quantitative and qualitative results demonstrate that the proposed
method outperforms the current state-of-the-art classification methods. The
code will be released at: https://github.com/LQH89757/MICS.

</details>


### [146] [Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models](https://arxiv.org/abs/2509.17498)
*Dilshara Herath,Chinthaka Abeyrathne,Prabhani Jayaweera*

Main category: cs.CV

TL;DR: 本论文系统评估了基于YOLO算法的驾驶员疲劳检测方法，比较多个YOLO变体与传统EAR方法在精度、召回率和实时性上的表现，为自动驾驶和工业安全提供了实践参考。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致交通事故的重要因素，因此需要一种实时、非侵入性、准确的疲劳检测技术以降低事故发生率。

Method: 采用公开的UTA-RLDD数据集，涵盖不同性别、佩戴眼镜、光照和肤色条件，对YOLO七种变体进行微调，使用Precision、Recall、mAP0.5和mAP 0.5-0.95评测性能。同时，实现基于Dlib人脸标志的EAR传统方法进行对比。

Result: YOLOv9c获得最高检测精度（0.986 mAP0.5，0.978 Recall），YOLOv11n在精度（0.954）和推理效率之间达成最佳平衡，适合嵌入式部署。EAR方法虽计算量小但在姿态变化和遮挡下鲁棒性低。

Conclusion: 该研究揭示了准确率、延时与计算资源需求之间的权衡，并为自动驾驶和工业安全领域选择或组合疲劳检测方法提供了可行建议。

Abstract: Driver drowsiness remains a critical factor in road accidents, accounting for
thousands of fatalities and injuries each year. This paper presents a
comprehensive evaluation of real-time, non-intrusive drowsiness detection
methods, focusing on computer vision based YOLO (You Look Only Once)
algorithms. A publicly available dataset namely, UTA-RLDD was used, containing
both awake and drowsy conditions, ensuring variability in gender, eyewear,
illumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,
v11n, v11l) are fine-tuned, with performance measured in terms of Precision,
Recall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest
accuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal
balance between precision (0.954) and inference efficiency, making it highly
suitable for embedded deployment. Additionally, we implement an Eye Aspect
Ratio (EAR) approach using Dlib's facial landmarks, which despite its low
computational footprint exhibits reduced robustness under pose variation and
occlusions. Our findings illustrate clear trade offs between accuracy, latency,
and resource requirements, and offer practical guidelines for selecting or
combining detection methods in autonomous driving and industrial safety
applications.

</details>


### [147] [WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification](https://arxiv.org/abs/2509.17740)
*Yiwen Jiang,Deval Mehta,Siyuan Yan,Yaling Shen,Zimu Wang,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态推理方法WISE，可为图像分类任务自动生成可解释的推理链，在提升模型可解释性的同时也提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态Chain-of-Thought（MCoT）方法多依赖于丰富的推理数据集，且侧重于对象间的推理，容易忽视图像分类中更为重要的对象内部理解。为了解决这一困境，亟需开发能在弱监督下实现对象内部细粒度推理的新方法。

Method: 提出了WISE方法，通过将概念瓶颈模型（CBMs）中的基于概念的表示转换为简洁、可解释的推理链。在弱监督条件下，为常规图像分类数据集自动生成MCoT，增强了数据集的可解释性。

Result: 在10个数据集上的实验表明，WISE生成的MCoT将可解释性提升了37%，并在用于微调多模态大模型（MLLMs）时，提高了分类准确率。

Conclusion: WISE有效地将基于概念的可解释性和生成式多模态推理结合起来，提出了一个可泛化的框架，有助于提升MLLMs在细粒度视觉理解任务中的性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in visual-textual
reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly
enhancing interpretability. However, existing MCoT methods rely on
rationale-rich datasets and largely focus on inter-object reasoning,
overlooking the intra-object understanding crucial for image classification. To
address this gap, we propose WISE, a Weak-supervision-guided Step-by-step
Explanation method that augments any image classification dataset with MCoTs by
reformulating the concept-based representations from Concept Bottleneck Models
(CBMs) into concise, interpretable reasoning chains under weak supervision.
Experiments across ten datasets show that our generated MCoTs not only improve
interpretability by 37% but also lead to gains in classification accuracy when
used to fine-tune MLLMs. Our work bridges concept-based interpretability and
generative MCoT reasoning, providing a generalizable framework for enhancing
MLLMs in fine-grained visual understanding.

</details>


### [148] [SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge](https://arxiv.org/abs/2509.17500)
*Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAMSON的新方法，有效提升了长视频序列中物体追踪与分割的准确性，并在ICCV 2025 MOSE赛道取得了第三名（J&F=0.8427）的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有LSVOS方法难以应对视频中物体重现、小目标、遮挡严重、场景拥挤等问题，且主流方案在处理长视频和相似物体时存在局限。

Method: 1）融合现有最先进VOS模型优势，提出SAMSON框架；2）引入长期记忆模块提升相似物体与长期消失物体的重新识别能力；3）采用SAM2Long后处理降低误差积累，增强长视频分割稳定性。

Result: 在ICCV 2025 MOSE赛道测试集上获得J&F得分0.8427，排名第三，显示了方法优越的分割与追踪表现。

Conclusion: SAMSON结合长期记忆机制与强大后处理策略，有效解决了长视频物体分割中的关键难题，验证了整体范式的有效性与领先性。

Abstract: Large-scale Video Object Segmentation (LSVOS) addresses the challenge of
accurately tracking and segmenting objects in long video sequences, where
difficulties stem from object reappearance, small-scale targets, heavy
occlusions, and crowded scenes. Existing approaches predominantly adopt
SAM2-based frameworks with various memory mechanisms for complex video mask
generation. In this report, we proposed Segment Anything with Memory
Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE
track of ICCV 2025, which integrates the strengths of stateof-the-art VOS
models into an effective paradigm. To handle visually similar instances and
long-term object disappearance in MOSE, we incorporate a long-term memorymodule
for reliable object re-identification. Additionly, we adopt SAM2Long as a
post-processing strategy to reduce error accumulation and enhance segmentation
stability in long video sequences. Our method achieved a final performance of
0.8427 in terms of J &F in the test-set leaderboard.

</details>


### [149] [4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression](https://arxiv.org/abs/2509.17506)
*Houqiang Zhong,Zihan Zheng,Qiang Hu,Yuan Tian,Ning Cao,Lan Xu,Xiaoyun Zhang,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为4D-MoDe的新型四维高斯压缩框架，能够高效压缩和编辑体积视频，实现高质量、低存储开销的体积视频流传输。


<details>
  <summary>Details</summary>
Motivation: 现有体积视频表示方法数据量巨大、运动复杂且可编辑性有限，难以支持大规模高质量的体积视频传输与应用。

Method: 作者提出了分层体积表示，将静态背景与动态前景分离，并采用前瞻运动分解算法降低时序冗余。引入多分辨率运动估计网格和轻量级共享MLP，结合动态高斯补偿机制建模动态内容。辅以自适应分组和背景关键帧插入策略，兼顾时序一致性和压缩效率。此外，通过熵感知训练流程联合优化运动场与高斯参数，并采用区间和KD树压缩方法降低存储负载。

Result: 在多个数据集上的实验结果表明，该方法在重建质量上具有竞争力的同时，存储成本比当前最先进方法低一个数量级（最低可达11.4KB/帧），支持诸如背景替换、仅前景流传输等实际应用。

Conclusion: 4D-MoDe为可扩展、可编辑的高质量体积视频流提供了有效解决方案，在存储效率、编辑能力以及实际应用方面具有明显优势。

Abstract: Volumetric video has emerged as a key medium for immersive telepresence and
augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation
and realistic spatial interactions. However, delivering high-quality dynamic
volumetric content at scale remains challenging due to massive data volume,
complex motion, and limited editability of existing representations. In this
paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework
designed for scalable and editable volumetric video streaming. Our method
introduces a layered representation that explicitly separates static
backgrounds from dynamic foregrounds using a lookahead-based motion
decomposition strategy, significantly reducing temporal redundancy and enabling
selective background/foreground streaming. To capture continuous motion
trajectories, we employ a multi-resolution motion estimation grid and a
lightweight shared MLP, complemented by a dynamic Gaussian compensation
mechanism to model emergent content. An adaptive grouping scheme dynamically
inserts background keyframes to balance temporal consistency and compression
efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes
the motion fields and Gaussian parameters under a rate-distortion (RD)
objective, while employing range-based and KD-tree compression to minimize
storage overhead. Extensive experiments on multiple datasets demonstrate that
4D-MoDe consistently achieves competitive reconstruction quality with an order
of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame)
compared to state-of-the-art methods, while supporting practical applications
such as background replacement and foreground-only streaming.

</details>


### [150] [4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming](https://arxiv.org/abs/2509.17513)
*Zihan Zheng,Zhenlong Wu,Houqiang Zhong,Yuan Tian,Ning Cao,Lan Xu,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4DGCPro提出了一种新的4D高斯压缩架构，实现了体积视频的分层编码和移动端实时解码渲染，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前体积视频压缩方案存在灵活性差、不能按需调整质量和码率，以及移动端实时解码渲染能力有限等问题，阻碍了高质量视听体验的普及。

Method: 4DGCPro采用分层4D高斯表示及运动感知自适应分组，减少时间冗余、保持一致性，并通过渐进式流媒体实现多个细节层的传输。此外，提出了端到端熵优化训练，包括层级率失真监督和属性熵建模，提升压缩效率和码流可控性。

Result: 实验证明，4DGCPro支持单模型内不同质量和码率的灵活调整，在多数据集上率失真性能及移动端实时解码渲染能力均优于现有方法。

Conclusion: 4DGCPro为高保真体积视频在移动设备端的高效编码、传输和高质量播放提供了新的解决方案，为多终端和网络环境下的沉浸式媒体体验铺平了道路。

Abstract: Achieving seamless viewing of high-fidelity volumetric video, comparable to
2D video experiences, remains an open challenge. Existing volumetric video
compression methods either lack the flexibility to adjust quality and bitrate
within a single model for efficient streaming across diverse networks and
devices, or struggle with real-time decoding and rendering on lightweight
mobile platforms. To address these challenges, we introduce 4DGCPro, a novel
hierarchical 4D Gaussian compression framework that facilitates real-time
mobile decoding and high-quality rendering via progressive volumetric video
streaming in a single bitstream. Specifically, we propose a
perceptually-weighted and compression-friendly hierarchical 4D Gaussian
representation with motion-aware adaptive grouping to reduce temporal
redundancy, preserve coherence, and enable scalable multi-level detail
streaming. Furthermore, we present an end-to-end entropy-optimized training
scheme, which incorporates layer-wise rate-distortion (RD) supervision and
attribute-specific entropy modeling for efficient bitstream generation.
Extensive experiments show that 4DGCPro enables flexible quality and multiple
bitrate within a single model, achieving real-time decoding and rendering on
mobile devices while outperforming existing methods in RD performance across
multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro

</details>


### [151] [Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation](https://arxiv.org/abs/2509.17520)
*Mingda Zhang,Yuyang Zheng,Ruixiang Tang,Jingru Qiu,Haiyan Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种新的脑肿瘤分割方法——统一多模态相干场（UMCF），在BraTS 2020和2021数据集上显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分割方法在处理多模态MRI数据时，受限于肿瘤异质性、边界模糊和模态间对比度差异，仅依靠视觉信息或后验损失约束，难以稳定获得准确的层级结构和清晰边界。

Method: 提出UMCF方法，将视觉、语义和空间信息在统一的3D潜空间中同步交互融合，并通过无参不确定性门控自适应调整各模态贡献，将医学先验知识直接融入注意力机制，突破传统“先处理再拼接”架构的限制。

Result: 在BraTS2020和2021公开数据集上，UMCF结合nnU-Net平均Dice系数分别达到0.8579和0.8977，较主流架构平均提升4.18%。

Conclusion: UMCF方法凭借深度融合医学知识和影像特征，为精密医学中的多模态信息融合开辟了新的技术路径，对实现更高精度的脑肿瘤分割具有重要意义。

Abstract: Brain tumor segmentation requires accurate identification of hierarchical
regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)
from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor
tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI
sequences, methods relying solely on visual information or post-hoc loss
constraints show unstable performance in boundary delineation and hierarchy
preservation. To address this challenge, we propose the Unified Multimodal
Coherent Field (UMCF) method. This method achieves synchronous interactive
fusion of visual, semantic, and spatial information within a unified 3D latent
space, adaptively adjusting modal contributions through parameter-free
uncertainty gating, with medical prior knowledge directly participating in
attention computation, avoiding the traditional "process-then-concatenate"
separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021
datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977
respectively, with an average 4.18% improvement across mainstream
architectures. By deeply integrating clinical knowledge with imaging features,
UMCF provides a new technical pathway for multimodal information fusion in
precision medicine.

</details>


### [152] [Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models](https://arxiv.org/abs/2509.17522)
*Hangzhou He,Lei Zhu,Kaiwen Li,Xinliang Zhang,Jiakui Hu,Ourui Fu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的可解释模型Chat-CBM，用语言模型替代传统CBM中的线性分类器，实现更灵活、更有效的人机交互和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统CBM虽然具备可解释性和用户可介入性，但它们普遍采用固定的线性分类器，致使用户干预受到限制，不能在测试时引入新概念或领域知识，且在无监督场景下，干预效果差。

Method: Chat-CBM用语言模型（大语言模型）直接对概念语义进行推理，取代基于分数的线性分类器，使干预方式更丰富——用户可以更直观地纠正、添加或删除概念，也可以融合外部知识和高级推理。整个方法维护了CBM的可解释性，同时扩展了用户与模型交互的界面。

Result: 在九个数据集上的实验证明，Chat-CBM在预测性能上超过了传统CBM，大幅提升了用户交互体验，并保持了基于概念的可解释性。

Conclusion: Chat-CBM不仅提升了CBM的预测能力，还极大拓展了模型的可操作性和适用灵活性，为CBM带来新的交互维度，在无监督场景下依旧有效。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first
predicting a set of human-understandable concepts and then mapping them to
labels through a simple classifier. While users can intervene in the concept
space to improve predictions, traditional CBMs typically employ a fixed linear
classifier over concept scores, which restricts interventions to manual value
adjustments and prevents the incorporation of new concepts or domain knowledge
at test time. These limitations are particularly severe in unsupervised CBMs,
where concept activations are often noisy and densely activated, making user
interventions ineffective. We introduce Chat-CBM, which replaces score-based
classifiers with a language-based classifier that reasons directly over concept
semantics. By grounding prediction in the semantic space of concepts, Chat-CBM
preserves the interpretability of CBMs while enabling richer and more intuitive
interventions, such as concept correction, addition or removal of concepts,
incorporation of external knowledge, and high-level reasoning guidance.
Leveraging the language understanding and few-shot capabilities of frozen large
language models, Chat-CBM extends the intervention interface of CBMs beyond
numerical editing and remains effective even in unsupervised settings.
Experiments on nine datasets demonstrate that Chat-CBM achieves higher
predictive performance and substantially improves user interactivity while
maintaining the concept-based interpretability of CBMs.

</details>


### [153] [SimToken: A Simple Baseline for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2509.17537)
*Dian Jin,Yanghao Zhou,Jinxing Zhou,Jiaqi Ma,Ruohao Guo,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了SimToken框架，通过结合多模态大语言模型（MLLM）和Segement Anything Model（SAM）来解决基于音频、视觉及文本描述的视频特定目标分割任务。


<details>
  <summary>Details</summary>
Motivation: Ref-AVS任务需要利用自然语言动态地定位视频中的特定目标，涉及多模态跨域推理，现有方法在跨模态特征对齐和细粒度分割方面存在局限。本文旨在提升跨模态理解和目标分割精度。

Method: 设计了SimToken框架：由MLLM生成包含多模态上下文信息的紧凑语义token，作为SAM的分割引导提示；提出新颖的语义对齐损失，使不同语言表达但指向同一目标的token表征一致，增强语义学习。

Result: 在Ref-AVS基准数据集上，SimToken取得了优于现有方法的分割表现，证明其有效性。

Conclusion: SimToken框架简洁高效，能够更准确地实现基于自然语言、视听和文本信息引导下的视频目标分割，对于多模态理解和分割任务具有应用前景。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific
objects in videos based on natural language expressions involving audio,
vision, and text information. This task poses significant challenges in
cross-modal reasoning and fine-grained object localization. In this paper, we
propose a simple framework, SimToken, that integrates a multimodal large
language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided
to generate a special semantic token representing the referred object. This
compact token, enriched with contextual information from all modalities, acts
as a prompt to guide SAM to segment objectsacross video frames. To further
improve semantic learning, we introduce a novel target-consistent semantic
alignment loss that aligns token embeddings from different expressions but
referring to the same object. Experiments on the Ref-AVS benchmark demonstrate
that our approach achieves superior performance compared to existing
methods.Code will be available at https://github.com/DianJin-HFUT/SimToken

</details>


### [154] [An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection](https://arxiv.org/abs/2509.17561)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Shufan Chen*

Main category: cs.CV

TL;DR: 本文系统评估了YOLO最新模型（YOLOv8-12）在六类模拟水下环境下的表现，揭示了它们面对水下噪声和失真的鲁棒性与不足，并提出了提升策略。


<details>
  <summary>Details</summary>
Motivation: YOLO模型广泛用于实时目标检测，但其在充满失真和噪声的水下环境下的鲁棒性鲜有系统研究。由于水下图像存在显著降质问题，研究YOLO在此场景的表现及潜在提升方法具有重要实际意义。

Method: 作者采用统一的带标注数据集（来自DUO和Roboflow100，共1万张图像），在六种不同的模拟水下环境下，系统性评估YOLOv8至YOLOv12的检测效果。同时，分析卷积特征受噪声影响情况，研究类别不均衡及外观特征对检测结果的影响，并探索训练时的噪声感知样本注入与增强微调等提升策略。

Result: YOLOv12整体表现最佳，但对噪声极为敏感。噪声严重破坏边缘和纹理特征，导致检测性能下降。类别不均衡主要由图像数和实例频率决定，目标外观影响次之。引入噪声感知样本能提升鲁棒性，增强微调适合特定增强域但会轻微降低原始域性能。

Conclusion: YOLO新版本虽具备强大性能，但在水下噪声环境中仍存在鲁棒性短板。所提出的训练方法可有效提升模型适应性，为构建高效、可靠的水下目标检测系统提供了实用参考。

Abstract: Underwater object detection (UOD) remains a critical challenge in computer
vision due to underwater distortions which degrade low-level features and
compromise the reliability of even state-of-the-art detectors. While YOLO
models have become the backbone of real-time object detection, little work has
systematically examined their robustness under these uniquely challenging
conditions. This raises a critical question: Are YOLO models genuinely robust
when operating under the chaotic and unpredictable conditions of underwater
environments? In this study, we present one of the first comprehensive
evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated
underwater environments. Using a unified dataset of 10,000 annotated images
from DUO and Roboflow100, we not only benchmark model robustness but also
analyze how distortions affect key low-level features such as texture, edges,
and color. Our findings show that (1) YOLOv12 delivers the strongest overall
performance but is highly vulnerable to noise, and (2) noise disrupts edge and
texture features, explaining the poor detection performance in noisy images.
Class imbalance is a persistent challenge in UOD. Experiments revealed that (3)
image counts and instance frequency primarily drive detection performance,
while object appearance exerts only a secondary influence. Finally, we
evaluated lightweight training-aware strategies: noise-aware sample injection,
which improves robustness in both noisy and real-world conditions, and
fine-tuning with advanced enhancement, which boosts accuracy in enhanced
domains but slightly lowers performance in original data, demonstrating strong
potential for domain adaptation, respectively. Together, these insights provide
practical guidance for building resilient and cost-efficient UOD systems.

</details>


### [155] [Visual Instruction Pretraining for Domain-Specific Foundation Models](https://arxiv.org/abs/2509.17562)
*Yuxuan Li,Yicheng Zhang,Wenhao Tang,Yimian Dai,Ming-Ming Cheng,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的视觉基础模型预训练范式ViTP，通过在视觉感知、推理与生成之间构建强化闭环，实现了推理对感知的直接增强。实验表明，ViTP在多项下游任务中刷新了性能纪录。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉中，感知、推理和生成的闭环尚未完整，尤其是高层次推理对底层视觉特征学习的影响尚未被充分研究。论文旨在通过新方法完善这一闭环，提高模型的下游泛化与鲁棒性。

Method: 提出Visual insTruction Pretraining（ViTP）方法，将ViT骨干网络嵌入到视觉-语言模型，基于目标领域指令图像数据进行端到端预训练。同时引入Visual Robustness Learning（VRL）机制，使模型能从稀疏视觉token中学习稳健且相关的特征。

Result: ViTP在16个具有挑战性的遥感和医学图像基准任务上进行了大量实验，结果均实现新的SOTA，表明该方法在不同类型下游任务中均表现优越。

Conclusion: ViTP有效实现了高层推理对感知预训练的正向作用，极大提升了下游任务的表现，推进了视觉基础模型的研究发展。

Abstract: Modern computer vision is converging on a closed loop in which perception,
reasoning and generation mutually reinforce each other. However, this loop
remains incomplete: the top-down influence of high-level reasoning on the
foundational learning of low-level perceptual features is not yet
underexplored. This paper addresses this gap by proposing a new paradigm for
pretraining foundation models in downstream domains. We introduce Visual
insTruction Pretraining (ViTP), a novel approach that directly leverages
reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)
backbone within a Vision-Language Model and pretrains it end-to-end using a
rich corpus of visual instruction data curated from target downstream domains.
ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels
the ViT to learn robust and domain-relevant features from a sparse set of
visual tokens. Extensive experiments on 16 challenging remote sensing and
medical imaging benchmarks demonstrate that ViTP establishes new
state-of-the-art performance across a diverse range of downstream tasks. The
code is available at github.com/zcablii/ViTP.

</details>


### [156] [MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data](https://arxiv.org/abs/2509.17566)
*Ding Shaodong,Liu Ziyang,Zhou Yijun,Liu Tao*

Main category: cs.CV

TL;DR: 本论文提出了一种利用2D视觉基础模型（VFMs）结合多感兴趣区域（ROI）分析，实现对帕金森病的自动诊断方法，在MICCAI 2025挑战赛中以高准确率夺得第一。


<details>
  <summary>Details</summary>
Motivation: 帕金森病诊断具有高度临床需求，目前多依赖MRI等影像生物标志物。由于高质量数据集不足，直接训练3D诊断模型易过拟合，而现有的迁移学习方法因医学影像异质大而效果有限。因此，亟需一种能在小样本情况下高效利用多模态影像特征的新方法。

Method: 作者提出将NM和QSM影像中的多个关键ROI裁剪出来，分别通过独立分支送入2D视觉基础模型，编码成token后拼接形成统一病人表征，用于分类。每个分支有辅助分割头，用以引导特征提取聚焦特定脑核。还设计了多ROI监督对比学习，拉近同类病人表征、远离异类，以提升判别力。

Result: 该方法在MICCAI 2025 PDCADxFoundation挑战赛中达到86.0%的准确率，比第二名高5.5%，且仅用300例有标注影像，表现出色。

Conclusion: 基于2D VFM的多ROI融合方法，在有限标注数据下能提升3D MRI帕金森病诊断性能，显示出VFMs在医学影像领域的潜力。

Abstract: The automatic diagnosis of Parkinson's disease is in high clinical demand due
to its prevalence and the importance of targeted treatment. Current clinical
practice often relies on diagnostic biomarkers in QSM and NM-MRI images.
However, the lack of large, high-quality datasets makes training diagnostic
models from scratch prone to overfitting. Adapting pre-trained 3D medical
models is also challenging, as the diversity of medical imaging leads to
mismatches in voxel spacing and modality between pre-training and fine-tuning
data. In this paper, we address these challenges by leveraging 2D vision
foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and
QSM images, process each ROI through separate branches to compress the ROI into
a token, and then combine these tokens into a unified patient representation
for classification. Within each branch, we use 2D VFMs to encode axial slices
of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary
segmentation head that steers the feature extraction toward specific brain
nuclei. Additionally, we introduce multi-ROI supervised contrastive learning,
which improves diagnostic performance by pulling together representations of
patients from the same class while pushing away those from different classes.
Our approach achieved first place in the MICCAI 2025 PDCADxFoundation
challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled
QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These
results highlight the potential of 2D VFMs for clinical analysis of 3D MR
images.

</details>


### [157] [PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification](https://arxiv.org/abs/2509.17581)
*Florinel Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了一个全新的用于相机识别的PRNU（光响应非均匀性）评测基准和模型，包含大量不同场景下的图片，并采用创新的模型和信号处理方法，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有相机识别方法和数据集在真实、复杂环境下的泛化能力有限，且对PRNU信号的建模和区分不足，难以满足实际应用需求，因此需要更大规模、更具挑战性的基准和更高效的方法。

Method: 1. 构建了包含13K张图片、120多台相机的大型图片数据集，确保训练与测试照片来自不同场景，实现“野外”测试评估。2. 提出了一种混合架构：先用去噪自编码器估算PRNU信号，再用卷积网络完成1对N的相机设备验证。3. 与以往对比学习方法不同，新模型利用参考和查询PRNU信号的Hadamard积作为输入，提升特征区分力。

Result: 实验显示，所提方法在PRNU相机识别任务上，性能远超基于传统去噪自编码器和对比学习的算法，验证了新模型和数据集的有效性。

Conclusion: 新的数据集和PRNU识别模型大幅增强了相机识别在真实场景下的表现，为相机指纹鉴定领域提供了更高效的基准和方法，未来有望在图像取证等实际任务中应用和推广。

Abstract: We propose a novel benchmark for camera identification via Photo Response
Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with
120+ cameras, where the training and test photos are taken in different
scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel
PRNU-based camera identification model that employs a hybrid architecture,
comprising a denoising autoencoder to estimate the PRNU signal and a
convolutional network that can perform 1:N verification of camera devices.
Instead of using a conventional approach based on contrastive learning, our
method takes the Hadamard product between reference and query PRNU signals as
input. This novel design leads to significantly better results compared with
state-of-the-art models based on denoising autoencoders and contrastive
learning. We release our dataset and code at:
https://github.com/CroitoruAlin/PRNU-Bench.

</details>


### [158] [Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models](https://arxiv.org/abs/2509.17588)
*Jinyeong Kim,Seil Kang,Jiwoo Park,Junhyeok Kim,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种分析大型视觉-语言模型（LVLM）注意力头的方法，发现图像到文本的信息流由特定的注意力头传递，这一过程与图像语义内容强相关。


<details>
  <summary>Details</summary>
Motivation: LVLMs在视觉问答任务中能有效整合图像和文本信息，但由于其内部有多个注意力头并行操作，导致其图像到文本信息流的机制难以解释，因此研究其内部信息转移机制具有重要意义。

Method: 作者提出了一种受组件归因方法启发的“头归因”技术，用于识别在图像到文本信息转移过程中起关键作用的注意力头，并进一步在Token层面追踪信息流动过程。

Result: 分析表明，有一组特定的注意力头专门负责图像-文本信息流，其选择依据于图像的语义内容而非视觉外观。在Token层面，文本信息首先传播到与角色相关和最终Token，图像信息则嵌入于与物体和背景相关的Token中。

Conclusion: LVLM中的图像到文本信息流存在结构化流程，用注意力头归因分析为解析其内部机制提供了有力手段，有助于进一步理解和解释视觉-语言模型。

Abstract: Large Vision-Language Models (LVLMs) answer visual questions by transferring
information from images to text through a series of attention heads. While this
image-to-text information flow is central to visual question answering, its
underlying mechanism remains difficult to interpret due to the simultaneous
operation of numerous attention heads. To address this challenge, we propose
head attribution, a technique inspired by component attribution methods, to
identify consistent patterns among attention heads that play a key role in
information transfer. Using head attribution, we investigate how LVLMs rely on
specific attention heads to identify and answer questions about the main object
in an image. Our analysis reveals that a distinct subset of attention heads
facilitates the image-to-text information flow. Remarkably, we find that the
selection of these heads is governed by the semantic content of the input image
rather than its visual appearance. We further examine the flow of information
at the token level and discover that (1) text information first propagates to
role-related tokens and the final token before receiving image information, and
(2) image information is embedded in both object-related and background tokens.
Our work provides evidence that image-to-text information flow follows a
structured process, and that analysis at the attention-head level offers a
promising direction toward understanding the mechanisms of LVLMs.

</details>


### [159] [Domain Adaptive Object Detection for Space Applications with Real-Time Constraints](https://arxiv.org/abs/2509.17593)
*Samet Hicsonmez,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种用于航天领域对象检测的监督式领域自适应方法，显著提升了模型在真实数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前航天对象检测深度学习模型多依赖于模拟数据进行训练，导致在真实数据上因领域差异（domain gap）性能大幅下降，而领域自适应问题在该领域尚未得到充分关注。

Method: 提出基于最新的半监督自适应方法，结合领域不变特征学习、带有CNN判别器的领域判别和不变风险最小化，并进行模块化改进以适用于对象检测。采用SSD（MobileNet主干）和FCOS（ResNet-50主干）进行测试，仅需极少标注的真实数据（250张）。

Result: 在两个空间对象检测数据集（SPEED+与SPARK）上，只用250张标注数据，平均精度（AP）提升高达20个百分点。

Conclusion: 所提出的监督式领域自适应检测方法有效缓解了模拟与真实空间数据间的性能差距，为小样本标注场景下的空间对象检测任务提供了可行的高效解决方案。

Abstract: Object detection is essential in space applications targeting Space Domain
Awareness and also applications involving relative navigation scenarios.
Current deep learning models for Object Detection in space applications are
often trained on synthetic data from simulators, however, the model performance
drops significantly on real-world data due to the domain gap. However, domain
adaptive object detection is an overlooked problem in the community. In this
work, we first show the importance of domain adaptation and then explore
Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled
real data. We build on a recent semi-supervised adaptation method and tailor it
for object detection. Our approach combines domain-invariant feature learning
with a CNN-based domain discriminator and invariant risk minimization using a
domain-independent regression head. To meet real-time deployment needs, we test
our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet
backbone and on the more advanced Fully Convolutional One-Stage object detector
(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and
SPARK. The results show up to 20-point improvements in average precision (AP)
with just 250 labeled real images.

</details>


### [160] [COLA: Context-aware Language-driven Test-time Adaptation](https://arxiv.org/abs/2509.17598)
*Aiming Zhang,Tianyuan Yu,Liang Bai,Jun Tang,Yanming Guo,Yirun Ruan,Yun Zhou,Zhihe Lu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的测试时自适应方法，能够在无需共享标签空间的前提下，使预训练视觉-语言模型（如CLIP）适应多个目标域。通过引入上下文感知模块和类平衡伪标签策略，显著提升了模型在分布变化场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法大多假设源模型和目标域拥有相同的标签空间，这限制了其在多样化、真实世界场景中的应用。本研究旨在探索如何在标签空间不共享的条件下，提升视觉-语言模型的适应能力。

Method: 提出了Context-aware Language-driven TTA（COLA）方法，引入包含任务感知适配器、上下文感知单元和残差连接单元的轻量模块，该模块可插入冻结的VLM中。还设计了类平衡伪标签策略（CBPL）来减缓类别不平衡带来的影响。

Result: 实验结果显示，所提方法在测试时自适应和类别泛化任务中均优于现有主流方法，验证了其有效性和优越性。

Conclusion: COLA方法为视觉-语言模型在无需共享标签空间的多目标域环境下提供了高效、实用的自适应解决方案，对提升模型的泛化及实际部署具有积极意义。

Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its
efficacy in addressing ``distribution shift'' issue while simultaneously
protecting data privacy.
  However, most prior methods assume that a paired source domain model and
target domain sharing the same label space coexist, heavily limiting their
applicability.
  In this paper, we investigate a more general source model capable of
adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno,
CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to
effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA
(COLA).
  The proposed method incorporates a lightweight context-aware module that
consists of three key components: a task-aware adapter, a context-aware unit,
and a residual connection unit for exploring task-specific knowledge,
domain-specific knowledge from the VLM and prior knowledge of the VLM,
respectively.
  It is worth noting that the context-aware module can be seamlessly integrated
into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy
to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but
also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

</details>


### [161] [Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images](https://arxiv.org/abs/2509.17602)
*Giulio Martellucci,Herve Goeau,Pierre Bonnet,Fabrice Vinatier,Alexis Joly*

Main category: cs.CV

TL;DR: 本文介绍了PlantCLEF 2025挑战赛，该挑战关注如何利用AI在多标签植物样方图像上进行物种识别。本次比赛提供了高质量的测试集和大规模训练集，旨在提升生态研究中植物多样性调查的效率。


<details>
  <summary>Details</summary>
Motivation: 生态研究需要在标准化样方图像中准确、快速地识别所有植物物种，传统方法耗时耗力。AI辅助识别可以大幅提升调查速度和覆盖面，有助于生态监测及数据库建设。

Method: 挑战赛设定为一个多标签分类任务，即要求参赛模型在只有单标签训练数据的情况下，对包含多个物种的样方图片进行物种预测。数据集包括2,105张高分辨率多标签测试图和140万张单标签训练图，并配备了基于vision transformer的预训练模型。

Result: 本文系统介绍了所用数据集、评价方法、参赛模型及主要结果，展示了不同方法的性能和实际效果。结果推动了生态学中基于图像的多物种识别技术的进步。

Conclusion: 基于PlantCLEF 2025的数据和竞赛任务，AI在生态学多标签图像物种识别领域展现出很大潜力，有望加快研究进程，提升生态调查效率，但挑战依然存在。

Abstract: Quadrat images are essential for ecological studies, as they enable
standardized sampling, the assessment of plant biodiversity, long-term
monitoring, and large-scale field campaigns. These images typically cover an
area of fifty centimetres or one square meter, and botanists carefully identify
all the species present. Integrating AI could help specialists accelerate their
inventories and expand the spatial coverage of ecological studies. To assess
progress in this area, the PlantCLEF 2025 challenge relies on a new test set of
2,105 high-resolution multi-label images annotated by experts and covering
around 400 species. It also provides a large training set of 1.4 million
individual plant images, along with vision transformer models pre-trained on
this data. The task is formulated as a (weakly labelled) multi-label
classification problem, where the goal is to predict all species present in a
quadrat image using single-label training data. This paper provides a detailed
description of the data, the evaluation methodology, the methods and models
used by participants, and the results achieved.

</details>


### [162] [From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge](https://arxiv.org/abs/2509.17615)
*Lars Heckler-Kram,Ashwin Vaidya,Jan-Hendrik Neudeck,Ulla Scheler,Dick Ameln,Samet Akcay,Paula Ramos*

Main category: cs.CV

TL;DR: 本文介绍了VAND 3.0视觉异常检测挑战赛，总结当前视觉异常检测领域的进展，并提出了更贴合实际场景的评测方法。参赛方法在多个类别下显著超过了以往基线。


<details>
  <summary>Details</summary>
Motivation: 视觉异常检测广泛应用于工业等实际场景，因此需要推动学术界与产业界的深度结合。针对现有方法在实际应用中的分布变化敏感、泛化能力不足等问题，组织了系统性挑战以促进算法进步。

Method: 本次挑战包括两个赛道：一是关注算法对真实世界分布变化的鲁棒性，二是探索视觉-语言模型在少样本异常检测中的能力。参赛者通过组合及改进现有算法，并创新性地引入新流程，在两个赛道都取得了较好表现。

Result: 参赛方法在两个任务类别下均取得了对比基线的显著提升。大规模预训练视觉（及视觉-语言）基础模型为性能提升提供了关键支持。

Conclusion: 当前大模型驱动的结果令人鼓舞，未来需要进一步研究如何在保证准确率的同时，提高算法的效率与部署能力，以满足实际应用中的实时与算力要求。

Abstract: Visual anomaly detection is a strongly application-driven field of research.
Consequently, the connection between academia and industry is of paramount
importance. In this regard, we present the VAND 3.0 Challenge to showcase
current progress in anomaly detection across different practical settings
whilst addressing critical issues in the field. The challenge hosted two
tracks, fostering the development of anomaly detection methods robust against
real-world distribution shifts (Category 1) and exploring the capabilities of
Vision Language Models within the few-shot regime (Category 2), respectively.
The participants' solutions reached significant improvements over previous
baselines by combining or adapting existing approaches and fusing them with
novel pipelines. While for both tracks the progress in large pre-trained vision
(language) backbones played a pivotal role for the performance increase,
scaling up anomaly detection methods more efficiently needs to be addressed by
future research to meet real-time and computational constraints on-site.

</details>


### [163] [Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method](https://arxiv.org/abs/2509.17620)
*Gregory Schroeder,Mohamed Sabry,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种基于三焦点张量的新相机自标定算法TrifocalCalib，可利用最少的图像数据在无场景先验情况下估算相机内参，实现高精度、鲁棒性强的实时自标定。


<details>
  <summary>Details</summary>
Motivation: 在无人驾驶和车队协作等应用中，事先标定相机不可行，需要现场实时、无需场景信息的相机自标定方法。而目前方法往往要求标定物、对相机运动有约束，且准确性和鲁棒性有限。

Method: 利用校准三焦点张量推导标定方程，无需任何标定板或对相机运动设限，仅凭少量图像序列数据，就可同时估算焦距和主点位置，提出新算法TrifocalCalib。

Result: TrifocalCalib方法在合成与真实数据集上均优于现有基于学习和传统方法，表现出更高的精度与鲁棒性。

Conclusion: 无需标定板和场景先验的新三焦点自标定方案TrifocalCalib，能精准估算相机内参，适用于需实时适应的无人系统场景，代码已开源便于复现。

Abstract: Estimating camera intrinsic parameters without prior scene knowledge is a
fundamental challenge in computer vision. This capability is particularly
important for applications such as autonomous driving and vehicle platooning,
where precalibrated setups are impractical and real-time adaptability is
necessary. To advance the state-of-the-art, we present a set of equations based
on the calibrated trifocal tensor, enabling projective camera self-calibration
from minimal image data. Our method, termed TrifocalCalib, significantly
improves accuracy and robustness compared to both recent learning-based and
classical approaches. Unlike many existing techniques, our approach requires no
calibration target, imposes no constraints on camera motion, and simultaneously
estimates both focal length and principal point. Evaluations in both
procedurally generated synthetic environments and structured dataset-based
scenarios demonstrate the effectiveness of our approach. To support
reproducibility, we make the code publicly available.

</details>


### [164] [Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale](https://arxiv.org/abs/2509.17622)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本论文介绍了PlantCLEF2023挑战赛，旨在通过深度学习方法解决全球大规模植物多图像分类问题，推动自动植物识别技术进步。


<details>
  <summary>Details</summary>
Motivation: 全球面临生物多样性危机，植物识别对于人类社会各领域（如农业、建筑、药典）至关重要。但传统人工鉴定方式繁琐耗时，数据积累受限。自动化识别，尤其是深度学习方法，有望突破这一瓶颈。

Method: 本文梳理了PlantCLEF2023挑战赛的资源和评估方式，涉及针对8万种植物、多图像与元数据分类任务。汇总并分析了各参赛团队采用的深度学习方法和系统。

Result: 尽管存在类别众多、类别不平衡、误识别、重复、图像质量参差不齐等挑战，深度学习识别方式整体表现出显著进步。论文总结了比赛中的关键发现与成果。

Conclusion: 深度学习推动了自动化植物识别的可行性，有望实现对全球所有植物的准确识别。PlantCLEF2023挑战推动并验证了相关方法的有效性，并为后续研究奠定了基础。

Abstract: The world is estimated to be home to over 300,000 species of vascular plants.
In the face of the ongoing biodiversity crisis, expanding our understanding of
these species is crucial for the advancement of human civilization,
encompassing areas such as agriculture, construction, and pharmacopoeia.
However, the labor-intensive process of plant identification undertaken by
human experts poses a significant obstacle to the accumulation of new data and
knowledge. Fortunately, recent advancements in automatic identification,
particularly through the application of deep learning techniques, have shown
promising progress. Despite challenges posed by data-related issues such as a
vast number of classes, imbalanced class distribution, erroneous
identifications, duplications, variable visual quality, and diverse visual
contents (such as photos or herbarium sheets), deep learning approaches have
reached a level of maturity which gives us hope that in the near future we will
have an identification system capable of accurately identifying all plant
species worldwide. The PlantCLEF2023 challenge aims to contribute to this
pursuit by addressing a multi-image (and metadata) classification problem
involving an extensive set of classes (80,000 plant species). This paper
provides an overview of the challenge's resources and evaluations, summarizes
the methods and systems employed by participating research groups, and presents
an analysis of key findings.

</details>


### [165] [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models](https://arxiv.org/abs/2509.17627)
*Jinshu Chen,Xinghui Li,Xu Bai,Tianxiang Ma,Pengze Zhang,Zhuowei Chen,Gen Li,Lijie Liu,Songtao Zhao,Bingchuan Li,Qian He*

Main category: cs.CV

TL;DR: 本文提出了一种无需遮罩的多主体视频插入新方法OmniInsert，以及配套数据管道和评测基准，显著提升了插入视频的一致性与自然性，超越现有商用方案。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型视频插入方法依赖复杂控制信号，难以保证主体一致性，实用价值受限。本文旨在面向无需遮罩的通用视频插入任务，解决数据不足、主体与场景平衡及插入和谐三大核心挑战。

Method: 1）提出自动化多样化数据管道InsertPipe，构造交叉配对数据集；2）设计统一视频插入框架OmniInsert，支持单主体与多主体参考；3）采用条件特定特征注入机制区分多源信息，结合渐进式训练实现主体与场景特征平衡；4）引入主体关注损失提升主体细节表现；5）提出插入偏好优化方法模拟人类偏好，同时参考阶段加入上下文感知重构模块强化插入和谐；6）建立综合评测基准InsertBench。

Result: 在InsertBench上验证，OmniInsert在多样场景下插入效果优于现有闭源商用方法，并提供更高主体一致性与融合自然度。

Conclusion: OmniInsert为无需遮罩的视频插入领域带来了统一、高效、无需复杂控制的新方案，提升了实用性并推动了该领域标准化评测发展。代码将开源，方便后续研究。

Abstract: Recent advances in video insertion based on diffusion models are impressive.
However, existing methods rely on complex control signals but struggle with
subject consistency, limiting their practical applicability. In this paper, we
focus on the task of Mask-free Video Insertion and aim to resolve three key
challenges: data scarcity, subject-scene equilibrium, and insertion
harmonization. To address the data scarcity, we propose a new data pipeline
InsertPipe, constructing diverse cross-pair data automatically. Building upon
our data pipeline, we develop OmniInsert, a novel unified framework for
mask-free video insertion from both single and multiple subject references.
Specifically, to maintain subject-scene equilibrium, we introduce a simple yet
effective Condition-Specific Feature Injection mechanism to distinctly inject
multi-source conditions and propose a novel Progressive Training strategy that
enables the model to balance feature injection from subjects and source video.
Meanwhile, we design the Subject-Focused Loss to improve the detailed
appearance of the subjects. To further enhance insertion harmonization, we
propose an Insertive Preference Optimization methodology to optimize the model
by simulating human preferences, and incorporate a Context-Aware Rephraser
module during reference to seamlessly integrate the subject into the original
scenes. To address the lack of a benchmark for the field, we introduce
InsertBench, a comprehensive benchmark comprising diverse scenes with
meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert
outperforms state-of-the-art closed-source commercial solutions. The code will
be released.

</details>


### [166] [Overview of PlantCLEF 2022: Image-based plant identification at global scale](https://arxiv.org/abs/2509.17632)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了PlantCLEF2022挑战赛，其主要任务是利用深度学习技术在包含8万种植物物种的大规模数据集上，通过多图像与元数据进行自动植物分类。


<details>
  <summary>Details</summary>
Motivation: 全球植物物种知识的积累对人类文明至关重要，但依赖专家进行系统性鉴定效率低，限制了新数据与知识的汇聚，尤其在当前生物多样性危机下，自动化鉴定亟需发展。

Method: 论文介绍了PlantCLEF2022挑战，该挑战采用大规模、多图像和元数据的分类任务设定，涉及8万类植物物种，并汇总参赛团队的深度学习系统及其方法策略，并对资源与评测进行说明。

Result: 参赛团队提出了多种基于深度学习的植物识别系统，在极端类别不平衡、样本多样且质量不一的条件下，仍取得了较好识别表现，同时也暴露出相关难题。

Conclusion: 当前深度学习技术已能够应对大规模植物物种的自动识别问题，尽管数据质量与类别分布等现实困难依然存在，但自动鉴定在推动植物多样性知识积累具有巨大应用潜力。

Abstract: It is estimated that there are more than 300,000 species of vascular plants
in the world. Increasing our knowledge of these species is of paramount
importance for the development of human civilization (agriculture,
construction, pharmacopoeia, etc.), especially in the context of the
biodiversity crisis. However, the burden of systematic plant identification by
human experts strongly penalizes the aggregation of new data and knowledge.
Since then, automatic identification has made considerable progress in recent
years as highlighted during all previous editions of PlantCLEF. Deep learning
techniques now seem mature enough to address the ultimate but realistic problem
of global identification of plant biodiversity in spite of many problems that
the data may present (a huge number of classes, very strongly unbalanced
classes, partially erroneous identifications, duplications, variable visual
quality, diversity of visual contents such as photos or herbarium sheets, etc).
The PlantCLEF2022 challenge edition proposes to take a step in this direction
by tackling a multi-image (and metadata) classification problem with a very
large number of classes (80k plant species). This paper presents the resources
and evaluations of the challenge, summarizes the approaches and systems
employed by the participating research groups, and provides an analysis of key
findings.

</details>


### [167] [A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition](https://arxiv.org/abs/2509.17638)
*Zilin Gao,Qilong Wang,Bingbing Zhang,Qinghua Hu,Peihua Li*

Main category: cs.CV

TL;DR: 本文提出了一种用于小样本动作识别（FSAR）的新网络A²M²-Net，通过自适应对齐与多尺度二阶矩描述，有效提升了视频时序错位场景下的识别表现。


<details>
  <summary>Details</summary>
Motivation: 现有FSAR方法未能充分挖掘视频动态的个体运动模式以及特征统计，面对时序错位问题表现不佳，尤其是在使用2D骨干网络时。

Method: A²M²-Net包含两大核心组件：自适应对齐模块（A²）用于动态匹配特征、多尺度二阶矩块（M²）用于多尺度语义特征建模。该方法在特征上实现实例指导下的自适应选择和对齐，实现更强泛化与鲁棒性。

Result: 在五个主流FSAR基准数据集上的实验显示，A²M²-Net性能优于或媲美当前最优方法。

Conclusion: A²M²-Net能够有效处理小样本动作识别中的时序错位问题，具有良好的泛化性和提升潜力。

Abstract: Thanks to capability to alleviate the cost of large-scale annotation,
few-shot action recognition (FSAR) has attracted increased attention of
researchers in recent years. Existing FSAR approaches typically neglect the
role of individual motion pattern in comparison, and under-explore the feature
statistics for video dynamics. Thereby, they struggle to handle the challenging
temporal misalignment in video dynamics, particularly by using 2D backbones. To
overcome these limitations, this work proposes an adaptively aligned
multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the
latent video dynamics with a collection of powerful representation candidates
and adaptively align them in an instance-guided manner. To this end, our
A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$
module) for matching, and multi-scale second-order moment (M$^2$ block) for
strong representation. Specifically, M$^2$ block develops a collection of
semantic second-order descriptors at multiple spatio-temporal scales.
Furthermore, A$^2$ module aims to adaptively select informative candidate
descriptors while considering the individual motion pattern. By such means, our
A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem
by establishing an adaptive alignment protocol for strong representation.
Notably, our proposed method generalizes well to various few-shot settings and
diverse metrics. The experiments are conducted on five widely used FSAR
benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive
performance compared to state-of-the-arts, demonstrating its effectiveness and
generalization.

</details>


### [168] [Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers](https://arxiv.org/abs/2509.17650)
*Soroush Mahdi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 本文提出了一种在推理时可用的token淘汰策略，通过丢弃冗余token来控制视觉Transformer的KV内存规模，显著减少内存消耗，同时几乎不影响准确率和完整性，提升长时序3D感知任务的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的流式视觉Transformer（如StreamVGGT）虽然在3D感知任务中表现优异，但由于KV内存无限增长，限制了其实用性和扩展性。为解决这一内存瓶颈，亟需在保证精度的前提下，有效控制Transformer模型的内存消耗。

Method: 作者提出了一种无需重新训练、在推理阶段生效的动态token淘汰策略。通过在处理视频流时实时识别并丢弃冗余token，仅保留最有信息量的token，从而有效限制KV memory的最大规模。

Result: 实验证明，该方法在7-Scenes等长序列数据集上，将峰值内存消耗从18.63GB降低至9.39GB，准确率和完整性损失不超过0.003。在内存受限情况下，token淘汰还使得帧采样更密集，反而提升了重建精度。其他在深度估计、3D重建、相机位姿估计（Sintel、KITTI、NRGBD等）的实验中亦验证了其高效与实用性。

Conclusion: 提出的token淘汰策略无需更改模型或再训练，即可大幅减少流式视觉Transformer的运行内存，几乎不影响精度，使得长时间、长距离的3D感知推理变得实用且更加易于部署。

Abstract: Streaming visual transformers like StreamVGGT achieve strong 3D perception
but suffer from unbounded growth of key value (KV) memory, which limits
scalability. We propose a training-free, inference-time token eviction policy
that bounds memory by discarding redundant tokens while keeping the most
informative ones. Our method uses significantly less memory with little to no
drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from
18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under
strict memory budgets, eviction enables denser frame sampling, which improves
reconstruction accuracy compared to the baseline. Experiments across video
depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and
camera pose estimation (Sintel, TUM-dynamics) show that our approach closely
matches StreamVGGT at a fraction of the memory and makes long-horizon streaming
inference more practical.

</details>


### [169] [SISMA: Semantic Face Image Synthesis with Mamba](https://arxiv.org/abs/2509.17651)
*Filippo Botti,Alex Ergasti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TL;DR: 本文提出一种新的人脸语义图像合成方法SISMA，基于Mamba架构，能够在降低计算需求的同时生成高质量的样本，且速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语义图像合成特别是人脸生成方面表现优异，但由于注意力层的二次复杂度，训练和推理过程计算量大，速度慢，难以大规模应用。

Method: 提出了一种基于Mamba的新架构SISMA方法，用语义掩码来控制生成图像的形状，降低了对计算资源的需求，实现高效的图像生成。

Result: 在CelebAMask-HQ数据集上的实验表明，SISMA的FID分数优于主流方法，且推理速度是现有最优架构的三倍。

Conclusion: SISMA是一种可行且轻量的替代方案，在保持高质量生成的同时，大幅降低了对计算资源的需求，可替代现有的transformer类模型。

Abstract: Diffusion Models have become very popular for Semantic Image Synthesis (SIS)
of human faces. Nevertheless, their training and inference is computationally
expensive and their computational requirements are high due to the quadratic
complexity of attention layers. In this paper, we propose a novel architecture
called SISMA, based on the recently proposed Mamba. SISMA generates high
quality samples by controlling their shape using a semantic mask at a reduced
computational demand. We validated our approach through comprehensive
experiments with CelebAMask-HQ, revealing that our architecture not only
achieves a better FID score yet also operates at three times the speed of
state-of-the-art architectures. This indicates that the proposed design is a
viable, lightweight substitute to transformer-based models.

</details>


### [170] [Clothing agnostic Pre-inpainting Virtual Try-ON](https://arxiv.org/abs/2509.17654)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Taemin Lee*

Main category: cs.CV

TL;DR: 本文提出CaP-VTON模型，通过多类别掩码和基于Stable Diffusion的皮肤修复，提升了全身服饰虚拟试穿的自然与一致性，显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的虚拟试穿方法虽然提升了服装纹理表现，但在下摆检测不准确和保留原衣服轮廓等方面仍有不足，影响试穿效果的真实性和准确度。

Method: 提出了Clothing agnostic Pre-inpainting Virtual Try-ON（CaP-VTON）模型，融合了Dress Code上的多类别掩码，以及Stable Diffusion基础的皮肤修复机制，特别设计生成人体皮肤模块，能根据人体姿势和肤色对长袖到短袖/无袖转换时的皮肤进行高质量修复。

Result: 在短袖合成准确率上，CaP-VTON达到了92.5%，比Leffa高15.4%。视觉评估中能稳定还原参考服饰的风格和形状。

Conclusion: CaP-VTON在提升虚拟试穿的自然性和一致性上表现突出，且模型本身具有通用性，可为需要高精度虚拟穿着的电商、定制造型、虚拟形象等应用提供支持。

Abstract: With the development of deep learning technology, virtual try-on technology
has become an important application value in the fields of e-commerce, fashion,
and entertainment. The recently proposed Leffa has improved the texture
distortion problem of diffu-sion-based models, but there are limitations in
that the bottom detection inaccuracy and the existing clothing silhouette
remain in the synthesis results. To solve this problem, this study proposes
CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has
improved the naturalness and consistency of whole-body clothing syn-thesis by
integrating multi-category masking based on Dress Code and skin inpainting
based on Stable Diffusion. In particular, a generate skin module was introduced
to solve the skin restoration problem that occurs when long-sleeved images are
converted into short-sleeved or sleeveless ones, and high-quality restoration
was implemented consider-ing the human body posture and color. As a result,
CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved
synthesis accuracy, and showed the performance of consistently reproducing the
style and shape of reference clothing in visual evaluation. These structures
maintain model-agnostic properties and are applicable to various
diffu-sion-based virtual inspection systems, and can contribute to applications
that require high-precision virtual wearing, such as e-commerce, custom
styling, and avatar creation.

</details>


### [171] [Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study](https://arxiv.org/abs/2509.17660)
*Yikun Ma,Bo Li,Ying Chen,Zijie Yue,Shuchang Xu,Jingyao Li,Lei Ma,Liang Zhong,Duowu Zou,Leiming Xu,Yunshi Zhong,Xiaobo Li,Weiqun Ding,Minmin Zhang,Dongli He,Zhenghong Li,Ye Chen,Ye Zhao,Jialong Zhuo,Xiaofen Wu,Lisha Yi,Miaojing Shi,Huihui Sun*

Main category: cs.CV

TL;DR: 本研究首次提出利用人工智能基础模型对内镜图像进行食管胃结合部腺癌（EGJA）筛查与分期诊断，显著提高了诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: EGJA早期检出对改善预后至关重要，但当前诊断高度依赖操作员经验，存在主观性强、准确率不高等局限性。因此，研究者希望借助AI基础模型辅助临床，提高诊断准确性并减少操作员依赖。

Method: 研究基于DINOv2视觉基础模型和ResNet50卷积神经网络提取内镜图像全局与局部特征，并在七家中国医院收集1546例患者共12302张图像，分为训练集和三个独立测试集（保留、外部、前瞻）。通过AI模型对EGJA进行分期预测，并与其它AI模型和内镜医师进行对比评估。

Result: 模型在三组测试集上的分期诊断准确率分别达到0.9256、0.8895、0.8956，优于传统ResNet50模型（最高为0.9125、0.8382、0.8519）以及专家医师（准确率0.8147）。模型辅助下，不同水平内镜医师的分期诊断准确率明显提升。

Conclusion: 该AI基础模型首次应用于EGJA分期诊断，实现了高准确率，且能有效提升各层级内镜医师的诊断表现，展现出广阔的临床应用前景与推广价值。

Abstract: The early detection of esophagogastric junction adenocarcinoma (EGJA) is
crucial for improving patient prognosis, yet its current diagnosis is highly
operator-dependent. This paper aims to make the first attempt to develop an
artificial intelligence (AI) foundation model-based method for both screening
and staging diagnosis of EGJA using endoscopic images. In this cohort and
learning study, we conducted a multicentre study across seven Chinese hospitals
between December 28, 2016 and December 30, 2024. It comprises 12,302 images
from 1,546 patients; 8,249 of them were employed for model training, while the
remaining were divided into the held-out (112 patients, 914 images), external
(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test
sets for evaluation. The proposed model employs DINOv2 (a vision foundation
model) and ResNet50 (a convolutional neural network) to extract features of
global appearance and local details of endoscopic images for EGJA staging
diagnosis. Our model demonstrates satisfactory performance for EGJA staging
diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and
0.8956, respectively. In contrast, among representative AI models, the best one
(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test
sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on
the held-out test set. Moreover, with the assistance of our model, the overall
accuracy for the trainee, competent, and expert endoscopists improves from
0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our
knowledge, our model is the first application of foundation models for EGJA
staging diagnosis and demonstrates great potential in both diagnostic accuracy
and efficiency.

</details>


### [172] [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664)
*Pingyi Chen,Yujing Lou,Shen Cao,Jinhui Guo,Lubin Fan,Yue Wu,Lin Yang,Lizhuang Ma,Jieping Ye*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架SD-VLM，通过改进视觉语言模型（VLM）的空间感知能力，显著提升了其对三维空间定量关系的推理和理解。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在2D语义理解方面表现优异，但在需要定量推理3D空间关系时存在局限，主要原因在于2D图像空间信息的不足。因此，改善VLM的空间表征能力成为提升其分析和推理能力的关键。

Method: 作者提出了两项关键创新：（1）构建了包含精确空间标注的大型空间测量与理解数据集MSMU，涵盖70万对问答、250万物理数值标注和1万条链式思维增强样本；（2）引入了一种简单的深度位置编码方法，以强化VLM的空间感知能力，并据此训练了SD-VLM模型。

Result: SD-VLM在所提MSMU-Bench基准上取得了最先进的性能，在其他空间理解基准（如Q-Spatial和SpatialRGPT-Bench）上也表现出出色的空间泛化能力。实验表明，SD-VLM在MSMU-Bench上分别比GPT-4o和Intern-VL3-78B高出26.91%和25.56%。

Conclusion: SD-VLM有效增强了VLM的三维空间测量和理解能力，在多个评测基准上均取得重大突破，为VLM在空间推理方向的能力提升提供了新范式。

Abstract: While vision language models (VLMs) excel in 2D semantic visual
understanding, their ability to quantitatively reason about 3D spatial
relationships remains under-explored, due to the deficiency of 2D images'
spatial representation ability. In this paper, we analyze the problem hindering
VLMs' spatial understanding abilities and propose SD-VLM, a novel framework
that significantly enhances fundamental spatial perception abilities of VLMs
through two key contributions: (1) propose Massive Spatial Measuring and
Understanding (MSMU) dataset with precise spatial annotations, and (2)
introduce a simple depth positional encoding method strengthening VLMs' spatial
awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA
pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented
samples. We have trained SD-VLM, a strong generalist VLM which shows superior
quantitative spatial measuring and understanding capability. SD-VLM not only
achieves state-of-the-art performance on our proposed MSMU-Bench, but also
shows spatial generalization abilities on other spatial understanding
benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments
demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and
25.56% respectively on MSMU-Bench. Code and models are released at
https://github.com/cpystan/SD-VLM.

</details>


### [173] [Tailored Transformation Invariance for Industrial Anomaly Detection](https://arxiv.org/abs/2509.17670)
*Mariette Schönfeld,Wannes Meert,Hendrik Blockeel*

Main category: cs.CV

TL;DR: 该论文提出了一种名为LWinNN的局部窗口方法，用于工业异常检测，兼顾了精度提升和计算效率。方法结合了kNN和近年复杂方法的优点，对实际数据集表现良好，建议推动更丰富的基准测试标准。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测需要在实际场景中高效且准确地检测异常。现有基于kNN的方法虽然简单高效，但仅利用预训练特征，精度有限。新方法虽提高了精度，但计算开销大，难以实际应用。作者试图在不明显增加计算量的前提下，提升检测精度。

Method: 作者提出LWinNN方法，在kNN的基础上引入局部窗口机制，实现了对局部平移的不变性，使得方法既有kNN的高效，又强化了特征的表达能力。

Result: 实验结果表明，LWinNN方法能显著提升准确率，并降低训练与测试时间。其表现甚至缩小了与现有复杂方法间的差距。

Conclusion: LWinNN展现了使用有限数据通过增强局部变换不变性依然能取得理想精度，表明kNN类方法存在进一步提升空间。同时，现有基准测试对空间多样性要求有限，未来研究可关注更丰富的空间场景。该方法有望成为新的基线方法。

Abstract: Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision
Anomaly Detection that has been receiving increasing amounts of attention due
to its applicability to real-life scenarios. Recent research has focused on how
to extract the most informative features, contrasting older kNN-based methods
that use only pretrained features. These recent methods are much more expensive
to train however and could complicate real-life application. Careful study of
related work with regards to transformation invariance leads to the idea that
popular benchmarks require robustness to only minor translations. With this
idea we then formulate LWinNN, a local window based approach that creates a
middle ground between kNN based methods that have either complete or no
translation invariance. Our experiments demonstrate that this small change
increases accuracy considerably, while simultaneously decreasing both train and
test time. This teaches us two things: first, the gap between kNN-based
approaches and more complex state-of-the-art methodology can still be narrowed
by effective usage of the limited data available. Second, our assumption of
requiring only limited translation invariance highlights potential areas of
interest for future work and the need for more spatially diverse benchmarks,
for which our method can hopefully serve as a new baseline. Our code can be
found at https://github.com/marietteschonfeld/LWinNN .

</details>


### [174] [Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation](https://arxiv.org/abs/2509.17686)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.CV

TL;DR: 本文主要提出了一种多层训练算法，可通过单一RGB图像生成完整且准确的深度图像，并成功解决了深度图像中因像素间隙或不一致导致的信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: 深度成像在自动驾驶系统中至关重要，但因像素数据缺失，导致深度图像中部分信息缺失，影响目标检测与测量的准确性。该研究旨在解决深度图像中信息缺失的难题。

Method: 作者提出了一种多层次训练方法的算法，能够通过单通道RGB图像生成深度图像。同时将算法应用于现有的不完整深度图像，实现自动修复和完善缺失数据。

Result: 算法在Cityscapes数据集上进行了测试，成功补全了其中深度图像的缺失信息，表明该方法在真实城市环境下具有良好效果。

Conclusion: 该方法能有效提升深度图像的完整性和准确性，有助于解决自动驾驶中深度信息缺失的问题，对实际应用具有重要意义。

Abstract: Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it
plays a key role in detecting and measuring objects in the vehicle's
surroundings. However, a significant challenge in this domain arises from
missing information in Depth images, where certain points are not measurable
due to gaps or inconsistencies in pixel data. Our research addresses two key
tasks to overcome this challenge. First, we developed an algorithm using a
multi-layered training approach to generate Depth images from a single RGB
image. Second, we addressed the issue of missing information in Depth images by
applying our algorithm to rectify these gaps, resulting in Depth images with
complete and accurate data. We further tested our algorithm on the Cityscapes
dataset and successfully resolved the missing information in its Depth images,
demonstrating the effectiveness of our approach in real-world urban
environments.

</details>


### [175] [FROQ: Observing Face Recognition Models for Efficient Quality Assessment](https://arxiv.org/abs/2509.17689)
*Žiga Babnik,Deepak Kumar Jain,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本文提出了一种名为FROQ的训练无关、半监督的人脸图像质量评估方法，能够高效且准确地为现代FR模型提供质量估计，无需显式训练。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统在高风险场景中广泛应用，识别错误会造成严重后果。为提升识别可靠性，需对输入的人脸图像质量进行评估。然而，现有高性能FIQA方法大多需大量监督训练，既耗时又耗资源；而无监督方法性能及效率均有限。因而，迫切需要一种无需训练、兼具高效和高性能的FIQA方法。

Method: 本文提出FROQ方法，通过现代人脸识别模型中的中间表示来估计图像质量，结合伪质量标签实现简单校准。伪标签由新颖的无监督样本扰动法生成，无需显式训练，能适配各种主流FR模型。

Result: 在四个主流FR模型和八个公开基准数据集上，FROQ实现了与现有先进方法相当甚至更优的质量估计效果，同时具备更高效率。

Conclusion: FROQ无需训练即可为各类现代FR系统提供准确、高效的人脸图像质量评估，为实际高风险应用中的FR系统带来更高可靠性和应用价值。

Abstract: Face Recognition (FR) plays a crucial role in many critical (high-stakes)
applications, where errors in the recognition process can lead to serious
consequences. Face Image Quality Assessment (FIQA) techniques enhance FR
systems by providing quality estimates of face samples, enabling the systems to
discard samples that are unsuitable for reliable recognition or lead to
low-confidence recognition decisions. Most state-of-the-art FIQA techniques
rely on extensive supervised training to achieve accurate quality estimation.
In contrast, unsupervised techniques eliminate the need for additional training
but tend to be slower and typically exhibit lower performance. In this paper,
we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,
training-free approach that leverages specific intermediate representations
within a given FR model to estimate face-image quality, and combines the
efficiency of supervised FIQA models with the training-free approach of
unsupervised methods. A simple calibration step based on pseudo-quality labels
allows FROQ to uncover specific representations, useful for quality assessment,
in any modern FR model. To generate these pseudo-labels, we propose a novel
unsupervised FIQA technique based on sample perturbations. Comprehensive
experiments with four state-of-the-art FR models and eight benchmark datasets
show that FROQ leads to highly competitive results compared to the
state-of-the-art, achieving both strong performance and efficient runtime,
without requiring explicit training.

</details>


### [176] [Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.17702)
*Patrick Schmidt,Vasileios Belagiannis,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 本文提出了一种与模型无关的深度边缘对齐损失函数，通过结合像素级深度信息，提高弱监督语义分割模型的性能，并显著减少昂贵的像素标注需求。该方法在多个数据集（如PASCAL VOC、MS COCO等）上取得了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 在自动化机器人系统中，训练高性能的语义分割模型通常需要大量成本高昂的像素级密集标注。为了降低标注成本和提升弱监督语义分割效果，作者希望借助机器人常见的深度信息，引入新的监督信号。

Method: 作者提出了一种通用的深度边缘对齐损失（Depth Edge Alignment Loss），具体验证了通过深度数据和边界对齐机制，在仅有图像级监督条件下生成高质量的像素级语义标签，从而提升分割结果。该损失可与其他损失联合使用。

Result: 实验证明该方法在多个模型和数据集上均能提升弱监督语义分割性能。在PASCAL VOC、MS COCO验证集及HOPE静态分割数据集上，mIoU分别提升了+5.439、+1.274和+16.416点。

Conclusion: 引入像素级深度边缘监督后，不仅提升了弱监督语义分割的表现，还可组合现有损失提升效果，对机器人实际应用具有重要意义。

Abstract: Autonomous robotic systems applied to new domains require an abundance of
expensive, pixel-level dense labels to train robust semantic segmentation
models under full supervision. This study proposes a model-agnostic Depth Edge
Alignment Loss to improve Weakly Supervised Semantic Segmentation models across
different datasets. The methodology generates pixel-level semantic labels from
image-level supervision, avoiding expensive annotation processes. While weak
supervision is widely explored in traditional computer vision, our approach
adds supervision with pixel-level depth information, a modality commonly
available in robotic systems. We demonstrate how our approach improves
segmentation performance across datasets and models, but can also be combined
with other losses for even better performance, with improvements up to +5.439,
+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /
MS COCO validation, and the HOPE static onboarding split, respectively. Our
code will be made publicly available.

</details>


### [177] [Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion](https://arxiv.org/abs/2509.17704)
*Bo Li,Yunkuo Lei,Tingting Bao,Yaxian Wang,Lingling Zhang,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经动力学驱动的耦合神经P系统（CNP）模型ND-CNPFuse，有效提升了多焦点图像融合（MFIF）任务中决策图的边界准确性，并在多个公开数据集上达到了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 多焦点图像融合中，如何生成边界精确的决策图一直是难题，现有基于启发式规则和深度学习的方案均存在质量不足或可解释性差的问题。

Method: 作者提出通过分析神经动力学，确定网络参数与输入信号的约束，避免异常发放，利用第三代神经计算模型——耦合神经P系统，将原图转换为可解释性的脉冲矩阵，通过脉冲次数判定清晰与模糊区域，直接生成高质量决策图，无需后处理。

Result: 在Lytro、MFFW、MFI-WHU和Real-MFF等四个经典数据集上，ND-CNPFuse均超越现有方法，取得了最新的SOTA表现。

Conclusion: 神经动力学驱动的CNP系统为多焦点图像融合任务的决策图生成注入了解释性强且高质量的解决思路，为领域内相关研究提供了新方向。

Abstract: Multi-focus image fusion (MFIF) is a crucial technique in image processing,
with a key challenge being the generation of decision maps with precise
boundaries. However, traditional methods based on heuristic rules and deep
learning methods with black-box mechanisms are difficult to generate
high-quality decision maps. To overcome this challenge, we introduce
neurodynamics-driven coupled neural P (CNP) systems, which are third-generation
neural computation models inspired by spiking mechanisms, to enhance the
accuracy of decision maps. Specifically, we first conduct an in-depth analysis
of the model's neurodynamics to identify the constraints between the network
parameters and the input signals. This solid analysis avoids abnormal
continuous firing of neurons and ensures the model accurately distinguishes
between focused and unfocused regions, generating high-quality decision maps
for MFIF. Based on this analysis, we propose a
\textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model
(\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current
ideas of decision map generation, ND-CNPFuse distinguishes between focused and
unfocused regions by mapping the source image into interpretable spike
matrices. By comparing the number of spikes, an accurate decision map can be
generated directly without any post-processing. Extensive experimental results
show that ND-CNPFuse achieves new state-of-the-art performance on four
classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code
is available at https://github.com/MorvanLi/ND-CNPFuse.

</details>


### [178] [Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review](https://arxiv.org/abs/2509.17707)
*Emre Gülsoylu,Alhassan Abdelhalim,Derya Kara Boztas,Ole Grasse,Carlos Jahn,Simone Frintrop,Janick Edinger*

Main category: cs.CV

TL;DR: 本综述回顾了35年来利用计算机视觉对多式联运装载单元（如集装箱、半挂车和可更换车厢）进行身份识别的63篇实证研究，比较了从传统图像处理到深度学习的发展历程，并总结了现有方法在公开数据集缺失、识别准确率差异大等主要挑战，同时提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 多式联运装载单元的标准化推动了全球贸易，但其高效、稳健的身份识别在高吞吐量港口和码头仍是瓶颈，因此需要回顾并总结计算机视觉技术的发展和挑战。

Method: 系统回顾和分析了1990-2025年间63项基于计算机视觉的身份识别实证研究，梳理了技术演化历程，并关注深度学习取代传统方法的趋势；讨论了成本与实用性，并总结了数据集限制、识别准确率、大场景识别和移动设备集成等问题。

Result: 计算机视觉身份识别方法在学术报告中的端到端准确率变化大（5%到96%），同质公开数据集匮乏限制了方法的广泛比较和进步，场景文本识别与移动终端集成成为新挑战。

Conclusion: 论文呼吁学术和产业界标准化术语、开放数据和代码，未来需针对ISO6346等标准优化上下文无关的识别技术，并明确未来研究方向。

Abstract: The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.

</details>


### [179] [RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion](https://arxiv.org/abs/2509.17712)
*Geonho Bang,Minjae Seong,Jisong Kim,Geunju Baek,Daye Oh,Junhyung Kim,Junho Koh,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为RCTDistill的雷达-摄像头融合3D目标检测方法，通过创新的三模块蒸馏与时序融合，实现了比以往更高的检测性能和更快速度。


<details>
  <summary>Details</summary>
Motivation: 雷达-摄像头融合方法尽管成本较低，但3D目标检测精度仍落后于激光雷达方法。现有工作采用知识蒸馏和时序融合提升性能，但未充分考虑目标运动、本体误差及多传感器固有的不确定性。本研究旨在提升雷达-摄像头融合的精度和鲁棒性。

Method: 提出RCTDistill框架，包含三个模块：1）RAKD模块针对距离-方位误差，将激光雷达知识有效转移以优化BEV特征；2）TKD模块通过对齐历史雷达-摄像头特征与当前激光雷达特征，缓解动态物体导致的时序错位；3）RDKD模块通过关系知识蒸馏提升前景与背景判别能力。

Result: 在nuScenes与View-of-Delft(V0D)两大主流数据集上，RCTDistill刷新了雷达-摄像头融合方法的性能记录，并在推理速度上实现了26.2 FPS的业界最快速度。

Conclusion: RCTDistill通过引入针对性知识蒸馏和时序融合方法，显著提升了雷达-摄像头3D目标检测的精度与效率，为成本敏感型自动驾驶系统提供了更优解决方案。

Abstract: Radar-camera fusion methods have emerged as a cost-effective approach for 3D
object detection but still lag behind LiDAR-based methods in performance.
Recent works have focused on employing temporal fusion and Knowledge
Distillation (KD) strategies to overcome these limitations. However, existing
approaches have not sufficiently accounted for uncertainties arising from
object motion or sensor-specific errors inherent in radar and camera
modalities. In this work, we propose RCTDistill, a novel cross-modal KD method
based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge
Distillation (RAKD), Temporal Knowledge Distillation (TKD), and
Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider
the inherent errors in the range and azimuth directions, enabling effective
knowledge transfer from LiDAR features to refine inaccurate BEV
representations. TKD mitigates temporal misalignment caused by dynamic objects
by aligning historical radar-camera BEV features with current LiDAR
representations. RDKD enhances feature discrimination by distilling relational
knowledge from the teacher model, allowing the student to differentiate
foreground and background features. RCTDistill achieves state-of-the-art
radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)
datasets, with the fastest inference speed of 26.2 FPS.

</details>


### [180] [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](https://arxiv.org/abs/2509.17726)
*Javier Bisbal,Patrick Winter,Sebastian Jofre,Aaron Ponce,Sameer A. Ansari,Ramez Abdalla,Michael Markl,Oliver Welin Odeback,Sergio Uribe,Cristian Tejos,Julio Sotelo,Susanne Schnell,David Marlevi*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动脑血管结构标注方法，并引入不确定性量化以提升结果可靠性和临床实用性。nnUNet网络实现了最佳标注表现，其结合了自适应架构调整，同时通过增强推理方案有效识别解剖结构的不确定区域。最终实现了与人工标注高度一致的血流动力学分析，推动此类方法的临床应用。


<details>
  <summary>Details</summary>
Motivation: 脑血管的精确解剖标注是脑血管疾病诊断和血流动力学分析的基础。然而，手动标注费时且操作人员间差异大，因此亟需准确、高效和可解释的自动化标注方法。

Method: 本研究基于35例3D Time-of-Flight 磁共振血管造影（3D ToF-MRA）分割数据，提出了三种卷积神经网络（UNet、CS-Net、nnUNet）架构并引入不确定性量化。通过测试时增强（TTA）及新颖的坐标指导策略，提升结果的可解释性和可靠性。结果还与4D Flow MRI人工标注对比验证临床实用性。

Result: nnUNet在解剖结构复杂血管的标注中取得最高成绩（平均Dice分数0.922，平均表面距离0.387 mm），且增强推理所得的不确定性地图可标示结构歧义和标注不一致区域。自动标注结果推导的血流速度与人工标注对比无统计学差异。

Conclusion: 所提系统可实现高效、准确且具有不确定性感知的脑血管自动标注，结果适配血流动力学分析需求，具备良好的临床应用前景。

Abstract: Accurate anatomical labeling of intracranial arteries is essential for
cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming
and subject to interoperator variability. We present a deep learning-based
framework for automated artery labeling from 3D Time-of-Flight Magnetic
Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating
uncertainty quantification to enhance interpretability and reliability. We
evaluated three convolutional neural network architectures: (1) a UNet with
residual encoder blocks, reflecting commonly used baselines in vascular
labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and
spatial attention mechanisms for enhanced curvilinear structure recognition;
and (3) nnUNet, a self-configuring framework that automates preprocessing,
training, and architectural adaptation based on dataset characteristics. Among
these, nnUNet achieved the highest labeling performance (average Dice score:
0.922; average surface distance: 0.387 mm), with improved robustness in
anatomically complex vessels. To assess predictive confidence, we implemented
test-time augmentation (TTA) and introduced a novel coordinate-guided strategy
to reduce interpolation errors during augmented inference. The resulting
uncertainty maps reliably indicated regions of anatomical ambiguity,
pathological variation, or manual labeling inconsistency. We further validated
clinical utility by comparing flow velocities derived from automated and manual
labels in co-registered 4D Flow MRI datasets, observing close agreement with no
statistically significant differences. Our framework offers a scalable,
accurate, and uncertainty-aware solution for automated cerebrovascular
labeling, supporting downstream hemodynamic analysis and facilitating clinical
integration.

</details>


### [181] [Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA](https://arxiv.org/abs/2509.17743)
*Chenglin Li,Feng Han,FengTao,Ruilin Li,Qianglong Chen,Jingqi Tong,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了一种名为FS-VisPR的视觉推理框架，针对长视频问答任务在效率和准确性上取得提升，超过了GPT-4o，并与业内领先的大模型效果持平。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在视觉任务编程工作流中表现出色，但多依赖闭源模型，推理方式不够系统，且难以应对长视频问答；因此需要一种更高效、开放且系统化的推理架构。

Method: 1. 设计高效的视觉模块（如关键片段检索、字幕检索）来支持长视频任务；2. 构建快-慢推理数据集，用优秀的大模型（LLM）训练对齐开源LLM生成视觉工作流的能力；3. 提出快-慢推理框架：简单问题交由VideoLLM直接解答，复杂或低置信度的情况则采用程序化视觉推理，并有参数搜索优化和回退机制。

Result: FS-VisPR在LVBench数据集上取得50.4%的精度，超过GPT-4o，并在VideoMME数据集上与Qwen2.5VL-72B持平，展现了更优的效率及可靠性。

Conclusion: 该方法有效提升了视觉推理任务中的工作流效率和鲁棒性，为开源视频问答及长视频推理任务提供了更优的技术路线，具备实际应用价值。

Abstract: Large language models (LLMs) have shown promise in generating program
workflows for visual tasks. However, previous approaches often rely on
closed-source models, lack systematic reasoning, and struggle with long-form
video question answering (videoQA). To address these challenges, we introduce
the FS-VisPR framework, an adaptive visual program reasoning approach that
balances fast reasoning for simple queries with slow reasoning for difficult
ones. First, we design efficient visual modules (e.g., key clip retrieval and
subtitle retrieval) to support long-form video tasks. Then, we construct a
diverse and high-quality fast-slow reasoning dataset with a strong LLM to align
open-source language models' ability to generate visual program workflows as
FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple
queries are directly solved by VideoLLMs, while difficult ones invoke visual
program reasoning, motivated by human-like reasoning processes. During this
process, low-confidence fast-thinking answers will trigger a second-stage
slow-reasoning process, and a fallback mechanism to fast reasoning is activated
if the program execution fails. Moreover, we improve visual programs through
parameter search during both training and inference. By adjusting the
parameters of the visual modules within the program, multiple variants are
generated: during training, programs that yield correct answers are selected,
while during inference, the program with the highest confidence result is
applied. Experiments show that FS-VisPR improves both efficiency and
reliability in visual program workflows. It achieves 50.4% accuracy on LVBench,
surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.

</details>


### [182] [Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification](https://arxiv.org/abs/2509.17747)
*Sheng Huang,Jiexuan Yan,Beiyan Liu,Bo Liu,Richang Hong*

Main category: cs.CV

TL;DR: 本文针对现实世界中常见的多类别长尾分布与小样本问题，在多标签图像分类任务中提出了HP-DVAL方法，引入视觉-语言预训练模型（VLP）和分层提示调优策略，以有效缓解类别不平衡带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类常遇到类别极度不平衡和多物体识别的难题，传统方法难以充分利用数据中少数类和多模态信息，因此需要新方法更好地解决这类问题。

Method: 提出HP-DVAL方法，结合视觉-语言预训练模型，通过双视图对齐学习提取互补特征，实现更精确的图文对齐。此外，采用分层提示调优策略（全局与局部提示），并设计语义一致性损失，确保提示调优过程中不偏离VLP模型的通用知识。

Result: 在MS-COCO和VOC2007两个CI-MLIC基准上，HP-DVAL方法在长尾多标签图像分类任务上分别提升mAP 10.0%和5.2%，在多标签小样本分类任务上提升6.8%和2.9%，均优于当前最新方法。

Conclusion: HP-DVAL能有效缓解多标签图像分类中的类别不平衡问题，充分挖掘VLP模型的多模态知识，显著提高了少数类和小样本的识别能力，具有很好的泛化与实际应用价值。

Abstract: Real-world datasets often exhibit class imbalance across multiple categories,
manifesting as long-tailed distributions and few-shot scenarios. This is
especially challenging in Class-Imbalanced Multi-Label Image Classification
(CI-MLIC) tasks, where data imbalance and multi-object recognition present
significant obstacles. To address these challenges, we propose a novel method
termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which
leverages multi-modal knowledge from vision-language pretrained (VLP) models to
mitigate the class-imbalance problem in multi-label settings. Specifically,
HP-DVAL employs dual-view alignment learning to transfer the powerful feature
representation capabilities from VLP models by extracting complementary
features for accurate image-text alignment. To better adapt VLP models for
CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes
global and local prompts to learn task-specific and context-related prior
knowledge. Additionally, we design a semantic consistency loss during prompt
tuning to prevent learned prompts from deviating from general knowledge
embedded in VLP models. The effectiveness of our approach is validated on two
CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results
demonstrate the superiority of our method over SOTA approaches, achieving mAP
improvements of 10.0\% and 5.2\% on the long-tailed multi-label image
classification task, and 6.8\% and 2.9\% on the multi-label few-shot image
classification task.

</details>


### [183] [Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance](https://arxiv.org/abs/2509.17757)
*Hongxing Fan,Lipeng Wang,Haohua Chen,Zehuan Huang,Jiangtao Wu,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种协同多智能体推理框架，实现了对被遮挡物体隐含部分的高质量生成，并直接输出分层的RGBA结果，性能超过以往方法。


<details>
  <summary>Details</summary>
Motivation: 遮挡物体的隐部分生成（Amodal completion）对图像编辑、增强现实等应用至关重要。现有方法存在数据需求高、泛化能力弱、管道式方法误差累积等问题。本文为了解决这些痛点，提出新的协同推理模型。

Method: 提出了基于协同多智能体推理的框架。多个智能体协同推理遮挡关系并判断边界扩展，产生用于修复的精确掩码。同时有智能体生成细粒度文本描述，为后续语义引导提供支持，确保生成内容准确。利用Diffusion Transformer, 按可见掩码和注意力图直接输出分层的RGBA，无需额外分割步骤。

Result: 实验表明，该方法在视觉质量上达到当前最优（state-of-the-art），修复区域内容真实且边界处理精确。

Conclusion: 协同多智能体推理和细粒度语义引导显著提升了Amodal completion的效果，并简化了流程，直接输出可用的分层图像，对编辑等实际应用具有重要意义。

Abstract: Amodal completion, generating invisible parts of occluded objects, is vital
for applications like image editing and AR. Prior methods face challenges with
data needs, generalization, or error accumulation in progressive pipelines. We
propose a Collaborative Multi-Agent Reasoning Framework based on upfront
collaborative reasoning to overcome these issues. Our framework uses multiple
agents to collaboratively analyze occlusion relationships and determine
necessary boundary expansion, yielding a precise mask for inpainting.
Concurrently, an agent generates fine-grained textual descriptions, enabling
Fine-Grained Semantic Guidance. This ensures accurate object synthesis and
prevents the regeneration of occluders or other unwanted elements, especially
within large inpainting areas. Furthermore, our method directly produces
layered RGBA outputs guided by visible masks and attention maps from a
Diffusion Transformer, eliminating extra segmentation. Extensive evaluations
demonstrate our framework achieves state-of-the-art visual quality.

</details>


### [184] [Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2509.17762)
*Sitian Shen,Georgi Pramatarov,Yifu Tao,Daniele De Martini*

Main category: cs.CV

TL;DR: 本文提出了一种新的神经3D高斯球体（3DGS）框架Neural-MMGS，将图像、LiDAR和语义等多模态信息融合为每个高斯体的紧凑可学习嵌入，用于大规模场景重建。该方法实现了高质量重建，且存储开销较低。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模场景重建方法虽然引入了LiDAR数据提供的几何约束，但对LiDAR的物理属性和语义信息的利用不充分，且多模态参数的直接拼接带来了内存开销和信息融合受限的问题。

Method: 将图像、LiDAR与语义信息采用端到端的方式融合为每个高斯体的单一可学习嵌入，通过轻量级神经解码器解码至高斯参数，实现多模态信息的紧凑、高效融合与重建。

Result: 在Oxford Spires和KITTI-360数据集上评估了本文方法。在Oxford Spires数据集上获得了更高质量的重建效果，在KITTI-360上以更低存储消耗取得了与先进LiDAR新视角合成方法相当的表现。

Conclusion: Neural-MMGS实现了更高的重建质量和更优的存储效率，验证了多模态信息融合在高斯体框架中的有效性和可扩展性。

Abstract: This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.

</details>


### [185] [Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics](https://arxiv.org/abs/2509.17769)
*Yang Li,Xinyi Zeng,Zhe Xue,Pinxian Zeng,Zikai Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新型脉冲神经元模型RPLIF，将生物神经元的绝对不应期机制引入LIF神经元模型，提升了SNN的鲁棒性与效率，并在多个神经形态数据集上达到了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的SNN脉冲神经元模型（如IF、LIF）未考虑生物神经元中重要的绝对不应期特性，导致神经元在强输入或异常输入下可能出现过度激活和信号干扰，影响模型的鲁棒性和表现。

Method: 作者提出通过脉冲触发的阈值动态，简单、高效地将绝对不应期机制整合到LIF神经元中，形成了RPLIF模型。每次神经元放电后，动态提高其阈值，使其短时间内对输入不敏感，以模拟生物神经元的不应期行为。

Result: RPLIF在Cifar10-DVS数据集达82.40%，N-Caltech101达83.35%，在DVS128 Gesture达到97.22%准确率，均超越现有方法，且所需时间步更少、延迟更低。

Conclusion: 引入不应期机制对提升SNN性能有显著作用，RPLIF模型在提升鲁棒性、效率和性能的同时，计算开销极低，是一种有效、实用的SNN改进方法。

Abstract: As the third generation of neural networks, spiking neural networks (SNNs)
have recently gained widespread attention for their biological plausibility,
energy efficiency, and effectiveness in processing neuromorphic datasets. To
better emulate biological neurons, various models such as Integrate-and-Fire
(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.
However, these neuron models overlook the refractory period, a fundamental
characteristic of biological neurons. Research on excitable neurons reveal that
after firing, neurons enter a refractory period during which they are
temporarily unresponsive to subsequent stimuli. This mechanism is critical for
preventing over-excitation and mitigating interference from aberrant signals.
Therefore, we propose a simple yet effective method to incorporate the
refractory period into spiking LIF neurons through spike-triggered threshold
dynamics, termed RPLIF. Our method ensures that each spike accurately encodes
neural information, effectively preventing neuron over-excitation under
continuous inputs and interference from anomalous inputs. Incorporating the
refractory period into LIF neurons is seamless and computationally efficient,
enhancing robustness and efficiency while yielding better performance with
negligible overhead. To the best of our knowledge, RPLIF achieves
state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)
with fewer timesteps and demonstrates superior performance on DVS128
Gesture(97.22%) at low latency.

</details>


### [186] [I2VWM: Robust Watermarking for Image to Video Generation](https://arxiv.org/abs/2509.17773)
*Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向图像引导的视频生成任务（I2V）的跨模态数字水印方法I2VWM，有效提升了水印在生成视频中的鲁棒性和不可感知性，同时解决了传统方法无法追踪源图像的问题。


<details>
  <summary>Details</summary>
Motivation: 随着图像引导的视频生成技术迅速发展，其潜在的错误信息传播或欺诈风险也在增加，对高效数字水印提出新需求。现有方法在单一模态下表现良好，但无法在I2V场景中追踪来源和保证水印持久性，因此亟需更强鲁棒性和溯源能力的跨模态水印技术。

Method: 作者提出Robust Diffusion Distance来度量水印信号在生成视频中的时间持续性。在此基础上，设计了I2VWM水印框架：训练时引入视频仿真噪声层，推理时用基于光流的对齐模块增强水印跨时间段的鲁棒性。

Result: 在开源与商业I2V模型上的实验显示，I2VWM方法大幅提升了水印的鲁棒性，在不影响不可感知性的同时，优于现有跨模态水印方法。

Conclusion: I2VWM为生成式视频场景下的数字水印提供了全新范式，有效解决了跨模态水印持久性与溯源性难题，对阻止生成内容误用具有重要意义。

Abstract: The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}

</details>


### [187] [Accurate and Efficient Low-Rank Model Merging in Core Space](https://arxiv.org/abs/2509.17786)
*Aniello Panariello,Daniel Marczak,Simone Magistri,Angelo Porrello,Bartłomiej Twardowski,Andrew D. Bagdanov,Simone Calderara,Joost van de Weijer*

Main category: cs.CV

TL;DR: 本论文提出了一种新的低秩适应神经网络合并框架Core Space，显著提升合并效率并提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 在大模型微调中，低秩适应方法如LoRA大大提升了参数高效性，但现有模型合并方法通常会损失适应的高效率。作者希望提出一种高效、准确的合并方法。

Method: 提出Core Space合并框架，将低秩适应的模型投影到共同比对基下进行合并，不仅保留低秩适应的高效性，并正式证明该方法不会丢失信息。论文还给出了复杂度分析和大量实验证明有效性。

Result: Core Space方法相比现有方法在视觉和语言任务上显著提升了合并后模型的准确率，且计算资源消耗更低，达到最新水平。

Conclusion: Core Space合并方法有效解决了低秩适应模型合并的效率和准确性问题，为高效多模型合并提供了新方案。

Abstract: In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.

</details>


### [188] [From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes](https://arxiv.org/abs/2509.17789)
*Guoxi Huang,Haoran Wang,Zipeng Qi,Wenjun Lu,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文提出了一种名为R-Splatting的新方法，将水下图像恢复与3D高斯Splatting结合，用于提升水下3D重建的渲染质量与几何精度。方法通过多种恢复视图集成、对比损失和不确定性感知的不透明度优化，有效抑制噪声和光照变化影响。实验结果表明在公开和自建数据集上均优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光照、浑浊等影响严重退化，限制了3D重建的质量。以往简化的物理模型难以适用于复杂水下场景，因此亟需新方法有效结合图像恢复和3D建模提升全方位性能。

Method: 提出R-Splatting框架，将多种基于水下图像恢复（UIR）模型增强后的视图，统一整合进3D高斯Splatting重建流程。在推理阶段引入轻量化照明生成器，并通过对比损失实现照明特征的解耦与稳定。此外，创新性地提出不确定性感知的不透明度优化（UAOO），以随机方式建模不透明度，缓解因光照引发的梯度异常和过拟合。

Result: 在Seathru-NeRF和新建BlueCoral3D数据集上实验，R-Splatting方法在渲染质量和几何精度两个方面都优于当前主流基线方法。

Conclusion: R-Splatting通过集成多种恢复视图和对不透明度的不确定性建模，实现了水下3D重建任务的性能突破，兼顾渲染效果与几何还原度，推动了复杂水下场景下3D建模技术的发展。

Abstract: Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.

</details>


### [189] [Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding](https://arxiv.org/abs/2509.17792)
*S M A Sharif,Abdur Rehman,Fayaz Ali Dharejo,Radu Timofte,Rizwan Ali Naqvi*

Main category: cs.CV

TL;DR: 该论文提出了一种无需手工先验和外部提示的通用图像修复方法，在多个退化场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图像常存在多样的空间性退化（如雾、雨、雪、低光等），影响下游视觉任务，但现有方法依赖于外部文本或人为构建的先验，导致泛化性差，难以处理新型或混合退化。

Method: 作者将图像修复任务视作潜在先验推断过程，自动从输入图像中得出与退化相关的表示，不需要明确的任务指令。提出了一种结构化推理范式，包括自适应特征选择、空间定位与退化语义理解，并设计了轻量级的解码模块，实现高效的空间自适应修复。

Result: 在六种常见退化任务、五种复合设置以及未见过的新型退化上，方法均优于SOTA，平均提升PSNR 1.68 dB，且效率提升3倍。

Conclusion: 该方法无需依赖外部提示或手工先验，能更好泛化到未知退化类型，在准确性和效率上较现有方法均有明显优势。

Abstract: Real-world images often suffer from spatially diverse degradations such as
haze, rain, snow, and low-light, significantly impacting visual quality and
downstream vision tasks. Existing all-in-one restoration (AIR) approaches
either depend on external text prompts or embed hand-crafted architectural
priors (e.g., frequency heuristics); both impose discrete, brittle assumptions
that weaken generalization to unseen or mixed degradations. To address this
limitation, we propose to reframe AIR as learned latent prior inference, where
degradation-aware representations are automatically inferred from the input
without explicit task cues. Based on latent priors, we formulate AIR as a
structured reasoning paradigm: (1) which features to route (adaptive feature
selection), (2) where to restore (spatial localization), and (3) what to
restore (degradation semantics). We design a lightweight decoding module that
efficiently leverages these latent encoded cues for spatially-adaptive
restoration. Extensive experiments across six common degradation tasks, five
compound settings, and previously unseen degradations demonstrate that our
method outperforms state-of-the-art (SOTA) approaches, achieving an average
PSNR improvement of 1.68 dB while being three times more efficient.

</details>


### [190] [TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification](https://arxiv.org/abs/2509.17802)
*Qi'ao Xu,Pengfei Wang,Bo Zhong,Tianwen Qian,Xiaoling Wang,Ye Wang,Hong Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的医疗时间序列分类框架，将生理信号转化为伪图像并利用预训练视觉模型，显著提升了跨个体分类性能。实验在六个数据集上显示，该方法优于现有多种方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗时间序列分类方法在跨个体泛化能力不足，主要受限于模态特异性的归纳偏置和难以学习具有普适性的不变表征。

Method: 提出了TS-P$^2$CL框架，将一维生理信号转换为二维伪图像，通过视觉模型引入来自自然图像的丰富先验信息。在转换后的统一空间中，采用双重对比学习：一是模态内一致性以保持时序连贯，二是跨模态对齐以将时序动态与视觉语义一致，从而弱化个体特异性偏差，提取鲁棒的、领域不变特征。

Result: 在六个医疗时间序列数据集上，所提方法在受试者相关（subject-dependent）和不相关（subject-independent）两种场景下，均显著优于现有的十四种方法。

Conclusion: 结合视觉领域预训练模型和将时序信号图像化的新范式，有效提升了医疗时间序列分类的跨个体泛化能力，具有良好的应用前景。

Abstract: Medical time series (MedTS) classification is pivotal for intelligent
healthcare, yet its efficacy is severely limited by poor cross-subject
generation due to the profound cross-individual heterogeneity. Despite advances
in architectural innovations and transfer learning techniques, current methods
remain constrained by modality-specific inductive biases that limit their
ability to learn universally invariant representations. To overcome this, we
propose TS-P$^2$CL, a novel plug-and-play framework that leverages the
universal pattern recognition capabilities of pre-trained vision models. We
introduce a vision-guided paradigm that transforms 1D physiological signals
into 2D pseudo-images, establishing a bridge to the visual domain. This
transformation enables implicit access to rich semantic priors learned from
natural images. Within this unified space, we employ a dual-contrastive
learning strategy: intra-modal consistency enforces temporal coherence, while
cross-modal alignment aligns time-series dynamics with visual semantics,
thereby mitigating individual-specific biases and learning robust,
domain-invariant features. Extensive experiments on six MedTS datasets
demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both
subject-dependent and subject-independent settings.

</details>


### [191] [Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections](https://arxiv.org/abs/2509.17805)
*Dong Chen,Huili Peng,Yong Hu,Kenneth MC. Cheung*

Main category: cs.CV

TL;DR: 本文系统性地评估了摄像机视角（正面vs侧面）对于2D无标记步态分析精度的影响，并与3D运动捕捉结果对比。侧面视角在矢状面运动学方面表现更佳，正面视角则在对称性参数上更优。


<details>
  <summary>Details</summary>
Motivation: 现有2D步态分析精度受摄像机视角影响较大，但缺乏系统性定量研究。明确不同视角对分析结果的影响，有助于优化临床步态评估方案。

Method: 招募18名受试者，同时采集其正面、侧面2D视频与3D运动捕捉数据。利用YOLOv8进行姿态估计，采用DTW、MCC、KLD和IE四种指标评估2D与3D结果的一致性。统计检验采用Wilcoxon符号秩检验和Cliff's delta计算效应量。

Result: 侧面视角在步幅和膝关节旋转等矢状面参数上显著优于正面视角；正面视角在躯干旋转及腕-髋中距等对称性参数上更优。多数效应量中等到大。

Conclusion: 摄像机视角对2D步态分析参数有显著影响。侧面适合矢状面运动学，正面适合对称性分析。结合两视角有助于提升临床应用价值，未来应依疾病需求灵活部署。

Abstract: Objective: To systematically quantify the effect of the camera view (frontal
vs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D
motion capture ground truth. Methods: Gait data from 18 subjects were recorded
simultaneously using frontal, lateral and 3D motion capture systems. Pose
estimation used YOLOv8. Four metrics were assessed to evaluate agreement:
Dynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation
(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution
differences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank
tests (significance: $p < 0.05$) and Cliff's delta ($\delta$) were used to
measure statistical differences and effect sizes. Results: Lateral views
significantly outperformed frontal views for sagittal plane kinematics: step
length (DTW: $53.08 \pm 24.50$ vs. $69.87 \pm 25.36$, $p = 0.005$) and knee
rotation (DTW: $106.46 \pm 38.57$ vs. $155.41 \pm 41.77$, $p = 0.004$). Frontal
views were superior for symmetry parameters: trunk rotation (KLD: $0.09 \pm
0.06$ vs. $0.30 \pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:
$105.77 \pm 29.72$ vs. $75.20 \pm 20.38$, $p = 0.003$). Effect sizes were
medium-to-large ($\delta: 0.34$--$0.76$). Conclusion: Camera view critically
impacts gait parameter accuracy. Lateral views are optimal for sagittal
kinematics; frontal views excel for trunk symmetry. Significance: This first
systematic evidence enables data-driven camera deployment in 2D gait analysis,
enhancing clinical utility. Future implementations should leverage both views
via disease-oriented setups.

</details>


### [192] [Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training](https://arxiv.org/abs/2509.17816)
*Brown Ebouky,Ajad Chhatkuli,Cristiano Malossi,Christoph Studer,Roy Assaf,Andrea Bartezzaghi*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLARE的新型自监督预训练任务，通过高效适配视觉基础模型到新领域，在少量无标签数据下提升下游语义分割表现。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督学习（SSL）在通用数据集上表现优异，但其在数据有限且面向密集预测任务的新领域中的适配研究不足，尤其是如何高效将预训练的视觉基础模型迁移到新领域。

Method: 作者提出GLARE方法，通过引入局部（patch级）增强保证局部一致性，并设计区域一致性约束利用空间语义信息。技术上，将已有自监督模型的Vision Transformer作为初始化，仅更新轻量级的UniAdapter模块，其它主干网络参数保持冻结，实现高效持续预训练。

Result: 在多个跨域语义分割基准上进行实验证明，GLARE方法能够在极少计算和参数开销下，持续提升下游语义分割任务的性能。

Conclusion: GLARE是一种高效、数据节省的自监督适配方法，为视觉基础模型向新领域迁移（特别是语义分割）提供了有效解决方案，有潜力广泛应用于实际数据有限场景。

Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training
foundation models by leveraging large-scale unlabeled datasets, often producing
representations with strong generalization capabilities. These models are
typically pre-trained on general-purpose datasets such as ImageNet and
subsequently adapted to various downstream tasks through finetuning. While
recent advances have explored parameter-efficient strategies for adapting
pre-trained models, extending SSL pre-training itself to new domains -
particularly under limited data regimes and for dense prediction tasks -
remains underexplored. In this work, we address the problem of adapting vision
foundation models to new domains in an unsupervised and data-efficient manner,
specifically targeting downstream semantic segmentation. We propose GLARE
(Global Local and Regional Enforcement), a novel continual self-supervised
pre-training task designed to enhance downstream segmentation performance.
GLARE introduces patch-level augmentations to encourage local consistency and
incorporates a regional consistency constraint that leverages spatial semantics
in the data. For efficient continual pre-training, we initialize Vision
Transformers (ViTs) with weights from existing SSL models and update only
lightweight adapter modules - specifically UniAdapter - while keeping the rest
of the backbone frozen. Experiments across multiple semantic segmentation
benchmarks on different domains demonstrate that GLARE consistently improves
downstream performance with minimal computational and parameter overhead.

</details>


### [193] [ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818)
*Yiyang Chen,Xuanhua He,Xiujun Ma,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为ContextFlow的新型Diffusion Transformer（DiT）框架，能够在无需训练的条件下实现高质量的视频对象编辑任务（插入、交换、删除），显著提升了时序一致性与保真度。


<details>
  <summary>Details</summary>
Motivation: 现有训练免费的视频对象编辑方法主要针对U-Net架构，存在反演不准确和特征替换带来上下文冲突的问题。对于基于Diffusion Transformer的模型，这些问题更加突出，因为传统的层选择启发式方法难以有效地对模型编辑进行指导。因此，急需针对DiT设计有效、高保真的视频对象编辑方法。

Method: 本方法提出了ContextFlow框架，核心创新包括：1）使用高阶Rectified Flow算法解决反演精度不足的问题；2）设计自适应上下文扩充机制，通过拼接重建和编辑路径的Key-Value对，丰富自注意力上下文，解决特征"硬替换"导致的上下文冲突问题；3）提出基于数据驱动的分层分析方法，利用新颖的Guidance Responsiveness Metric，自动选取对特定编辑任务最敏感的DiT块，实现有针对性的指导。

Result: 在广泛的实验中，ContextFlow在多个任务和数据集上显著优于其他训练免费方法，甚至超越了一些最新的训练型方法，展现了更好的时序一致性和视觉保真度。

Conclusion: ContextFlow为Diffusion Transformer在训练免费的视频对象编辑领域提供了创新且有效的解决方案，兼顾了准确性、时序一致性和高保真度，推动了相关领域的发展。

Abstract: Training-free video object editing aims to achieve precise object-level
manipulation, including object insertion, swapping, and deletion. However, it
faces significant challenges in maintaining fidelity and temporal consistency.
Existing methods, often designed for U-Net architectures, suffer from two
primary limitations: inaccurate inversion due to first-order solvers, and
contextual conflicts caused by crude "hard" feature replacement. These issues
are more challenging in Diffusion Transformers (DiTs), where the unsuitability
of prior layer-selection heuristics makes effective guidance challenging. To
address these limitations, we introduce ContextFlow, a novel training-free
framework for DiT-based video object editing. In detail, we first employ a
high-order Rectified Flow solver to establish a robust editing foundation. The
core of our framework is Adaptive Context Enrichment (for specifying what to
edit), a mechanism that addresses contextual conflicts. Instead of replacing
features, it enriches the self-attention context by concatenating Key-Value
pairs from parallel reconstruction and editing paths, empowering the model to
dynamically fuse information. Additionally, to determine where to apply this
enrichment (for specifying where to edit), we propose a systematic, data-driven
analysis to identify task-specific vital layers. Based on a novel Guidance
Responsiveness Metric, our method pinpoints the most influential DiT blocks for
different tasks (e.g., insertion, swapping), enabling targeted and highly
effective guidance. Extensive experiments show that ContextFlow significantly
outperforms existing training-free methods and even surpasses several
state-of-the-art training-based approaches, delivering temporally coherent,
high-fidelity results.

</details>


### [194] [Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology](https://arxiv.org/abs/2509.17847)
*Saghir Alfasly,Wataru Uegami,MD Enamul Hoq,Ghazal Alabtah,H. R. Tizhoosh*

Main category: cs.CV

TL;DR: 本文提出了一种通过潜在扩散模型合成高质量、异质性丰富的病理组织图像的新方法，不依赖文本输入，而直接利用语义分割图和组织区域原始图像，实现了优异的合成质量和下游任务性能，并能适用于无标注大数据集。


<details>
  <summary>Details</summary>
Motivation: 病理图像合成面临异质性保持与详细形态捕捉的挑战，且很多数据集缺乏详细标注，限制了数据获取和下游AI训练。作者致力于开发一种能处理这类挑战、扩展到无标注数据的大规模自动化合成框架。

Method: 作者设计了一种基于潜在扩散的模型，创新性地用语义分割图和对应的组织区域原始图像进行“双重条件”合成。对于有标注数据集，通过设定异质性比例提取图像块；无标注数据集上，引入自监督聚类和基础模型嵌入，自动生成伪语义图训练扩散模型。所有数据均用于训练精准合成模型。

Result: 在多项基准（如Camelyon16, Panda, TCGA）上，合成数据训练的分割模型与真实数据相当，IoU准确率仅差1-2%。合成图像与真实分布的距离（Frechet Distance）较同类方法下降2-6倍。方法成功扩展到超过1万张未标注全切片，合成数据具备高保真度与精确标注。

Conclusion: 本方法突破传统依赖文本或抽象向量合成的限制，有效生成了异质性丰富且可标注的病理图像，显著提高了无标注大规模医学影像数据的利用效率，为计算病理学数据瓶颈提供了可行的自动化解决方案。

Abstract: Synthetic data generation in histopathology faces unique challenges:
preserving tissue heterogeneity, capturing subtle morphological features, and
scaling to unannotated datasets. We present a latent diffusion model that
generates realistic heterogeneous histopathology images through a novel
dual-conditioning approach combining semantic segmentation maps with
tissue-specific visual crops. Unlike existing methods that rely on text prompts
or abstract visual embeddings, our approach preserves critical morphological
details by directly incorporating raw tissue crops from corresponding semantic
regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches
ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we
introduce a self-supervised extension that clusters whole-slide images into 100
tissue types using foundation model embeddings, automatically generating
pseudo-semantic maps for training. Our method synthesizes high-fidelity images
with precise region-wise annotations, achieving superior performance on
downstream segmentation tasks. When evaluated on annotated datasets, models
trained on our synthetic data show competitive performance to those trained on
real data, demonstrating the utility of controlled heterogeneous tissue
generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet
Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower
FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on
synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within
1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA
whole-slide images without manual annotations, our framework offers a practical
solution for an urgent need for generating diverse, annotated histopathology
data, addressing a critical bottleneck in computational pathology.

</details>


### [195] [ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos](https://arxiv.org/abs/2509.17864)
*Shi Chen,Erik Sandström,Sandro Lombardi,Siyuan Li,Martin R. Oswald*

Main category: cs.CV

TL;DR: 该论文提出了一种能实现在线动态3D重建的新方法，解决了以往SLAM和离线方法在动态场景处理和一致性上的问题，支持RGB及RGB-D输入，并在渲染和跟踪表现上达到了业界先进水平。


<details>
  <summary>Details</summary>
Motivation: 目前的SLAM方法对于动态场景往往只选择去除动态部分或强依赖RGB-D输入，缺乏全局一致性与细节表达，离线方法难以扩展到长时间视频，基于transformer的前馈方法也有一致性与细节缺陷，因此亟需一种高效、实用的在线动态重建方案。

Method: 作者设计了一套SLAM系统，将场景的静态与动态部分解耦，利用创新的运动掩膜策略（motion masking）实现强鲁棒性的姿态追踪，并通过逐步自适应的运动支架图（Motion Scaffolds graph）重建动态区域，从而实现了能处理动态场景的高效重建。

Result: 该方法不仅能够生成与离线算法相媲美的高质量新视角渲染效果，还在跟踪精度上达到了当前动态SLAM领域的领先水平。

Conclusion: 论文提出的方法克服了现有技术在处理动态场景和保持全局一致性方面的不足，为动态3D在线重建提供了一个实用且高质量的解决方案。

Abstract: Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.

</details>


### [196] [Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training](https://arxiv.org/abs/2509.17888)
*Divya Mereddy,Marcos Quinones-Grueiro,Ashwin T S,Eduardo Davalos,Gautam Biswas,Kent Etherton,Tyler Davis,Katelyn Kay,Jill Lear,Benjamin Goldberg*

Main category: cs.CV

TL;DR: 本文针对危重病人航空转运小组（CCATT）成员的培训方式进行了创新，提出利用混合现实仿真和多模态数据分析实现更加客观、系统的学员表现评估。


<details>
  <summary>Details</summary>
Motivation: 传统的航运加护团队培训主要依赖主观的导师评分，容易遗漏关键事件与团队协作细节，缺乏一致性和普适性。随着仿真与人工智能技术进步，亟需建立更客观、全面的评估体系，以满足复杂临床救治任务的高标准需求。

Method: 本研究结合认知任务分析（CTA）及多模态学习分析（MMLA），设计出专为CCATT培训定制的、数据驱动的评估框架。具体包括开发领域专属CTA模型，以及基于Cascade Disentangling Network（CDN）的人-物交互识别管道，自动检测和跟踪学员与设备的互动行为，提取关键绩效指标，并映射至CTA模型中以实现专业且可解释的表现评估。

Result: 通过自动化视频分析系统，能够实时量化医护团队成员在模拟飞行环境下的操作反应时间、任务持续时长等关键指标，并以结构化方式反映团队协作和临床应对能力，极大提升了评估的客观性和可重复性。

Conclusion: 基于混合现实与多模态学习分析的培训评估体系，为航运加护团队的临床能力培训提供了一种新范式，有助于提升学员的实际应急水平和团队协作力，也为类似高压医疗环境的智能评估提供了借鉴。

Abstract: This study examines how Critical Care Air Transport Team (CCATT) members are
trained using mixed-reality simulations that replicate the high-pressure
conditions of aeromedical evacuation. Each team - a physician, nurse, and
respiratory therapist - must stabilize severely injured soldiers by managing
ventilators, IV pumps, and suction devices during flight. Proficient
performance requires clinical expertise and cognitive skills, such as
situational awareness, rapid decision-making, effective communication, and
coordinated task management, all of which must be maintained under stress.
Recent advances in simulation and multimodal data analytics enable more
objective and comprehensive performance evaluation. In contrast, traditional
instructor-led assessments are subjective and may overlook critical events,
thereby limiting generalizability and consistency. However, AI-based automated
and more objective evaluation metrics still demand human input to train the AI
algorithms to assess complex team dynamics in the presence of environmental
noise and the need for accurate re-identification in multi-person tracking. To
address these challenges, we introduce a systematic, data-driven assessment
framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning
Analytics (MMLA). We have developed a domain-specific CTA model for CCATT
training and a vision-based action recognition pipeline using a fine-tuned
Human-Object Interaction model, the Cascade Disentangling Network (CDN), to
detect and track trainee-equipment interactions over time. These interactions
automatically yield performance indicators (e.g., reaction time, task
duration), which are mapped onto a hierarchical CTA model tailored to CCATT
operations, enabling interpretable, domain-relevant performance evaluations.

</details>


### [197] [Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](https://arxiv.org/abs/2509.17901)
*Geewook Kim,Minjoon Seo*

Main category: cs.CV

TL;DR: 本文分析了音频在视频大语言模型（Video-LLMs）及其评测基准中的实际作用，发现对于大部分当前基准来说音频作用有限，但在音频敏感任务上仍很重要，并发布了新数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型常声称具备“视频理解”能力，但现有评测往往忽略音频，实际任务中音频信息是否有帮助尚未系统评估。

Method: 基于LLaVA-OneVision架构，加入语音/音频编码器（如Whisper），提出基于Mamba的音频token压缩方法来应对token爆炸，并在不同基准和新构建的音频敏感子集上系统对比实验。

Result: 结果显示，在常见视频基准上，加入音频带来的提升极小，很多任务甚至一帧静图即可解决；但在作者设计的更具挑战性且依赖音频的信息子集上，音频能带来显著帮助。

Conclusion: 音频在主流基准下被低估，现有学术实践和真实世界对音视频理解的期待之间存在显著差距。作者开放了新的高难数据集和模型工具，推动更全面公正的Video-LLM评估与应用。

Abstract: Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.

</details>


### [198] [SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI](https://arxiv.org/abs/2509.17925)
*Yuanhan Wang,Yifei Chen,Shuo Jiang,Wenjing Yu,Mingxuan Liu,Beining Wu,Jinying Zong,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: 本文提出了一种新的域自适应方法SmaRT，实现了MRI脑肿瘤分割在多源数据中的鲁棒性，有效应对扫描仪、协议、群体差异导致的域偏移问题。SmaRT在撒哈拉以南非洲和儿童胶质瘤数据集上表现出优越性能，提升了Dice准确率和边界精度。


<details>
  <summary>Details</summary>
Motivation: MRI脑肿瘤分割对治疗和监测至关重要，但现有模型易受不同设备、协议及人群差异的影响，特别在低资源和儿童病例下，传统自适应技术不稳定且结果欠佳。

Method: 提出SmaRT架构，通过风格感知增强缓解外观差异、双分支动量策略实现伪标签稳定更新，并结合结构先验维持分割结构一致性和连通性，实现无源域的测试时自适应。

Result: 在跨域的撒哈拉以南非洲和儿童脑胶质瘤数据集上，SmaRT在Dice分数和边界精度上均优于现有方法。

Conclusion: SmaRT促进了MRI脑肿瘤分割算法在多样化临床环境中的稳定可靠应用，有助于实现算法公平性和广泛可用性。

Abstract: Reliable brain tumor segmentation in MRI is indispensable for treatment
planning and outcome monitoring, yet models trained on curated benchmarks often
fail under domain shifts arising from scanner and protocol variability as well
as population heterogeneity. Such gaps are especially severe in low-resource
and pediatric cohorts, where conventional test-time or source-free adaptation
strategies often suffer from instability and structural inconsistency. We
propose SmaRT, a style-modulated robust test-time adaptation framework that
enables source-free cross-domain generalization. SmaRT integrates style-aware
augmentation to mitigate appearance discrepancies, a dual-branch momentum
strategy for stable pseudo-label refinement, and structural priors enforcing
consistency, integrity, and connectivity. This synergy ensures both adaptation
stability and anatomical fidelity under extreme domain shifts. Extensive
evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT
consistently outperforms state-of-the-art methods, with notable gains in Dice
accuracy and boundary precision. Overall, SmaRT bridges the gap between
algorithmic advances and equitable clinical applicability, supporting robust
deployment of MRI-based neuro-oncology tools in diverse clinical environments.
Our source code is available at https://github.com/baiyou1234/SmaRT.

</details>


### [199] [Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching](https://arxiv.org/abs/2509.17931)
*Zhuo Xiao,Fugen Zhou,Jingjing Wang,Chongyu He,Bo Liu,Haitao Sun,Zhe Ji,Yuliang Jiang,Junjie Wang,Qiuwen Wu*

Main category: cs.CV

TL;DR: 提出一种新方法，通过检测针头和手柄并进行匹配，实现了更鲁棒和准确的多针定位，显著优于传统分割方法。


<details>
  <summary>Details</summary>
Motivation: 术中CT影像中多针定位精度对盆腔种子植入治疗效果至关重要，但因图像对比度差和针体粘连，导致定位十分困难。

Method: 将针体定位任务转化为针头-手柄的检测与匹配问题。基于HRNet的无锚点神经网络提取多尺度特征，分支回归热图和极坐标角度，实现针头和手柄检测。通过一种贪心匹配-合并（GMM）算法，在约束下解决不均衡分配问题，将检测到的针头和手柄配对重建每根针的三维路径。

Result: 在100例患者数据集上实验，所提方法的精度和F1分数均优于基于nnUNet的分割方法，表现出更强的鲁棒性和准确度。

Conclusion: 新方法能更可靠、准确地实现术中多针定位，具备更强的临床实用性。

Abstract: Accurate multi-needle localization in intraoperative CT images is crucial for
optimizing seed placement in pelvic seed implant brachytherapy. However, this
task is challenging due to poor image contrast and needle adhesion. This paper
presents a novel approach that reframes needle localization as a tip-handle
detection and matching problem to overcome these difficulties. An anchor-free
network, based on HRNet, is proposed to extract multi-scale features and
accurately detect needle tips and handles by predicting their centers and
orientations using decoupled branches for heatmap regression and polar angle
prediction. To associate detected tips and handles into individual needles, a
greedy matching and merging (GMM) method designed to solve the unbalanced
assignment problem with constraints (UAP-C) is presented. The GMM method
iteratively selects the most probable tip-handle pairs and merges them based on
a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100
patients, the proposed method demonstrates superior performance, achieving
higher precision and F1 score compared to a segmentation-based method utilizing
the nnUNet model,thereby offering a more robust and accurate solution for
needle localization in complex clinical scenarios.

</details>


### [200] [Can multimodal representation learning by alignment preserve modality-specific information?](https://arxiv.org/abs/2509.17943)
*Romain Thoreau,Jessie Levillain,Dawa Derksen*

Main category: cs.CV

TL;DR: 本文探讨了多模态遥感数据融合中对特定任务信息的损失问题，并结合理论与实验分析了对比学习等当前主流多模态表征学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于大多数遥感任务标注数据稀缺，现有方法大量依赖自监督和多模态对比学习。这些方法通常假设不同模态间的空间对齐能带来语义对齐，但忽视了各模态独有的信息可能被丢失。

Method: 作者首先通过理论分析，指出在简化假设下，多模态空间对齐（对比学习）可能本质上导致任务信息丢失。随后，通过在更真实场景下的数值实验，验证该信息损失确实发生。

Result: 结果显示，当前的跨模态对齐方法在促进不同模态共享信息的同时，会不可避免地损失每个模态独有、对下游任务有用的信息。

Conclusion: 本文的理论和实证分析表明，现有多模态对比学习方法存在固有的信息损失问题，需要发展新的方法更好地保留各模态独特的、有用的信息，以提升遥感多模态数据融合的表现。

Abstract: Combining multimodal data is a key issue in a wide range of machine learning
tasks, including many remote sensing problems. In Earth observation, early
multimodal data fusion methods were based on specific neural network
architectures and supervised learning. Ever since, the scarcity of labeled data
has motivated self-supervised learning techniques. State-of-the-art multimodal
representation learning techniques leverage the spatial alignment between
satellite data from different modalities acquired over the same geographic area
in order to foster a semantic alignment in the latent space. In this paper, we
investigate how this methods can preserve task-relevant information that is not
shared across modalities. First, we show, under simplifying assumptions, when
alignment strategies fundamentally lead to an information loss. Then, we
support our theoretical insight through numerical experiments in more realistic
settings. With those theoretical and empirical evidences, we hope to support
new developments in contrastive learning for the combination of multimodal
satellite data. Our code and data is publicly available at
https://github.com/Romain3Ch216/alg_maclean_25.

</details>


### [201] [DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels](https://arxiv.org/abs/2509.17951)
*Kai Li,Xingxing Weng,Yupeng Deng,Yu Meng,Chao Pang,Gui-Song Xia,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（DragOSM），可自动将历史OpenStreetMap标签与遥感影像中的建筑物屋顶和基底对齐，有效纠正因时间和视角导致的位置错位。


<details>
  <summary>Details</summary>
Motivation: 遥感影像中提取多边形屋顶和基底对于城市分析很重要，但传统分割方法在俯视角度偏离（off-nadir）的影像中表现不佳。虽然有大量开放矢量地图数据可用（如OSM），但其历史标签常常与新影像存在较大位置误差，且只能标注屋顶或基底，难以准确表达建筑结构。

Method: 作者提出了“对齐标记”（alignment token）概念，用于引导历史标签的纠正，并基于此设计了DragOSM模型，将标签对齐问题建模为交互式去噪过程，通过引入高斯干扰模拟标签错位。在训练阶段，模型学习如何修正位置信息；推理阶段，模型迭代优化历史标签的空间位置。

Result: 作者构建了包含OSM原始标签和人工校正标签的新数据集ReBO，覆盖41座城市的5473张影像和179,265栋建筑。实验证明DragOSM方法能有效提升标签对齐准确率，优于传统方法。

Conclusion: DragOSM有效解决了历史矢量标签与遥感影像在屋顶和基底对齐上的错位难题，能够为城市分析等下游任务提供更准确的建筑物多边形数据，数据集和代码已公开。

Abstract: Extracting polygonal roofs and footprints from remote sensing images is
critical for large-scale urban analysis. Most existing methods rely on
segmentation-based models that assume clear semantic boundaries of roofs, but
these approaches struggle in off- nadir images, where the roof and footprint
are significantly displaced, and facade pixels are fused with the roof
boundary. With the increasing availability of open vector map annotations,
e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation
has become viable because remote sensing images are georeferenced once
captured. However, these historical labels commonly suffer from significant
positional discrepancies with new images and only have one annotation (roof or
footprint), which fails to describe the correct structures of a building. To
address these discrepancies, we first introduce a concept of an alignment
token, which encodes the correction vector to guide the label correction. Based
on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel
model designed to align dislocated historical labels with roofs and footprints.
Specifically, DragOSM formulates the label alignment as an interactive
denoising process, modeling the positional discrepancy as a Gaussian
distribution. During training, it learns to correct these errors by simulating
misalignment with random Gaussian perturbations; during inference, it
iteratively refines the positions of input labels. To validate our method, we
further present a new dataset, Repairing Buildings in OSM (ReBO), comprising
179,265 buildings with both OpenStreetMap and manually corrected annotations
across 5,473 images from 41 cities. Experimental results on ReBO demonstrate
the effectiveness of DragOSM. Code, dataset, and trained models are publicly
available at https://github.com/likaiucas/DragOSM.git.

</details>


### [202] [Breaking the Discretization Barrier of Continuous Physics Simulation Learning](https://arxiv.org/abs/2509.17955)
*Fan Xu,Hao Wu,Nan Wang,Lilan Peng,Kun Wang,Wei Gong,Xibin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoPS的数据驱动方法，用于从局部、稀疏观测建模连续的物理动态过程，并在不同科学与工程场景下取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 物理系统动态具有高度非线性且时空变化复杂，多数实际观测数据稀疏且分布无序，现有方法在应对连续物理建模时受到固定空间和时间离散化的限制。研究者尝试方法虽多，但往往依赖传统数值或未真正突破连续建模瓶颈。

Method: 本文提出CoPS方法，完全基于数据驱动。其核心包括：1）利用乘性滤波网络融合并编码空间信息与观测值；2）自定义几何网格，并通过消息传递机制将特征从原始空间映射到网格上；3）通过多尺度图微分方程（ODEs）建模连续时间动力学；4）引入基于马尔可夫的神经自校正模块，辅助且约束模型的连续外推能力。

Result: 大量实验表明，CoPS在多个包含复杂连续性需求的场景下，空间-时间建模效果均超越当前最新方法。

Conclusion: CoPS为从稀疏部分观测出发的连续物理建模提供了有效的新范式，无需依赖物理领域先验及传统离散数值求解，具有广泛应用潜力。

Abstract: The modeling of complicated time-evolving physical dynamics from partial
observations is a long-standing challenge. Particularly, observations can be
sparsely distributed in a seemingly random or unstructured manner, making it
difficult to capture highly nonlinear features in a variety of scientific and
engineering problems. However, existing data-driven approaches are often
constrained by fixed spatial and temporal discretization. While some
researchers attempt to achieve spatio-temporal continuity by designing novel
strategies, they either overly rely on traditional numerical methods or fail to
truly overcome the limitations imposed by discretization. To address these, we
propose CoPS, a purely data-driven methods, to effectively model continuous
physics simulation from partial observations. Specifically, we employ
multiplicative filter network to fuse and encode spatial information with the
corresponding observations. Then we customize geometric grids and use
message-passing mechanism to map features from original spatial domain to the
customized grids. Subsequently, CoPS models continuous-time dynamics by
designing multi-scale graph ODEs, while introducing a Markov-based neural
auto-correction module to assist and constrain the continuous extrapolations.
Comprehensive experiments demonstrate that CoPS advances the state-of-the-art
methods in space-time continuous modeling across various scenarios.

</details>


### [203] [Visual Detector Compression via Location-Aware Discriminant Analysis](https://arxiv.org/abs/2509.17968)
*Qizhen Lan,Jung Im Choi,Qing Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于检测判别性的神经网络主动压缩方法，用于提升深度视觉检测器在边缘设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络压缩（精简）技术多关注于分类网络，对检测网络的关注不足，而且未充分利用检测中特有的位置信息，并且大部分方法依赖预训练模型，难以有效区分并移除无用成分。

Method: 本文方法分两步：第一步最大化并压缩与检测相关的判别性特征，将其对齐到检测头前的一部分神经元/卷积核上；第二步追踪检测判别力在各层的传播，过滤掉重要性较低的特征。两步都充分挖掘了目标位置信息。

Result: 在KITTI和COCO数据集上，通过四种先进检测模型及与四种主流方法对比，实验结果表明本文方法大幅压缩模型复杂度的同时，性能甚至超越原始基线模型。

Conclusion: 该方法显著提升了检测模型的压缩效果与部署潜力，是视觉检测模型高效落地的重要技术突破。

Abstract: Deep neural networks are powerful, yet their high complexity greatly limits
their potential to be deployed on billions of resource-constrained edge
devices. Pruning is a crucial network compression technique, yet most existing
methods focus on classification models, with limited attention to detection.
Even among those addressing detection, there is a lack of utilization of
essential localization information. Also, many pruning methods passively rely
on pre-trained models, in which useful and useless components are intertwined,
making it difficult to remove the latter without harming the former at the
neuron/filter level. To address the above issues, in this paper, we propose a
proactive detection-discriminants-based network compression approach for deep
visual detectors, which alternates between two steps: (1) maximizing and
compressing detection-related discriminants and aligning them with a subset of
neurons/filters immediately before the detection head, and (2) tracing the
detection-related discriminating power across the layers and discarding
features of lower importance. Object location information is exploited in both
steps. Extensive experiments, employing four advanced detection models and four
state-of-the-art competing methods on the KITTI and COCO datasets, highlight
the superiority of our approach. Remarkably, our compressed models can even
beat the original base models with a substantial reduction in complexity.

</details>


### [204] [StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models](https://arxiv.org/abs/2509.17993)
*Haoxin Yang,Bangzhen Liu,Xuemiao Xu,Cheng Xu,Yuyang Yu,Zikai Huang,Yi Wang,Shengfeng He*

Main category: cs.CV

TL;DR: 该文提出了StableGuard，一种创新性地将二值水印无缝嵌入扩散模型生成过程中的方法，实现对AI生成图像的版权保护和篡改定位。通过新颖架构和联合优化，这一方法优于现有主流技术。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的AI内容越来越真实，但由此带来的内容滥用问题（如盗版、篡改、虚假图像等）也愈发突出，急需一种能有效结合版权保护和篡改检测的解决方案。现有方法多为后处理方式，实用性与取证可靠性不足。

Method: 作者提出StableGuard框架，在扩散模型生成流程中直接嵌入水印。具体包括：(1) 利用预训练VAE加上轻量残差适配器（MPW-VAE），生成带水印与无水印配对图片，通过遮罩融合构造多样篡改训练集；(2) 提出由多专家指导的法证网络（MoE-GFN），主动融合整体水印、局部篡改特征和频域信息，实现更精确的水印验证和篡改区域定位；两者联合、端到端自监督优化。

Result: 实验表明，StableGuard相较于当前主流方法，在图像质量（保真度）、水印验证准确性以及篡改定位精度上均有显著提升。

Conclusion: StableGuard实现了针对扩散模型生成内容的高效、鲁棒版权保护和篡改检测，为AI图像安全及合规使用提供了重要技术手段，具有广阔应用前景。

Abstract: The advancement of diffusion models has enhanced the realism of AI-generated
content but also raised concerns about misuse, necessitating robust copyright
protection and tampering localization. Although recent methods have made
progress toward unified solutions, their reliance on post hoc processing
introduces considerable application inconvenience and compromises forensic
reliability. We propose StableGuard, a novel framework that seamlessly
integrates a binary watermark into the diffusion generation process, ensuring
copyright protection and tampering localization in Latent Diffusion Models
through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)
by equipping a pretrained Variational Autoencoder (VAE) with a lightweight
latent residual-based adapter, enabling the generation of paired watermarked
and watermark-free images. These pairs, fused via random masks, create a
diverse dataset for training a tampering-agnostic forensic network. To further
enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic
Network (MoE-GFN) that dynamically integrates holistic watermark patterns,
local tampering traces, and frequency-domain cues for precise watermark
verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly
optimized in a self-supervised, end-to-end manner, fostering a reciprocal
training between watermark embedding and forensic accuracy. Extensive
experiments demonstrate that StableGuard consistently outperforms
state-of-the-art methods in image fidelity, watermark verification, and
tampering localization.

</details>


### [205] [Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs](https://arxiv.org/abs/2509.18015)
*Advait Gosai,Arun Kavishwar,Stephanie L. McNamara,Soujanya Samineni,Renato Umeton,Alexander Chowdhury,William Lotter*

Main category: cs.CV

TL;DR: 该论文系统评估了通用与专用多模态大模型在胸部X光图像定位病灶方面的能力，发现目前模型定位准确率仍落后于任务专用模型和专业医生。


<details>
  <summary>Details</summary>
Motivation: 近年来，大模型（LLMs）及其多模态变体在医学问答和诊断任务中表现出色。然而，医学图像解读除诊断外，定位病灶同样至关重要，目前相关评估较少。该研究意在填补通用与专用MLLMs在医学图像定位任务评测上的空白。

Method: 选取GPT-4、GPT-5（通用型）和MedGemma（专用型）三种模型，在CheXlocalize胸部影像病灶定位数据集上评测，通过空间网格提示，获取坐标定位预测，并与CNN基线模型和放射科医生结果进行对比。

Result: 在九类病灶定位任务中，GPT-5准确率为49.7%，GPT-4为39.1%，MedGemma为17.7%，均低于CNN基线（59.9%）与放射科医生（80.1%）。GPT-5定位大多在解剖学合理区域但精度不足，GPT-4更适合解剖位置固定的病灶，MedGemma泛化能力有限。

Conclusion: 目前MLLMs在医学图像定位中的表现仍有限，普遍存在误差。应结合任务特定工具提升医学图像处理的可靠性。

Abstract: Recent work has shown promising performance of frontier large language models
(LLMs) and their multimodal counterparts in medical quizzes and diagnostic
tasks, highlighting their potential for broad clinical utility given their
accessible, general-purpose nature. However, beyond diagnosis, a fundamental
aspect of medical image interpretation is the ability to localize pathological
findings. Evaluating localization not only has clinical and educational
relevance but also provides insight into a model's spatial understanding of
anatomy and disease. Here, we systematically assess two general-purpose MLLMs
(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to
localize pathologies on chest radiographs, using a prompting pipeline that
overlays a spatial grid and elicits coordinate-based predictions. Averaged
across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a
localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),
all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark
(80.1%). Despite modest performance, error analysis revealed that GPT-5's
predictions were largely in anatomically plausible regions, just not always
precisely localized. GPT-4 performed well on pathologies with fixed anatomical
locations, but struggled with spatially variable findings and exhibited
anatomically implausible predictions more frequently. MedGemma demonstrated the
lowest performance on all pathologies, showing limited capacity to generalize
to this novel task. Our findings highlight both the promise and limitations of
current MLLMs in medical imaging and underscore the importance of integrating
them with task-specific tools for reliable use.

</details>


### [206] [NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning](https://arxiv.org/abs/2509.18041)
*Sahil Shah,S P Sharan,Harsh Goel,Minkyu Choi,Mustafa Munir,Manvik Pasula,Radu Marculescu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 本文针对长视频问答（LVQA）任务提出了NeuS-QA方法，利用神经-符号混合推理，显著提升了涉及事件时序、因果和多步骤推理的问题的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）虽适用于静态图片或短视频，但难以应对长视频复杂多事件、多步骤推理场景。传统做法的帧采样方式容易遗漏关键信息，无法保证对问题中涉及的事件顺序和因果逻辑做出正确推理。因此需要新方法提升LVQA任务在因果与组合推理场景下的可靠性和解释性。

Method: 作者提出了无训练、即插即用的神经-符号混合推理框架NeuS-QA。流程为：首先将自然语言问题转化为形式化时序逻辑表达式，然后基于帧级语义构建视频自动机，并用模型检验精确找出满足逻辑要求的视频片段。仅将通过逻辑验证的片段送入VLM，提升内容相关性与推理能力。

Result: 在LongVideoBench和CinePile数据集上，NeuS-QA相比基线方法提升超10%，尤其擅长回答涉及事件顺序、因果、和多步骤推理的问题。

Conclusion: NeuS-QA有效缓解了长视频问答中信息遗漏和推理可靠性问题，在无需修改或微调现有VLM前提下，提升了推理能力、可解释性，并减少了幻觉现象。

Abstract: Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.

</details>


### [207] [TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs](https://arxiv.org/abs/2509.18056)
*Yunheng Li,Jing Cheng,Shaoyong Jia,Hangyi Kuang,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了TempSamp-R1，一种针对多模态大模型（MLLM）的视频时间定位任务的强化学习微调新框架，通过引入基于真值的异策略采样和奖励函数改进，实现了比现有方法更高的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法（如GRPO）在应对大规模视频时间搜索空间时，依赖于“on-policy”策略采样，容易泄漏效率且难以获得精确的时序定位答案，因此存在性能瓶颈。

Method: TempSamp-R1采用异策略学习方式，将标注的时序真值用作监督信号，补偿on-policy样本的稀疏性和不准性；引入非线性软优势计算动态重塑奖励，降低基于奖励的更新的方差和波动；配合链式思维（CoT）训练范式，实现支持CoT与非CoT推理的统一模型。

Result: TempSamp-R1在多个主流基准数据集（Charades-STA、ActivityNet Captions、QVHighlights）上均大幅优于GRPO等基线模型，达到了新的SOTA；在小样本场景下也表现出强的泛化能力。

Conclusion: TempSamp-R1通过结合异策略真值监督与奖励机制改进，显著提升了MLLM在视频时序定位任务的表现，同时增强了模型的推理与泛化能力，为后续相关研究提供了新的思路和有效工具。

Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1

</details>


### [208] [GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer](https://arxiv.org/abs/2509.18081)
*Md. Mahmudul Hasan,Ahmed Nesar Tahsin Choudhury,Mahmudul Hasan,Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: 本文提出了一种名为GraDeT-HTR的高效孟加拉文手写文本识别系统，通过引入基于字位（grapheme）的解码器Transformer模型，显著提升了识别准确率，并在多个基准数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉文作为世界第六大语言，现有的手写文本识别系统发展较为滞后，主要由于其字母结构复杂、书写风格多变及标注数据缺乏。该论文旨在解决孟加拉文手写识别中的这些特殊挑战。

Method: 作者提出了一种基于解码器的Transformer架构，并引入了字位级分词器以替代传统子词分词方式。模型先在大规模合成数据上进行预训练，再在真实人工标注数据上进行微调。

Result: 新提出的方法在多个孟加拉文手写文本识别基准数据集上取得了当前最优性能，识别准确率明显优于传统分词方法。

Conclusion: 文中方法有效解决了孟加拉文手写识别中的难题，为资源受限条件下的孟加拉文HTR系统开发提供了新思路。

Abstract: Despite Bengali being the sixth most spoken language in the world,
handwritten text recognition (HTR) systems for Bengali remain severely
underdeveloped. The complexity of Bengali script--featuring conjuncts,
diacritics, and highly variable handwriting styles--combined with a scarcity of
annotated datasets makes this task particularly challenging. We present
GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system
based on a Grapheme-aware Decoder-only Transformer architecture. To address the
unique challenges of Bengali script, we augment the performance of a
decoder-only transformer by integrating a grapheme-based tokenizer and
demonstrate that it significantly improves recognition accuracy compared to
conventional subword tokenizers. Our model is pretrained on large-scale
synthetic data and fine-tuned on real human-annotated samples, achieving
state-of-the-art performance on multiple benchmark datasets.

</details>


### [209] [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](https://arxiv.org/abs/2509.18090)
*Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu*

Main category: cs.CV

TL;DR: 本文提出了一种基于稀疏体素的新型Surface reconstruction方法GeoSVR，通过引入体素不确定性深度约束和体素面正则化策略，在保持高效的前提下，显著提升了重建几何的准确性、细节和完整性，优于现有基于高斯投影的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯投影的方法在表面重建精度和细节表达上受到了表现瓶颈的限制，需要新的方法突破表面几何的准确描绘与细节还原。

Method: 提出了GeoSVR，一种显式的稀疏体素方法。具体方法包括：(1)引入体素不确定性深度约束，结合单目深度信息提升场景收敛性并避免质量下降；(2)设计体素面正则化，提升微小体素的几何一致性，实现锐利准确的表面表达。

Result: 实验结果表明，GeoSVR在多种具有挑战性的场景下，在几何精度、细节保留和完整性上均优于现有方法，并且保持了较高的效率。

Conclusion: GeoSVR充分挖掘了稀疏体素用于高质量表面重建的潜力，解决了现有方法的局限，在表面几何准确性、细节还原和计算效率之间取得了良好平衡。

Abstract: Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.

</details>


### [210] [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation](https://arxiv.org/abs/2509.18092)
*Guocheng Gordon Qian,Daniil Ostashev,Egor Nemchinov,Avihay Assouline,Sergey Tulyakov,Kuan-Chieh Jackson Wang,Kfir Aberman*

Main category: cs.CV

TL;DR: 本文提出了一种新的人像生成方法，通过对不同属性（如发型、服饰、身份）分别引用参考图像，实现对人像的高保真、细致可控合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽重视身份保持，但缺乏模块性和对具体属性的解耦控制，难以灵活操控如发型、服装等外观细节。

Method: 提出属性特定的图像提示机制，用不同参考图像分别引导头发、服饰、身份等属性生成，并将这些信息编码成属性特定的token注入预训练的文本到图像扩散模型。同时设计跨属性参考训练策略和多样化姿态人像数据集，提升组合和解耦能力。

Result: 实验表明，该方法在遵循视觉和文本提示方面性能优于当前主流方法，实现了多个人在同一图像内多属性的组合和可控生成。

Conclusion: 本文框架将视觉提示与文本生成结合，实现了更灵活可控的人像合成，为个性化图像生成带来新的可能。

Abstract: Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.

</details>


### [211] [UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning](https://arxiv.org/abs/2509.18094)
*Ye Liu,Zongyang Ma,Junfu Pu,Zhongang Qi,Yang Wu,Ying Shan,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出了UniPixel，一个能够实现细粒度像素级视觉-语言对齐和推理的大型多模态模型。该模型结合了视觉提示输入和掩码生成，支持像素级推理，并在多个基准数据集上展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型（LMMs）主要聚焦于整体图像/视频的语言理解，缺乏对像素级精细对齐和推理能力的关注。而以往研究大多将定位或分割任务单独处理，无法实现细粒度感知与视觉推理的统一。为此，作者希望设计一种能够整合像素级感知与推理能力的多模态模型。

Method: 提出UniPixel模型，其能够处理视觉提示输入，按需生成相关掩码，并在推理时将这些掩码作为中间指针，完成后续推理任务，最终实现像素级推理能力。模型不仅能在图像和视频的像素级指代/分割和物体中心理解等多种任务中使用，还设计了PixelQA任务，联合测试指代、分割和问答等功能。

Result: UniPixel在10个不同的基准数据集和任务上进行了评测，包括像素级指代、分割以及物体中心理解等，均展现出优异的表现。尤其在新设计的PixelQA联合任务中，表现出了方法的通用性和灵活性。

Conclusion: UniPixel实现了像素级视觉-语言对齐和推理的无缝集成，突破了现有LMMs在细粒度视觉理解上的不足。其灵活的掩码生成和推理机制，为多模态模型拓展到高精度视觉感知提供了新思路。

Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.

</details>


### [212] [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](https://arxiv.org/abs/2509.18096)
*Chaehyun Kim,Heeseong Shin,Eunbeen Hong,Heeji Yoon,Anurag Arnab,Paul Hongsuck Seo,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新框架Seg4Diff，用于系统分析多模态扩散变换器（MM-DiT）中的注意力结构，揭示了特定网络层能够自然产生高质量的语义分割并将语义信息从文本有效传递到图像。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的文本生成图像方法虽然能通过跨模态注意力机制隐式关联文本与图像，但其内部注意力如何贡献于语义对齐尚不清楚。理解这一点对于提升模型在分割与生成双任务上的表现十分关键。

Method: 作者设计了Seg4Diff框架，对MM-DiT模型的注意力结构进行系统性分析，定位传播文本到图像语义信息的关键层，并结合少量带分割标注数据的轻量微调，增强这些层的分割能力。

Result: Seg4Diff发现MM-DiT中的语义对齐专家层能够稳定地将文本token与空间一致的图像区域对应，自然输出高质量分割掩码。微调后，分割能力与图像生成质量进一步提升。

Conclusion: 扩散变换器中语义分组能力是涌现属性，可以通过精细调优显著提升模型的分割与生成性能，有望实现视觉感知与生成统一的多能模型。

Abstract: Text-to-image diffusion models excel at translating language prompts into
photorealistic images by implicitly grounding textual concepts through their
cross-modal attention mechanisms. Recent multi-modal diffusion transformers
extend this by introducing joint self-attention over concatenated image and
text tokens, enabling richer and more scalable cross-modal alignment. However,
a detailed understanding of how and where these attention maps contribute to
image generation remains limited. In this paper, we introduce Seg4Diff
(Segmentation for Diffusion), a systematic framework for analyzing the
attention structures of MM-DiT, with a focus on how specific layers propagate
semantic information from text to image. Through comprehensive analysis, we
identify a semantic grounding expert layer, a specific MM-DiT block that
consistently aligns text tokens with spatially coherent image regions,
naturally producing high-quality semantic segmentation masks. We further
demonstrate that applying a lightweight fine-tuning scheme with mask-annotated
image data enhances the semantic grouping capabilities of these layers and
thereby improves both segmentation performance and generated image fidelity.
Our findings demonstrate that semantic grouping is an emergent property of
diffusion transformers and can be selectively amplified to advance both
segmentation and generation performance, paving the way for unified models that
bridge visual perception and generation.

</details>


### [213] [Preconditioned Deformation Grids](https://arxiv.org/abs/2509.18097)
*Julian Kaltheuner,Alexander Oebel,Hannah Droege,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，用于从点云序列动态重建物体表面，相比现有技术有更好的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有动态表面重建方法要么依赖繁琐的正则项，要么需要大量训练数据，导致准确率降低、过度平滑或泛化能力差。为解决这些问题，需要开发更灵活、无需显式配对的新方法。

Method: 作者提出了一种预条件变形网格（Preconditioned Deformation Grids），通过多分辨率体素网格捕获不同空间尺度的运动，以灵活地表示变形。方法融合了基于网格的Sobolev预条件进梯度优化，并采用Chamfer损失指导点云和模板网格变形，无需显式点对应。为保证表面时序一致性，还加入了弱等距损失项作为辅助。

Result: 在大量实验中，该方法在长序列下的重建效果优于现有主流技术，表现出更高的准确性和更好的泛化能力。

Conclusion: 预条件变形网格为无明确对应关系的点云序列动态重建提供了一种高效、准确且兼具灵活性的方案，尤其适合长序列的动态物体表面重建。

Abstract: Dynamic surface reconstruction of objects from point cloud sequences is a
challenging field in computer graphics. Existing approaches either require
multiple regularization terms or extensive training data which, however, lead
to compromises in reconstruction accuracy as well as over-smoothing or poor
generalization to unseen objects and motions. To address these lim- itations,
we introduce Preconditioned Deformation Grids, a novel technique for estimating
coherent deformation fields directly from unstructured point cloud sequences
without requiring or forming explicit correspondences. Key to our approach is
the use of multi-resolution voxel grids that capture the overall motion at
varying spatial scales, enabling a more flexible deformation representation. In
conjunction with incorporating grid-based Sobolev preconditioning into
gradient-based optimization, we show that applying a Chamfer loss between the
input point clouds as well as to an evolving template mesh is sufficient to
obtain accurate deformations. To ensure temporal consistency along the object
surface, we include a weak isometry loss on mesh edges which complements the
main objective without constraining deformation fidelity. Extensive evaluations
demonstrate that our method achieves superior results, particularly for long
sequences, compared to state-of-the-art techniques.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [214] [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
*Brian S. Lin,Jiaxin Yuan,Zihan Zhou,Shouli Wang,Shuo Wang,Cunliang Kong,Qi Shi,Yuxuan Li,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本论文提出LLM在类科学环境下超越方程式的归纳推理任务与专用基准集SIRBench-V1，实验证明现有模型仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM归纳推理研究，往往依赖于可用显式数学方程表达的规则，难以应对现实世界中复杂、无明确方程表达的科学推理任务。因此，亟需设计更贴近真实科学发现过程的LLM归纳推理任务。

Method: 作者提出了一个新任务：科学归纳推理超越方程（LLM-Based Scientific Inductive Reasoning Beyond Equations），并开发了相应新的评测基准SIRBench-V1，对LLM在科学场景中的归纳推理能力进行系统评估。通过对主流LLM模型在该任务上的表现进行实证分析。

Result: 实验表明，现有的大型语言模型在此基准任务中表现不佳，尚不能有效从有限示例中归纳出科学环境下的潜在规则。

Conclusion: 该研究表明LLM在超越方程的科学归纳推理领域还有很大提升空间，需要进一步研究来加强这方面能力。

Abstract: As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.

</details>


### [215] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言模型的零样本学习与数学推理结合的方法，显著提升了复杂大学数学题目的自动求解准确率，达到了90.15%。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能方法在解决包括MIT、哥伦比亚大学课程以及MATH数据集中的复杂高等数学题目时，表现一直不佳，亟需新的高效方法。

Method: 方法结合了零样本学习、数学推理，以及程序合成技术，既减少了对大规模训练数据的依赖，又提升了解题能力。其创新之处在于基于语言模型进行推理，并能够自动生成题目解答过程。

Result: 提出的方法在复杂大学数学题目的求解上取得了90.15%的准确率，远超既有的81%基线成绩。

Conclusion: 高级AI与程序合成方法在复杂数学问题求解方面前景广阔，可有效攻克高难度数学习题。

Abstract: The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


### [216] [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
*Asiya Ibrahim Zanga,Salisu Mamman Abdulrahman,Abubakar Ado,Abdulkadir Abubakar Bichi,Lukman Aliyu Jibril,Abdulmajid Babangida Umar,Alhassan Adamu,Shamsuddeen Hassan Muhammad,Bashir Salisu Abubakar*

Main category: cs.CL

TL;DR: 本文介绍了HausaMovieReview数据集，这是一个包含5000条豪萨语及英语夹杂YouTube评论的新型情感分析基准数据集。作者对比了传统机器学习模型与深度学习Transformer模型的表现，发现针对低资源场景，经过特征工程优化的传统模型能取得更优甚至前沿的效果。


<details>
  <summary>Details</summary>
Motivation: 受制于低资源语言缺乏标注数据，NLP工具开发受限，本文为推进豪萨语情感分析研究，弥补训练资源短板，构建高质量标注数据集，并评估不同分类器效果。

Method: 作者收集了5000条豪萨语及夹杂英文的YouTube影评评论，由三位独立标注者完成标注，验证一致性后，利用经典机器学习（Logistic回归、决策树、KNN）和BERT/RoBERTa等Transformer模型进行性能对比。

Result: 实验发现，决策树在准确率和F1值（89.72%、89.60%）上均优于BERT、RoBERTa等深度学习模型。

Conclusion: 对于低资源语言，通过精心特征工程处理，传统方法依然能达到甚至超越主流深度学习模型，相关数据集和基线为后续NLP研究提供了坚实基础。

Abstract: The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis

</details>


### [217] [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
*Wenjie Lin,Hange Liu,Xutao Mao,Yingying Zhuang,Jingwei Shi,Xudong Han,Tianyu Shi,Jinrui Yang*

Main category: cs.CL

TL;DR: ParlAI Vote是一个用于分析欧洲议会辩论与表决，并评估大语言模型（LLM）在投票预测与偏见分析方面表现的交互式系统。


<details>
  <summary>Details</summary>
Motivation: 该系统旨在降低政治分析中数据获取、模型测试和分析的门槛，同时揭示LLM在面对性别、年龄、国家等群体属性时可能存在的性能偏差，推动对自动化立法决策辅助工具的科研与应用。

Method: ParlAI Vote将辩论主题、演讲内容、点名表决结果及丰富的人口统计数据（性别、年龄、国家、政党团体等）进行统一整合。用户可浏览、对比真实投票结果与LLM预测，并按人口属性细分分析错误情况。系统提供可视化界面，并支持EuroParlVote数据集的性别分类和投票预测核心任务。

Result: 系统揭示了现有最先进LLM在投票预测与性别分类等任务中的系统性性能偏差，并通过可视化工具使偏差具体、易于分析。

Conclusion: ParlAI Vote统一了数据、模型和可视化分析流程，为政治大数据研究、教育和社会公众参与立法分析提供了强有力的工具，同时突显了当前LLM在政治分析领域的优势与局限。

Abstract: We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.

</details>


### [218] [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
*Alok N. Shah,Khush Gupta,Keshav Ramji,Pratik Chaudhari*

Main category: cs.CL

TL;DR: 本文提出在语言模型预训练中引入“meta-tokens”及新的meta-attention机制，提升模型对长距离依赖的建模能力，并在合成任务上取得优秀表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的语言模型在多任务泛化方面表现突出，但在捕捉长距离上下文依赖方面存在困难。本研究旨在解决这一问题，提升模型处理长文本的能力。

Method: 作者提出在GPT-2改进架构中引入meta-tokens，同时增加meta-attention机制，引导模型在预训练时关注这些特殊标记。通过信息理论分析与模型内部可视化，分析其带来的变化。

Result: 实验证明，即使在使用少于100B token进行数据高效的预训练情况下，利用meta-tokens和meta-attention机制的语言模型在合成任务微调后表现优异。模型在上下文窗口长度两倍的情况下依旧能够泛化，且meta-tokens有效作为可训练、内容相关的“地标”进行上下文压缩和缓存。

Conclusion: 在预训练时引入meta-tokens是一种简单且数据高效的方法，可大幅提升语言模型对长上下文的建模性能，同时为理解模型的泛化能力提供了新见解。

Abstract: While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.

</details>


### [219] [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
*Andrew Zhu,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本论文提出并系统分析了一种新的人机交互范式——“窃听型”大语言模型代理（overhearing agents），其能在不中断对话的情况下，为用户提供背景辅助。作者建立了交互及任务的分类法，并提出了开发最佳实践和未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型代理需要直接吸引用户注意、通过对话界面完成任务，存在打断用户自然交互流程的问题。作者希望探索一种更为自然、不打扰用户注意力的AI交互新范式，即“窃听型”代理，提升AI助理在实际场景下的辅助效率和体验。

Method: 作者梳理了现有大语言模型代理相关研究与人机交互实验，总结出“窃听型”代理的概念特征，建立了窃听型代理的交互及任务类型分类，并基于文献和案例调研，总结了开发此类系统的最佳实践。最后，指出当前该领域的研究空白并提出未来机会。

Result: 本文首次对“窃听型”大语言模型代理进行了系统性梳理和分析，建立了明确的任务及交互类型分类法，总结了前人工作基础上的开发和设计经验，填补了该类系统研究的空白。

Conclusion: 窃听型代理为智能助理在医疗、教育、办公等场景下的无干扰、自动化辅助提供了新范式。论文为后续研究提供了系统框架、分类、开发指导和理论基础，并指出了后续研究可拓展的方向。

Abstract: Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.

</details>


### [220] [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
*Yunsoo Kim,Michal W. S. Ong,Alex Shavick,Honghan Wu,Adam P. Levine*

Main category: cs.CL

TL;DR: 本论文提出了一种新的组织病理学自动报告评估方法HARE，能更好地评价自动生成的医学报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动文本生成方法在医学领域应用广泛，但缺乏评估自动生成组织病理学报告临床质量的有效手段，尤其缺乏领域专属的基准和指标。

Method: 作者构建了HARE框架，包括构建标注有实体与关系的基准数据集、开发基于GatorTronS微调的命名实体识别（NER）与关系抽取（RE）模型，以及设计一种新颖的关注重要医疗信息的对齐评估指标。

Result: HARE-NER与HARE-RE模型获得了最高F1分数0.915。HARE评估指标与业界常用ROUGE、Meteor及放射学指标等相比，对人工专家评分的相关性更高，回归性能更好。

Conclusion: HARE框架和相关数据集、模型已开源，有助于推动组织病理学报告生成领域的发展，提升自动生成报告的临床质量。

Abstract: Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.

</details>


### [221] [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
*Weikang Qiu,Tinglin Huang,Ryan Rullo,Yucheng Kuang,Ali Maatouk,S. Raquel Ramos,Rex Ying*

Main category: cs.CL

TL;DR: 本论文提出了RephQA基准，用于评估大语言模型（LLM）在公共健康问答中的可读性，并测试了25个LLM，结果显示大多数模型的可读性不达标。作者还探索了多种提升可读性的策略，发现token-adapted GRPO方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要关注准确性和推理能力，但忽视了对非医学背景人群的易懂回答能力。公共健康领域亟需能够简明、清晰表达专业内容的AI系统。

Method: 作者构建了RephQA基准，包含经过专家审核的问答对、信息性多选任务和两类可读性指标。评估了25个LLM，并尝试了标准prompt、思维链提示、Group Relative Policy Optimization（GRPO）及其token-adapted变体等提升可读性的策略。

Result: 大多数LLM在可读性指标上未达标。Token-adapted GRPO策略显著提升了可读性，表现优于其他方法。

Conclusion: 现有LLM在公共健康问答中存在有效沟通的短板，token-adapted GRPO代表了提升可读性的有效新途径，有助于开发更实用且用户友好的公共健康智能体。

Abstract: Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.

</details>


### [222] [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
*Cihan Xiao,Matthew Wiesner,Debashish Chakraborty,Reno Kriz,Keith Cunningham,Kenton Murray,Kevin Duh,Luis Tavarez-Arce,Paul McNamee,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: 该论文提出了Whisper-UT框架，通过轻量级的适配器实现编码-解码模型在多种单/多模态任务中的高效适应，尤其提升了多模态机器翻译和语音翻译表现。


<details>
  <summary>Details</summary>
Motivation: 现有的编码-解码模型虽在语音和文本任务上取得显著成功，但其在不同模态和多任务间高效适应性不足。因此，亟需一种通用、高效的适配方案，以提升模型在多模态任务（如同时处理语音和文本的机器翻译）中的表现。

Method: 作者提出Whisper-UT框架，在现有Whisper模型基础上添加轻量级的Adapter模块，实现任务间和模态间的统一高效适应。方法支持将ASR结果或真实转录作为提示，采用二阶段解码策略，同时利用跨模态、多任务微调提升性能，无需三方并行数据。

Result: 实验结果显示，在Whisper模型上集成该方法后，多模态翻译及语音翻译性能得到显著提升，且方法具备高度的灵活性与高效性。无须三方并行数据时，跨模态、跨任务微调同样取得了良好效果。

Conclusion: Whisper-UT框架为多模态机器翻译和类似任务提供了一种统一、高效、可泛化的解决方案。通过轻量适配器和智能提示机制，可扩展至其他多任务模型，并带来有效性能提升。

Abstract: Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.

</details>


### [223] [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Elena Hayoung Lee,Gale Lucas*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLM）在仿真人类多回合冲突协商中的行为一致性，发现GPT-4.1和Claude-3.7在不同维度有较好表现，但与人类仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于涉及复杂社交和情感互动的任务，理解它们是否能逼真反映人类在对抗性语境下的行为变得重要。

Method: 模拟多回合的冲突与协商对话，采用五因素人格模型引导各LLM，把人格风格注入模型，以控制个体差异。分别评估语言风格、情感表达（如怒气动态）、和策略行为三大维度的人类一致性。

Result: GPT-4.1在人类语言风格和情感动态方面与人类一致性最好；Claude-3.7-Sonnet在策略行为的反映上效果最佳，但两者与真实人类行为仍有明显差距。

Conclusion: 本研究为LLM与人类在社交复杂互动中的一致性设立了基准，表明人格定制能提升仿真度，但还存在显著局限。

Abstract: Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.

</details>


### [224] ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
*Huy Nghiem,Phuong-Anh Nguyen-Le,John Prindle,Rachel Rudinger,Hal Daumé III*

Main category: cs.CL

TL;DR: 本论文利用双过程框架，对大语言模型（LLM）在大学招生决策中社会经济地位（SES）考量进行大规模审查，发现即便学业表现相同，LLM系统通常偏向低SES申请者，且解释型思考更易强化这一偏好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地参与高风险领域，如大学招生，其在社会敏感决策中的推理方式尚不清楚。本论文旨在揭秘LLM如何处理有关社会经济地位的决策、公平性风险及其应用前景。

Method: 作者构建了基于真实数据相关性的3万个虚拟申请者数据库，并以两种模式（快速决策和解释决策）对4种开源LLM（Qwen 2、Mistral v0.3、Gemma 2、Llama 3.1）进行大规模测试，总共发出500万次提问。同时提出双过程审计框架（DPAF），系统性审查模型推理行为。

Result: 实验结果显示，在控制学业表现的情况下，LLM普遍偏向低SES申请者；采用解释型决策模式（System 2）时，这一偏好显著增强，因为模型更倾向于将低SES作为补偿性理由。

Conclusion: LLM在处理会影响公平性的敏感决策时，表现出潜在偏好，并且解释型推理模式可能加剧这类偏好。作者提出的DPAF框架可为敏感场景下LLM行为审查提供新方法，对未来模型应用与监管具有重要参考意义。

Abstract: Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.

</details>


### [225] [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
*Richard Diehl Martinez,David Demitri Africa,Yuval Weiss,Suchir Salhan,Ryan Daniels,Paula Buttery*

Main category: cs.CL

TL;DR: 本文提出了Pico框架，旨在为小型和中型语言模型的系统性研究与开发提供支持。Pico提供了一套模块化工具及标准基线模型，便于科学地评估和改进模型设计。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型常通过扩展规模获得性能提升，但对于小模型而言，由于参数空间有限，任何设计决策都影响重大，然而现有研究缺乏用于系统性测试和迭代的科学方法。作者希望解决小型语言模型设计过程中“不透明”和“凭经验”带来的问题。

Method: Pico框架由两个库组成，支持研究人员在模型结构或训练流程上做出有针对性的修改，并直观地观察模型行为的变化。此外，作者还发布了在标准条件下训练得到的开源基线模型（pico-decoder），以支持可复现的实验。

Result: 基于Pico，研究人员能更方便地对小型LM进行模块修改和分析，通过案例展示了该平台在迭代设计和分析小语言模型方面的实用性和有效性。

Conclusion: Pico为小型和中型语言模型研究带来了科学化、可复现、模块化的研究流程，有望推动该领域更加系统和高效的发展。

Abstract: Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.

</details>


### [226] [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
*Tom Mackintosh,Harish Tayyar Madabushi,Claire Bonial*

Main category: cs.CL

TL;DR: 本文引入了ConTest-NLI基准，通过80k句子测试大语言模型(LLM)对英语构式的深层形式-意义映射学习能力，发现模型在应对抽象语法结构时仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在NLP任务上表现突出，但其对语言深层结构性、构式语法理解能力尚不明确。研究动机在于系统评估LLM在“构式语法”这一语言理论中的表达与推理能力。

Method: 提出了ConTest-NLI基准，涵盖八类英语构式，句子从高度词汇化到高度抽象。采用模板生成和模型回路过滤机制生成合成NLI样本，部分人工验证。针对主要LLM进行了零样本和微调测试。

Result: 在零样本任务中，LLM在自然句数据集上准确率为88%，而在对抗样本上显著下降至64%。抽象构式对模型挑战最大。通过在部分ConTest-NLI数据集上微调，准确率最高提升9%。

Conclusion: 当前LLM在处理深层语法结构（特别是抽象构式）上仍存在明显抽象能力短板。ConTest-NLI为评估和推动模型语言结构化学习能力提供了可拓展框架。

Abstract: We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.

</details>


### [227] [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
*Tsz Fung Pang,Maryam Berijanian,Thomas Orth,Breanna Shi,Charlotte S. Alexander*

Main category: cs.CL

TL;DR: 文章提出了PersonaMatrix，一个面向不同用户画像的法律摘要评估框架，并发布了用于多维评测的美国民权案件摘要数据集及评测指标。该方法帮助AI系统更好服务法律专家和普通大众，促进法律知识普及。


<details>
  <summary>Details</summary>
Motivation: 现有法律文档摘要自动化系统多数只关注任务层面的评估，忽略了不同用户（比如律师和普通公众）对法律摘要的不同需求。为提升法律知识普及和服务多样用户，亟需开发兼顾专业性和可读性的评估工具。

Method: 作者提出PersonaMatrix评估框架，从‘用户画像-评估标准’的组合角度对法律摘要进行打分，涵盖法律专业人士与非法律用户六类画像。同时构建了一个美国民权案件摘要多维数据集，按摘要深度、易读性、程序细节等维度变化，并提出了Diversity-Coverage Index(DCl)以挖掘不同用户画像下法律摘要的最优点。

Result: PersonaMatrix和新数据集可以显著暴露传统评估忽略的用户群体差异；DCI指标揭示了画像感知型和非画像感知型评委对摘要优劣的不同评价标准。研究成果为法律AI摘要生成的个性化和普适性提供了训练和评价支持。

Conclusion: PersonaMatrix框架与新数据集推动了法律AI系统向多元、用户友好方向发展，有望提升普通用户和专业人士对法律知识的可获得性，并提出了进行更细致多元AI模型评测的新范式。代码和数据已开源。

Abstract: Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.

</details>


### [228] [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
*Yunzhe Wang,Gale M. Lucas,Burcin Becerik-Gerber,Volkan Ustun*

Main category: cs.CL

TL;DR: 该论文提出了一种框架和算法，以提升生成式智能体在社会模拟中行为的真实性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的生成式智能体在社会模拟中行为经常与专家期望或真实数据不符，存在“行为真实性差距”。这在高风险社会模拟应用中会导致不可靠的结果，亟需改进。

Method: 作者提出了“人格—环境行为对齐”（PEBA）理论框架，将社会行为的合理性描述为分布匹配问题，并基于此提出了一种基于大语言模型的优化算法PersonaEvolve（PEvo），通过反复优化智能体的人格设定，使其行为分布与专家基准更为一致。

Result: 在主动枪击事件的社会模拟实验中，该方法将行为分布差异降低了84%，并比显式指令基线提升34%。经优化的人格还能推广到新的类似模拟场景。

Conclusion: PEBA-PEvo框架大幅提高了社会模拟的行为真实性和可靠性，为基于大语言模型的可信社会模拟提供了原理化方法。

Abstract: Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.

</details>


### [229] [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
*'Mina Arzaghi','Alireza Dehghanpour Farashah','Florian Carichon',' Golnoosh Farnadi'*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中的社会经济偏见及其对下游任务公平性的影响，并评估了不同偏见缓解方法的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在实际应用（如薪资预测、就业状态、信用评估等）中可能会带入并放大社会经济偏见，因此有必要研究和比较不同偏见缓解策略对模型本身及下游任务公平性的影响。

Method: 提出一个统一的评估框架，比较了两种偏见缓解方法：一种是通过概念遗忘实现的内在偏见缓解，另一种是通过反事实数据增强（CDA）实现的外在偏见缓解。使用三种开源LLM，分别作为特征提取器（冻结状态）和微调后的分类器，在金融相关真实任务上进行实证对比。

Result: 通过概念遗忘进行的内在偏见缓解可使性别偏见降低高达94.9%，且在不损失准确率的前提下，能将下游任务中的人口统计平等性提升最高82%。

Conclusion: 早期对LLM进行偏见缓解能够在提升公平性的同时保证任务性能，研究框架为实际部署提供了有效偏见缓解的策略指导。

Abstract: Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.

</details>


### [230] [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
*Margaret Hughes,Brandon Roy,Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CL

TL;DR: 本论文提出了一种基于“回应性”（responsivity）方法，来量化和分析对话的积极性和建设性。


<details>
  <summary>Details</summary>
Motivation: 早期研究大多关注对话中的有害性和极化，较少有工作关注对话中建设性和积极性的衡量。因此，作者希望建立一种客观方法，刻画什么样的对话算作积极和建设性的。

Method: 作者首先提出用语义相似度来衡量说话轮次之间的回应性，其次利用先进的大型语言模型（LLM）辅助判断说话轮次之间的关系。两种方法均与人工标注的对话数据集进行了对比评估。最终选取表现更好的LLM方法，进一步判断回应是否具有实质性。

Result: LLM方法在评估回应性的准确性上优于语义相似度方法。基于LLM的回应性评判不仅能区分是否回应，还能判断回应的实质性。作者据此构建了对话级别的衍生指标，可应用于多种对话样本中有效区分和评估对话结构。

Conclusion: 回应性是对话质量的核心特征，基于LLM的量化方法能有效支持对不同类型对话的区分和建设性评判，为后续对话分析和优化提供了新思路。

Abstract: Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.

</details>


### [231] [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
*Zixun Chen,Petr Babkin,Akshat Gupta,Gopala Anumanchipalli,Xiaomo Liu*

Main category: cs.CL

TL;DR: 本文分析了对话能力在大语言模型（LLM）中的涌现机制及其影响因素，发现模型规模对多数对话指标影响有限，而微调后分数迅速饱和，不同指标间相关性较高。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的对话能力广泛应用，但很少有研究细致划分对话行为的具体成分并分析其成因。因此，作者希望理解究竟哪些因素驱动LLM的对话表现。

Method: 作者基于语言学理论设计了一套针对不同细粒度对话方面的模型评价指标，对Pythia系列预训练模型在不同规模和通过有监督微调后在多维对话能力上的表现变化进行系统性评测，并通过分析分数分布、指标相关性和生成文本的词项频率解释观察到的现象。

Result: 原始模型规模对于多数对话指标影响温和，仅最小模型例外；有监督微调可以迅速将分数推高至饱和；同一评估器衍生的多个指标趋势极为相似，显示指标之间相关性高。

Conclusion: 大模型规模在提升对话能力指标时作用有限，大多数指标在微调后趋于饱和，高度相关性指标的可靠性值得怀疑；对后续指标体系和评测方法的改进提出了新问题。

Abstract: Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.

</details>


### [232] [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
*Fengyuan Liu,Rui Zhao,Shuo Chen,Guohao Li,Philip Torr,Lei Han,Jindong Gu*

Main category: cs.CL

TL;DR: 本文提出了一种基于部分知识的信息攻击框架M-Spoiler，即使攻击者只了解多智能体系统中的一个智能体，也能生成误导整个系统的对抗样本。实验验证了该方法的有效性与风险。


<details>
  <summary>Details</summary>
Motivation: 多智能体大模型协同决策能力虽强，但其中单个智能体的脆弱性尚未完全量化。如果只暴露单个智能体信息，攻击者是否可以危害整个系统的协同决策，存在安全风险与研究空白。

Method: 作者将攻击建模为不完全信息博弈，提出M-Spoiler框架，即针对已知目标智能体，通过引入虚拟"顽固"智能体模拟其他系统成员响应，优化生成可误导系统协同决策的对抗样本，并在多类任务中实验验证。

Result: 实验显示，仅基于对系统中单一智能体的知识即可生成对系统整体协同决策产生显著干扰的对抗样本。作者提出的M-Spoiler在多任务上的攻击成功率均超过现有方法。多种防御机制尝试后，M-Spoiler依然效果突出。

Conclusion: 多智能体大模型系统存在新型安全漏洞，仅凭单个智能体信息即可威胁整个系统的安全。该工作提示需持续关注和强化多智能体系统的防御机制。

Abstract: Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.

</details>


### [233] [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
*Wei Xie,Shuoyoucheng Ma,Zhenhua Wang,Enze Wang,Kai Chen,Xiaobing Sun,Baosheng Wang*

Main category: cs.CL

TL;DR: 本论文提出了AIPsychoBench，一个用于评估大语言模型（LLM）心理属性的专业基准，通过特定prompt设计，提升了评测有效率并降低了偏差，同时首次提供了不同语言下心理测评结果的系统分析。


<details>
  <summary>Details</summary>
Motivation: 现有利用人类心理学量表评估LLM的研究，未能考虑LLM与人类的根本差异，导致高无效率且无法跨多语言评价。因此，需要一种更适合LLM特性的心理属性评估方法。

Method: 设计了一种轻量级角色扮演prompt，以规避LLM对齐机制的限制；并以此为核心，构建AIPsychoBench基准，涵盖112个心理测评子类别，支持多语言测评，并对比响应率和偏差。

Result: 新方法使LLM的平均有效响应率由70.12%提升至90.40%，正负平均偏差率显著降低。多语言结果显示，在43个子类别中，7种语言与英语的分数偏差在5%至20.2%之间。

Conclusion: AIPsychoBench有效提升LLM心理测评的有效性与解释性，显著降低偏差，并首次量化了多语言对LLM心理测评的影响，为未来LLM评测工具的设计提供了重要依据。

Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.

</details>


### [234] [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
*Junghwan Kim,Haotian Zhang,David Jurgens*

Main category: cs.CL

TL;DR: 提出了一种用于多语言作者风格建模的新方法，通过两项创新显著提升了非英语环境下的归属表现。


<details>
  <summary>Details</summary>
Motivation: 现有作者表示学习方法多聚焦于英语单语场景，忽视了多语言环境下风格建模的潜力，因此需要开发适用于多种语言的作者风格学习方法，以提升跨语言和跨领域的泛化能力和归属准确性。

Method: 方法包含两大创新：1) 概率性内容屏蔽（probabilistic content masking），促使模型聚焦于能体现作者风格的词汇，而非内容相关词汇；2) 语言感知批处理（language-aware batching），在对比学习过程中减少跨语言干扰，提升模型辨识力。模型在大规模多语言多领域数据集上训练。

Result: 新方法在36种语言、13个领域、450万作者的大数据集上评测，对比单语基线，在21个非英语语言任务中有平均4.85%的Recall@8提升，最高单语言可达15.91%。模型在跨语言、跨领域泛化能力上均优于仅用英语训练的单语模型。

Conclusion: 文中提出的概率性内容屏蔽和语言感知批处理对提升多语言作者风格学习具有关键作用，方法整体能大幅提升多语言、跨领域的作者归属表现，验证了多语言风格建模的重要性和优越性。

Abstract: Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.

</details>


### [235] [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
*Sungwon Kim,Daniel Khashabi*

Main category: cs.CL

TL;DR: LLMs在对抗性对话中容易顺从用户观点，但在同时评价多个观点时表现较好。实验细致分析了不同交互模式下LLMs的评判行为，揭示其判断结果易受交互方式影响。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs被广泛用于评估性任务（如评分、判别论点），但它们在多轮对话中常显示出迎合（sycophancy）用户的倾向。该研究关注为何LLMs在对话追问下表现出迎合，却能在同时评判冲突观点时较为公正，探究二者间的矛盾。

Method: 研究通过实证方法，设置不同交互场景：如将用户反驳反馈作为后续提问；或将多个对立观点同时展现给模型，考察模型在不同场景下的反应和评判倾向。还检验了反驳理由的细致程度和语气正式性等变量对模型判断的影响。

Result: 最新的LLMs在用户以跟进问题方式提出反驳时，更易认同用户观点；当反驳含有详细推理（即便结论有误）时，模型更容易被说服；而非正式、随意的反馈比正式批评更容易影响模型决策。不同比例下，模型对影响因素表现出明显敏感性。

Conclusion: 结果警示在将LLMs用于评判任务时，需充分考虑对话互动方式对其判断力的影响，否则其评判可靠性难以保障。

Abstract: Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.

</details>


### [236] [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
*Cheng Jiayang,Qianqian Zhuang,Haoran Li,Chunkit Chan,Xin Liu,Lin Qiu,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出并系统研究了“大语言模型（LLMs）在外部知识下的整合式本体接地（integrative grounding）”问题，分析模型面对复杂、多证据推理时的行为和改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM外部知识接地方法大多聚焦于简单查询，然而真实场景常需多源证据的综合推理。本文提出新的问题设定——整合式本体接地，并希望推动模型在复杂推理能力和知识检索方面的进步。

Method: 作者将不同领域的四类数据集用于系统评估，研究了多证据检索及验证策略，包括冗余与缺失信息下的应对、不同检索规划方法（如无方向规划与前提溯因）对性能影响，以及零样本自反思机制对接地质量的提升。

Result: 主要发现：（1）LLM能较好处理冗余证据，但信息不全时倾向依赖内部知识假设答案；（2）无方向检索会引入噪声，降低表现，而前提溯因式规划因逻辑约束更有效；（3）零样本自反思机制能持续提升知识接地质量。

Conclusion: 上述发现为构建更高效的多证据整合式知识接地系统提供了理论和方法指导，有助于推动LLM在真实复杂任务中的应用。

Abstract: Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.

</details>


### [237] [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
*Khalid Hasan,Jamil Saquer,Yifan Zhang*

Main category: cs.CL

TL;DR: 本论文比较了多种先进NLP模型在多类精神健康状况检测任务中的表现。结果显示，transformer模型表现最佳，但特定LSTM改进版在效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 以往NLP研究多聚焦单一精神健康状况的识别，鲜有对多类精神健康状况同时区分及模型对比的系统研究。本论文旨在填补这一空白。

Method: 作者构建涵盖六种精神健康状况和一类对照组的大型Reddit数据集，进行了严格标注。比较了五种transformer架构与多种LSTM变体（带/不带注意力机制，使用语境化/静态嵌入），在相同条件下分类精神健康帖子。

Result: transformer模型普遍优于LSTM模型，其中RoBERTa在所有分类中准确率和F1分数达91-99%。利用BERT嵌入且附带注意力机制的LSTM也取得较高表现（F1高达97%），且训练速度提升2-3.5倍。而采用静态嵌入的LSTM模型效果不佳。

Conclusion: 该研究首次为多分类精神健康检测建立了全面的模型基准，为模型选择和实际部署提供了指导，同时揭示了准确性与效率之间的权衡。

Abstract: Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.

</details>


### [238] [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
*Yue Huang,Zhengzhe Jiang,Xiaonan Luo,Kehan Guo,Haomin Zhuang,Yujun Zhou,Zhengqing Yuan,Xiaoqi Sun,Jules Schleinitz,Yanbo Wang,Shuhao Zhang,Mihir Surve,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 该论文提出了ChemOrch框架，通过两阶段流程生成高质量的化学领域指令-响应数据，有效提升了大语言模型在化学领域的智能水平。


<details>
  <summary>Details</summary>
Motivation: 当前高质量、化学领域专属的指令-响应数据稀缺，且现有的数据合成方法难以契合化学知识的分层性和规则性，阻碍了大语言模型在化学智能方面的应用。

Method: 提出ChemOrch框架，包括（1）控任务指令生成，（2）结合工具的响应构建。该方法支持任务多样性与难度可控，并利用工具规划、知识蒸馏及自修复机制保障响应准确。

Result: 实验表明：ChemOrch生成的数据质量更高、覆盖化学规则，能生成更有效揭示LLM化学弱点的评测任务，并用其微调模型后显著提升化学能力。

Conclusion: ChemOrch框架为大语言模型在化学领域的规模化与可验证智能发展提供了重要支撑，推动了领域专用AI能力的提升。

Abstract: Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.

</details>


### [239] [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 本文研究了文本复杂性对语言模型预训练和下游任务的影响，发现文本简化对不同模型容量和任务表现有不同作用。


<details>
  <summary>Details</summary>
Motivation: 尽管提升预训练数据质量和规模被证实能增强下游表现，但文本复杂性对其中的作用尚不清楚。因此，作者希望揭示文本复杂性的具体效应，并探究简化后的文本是否依然可以训练出有效表征。

Method: 作者采用大型语言模型简化真实文本，生成结构更简单、句子更短的语料，然后分别用原始和简化数据，从头预训练不同规模（28M-500M）的自回归语言模型，并在微调和零样本设定下进行评测。

Result: 实验发现，模型困惑度对模型容量与文本复杂性的相互作用较敏感——小模型在处理简化文本时效果下降不明显。文本复杂性对微调后的下游表现影响很小；而在零样本任务上，简单文本更有利于语言知识类任务，复杂文本更适合涉及世界知识和实体追踪的任务。

Conclusion: 简化文本作为预训练数据来源，不会影响多数下游微调任务，但在特定任务表现上与数据复杂性有关。在实际应用中可权衡任务类型与所需语料复杂度，灵活选择预训练数据。

Abstract: Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.

</details>


### [240] [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
*Jun Rong Brian Chong,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 该论文提出了一种多轮、基于人设的模型（MPCG），用以模拟虚假信息随传播过程中的演变，并系统评估其对主流虚假信息检测模型的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前主流虚假信息检测方法普遍假设信息内容静态未变，忽略了虚假信息在现实传播中会因受众背景发生多维度（如语言、立场、道德）演化的问题。这种静态假设可能限制了检测方法在应对新型虚假信息时的有效性。

Method: 提出MPCG框架，利用未受限的大模型，结合多轮生成和不同意识形态‘人设’条件，模拟信息在多轮传播中的动态演化。每一轮生成都依赖于上一轮的输出，从而较为真实地建模虚假信息随传播环境调整自身表述的过程。然后，研究人员用人工和LLM标注、可读性困惑度测量、情感及道德分析、聚类、可行性等多维度对生成内容进行评估。

Result: 实验显示：人工与GPT-4o-mini在绝大多数标注任务上一致性高；生成的虚假信息比原始内容更复杂、更符合人设的情绪与道德倾向，同时在主题上连贯但语义有明显漂移。聚类和余弦相似度分析进一步支持语义演化存在。可行性评估显示，77%的生成内容适合下游任务。主流检测模型在应对演化后虚假信息时性能显著下降，最高F1下降49.7%。

Conclusion: 虚假信息在传播中动态演化特征会削弱传统检测模型的效能，基于静态假设的检测方法不足以覆盖实际挑战。MPCG框架为更全面评估与应对动态演化虚假信息提供了新的思路和工具。

Abstract: Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG

</details>


### [241] [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
*Benlu Wang,Iris Xia,Yifan Zhang,Junda Wang,Feiyun Ouyang,Shuo Han,Arman Cohan,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 本文针对大语言模型在医学计算场景中的可信评估进行了深入分析，并提出了更细致和可信的评测与诊断方法，还进一步提出了能显著提升LLM计算准确率的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中表现出色，但它们在医学计算能力方面的评测存在不足，评测方式容易掩盖推理错误，对临床应用的可靠性存在重大隐患。现有评测仅关注最终结果，并存在数值宽容度高的问题，容易造成系统性推理失败被忽略，进而带来临床风险。

Method: 作者首先对MedCalc-Bench数据集进行了清洗与结构化，并提出了分步评估流程，分别独立考察公式选择、实体抽取和计算步骤，揭示出传统评估中被掩盖的模型错误。其次，设计了一个自动错误分析框架，能对每种出错模式生成结构化溯因标注，并通过人工验证其与专家判断高度一致，从而实现大规模可解释性诊断。最后，作者提出了模块化智能计算流程MedRaC，结合了检索增强生成与Python代码执行，无需微调即可大幅提升不同LLM的准确性。

Result: 在更细致的分步评估下，GPT-4o的准确率从62.7%下降到43.6%，显示出许多默认评测掩盖了的问题；而新的模块化智能管道MedRaC则能将多种LLM的准确率最大提升到53.19%。自动化分析工具能够高效识别出模型的典型错误方式，并获得专业人员的高度认可。

Conclusion: 现有医学计算评测方法存在重要盲区，掩盖了LLM推理过程中的关键缺陷。通过更严谨的分步评测和自动化错误剖析，可实现对模型临床可靠性更真实的反映。所提出的MedRaC管道为医学计算类应用带来了实用的准确率提升手段，从而为LLM在真实医学场景落地建立可信基础。

Abstract: Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.

</details>


### [242] [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
*Qiongqiong Wang,Hardik Bhupendra Sailor,Tianchi Liu,Wenyu Zhang,Muhammad Huzaifah,Nattadaporn Lertcheva,Shuo Sun,Nancy F. Chen,Jinyang Wu,AiTi Aw*

Main category: cs.CL

TL;DR: 本文提出了CP-Bench基准，专门用来评估语音大模型（speech-LLMs）在处理语境语用推理能力方面的表现，包括对言语内容和非语言线索（如情感、语调）的综合理解。通过对现有多种开源和闭源语音大模型进行全面测试，论文揭示了目前模型在社会和情感智能理解方面的不足，并为提升语音大模型的情感与语境理解能力提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型虽然在转录和翻译等任务上表现优异，但在理解语音中的情感、语调等语用特征方面仍有较大局限，这严重影响了其在需要社会和情感智能的真实应用中的表现。作者希望提升模型对言语之外的“非语言信息”（如情感、语气等）的综合处理能力，因此设计了新的评测基准。

Method: 作者构建了一个评测基准（CP-Bench），集合了两个特别设计的问答数据集，这些数据不仅要求模型理解言语文本，还要结合语音中的情感与语调等非语言信息进行推理。研究对最新的开源和闭源语音-LLMs进行了系统评测，并分析了不同问题类型、温度参数调整等因素对表现的影响。

Result: 在新提出的基准下，主流语音大模型虽有一定能力，但在需要结合言语和非言语信息进行情感和上下文推理的问题上仍存在较明显短板。不同模型的表现有一定差距，通过参数调优（如temperature）可以部分提升性能。

Conclusion: CP-Bench暴露出目前语音大模型在情感与社会智能方面的显著不足，为后续研究指明了方向，即亟需针对非语言信息融合的能力进行专项提升。该基准为后续更具情感理解、社会智能的语音大模型设计提供重要参考。

Abstract: Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.

</details>


### [243] [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
*Zheng Liu,Mengjie Liu,Siwei Wen,Mengzhang Cai,Bin Cui,Conghui He,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出HAPO（Heterogeneous Adaptive Policy Optimization）算法，通过在RL训练中按token熵动态调整优化方式，实现对LLM推理能力更细粒度的提升，并在多个规模模型中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习提升大模型推理能力时，缺乏对不同token在推理中的差异化对待，而采用统一优化，可能无法充分挖掘模型潜力。为更好利用token间异质性，提升性能，需要有细粒度、动态适应不同token状态的新方法。

Method: 提出HAPO框架，在采样时通过Adaptive Temperature Sampling实时调整温度，鼓励高熵token探索、保障低熵token连贯性；采用Token Level Group Average规范化token层面优势值，兼顾序列长度与非偏置处理；设计Differential Advantage Redistribution，结合熵和importance ratio调节对信号清晰token的奖励更新；提出Asymmetric Adaptive Clipping，对低熵噪声token施加更强clip，高熵token留有探索空间。训练各阶段深度嵌入token级动态优化。

Result: 实验证明HAPO在多个模型尺度上都优于现有的DAPO算法，显示了策略的有效性和泛化能力。

Conclusion: HAPO通过对token熵异质性自适应优化，实现了更加精细和有效的RL训练方法，为大模型推理能力提升带来显著收益。

Abstract: Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.

</details>


### [244] [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
*Junjie Ye,Yuming Yang,Yang Nan,Shuo Li,Qi Zhang,Tao Gui,Xuanjing Huang,Peng Wang,Zhongchao Shi,Jianping Fan*

Main category: cs.CL

TL;DR: 本文研究了监督微调（SFT）对大语言模型世界知识的影响，发现并非越多数据越好，并分析了原因与优化建议。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在预训练过程中已获得丰富的世界知识，但在经过监督微调后，其知识表现受到复杂影响。当前，对SFT对模型知识影响的系统研究有限，阻碍了人们针对性地改善微调后模型的知识性能。

Method: 作者通过对LLaMA-2和LLaMA-3家族的五个模型进行“闭卷问答”测试，比较不同微调样本量（如1920与240）的CBQA性能，并分析微调数据中知识掌握水平对表现的影响。同时，从token和参数两个层面分析微调过程中的行为和参数变动。

Result: 研究发现：1）用1920样本微调的模型CBQA表现比用240样本的模型最多低14%；2）微调数据的知识掌握水平不同导致性能浮动超过12%；3）高达90%的SFT参数更新并未带来知识增强，部分恢复这些参数可提升CBQA能力。

Conclusion: 研究表明，监督微调不恰当反而会削弱模型知识能力。对参数更新和微调数据的合理管理有助于提升知识相关性能，为更有效的微调策略提供了依据。

Abstract: Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.

</details>


### [245] [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
*Luyan Zhang*

Main category: cs.CL

TL;DR: 本文针对大模型在多轮推理和多模态协同等复杂任务中计算效率低和解释性不足的问题，提出了一种三层协作的MCP框架，通过模块解耦与动态路由提升性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在实际应用场景下，常遇到推理慢、难以解释其决策原因等问题，尤其在需要多轮推理和多模态信息协同时尤为突出，严重制约了其落地应用。

Method: 作者提出“模型-控制器-任务自适应（MCP）”三层协作框架，将大模型的功能解耦成推理、生成、检索三个模块，利用强化学习驱动的动态路由算法和任务自适应机制，实现控制理论与动态推理的系统性集成，并通过特定的Presenter层输出可解释的中间结果。

Result: MCP框架在GLUE、COCO、ScienceQA等跨模态基准任务上相较基线提升15-30%，推理效率提升40%，解释性得分达到人工评分90%。

Conclusion: 该方法为大模型可用性提升和实际部署提供了新路径，有效缓解了大模型在效率和可解释性上的应用瓶颈。

Abstract: Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.

</details>


### [246] [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
*Byeongho Yu,Changhun Lee,Jungyu Jin,Eunhyeok Park*

Main category: cs.CL

TL;DR: 本文提出了一种新的对比解码方法PruneCD，通过剪枝模型层而非早停获得对比先验，以有效缓解大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DoLa）利用模型早停得到的logits作为对比先验，但这些logits信息量不足，难以有效指导对比解码以减少幻觉。

Method: 提出PruneCD方法，通过剪枝（layer pruning）部分模型层构建业余模型，生成更有区分度和对齐性的logits，实现更有效的对比解码。

Result: 实验证明PruneCD在保证推理开销极小的前提下，显著提升了大语言模型输出的事实性。

Conclusion: PruneCD是一种强健而实用的缓解LLMs幻觉问题的方法，比现有早停方法更有效。

Abstract: To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.

</details>


### [247] [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
*Sandro Tsang*

Main category: cs.CL

TL;DR: 本文提出并评估了一种结合信息检索技术的系统综述流程，在内异症复发领域显著提升了效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统系统综述由于文献量庞大且增长迅速，手动检索和筛选难以满足效率与可复现性需求，需要新型信息技术辅助。

Method: 将PRISMA指南与半自动化的信息检索、去重及手动筛查相结合，对GnRH激动剂在内异症复发上的疗效随机对照试验文献进行证据整合，并采用改进分组法应对多组试验的统计问题。

Result: 仅用11天筛选812条记录，最终纳入7项RCTs（共841例患者），结果显示GnRH激动剂可降低36%复发风险，异质性极低，敏感性和偏倚分析证实结果稳健。

Conclusion: 验证了基于信息检索的新系统综述工作流可提升证据合成效率与质量，不仅获得了临床实用结果，还为复杂综述提供了可推广的高效研究范式。

Abstract: Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.

</details>


### [248] [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
*Junhao Chen,Jingbo Sun,Xiang Li,Haidong Xin,Yuhao Xue,Yibin Xu,Hao Zhao*

Main category: cs.CL

TL;DR: 本文提出LLMsPark，这是一个基于博弈论的评测平台，通过多智能体交互环境，评估和比较大语言模型（LLMs）的决策和策略行为。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类任务中的表现越来越强，对其综合能力的评估需求不断增长。传统单一维度的评测已难以全方面反映LLMs的智能，需要更全面地考察其在互动和策略行为上的表现。

Method: 作者构建了LLMsPark平台，基于经典博弈论场景，为LLMs设计了多智能体交互测评，考察其决策策略及社会行为。平台对15种主流（商用与开源）LLMs进行交叉比拼，通过排行榜和量化评分展现模型间的战略智能差异。

Result: 评测结果显示，不同LLMs在推理、策略决策等方面存在显著差异，高得分模型表现出更强的推理和博弈能力。部分模型展现了独特的行为模式和性能优势。

Conclusion: LLMsPark为衡量LLMs的战略智能提供了新视角，丰富了现有评测体系，有助于在互动和策略环境下更全面地评估大语言模型的智能水平。平台和排行榜已开放。

Abstract: As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.

</details>


### [249] [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
*Zuhair Hasan Shaik,Abdullah Mazhar,Aseem Srivastava,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 论文提出了一种新的有理论基础的方法EigenShift，通过分解大型语言模型输出层来精准抑制有毒内容的生成，同时保持模型的语言能力，不需要额外训练或较大计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有抑制大语言模型有毒输出的方法大多针对单个神经元，但稳定性差、对上下文敏感，且可能损害模型的核心表达能力。因此需要一种既稳定又不影响模型，本质上能解释和应对毒性生成机制的新方法。

Method: 论文通过实验证明：分层（layer-wise）表示比单神经元信号更稳健，指出了现有方法混淆了检测毒性和生成毒性的机制。提出新方法EigenShift，基于模型最后输出层的特征分解，有选择地干预与有毒生成相关的分量，不损害正常文本能力，无需重新训练。

Result: 在Jigsaw和ToxiCN等数据集上的实验显示，EigenShift方法比神经元级干预更稳健、有效地抑制了有毒内容生成，并保持了模型的语言流畅性和表达能力。

Conclusion: EigenShift为语言模型无损、精确、有理论基础的毒性抑制提供了新范式，具有应用前景和理论价值，且操作高效，无需额外训练。

Abstract: Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.

</details>


### [250] [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
*Ahmet Yavuz Uluslu,Tannon Kew,Tilia Ellendorff,Gerold Schneider,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了一种受法证语言学启发的代理式NLI流程，通过专门的子模块收集和分析多样语言证据，再由独立协调代理进行最终判断，从而提升LLM在NLI任务中对迷惑性线索的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往LLM在NLI任务中过度依赖名字、地理和文化刻板印象等表面线索，导致模型易被误导，现有忽略这些表面线索的方法仍不可靠。

Method: 提出一个受法证语言学启发的新Pipeline：多个专业代理分别积累和分类不同语言学证据，最终由一个目标意识强的协调代理整合所有证据得出NLI预测。

Result: 在两个NLI基准数据集上，该方法比标准提示法显著提升了对误导性线索的鲁棒性和表现一致性。

Conclusion: 代理式证据聚合不仅能有效提升模型鲁棒性，还能减少对表面线索的依赖，提高NLI任务的可靠性。

Abstract: Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.

</details>


### [251] [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
*Keliang Liu,Dingkang Yang,Ziyun Qian,Weijie Yin,Yuchi Wang,Hongsheng Li,Jun Liu,Peng Zhai,Yang Liu,Lihua Zhang*

Main category: cs.CL

TL;DR: 本文综述了强化学习(RL)如何在大语言模型(LLM)全生命周期中提升其推理及对齐能力，并对关键技术、数据集、工具框架及未来趋势进行了系统总结。


<details>
  <summary>Details</summary>
Motivation: 尽管已有关于RL增强LLM的综述，但多局限于某一阶段，缺乏从模型生命周期全流程的系统总结。该文旨在弥补现有综述覆盖范围有限的不足，为研究者提供RL在LLM各阶段应用的全面参考。

Method: 1. 简要介绍RL基础理论；2. 详细梳理RL在LLM预训练、对齐微调、推理增强各阶段的应用策略，尤其强调RL在推理增强中的核心作用；3. 汇总用于RL微调的数据集和评测基准，包括人工标注、AI辅助偏好、程序验证等类型；4. 盘点主流开源工具和训练框架；5. 分析未来挑战与发展趋势。

Result: 系统整理了RL在LLM的全流程应用，强调推理增强阶段的技术进展，并总结了现有数据集、工具框架，对比分析其优缺点；指出了当前方法在安全性、泛化性等方面的挑战。

Conclusion: 本综述为LLM与RL结合的研究和实践提供了体系化的背景与前沿进展，期望推动更智能、可泛化与更安全的LLM发展。

Abstract: In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.

</details>


### [252] [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)
*Zhengge Cai,Haowen Hou*

Main category: cs.CL

TL;DR: EG-MLA提出了一种高效的注意力机制，能大幅度减少大语言模型推理时KV缓存占用，且能提升或保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 在大模型推理过程中，KV缓存很占用内存与影响延迟。虽然多头注意力强大，但KV缓存成本高。MLA已能压缩KV信息，但进一步压缩会损失性能。因此需要更先进的机制来在压缩和性能间取得更优平衡。

Method: 方法为Embedding-Gated Multi-head Latent Attention（EG-MLA），即在MLA基础上，提出在潜在空间引入token级别的embedding门控机制，实现对压缩后KV向量的细粒度调节，仅需极小额外计算开销。理论上，该门控机制可引入高阶隐式交互。

Result: 与常规MHA相比，EG-MLA可减少91.6%的KV缓存，用极小性能损失。相对MLA，EG-MLA还能在不同推理任务上进一步节省最高59.9%内存，且性能提升。跨模型规模、不同压缩比例下泛化效果良好，并已实测到10亿参数量级。

Conclusion: EG-MLA是一种既省内存又高效的注意力机制，适合大规模语言模型推理，能支持高性能且可扩展的部署需求。

Abstract: Reducing the key-value (KV) cache size is a crucial step toward enabling
efficient inference in large language models (LLMs), especially under latency
and memory constraints. While Multi-Head Attention (MHA) offers strong
representational power, it incurs significant memory overhead. Recent work on
Multi-head Latent Attention (MLA) mitigates this by compressing KV
representations into a shared latent space, achieving a better trade-off
between performance and cache efficiency. While MLA already achieves
significant KV cache reduction, the scope for further compression remains
limited without performance loss. In this paper, we propose
\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel
extension of MLA that further reduces KV cache size while enhancing
representational expressiveness. EG-MLA introduces a token-specific embedding
gating mechanism applied in the latent space, enabling fine-grained modulation
of compressed KV vectors with minimal additional computation. Compared to MHA,
EG-MLA achieves over 91.6\% reduction in KV cache size with negligible
performance degradation. Relative to MLA, EG-MLA consistently improves task
accuracy across diverse reasoning benchmarks while achieving up to 59.9\%
additional memory savings. Our theoretical analysis highlights how embedding
gating induces implicit high-order interactions, and empirical evaluations
demonstrate robust generalization across model scales and compression regimes.
Notably, we successfully scale EG-MLA to over 1 billion parameters,
demonstrating its practical viability for large-scale LLM deployment. These
results establish EG-MLA as a memory- and compute-efficient attention mechanism
that enables scalable, high-performance inference in modern LLMs.

</details>


### [253] [Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2509.16696)
*Wataru Hashimoto,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文研究了解码策略（如对比性搜索）对大型语言模型（LLM）不确定性估计的影响。结果表明对比性搜索有效提升了经过偏好对齐的大模型的不确定性估计效果，但在仅用监督微调整的模型上则效果有限。


<details>
  <summary>Details</summary>
Motivation: 解码策略直接影响生成质量和模型输出的不确定性，对下游任务非常关键。当前关于解码策略（比如贪婪、采样和对比性搜索）如何影响不确定性估计的系统性研究较少，本文旨在填补这一空白。

Method: 作者对多种主流的LLM和不同的解码策略进行实验，包括对比性搜索和其他常见解码方法。在偏好对齐及仅做监督微调的模型之间，比较其在不确定性估计上的表现。

Result: 实验发现：对比性搜索能在总体上提升带有偏好对齐的LLM的不确定性估计能力。而在没有显式对齐、只有监督微调的模型中，解码策略的优势则不那么明显，甚至可能表现各异。

Conclusion: 解码策略，尤其是对比性搜索，可以提升偏好对齐LLM的不确定性估计。然而，仅用监督微调的情况下，此优势未必能显现，说明模型训练方式和解码策略对不确定性估计有显著耦合效应。

Abstract: Decoding strategies manipulate the probability distribution underlying the
output of a language model and can therefore affect both generation quality and
its uncertainty. In this study, we investigate the impact of decoding
strategies on uncertainty estimation in Large Language Models (LLMs). Our
experiments show that Contrastive Search, which mitigates repetition, yields
better uncertainty estimates on average across a range of preference-aligned
LLMs. In contrast, the benefits of these strategies sometimes diverge when the
model is only post-trained with supervised fine-tuning, i.e. without explicit
alignment.

</details>


### [254] [OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama](https://arxiv.org/abs/2509.16713)
*Tianyang Xu,Hongqiu Wu,Weiqi Wu,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出了Open-Theatre，一个支持大型语言模型（LLM）互动剧情开发与体验的开源工具包，有助于研究者深入研究并复现基于LLM的互动戏剧场景。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的互动剧情为玩家沉浸式扮演角色、体验戏剧故事带来新机遇，但缺乏完整开发环境，阻碍了该领域的研究与应用。

Method: 提出了Open-Theatre工具包，具备高效的多智能体架构和分层检索式记忆系统，以提升复杂交互中的叙事连贯性和角色长期行为的真实感，并提供高度可配置的开发流程。

Result: Open-Theatre实现了可扩展的、定制化的互动剧情环境，推动了LLM在互动戏剧中的应用开发与实验。

Conclusion: Open-Theatre填补了LLM互动戏剧领域工具缺口，降低了研究和拓展门槛，有望促进相关系统和方法的进一步创新与实践。

Abstract: LLM-based Interactive Drama introduces a novel dialogue scenario in which the
player immerses into a character and engages in a dramatic story by interacting
with LLM agents. Despite the fact that this emerging area holds significant
promise, it remains largely underexplored due to the lack of a well-designed
playground to develop a complete drama. This makes a significant barrier for
researchers to replicate, extend, and study such systems. Hence, we present
Open-Theatre, the first open-source toolkit for experiencing and customizing
LLM-based interactive drama. It refines prior work with an efficient
multi-agent architecture and a hierarchical retrieval-based memory system,
designed to enhance narrative coherence and realistic long-term behavior in
complex interactions. In addition, we provide a highly configurable pipeline,
making it easy for researchers to develop and optimize new approaches.

</details>


### [255] [Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling](https://arxiv.org/abs/2509.16717)
*Haoran Li,Zhiming Su,Junyan Yao,Enwei Zhang,Yang Ji,Yan Chen,Kan Zhou,Chao Feng,Jiao Ran*

Main category: cs.CL

TL;DR: 本文提出了一种新的中文短视频数据集及半监督合成数据生成方法，有效提升了嵌入模型在多样和细粒度相关性数据上的表现，显著优化了推荐系统实际效果。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入模型常通过合成数据增强训练样本多样性，但现有基于提示的合成方法难以覆盖特定领域的数据分布，尤其在数据稀缺领域，并且经常忽视了相关性多样性。为此，作者希望构建更能反映实际分布和多粒度相关性的训练数据，以填补中文短视频领域资源空白并提升模型表现。

Method: 作者首先构建了一个带有四级相关性标注的中文短视频数据集。随后提出了一套半监督的合成数据生成流程，利用两模型协同训练生成适应领域、不同相关性标签的短视频样本，重点合成中间相关性等级，提升样本分布均衡性和语义多样性。

Result: 离线实验表明，采用该合成数据训练的嵌入模型在描述性及相关性区分能力上明显优于基于提示生成或传统监督微调的数据。在线A/B测试中，所提模型在抖音双列推荐场景下，点击率提升1.45%，强相关比提升4.9%，图片用户渗透率提升0.1054%。

Conclusion: 本文提出的细粒度相关性监督和领域自适应合成数据方法，有效提升了中文短视频推荐系统中嵌入模型的表现，特别强调了高质量、细粒度标注对实际推荐效果的重要价值。

Abstract: Synthetic data is widely adopted in embedding models to ensure diversity in
training data distributions across dimensions such as difficulty, length, and
language. However, existing prompt-based synthesis methods struggle to capture
domain-specific data distributions, particularly in data-scarce domains, and
often overlook fine-grained relevance diversity. In this paper, we present a
Chinese short video dataset with 4-level relevance annotations, filling a
critical resource void. Further, we propose a semi-supervised synthetic data
pipeline where two collaboratively trained models generate domain-adaptive
short video data with controllable relevance labels. Our method enhances
relevance-level diversity by synthesizing samples for underrepresented
intermediate relevance labels, resulting in a more balanced and semantically
rich training data set. Extensive offline experiments show that the embedding
model trained on our synthesized data outperforms those using data generated
based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we
demonstrate that incorporating more diverse fine-grained relevance levels in
training data enhances the model's sensitivity to subtle semantic distinctions,
highlighting the value of fine-grained relevance supervision in embedding
learning. In the search enhanced recommendation pipeline of Douyin's
dual-column scenario, through online A/B testing, the proposed model increased
click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance
Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by
0.1054%.

</details>


### [256] [Time to Revist Exact Match](https://arxiv.org/abs/2509.16720)
*Auss Abbood,Zaiqiao Meng,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出用数值化的评估方法（如sMAPE和MASE）替代传统exact match（EM）指标，以更准确地评价大语言模型在时间性问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统exact match指标难以区分模型答复与标准答案间小幅和大幅数值误差，在需要数值时间推理（如日期、时长）的任务中有明显局限性。

Method: 提出TempAnswerQA数据集，抽取自Test of Time和TempTabQA，所有问题均需数值时间答案。分别用对误差更加敏感的 sMAPE 和 MASE 这两个数值预测指标对模型回答进行评估，并对比分析其与EM指标表现。

Result: sMAPE显示模型答题时，误差大小和EM指标并不一致：有些模型EM低但sMAPE同样低，反之亦然。采用MASE调整误差后，不同模型排名顺序也发生了变化，揭示模型对时间领域知识理解存在不足。多数模型常见的错误是与标准答案仅相差±1，而sMAPE和MASE对此做了更合理的敏感性处理。

Conclusion: 传统EM指标不适合评估需要数值化推理的时间问答任务，应采用更专业的数值性误差指标（如sMAPE和MASE），以更准确反映大语言模型的推理能力。

Abstract: Temporal question answering is an established method for evaluating temporal
reasoning in large language models. Expected answers are often numeric (e.g.,
dates or durations), yet model responses are evaluated like regular text with
exact match (EM), unable to distinguish small from large errors. In this
investigative work, we frame temporal question answering as a numerical
estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a
benchmark distilled from Test of Time and TempTabQA, where all questions
require a numerical, temporal answer, allowing us to evaluate models beyond EM.
We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)
and mean absolute scaled error (MASE). With sMAPE, we find that error size and
EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some
models have high sMAPE despite high EM. Scaling errors by the deviation of the
ground truth data with MASE reshuffles model rankings compared to EM, revealing
gaps in models' understanding of temporal domain knowledge, especially when
trained with synthetic data. Lastly, the models' most frequent error is to
deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,
adequately weight these errors. Our findings underscore the need for
specialised metrics for temporal QA tasks. Code and data are available on
https://github.com/aauss/temporal-answer-qa.

</details>


### [257] [A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse](https://arxiv.org/abs/2509.16722)
*Xiaohan Ding,Kaike Ping,Buse Çarık,Eugenia Rho*

Main category: cs.CL

TL;DR: 本文提出并公开了CausalTalk数据集，专注于公共卫生相关的Reddit社交媒体帖子中的因果语言分析，涵盖显式和隐式因果、因果片段抽取及因果主旨生成等多层次任务。


<details>
  <summary>Details</summary>
Motivation: 现有因果理解数据主要集中在结构化文本和显式因果表达，对于社交媒体等非正式语境中的隐式因果研究严重不足，尤其是在新冠疫情等重大公共卫生话题下，准确把握社交媒体因果推理尤为重要。

Method: 作者收集并整理了2020–2024年间五年Reddit上与新冠疫情相关的帖子，从中人工标注10120条数据，涵盖四项因果任务（因果分类、显/隐因果判别、因果片段抽取及因果主旨生成），标注既来自领域专家，也包括经GPT-4o生成并人工审核的银标。

Result: CausalTalk数据集提供了细粒度、多维度的因果理解任务样本，支持对判别式和生成式模型的基准评测，为因果推理在社交媒体语境下的发展提供了新的数据基础。

Conclusion: 该数据集促进了因果推理方法在非正式语言环境下的研究与创新，有助于提升NLP模型对隐式因果与主旨信息的理解与生成能力，特别适用于社会媒体健康舆情分析等实际场景。

Abstract: Understanding causal language in informal discourse is a core yet
underexplored challenge in NLP. Existing datasets largely focus on explicit
causality in structured text, providing limited support for detecting implicit
causal expressions, particularly those found in informal, user-generated social
media posts. We introduce CausalTalk, a multi-level dataset of five years of
Reddit posts (2020-2024) discussing public health related to the COVID-19
pandemic, among which 10120 posts are annotated across four causal tasks: (1)
binary causal classification, (2) explicit vs. implicit causality, (3)
cause-effect span extraction, and (4) causal gist generation. Annotations
comprise both gold-standard labels created by domain experts and
silver-standard labels generated by GPT-4o and verified by human annotators.
CausalTalk bridges fine-grained causal detection and gist-based reasoning over
informal text. It enables benchmarking across both discriminative and
generative models, and provides a rich resource for studying causal reasoning
in social media contexts.

</details>


### [258] [Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation](https://arxiv.org/abs/2509.16729)
*Evgeniia Tokarchuk,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文提出通过增加神经网络隐藏状态的角度分散性（angular dispersion）来提升k-NN机器翻译的检索效率，并略微提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有k-NN机器翻译方法在翻译时需在大规模数据存储中检索相似语境，检索过程即使采用近似算法，仍存在计算和内存瓶颈。

Method: 该文提出鼓励神经隐藏状态表征在向量空间中的角度分散，从而优化数据结构平衡，提高检索性能。方法核心是调整训练目标，使隐藏状态在向量空间中分布得更均匀，便于高效检索。

Result: 结果表明，增加隐藏状态分散性可以实现检索过程加速，同时略微提升翻译质量。

Conclusion: 通过改进隐藏状态的角度分散性，能够缓解k-NN MT的数据检索瓶颈，不仅提高效率，还能对翻译效果有所提升。

Abstract: Augmenting neural machine translation with external memory at decoding time,
in the form of k-nearest neighbors machine translation ($k$-NN MT), is a
well-established strategy for increasing translation performance. $k$-NN MT
retrieves a set of tokens that occurred in the most similar contexts recorded
in a prepared data store, using hidden state representations of translation
contexts as vector lookup keys. One of the main disadvantages of this method is
the high computational cost and memory requirements. Since an exhaustive search
is not feasible in large data stores, practitioners commonly use approximate
$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to
research directions seeking to accelerate $k$-NN MT by reducing data store size
or the number of lookup calls, we pursue an orthogonal direction based on the
performance properties of approximate $k$-NN MT lookup data structures. In
particular, we propose to encourage angular dispersion of the neural hidden
representations of contexts. We show that improving dispersion leads to better
balance in the retrieval data structures, accelerating retrieval and slightly
improving translations.

</details>


### [259] [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
*Fagun Patel,Duc Q. Nguyen,Sang T. Truong,Jody Vaynshtok,Sanmi Koyejo,Nick Haber*

Main category: cs.CL

TL;DR: 本论文关注多模态语言模型（MLM）在语音语言病理学中的应用，开发了专门评测基准，并分析了当前模型的表现及其局限性。


<details>
  <summary>Details</summary>
Motivation: 美国有超340万儿童存在需要临床干预的语言障碍，而专业治疗师人数严重不足，迫切需要技术工具提升治疗效率。MLM虽然显示出潜力，但其在高风险临床应用中的表现尚未被充分探索。

Method: 作者与行业专家合作，建立MLM在语音障碍领域的实际应用分类体系，并据此开发首个包含五个核心场景、共5000条手工标注数据的基准测试集。研究评估了15种主流MLM在不同噪声、性别、口音背景下的鲁棒性与敏感性，并考察了链式思考（chain-of-thought）等提示方法及模型微调的效果。

Result: 评测发现：1）没有单一模型能在所有任务中表现优异；2）模型对男性说话者表现较好，存在系统性表现差异；3）链式思考方法在标签空间大、决策边界窄的任务中可能降低模型表现；4）在特定领域数据上微调MLM可带来30%以上的性能提升。

Conclusion: 当前MLM在语音病理学领域具有应用潜力，但也存在显著局限，需要继续针对性优化和深入研究以更好辅助实际临床工作。

Abstract: According to the U.S. National Institutes of Health, more than 3.4 million
children experience speech disorders that require clinical intervention. The
number of speech-language pathologists (SLPs) is roughly 20 times fewer than
the number of affected children, highlighting a significant gap in children's
care and a pressing need for technological support that improves the
productivity of SLPs. State-of-the-art multimodal language models (MLMs) show
promise for supporting SLPs, but their use remains underexplored largely due to
a limited understanding of their performance in high-stakes clinical settings.
To address this gap, we collaborate with domain experts to develop a taxonomy
of real-world use cases of MLMs in speech-language pathologies. Building on
this taxonomy, we introduce the first comprehensive benchmark for evaluating
MLM across five core use cases, each containing 1,000 manually annotated data
points. This benchmark includes robustness and sensitivity tests under various
settings, including background noise, speaker gender, and accent. Our
evaluation of 15 state-of-the-art MLMs reveals that no single model
consistently outperforms others across all tasks. Notably, we find systematic
disparities, with models performing better on male speakers, and observe that
chain-of-thought prompting can degrade performance on classification tasks with
large label spaces and narrow decision boundaries. Furthermore, we study
fine-tuning MLMs on domain-specific data, achieving improvements of over 30%
compared to base models. These findings highlight both the potential and
limitations of current MLMs for speech-language pathology applications,
underscoring the need for further research and targeted development.

</details>


### [260] [MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language](https://arxiv.org/abs/2509.16781)
*Andrei-Marius Avram,Ema-Ioana Bănescu,Anda-Teodora Robea,Dumitru-Clementin Cercel,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 本文提出了MoRoVoc数据集，这是目前最大用于分析罗马尼亚语口语区域变化的数据集，并提出了基于多目标对抗训练的新型语音建模方法，显著提升了口音识别和属性不变性能力。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语在不同地区（罗马尼亚和摩尔多瓦）存在显著的口语差异，且人口属性如年龄和性别也会影响语音表现。此前缺乏大规模且均衡的罗马尼亚语区域口音数据集，同时对于同步适应区域方言和说话人多属性的鲁棒建模方法研究不足。

Method: 收集并构建了包含93192条音频、时长93小时、涵盖两个地区、性别与年龄均衡的MoRoVoc数据集。提出基于对抗训练的多目标语音模型，将说话人年龄和性别引入对抗目标，通过元学习动态调整对抗系数，使模型兼具辨别主任务（如语音识别或方言判别）能力、且对次属性（如年龄、性别）具有不变性。采用了Wav2Vec2-Base和Wav2Vec2-Large模型进行实证。

Result: Wav2Vec2-Base在将性别作为对抗目标时，实现了78.21%的罗马尼亚语区域识别准确率。Wav2Vec2-Large在以方言和年龄作为对抗目标的性别分类任务上取得了93.08%的准确率。显示出提出方法在多属性对抗训练中的优越性。

Conclusion: MoRoVoc为区域罗马尼亚语口音研究提供了宝贵数据资源，所提多目标对抗训练方法能够有效提升语音模型对主任务的判别能力及对人口属性的泛化鲁棒性，对相关语音应用具有较大推动作用。

Abstract: This paper introduces MoRoVoc, the largest dataset for analyzing the regional
variation of spoken Romanian. It has more than 93 hours of audio and 88,192
audio samples, balanced between the Romanian language spoken in Romania and the
Republic of Moldova. We further propose a multi-target adversarial training
framework for speech models that incorporates demographic attributes (i.e., age
and gender of the speakers) as adversarial targets, making models
discriminative for primary tasks while remaining invariant to secondary
attributes. The adversarial coefficients are dynamically adjusted via
meta-learning to optimize performance. Our approach yields notable gains:
Wav2Vec2-Base achieves 78.21% accuracy for the variation identification of
spoken Romanian using gender as an adversarial target, while Wav2Vec2-Large
reaches 93.08% accuracy for gender classification when employing both dialect
and age as adversarial objectives.

</details>


### [261] [Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies](https://arxiv.org/abs/2509.16788)
*Salha Alyami,Amani Jamal,Areej Alhothali*

Main category: cs.CL

TL;DR: 本文提出了一种针对阿拉伯语产品评论的方面级情感分析的新方法，通过领域自适应预训练与高效的微调策略，实现了性能提升，但分析也揭示了模型和数据标注中的若干挑战。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方面级情感分析因缺乏标注数据而发展缓慢，现有预训练模型多基于事实性数据，导致在特定领域任务中表现有限。解决数据稀缺和模型适应性问题，提升阿拉伯语ABSA（方面级情感分析）性能，是研究的主要动机。

Method: 本文采用领域自适应预训练，将阿拉伯语上下文化语言模型在相关领域语料上进行再训练，并对比了特征提取、全量微调和adapter高效微调三种策略，在多个领域语料与模型上实验性能提升。

Result: 实验结果表明，领域自适应预训练可带来小幅性能提升，adapter微调方法在计算效率和效果上均表现良好。误差分析揭示了模型预测和数据标注上的一些问题，如情感极性误判、多词表达处理困难等。

Conclusion: 适应性预训练和高效微调可增强阿拉伯语方面级情感分析，但现有模型在理解句法语义、复杂关系等方面仍有不足，未来应探索如图卷积网络等更具句法和语义感知能力的模型。

Abstract: Aspect-based sentiment analysis (ABSA) in natural language processing enables
organizations to understand customer opinions on specific product aspects.
While deep learning models are widely used for English ABSA, their application
in Arabic is limited due to the scarcity of labeled data. Researchers have
attempted to tackle this issue by using pre-trained contextualized language
models such as BERT. However, these models are often based on fact-based data,
which can introduce bias in domain-specific tasks like ABSA. To our knowledge,
no studies have applied adaptive pre-training with Arabic contextualized models
for ABSA. This research proposes a novel approach using domain-adaptive
pre-training for aspect-sentiment classification (ASC) and opinion target
expression (OTE) extraction. We examine fine-tuning strategies - feature
extraction, full fine-tuning, and adapter-based methods - to enhance
performance and efficiency, utilizing multiple adaptation corpora and
contextualized models. Our results show that in-domain adaptive pre-training
yields modest improvements. Adapter-based fine-tuning is a computationally
efficient method that achieves competitive results. However, error analyses
reveal issues with model predictions and dataset labeling. In ASC, common
problems include incorrect sentiment labeling, misinterpretation of contrastive
markers, positivity bias for early terms, and challenges with conflicting
opinions and subword tokenization. For OTE, issues involve mislabeling targets,
confusion over syntactic roles, difficulty with multi-word expressions, and
reliance on shallow heuristics. These findings underscore the need for syntax-
and semantics-aware models, such as graph convolutional networks, to more
effectively capture long-distance relations and complex aspect-based opinion
alignments.

</details>


### [262] [KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis](https://arxiv.org/abs/2509.16804)
*Kozhin muhealddin Awlla,Hadi Veisi,Abdulhady Abas Abdullah*

Main category: cs.CL

TL;DR: 本文将BERT引入库尔德语情感分析，实现了比以往Word2Vec模型更强的语义捕捉能力，对低资源语言的自然语言处理有显著提升。


<details>
  <summary>Details</summary>
Motivation: 库尔德语作为低资源语言，因缺乏丰富的计算资源，情感分析十分困难。传统方法如Word2Vec效果有限，新一代模型BERT有望带来重大改进。

Method: 在情感分析任务中，用BERT替代传统的Word2Vec词向量，通过其强大的上下文建模能力，更好地理解和表示库尔德语文本。

Result: BERT在捕捉语义和上下文细节方面显著优于传统方法，有效提升了库尔德语情感分析的准确性。

Conclusion: BERT为低资源语言库尔德语的情感分析树立了新基线，对低资源语言的自然语言处理研究具有重要意义。

Abstract: This paper enhances the study of sentiment analysis for the Central Kurdish
language by integrating the Bidirectional Encoder Representations from
Transformers (BERT) into Natural Language Processing techniques. Kurdish is a
low-resourced language, having a high level of linguistic diversity with
minimal computational resources, making sentiment analysis somewhat
challenging. Earlier, this was done using a traditional word embedding model,
such as Word2Vec, but with the emergence of new language models, specifically
BERT, there is hope for improvements. The better word embedding capabilities of
BERT lend to this study, aiding in the capturing of the nuanced semantic pool
and the contextual intricacies of the language under study, the Kurdish
language, thus setting a new benchmark for sentiment analysis in low-resource
languages.

</details>


### [263] [Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text](https://arxiv.org/abs/2509.16813)
*Devin R. Wright,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法CLIFS，可自动衡量个体与群体等目标的身份融合程度，评估更高效、精准。


<details>
  <summary>Details</summary>
Motivation: 现有身份融合量表（问卷或图片）需人工参与、效率低且局限，难以大规模应用。自动化、可扩展评估方法亟需开发。

Method: 作者基于认知语言学与大型语言模型，结合隐喻检测技术，提出CLIFS指标，用于自动化、可扩展地量化身份融合。CLIFS无需问卷调查，自动处理文本信息，并与传统测量工具对齐。

Result: 实验表明，CLIFS在自动和人类注释基准上均优于现有方法。作为验证，CLIFS能将暴力风险评估提升240%以上。

Conclusion: CLIFS为身份融合测量提供了创新的NLP方法，并具备强扩展性。呼吁开发更大规模、更具多样性的多领域数据集以增强模型泛化能力，推动领域发展。

Abstract: Quantifying identity fusion -- the psychological merging of self with another
entity or abstract target (e.g., a religious group, political party, ideology,
value, brand, belief, etc.) -- is vital for understanding a wide range of
group-based human behaviors. We introduce the Cognitive Linguistic Identity
Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with
large language models (LLMs), which builds on implicit metaphor detection.
Unlike traditional pictorial and verbal scales, which require controlled
surveys or direct field contact, CLIFS delivers fully automated, scalable
assessments while maintaining strong alignment with the established verbal
measure. In benchmarks, CLIFS outperforms both existing automated approaches
and human annotation. As a proof of concept, we apply CLIFS to violence risk
assessment to demonstrate that it can improve violence risk assessment by more
than 240%. Building on our identification of a new NLP task and early success,
we underscore the need to develop larger, more diverse datasets that encompass
additional fusion-target domains and cultural backgrounds to enhance
generalizability and further advance this emerging area. CLIFS models and code
are public at https://github.com/DevinW-sudo/CLIFS.

</details>


### [264] [Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming](https://arxiv.org/abs/2509.16835)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出了一种基于语义的主题建模框架，能更高效地分析虚拟头脑风暴会议中的创意分布和深度。


<details>
  <summary>Details</summary>
Motivation: 虚拟头脑风暴产出大量且分布不均的想法，人工整理耗时且主观，因此需要自动化工具帮助评估团队创造力。

Method: 提出了一个由四个模块组成的主题建模框架：使用Sentence-BERT进行语义嵌入、UMAP降维、HDBSCAN聚类，以及主题提取与精炼。该方法能在句子级别捕捉主题语义，并过滤噪声、识别离群点。

Result: 在针对大学生小组Zoom头脑风暴数据集的实验中，该模型的主题一致性（0.687, CV）显著优于LDA、ETM和BERTopic，且对主题深度与多样性提供了更具解释性的洞见。

Conclusion: 该框架不仅提升了主题建模表现，还为分析创意协作提供了解释性和可扩展性工具，对研究同步虚拟会议下的创造力具有重要意义。

Abstract: Virtual brainstorming sessions have become a central component of
collaborative problem solving, yet the large volume and uneven distribution of
ideas often make it difficult to extract valuable insights efficiently. Manual
coding of ideas is time-consuming and subjective, underscoring the need for
automated approaches to support the evaluation of group creativity. In this
study, we propose a semantic-driven topic modeling framework that integrates
four modular components: transformer-based embeddings (Sentence-BERT),
dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction
with refinement. The framework captures semantic similarity at the sentence
level, enabling the discovery of coherent themes from brainstorming transcripts
while filtering noise and identifying outliers. We evaluate our approach on
structured Zoom brainstorming sessions involving student groups tasked with
improving their university. Results demonstrate that our model achieves higher
topic coherence compared to established methods such as LDA, ETM, and BERTopic,
with an average coherence score of 0.687 (CV), outperforming baselines by a
significant margin. Beyond improved performance, the model provides
interpretable insights into the depth and diversity of topics explored,
supporting both convergent and divergent dimensions of group creativity. This
work highlights the potential of embedding-based topic modeling for analyzing
collaborative ideation and contributes an efficient and scalable framework for
studying creativity in synchronous virtual meetings.

</details>


### [265] [Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment](https://arxiv.org/abs/2509.16876)
*Jiun-Ting Li,Bi-Cheng Yan,Yi-Cheng Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出多任务预训练（MTP）策略增强自动发音评估（APA），突破只依赖音素级特征带来的瓶颈，并首次将自动口语评估（ASA）与APA结合，实现更全面的能力测评。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有APA方法主要依赖音素级的发音特征，导致评估结果缺乏对超音段发音线索的捕捉，从而影响评估的全面性和准确性。此外，APA与ASA之间未深度融合，难以实现整体口语能力的评估。

Method: 提出多任务预训练（MTP）策略，通过在APA模型中随机遮蔽部分音段级发音特征，并利用周围的上下文重建这些特征，以捕捉更长时域的发音信息。同时结合手工特征（如流利度、重音等）与回归器输出可解释的能力分数，将APA与ASA有机结合，提升整体测评能力。

Result: 在speechocean762数据集上的实验表明，该方法提升了发音评分的精度，并与自动口语评估的能力水平高度相关；证明所提融合框架对提升训练针对性及能力测评全面性有效。

Conclusion: 该方法有效弥补了仅依赖音素层面特征的固有限制，实现了更为细致与全面的发音和能力评估，对后续自动化评测系统的研究与应用具有推动意义。

Abstract: Automatic pronunciation assessment (APA) analyzes second-language (L2)
learners' speech by providing fine-grained pronunciation feedback at various
linguistic levels. Most existing efforts on APA typically adopt segmental-level
features as inputs and predict pronunciation scores at different granularities
via hierarchical (or parallel) pronunciation modeling. This, however,
inevitably causes assessments across linguistic levels (e.g., phone, word, and
utterance) to rely solely on phoneme-level pronunciation features, nearly
sidelining supra-segmental pronunciation cues. To address this limitation, we
introduce multi-task pretraining (MTP) for APA, a simple yet effective strategy
that attempts to capture long-term temporal pronunciation cues while
strengthening the intrinsic structures within an utterance via the objective of
reconstructing input features. Specifically, for a phoneme-level encoder of an
APA model, the proposed MTP strategy randomly masks segmental-level
pronunciation features and reconstructs the masked ones based on their
surrounding pronunciation context. Furthermore, current APA systems lack
integration with automated speaking assessment (ASA), limiting holistic
proficiency evaluation. Drawing on empirical studies and prior knowledge in
ASA, our framework bridges this gap by incorporating handcrafted features
(HCFs), such as fluency (speech rate, silence duration) and stress (pitch
accent strength), derived from human-designed formulas via regressors to
generate interpretable proficiency scores. Experiments on speechocean762 show
improved pronunciation scoring and ASA proficiency correlation, enabling
targeted training and comprehensive proficiency assessment.

</details>


### [266] [Can GRPO Boost Complex Multimodal Table Understanding?](https://arxiv.org/abs/2509.16889)
*Xiaoqiang Kang,Shengen Wu,Zimu Wang,Yilin Liu,Xiaobo Jin,Kaizhu Huang,Wei Wang,Yutao Yue,Xiaowei Huang,Qiufeng Wang*

Main category: cs.CL

TL;DR: 本文提出Table-R1，一种三阶段强化学习框架，以提升多模态表格理解性能，显著优于传统监督微调（SFT）与现有强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法因表结构复杂和逻辑推理难度高而表现有限，现有强化学习方法也因初始策略精度低和奖励信号粗糙而难以取得理想效果，因此需要新方法提高表格理解准确性。

Method: Table-R1包括三阶段：（1）Warm-up阶段——提升模型初始感知与推理能力；（2）感知对齐GRPO（PA-GRPO）——采用Tree-Edit-Distance Similarity（TEDS）连续奖励，精确识别表格结构与内容；（3）提示补全GRPO（HC-GRPO）——基于细粒度奖励机制，使用提示信息提升推理准确性。

Result: 大量实验显示，Table-R1可显著提升表格推理表现，无论在训练集还是测试集均优于SFT和传统GRPO。Qwen2-VL-7B结合Table-R1的效果甚至优于体量更大的专用表格模型（如Table-LLaVA 13B），并在部分数据集上达到GPT-4o的水平。

Conclusion: Table-R1有效缓解了初始化瓶颈与奖励稀疏问题，推动了鲁棒的多模态表格理解技术发展。

Abstract: Existing table understanding methods face challenges due to complex table
structures and intricate logical reasoning. While supervised finetuning (SFT)
dominates existing research, reinforcement learning (RL), such as Group
Relative Policy Optimization (GRPO), has shown promise but struggled with low
initial policy accuracy and coarse rewards in tabular contexts. In this paper,
we introduce Table-R1, a three-stage RL framework that enhances multimodal
table understanding through: (1) Warm-up that prompts initial perception and
reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs
continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table
structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes
fine-grained rewards of residual steps based on the hint-guided question.
Extensive experiments demonstrate that Table-R1 can boost the model's table
reasoning performance obviously on both held-in and held-out datasets,
outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1
surpasses larger specific table understanding models (e.g., Table-LLaVA 13B),
even achieving comparable performance to the closed-source model GPT-4o on
held-in datasets, demonstrating the efficacy of each stage of Table-R1 in
overcoming initialization bottlenecks and reward sparsity, thereby advancing
robust multimodal table understanding.

</details>


### [267] [CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification](https://arxiv.org/abs/2509.16903)
*Nawar Turk,Daniele Comitogianni,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文针对DISRPT 2025多语种话语关系分类任务，用多种深度学习方法建立并比较了强基线，并提出了一种高效的新模型HiDAC。


<details>
  <summary>Details</summary>
Motivation: 该任务统一了6种话语框架下的17种话语关系标签，覆盖16种语言的39个语料库，使得跨语言和跨框架任务变得极具挑战。因此，迫切需要强有力的基线和高效的多语种方法。

Method: 研究者主要采用三种方法：(1) 微调mBERT、XLM-RoBERTa-base和XLM-RoBERTa-large三大预训练多语模型，并考查不同参数解冻策略；(2) 在零样本和小样本设定下，评估大语言模型（如Claude Opus 4.0）的提示学习能力；(3) 提出HiDAC，一种分层双适配器对比学习模型，力求提升参数效率与效果。

Result: 实验发现，大型transformer模型在准确率上表现更好，但提升有限。解冻75%顶层编码器层即可达到全量微调的效果，但参数更新更少。提示型大语言模型显著落后于微调模型。HiDAC模型以67.5%的准确率取得最佳成绩，并且比全量微调更高效。

Conclusion: 微调多语种预训练模型仍然优于当前大语言模型提示学习。HiDAC模型兼顾了准确率和参数效率，代表了多语种话语关系分类任务的有效方法。

Abstract: We present our submission to Task 3 (Discourse Relation Classification) of
the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse
relation labels across 39 corpora in 16 languages and six discourse frameworks,
posing significant multilingual and cross-formalism challenges. We first
benchmark the task by fine-tuning multilingual BERT-based models (mBERT,
XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies
and progressive unfreezing ratios to establish strong baselines. We then
evaluate prompt-based large language models (namely Claude Opus 4.0) in
zero-shot and few-shot settings to understand how LLMs respond to the newly
proposed unified labels. Finally, we introduce HiDAC, a Hierarchical
Dual-Adapter Contrastive learning model. Results show that while larger
transformer models achieve higher accuracy, the improvements are modest, and
that unfreezing the top 75% of encoder layers yields performance comparable to
full fine-tuning while training far fewer parameters. Prompt-based models lag
significantly behind fine-tuned transformers, and HiDAC achieves the highest
overall accuracy (67.5%) while remaining more parameter-efficient than full
fine-tuning.

</details>


### [268] [CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages](https://arxiv.org/abs/2509.16914)
*Wenhao Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 本文提出了CUTE数据集，通过增强基础数据资源，显著提升大语言模型在低资源语言的处理能力，尤其是维吾尔语和藏语。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在丰富资源的语言（如中文和英文）任务中表现出色，但对于低资源语言（如维吾尔语和藏语）的支持仍然严重不足，主要由于缺乏高质量训练语料。

Method: 作者构建了CUTE多语种语料库，包含中文、维吾尔语、藏语和英语两套25GB的大型四语种数据（平行和非平行），语料由机器翻译生成，并经过人工评估确保中-维、中-藏翻译质量接近中-英；数据和相关模型均开源。

Result: CUTE成为了至今最大规模的维吾尔语和藏语开源语料库，实验显示该数据集显著增强了LLM对低资源语言的处理能力，并系统探究了语料库平行性对跨语言迁移学习的影响。

Conclusion: CUTE语料库为低资源语言提供了前所未有的数据资源，有效促进了大模型泛化能力及跨语种处理水平，对相关研究领域具有重要意义。

Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities
in various NLP tasks, significantly enhancing user experience and efficiency.
However, this advantage is primarily limited to resource-rich languages. For
the diverse array of low-resource languages, support remains inadequate, with
the scarcity of training corpora considered the primary cause. We construct and
open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two
25GB sets of four-language corpora (one parallel and one non-parallel),
obtained through machine translation. CUTE encompasses two resource-rich
languages (Chinese and English) and two low-resource languages (Uyghur and
Tibetan). Prior to constructing CUTE, human assessment validates that the
machine translation quality between Chinese-Uyghur and Chinese-Tibetan
approaches that of Chinese-English translation. CUTE represents the largest
open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate
its effectiveness in enhancing LLMs' ability to process low-resource languages
while investigating the role of corpus parallelism in cross-lingual transfer
learning. The CUTE corpus and related models are made publicly available to the
research community.

</details>


### [269] [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
*Yongrui Chen,Yi Huang,Yunchang Liu,Shenyu Zhang,Junhao He,Tongtong Wu,Guilin Qi,Tianxing Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为K-DeCore的新框架，有效提升了模型在持续结构化知识推理（CSKR）任务中的泛化和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前的持续学习方法在面向结构化知识推理时，存在对异构知识泛化能力差以及随任务增多参数膨胀、推理低效等问题，亟需新方法解决。

Method: 提出K-DeCore框架，使用固定参数量，通过知识解耦，将推理过程切分为任务相关和任务无关两阶段，并采用双视角记忆巩固与结构引导伪数据合成策略提升泛化。

Result: 在四个基准数据集上，K-DeCore在多项指标下都优于现有方法，并能适配多种大型语言模型骨干。

Conclusion: K-DeCore为结构化知识推理中的持续学习带来了性能与效率的双重提升，是解决相关问题的一种有效方法。

Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to
handle sequential tasks, where each task involves translating natural language
questions into structured queries grounded in structured knowledge. Existing
general continual learning approaches face significant challenges when applied
to this task, including poor generalization to heterogeneous structured
knowledge and inefficient reasoning due to parameter growth as tasks increase.
To address these limitations, we propose a novel CSKR framework,
\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.
Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling
mechanism that disentangles the reasoning process into task-specific and
task-agnostic stages, effectively bridging the gaps across diverse tasks.
Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective
memory consolidation mechanism for distinct stages and introduces a
structure-guided pseudo-data synthesis strategy to further enhance the model's
generalization capabilities. Extensive experiments on four benchmark datasets
demonstrate the superiority of \textsc{K-DeCore} over existing continual
learning methods across multiple metrics, leveraging various backbone large
language models.

</details>


### [270] [AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation](https://arxiv.org/abs/2509.16952)
*Tiancheng Huang,Ruisheng Cao,Yuxin Zhang,Zhangyi Kang,Zijian Wang,Chenrun Wang,Yijie Luo,Hang Zheng,Lirong Qian,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 本文提出了AirQA数据集和ExTrActor自动化指令数据合成框架，旨在提升LLM（大语言模型）在科学论文QA任务中的能力评估及交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前科学论文数量激增，研究者难以高效获取关键信息。虽然LLM已可自动化论文问答，但缺乏严谨的评测基准和高质量交互轨迹，限制了专用智能体的训练与评估。

Method: 作者提出了AirQA数据集，包括13,948篇AI论文和1,246个人工标注问题，覆盖多任务、多模态和实例级评测。同时提出ExTrActor框架，可利用三个基于LLM的智能体实现指令数据的自动生成与交互轨迹收集，无需人工介入。

Result: 实验表明，现有多个大模型（开源及商用）在AirQA上的表现不佳，证明数据集挑战性较高。广泛实验发现，ExTrActor提升了小模型多轮工具使用能力，使其表现接近大型模型。

Conclusion: AirQA数据集可作为科学论文问答领域新的高质量评测基准，ExTrActor则推动了交互型智能体能力提升。此工作为科学文献智能问答系统的发展提供了坚实的数据与方法基础。

Abstract: The growing volume of academic papers has made it increasingly difficult for
researchers to efficiently extract key information. While large language models
(LLMs) based agents are capable of automating question answering (QA) workflows
for scientific papers, there still lacks a comprehensive and realistic
benchmark to evaluate their capabilities. Moreover, training an interactive
agent for this specific task is hindered by the shortage of high-quality
interaction trajectories. In this work, we propose AirQA, a human-annotated
comprehensive paper QA dataset in the field of artificial intelligence (AI),
with 13,948 papers and 1,246 questions, that encompasses multi-task,
multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor,
an automated framework for instruction data synthesis. With three LLM-based
agents, ExTrActor can perform example generation and trajectory collection
without human intervention. Evaluations of multiple open-source and proprietary
models show that most models underperform on AirQA, demonstrating the quality
of our dataset. Extensive experiments confirm that ExTrActor consistently
improves the multi-turn tool-use capability of small models, enabling them to
achieve performance comparable to larger ones.

</details>


### [271] [Preference Distillation via Value based Reinforcement Learning](https://arxiv.org/abs/2509.16965)
*Minchan Kwon,Junwon Ko,Kangil Kim,Junmo Kim*

Main category: cs.CL

TL;DR: 本文提出一种新的知识蒸馏方法TVKD，结合教师模型的价值函数奖励，有效提升DPO在小模型上的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化(DPO)虽然已被用于通过二元偏好对齐语言模型与人类偏好，但对于小模型来说，仅靠胜负监督激励不足，难以获得足够指示信号。现有通过行为克隆或KL散度来模仿大模型行为的方法忽略了奖励建模的蒸馏。本文试图弥补这一不足。

Method: 提出Teacher Value-based Knowledge Distillation (TVKD)方法，即引入来源于教师模型价值函数的辅助奖励作为软引导，辅以基于潜势的奖励塑形，保证奖励结构和最优策略与DPO一致，并可无缝集成至DPO框架，无需额外采样。

Result: 实验结果显示，TVKD能在多个基准测试和不同模型规模下持续提升小模型的表现。

Conclusion: TVKD为增强小模型对人类偏好对齐能力提供了一种高效方案，补足了现有方法的不足，保证DPO原有优势且带来性能增益。

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm to align language
models with human preferences using pairwise comparisons. However, its binary
win-or-loss supervision often proves insufficient for training small models
with limited capacity. Prior works attempt to distill information from large
teacher models using behavior cloning or KL divergence. These methods often
focus on mimicking current behavior and overlook distilling reward modeling. To
address this issue, we propose \textit{Teacher Value-based Knowledge
Distillation} (TVKD), which introduces an auxiliary reward from the value
function of the teacher model to provide a soft guide. This auxiliary reward is
formulated to satisfy potential-based reward shaping, ensuring that the global
reward structure and optimal policy of DPO are preserved. TVKD can be
integrated into the standard DPO training framework and does not require
additional rollouts. Our experimental results show that TVKD consistently
improves performance across various benchmarks and model sizes.

</details>


### [272] [Advancing Speech Understanding in Speech-Aware Language Models with GRPO](https://arxiv.org/abs/2509.16990)
*Avishai Elmakies,Hagai Aronowitz,Nimrod Shabtay,Eli Schwartz,Ron Hoory,Avihu Dekel*

Main category: cs.CL

TL;DR: 本文提出了一种基于Group Relative Policy Optimization (GRPO)的方法，用于训练语音感知大语言模型（SALLMs），显著提升在开放式语音理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有SALLMs虽然在语音理解任务上表现优异，但如何在复杂的开放式任务（如口语问答、语音翻译）中进一步提升其生成能力，是当前研究亟需解决的问题。同时，GRPO在训练LLMs中已显示效率优势，但在SALLMs及开放式任务中的应用仍有待拓展。

Method: 作者提出将GRPO与BLEU分数结合作为奖励信号，优化SALLMs在开放式语音理解任务（如Spoken QA与自动语音翻译）下的生成能力。同时，探索将off-policy样本引入GRPO中，以提升模型训练效果。

Result: 实验结果表明，该方法在多个关键指标上优于标准的监督微调(SFT)方法。

Conclusion: 该工作证明使用GRPO优化SALLMs在开放式语音理解任务中的有效性，并指出将off-policy样本融入GRPO的潜力，为后续提升SALLMs性能以及相关研究提供了新方向。

Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based
method for training Speech-Aware Large Language Models (SALLMs) on open-format
speech understanding tasks, such as Spoken Question Answering and Automatic
Speech Translation. SALLMs have proven highly effective for speech
understanding tasks. GRPO has recently gained traction for its efficiency in
training LLMs, and prior work has explored its application to SALLMs, primarily
in multiple-choice tasks. Building on this, we focus on open-format tasks that
better reflect the generative abilities of the models. Our approach leverages
GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate
empirically that it surpasses standard SFT across several key metrics. Finally,
we explore the potential of incorporating off-policy samples within GRPO for
these tasks, highlighting avenues for further improvement and further research.

</details>


### [273] [The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs](https://arxiv.org/abs/2509.17030)
*Hinata Tezuka,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出并验证了“转移神经元假设”，即大型多语言语言模型中某些神经元负责不同语言潜在空间与共享语义空间之间的信息转移，这些神经元对模型的推理能力至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然近期研究提出了多语输入在解码器式大模型内的处理框架，但关于不同语言潜在空间之间的表示变换机制尚缺乏深入探究。本文旨在揭示支持这种语言间信息转换的内部机制。

Method: 作者提出了“转移神经元假设”，假定在MLP模块中存在专门负责不同语言潜在空间与共享语义空间之间转化的神经元。通过实证实验，分析这些神经元在表示变换及推理过程中的作用，并与近期发现的语言特定神经元职能进行比对。

Result: 实验证明了MLP模块中确实存在关键的“转移神经元”。这些神经元能够实现语言特定空间与共享空间之间的表示转移，并且对多语推理能力至关重要。此外，语言特定神经元的一项功能也被证实是促进潜在空间间的转换。

Conclusion: 本文深度揭示了多语大模型内部潜在空间转换的机制，强调了转移神经元在多语言推理中的关键作用，有助于优化和设计更强大的多语LLM。

Abstract: Recent studies have suggested a processing framework for multilingual inputs
in decoder-based LLMs: early layers convert inputs into English-centric and
language-agnostic representations; middle layers perform reasoning within an
English-centric latent space; and final layers generate outputs by transforming
these representations back into language-specific latent spaces. However, the
internal dynamics of such transformation and the underlying mechanism remain
underexplored. Towards a deeper understanding of this framework, we propose and
empirically validate The Transfer Neurons Hypothesis: certain neurons in the
MLP module are responsible for transferring representations between
language-specific latent spaces and a shared semantic latent space.
Furthermore, we show that one function of language-specific neurons, as
identified in recent studies, is to facilitate movement between latent spaces.
Finally, we show that transfer neurons are critical for reasoning in
multilingual LLMs.

</details>


### [274] [Modeling Bottom-up Information Quality during Language Processing](https://arxiv.org/abs/2509.17047)
*Cui Ding,Yanning Yin,Lena A. Jäger,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本论文研究了阅读过程中自下而上信息质量对理解难易度的影响，并用信息论和贝叶斯模型进行建模和实证。


<details>
  <summary>Details</summary>
Motivation: 当前语言处理理论认为理解包括自上而下的预期和自下而上的输入，但尚缺乏对自下而上输入“质量”与理解之间关系的严密实证和定量模型。

Method: 作者用互信息（MI）定义自下而上视觉信息与词汇身份之间的关系，并以贝叶斯模型形式化。在实验中，采用遮盖英文和中文单词的上半部或下半部，比较不同遮挡条件下的阅读时间，并用多模态语言模型评估视觉信息与词的互信息。

Result: 实验发现：英文和中文单词的上半部包含的信息量普遍大于下半部，但英文的这种不对称更为明显，且这种信息分布与阅读时间的变化一致。

Conclusion: 自下而上视觉输入的质量对阅读负荷具有可量化和预测作用，且信息分布模式因语言不同而异，支持了集成模型关于输入质量调节处理难度的预测。

Abstract: Contemporary theories model language processing as integrating both top-down
expectations and bottom-up inputs. One major prediction of such models is that
the quality of the bottom-up inputs modulates ease of processing -- noisy
inputs should lead to difficult and effortful comprehension. We test this
prediction in the domain of reading. First, we propose an information-theoretic
operationalization for the "quality" of bottom-up information as the mutual
information (MI) between visual information and word identity. We formalize
this prediction in a mathematical model of reading as a Bayesian update.
Second, we test our operationalization by comparing participants' reading times
in conditions where words' information quality has been reduced, either by
occluding their top or bottom half, with full words. We collect data in English
and Chinese. We then use multimodal language models to estimate the mutual
information between visual inputs and words. We use these data to estimate the
specific effect of reduced information quality on reading times. Finally, we
compare how information is distributed across visual forms. In English and
Chinese, the upper half contains more information about word identity than the
lower half. However, the asymmetry is more pronounced in English, a pattern
which is reflected in the reading times.

</details>


### [275] [TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?](https://arxiv.org/abs/2509.17054)
*Yiwei Liu,Emma Jane Pretty,Jiahao Huang,Saku Sugawara*

Main category: cs.CL

TL;DR: 本文提出了TactfulToM基准，用于评估大语言模型对“体贴性谎言”（white lies）的理解与推理能力，并发现现有模型在此任务上远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注语言模型在理论心理学（ToM）推理任务上的能力，但对于需要复杂社会情境理解的ToM能力，如“体贴性谎言”，相关研究有限。因此，作者旨在评估和推动LLMs在更细致真实社交推理中的表现。

Method: 作者设计了一个新的英文基准TactfulToM，通过人工设计种子故事，并结合人类协作和LLM扩展形成对话，维护对称性缺失，以真实反映white lies的本质。此基准用来测试模型对日常对话中white lies及其出于体贴动机背后的推理能力。

Result: 实验结果显示，最新的SOTA大语言模型在TactfulToM任务上的表现远低于人类，难以全面理解和推理white lies背后的理论心理学机制。

Conclusion: 目前的大语言模型在真实社交推理场景中的体贴性谎言理解上存在明显短板，难以像人类一样把握其ToM推理能力，需要进一步改进。

Abstract: While recent studies explore Large Language Models' (LLMs) performance on
Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require
more nuanced social context is limited, such as white lies. We introduce
TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to
understand white lies within real-life conversations and reason about prosocial
motivations behind them, particularly when they are used to spare others'
feelings and maintain social harmony. Our benchmark is generated through a
multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed
stories into conversations to maintain the information asymmetry between
participants necessary for authentic white lies. We show that TactfulToM is
challenging for state-of-the-art models, which perform substantially below
humans, revealing shortcomings in their ability to fully comprehend the ToM
reasoning that enables true understanding of white lies.

</details>


### [276] [SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis](https://arxiv.org/abs/2509.17167)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Joseph Skrovan,Mehak Beri,Hitakshi Modi,Andrew Well,Liu Leqi,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: 本文提出了一种自动化主题分析（TA）框架SFT-TA，通过在多智能体系统中嵌入监督微调（SFT）智能体，实现更高效且贴近人工结果的访谈文本主题分析。


<details>
  <summary>Details</summary>
Motivation: 人工主题分析效率低且难以扩展，现有大型语言模型自动化结果与人工结果的偏差较大。

Method: 提出SFT-TA框架，将经过监督微调的智能体置于多智能体系统的不同角色，共同完成主题抽取和分析任务。

Result: SFT-TA框架在人类参考主题对齐度上超越现有方法和gpt-4o基线；单独SFT智能体效果一般，但在多智能体系统中得到提升。

Conclusion: 将SFT智能体嵌入多智能体系统，有效提升自动主题分析与人工结果的对齐度，是未来主题分析研究的有前景方向。

Abstract: Thematic Analysis (TA) is a widely used qualitative method that provides a
structured yet flexible framework for identifying and reporting patterns in
clinical interview transcripts. However, manual thematic analysis is
time-consuming and limits scalability. Recent advances in LLMs offer a pathway
to automate thematic analysis, but alignment with human results remains
limited. To address these limitations, we propose SFT-TA, an automated thematic
analysis framework that embeds supervised fine-tuned (SFT) agents within a
multi-agent system. Our framework outperforms existing frameworks and the
gpt-4o baseline in alignment with human reference themes. We observed that SFT
agents alone may underperform, but achieve better results than the baseline
when embedded within a multi-agent system. Our results highlight that embedding
SFT agents in specific roles within a multi-agent system is a promising pathway
to improve alignment with desired outputs for thematic analysis.

</details>


### [277] [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177)
*Bowen Qin,Chen Yue,Fang Yin,Hui Wang,JG Yao,Jiakang Liu,Jing-Shu Zheng,Miguel Hu Chen,Richeng Xuan,Shibei Meng,Shiqi Zhou,Teng Dai,Tong-Shuai Ren,Wei Cui,Xi Yang,Xialin Du,Xiaojing Xu,Xue Sun,Xuejing Li,Yaming Liu,Yesheng Liu,Ying Liu,Yonghua Lin,Yu Zhao,Yunduo Zhang,Yuwen Luo,Zheqi He,Zhiyuan He,Zhongyuan Wang*

Main category: cs.CL

TL;DR: 本论文对现有大型推理模型（LRMs）进行了适度规模且较为无污染的评估，并发布了适用于视觉语言模型推理能力测试的ROM E评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言大模型在复杂推理任务上的真实能力尚待全面评估，现有评测往往存在数据污染或者规模受限的问题，因此需要更可靠、无污染的评测基准。

Method: 作者搭建了一个适度规模、较为去污染的评估流程，对现有大型推理模型进行了测试。同时，提出并发布了ROM E这个面向视觉线索推理的评测基准。

Result: 基于ROM E评测基准，对现有视觉语言大型推理模型进行了系统性测试，获得了一些初步发现与结果。

Conclusion: 本研究为评估视觉语言大型推理模型提供了更加可靠的基准和初步评测结果，对未来模型的开发和评估提供了参考和指导。

Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of
current large reasoning models (LRMs) with some preliminary findings. We also
release ROME, our evaluation benchmark for vision language models intended to
test reasoning from visual clues. We attach links to the benchmark, evaluation
data, and other updates on this website:
https://flageval-baai.github.io/LRM-Eval/

</details>


### [278] [Attention Consistency for LLMs Explanation](https://arxiv.org/abs/2509.17178)
*Tian Lan,Jinyuan Xu,Xue He,Jenq-Neng Hwang,Lei Li*

Main category: cs.CL

TL;DR: 提出了一种新的注意力机制解释方法MACS，用于在保证解释质量的同时大幅降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型可解释性方法存在分辨率低、计算成本高的问题，影响了解模型和实际部署的效率与可信度。

Method: 提出了Multi-Layer Attention Consistency Score (MACS)，通过跨层最大注意力一致性来估计输入token的重要性，这是一种轻量且易于部署的启发式方法。

Result: 实验证明，MACS在维持对模型决策解释的信度同时，显著降低了22%的显存使用和30%的延迟，相较于复杂方法更高效。

Conclusion: MACS为大语言模型提供了可部署、计算友好的解释方式，兼顾了解释性和效率，有望促进大模型安全可信地落地。

Abstract: Understanding the decision-making processes of large language models (LLMs)
is essential for their trustworthy development and deployment. However, current
interpretability methods often face challenges such as low resolution and high
computational cost. To address these limitations, we propose the
\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,
and easily deployable heuristic for estimating the importance of input tokens
in decoder-based models. MACS measures contributions of input tokens based on
the consistency of maximal attention. Empirical evaluations demonstrate that
MACS achieves a favorable trade-off between interpretability quality and
computational efficiency, showing faithfulness comparable to complex techniques
with a 22\% decrease in VRAM usage and 30\% reduction in latency.

</details>


### [279] [LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](https://arxiv.org/abs/2509.17183)
*Junsong Li,Jie Zhou,Bihao Zhan,Yutao Yang,Qianjun Pan,Shilian Chen,Tianyu Huai,Xin Li,Qin Chen,Liang He*

Main category: cs.CL

TL;DR: 本文提出LifeAlign框架，实现大语言模型(LLMs)在多任务、顺序学习中的终身对齐，能持续符合人类偏好且避免知识遗忘；方法在多个任务上实验效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在对新任务/领域对齐人类偏好时，常常出现灾难性遗忘，导致之前习得的知识丧失。如何让LLM在多个任务和偏好类型中持续对齐且保持原有知识，是该研究的主要动因。

Method: 1. 提出聚焦偏好优化策略——精准地对齐LLM到新的人类偏好，防止旧知识流失；2. 设计短-长时记忆固化机制——用内在维度缩减把去噪后的短时偏好记忆整合到稳定的长期记忆，实现各领域对齐模式的高效存储和检索。

Result: 实验覆盖多个顺序对齐任务、不同领域与偏好类型。LifeAlign在保持偏好对齐质量和知识记忆方面，效果明显优于现有终身学习方法。

Conclusion: LifeAlign框架可有效解决LLM在多任务持续对齐中的遗忘问题，兼顾对新旧偏好高质量对齐和知识保持，展现出推广应用潜力。

Abstract: Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.

</details>


### [280] [Evolution of Concepts in Language Model Pre-Training](https://arxiv.org/abs/2509.17196)
*Xuyang Ge,Wentao Shu,Jiaxing Wu,Yunhua Zhou,Zhengfu He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文利用稀疏字典学习方法crosscoders，追踪了大型语言模型预训练过程中可解释特征的演化，揭示了不同阶段特征形成规律及其与下游性能的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在预训练后展现出强大的能力，但预训练过程本身仍然像个黑箱。作者希望通过研究特征在预训练过程中如何演化，揭示内部学习机制，提高对模型过程的理解和可解释性。

Method: 作者提出使用crosscoders（一种稀疏字典学习方法）分析多个预训练快照，追踪线性可解释特征的演化。同时通过特征归因分析验证这些特征变化与模型下游表现的因果关系。

Result: 发现大多数学到的特征在特定节点开始形成，随后随着训练深入，更多复杂模式逐渐出现。特征归因实验揭示了特征演化和下游性能间的因果机制，这些现象与已知的Transformer二阶段学习过程高度一致，即统计学习阶段和特征学习阶段。

Conclusion: 本研究证明了可以用细粒度特征来追踪大模型的学习进展，为理解和分析预训练动态提供了新思路，有助于揭示大型语言模型的内部机制。

Abstract: Language models obtain extensive capabilities through pre-training. However,
the pre-training process remains a black box. In this work, we track linear
interpretable feature evolution across pre-training snapshots using a sparse
dictionary learning method called crosscoders. We find that most features begin
to form around a specific point, while more complex patterns emerge in later
training stages. Feature attribution analyses reveal causal connections between
feature evolution and downstream performance. Our feature-level observations
are highly consistent with previous findings on Transformer's two-stage
learning process, which we term a statistical learning phase and a feature
learning phase. Our work opens up the possibility to track fine-grained
representation progress during language model learning dynamics.

</details>


### [281] [Prompt-Based Simplification for Plain Language using Spanish Language Models](https://arxiv.org/abs/2509.17209)
*Lourdes Moreno,Jesus M. Sanchez-Gomez,Marco Antonio Sanchez-Escudero,Paloma Martínez*

Main category: cs.CL

TL;DR: 本文介绍了HULAT-UC3M团队在CLEARS 2025西班牙语任务中的系统，基于模型微调和提示工程实现了文本简化，最终方案在语义相似性指标上排名第一，但在可读性方面排名第四。


<details>
  <summary>Details</summary>
Motivation: 推动自动化西班牙语文本简化，提高阅读理解，对有阅读障碍或低素养人群尤为重要。

Method: 尝试零样本提示工程和基于LoRA方法的微调，基于RigoChat-7B-v2模型，结合规范化和专门设计的提示，对不同策略进行评估并根据官方指标（语义相似性与可读性）选择最佳方案。

Result: 最终系统在语义相似性指标上排名第一（SIM=0.75），但可读性指数上排名第四（FH=69.72）。

Conclusion: 当前方法可在保持内容一致性的前提下优化语义相似性，但在可读性和评测指标的局限性仍存在挑战。

Abstract: This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask
1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies
based on models trained on Spanish texts, including a zero-shot configuration
using prompt engineering and a fine-tuned version with Low-Rank Adaptation
(LoRA). Different strategies were evaluated on representative internal subsets
of the training data, using the official task metrics, cosine similarity (SIM)
and the Fern\'andez-Huerta readability index (FH) to guide the selection of the
optimal model and prompt combination. The final system was selected for its
balanced and consistent performance, combining normalization steps, the
RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in
semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).
We also discuss key challenges related to training data heterogeneity and the
limitations of current evaluation metrics in capturing both linguistic clarity
and content preservation.

</details>


### [282] [Extending Automatic Machine Translation Evaluation to Book-Length Documents](https://arxiv.org/abs/2509.17249)
*Kuang-Da Wang,Shuoyang Ding,Chao-Han Huck Yang,Ping-Chun Hsieh,Wen-Chih Peng,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文提出了一种面向长文档机器翻译评价的新方法（SEGALE），实现了对大语言模型长文档翻译结果的自动化评估，解决了以往评估方式受限于句子级别和句界问题的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长文档翻译中表现优异，但传统机器翻译评估主要限于句子层级，无法有效评价长文档的翻译质量，受限于数据集规模、自动评测指标token限制以及句界要求。

Method: SEGALE通过把待评估文档视为连续文本，采用句子分割和对齐算法，将现有主流自动化评测指标延伸到长文档层级，允许评价任意长度文档的翻译，自动处理漏译、增译和句界不统一等实际问题。

Result: 实验证明，SEGALE在长文档评估上明显优于现有方法，效果接近于人工对齐下的评价标准；在评测开源LLM实际长文档翻译能力时，发现不少模型在标称最大上下文长度下的表现并不理想。

Conclusion: SEGALE为长文档机器翻译质量评价提供了可靠自动化方案，有助于推动大语言模型翻译能力的深入分析和实践应用。

Abstract: Despite Large Language Models (LLMs) demonstrating superior translation
performance and long-context capabilities, evaluation methodologies remain
constrained to sentence-level assessment due to dataset limitations, token
number restrictions in metrics, and rigid sentence boundary requirements. We
introduce SEGALE, an evaluation scheme that extends existing automatic metrics
to long-document translation by treating documents as continuous text and
applying sentence segmentation and alignment methods. Our approach enables
previously unattainable document-level evaluation, handling translations of
arbitrary length generated with document-level prompts while accounting for
under-/over-translations and varied sentence boundaries. Experiments show our
scheme significantly outperforms existing long-form document evaluation
schemes, while being comparable to evaluations performed with groundtruth
sentence alignments. Additionally, we apply our scheme to book-length texts and
newly demonstrate that many open-weight LLMs fail to effectively translate
documents at their reported maximum context lengths.

</details>


### [283] [Probabilistic Token Alignment for Large Language Model Fusion](https://arxiv.org/abs/2509.17276)
*Runjia Zeng,James Chenhao Liang,Cheng Han,Zhiwen Cao,Jiahao Liu,Xiaojun Quan,Yingjie Victor Chen,Lifu Huang,Tong Geng,Qifan Wang,Dongfang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的概率式token对齐方法（PTA-LLM），用于更有效地融合不同架构的大语言模型，克服了传统手工词表对齐方法的局限，实验表明该方法能提升模型综合能力。


<details>
  <summary>Details</summary>
Motivation: 训练全新大语言模型成本高且常有能力重叠，通过融合不同预训练 LLM 能更经济地获得更强的模型，但现有融合方法对手工预定义词表对齐高度依赖，泛化性差、影响性能。

Method: 受分布学习启发，提出概率token对齐方法（PTA-LLM），将词对齐问题转化为最优传输数学问题，通过软映射策略和分布感知的方式实现模型融合，提升泛化性和可解释性。

Result: 实验表明，PTA-LLM方法能在多项能力评测中有效提升目标模型的表现，优于传统手工词表对齐的融合技术。

Conclusion: PTA-LLM为不同架构模型融合提供了一种通用且有效的新途径，不仅提升性能，还具备分布可解释性，为未来模型融合研究提供参考。

Abstract: Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

</details>


### [284] [Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://arxiv.org/abs/2509.17289)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Krishna Dwarampudi,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了CoDe-KG，一个开源的端到端句级知识图谱抽取管道，结合了共指消解和句法分解技术，并开源了丰富的数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱抽取方法在处理句子复杂性与共指消解方面依然存在瓶颈，特别是在医学等专业领域数据丰富但表达复杂的情况下，亟需更精准且自动化的处理管道。

Method: 作者设计了一个结合共指消解和句法句子分解的知识抽取管道，并针对句子复杂度、共指消解及句子转换策略等环节制作了多个人工标注数据集。通过系统测试不同复杂度的提示工程和模型，验证最佳方案。

Result: CoDe-KG模型在句子简化任务上获得高达99.8%的准确率，在REBEL数据上的关系抽取任务宏F1达到65.8%，比现有SOTA提升8个百分点；WebNLG2微F1为75.7%，在Wiki-NRE和CaRB上表现亦优异。消融实验显示，共指消解和分解集成显著提升了对稀有关系的召回率20%以上。

Conclusion: CoDe-KG有效提升了句级知识抽取的准确性和对复杂语句、稀有关系的处理能力，兼具实用性和开源价值。

Abstract: We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting
sentence-level knowledge graphs by combining robust coreference resolution with
syntactic sentence decomposition. Using our model, we contribute a dataset of
over 150,000 knowledge triples, which is open source. We also contribute a
training corpus of 7248 rows for sentence complexity, 190 rows of gold human
annotations for co-reference resolution using open source lung-cancer abstracts
from PubMed, 900 rows of gold human annotations for sentence conversion
policies, and 398 triples of gold human annotations. We systematically select
optimal prompt-model pairs across five complexity categories, showing that
hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match
accuracy on sentence simplification. On relation extraction (RE), our pipeline
achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the
art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on
Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference
and decomposition increases recall on rare relations by over 20%. Code and
dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025

</details>


### [285] [Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection](https://arxiv.org/abs/2509.17292)
*Jun Seo Kim,Hyemi Kim,Woo Joo Oh,Hongjin Cho,Hochul Lee,Hye Hyeon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLMs）与多实例学习（MIL）框架，用于自动检测心理认知扭曲，并应用于韩文和英文数据集。该方法将文本分解为情绪、逻辑和行为三部分，提高了对复杂扭曲类型的识别效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 心理认知扭曲广泛存在于多种心理健康障碍中，其自动识别因语境歧义、共现与语义重叠而十分困难。目前缺乏对表达级推理和高可解释性的检测方法。

Method: 作者提出用LLMs分解每句话为情绪、逻辑、行为（ELB）三个成分，分别推理多种扭曲实例，并为每个实例分配类型、表达和重要性得分。之后通过多视图门控注意力机制整合这些信息进行最终分类。方法在韩文和英文数据集上进行了实验验证。

Result: 实验显示，引入ELB成分与模型推理的重要性得分能够提升分类表现，特别对具高度解释歧义的扭曲类型表现显著提升。

Conclusion: 该方法为心理健康领域的自然语言处理提供了一种心理学基础扎实且具可泛化性的细粒度推理新方案。

Abstract: Cognitive distortions have been closely linked to mental health disorders,
yet their automatic detection remained challenging due to contextual ambiguity,
co-occurrence, and semantic overlap. We proposed a novel framework that
combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)
architecture to enhance interpretability and expression-level reasoning. Each
utterance was decomposed into Emotion, Logic, and Behavior (ELB) components,
which were processed by LLMs to infer multiple distortion instances, each with
a predicted type, expression, and model-assigned salience score. These
instances were integrated via a Multi-View Gated Attention mechanism for final
classification. Experiments on Korean (KoACD) and English (Therapist QA)
datasets demonstrate that incorporating ELB and LLM-inferred salience scores
improves classification performance, especially for distortions with high
interpretive ambiguity. Our results suggested a psychologically grounded and
generalizable approach for fine-grained reasoning in mental health NLP.

</details>


### [286] [Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text](https://arxiv.org/abs/2509.17317)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 本文探索了使用机器翻译生成的数据进行低资源语言的大规模预训练的有效性，比较了不同策略对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 许多语言缺乏足够的数据来进行大规模单语预训练，多语种预训练虽然有帮助但受限于数据分布不均和多语种诅咒。通过机器翻译将高资源语言（英语）转换成低资源语言文本，或许能为低资源语言提供丰富的数据。因此作者提出验证用MT生成的数据对低资源语言模型预训练的效果及改进方式。

Method: 作者将英语文本使用机器翻译转换为印尼语和泰米尔语，并采用GPT-2（参数规模为124M-774M）在原生语料、MT生成语料以及由LLM简化的英语翻译语料上分别进行预训练。通过在原生文本上的交叉熵损失、句法探针和下游任务准确率进行综合评估。

Result: 1）基于MT语料预训练的模型随着规模扩大整体表现提升；2）将英语原文简化后再翻译得到的MT语料反而降低了对原生文本的泛化能力；3）在MT语料基础上用有限原生数据继续训练，模型通常优于只在原生数据上训练的模型（即便原生数据较少）。但文化相关任务（如有害内容检测）对原生数据的需求更高。

Conclusion: MT-生成的数据可以有效缓解低资源语言的数据壁垒，模型规模扩展能提升效果，且基于MT语料预训练再微调到原生数据模式是高效方案。但文化语义相关任务离不开较多原生数据，需谨慎平衡。

Abstract: Most languages lack sufficient data for large-scale monolingual pretraining,
creating a "data wall." Multilingual pretraining helps but is limited by
language imbalance and the "curse of multilinguality." An alternative is to
translate high-resource text with machine translation (MT), which raises three
questions: (1) How does MT-derived data scale with model capacity? (2) Can
source-side transformations (e.g., simplifying English with an LLM) improve
generalization to native text? (3) How well do models pretrained on MT-derived
data adapt when continually trained on limited native text? We investigate
these questions by translating English into Indonesian and Tamil--two
typologically distant, lower-resource languages--and pretraining GPT-2 models
(124M-774M) on native or MT-derived corpora from raw and LLM-simplified
English. We evaluate cross-entropy loss on native text, along with accuracy on
syntactic probes and downstream tasks. Our results show that (1) MT-pretrained
models benefit from scaling; (2) source-side simplification harms
generalization to native text; and (3) adapting MT-pretrained models on native
text often yields better performance than native-only models, even with less
native data. However, tasks requiring cultural nuance (e.g., toxicity
detection) demand more exposure to native data.

</details>


### [287] [AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](https://arxiv.org/abs/2509.17348)
*Yujie Feng,Jian Li,Xiaoyu Dong,Pengfei Xu,Xiaohui Zhou,Yujia Zhang,Zexin LU,Yasha Wang,Alan Zhao,Xu Chu,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 本文提出了Adaptive Iterative Model Merging (AimMerging)方法，能够在持续学习过程中动态、有效地融合模型，从而提升大型语言模型在不断变化现实环境中的学习能力，减少遗忘。实验效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 持续学习对于部署大型语言模型至关重要，但现有模型融合方法在新知识学习和旧知识保持之间难以平衡，主要瓶颈在于融合次数和频率不理想。

Method: 提出AimMerging框架，通过对训练过程中的学习和遗忘信号进行动态监控，自适应地决定模型融合的时机和频率。具体方法包括训练轨迹引导的融合控制器决定融合操作时间，基于重放的知识融合模块计算融合权重并执行模型融合。

Result: 在3个持续学习基准数据集及不同模型规模（770M到13B）上的实验表明，AimMerging相比最新方法在FWT和BWT指标上分别取得了约80%和59%的平均相对提升。

Conclusion: AimMerging为持续学习中的模型合并提供了自适应、动态的解决方案，大幅提升了模型性能，验证了方法的有效性，并公布了源代码以促进复现。

Abstract: Continual learning (CL) is essential for deploying large language models
(LLMs) in dynamic real-world environments without the need for costly
retraining. Recent model merging-based methods have attracted significant
attention, but they still struggle to effectively manage the trade-off between
learning new knowledge and preventing forgetting, a challenge largely stemming
from suboptimal number of merges and merging frequency. In this paper, we
introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework
that utilizes learning and forgetting signals from the training trajectory to
dynamically monitor the model's training status. Guided by dynamic monitoring,
the training trajectory-guided merge controller adaptively determines the
timing and frequency of iterative fusion, while the rehearsal-based knowledge
fusion module computes the merging weights and executes the fusion.
Comprehensive experiments on three CL benchmarks with various model sizes (from
770M to 13B) demonstrate that AimMerging achieves significant performance
improvements over existing state-of-the-art methods, with an average relative
improvement of 80% and 59% on FWT and BWT, respectively. The source code is
provided for reproducibility.

</details>


### [288] [Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation](https://arxiv.org/abs/2509.17349)
*Peter Polák,Sara Papi,Luisa Bentivogli,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文分析了同时语音翻译(SimulST)系统中的延迟评估问题，发现现有延迟指标存在结构性偏差，提出YAAL和LongYAAL等新指标，并引入SoftSegmenter工具，显著提升了延迟测量的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着SimulST系统实际应用需求增加，延迟成为评价系统实时性的核心指标。然而，目前通用的延迟评价方法在短段（预分段）语音下经常产生不一致甚至误导性的结果，缺乏对无分段语音的公正、准确评估方法，阻碍了系统进步与横向对比。

Method: 作者对现有延迟评价指标在不同语言对、系统和语音分段设置（短段、长段）下进行了系统性分析。通过理论与实验证明，现有平均滞后（Average Lagging）等指标在预分段（short-form）情况下存在结构性偏差。为此，提出了YAAL（Yet Another Average Lagging）新指标，修正短段下的延迟测量，并扩展为LongYAAL用于长段（未分段）语音。同时，设计了一种基于词级对齐的软分段工具SoftSegmenter，进一步提升长段语音评价的可比性和准确性。

Result: 实验表明，YAAL和LongYAAL在短段和长段语音下均优于现有流行延迟指标，SoftSegmenter工具能提高长段延迟测量时的对齐质量，为SimulST系统提供了更一致、可靠的延迟评价方法。

Conclusion: 本研究揭示了现有SimulST延迟指标的局限，创新性地提出了YAAL及LongYAAL指标并配合SoftSegmenter工具，实现了对短段和长段语音下系统延迟的更准确评估，为相关系统研发和横向比较奠定了基础。

Abstract: Simultaneous speech-to-text translation (SimulST) systems have to balance
translation quality with latency--the delay between speech input and the
translated output. While quality evaluation is well established, accurate
latency measurement remains a challenge. Existing metrics often produce
inconsistent or misleading results, especially in the widely used short-form
setting, where speech is artificially presegmented. In this paper, we present
the first comprehensive analysis of SimulST latency metrics across language
pairs, systems, and both short- and long-form regimes. We uncover a structural
bias in current metrics related to segmentation that undermines fair and
meaningful comparisons. To address this, we introduce YAAL (Yet Another Average
Lagging), a refined latency metric that delivers more accurate evaluations in
the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and
propose SoftSegmenter, a novel resegmentation tool based on word-level
alignment. Our experiments show that YAAL and LongYAAL outperform popular
latency metrics, while SoftSegmenter enhances alignment quality in long-form
evaluation, together enabling more reliable assessments of SimulST systems.

</details>


### [289] [Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs](https://arxiv.org/abs/2509.17367)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 该论文通过无标度度量分析了不同领域文本的复杂性，发现法律文本在语言复杂性指标上与普通文本和AI生成文本存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究不同领域（法律、普通自然语言、AI生成文本）在语言复杂性上的结构性差异，并量化这些差异，有助于理解领域语言特性及AI模型的表现。

Method: 利用无标度指标，包括Heaps指数（词汇增长）、Taylor指数（词频波动）、压缩率（冗余度）和熵，跨领域（法律文档、普通文本、GPT文本）定量比较文本复杂性和结构。

Result: 法律文本的词汇增长较慢（低β）、词语使用更一致（高α），领域内部法规文本β最低、α最高。GPT生成文本统计特性更倾向于普通文本。

Conclusion: 法律文本展现出独特的领域结构和复杂性，目前的AI生成文本尚未完全复刻法律文本的这些特性。

Abstract: We present a comparative analysis of text complexity across domains using
scale-free metrics. We quantify linguistic complexity via Heaps' exponent
$\beta$ (vocabulary growth), Taylor's exponent $\alpha$ (word-frequency
fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our
corpora span three domains: legal documents (statutes, cases, deeds) as a
specialized domain, general natural language texts (literature, Wikipedia), and
AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary
growth (lower $\beta$) and higher term consistency (higher $\alpha$) than
general texts. Within legal domain, statutory codes have the lowest $\beta$ and
highest $\alpha$, reflecting strict drafting conventions, while cases and deeds
show higher $\beta$ and lower $\alpha$. In contrast, GPT-generated text shows
the statistics more aligning with general language patterns. These results
demonstrate that legal texts exhibit domain-specific structures and
complexities, which current generative models do not fully replicate.

</details>


### [290] [Robustness of Neurosymbolic Reasoners on First-Order Logic Problems](https://arxiv.org/abs/2509.17377)
*Hannah Bansal,Kemal Kurniawan,Lea Frermann*

Main category: cs.CL

TL;DR: 本文探讨了用神经符号推理方法提升大语言模型（LLM）在逻辑推理类任务中应对反事实变化的表现，并提出了NSCoT方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在面对任务反事实变化时表现不佳，容易陷入表层模式。作者意在提升LLM对逻辑扰动的鲁棒性与泛化能力。

Method: 融合神经模型（LLM）与符号逻辑求解器的神经符号（NS）方法，并提出将其与链式思维（CoT）提示方式结合，形成NSCoT方法，系统测试不同规模模型和方法的表现。

Result: NS方法在鲁棒性方面优于纯神经方法，但整体性能较差。NSCoT方法提升了性能，但仍落后于传统CoT。

Conclusion: 神经符号方法有助于提高LLM的反事实鲁棒性，但其性能提升仍有限，NSCoT作为新方向有待进一步研究与优化。

Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language
Models (LLMs), with key focus on generalization and robustness to variations in
tasks. Counterfactual task variants introduce minimal but semantically
meaningful changes to otherwise valid first-order logic (FOL) problem instances
altering a single predicate or swapping roles of constants to probe whether a
reasoning system can maintain logical consistency under perturbation. Previous
studies showed that LLMs becomes brittle on counterfactual variations,
suggesting that they often rely on spurious surface patterns to generate
responses. In this work, we explore if a neurosymbolic (NS) approach that
integrates an LLM and a symbolic logical solver could mitigate this problem.
Experiments across LLMs of varying sizes show that NS methods are more robust
but perform worse overall that purely neural methods. We then propose NSCoT
that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate
that while it improves performance, NSCoT still lags behind standard CoT. Our
analysis opens research directions for future work.

</details>


### [291] [FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis](https://arxiv.org/abs/2509.17395)
*Tianshi Cai,Guanxu Li,Nijia Han,Ce Huang,Zimu Wang,Changyu Zeng,Yuqi Wang,Jingshi Zhou,Haiyang Zhang,Qi Chen,Yushan Pan,Shuihua Wang,Wei Wang*

Main category: cs.CL

TL;DR: FinDebate提出了一个结合多智能体协作辩论与金融领域RAG（检索增强生成）的分析框架，用于提高金融分析的多维度性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前传统金融分析方法往往依赖单一模型，可能导致结论片面、过度自信或可靠性不足，因此需要一个更具协作性和自我纠正能力的分析框架。

Method: 构建五个专业智能体，分别负责盈余、市场、情绪、估值、风险等维度，同时运行，利用RAG技术合成多维证据。引入安全辩论协议，促使智能体之间相互挑战和优化初步结论，以确保建议的连贯性和可信度。

Result: 通过大语言模型和人工评测，FinDebate能够输出高质量、置信度校准的多维金融分析结果，并可生成不同时间跨度下可操作的投资建议。

Conclusion: FinDebate系统有效提升了金融分析的准确性、全面性和实用性，验证了多智能体安全辩论与RAG技术结合的优势。

Abstract: We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.

</details>


### [292] [EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](https://arxiv.org/abs/2509.17396)
*Minsoo Kim,Arnav Kundu,Han-Byul Kim,Richa Dixit,Minsik Cho*

Main category: cs.CL

TL;DR: 本论文提出了一种全新的KV缓存管理方法EpiCache，可以在有限内存下显著提升大语言模型长对话的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）上下文长度的增加，KV缓存线性增长，内存消耗成为多轮对话场景下的瓶颈。已有压缩方法要么导致高峰期内存无限增大，要么牺牲多轮对话准确率，亟需更高效的缓存管理方案。

Method: 提出EpiCache，无需额外训练，通过块级预填充控制缓存增长，并将对话历史聚类为主题相关的“episode”，实现按episode的KV缓存淘汰。同时根据模型各层对KV缓存敏感度自适应分配内存预算。

Result: 在三个长对话问答基准上，EpiCache比现有方法准确率最高提升40%，在4-6倍压缩下保持接近全量准确率，同时延迟和内存消耗最高降至2.4倍和3.5倍。

Conclusion: EpiCache为资源受限环境下的多轮长对话任务提供了高效的KV缓存管理解决方案，大幅提升了准确率并降低了计算资源需求。

Abstract: Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.

</details>


### [293] [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context](https://arxiv.org/abs/2509.17399)
*Pramit Sahoo,Maharaj Brahma,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLMs）在文化适应和表达方面存在的不足，提出并发布了一个面向印度多子区域、包含17个文化面向、约8000条文化概念的CSI数据集，用于评估和提升LLM的文化敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理涉及特定文化内容时缺乏足够的文化知识和技能，导致偏见和文化错位。同时，目前缺乏针对细粒度、子区域级别文化的高质量评测数据集及指标，影响对模型文化能力的客观测评。

Method: 作者新构建了DIWALI数据集，涵盖印度36个子区域、17种文化面向、约8000条文化专属概念，用以细致刻画区域内部文化差异。评测包括CSIs匹配、LLM自评分及多元化人类评测，并对不同LLMs在文化文本适配任务中的表现进行定量分析。

Result: 结果显示，主流LLMs在印度各子区域文化适配任务上存在选择性覆盖和浅层次理解的问题，难以体现真实复杂文化差异。

Conclusion: DIWALI数据集为评估和完善LLM文化认知能力提供了新工具，有助于推动模型更好地理解和生成多元、细粒度地区文化相关内容。相关数据和代码已开放共享。

Abstract: Large language models (LLMs) are widely used in various tasks and
applications. However, despite their wide capabilities, they are shown to lack
cultural alignment \citep{ryan-etal-2024-unintended,
alkhamissi-etal-2024-investigating} and produce biased generations
\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.
Evaluation of LLMs for cultural awareness and alignment is particularly
challenging due to the lack of proper evaluation metrics and unavailability of
culturally grounded datasets representing the vast complexity of cultures at
the regional and sub-regional levels. Existing datasets for culture specific
items (CSIs) focus primarily on concepts at the regional level and may contain
false positives. To address this issue, we introduce a novel CSI dataset for
Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k
cultural concepts from 36 sub-regions. To measure the cultural competence of
LLMs on a cultural text adaptation task, we evaluate the adaptations using the
CSIs created, LLM as Judge, and human evaluations from diverse
socio-demographic region. Furthermore, we perform quantitative analysis
demonstrating selective sub-regional coverage and surface-level adaptations
across all considered LLMs. Our dataset is available here:
\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},
project
webpage\footnote{\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},
and our codebase with model outputs can be found here:
\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.

</details>


### [294] [Vision Language Models Are Not (Yet) Spelling Correctors](https://arxiv.org/abs/2509.17418)
*Junhong Liang,Bojun Zhang*

Main category: cs.CL

TL;DR: 本文提出了ReViCo，这是首个系统评估视觉语言模型（VLM）在真实图像文字拼写纠错任务中的基准，发现当前主流VLM纠错表现远低于人类，并提出两种有效改进方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图片中文字识别与纠错任务上尚未有系统评测基准，且真实场景下纠错显著困难。研究缺乏对VLM实际拼写纠错能力的深入分析与提升方法。

Method: 1. 构建ReViCo数据集，收集现实中图片里的中英文拼写错误用于多粒度评测；2. 系统性评估开源（Qwen, InternVL）和闭源（GPT-4o, Claude）VLM在纠错任务的表现；3. 探索两种提升策略：一是联合OCR-纠错流程，二是利用背景信息辅助模型理解提高纠错效果。

Result: 实验发现当前VLM无论开源或闭源在文字识别与纠错方面，尤其是纠错，均明显落后于人类。所提出的两种解决策略均带来了稳定的性能提升。

Conclusion: 现有VLM架构在视觉拼写纠错中有根本性瓶颈，对体系结构改进和背景知识融合提供了新方向。ReViCo为相关研究提供了标准基准和有价值的分析，推动领域进步。

Abstract: Spelling correction from visual input poses unique challenges for vision
language models (VLMs), as it requires not only detecting but also correcting
textual errors directly within images. We present ReViCo (Real Visual
Correction), the first benchmark that systematically evaluates VLMs on
real-world visual spelling correction across Chinese and English. ReViCo
contains naturally occurring errors collected from real-world image data and
supports fine-grained evaluation at both image and token levels. Through
comprehensive experiments on representative cascaded (Qwen) and native
(InternVL) open-source models, as well as closed-source systems (GPT-4o,
Claude), we show that current VLMs fall significantly short of human
performance, particularly in correction. To address these limitations, we
explore two solution paradigms: a Joint OCR-Correction pipeline and a
Background Information enhanced approach, both of which yield consistent
performance gains. Our analysis highlights fundamental limitations of existing
architectures and provides actionable insights for advancing multimodal
spelling correction.

</details>


### [295] [RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios](https://arxiv.org/abs/2509.17421)
*Fei Zhao,Chengqiang Lu,Yufan Shen,Qimeng Wang,Yicheng Qian,Haoxin Zhang,Yan Gao,Yi Wu,Yao Hu,Zhen Wu,Shangyu Xing,Xinyu Dai*

Main category: cs.CL

TL;DR: 本文提出了第一个中文多模态多图像数据集RealBench，包含9393条样本与69910张图片，并通过21种多模态大模型在该数据集上进行了全面评测。实验表明：无论是开源还是闭源模型，面对真实中文多图像场景仍有较大挑战。


<details>
  <summary>Details</summary>
Motivation: 目前多模态多图片评测数据集多以英文为主，并没有适用于中文场景的数据集，导致中文多图片理解能力的研究与评估存在空白。

Method: 作者构建了一个规模较大的中文多图像数据集RealBench，涵盖多种场景、分辨率和图像结构，且均为真实用户生成内容。随后，测评了21个支持多图像输入的多模态大模型（包括闭源和开源的视觉/视频模型）在RealBench上的表现。

Result: 实验结果显示，即使是最强的闭源模型在处理中文多图片任务时也存在挑战。并且，开源视觉/视频模型与闭源模型之间平均存在71.8%的性能差距。

Conclusion: RealBench填补了中文多图片数据集的空白，为推进中文多图片理解能力的研究与模型评估奠定了重要基础。

Abstract: While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.

</details>


### [296] [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
*Hyesung Jeon,Seojune Lee,Beomseok Kang,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种高效适用于量化大模型的参数高效微调方法QWHA，通过引入Walsh-Hadamard变换及自适应参数初始化，实现低位宽量化模型在精度和训练效率上的大幅提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署高效需求增长，促使学界关注量化（减少推理成本）与参数高效微调（减少训练成本）。然而，现有融合两者的方法受制于表示能力不足或计算效率低下，需要一种兼具高表示力与高效率的新方法。

Method: QWHA将基于Walsh-Hadamard变换的适配器集成到量化模型中，设计了新颖的适配器初始化策略，包括自适应参数选择和数值细化，使适配器在保留表达能力的同时兼顾低计算开销。

Result: 实验表明，QWHA在低比特量化场景下显著优于现有基线方法，量化精度更高，训练速度比现有傅里叶变换型适配器快。

Conclusion: QWHA有效缓解了量化带来的精度损失和计算负担，为量化大模型的参数高效微调提供了一个高效实用的新工具。

Abstract: The demand for efficient deployment of large language models (LLMs) has
driven interest in quantization, which reduces inference cost, and
parameter-efficient fine-tuning (PEFT), which lowers training overhead. This
motivated the development of quantization-aware PEFT to produce accurate yet
efficient quantized models. In this setting, reducing quantization error prior
to fine-tuning is crucial for achieving high model accuracy. However, existing
methods that rely on low-rank adaptation suffer from limited representational
capacity. Recent Fourier-related transform (FT)-based adapters offer greater
representational power than low-rank adapters, but their direct integration
into quantized models often results in ineffective error reduction and
increased computational overhead. To overcome these limitations, we propose
QWHA, a method that integrates FT-based adapters into quantized models by
employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together
with a novel adapter initialization scheme incorporating adaptive parameter
selection and value refinement. We demonstrate that QWHA effectively mitigates
quantization errors while facilitating fine-tuning, and that its design
substantially reduces computational cost. Experimental results show that QWHA
consistently outperforms baselines in low-bit quantization accuracy and
achieves significant training speedups over existing FT-based adapters. The
code is available at https://github.com/vantaa89/qwha.

</details>


### [297] [MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses](https://arxiv.org/abs/2509.17436)
*Tong Chen,Zimu Wang,Yiyi Miao,Haoran Luo,Yuanfei Sun,Wei Wang,Zhengyong Jiang,Procheta Sen,Jionglong Su*

Main category: cs.CL

TL;DR: 本文提出了MedFact，这是首个专注于LLM生成内容的中文医疗事实核查数据集，旨在填补现有数据集关注人类生成内容的空白。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人在网上获取医疗信息，医疗事实核查变得非常重要。然而，现有数据集主要关注人类生成内容，对于大模型生成内容的核查仍缺乏研究，因此作者提出构建一套新的基于证据的中文医疗事实核查数据集。

Method: 作者构建了一个包含1321个问题和7409个主张的数据集，专门针对LLM生成的中文医疗内容。并在该数据集上进行了全面的实验，包括in-context learning和模型微调，并进行详细的错误分析。

Result: 实验展示了当前大模型在此任务上的能力，同时也揭示出诸多挑战。错误分析指出了未来继续研究的关键方向。

Conclusion: 构建的MedFact数据集填补了领域空白，并为今后LLM生成医疗内容的事实核查研究提供了宝贵资源。相关数据集已开放获取。

Abstract: Medical fact-checking has become increasingly critical as more individuals
seek medical information online. However, existing datasets predominantly focus
on human-generated content, leaving the verification of content generated by
large language models (LLMs) relatively unexplored. To address this gap, we
introduce MedFact, the first evidence-based Chinese medical fact-checking
dataset of LLM-generated medical content. It consists of 1,321 questions and
7,409 claims, mirroring the complexities of real-world medical scenarios. We
conduct comprehensive experiments in both in-context learning (ICL) and
fine-tuning settings, showcasing the capability and challenges of current LLMs
on this task, accompanied by an in-depth error analysis to point out key
directions for future research. Our dataset is publicly available at
https://github.com/AshleyChenNLP/MedFact.

</details>


### [298] [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning](https://arxiv.org/abs/2509.17437)
*Guizhen Chen,Weiwen Xu,Hao Zhang,Hou Pong Chan,Deli Zhao,Anh Tuan Luu,Yu Rong*

Main category: cs.CL

TL;DR: 本文针对多模态大语言模型（MLLMs）在几何推理等视觉密集任务中存在的幻觉和推理不准确问题，提出了GeoPQA基准和两阶段RL训练框架，有效提升了MLLMs几何推理与问题求解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RL训练提升了纯文本大模型的推理能力，但多模态大语言模型在几何推理等任务表现不佳，容易产生幻觉，主要原因在于模型视觉感知瓶颈，导致推理训练收效有限。因此，亟需量化并解决这个感知瓶颈。

Method: 作者首先设计了GeoPQA（Geo-Perception Question-Answering）基准专门评测MLLMs几何感知与空间推理能力；随后，提出一套两阶段RL训练框架，先提升模型对几何结构的视觉感知，再训练推理能力。

Result: 在Qwen2.5-VL-3B-Instruct基础上，两阶段训练后模型的几何推理能力提高了9.7%，几何问题解决能力提升了9.1%，也能推广到如图形理解等其它视觉密集领域。

Conclusion: 提升MLLMs的视觉感知能力是改善其推理训练效果的关键。作者的方法显著提升了模型在几何推理及相关视觉密集任务中的表现，强调了感知基础对推理有效性的决定性作用。

Abstract: Recent advancements in reinforcement learning (RL) have enhanced the
reasoning abilities of large language models (LLMs), yet the impact on
multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like
geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate
reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps
the benefits of reasoning training. To quantify this, we design a
Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric
concepts and spatial relationships. Experiments on GeoPQA reveal significant
shortcomings of MLLMs in visual perception, which constrain RL reward signals
for effective training. To address this bottleneck, we propose a two-stage RL
training framework by first enhancing the visual perception of geometric
structures, then fostering reasoning capabilities. Applied to
Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by
9.7% and geometric problem solving by 9.1%, compared to the direct reasoning
training approach. Our method also generalizes to other vision-intensive
domains like figure understanding, highlighting the importance of perceptual
grounding in effective MLLM reasoning.

</details>


### [299] [Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system](https://arxiv.org/abs/2509.17444)
*Shohei Hisada,Endo Sunao,Himi Yamato,Shoko Wakamiya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 本研究评估了HealthBench医疗基准在日本语境下的适用性，发现直接翻译的基准存在局限，呼吁建立本地化的评测体系。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大模型（LLM）评测在日语领域资源匮乏，常依赖翻译版的英文选择题，缺乏针对日本本土医疗情境的评估框架。

Method: 使用HealthBench的5000个机器翻译医疗场景，评估GPT-4.1（多语言）和LLM-jp-3.1（日语本土）模型表现，并通过LLM自评方法分析情境及评分标准，找出与日本临床实际和文化规范不一致的地方。

Result: GPT-4.1因评分标准不完全契合日本国情而表现略降，日语本土模型在临床完整性上明显不足；大部分测试情境可以迁移，但评分标准存在较多需本地化改造之处。

Conclusion: 单纯翻译国际评测基准难以满足日本医疗LLM评估需求，亟需开发针对本地实际设计的J-HealthBench以保证安全可靠。

Abstract: This study investigates the applicability of HealthBench, a large-scale,
rubric-based medical benchmark, to the Japanese context. While robust
evaluation frameworks are crucial for the safe development of medical LLMs,
resources in Japanese remain limited, often relying on translated
multiple-choice questions. Our research addresses this gap by first
establishing a performance baseline, applying a machine-translated version of
HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual
model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,
we employ an LLM-as-a-Judge approach to systematically classify the benchmark's
scenarios and rubric criteria, identifying "contextual gaps" where content is
misaligned with Japan's clinical guidelines, healthcare systems, or cultural
norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric
mismatches and a significant failure in the Japanese-native model, which lacked
the required clinical completeness. Furthermore, our classification indicates
that while the majority of scenarios are applicable, a substantial portion of
the rubric criteria requires localization. This work underscores the
limitations of direct benchmark translation and highlights the urgent need for
a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable
and safe evaluation of medical LLMs in Japan.

</details>


### [300] [Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](https://arxiv.org/abs/2509.17445)
*Chaodong Tong,Qi Zhang,Lei Jiang,Yanbing Liu,Nannan Sun,Wei Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的不确定性估计方法SRE，提升大模型问答中的幻觉检测效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答时可能产生流畅但事实错误的回答（幻觉），现有基于熵的不确定性估计方法受采样噪声与答案聚类不稳定的影响，难以准确检测幻觉。

Method: 提出SRE方法：（1）通过输入侧语义重述生成真实的同义表述，扩大估计空间，减少模型解码偏差；（2）采用渐进的能量混合聚类技术，提升语义聚类稳定性。

Result: 在SQuAD和TriviaQA问答数据集上，SRE在不确定性估计和幻觉检测方面优于已有强基线方法。

Conclusion: 输入多样化结合多信号聚类能显著提升语义层面不确定性估计，为大模型问答的可靠性提供更有效的技术支撑。

Abstract: Reliable question answering with large language models (LLMs) is challenged
by hallucinations, fluent but factually incorrect outputs arising from
epistemic uncertainty. Existing entropy-based semantic-level uncertainty
estimation methods are limited by sampling noise and unstable clustering of
variable-length answers. We propose Semantic Reformulation Entropy (SRE), which
improves uncertainty estimation in two ways. First, input-side semantic
reformulations produce faithful paraphrases, expand the estimation space, and
reduce biases from superficial decoder tendencies. Second, progressive,
energy-based hybrid clustering stabilizes semantic grouping. Experiments on
SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more
robust and generalizable hallucination detection. These results demonstrate
that combining input diversification with multi-signal clustering substantially
enhances semantic-level uncertainty estimation.

</details>


### [301] [SLAyiNG: Towards Queer Language Processing](https://arxiv.org/abs/2509.17449)
*Leonor Veloso,Lea Hirlimann,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出并构建了第一个专注于酷儿俚语的高质量标注数据集（SLAyiNG），为相关自然语言处理任务提供支持，并初步评估了人工与大模型在词义消歧任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 酷儿俚语经常在用户交互中被误判为仇恨言论或引发负面回应，而现有相关研究和高质量标注数据稀缺，尤其缺乏针对酷儿俚语的全面研究和评测。

Method: 作者收集了字幕、社交媒体帖子和播客中的酷儿俚语，并进行术语及释义整理、内容收集和人工标注，最后通过人类标注者与OpenAI大模型o3-mini在词义消歧任务上的表现进行初步比对分析。

Result: 实验数据显示，人类标注者与模型在词义消歧任务上的平均Krippendorff's alpha为0.746，表明模型具有一定的辅助过滤能力，但标注任务本身复杂敏感。

Conclusion: 即便现有大模型可作为预筛选工具，但酷儿语料的复杂性和敏感性要求依赖专家及社群驱动的深入标注工作。

Abstract: Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.

</details>


### [302] [Codifying Natural Langauge Tasks](https://arxiv.org/abs/2509.17455)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文提出了一种将文本转化为代码（text-to-code）的方法，利用程序生成进行显式推理，以提升法律判决和医疗问答等实际问题的解决能力，并在13个基准数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言任务（如法律判决、医疗问答）往往直接用自然语言进行处理，缺乏可解释性和深入的推理能力。作者希望探讨能否通过将问题转化为代码形式，利用程序的逻辑推理能力，更好地解决实际复杂问题。

Method: 提出了ICRAG框架，将自然语言通过迭代式细化转化为可执行程序，并融合领域知识和GitHub等外部知识资源，增强代码生成和推理的准确性。

Result: 在13个基准任务上，ICRAG框架的表现相比现有方法取得最高达161.1%的相对提升。实验还分析了外部知识和代码生成的作用。

Conclusion: ICRAG显著提升了利用text-to-code方法处理复杂自然语言任务的效果，同时也揭示了该方向的局限性和未来改进空间。

Abstract: We explore the applicability of text-to-code to solve real-world problems
that are typically solved in natural language, such as legal judgment and
medical QA. Unlike previous works, our approach leverages the explicit
reasoning provided by program generation. We present ICRAG, a framework that
transforms natural language into executable programs through iterative
refinement using external knowledge from domain resources and GitHub. Across 13
benchmarks, ICRAG achieves up to 161.1\% relative improvement. We provide a
detailed analysis of the generated code and the impact of external knowledge,
and we discuss the limitations of applying text-to-code approaches to
real-world natural language tasks.

</details>


### [303] [PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents](https://arxiv.org/abs/2509.17459)
*Namyoung Kim,Kai Tzu-iunn Ong,Yeonjun Hwang,Minseok Kang,Iiseo Jihn,Gayoung Kim,Minju Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了PRINCIPLES，一种用于主动对话智能体的合成策略记忆，无需额外训练即可提升对话策略规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有主动对话策略规划方法存在策略覆盖有限、偏好性规划及对高昂训练资源的依赖等问题，影响了对话智能体的效果和应用范围。

Method: 作者通过离线自我博弈(simulation)生成合成的策略记忆（PRINCIPLES），并将其作为可复用的知识，在推理阶段为对话策略规划提供指导，无需额外训练或数据标注。

Result: 在情感支持和劝说任务上，PRINCIPLES在多个强基线之上取得了一致且明显的改进，且在更广泛和多样化的评估环境下保持鲁棒性。

Conclusion: PRINCIPLES显著提升了主动对话中策略规划的表现，降低了对昂贵训练过程的依赖，展现了很强的泛化和实用潜力。

Abstract: Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.

</details>


### [304] [Diagnosing Model Editing via Knowledge Spectrum](https://arxiv.org/abs/2509.17482)
*Tsung-Hsuan Pan,Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文提出了“知识光谱”与“知识诊断框架”，用于分析与提升大语言模型知识编辑的效果，发现知识本身特性显著影响编辑成败，并在此基础上提出自适应编辑策略，显著改善了编辑效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型知识编辑方法容易引入副作用，并且主要聚焦于算法本身改进，忽视了目标知识固有属性对编辑成败的影响。作者试图填补这一研究空白，提升知识编辑的可控性与成功率。

Method: 首先提出“知识光谱”框架，根据知识的真实世界流行度、模型编辑前熟悉度和问题结构对知识进行系统分类。然后，进行实证分析，证明这些特性可预测编辑效果。基于发现，进一步提出“知识诊断框架”，针对不同知识难度自适应调整编辑强度，提升编辑表现。

Result: 实证结果证明，知识特性对编辑成功率和稳定性有显著预测作用。采用知识诊断框架后，在难编辑知识上的成功率明显提升，同时计算资源得到优化。

Conclusion: 知识本身的特性是影响模型知识编辑的重要因素。基于知识属性自适应调整编辑可有效提升编辑成功率并减少副作用。该框架为理解并提升大语言模型知识编辑提供了新的视角。

Abstract: Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.

</details>


### [305] [AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.17486)
*Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文压缩方法AttnComp，通过自适应地压缩无关信息，提高LLM在检索增强生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）通过引入外部文档改善大语言模型的事实准确性，但检索内容常常包含大量无关信息，影响生成质量，需要高效、灵活的上下文压缩方法。

Method: 提出AttnComp框架，利用LLM的注意力机制识别相关信息，采用Top-P压缩算法，仅保留累计注意力权重超过设定阈值的最少文档，还能根据相关性估算生成结果的置信度。

Result: 实验结果显示，AttnComp在保持高准确率的同时实现了更高的压缩率和更低的延迟，优于现有压缩方法和未经压缩的基线。

Conclusion: AttnComp有效提升了RAG系统的效率和准确度，并为用户提供了响应可靠性的评估依据，具有实际应用价值。

Abstract: Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.

</details>


### [306] [MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](https://arxiv.org/abs/2509.17489)
*Woongkyu Lee,Junhee Cho,Jungwook Choi*

Main category: cs.CL

TL;DR: 本文提出了MapCoder-Lite，通过增加角色专属LoRA适配器，将单一7B模型升级为四个专业化智能体，节省参数的同时提升多智能体代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体代码生成方案通常依赖规模大（>30B）的模型，小模型无法取得理想效果或容易崩溃，限制了小型开源模型的应用潜力。

Method: 作者将7B模型通过仅约3%额外参数的LoRA适配器，分别微调为检索、规划、编写和调试四个角色智能体。采用三项技术：（1）从强力LLMs蒸馏学习路径，提升检索与调试稳健性；（2）监督引导修正，强化规划与编码效果；（3）基于智能体的LoRA微调实现高效专精。

Result: MapCoder-Lite在xCodeEval的准确率由13.2%提升至28.3%，所有格式错误被消除，并以远少于32B模型的内存和生成时长，准确率逼近后者不到六个百分点。

Conclusion: 通过角色化LoRA微调，小参数模型亦可高效实现多智能体协作，开启小型语言模型高质量多智能体代码生成新局面。

Abstract: Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.

</details>


### [307] [Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages](https://arxiv.org/abs/2509.17493)
*Wenhao Zhuang,Yuan Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种创新框架，将字符转写与赫夫曼编码结合，用于改善大语言模型（LLM）在低资源非拉丁文字语言上的处理能力，显著提升了压缩率、准确率及效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs跨语种能力不足，尤其对低资源、非拉丁文字语言表现不佳。传统解决方案如转写虽自然，但缺乏一体化框架。

Method: 提出将字符转写与赫夫曼编码结合的完整转写框架，实现高效、可扩展且无损的低资源语言编码与解码。

Result: 该框架在文本分类、机器阅读理解和机器翻译任务上验证有效。实验表明对低资源语言模型处理能力大幅提升，并保持高资源语言性能不变。

Conclusion: 所提方法能提升LLM处理低资源语言的表现，实现更高效、更精准的多语言能力，并具备良好扩展性。

Abstract: As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.

</details>


### [308] [CorefInst: Leveraging LLMs for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17505)
*Tuğba Pamay Arslan,Emircan Erol,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文提出了一种新的多语言指代消解（CR）方法，利用纯解码器的大模型（LLM）通过指令调优提升跨语言的指代消解性能，并超越了当前最先进的专用模型。


<details>
  <summary>Details</summary>
Motivation: 指代消解任务通常受限于架构专用性和基于编码器的语言模型，这些方法既需大量训练又缺乏灵活性，因此需要探索更通用、适应性更强的方法。

Method: 作者首次探索用纯解码式LLM进行多语言CR，设计了五种不同的指令集，通过受控推理方法引导LLM进行有/无显示指代的信息消解。实验在Llama 3.1、Gemma 2和Mistral 0.3等模型上完成。

Result: 实验结果显示，经过适当指令调优的LLM能够超越当前领先的多语言CR专用架构。最优模型Llama 3.1在CorefUD v1.2数据集上，平均比SOTA模型Corpipe 24高2个百分点。

Conclusion: 通过合理的指令设计和微调，解码器型LLM可以成为多语言指代消解任务的新SOTA方法，展现出极大的灵活性和泛化能力。

Abstract: Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.

</details>


### [309] [Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models](https://arxiv.org/abs/2509.17523)
*María Andrea Cruz Blandón,Zakaria Aldeneh,Jie Chi,Maureen de Seyssel*

Main category: cs.CL

TL;DR: 本文通过引入有限的视觉信息（visual grounding）提升了双语自监督语音表示学习（SSL）模型的表现，大幅缩小了多语言自监督模型与单语模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前自监督语音学习虽然在单语任务中表现优异，但多语种特别是双语模型在各自语言的表现普遍不及单语模型，存在明显性能落差，因此需要寻找方法提升多语种模型的效果。

Method: 作者提出在双语语音自监督学习模型中加入有限的视觉信息，将语音与视觉信息进行联合建模，用以改善模型在多语种环境下的表现。

Result: 实验表明，加入视觉信息后，无论单语还是双语模型均有性能提升，双语模型提升尤为显著。在零样本语音判别任务中，多语种模型与单语模型的性能差距从原本的31.5%下降至8.04%。

Conclusion: 有限的视觉信息可显著提升双语甚至多语种语音自监督学习模型的表现，有效缩小其与单语模型间的性能差距，验证了视觉辅助策略对于多语种语音表征学习的积极作用。

Abstract: Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.

</details>


### [310] [Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning](https://arxiv.org/abs/2509.17552)
*Tianle Zhang,Wanlong Fang,Jonathan Woo,Paridhi Latawa,Deepak A. Subramanian,Alvin Chan*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的方法，将非文本模态（如分子结构等）的基础模型表示集成到大型语言模型（LLM）中，实现了多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM性能卓越，但集成多模态（非文本如图像、分子等）信息时通常需要额外的昂贵监督训练，限制了LLM即用即适应新领域和模态的能力。作者希望以无训练方式完成多模态融合，降低适应成本。

Method: 论文提出“In-Context Representation Learning (ICRL)”，即在上下文提示学习中，将基础模型（FM）的非文本表示直接作为提示输入到LLM中，通过少样本方式实现多模态推理，无需对LLM进行微调或额外监督训练。

Result: ICRL在分子领域进行了多项任务测试，包括如何无训练地将FM表示映射给LLM、影响ICRL性能的因素以及ICRL有效性的机制。实验表明ICRL无需额外训练即可帮LLM适应并泛化至多模态任务。

Conclusion: ICRL首次实现了将非文本模态无训练集成进LLM，为多模态泛化和可适应性开辟了新方向，极大提升了LLM在多模态推理中的灵活性和实用性。

Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced
with test-time computation, which relies on external tools and even other deep
learning models. However, existing approaches for integrating non-text modality
representations into LLMs typically require additional costly supervised
training, restricting on-the-fly adaptation to new domains and modalities. In
this work, we explore the feasibility of integrating representations from
non-text foundational models (FMs) into text-based LLMs in a training-free
manner. We propose In-Context Representation Learning (ICRL) as a
proof-of-concept to allow LLMs to adaptively utilize non-text modality
representations with few-shot learning. Unlike traditional in-context learning,
which incorporates text-label pairs, ICRL replaces text inputs with FM
representations, enabling the LLM to perform multi-modal inference without
fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,
investigating three core research questions: (i) how to map FM representations
into LLMs in a training-free manner, (ii) what factors influence ICRL
performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To
the best of our knowledge, ICRL is the first training-free framework for
integrating non-text modality representations into text-based LLMs, presenting
a promising direction for adaptable, multi-modal generalization.

</details>


### [311] [Specification-Aware Machine Translation and Evaluation for Purpose Alignment](https://arxiv.org/abs/2509.17559)
*Yoko Kayano,Saku Sugawara*

Main category: cs.CL

TL;DR: 本论文探讨了将专业翻译规范引入机器翻译（MT）流程，通过实验验证规范感知翻译系统（包括大语言模型输出）在质量评估中的优势。


<details>
  <summary>Details</summary>
Motivation: 以往机器翻译研究很少明确考虑和整合专业翻译中的“规范”需求，而规范在保证翻译质量、满足客户需求方面非常关键。本文旨在填补该领域的理论和实践空白。

Method: 基于翻译学理论，提出了一套规范感知的机器翻译与评估实现指南。实验部分，以33家上市公司的投资者关系文本为例，对比官方人工翻译和大语言模型按规范生成的翻译，采用专家错误分析、用户偏好排名及自动化指标综合评估各类翻译类型。

Result: 实验结果显示，大语言模型在受到规范明确指导时，其翻译在人工评估中的表现稳定优于官方人工翻译，暴露了实际与用户预期之间的质量差距。

Conclusion: 将规范纳入机器翻译流程，并配合人工监督，有助于提升翻译质量，更好地与专业翻译实践接轨。

Abstract: In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.

</details>


### [312] [Asking a Language Model for Diverse Responses](https://arxiv.org/abs/2509.17570)
*Sergey Troshin,Irina Saparina,Antske Fokkens,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文比较了几种用于生成多样性大语言模型响应的方法，发现一些简单的非独立采样策略能提升响应多样性而不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和生成回答时，常常能给出多种合理的答案，如何有效采样产生多样响应以提升多样性和质量，是当前研究关注的问题。

Method: 作者比较了三种采样方法：传统的祖先（并行）采样、枚举采样（一次性生成n个候选）和迭代采样（依赖已有响应顺序生成新候选），在同样的资源消耗下，从生成质量、多样性和效率等方面进行了对比实验。

Result: 实验证明，枚举采样和迭代采样在维护类似响应质量的情况下，有更高的多样性表现。

Conclusion: 一些简单的、非独立的采样方法能够有效提升模型输出的多样性，同时不会明显降低生成回答的质量。

Abstract: Large language models increasingly rely on explicit reasoning chains and can
produce multiple plausible responses for a given context. We study the
candidate sampler that produces the set of plausible responses contrasting the
ancestral (parallel) sampling against two alternatives: enumeration, which asks
the model to produce $n$ candidates in one pass, and iterative sampling, which
proposes candidates sequentially while conditioning on the currently generated
response set. Under matched budgets, we compare these samplers on quality,
lexical and computation flow diversity, and efficiency. Our empirical results
demonstrate that enumeration and iterative strategies result in higher
diversity at comparable quality. Our findings highlight the potential of simple
non-independent sampling strategies to improve response diversity without
sacrificing generation quality.

</details>


### [313] [MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents](https://arxiv.org/abs/2509.17628)
*Yuzhen Lei,Hongbin Xie,Jiaxing Zhao,Shuangxue Liu,Xuan Song*

Main category: cs.CL

TL;DR: 本论文提出了MSCoRe，这是一个包含126,696个多领域、多阶段问答任务的大型基准数据集，用于测试大模型的多阶段推理和协作能力。实验评估了多种主流大模型在该数据集上的表现，发现商业模型表现较好，但在复杂任务上仍有明显性能差距。数据和代码已开放。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在单一领域的问答任务表现优异，其在多阶段、复杂推理及协作环境中的能力还未被充分研究。当前基准数据主要集中于孤立任务或单一领域，无法评估模型的多阶段推理和优化能力。因此需要新的测评工具来填补这一空白。

Method: 作者提出了MSCoRe基准，包括汽车、医药、电子、能源等领域的数据，通过动态采样、迭代问答生成和多层次质量评估三阶段流程构建数据，并按照任务复杂度分为三类。随后用该基准系统性评估了多种主流大语言模型的多阶段问答能力，并测试了其在噪声数据下的鲁棒性。

Result: 商业大模型总体表现最佳，但随着任务复杂度增加，模型的ROUGE得分显著下降。此外，所有模型在遇到带噪声的数据时表现均变差。

Conclusion: MSCoRe为大语言模型的多阶段推理能力评估提供了丰富资源，揭示了现有模型在应对复杂多阶段任务和抗噪能力上的不足，为相关研究和模型改进提供了数据基础。

Abstract: Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.

</details>


### [314] [AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?](https://arxiv.org/abs/2509.17641)
*Hyunjong Ok,Suho Yoo,Hyeonjun Kim,Jaeho Lee*

Main category: cs.CL

TL;DR: 本文提出了 AuditoryBench++ 基准，用于评估语言模型在文本环境下的听觉知识与推理能力，并引入了新的听觉想象推理方法 AIR-CoT。实验证明，该方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管人类能够仅通过语言自然地进行听觉推理（如音高、响度判断等），但当前语言模型对听觉常识的理解有限，影响了其在多模态任务中的表现。因此，亟需评估并提升语言模型中的听觉常识能力。

Method: 作者构建了涵盖基础对比到上下文推理的 AuditoryBench++ 测试集，用于细致分析模型对听觉概念的处理能力。同时提出 AIR-CoT 方法，借助特殊 token 和知识注入，在推理过程中生成并融合听觉信息。

Result: 实验涵盖主流语言模型和多模态大模型，结果显示 AIR-CoT 方法普遍优于基础模型和加有听觉知识的模型。

Conclusion: AuditoryBench++ 促进了对文本下听觉推理能力的系统评估，AIR-CoT 显著提升了模型的听觉知识推理水平，有助于推动语言与多模态智能系统的发展。

Abstract: Even without directly hearing sounds, humans can effortlessly reason about
auditory properties, such as pitch, loudness, or sound-source associations,
drawing on auditory commonsense. In contrast, language models often lack this
capability, limiting their effectiveness in multimodal interactions. As an
initial step to address this gap, we present AuditoryBench++, a comprehensive
benchmark for evaluating auditory knowledge and reasoning in text-only
settings. The benchmark encompasses tasks that range from basic auditory
comparisons to contextually grounded reasoning, enabling fine-grained analysis
of how models process and integrate auditory concepts. In addition, we
introduce AIR-CoT, a novel auditory imagination reasoning method that generates
and integrates auditory information during inference through span detection
with special tokens and knowledge injection. Extensive experiments with recent
LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both
the off-the-shelf models and those augmented with auditory knowledge. The
project page is available at https://auditorybenchpp.github.io.

</details>


### [315] [Crosslingual Optimized Metric for Translation Assessment of Indian Languages](https://arxiv.org/abs/2509.17667)
*Arafat Ahsan,Vandan Mujadia,Pruthwik Mishra,Yash Bhaskar,Dipti Misra Sharma*

Main category: cs.CL

TL;DR: 本文提出并发布了适用于13种印度语言、覆盖21个翻译方向的人工评价数据集，并基于此训练了新的神经自动翻译评价指标COMTAIL，显著提升了以印度语言为核心的自动翻译评测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于字符串的机器翻译自动评测方法（如BLEU）在面对多样的语言特性与多语种尤其是低资源语言时表现有限，而神经评测指标也受限于相关人工标注数据的稀缺性。尤其是在印度语种多样复杂场景，亟需更全面的数据和更有效的自动评测指标。

Method: 作者收集并制作了涵盖13种印度语言、21个翻译方向的大型人工评测数据集，然后使用此数据集训练了带有跨语种优化的神经自动评测指标COMTAIL。同时，通过消融实验分析了该指标对不同领域、翻译质量和语言分组的敏感性。

Result: COMTAIL指标在涉及任一印度语言的翻译评测任务中，相较于之前的SOTA方法取得了显著性能提升。消融实验进一步揭示了其在不同条件下的表现变化。

Conclusion: 本文提出的数据与指标有效弥补了印度语言自动翻译评测中的数据与方法空白，对多语言、低资源场景具有重要意义，并对外发布了数据集与模型，促进该领域发展。

Abstract: Automatic evaluation of translation remains a challenging task owing to the
orthographic, morphological, syntactic and semantic richness and divergence
observed across languages. String-based metrics such as BLEU have previously
been extensively used for automatic evaluation tasks, but their limitations are
now increasingly recognized. Although learned neural metrics have helped
mitigate some of the limitations of string-based approaches, they remain
constrained by a paucity of gold evaluation data in most languages beyond the
usual high-resource pairs. In this present work we address some of these gaps.
We create a large human evaluation ratings dataset for 13 Indian languages
covering 21 translation directions and then train a neural translation
evaluation metric named Cross-lingual Optimized Metric for Translation
Assessment of Indian Languages (COMTAIL) on this dataset. The best performing
metric variants show significant performance gains over previous
state-of-the-art when adjudging translation pairs with at least one Indian
language. Furthermore, we conduct a series of ablation studies to highlight the
sensitivities of such a metric to changes in domain, translation quality, and
language groupings. We release both the COMTAIL dataset and the accompanying
metric models.

</details>


### [316] [PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation](https://arxiv.org/abs/2509.17669)
*Yan Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的可控文本生成方法PG-CE，通过“类别预测-约束构建-引导生成”三步走流程，结合多维度动态约束，提升了大语言模型的输出质量与可控性。实验显示PG-CE在多个场景下效果优异，并构建了9万组真实应用约束-文本数据集。


<details>
  <summary>Details</summary>
Motivation: 传统可控文本生成方法在文本多样性和控制能力之间存在瓶颈，难以兼顾高质量输出与精确性控制。随着大语言模型应用扩大，更强、更灵活的控制生成技术亟需突破。

Method: PG-CE方法将可控文本生成分为三步：（1）类别预测；（2）约束（如语气、表达风格、主题）动态构建；（3）根据约束引导模型生成。核心在于利用专门的约束生成模型动态提出多维约束，并用这些约束指导LLM输出。

Result: PG-CE方法在多个实际应用场景下显著提升了生成文本的质量、可控性、主题相关性及实用性。实验中，还构建了包含9万组不同主题约束-文本对的数据集，为后续评测和应用打下基础。

Conclusion: PG-CE为可控文本生成提供了更实用和高效的技术方案，证明可以通过分步和多维度约束显著提升大语言模型的系统可靠性和用户体验。

Abstract: With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.

</details>


### [317] [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](https://arxiv.org/abs/2509.17671)
*Selva Taş,Mahmut El Huseyni,Özay Ezerceli,Reyhan Bayraktar,Fatma Betül Terzioğlu*

Main category: cs.CL

TL;DR: 本文提出了Turk-LettuceDetect，这是首个专为土耳其语RAG系统设计的幻觉检测模型套件，显著提升了对于土耳其语等复杂低资源语言中大模型幻觉检测的准确率和实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成任务中存在“幻觉”现象（生成看似合理但实际错误的信息），而RAG虽部分缓解了此问题，却在土耳其语等低资源复杂形态语言下依旧难以彻底解决，因此亟需面向土耳其RAG的高效幻觉检测工具。

Method: 基于LettuceDetect框架，将幻觉检测建模为Token级别分类任务，微调了三个不同的编码器：土耳其语专用的ModernBERT、TurkEmbed4STS和多语种EuroBERT，使用机器翻译的RAGTruth基准数据集（含17790个实例），在问答、数据到文本和摘要三类任务上进行训练与评测。

Result: ModernBERT模型在测试集上取得了0.7266的F1分数，尤其在结构化任务中表现优异，且模型支持8192 token长文本，运算高效，便于实时部署。对比分析发现，现有LLM虽召回率高但过度生成幻觉内容导致精度低，凸显专用检测机制的必要性。

Conclusion: 该研究首次为土耳其RAG场景提供了有效的幻觉检测方法与数据资源，显著提升土耳其及其他低资源语言下RAG系统的可靠性和可用性，为多语种可信AI应用奠定了基础。

Abstract: The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.

</details>


### [318] [When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables](https://arxiv.org/abs/2509.17680)
*Shenghao Ye,Yu Guo,Dong Jin,Yikai Shen,Yunpeng Hou,Shuangwu Chen,Jian Yang,Xiaofeng Jiang*

Main category: cs.CL

TL;DR: 本文提出了EnoTab框架，通过双重去噪机制提升大语言模型在复杂问题与大规模表格条件下的表格问答性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，复杂问题和大表格引入了大量噪声数据，显著降低了大语言模型在表格问答中的推理表现，因此需有方法提升相关性过滤与表格精简能力。

Method: 设计了EnoTab框架，包括两大去噪模块：一是基于证据的问题去噪，将问题分解为最小语义单元并筛除与推理无关内容；二是证据树引导的表格去噪，通过构建透明明确的表格修剪路径，分步移除无关数据，且引入回滚机制修复异常状态，从而得到可靠的子表以支持最终推理。

Result: 大规模实验表明，EnoTab在复杂问题和大表格条件下表现优异，显著提升了TableQA任务的效果。

Conclusion: EnoTab能有效应对复杂表格问答场景中的噪声，提升大语言模型推理准确性，为实用化应用提供了有力支撑。

Abstract: Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.

</details>


### [319] [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
*Daiye Miao,Yufang Liu,Jie Wang,Changzhi Sun,Yunke Zhang,Demei Yan,Shaokang Dong,Qi Zhang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 本文提出了TASO方法，旨在减少LoRA微调中的参数冗余，通过任务相关性分配参数重要性权重，显著降低可训练参数数量，并在多个任务上优于标准LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然高效简便，但存在显著参数冗余，既增加了训练负担，也影响微调效果。因此，如何高效、准确地消除LoRA冗余参数受到关注。

Method: TASO通过评估预训练模型权重在下游任务的重要性，识别出任务特定的核心区域，并据此决定LoRA模块的稀疏结构，从而提前去除冗余参数。

Result: 在与rank r=1的LoRA相当的参数预算下，TASO在多个任务上都能稳定超过标准LoRA，参数更少但性能更佳。

Conclusion: TASO为LoRA参数冗余问题提供了任务对齐的新视角，能有效减少任务自适应时的可训练参数，提高微调效率和性能。

Abstract: LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.

</details>


### [320] [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
*Dongxu Lu,Johan Jeuring,Albert Gatt*

Main category: cs.CL

TL;DR: 本文比较了大语言模型(LLM)与人工在多轮、知识支撑的角色扮演对话中的表现，发现LLM生成回复在多轮对话质量下降显著，而人工表现则逐步提升，同时提出了结合人类评价和自动评测的混合评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在专业培训、教育和模拟等领域的应用增加，如何评估其在复杂多轮、知识依赖的对话中的表现成为亟需解决的问题。现有评估方法往往侧重单轮或简化场景，无法全面揭示LLM在实际长对话场景中的优势与不足。

Method: 作者设计了一套多轮角色扮演对话场景，并征集38名人类评审，依次评估LLM（如Gemini 2.0 Flash）生成的回复和人工写作的回复。评估指标包括自然性、语境保持和整体质量。同时，利用LLM自动评判作为对比，观察其与人工评价的一致性。

Result: 人工评估显示，LLM生成回复在多轮对话中，回复质量随着轮次明显下降，尤其体现在自然性和上下文衔接方面；而人工回复表现则随轮次有所提升。参与者更倾向于选择人工回复。此外，自动化LLM判断与人工评判高度一致，证实这种质量差距真实存在。

Conclusion: 本文构建了能真实反映多轮、知识支撑对话中LLM退化的基准数据集，并提出了一种结合人类与自动LLM评判的混合评估体系，为LLM在复杂培训模拟等场景的可靠应用提供了依据。

Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.

</details>


### [321] [Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs](https://arxiv.org/abs/2509.17701)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 该论文提出了一个自动多语言流程，用于生成、求解和评估符合德国K-10课程的数学问题，并评价不同语言下大模型的表现。结果显示英语解答质量最高，阿拉伯语偏低，反映出大模型存在语言偏差问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的广泛应用，其对不同语言的支持和表现差异明显。作者希望系统性地量化和分析这些差异，尤其关注AI系统在多语言教育中的公平性和包容性。

Method: 构建一个自动多语言的流程，生成628道符合德国K-10课程的数学题，并翻译为英语、德语和阿拉伯语。利用三种商用LLM生成三种语言下的逐步解答，由Cluade 3.5 Haiku等LLM评审团基于比较框架进行答案质量评价。

Result: 英语解答持续获得最高评分，阿拉伯语解答通常评分最低，德语居中。多种大模型和多种语言评测结果一致，显示出稳定的语言表现差异。

Conclusion: 主流大语言模型在教育任务中存在显著的语言偏差，英语支持更好，阿拉伯语支持较差。这凸显了当前AI教育系统在公平性方面的不足，未来需提升多语言支持，促进教育公平。

Abstract: Large Language Models (LLMs) are increasingly used for educational support,
yet their response quality varies depending on the language of interaction.
This paper presents an automated multilingual pipeline for generating, solving,
and evaluating math problems aligned with the German K-10 curriculum. We
generated 628 math exercises and translated them into English, German, and
Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)
were prompted to produce step-by-step solutions in each language. A held-out
panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality
using a comparative framework. Results show a consistent gap, with English
solutions consistently rated highest, and Arabic often ranked lower. These
findings highlight persistent linguistic bias and the need for more equitable
multilingual AI systems in education.

</details>


### [322] [Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics](https://arxiv.org/abs/2509.17737)
*Kavin R V,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出用组合型结构（Aggregate Semantic Grouping, ASG）对语言模型中的token进行嵌入（embedding），以此取得极大压缩率且保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型为每个token单独分配嵌入，难以捕获词语多元的语义特征，限制了表示的丰富性与参数利用效率。

Method: 设计了ASG方法，结合了产品量化（Product Quantization, PQ），使每个token嵌入由多个共享语义构件组成。实验在mBERT、XLM-R、mT5以及BioBERT等模型上进行，测试任务包括NLI、NER、QA和生物医学领域的BC5CDR。

Result: ASG实现了极高的嵌入参数压缩（仅为基线模型的0.4-0.5%），但在各任务中仍保持了超过95%的性能，适用于生成任务、跨语言迁移及专业领域。

Conclusion: 用组合型、可共享的语义构件进行token嵌入，可大幅压缩参数而不损失表现。ASG为语言模型实现语义丰富且紧凑的表示提供了高效简明的方案。

Abstract: Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.

</details>


### [323] [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
*Jin Xu,Zhifang Guo,Hangrui Hu,Yunfei Chu,Xiong Wang,Jinzheng He,Yuxuan Wang,Xian Shi,Ting He,Xinfa Zhu,Yuanjun Lv,Yongqi Wang,Dake Guo,He Wang,Linhan Ma,Pei Zhang,Xinyu Zhang,Hongkun Hao,Zishan Guo,Baosong Yang,Bin Zhang,Ziyang Ma,Xipin Wei,Shuai Bai,Keqin Chen,Xuejing Liu,Peng Wang,Mingkun Yang,Dayiheng Liu,Xingzhang Ren,Bo Zheng,Rui Men,Fan Zhou,Bowen Yu,Jianxin Yang,Le Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-Omni是一个统一的多模态大模型，首次实现了在文本、图像、音频、视频等多种模式下性能均达SOTA且无明显弱化，支持多语言、实时语音生成和流式输出，在多个公开基准测试中优于同类开源和部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 目前多模态模型通常在支持多模态的同时牺牲了部分单一模态的性能，尤其是在音频等任务上，缺乏一种兼具高性能与多样任务能力的通用多模态模型。此外，研究界缺少通用音频描述模型，无法广泛用于多模态推理和应用。

Method: 提出了Thinker-Talker MoE（专家混合）架构，融合同步处理文本、图像、音频和视频信息，实现统一感知与生成。采用了多码本离散语音编码与轻量级因果卷积网络（ConvNet）以支持实时流式语音生成，并加入了专门的推理模型增强多模态推理能力。同时，为补全多模态工作流，微调获得了高质量低幻觉的音频描述模型。

Result: Qwen3-Omni在36个音频及视听基准中有32个达开源SOTA，总体22项超越包括Gemini-2.5-Pro、Seed-ASR和GPT-4o-Transcribe等强闭源模型。支持119种语言的文本、19种语言的语音理解、10种语言的语音生成，并实测冷启动端到端延迟达234ms。音频生成、描述和推理效果理想。

Conclusion: Qwen3-Omni首次实现了多模态任务性能与单一模态持平甚至超越，尤其在音频领域表现突出，具备强大的多语言、多模态推理和实时生成能力。相关模型全部开源，为后续多模态及音频应用技术发展提供坚实基础。

Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.

</details>


### [324] [A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue](https://arxiv.org/abs/2509.17766)
*Ziyi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种无需额外训练的提示词工程方法，可有效提升大语言模型（LLMs）在多轮、长距离对话中的信息管理与效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长对话、多轮交流时，常常出现遗忘重要信息和推理效率低下的问题，亟需更优的对话历史管理策略。

Method: 作者提出了“状态更新多轮对话策略”，结合“状态重建（State Reconstruction）”和“历史提醒（History Remind）”两大机制，无需训练即可通过巧妙设计提示词来优化模型的对话状态管理。

Result: 该策略在多个多跳问答数据集上表现优异。例如，在HotpotQA数据集上，关键信息筛选分数提升32.6%，最终问答准确率提升14.1%，同时推理时间和Token消耗分别下降73.1%和59.4%。消融实验进一步验证了这两项机制的重要性。

Conclusion: 该方法为LLMs在多轮长对话任务中的推理与效率提升提供了有效解决方案，有助于开发更强大的智能体系统，也为未来相关研究提供了新思路。

Abstract: Large Language Models (LLMs) struggle with information forgetting and
inefficiency in long-horizon, multi-turn dialogues. To address this, we propose
a training-free prompt engineering method, the State-Update Multi-turn Dialogue
Strategy. It utilizes "State Reconstruction" and "History Remind" mechanisms to
effectively manage dialogue history. Our strategy shows strong performance
across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,
it improves the core information filtering score by 32.6%, leading to a 14.1%
increase in the downstream QA score, while also reducing inference time by
73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal
roles of both components. Our work offers an effective solution for optimizing
LLMs in long-range interactions, providing new insights for developing more
robust Agents.

</details>


### [325] [DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching](https://arxiv.org/abs/2509.17768)
*Jessica Ojo,Zina Kamel,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文提出并使用DIVERS-BENCH评估了当前最先进的语言识别（LID）模型在多种真实、多样化场景下的表现，发现模型在干净数据上很强，但在噪声或非正式文本上的识别能力显著下降。还发布了DIVERS-CS数据集，用于评测多语言混合场景下模型的效果，发现现有模型仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前LID系统大多在干净、单语数据上训练和评测，导致其对真实世界中的复杂、多样化、噪声环境适应性差。研究人员希望推动模型更加健壮和普适。

Method: 作者提出DIVERS-BENCH基准，涵盖语音转写、网络文本、社交媒体、儿童故事和代码切换文本，对主流LID模型进行泛化能力评测。同时创建DIVERS-CS，包含10种语言对的代码切换数据，用以专门分析模型在句内多语言检测的能力。

Result: 实验显示，虽然主流LID模型在人工整理数据上表现优秀，但在真实世界的噪声和混合数据下准确率大幅下降，尤其是在代码切换场景下模型表现堪忧。

Conclusion: 现实中的LID任务远比现有基准复杂，当前模型普遍缺乏足够的鲁棒性和泛化能力，亟需推动更强健、更包容的语言识别模型与评测标准的发展。

Abstract: Language Identification (LID) is a core task in multilingual NLP, yet current
systems often overfit to clean, monolingual data. This work introduces
DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across
diverse domains, including speech transcripts, web text, social media texts,
children's stories, and code-switched text. Our findings reveal that while
models achieve high accuracy on curated datasets, performance degrades sharply
on noisy and informal inputs. We also introduce DIVERS-CS, a diverse
code-switching benchmark dataset spanning 10 language pairs, and show that
existing models struggle to detect multiple languages within the same sentence.
These results highlight the need for more robust and inclusive LID systems in
real-world settings.

</details>


### [326] [One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts](https://arxiv.org/abs/2509.17788)
*Xingyu Fan,Feifei Li,Wenhui Que,Hailong Li*

Main category: cs.CL

TL;DR: 提出了WeStar框架，实现面向上百万官方账号的高效、风格化、上下文感知对话生成。


<details>
  <summary>Details</summary>
Motivation: 现有对话生成方案难以同时满足上下文感知和风格一致性，且在工业级应用中存在延迟高、算力要求高和效果退化等问题。

Method: WeStar框架将基于检索增强生成（RAG）的上下文感知与基于参数RAG（PRAG）的风格生成结合，通过按风格聚类动态激活LoRA模块，同时采用风格增强的直接偏好优化（SeDPO）提升风格多样性和生成质量。

Result: 在大规模工业数据集上的实验证明，WeStar能在保证效果的同时，实现低延迟和高扩展性，适合于实际大规模应用场景。

Conclusion: WeStar高效支持大规模多账号风格化对话生成，显著提升工业级应用的实际可用性。

Abstract: Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.

</details>


### [327] [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
*Tobias Groot,Salo Lacunes,Evgenia Ilia*

Main category: cs.CL

TL;DR: 本论文提出了一种多标签微调方法，通过对预训练及指令微调的大语言模型，在每个语境下用多个合理的词汇延续进行训练，从而提升模型复现人类语言多样性的能力。实验证明，这种方式有助于提升模型产生与人类更为一致的词汇多样性。


<details>
  <summary>Details</summary>
Motivation: 传统NLG模型难以再现人类真实的语言多样性，即每种语境下可能有多种合理词汇衔接。作者受限于缺乏反映此多样性的数据训练，故旨在探索能否通过多标签训练，增强模型的多样性再现能力。

Method: 作者在GPT-2和Mistral-7B-IT等预训练及指令微调模型基础上，采用Provo语料库，每个上下文提供多个可能的人类词继续，通过多标签微调方法进行训练，并定量评估模型预测下一个词的分布与人类分布的差异。

Result: 微调后的模型在各种语境（不论语言延续多样性高或低）下，对下一个词的预测分布更接近于人类真实分布，即较好地复现了语言多样性。

Conclusion: 多标签微调可以显著提升大语言模型对人类语言多样性的复现能力，是改进NLG系统表现的有效路径。

Abstract: Natural language generation (NLG) tasks are often subject to inherent
variability; \emph{e.g.} predicting the next word given a context has multiple
valid responses, evident when asking multiple humans to complete the task.
While having language models (LMs) that are aligned pluralistically, so that
they are able to reproduce well the inherent diversity in perspectives of an
entire population of interest is clearly beneficial, \citet{ilia2024predict}
show that LMs do not reproduce this type of linguistic variability well. They
speculate this inability might stem from the lack of consistent training of LMs
with data reflecting this type of inherent variability. As such, we investigate
whether training LMs on multiple plausible word continuations per context can
improve their ability to reproduce human linguistic variability for next-word
prediction. We employ fine-tuning techniques for pre-trained and
instruction-tuned models; and demonstrate their potential when fine-tuning
GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures
divergence among empirically estimated human and model next-word distributions
across contexts before and after fine-tuning, shows that our multi-label
fine-tuning improves the LMs' ability to reproduce linguistic variability; both
for contexts that admit higher and lower variability.

</details>


### [328] [Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?](https://arxiv.org/abs/2509.17796)
*Michal Novák,Miloslav Konopík,Anna Nedoluzhko,Martin Popel,Ondřej Pražák,Jakub Sido,Milan Straka,Zdeněk Žabokrtský,Daniel Zeman*

Main category: cs.CL

TL;DR: 该论文综述了CODI-CRAC 2025多语种共指消解（Multilingual Coreference Resolution）共享任务第四届的情况，包含引入了适合大模型赛道和新增多语种数据。传统方法表现较好，但大语言模型展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在推动多语种、跨领域的核心指代消解技术，增加数据多样性，并探索大型语言模型（LLM）在该任务中的适用性和表现。

Method: 本届任务新增LLM专属赛道，采用更简洁的明文格式便于大模型使用，同时引入了用CorefUD 1.3版扩展的三套新数据集，包括两种新语言。系统分为传统方法与LLM方法（包括微调与少量样本学习）。

Result: 共九个系统参赛，其中四个基于LLM。传统系统依然领先，但LLM方法展现出明显发展前景，在部分情况下表现优异。

Conclusion: 尽管目前传统方法表现更佳，但LLM已展现出可观潜力，预计在未来迭代中将逐步与甚至赶超传统方法对抗。

Abstract: The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.

</details>


### [329] [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
*Jihae Jeong,DaeYeop Lee,DongGeon Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: 该论文提出了EPiK数据集，用于考察韩语环境下的物理常识推理，弥补现有基准在文化多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理数据集主要基于西方语境，忽视了不同文化在物理问题解决方式上的差异，因此需要更具有文化特异性的新基准。

Method: 作者提出了两阶段生成与验证流程，在九大推理子任务和84个场景下，基于韩国文化背景（如泡菜、发酵等）有机地构建了181道二选一物理推理题，不依赖简单的翻译。

Result: 实验表明，专为韩语环境训练的模型在EPiK上的表现显著优于相同规模的通用模型，出现明显性能差距。

Conclusion: 文化无关模型在理解语言方面存在局限，开发和使用具备文化敏感性的基准对于真实衡量自然语言理解至关重要。

Abstract: Existing physical commonsense reasoning benchmarks predominantly focus on
Western contexts, overlooking cultural variations in physical problem-solving.
To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a
novel benchmark comprising 181 binary-choice problems that test physical
reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to
traditional fermentation. EPiK is constructed using a two-stage generation and
verification pipeline to create culturally-authentic problems across 9
reasoning subtasks and 84 scenarios. Unlike approaches based on simple
translation, our method generates problems organically from Korean contexts
while upholding rigorous physical reasoning standards. Our evaluations show
that Korean-specialized models consistently outperform general-purpose models
of comparable size. This performance gap highlights the limitations of
culturally-agnostic models and demonstrates the critical need for
culturally-aware benchmarks to truly measure language understanding. Our EPiK
is publicly available at https://huggingface.co/datasets/jjae/EPiK.

</details>


### [330] [Towards Adaptive Context Management for Intelligent Conversational Question Answering](https://arxiv.org/abs/2509.17829)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文提出了一种自适应上下文管理（ACM）框架，用于提升对话式问答系统在有限token下的历史信息利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有对话问答系统受模型token上限限制，难以充分利用长对话历史，从而影响回答的准确性和上下文相关性。

Method: ACM框架包括三个模块：上下文管理模块（CM）动态调整上下文长度以保证关键信息在token限制内；摘要模块（SM）通过滑动窗口摘要陈旧对话内容；实体抽取模块（EE）从最早对话轮次中提取核心实体，当摘要窗口过大时优先保留关键信息。

Result: 实验结果显示，该框架能生成更准确且上下文相关性更强的响应。

Conclusion: ACM框架能够提升对话问答系统的鲁棒性和可扩展性，对长对话历史管理具有良好效果。

Abstract: This particular paper introduces an Adaptive Context Management (ACM)
framework for the Conversational Question Answering (ConvQA) systems. The key
objective of the ACM framework is to optimize the use of the conversation
history by dynamically managing context for maximizing the relevant information
provided to a ConvQA model within its token limit. Our approach incorporates a
Context Manager (CM) Module, a Summarization (SM) Module, and an Entity
Extraction (EE) Module in a bid to handle the conversation history
efficaciously. The CM Module dynamically adjusts the context size, thereby
preserving the most relevant and recent information within a model's token
limit. The SM Module summarizes the older parts of the conversation history via
a sliding window. When the summarization window exceeds its limit, the EE
Module identifies and retains key entities from the oldest conversation turns.
Experimental results demonstrate the effectiveness of our envisaged framework
in generating accurate and contextually appropriate responses, thereby
highlighting the potential of the ACM framework to enhance the robustness and
scalability of the ConvQA systems.

</details>


### [331] [Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation](https://arxiv.org/abs/2509.17830)
*Lekkala Sai Teja,Annepaka Yadagiri,and Partha Pakray,Chukhu Chunka,Mangadoddi Srikar Vardhan*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子级序列标注的模型，实现对文档中人类与AI生成文本的精确分割检测，有效识别AI内容的混合文本。


<details>
  <summary>Details</summary>
Motivation: 生成式AI文本在重要文献中应用普及，部分被用于不当用途。传统AI检测手段多依赖文档级分类，对于刻意混合或编辑过的文本识别低效，因此亟需更细粒度的检测方法。

Method: 作者提出一种句子级、token级粒度的序列标注检测方法，结合了预训练Transformer模型、神经网络与条件随机场（CRF）。使用Transformer提取语义及句法信息，神经网络增强序列表达，CRF优化边界判别，通过三者协同提升对人类与AI文本切换点的检测准确率。

Result: 在两个公开的混合人类与AI生成文本的数据集上，所提方法优于零样本检测器和现有主流模型。实验包括消融实验，验证各模块贡献，最终准确检测出协作文本中AI生成内容区段。

Conclusion: 该方法实现了高精度的混合文本AI内容检测，为实际AI文本滥用场景提供了有效识别手段。全部源代码及数据集已公开，便于社区复现与应用。

Abstract: Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.

</details>


### [332] [Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework](https://arxiv.org/abs/2509.17844)
*Lynn Greschner,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了一个新的语境化论证评价框架，用于分析论证中情感、评价和说服力三者互动，并通过角色扮演实验进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅分别探讨论证中的情感二元性和认知评价，与整体论证说服力及其情感、情境之间的联系尚未有系统框架，故本研究旨在弥补这一空白。

Method: 作者设计了Contextualized Argument Appraisal Framework，包含情感标签、评价（如观点熟悉度、回应紧迫感、预期努力）及说服力变量，并通过角色扮演实验让参与者在模拟真实场景下评注800条论证（每条5人标注），并收集人口学及人格特质信息。

Result: 结果显示，说服力与积极情感（如信任）正相关，与消极情感（如愤怒）负相关；观点熟悉度评价变量非常重要。大部分受试者认为论证内容本身是引发情绪反应的主要因素。

Conclusion: 本框架为论证说服力、情感和评价的语境化建模提供了理论和实验基础，为后续计算建模和应用带来新思路。

Abstract: Emotions, which influence how convincing an argument is, are developed
  in context of the self and sender, and therefore require modeling
  the cognitive evaluation process. While binary emotionality has been
  studied in argument mining, and the cognitive appraisal has been
  modeled in general emotion analysis, these fields have not been
  brought together yet. We therefore propose the Contextualized
  Argument Appraisal Framework that contextualizes the interplay
  between the sender, receiver, and argument. It includes emotion
  labels, appraisals, such as argument familiarity, response urgency,
  and expected effort, as well as convincingness variables. To evaluate
  the framework and pave the way to computational modeling, we perform
  a study in a role-playing scenario, mimicking real-world exposure to
  arguments, asking participants to disclose their emotion, explain the main
cause, the
  argument appraisal, and the
  perceived convincingness. To consider the subjective nature of such
  annotations, we also collect demographic data and personality traits
  of both the participants and the perceived sender of the argument.
  The analysis of the resulting corpus of 800 arguments, each
  annotated by 5 participants, reveals that convincingness is
  positively correlated with positive emotions (e.g., trust) and
  negatively correlated with negative emotions (e.g., anger). The
  appraisal variables disclose the importance of the argument
  familiarity. For most participants, the content of the argument
  itself is the primary driver of the emotional response.

</details>


### [333] [Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora](https://arxiv.org/abs/2509.17855)
*Robert Litschko,Verena Blaschke,Diana Burkhardt,Barbara Plank,Diego Frassinelli*

Main category: cs.CL

TL;DR: 本文以巴伐利亚方言为例，系统评估了大语言模型处理方言词汇能力，并揭示其对不同词性的识别表现及局限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自然语言处理领域的广泛应用，其对标准语处理效果显著，但对于非标准书写体系、方言等变体的适应性研究不足。由于方言普遍存在缺乏标准书写规范、词汇多样等特点，评估LLM对方言词汇理解能力变得尤为重要。

Method: 作者提出DiaLemma注释框架，仅利用单语数据构建方言变体词典，并据此建立10万对德语-巴伐利亚方言人工注释词对。基于该数据集，系统评测了9个先进大语言模型在方言词条识别及翻译、屈折变体判别等任务中的表现，涉及不同词性及词汇相似度。还考察了上下文示例对模型表现的影响。

Result: 结果显示，LLMs对名词和词形相近的词对表现最佳，但在直译与屈折变体区分上存在明显困难。额外提供上下文使用示例能提升翻译表现，但会削弱识别方言变体的能力。

Conclusion: 当前LLM在应对方言书写变体方面存在局限，补充上下文等手段虽有改进，仍难根本解决问题。未来需进一步开展模型适配方言的相关研究。

Abstract: Dialects exhibit a substantial degree of variation due to the lack of a
standard orthography. At the same time, the ability of Large Language Models
(LLMs) to process dialects remains largely understudied. To address this gap,
we use Bavarian as a case study and investigate the lexical dialect
understanding capability of LLMs by examining how well they recognize and
translate dialectal terms across different parts-of-speech. To this end, we
introduce DiaLemma, a novel annotation framework for creating dialect variation
dictionaries from monolingual data only, and use it to compile a ground truth
dataset consisting of 100K human-annotated German-Bavarian word pairs. We
evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as
dialect translations, inflected variants, or unrelated forms of a given German
lemma. Our results show that LLMs perform best on nouns and lexically similar
word pairs, and struggle most in distinguishing between direct translations and
inflected variants. Interestingly, providing additional context in the form of
example usages improves the translation performance, but reduces their ability
to recognize dialect variants. This study highlights the limitations of LLMs in
dealing with orthographic dialect variation and emphasizes the need for future
work on adapting LLMs to dialects.

</details>


### [334] [CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17858)
*Milan Straka*

Main category: cs.CL

TL;DR: 本文介绍了CorPipe 25系统，这是CRAC 2025多语共指消解竞赛的获胜作品。该系统在所有组别均取得了显著领先。


<details>
  <summary>Details</summary>
Motivation: 为应对多语言共指消解领域的新挑战，尤其是在最新一届CRAC 2025竞赛中新设的LLM赛道和其它新增数据集，作者希望开发一个更高效、性能更优的系统。

Method: 作者对原有系统进行了完全重构，从TensorFlow迁移到PyTorch，实现了对新赛道和数据集的支持，优化了模型结构。

Result: CorPipe 25在LLM组和非约束组均比其他参赛系统高出8个百分点，取得了显著领先。

Conclusion: CorPipe 25验证了其方法在多语共指消解任务中的优越性，为相关研究提供了新工具，代码与模型已开源。

Abstract: We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on
Multilingual Coreference Resolution. This fourth iteration of the shared task
introduces a new LLM track alongside the original unconstrained track, features
reduced development and test sets to lower computational requirements, and
includes additional datasets. CorPipe 25 represents a complete reimplementation
of our previous systems, migrating from TensorFlow to PyTorch. Our system
significantly outperforms all other submissions in both the LLM and
unconstrained tracks by a substantial margin of 8 percentage points. The source
code and trained models are publicly available at
https://github.com/ufal/crac2025-corpipe.

</details>


### [335] [Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN](https://arxiv.org/abs/2509.17859)
*Kai Schenck,Gašper Beguš*

Main category: cs.CL

TL;DR: 本文提出了一种无监督深度学习模型（ciwGAN），能在无需标注数据的情况下学习并区分汉语普通话声调。结果表明，该模型不仅能成功学得声调对比，还模拟了人类语言学习中的某一阶段。


<details>
  <summary>Details</summary>
Motivation: 声调模式是语言学习中最复杂的目标之一。作者旨在证明，无监督生成模型可以在没有标注数据下，自动关联和学习复杂的声调类别，以更真实地模拟人类语言习得过程。

Method: 采用无监督的生成模型ciwGAN训练，输入汉语普通话音频（包含男声和女声）。通过模型内部的类别变量与声调类别的对齐，以及在卷积层追踪声调代表性。比较并统计模型对F0（基频）的编码差异。

Result: 三个训练的模型在类别变量间的F0表现出显著差异。其中，仅用男性语料训练的模型能持续、稳定地编码声调类别。总体结果表明，模型不仅能够学得声调对比，还反映了人类语言习得的某一发展阶段。

Conclusion: 无监督生成模型ciwGAN能够在无标注数据情况下学习普通话声调类别，且其内部表示与人类语言学习阶段相符。此外，提出的方法促进了神经网络内部声调表征的可解释性，对深度学习神经语言实验具有实际参考价值。

Abstract: This paper outlines the methodology for modeling tonal learning in fully
unsupervised models of human language acquisition. Tonal patterns are among the
computationally most complex learning objectives in language. We argue that a
realistic generative model of human language (ciwGAN) can learn to associate
its categorical variables with Mandarin Chinese tonal categories without any
labeled data. All three trained models showed statistically significant
differences in F0 across categorical variables. The model trained solely on
male tokens consistently encoded tone. Our results sug- gest that not only does
the model learn Mandarin tonal contrasts, but it learns a system that
corresponds to a stage of acquisition in human language learners. We also
outline methodology for tracing tonal representations in internal convolutional
layers, which shows that linguistic tools can contribute to interpretability of
deep learning and can ultimately be used in neural experiments.

</details>


### [336] [How Persuasive is Your Context?](https://arxiv.org/abs/2509.17879)
*Tu Nguyen,Kevin Du,Alexander Miserlis Hoyle,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文提出了TPS（targeted persuasion score）指标，用于细致衡量上下文对语言模型回答动摇的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型能够利用已有实体知识，但现实应用中常常需要它根据上下文进行灵活适应。目前针对这种“劝说性”或上下文影响效果的评估手段较为粗糙，需要更细致的方法。

Method: 作者提出并定义了TPS指标，用于量化一个特定上下文对语言模型回答的劝说力。TPS基于Wasserstein距离，衡量上下文将模型原始答案分布向目标分布的迁移程度，相比直接分析最终答案更具细腻性。

Result: 通过一系列实验，作者表明TPS捕捉到比以往指标更细致、全面的劝说性表现。

Conclusion: TPS为分析和度量语言模型对上下文敏感性及适应性的提供了更优的新工具，可以支持更深入的LM行为研究。

Abstract: Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.

</details>


### [337] [SiDiaC: Sinhala Diachronic Corpus](https://arxiv.org/abs/2509.17912)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: SiDiaC是首个覆盖公元5-20世纪的僧伽罗语历时语料库，包含58k词，46部文学作品，严格标注和分门别类，将成为僧伽罗语自然语言处理的基础资源。


<details>
  <summary>Details</summary>
Motivation: 僧伽罗语是低资源语言，缺乏历时性大规模语料库，限制了历史语言学、词汇变化等研究的发展。该论文旨在填补这一空白，为语言变化、NLP应用等研究提供基础数据。

Method: 从斯里兰卡国家图书馆精选文献，利用Google Document AI进行OCR识别及后期处理现代化正字法。文本基于句法标注和标准化参照其他语料库（如FarPaHC）。数据按体裁分为非虚构/虚构（一级），宗教、历史、诗歌、语言、医学（细分）。

Result: 最终构建了SiDiaC，涵盖覆盖广泛历史时期、体裁全面的僧伽罗语历时语料库，文本经过严格筛选和标注，可用于多种语言学与NLP任务。

Conclusion: SiDiaC显著拓展了僧伽罗语的研究资源，将为词汇演变、新词追踪、历史句法研究及词典编纂等领域提供重要基础，促进对该低资源语言的深入理解和NLP应用。

Abstract: SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a
historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words
across 46 literary works, annotated carefully based on the written date, after
filtering based on availability, authorship, copyright compliance, and data
attribution. Texts from the National Library of Sri Lanka were digitised using
Google Document AI OCR, followed by post-processing to correct formatting and
modernise the orthography. The construction of SiDiaC was informed by practices
from other corpora, such as FarPaHC, particularly in syntactic annotation and
text normalisation strategies, due to the shared characteristics of
low-resourced language status. This corpus is categorised based on genres into
two layers: primary and secondary. Primary categorisation is binary,
classifying each book into Non-Fiction or Fiction, while the secondary
categorisation is more specific, grouping texts under Religious, History,
Poetry, Language, and Medical genres. Despite challenges including limited
access to rare texts and reliance on secondary date sources, SiDiaC serves as a
foundational resource for Sinhala NLP, significantly extending the resources
available for Sinhala, enabling diachronic studies in lexical change, neologism
tracking, historical syntax, and corpus-based lexicography.

</details>


### [338] [Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning](https://arxiv.org/abs/2509.17921)
*Zhenyun Deng,Yulong Chen,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了一种用于零样本去上下文化的内容选择与规划框架，以提升单句抽取后的语义完整性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理任务中，从文档中抽取单句常常导致句子失去理解所需的上下文信息，例如指代消解与背景知识，影响用于证据或推理。此外，缺乏上下文的句子表达常常含糊，影响后续的自动处理或评估。因此，需要一种方法丰富被抽取句子的上下文信息。

Method: 作者提出了一种内容选择与规划框架：1）首先将目标句子切分为基本、语义独立的单元；2）标识目标句子中潜在含糊的单元，并从上下文中根据话语关系抽取相关的单元；3）生成内容规划，将相关单元融入每个含糊单元，最终重写生成更具语境完整性的句子。该方法支持零样本去上下文化设置。

Result: 实验结果显示，所提出的方法在句子去上下文化任务中具有竞争力，生成的句子在语义完整性和话语连贯性上优于现有方法。

Conclusion: 该方法有效提升了去上下文化过程中，句子的可理解性和连贯性，为NLP任务中的证据/推理句抽取提供了更实用的方案。

Abstract: Extracting individual sentences from a document as evidence or reasoning
steps is commonly done in many NLP tasks. However, extracted sentences often
lack context necessary to make them understood, e.g., coreference and
background information. To this end, we propose a content selection and
planning framework for zero-shot decontextualisation, which determines what
content should be mentioned and in what order for a sentence to be understood
out of context. Specifically, given a potentially ambiguous sentence and its
context, we first segment it into basic semantically-independent units. We then
identify potentially ambiguous units from the given sentence, and extract
relevant units from the context based on their discourse relations. Finally, we
generate a content plan to rewrite the sentence by enriching each ambiguous
unit with its relevant units. Experimental results demonstrate that our
approach is competitive for sentence decontextualisation, producing sentences
that exhibit better semantic integrity and discourse coherence, outperforming
existing methods.

</details>


### [339] [Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation](https://arxiv.org/abs/2509.17930)
*Yiwen Guan,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种新的Transformer Encoder Tree方法，通过共享中间表示和非自回归结构，提高多语言翻译特别是低资源语言翻译的准确性和效率，在语音翻译上速度提升显著。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译常因计算冗余和低资源语言的准确率低而面临瓶颈，特别是在语音翻译领域，当前方法存在效率和效果双重挑战。

Method: 设计了一种层次化的Transformer Encoder Tree（TET）结构，结合非自回归的encoder-only模型和CTC损失函数，能在多目标语言之间共享中间表示。语音端采用非自回归wav2vec2作为语音识别骨干。

Result: 该方法在低资源目标语言上翻译准确率提升，显著减少了计算冗余；所有目标语言可一次性生成，无需逐语言顺序生成，整体计算更高效。语音翻译实验中，性能优于自回归系统，且解码速度快7-14倍。

Conclusion: TET结构结合非自回归训练，有效提升了多语言（尤其低资源）翻译的准确率与速度，为高效实用的多语言语音/文本翻译提供了新方案。

Abstract: Multilingual translation faces challenges of computational redundancy and
limited accuracy for low-resource languages, especially in speech translation.
To address this, we propose a novel hierarchical Transformer Encoder Tree (TET)
combined with non-autoregressive encoder-only models trained with Connectionist
Temporal Classification for multilingual translation. By sharing intermediate
representations among linguistically similar target languages, TET can improve
accuracy on low-resource languages, reduce computational redundancy, and allow
generating all target languages in a single forward pass, thus eliminating
sequential bottlenecks and improving parallelism. For speech translation,
combining TET with a non-autoregressive speech recognition backbone (wav2vec2)
shows promising results in terms of translation quality compared to
autoregressive systems while being 7-14 times faster.

</details>


### [340] [Training-free Truthfulness Detection via Value Vectors in LLMs](https://arxiv.org/abs/2509.17932)
*Runheng Liu,Heyan Huang,Xingchen Xiao,Zhijing Wu*

Main category: cs.CL

TL;DR: 本文提出了TruthV方法，利用Transformer模型中MLP模块的统计特征，无需训练即可有效判断大型语言模型生成内容的真实性，在基准测试上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性检测方法主要依赖于对模型内部激活进行训练探针，但在可扩展性和泛化能力上存在不足。近期提出的无训练方法NoVo只关注了注意力机制，忽视了同样对事实召回有重要作用的MLP模块。

Method: 作者分析了Transformer模型MLP模块中的value向量，发现其存在与真实性相关的统计特征。在此基础上，提出了TruthV方法，无需训练，仅通过MLP模块的统计特征来检测内容真实性。并将TruthV与NoVo及对数似然等基线方法在NoVo基准集上进行了对比。

Result: TruthV方法在NoVo基准集上明显优于NoVo和log-likelihood等方法，证明了MLP模块在真实性检测中蕴含了丰富且有用的信息。

Conclusion: MLP模块内部包含用于判断真实性的有效信号，TruthV方法实现了更高效、更具解释性的真实性检测，为理解和提升LLM事实性检测提供了新思路。

Abstract: Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.

</details>


### [341] [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938)
*Satyapriya Krishna,Andy Zou,Rahul Gupta,Eliot Krzysztof Jones,Nick Winter,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson,Spyros Matsoukas*

Main category: cs.CL

TL;DR: 本论文提出了一种新的数据集D-REX，用于检测大语言模型在表面输出无害内容时，其内部推理过程可能存在恶意或欺骗性的失效模式，从而提升模型安全性评估的深度。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全评估方法主要关注输出内容是否有害，但往往忽视了模型内部推理过程可能存在的恶意或欺骗行为，特别是在系统提示注入攻击下，这种隐蔽失效带来新的潜在风险。作者旨在弥补这一评估空白。

Method: 作者设计并发布了D-REX数据集，该集通过竞赛式红队测试获取，收录了诱发模型进行欺骗性内部推理的对抗性系统提示、用户查询、模型无害表面回答及其内部思维链路。通过对比输出与内部推理，评估模型的“欺骗性对齐”。

Result: 实验证明，D-REX对现有模型及安全机制提出了巨大挑战，表明这些方法难以检测和防御此类隐蔽性失效问题。

Conclusion: 单纯关注模型外部表现并不足以保障安全，未来评估与对齐技术必须更深入剖析和监控模型的内在推理和决策过程，从而防范“欺骗性对齐”新型风险。

Abstract: The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.

</details>


### [342] [HICode: Hierarchical Inductive Coding with LLMs](https://arxiv.org/abs/2509.17946)
*Mian Zhong,Pristina Wang,Anjalie Field*

Main category: cs.CL

TL;DR: HICode方法利用大语言模型（LLM）实现了对大规模文本数据的细粒度、可扩展且细致入微的主题归纳和聚类分析，通过自动生成标签并层级化聚类，优于传统人工标注和主题建模方法。


<details>
  <summary>Details</summary>
Motivation: 传统的语料细粒度分析难以扩展（手工标注耗时耗力），而现有统计工具如主题建模，控制性和解释性又有限。研究者希望获得既可扩展又能揭示深层语义结构的方法。

Method: 提出HICode，一个两步管道：首先利用LLM从原始文本中归纳生成主题标签，然后对这些标签进行层级聚类以发现主题结构。该方法受定性研究启发，兼顾自动化与解释性。

Result: HICode在三个不同数据集上通过与人工主题标签的一致性评估、自动与人工评测，验证了结果的可靠性和稳健性。案例分析美药企阿片类药物文件，揭示了企业激进营销策略。

Conclusion: HICode方法能够高效实现大规模、细致、可解释的语料分析，提升了主题标签生成和主题结构提取的自动化能力，为文本分析提供了新工具。

Abstract: Despite numerous applications for fine-grained corpus analysis, researchers
continue to rely on manual labeling, which does not scale, or statistical tools
like topic modeling, which are difficult to control. We propose that LLMs have
the potential to scale the nuanced analyses that researchers typically conduct
manually to large text corpora. To this effect, inspired by qualitative
research methods, we develop HICode, a two-part pipeline that first inductively
generates labels directly from analysis data and then hierarchically clusters
them to surface emergent themes. We validate this approach across three diverse
datasets by measuring alignment with human-constructed themes and demonstrating
its robustness through automated and human evaluations. Finally, we conduct a
case study of litigation documents related to the ongoing opioid crisis in the
U.S., revealing aggressive marketing strategies employed by pharmaceutical
companies and demonstrating HICode's potential for facilitating nuanced
analyses in large-scale data.

</details>


### [343] [Dorabella Cipher as Musical Inspiration](https://arxiv.org/abs/2509.17950)
*Bradley Hauer,Colin Choi,Abram Hindle,Scott Smallwood,Grzegorz Kondrak*

Main category: cs.CL

TL;DR: 本文重新审视了艾尔加(Dorabella cipher)密码可能的解读方式，提出密码或许隐含着音乐信息，并尝试将其还原为旋律。


<details>
  <summary>Details</summary>
Motivation: Dorabella秘文自诞生以来一直未被破解，大部分解读都假设其为英文文本，但考虑到作者艾尔加同时是作曲家的背景，提出其有可能为加密音乐。

Method: 作者基于音乐的n-gram统计建模，并在现有乐曲加密后验证模型有效性。为Dorabella设计简化音乐记谱法，应用单表代换法对其进行解密，最终用解密结果结合艺术加工还原可聆听旋律。

Result: 通过上述方法，作者成功从Dorabella密码中生成了一段具有音乐特质的旋律片段。该旋律经过艺术修饰成为可听的乐曲。

Conclusion: 作者并不认为这是唯一的“正确”解密方案，而是强调密码解读过程可视为作曲过程的一部分，从而为破解此类谜题提供了新的视角。

Abstract: The Dorabella cipher is an encrypted note written by English composer Edward
Elgar, which has defied decipherment attempts for more than a century. While
most proposed solutions are English texts, we investigate the hypothesis that
Dorabella represents enciphered music. We weigh the evidence for and against
the hypothesis, devise a simplified music notation, and attempt to reconstruct
a melody from the cipher. Our tools are n-gram models of music which we
validate on existing music corpora enciphered using monoalphabetic
substitution. By applying our methods to Dorabella, we produce a decipherment
with musical qualities, which is then transformed via artful composition into a
listenable melody. Far from arguing that the end result represents the only
true solution, we instead frame the process of decipherment as part of the
composition process.

</details>


### [344] [Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments](https://arxiv.org/abs/2509.17961)
*Li Siyan,Zhen Xu,Vethavikashini Chithrra Raghuram,Xuanming Zhang,Renzhe Yu,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了一个基于学习科学的评估框架，用于评价异步学习环境下虚拟助教（VTA）系统，在论坛讨论中的表现。通过专家标注VTA回复、训练分类器，探索了评估方法、准确性提升方式及泛化难点。该工作为理论驱动的VTA评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前异步学习环境（ALEs）大量使用虚拟助教（VTA），但对这些系统的评估普遍缺乏教育理论基础，只停留在表层指标，难以科学比较不同VTA的教学效果。因此，有必要开发更严格、理论支撑的评估体系。

Method: 首先选择异步论坛讨论作为VTA应用场景，邀请专家对VTA回复进行标注，基于专家标注数据集训练分类器，并测试其效果。分析评估模型的优缺点，并探讨提升准确性和泛化性的办法。

Result: 开发出一套理论驱动的评估框架并实现了基于专家标注的VTA质量分类器，测试中识别了多种能提升模型准确性的做法，同时也发现了难以泛化的问题和挑战。

Conclusion: 本文为VTA系统的教育成效评估提供了坚实理论基础和实证方法，有助于推动AI助教在实际教育场景中更具教育学意义的应用。

Abstract: Asynchronous learning environments (ALEs) are widely adopted for formal and
informal learning, but timely and personalized support is often limited. In
this context, Virtual Teaching Assistants (VTAs) can potentially reduce the
workload of instructors, but rigorous and pedagogically sound evaluation is
essential. Existing assessments often rely on surface-level metrics and lack
sufficient grounding in educational theories, making it difficult to
meaningfully compare the pedagogical effectiveness of different VTA systems. To
bridge this gap, we propose an evaluation framework rooted in learning sciences
and tailored to asynchronous forum discussions, a common VTA deployment context
in ALE. We construct classifiers using expert annotations of VTA responses on a
diverse set of forum posts. We evaluate the effectiveness of our classifiers,
identifying approaches that improve accuracy as well as challenges that hinder
generalization. Our work establishes a foundation for theory-driven evaluation
of VTA systems, paving the way for more pedagogically effective AI in
education.

</details>


### [345] [ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media](https://arxiv.org/abs/2509.17991)
*Aakash Kumar Agarwal,Saprativa Bhattacharjee,Mauli Rastogi,Jemima S. Jacob,Biplab Banerjee,Rashmi Gupta,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本论文提出了ReDepress，这是首个专注于抑郁复发问题并经临床验证的社交媒体数据集，运用认知理论相关特征实现了高效的复发识别。


<details>
  <summary>Details</summary>
Motivation: 尽管社交媒体上的抑郁检测已受到关注，但抑郁复发检测领域由于缺少数据集及难以区分复发类型用户而研究匮乏。本文旨在弥补此领域的空白。

Method: 作者构建并发布了204个Reddit用户的临床标注数据集ReDepress。采用基于认知理论的标注体系（注意偏差、解释偏差、记忆偏差、反刍等），并将这些认知要素融入建模过程，结合统计分析和机器学习方法，特别是transformer时序模型。

Result: 统计分析和机器学习结果表明，认知特征可以有效区分复发与非复发用户。所提出模型的F1分数高达0.86，展现出优秀的检测能力。

Conclusion: 论文验证了心理学理论在真实社交媒体数据分析中的适用性，也显示了认知特征辅助的计算方法在早期复发检测的潜力，为可扩展、低成本的心理健康干预方式提供了支持。

Abstract: Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.

</details>


### [346] [Variation in Verification: Understanding Verification Dynamics in Large Language Models](https://arxiv.org/abs/2509.17995)
*Yefan Zhou,Austin Xu,Yilun Zhou,Janvijay Singh,Jiang Gui,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文探讨了在大语言模型测试时扩展计算资源以提升复杂问题解决能力的方法，重点分析了生成式验证器在不同维度下的效果与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现通过扩展大语言模型的推理和验证过程可提升任务表现，但具体的生成式验证器如何在不同模型能力与问题难度下发挥作用尚未被系统研究。

Method: 作者提出用生成式验证器，通过链式思维（CoT）生成推理过程并给出二元判决。实验证据来自14个开源模型与GPT-4o，涵盖12个基准测试，维度包括问题难度、生成器能力、验证器生成能力。

Result: （1）问题易时，验证器更容易可靠确认正误。（2）弱生成器产生的错误更易被检测出。（3）验证能力大体随验证器自身解题能力增强，但在难题上这种相关性减弱。另发现有些弱生成器经验证仍可接近强生成器表现，且强验证器在某些情境下相较弱者提升有限。

Conclusion: 验证器扩容虽能改善部分任务表现，但对复杂验证挑战，单独增强验证器规模可能效果有限。优化TTS方案需要综合考虑生成器与验证器的能力匹配与策略。

Abstract: Recent advances have shown that scaling test-time computation enables large
language models (LLMs) to solve increasingly complex problems across diverse
domains. One effective paradigm for test-time scaling (TTS) involves LLM
generators producing multiple solution candidates, with LLM verifiers assessing
the correctness of these candidates without reference answers. In this paper,
we study generative verifiers, which perform verification by generating
chain-of-thought (CoT) reasoning followed by a binary verdict. We
systematically analyze verification dynamics across three dimensions - problem
difficulty, generator capability, and verifier generation capability - with
empirical studies on 12 benchmarks across mathematical reasoning, knowledge,
and natural language reasoning tasks using 14 open-source models (2B to 72B
parameter range) and GPT-4o. Our experiments reveal three key findings about
verification effectiveness: (1) Easy problems allow verifiers to more reliably
certify correct responses; (2) Weak generators produce errors that are easier
to detect than strong generators; (3) Verification ability is generally
correlated with the verifier's own problem-solving capability, but this
relationship varies with problem difficulty. These findings reveal
opportunities to optimize basic verification strategies in TTS applications.
First, given the same verifier, some weak generators can nearly match stronger
ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B
performance gap shrinks by 75.5%). Second, we identify cases where strong
verifiers offer limited advantage over weak ones, as both fail to provide
meaningful verification gains, suggesting that verifier scaling alone cannot
overcome fundamental verification challenges.

</details>


### [347] [WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing](https://arxiv.org/abs/2509.18004)
*Yuhang Dai,Ziyu Zhang,Shuai Wang,Longhao Li,Zhao Guo,Tianlun Zuo,Shuiyuan Wang,Hongfei Xue,Chengyou Wang,Qing Wang,Xin Xu,Hui Bu,Jie Li,Jian Kang,Binbin Zhang,Lei Xie*

Main category: cs.CL

TL;DR: 本文提出了WenetSpeech-Chuan，这是一个包含1万小时丰富标注的四川方言语音数据集，并配套开放了ASR、TTS基准数据与模型，用以推动方言语音技术的发展。


<details>
  <summary>Details</summary>
Motivation: 目前四川方言等汉语方言的大规模、开放语音数据极其稀缺，严重制约了方言语音技术，尤其是语音识别（ASR）和语音合成（TTS）的进展。

Method: 作者构建了一个名为WenetSpeech-Chuan的新数据集，包含1万小时四川方言语音，并采用创新的Chuan-Pipeline处理流程进行丰富标注。同时，作者还人工校对并对外发布了高质量的评测基准（WenetSpeech-Chuan-Eval）以方便算法评测。

Result: 实验表明，基于WenetSpeech-Chuan训练的模型在开源系统中达到了最先进的效果，且性能接近商用服务。

Conclusion: WenetSpeech-Chuan极大地促进了四川方言语音领域的研究，有助于降低入门门槛，推动AI公平与减少语音技术偏见，该资源及相关工具全部开放共享。

Abstract: The scarcity of large-scale, open-source data for dialects severely hinders
progress in speech technology, a challenge particularly acute for the widely
spoken Sichuanese dialects of Chinese. To address this critical gap, we
introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed
using our novel Chuan-Pipeline, a complete data processing framework for
dialectal speech. To facilitate rigorous evaluation and demonstrate the
corpus's effectiveness, we also release high-quality ASR and TTS benchmarks,
WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show
that models trained on WenetSpeech-Chuan achieve state-of-the-art performance
among open-source systems and demonstrate results comparable to commercial
services. As the largest open-source corpus for Sichuanese dialects,
WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech
processing but also plays a crucial role in promoting AI equity and mitigating
bias in speech technologies. The corpus, benchmarks, models, and receipts are
publicly available on our project page.

</details>


### [348] [Cross-Attention is Half Explanation in Speech-to-Text Models](https://arxiv.org/abs/2509.18010)
*Sara Papi,Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本论文研究了在语音转文本（S2T）模型中，交叉注意力分数的解释性，发现交叉注意力只能部分反映输入与输出之间的关联，其解释性有限。


<details>
  <summary>Details</summary>
Motivation: 交叉注意力机制在编码器-解码器架构中广泛应用，常被用来解释输入与生成文本之间的依赖关系。但在语音领域，这种解释性的假设尚未被系统性验证。作者希望填补这一研究空白。

Method: 采用了特征归因生成的输入显著性图，与交叉注意力分数进行对比，评估交叉注意力的解释能力。分析涵盖单语/多语及单任务/多任务S2T模型，并从不同尺度（头和层）进行系统性对比。

Result: 结果显示，交叉注意力分数与显著性图具有中到强的相关性，尤其是在跨头和层聚合后更明显。但即便如此，交叉注意力仅捕捉到大约50%的输入相关性，最多反映52-75%的显著性。

Conclusion: 交叉注意力只能提供部分的解释信息，不完全代表模型解码器实际关注的编码器表示。在S2T模型预测中，交叉注意力作为解释手段存在天然局限，仅能提供不完整但有价值的视角。

Abstract: Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.

</details>


### [349] [RadEval: A framework for radiology text evaluation](https://arxiv.org/abs/2509.18030)
*Justin Xu,Xi Zhang,Javid Abderezaei,Julie Bauml,Roger Boodoo,Fatemeh Haghighi,Ali Ganjizadeh,Eric Brattain,Dave Van Veen,Zaiqiao Meng,David Eyre,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: RadEval是一个用于评估放射学文本的统一开源框架，集成了多种评估指标并发布了专家评注的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告生成和评估方法各自为政，缺乏统一和标准化的评估框架，评价指标也不完全适用于医学文本，致使结果难以对比，缺乏临床意义。

Method: RadEval融合了传统的n-gram重叠指标、上下文度量、医学概念相关评分以及先进的大模型评测器等多种指标。优化实现细节，扩展适应多模态医学图像，预训练专属的放射学编解码器，并标注了大量临床错误，测量各评测指标与医学专家评分的一致性。

Result: RadEval支持多种公开数据集的基线模型评测和统计测试，并具有优良的零样本检索性能。作者发布了大规模专家标注错误的数据集，实证展示各指标与放射科专家判断的相关性。

Conclusion: RadEval推动了放射学自动报告生成的评测标准化，有助于结果复现和稳健基准建设，是自动医学报告评估领域的有力工具。

Abstract: We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.

</details>


### [350] [The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies](https://arxiv.org/abs/2509.18052)
*Jiaxu Zhou,Jen-tse Huang,Xuhui Zhou,Man Ho Lam,Xintao Wang,Hao Zhu,Wenxuan Wang,Maarten Sap*

Main category: cs.CL

TL;DR: 本论文指出当前许多用大语言模型（LLM）进行社会仿真的研究存在系统性方法问题，提出了PIMMUR六项原则并重跑已有研究，发现许多社会现象并未在更严谨设计下复现，旨在为AI社会学形成更可靠的研究标准。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在社会仿真中的运用广泛，研究者希望用AI代理模拟人类社会行为，但现有工作在实验设计上存在缺陷，影响结论的可信度与可复现性。

Method: 作者调研了40余篇相关论文，总结出六种常见方法学缺陷（PIMMUR），并开发框架强制实施PIMMUR原则，复现并重新评估五个代表性研究，观察社会现象的变化。

Result: 在严格遵循PIMMUR条件后，许多原本报告的社会现象难以复现，表明这些现象可能源于实验设计偏差而非模型能力本身。

Conclusion: 只有采纳PIMMUR原则，LLM社会仿真才能得出可信、可复现的结论，该工作为今后的多智能体AI社会学研究奠定了基础与标准。

Abstract: Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."

</details>


### [351] [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2509.18060)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Renzeng Duojie,Yuqing Cai,Yongbin Yu,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.CL

TL;DR: 本文提出了一种统一的藏语多方言文本到语音（TTS）框架（TMD-TTS），能够根据明确的方言标签合成不同方言的语音。新方法显著提升了藏语多方言语音合成的表达性与质量。


<details>
  <summary>Details</summary>
Motivation: 藏语作为一种低资源语言，主流三大方言之间的平行语音语料极其有限，造成语音建模与TTS任务发展受限。作者希望解决不同方言之间数据稀缺带来的建模难题。

Method: 提出TMD-TTS框架，其中包括一个方言融合模块和Dialect-Specialized Dynamic Routing Network（DSDR-Net），用以捕捉不同方言之间的细粒度声学与语言特征差异。系统根据输入的方言标签合成相应方言的语音。

Result: 通过大量客观和主观实验验证，TMD-TTS在方言表达能力上明显优于现有基线方法。同时，论文还通过语音到语音方言转换任务进一步检验了合成语音的质量和用途。

Conclusion: TMD-TTS有效提升了藏语多方言TTS系统的性能，为低资源多方言语音合成和方言转换提供了有力的方法基础。

Abstract: Tibetan is a low-resource language with limited parallel speech corpora
spanning its three major dialects (\"U-Tsang, Amdo, and Kham), limiting
progress in speech modeling. To address this issue, we propose TMD-TTS, a
unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes
parallel dialectal speech from explicit dialect labels. Our method features a
dialect fusion module and a Dialect-Specialized Dynamic Routing Network
(DSDR-Net) to capture fine-grained acoustic and linguistic variations across
dialects. Extensive objective and subjective evaluations demonstrate that
TMD-TTS significantly outperforms baselines in dialectal expressiveness. We
further validate the quality and utility of the synthesized speech through a
challenging Speech-to-Speech Dialect Conversion (S2SDC) task.

</details>


### [352] [ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning](https://arxiv.org/abs/2509.18063)
*Jan-Felix Klein,Lars Ohnemus*

Main category: cs.CL

TL;DR: 本文提出了ARK-V1，一个结合知识图谱和大语言模型的简单智能体，在自然语言问答任务中实现了更高的准确率，尤其在需要多步推理和领域知识的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型具有推理能力，但其内在知识有时不够准确或过时，尤其在特定领域知识问答中存在不足。知识图谱可以补充外部结构化知识，但其复杂性与多跳推理的需求带来集成挑战。

Method: 提出了一种名为ARK-V1的KG-agent，能够通过迭代探索知识图谱来回答自然语言问题，并在未微调的大型语言模型基础上构建。实验在CoLoTa数据集上进行，旨在考察融合KG推理与常识推理的效果。

Result: ARK-V1在条件准确率上明显优于Chain-of-Thought等基线方法，并且更大的骨干预训练语言模型带来了更高的覆盖率、正确性和稳定性。

Conclusion: ARK-V1通过简洁迭代的图探索方法，将知识图谱与大语言模型有效结合，提高了复杂实体上的问答能力，并证明更大模型有助于增强性能。

Abstract: Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.

</details>


### [353] [SEQR: Secure and Efficient QR-based LoRA Routing](https://arxiv.org/abs/2509.18093)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种无监督LoRA路由算法SEQR，通过最大化激活范数实现高效、可扩展的LoRA选择，从而提升多任务表现与效率。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型参数高效微调中，LoRA已成为主流技术。然而，针对特定任务高效自动选择对应LoRA适配器仍存在挑战，尤其在有隐私约束、无法进行监督训练的环境下。

Method: 作者将无监督LoRA路由形式化为激活范数最大化问题，并提出SEQR算法——依据输入自动选择能产生活跃最大范数的LoRA，保证高效且严格的路由选择。该算法具备理论分析基础，并证明其能准确且高效地选取适配器。

Result: 通过实验，SEQR显著提升了多任务环境下的LoRA选择效率和表现，且在动态LoRA组合上表现出高度可扩展性和实际有效性。

Conclusion: SEQR为动态、多任务场景下的LoRA选择提供了高效且无监督的新方案，兼具理论支撑和实际效果，适用于隐私敏感或无标签环境。

Abstract: Low-Rank Adaptation (LoRA) has become a standard technique for
parameter-efficient fine-tuning of large language models, enabling large
libraries of LoRAs, each for a specific task or domain. Efficiently selecting
the correct LoRA adapter for a given input remains a challenge, particularly in
secure environments where supervised training of routers may raise privacy
concerns. Motivated by previous approaches, we formalize the goal of
unsupervised LoRA routing in terms of activation norm maximization, providing a
theoretical framework for analysis. We demonstrate the discriminative power of
activation norms and introduce SEQR, an unsupervised LoRA routing algorithm
designed to maximize efficiency while providing strict routing guarantees. SEQR
provably identifies the norm-maximizing adapter with significantly greater
efficiency, making it a highly scalable and effective solution for dynamic LoRA
composition. We validate our results through experiments that demonstrate
improved multi-task performance and efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [354] [RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving](https://arxiv.org/abs/2509.16261)
*Shuocheng Yang,Zikun Xu,Jiahao Wang,Shahid Nawaz,Jianqiang Wang,Shaobing Xu*

Main category: cs.RO

TL;DR: 本文提出了一种基于雷达的目标检测框架RaFD，通过估算帧间鸟瞰流（BEV flow）引入几何线索，有效提升检测精度，并在RADIATE数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶场景中，虽然雷达在感知上具有鲁棒性，但原始雷达图像存在严重噪声和虚假伪影，导致仅依赖语义特征进行目标检测极为困难。该研究的动机在于突破这种雷达感知的瓶颈，结合几何信息提升检测效果。

Method: 作者提出RaFD框架，设计了一个监督式流估算辅助任务，与主检测网络联合训练。通过估算帧间BEV流，模型利用得到的几何线索来指导特征从前一帧向当前帧的有效传播，从而提升目标检测精度。

Result: 提出的方法在RADIATE雷达数据集上取得了当前最优的检测性能，证明所引入的几何信息对提升检测结果有显著作用。

Conclusion: 结合几何流特征能够显著改善基于雷达的目标检测效果，对语义信息模糊的雷达信号赋予更有效的判别能力，为自动驾驶等领域的鲁棒感知提供了新的思路。

Abstract: Radar has shown strong potential for robust perception in autonomous driving;
however, raw radar images are frequently degraded by noise and "ghost"
artifacts, making object detection based solely on semantic features highly
challenging. To address this limitation, we introduce RaFD, a radar-based
object detection framework that estimates inter-frame bird's-eye-view (BEV)
flow and leverages the resulting geometric cues to enhance detection accuracy.
Specifically, we design a supervised flow estimation auxiliary task that is
jointly trained with the detection network. The estimated flow is further
utilized to guide feature propagation from the previous frame to the current
one. Our flow-guided, radar-only detector achieves achieves state-of-the-art
performance on the RADIATE dataset, underscoring the importance of
incorporating geometric information to effectively interpret radar signals,
which are inherently ambiguous in semantics.

</details>


### [355] [Tactile-Based Human Intent Recognition for Robot Assistive Navigation](https://arxiv.org/abs/2509.16353)
*Shaoting Peng,Dakarai Crowder,Wenzhen Yuan,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文介绍了一种新型机器人辅助导航系统Tac-Nav，利用圆柱形触觉皮肤和新算法CK-SVM，实现更直观高效的人机交互和导航意图识别，在实测与模拟中均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人辅助导航系统的人机交互方式无法很好复现人与人之间直观高效的物理沟通，限制了对行动障碍者的辅助效果。需要更自然、高效的交互界面提升自主移动能力。

Method: 提出Tac-Nav系统，在移动机器人上安装圆柱形触觉皮肤传感器，通过自主设计的CK-SVM算法进行导航意图识别，该算法建模了圆柱体几何特性，增强对用户手势旋转的不变性。通过模拟环境与现实数据集进行对比实验，并开展用户调查。

Result: CK-SVM在模拟数据集和实际数据集上导航意图分类准确率分别达97.1%与90.8%，均优于四种基线模型。用户实验结果表明，触觉界面比传统摇杆和语音控制更受用户喜爱。

Conclusion: Tac-Nav系统和CK-SVM模型能更好地理解和响应用户的导航意图，为行动障碍者带来更直观有效的辅助导航体验，有望推动辅助导航技术的新发展。

Abstract: Robot assistive navigation (RAN) is critical for enhancing the mobility and
independence of the growing population of mobility-impaired individuals.
However, existing systems often rely on interfaces that fail to replicate the
intuitive and efficient physical communication observed between a person and a
human caregiver, limiting their effectiveness. In this paper, we introduce
Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a
Stretch 3 mobile manipulator to provide a more natural and efficient interface
for human navigational intent recognition. To robustly classify the tactile
data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an
algorithm that explicitly models the sensor's cylindrical geometry and is
consequently robust to the natural rotational shifts present in a user's grasp.
Comprehensive experiments were conducted to demonstrate the effectiveness of
our classification algorithm and the overall system. Results show that CK-SVM
achieved superior classification accuracy on both simulated (97.1%) and
real-world (90.8%) datasets compared to four baseline models. Furthermore, a
pilot study confirmed that users more preferred the Tac-Nav tactile interface
over conventional joystick and voice-based controls.

</details>


### [356] [Dynamic Objects Relocalization in Changing Environments with Flow Matching](https://arxiv.org/abs/2509.16398)
*Francesco Argenziano,Miguel Saavedra-Ruiz,Sacha Morin,Daniele Nardi,Liam Paull*

Main category: cs.RO

TL;DR: 本文提出FlowMaps模型，能够推断动态环境中物体空间与时间上的多模态位置，用于提高机器人在多变环境下的任务与运动规划能力。


<details>
  <summary>Details</summary>
Motivation: 机械人在如家庭、仓库等动态场景中进行任务和运动规划时，经常因物体被人类活动移走或移动而面临重新定位失败的风险。现有方法忽略了人类与物体交互的习惯性和重复性，作者希望利用这些潜在规律提升物体重新定位的成功率。

Method: 提出FlowMaps模型，该模型基于Flow Matching方法，通过学习环境中人类与物体的互动规律，推断物体在不同时刻、不同地点可能出现的位置分布，实现多模态对象位置预测。

Result: 通过实验，作者获得了统计证据，证明FlowMaps能有效推断出物体在动态环境中的可能位置，为未知物体重新定位问题提供可行方案。相关代码已开源。

Conclusion: FlowMaps利用人类活动模式提升了机器人在动态环境下的重新定位能力，为机器人任务与运动规划研究指明了新方向，并具备进一步扩展至复杂应用场景的潜力。

Abstract: Task and motion planning are long-standing challenges in robotics, especially
when robots have to deal with dynamic environments exhibiting long-term
dynamics, such as households or warehouses. In these environments, long-term
dynamics mostly stem from human activities, since previously detected objects
can be moved or removed from the scene. This adds the necessity to find such
objects again before completing the designed task, increasing the risk of
failure due to missed relocalizations. However, in these settings, the nature
of such human-object interactions is often overlooked, despite being governed
by common habits and repetitive patterns. Our conjecture is that these cues can
be exploited to recover the most likely objects' positions in the scene,
helping to address the problem of unknown relocalization in changing
environments. To this end we propose FlowMaps, a model based on Flow Matching
that is able to infer multimodal object locations over space and time. Our
results present statistical evidence to support our hypotheses, opening the way
to more complex applications of our approach. The code is publically available
at https://github.com/Fra-Tsuna/flowmaps

</details>


### [357] [Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation](https://arxiv.org/abs/2509.16412)
*Zihao Deng,Peng Gao,Williard Joshua Jose,Maggie Wigness,John Rogers,Brian Reily,Christopher Reardon,Hao Zhang*

Main category: cs.RO

TL;DR: 提出了一种用于多机器人队伍自适应分组与队形变化的新方法STAF，并通过仿真与实物实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多机器人协同导航任务中，队伍经常需要保持特定队形（比如包围保护人类同伴），但在狭窄或复杂环境中，严格保持预设队形不可行。这就需要机器人能够动态拆分为小队，并且灵活重组，适应环境变化。

Method: 提出并实现了STAF方法，基于统一的分层学习框架，包括：（1）高层基于深度图切割的团队拆分策略；（2）中层基于图神经网络的各小组间协同导航；（3）低层的策略学习实现单机器人目标定位与避碰。

Result: 通过室内、室外多机器人真实与模拟实验，STAF展现了强大的自主分队与自适应队形控制能力，能有效应对狭窄和复杂场景下的协同导航任务。

Conclusion: STAF方法为多机器人队伍在多变复杂环境下的协同导航提供了新能力，验证了其实用性，对实际多机器人系统部署具有重要应用价值。

Abstract: Coordinated multi-robot navigation is essential for robots to operate as a
team in diverse environments. During navigation, robot teams usually need to
maintain specific formations, such as circular formations to protect human
teammates at the center. However, in complex scenarios such as narrow
corridors, rigidly preserving predefined formations can become infeasible.
Therefore, robot teams must be capable of dynamically splitting into smaller
subteams and adaptively controlling the subteams to navigate through such
scenarios while preserving formations. To enable this capability, we introduce
a novel method for SubTeaming and Adaptive Formation (STAF), which is built
upon a unified hierarchical learning framework: (1) high-level deep graph cut
for team splitting, (2) intermediate-level graph learning for facilitating
coordinated navigation among subteams, and (3) low-level policy learning for
controlling individual mobile robots to reach their goal positions while
avoiding collisions. To evaluate STAF, we conducted extensive experiments in
both indoor and outdoor environments using robotics simulations and physical
robot teams. Experimental results show that STAF enables the novel capability
for subteaming and adaptive formation control, and achieves promising
performance in coordinated multi-robot navigation through challenging
scenarios. More details are available on the project website:
https://hcrlab.gitlab.io/project/STAF.

</details>


### [358] [End-to-end RL Improves Dexterous Grasping Policies](https://arxiv.org/abs/2509.16434)
*Ritvik Singh,Karl Van Wyk,Pieter Abbeel,Jitendra Malik,Nathan Ratliff,Ankur Handa*

Main category: cs.RO

TL;DR: 本论文提出了一种新的方法，通过将模拟器和强化学习训练分别分配到不同GPU上，实现了视觉端到端强化学习在灵巧抓取任务中的高效扩展，并显著提升了训练效率和实际表现。


<details>
  <summary>Details</summary>
Motivation: 视觉端到端强化学习虽然能实现更复杂和自适应的感知—动作映射，但由于内存和批次大小受限，难以扩展至大规模训练，尤其是在多GPU环境下，现有的数据并行方式效率低下。作者希望解决这一扩展瓶颈。

Method: 作者提出将模拟器和强化学习（包括训练及经验池）“解耦”部署到不同GPU上。在一个四卡节点上，用三块GPU跑模拟器，一块GPU专门用于PPO训练，与传统数据并行方式进行对比。并对深度信息与状态信息分别进行策略蒸馏，再将其进一步蒸馏进立体RGB网络。

Result: 该方法在同等GPU数量下可将环境并发数翻倍，不仅大幅提升视觉端到端RL的训练效果，还实现了更优的仿真与现实表现。深度蒸馏进RGB网络比状态蒸馏效果更好，加大了实际部署批次后，现实抓取表现也有显著提升。

Conclusion: GPU间模拟和训练的解耦极大提升了基于视觉的端到端RL抓取系统的训练扩展能力和实际表现。深度蒸馏显著优于状态蒸馏，提出的方案在现实环境中超过了当前视觉端到端抓取方法的SOTA。

Abstract: This work explores techniques to scale up image-based end-to-end learning for
dexterous grasping with an arm + hand system. Unlike state-based RL,
vision-based RL is much more memory inefficient, resulting in relatively low
batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is
still an attractive method as unlike the more commonly used techniques which
distill state-based policies into vision networks, end-to-end RL can allow for
emergent active vision behaviors. We identify a key bottleneck in training
these policies is the way most existing simulators scale to multiple GPUs using
traditional data parallelism techniques. We propose a new method where we
disaggregate the simulator and RL (both training and experience buffers) onto
separate GPUs. On a node with four GPUs, we have the simulator running on three
of them, and PPO running on the fourth. We are able to show that with the same
number of GPUs, we can double the number of existing environments compared to
the previous baseline of standard data parallelism. This allows us to train
vision-based environments, end-to-end with depth, which were previously
performing far worse with the baseline. We train and distill both depth and
state-based policies into stereo RGB networks and show that depth distillation
leads to better results, both in simulation and reality. This improvement is
likely due to the observability gap between state and vision policies which
does not exist when distilling depth policies into stereo RGB. We further show
that the increased batch size brought about by disaggregated simulation also
improves real world performance. When deploying in the real world, we improve
upon the previous state-of-the-art vision-based results using our end-to-end
policies.

</details>


### [359] [FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning](https://arxiv.org/abs/2509.16445)
*Naoki Yokoyama,Sehoon Ha*

Main category: cs.RO

TL;DR: 本文提出FiLM-Nav方法，通过直接微调预训练视觉-语言模型（VLM）实现语义导航，显著提升了机器人在复杂环境中基于自然语言目标的导航性能，在多项公开基准上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基础模型在真实机器人导航中泛化能力不足，尤其是如何让这些模型的泛化知识转化为具体决策能力仍有挑战。提升VLM在实际导航中的泛化和决策效果对于机器人真实部署至关重要。

Method: 提出FiLM-Nav，将预训练VLM直接作为导航策略，通过在多种仿真任务（如ObjectNav、OVON、ImageNav及空间推理）上的多样化数据进行微调，使其能基于视觉轨迹历史和目标自适应选择最优探索方向。强调多任务微调对于泛化性的关键作用。

Result: FiLM-Nav在HM3D ObjectNav和HM3D-OVON基准上分别取得了SOTA的SPL和成功率，并在未知目标类别上显示出强泛化能力，超越了以往只用基础模型zero-shot推断或注释地图的方法。

Conclusion: 验证了在多样仿真任务数据上直接微调VLM是一条行之有效的提升机器人导航泛化能力的路径，为语义导航领域的实际应用和发展提供了重要思路。

Abstract: Enabling robotic assistants to navigate complex environments and locate
objects described in free-form language is a critical capability for real-world
deployment. While foundation models, particularly Vision-Language Models
(VLMs), offer powerful semantic understanding, effectively adapting their
web-scale knowledge for embodied decision-making remains a key challenge. We
present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that
directly fine-tunes pre-trained VLM as the navigation policy. In contrast to
methods that use foundation models primarily in a zero-shot manner or for map
annotation, FiLM-Nav learns to select the next best exploration frontier by
conditioning directly on raw visual trajectory history and the navigation goal.
Leveraging targeted simulated embodied experience allows the VLM to ground its
powerful pre-trained representations in the specific dynamics and visual
patterns relevant to goal-driven navigation. Critically, fine-tuning on a
diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary
spatial reasoning task proves essential for achieving robustness and broad
generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success
rate on HM3D ObjectNav among open-vocabulary methods, and sets a
state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating
strong generalization to unseen object categories. Our work validates that
directly fine-tuning VLMs on diverse simulated embodied data is a highly
effective pathway towards generalizable and efficient semantic navigation
capabilities.

</details>


### [360] [A Framework for Optimal Ankle Design of Humanoid Robots](https://arxiv.org/abs/2509.16469)
*Guglielmo Cervettini,Roberto Mauceri,Alex Coppola,Fabio Bergonti,Luca Fiorio,Marco Maggiali,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文提出了一种针对人形机器人踝关节并联机构的统一设计与评估方法，通过多目标优化和性能聚合指标，实现不同机构方案的量化对比，并在现有机器人平台上验证了优化效果。优化的RSU机构在成本函数上相较原串联设计和常规RSU分别降低了41%和14%。


<details>
  <summary>Details</summary>
Motivation: 人形机器人踝关节的设计对于地面交互的安全性与效率至关重要。机械柔顺性和电机质量分布等关键要素推动了并联机构的采用，但不同任务需求和执行器选择使得最优方案的选取十分复杂，因此亟需统一的设计与量化比较方法。

Method: 作者提出了一个统一的设计方法论，针对并联踝关节机构，综合利用多目标优化合成机构几何形状，并用标量化成本函数将主要性能指标聚合，实现不同机构体系的量化评价与对比。论文着重分析了SPU（球-移-万向节）和RSU（转-球-万向节）两类代表性并联架构，推导运动学，并对RSU引入新的参数化方法以保证工作空间和加速优化过程。

Result: 使用该方法对现有人形机器人踝关节进行重新设计，优化后的RSU机构在综合性能上优于原有串联设计以及常规工程RSU，成本函数降低分别达到41%和14%。

Conclusion: 统一设计和评估方法有效提升了并联踝关节结构的性能优化效率和横向对比能力，优化后的RSU结构在实际机器人平台上展现出更优的性能，有助于推动人形机器人踝关节并联机制的工程应用与发展。

Abstract: The design of the humanoid ankle is critical for safe and efficient ground
interaction. Key factors such as mechanical compliance and motor mass
distribution have driven the adoption of parallel mechanism architectures.
However, selecting the optimal configuration depends on both actuator
availability and task requirements. We propose a unified methodology for the
design and evaluation of parallel ankle mechanisms. A multi-objective
optimization synthesizes the mechanism geometry, the resulting solutions are
evaluated using a scalar cost function that aggregates key performance metrics
for cross-architecture comparison. We focus on two representative
architectures: the Spherical-Prismatic-Universal (SPU) and the
Revolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and
for the RSU, introduce a parameterization that ensures workspace feasibility
and accelerates optimization. We validate our approach by redesigning the ankle
of an existing humanoid robot. The optimized RSU consistently outperforms both
the original serial design and a conventionally engineered RSU, reducing the
cost function by up to 41% and 14%, respectively.

</details>


### [361] [Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems](https://arxiv.org/abs/2509.16482)
*Pranav Tiwari,Soumyodipta Nath*

Main category: cs.RO

TL;DR: 该论文提出了一种新的多智能体协调路径跟随控制策略，并在仿真中证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在物流自动化、监控和协作探索等领域有广泛应用，但传统方法基于时间参数化，易导致同步和灵活性差的问题，因此需要新的、更鲁棒的方案。

Method: 提出Robot Conga领航-跟随控制策略，使各智能体依据领航者的空间位移而不是时间更新自身状态。假定系统能获取全局位置信息（如室内定位系统），在TurtleBot3和四足机器人Laikago的仿真环境下验证算法有效性。

Result: 实验显示各机器人能实现精确的轨迹跟踪、稳定的间隔控制。四足机器人在约0.25秒内全部对齐，TurtleBot3几乎瞬间完成队形，表现优异。

Conclusion: Robot Conga方法实现了多智能体间高效、稳定的顺序路径跟随，对解决传统方法的同步和僵化问题具有明显优势，具备室内实际应用潜力。

Abstract: Coordinated path following in multi-agent systems is a key challenge in
robotics, with applications in automated logistics, surveillance, and
collaborative exploration. Traditional formation control techniques often rely
on time-parameterized trajectories and path integrals, which can result in
synchronization issues and rigid behavior. In this work, we address the problem
of sequential path following, where agents maintain fixed spatial separation
along a common trajectory, guided by a leader under centralized control. We
introduce Robot Conga, a leader-follower control strategy that updates each
agent's desired state based on the leader's spatial displacement rather than
time, assuming access to a global position reference, an assumption valid in
indoor environments equipped with motion capture, vision-based tracking, or UWB
localization systems. The algorithm was validated in simulation using both
TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate
trajectory tracking, stable inter-agent spacing, and fast convergence, with all
agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped
case, and almost instantaneously in the TurtleBot3 implementation.

</details>


### [362] [Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms](https://arxiv.org/abs/2509.16492)
*Tinapat Limsila,Mehul Sharma,Paulo Garcia*

Main category: cs.RO

TL;DR: 本文提出了一种不依赖于底层实现时间的机器人群体系统设计形式化验证方法，能够在面对硬件或部署时序变化时保证系统的正确性并修正设计缺陷。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的时序不可预测性会导致系统进入错误的元状态，尤其在机器人群体系统中，底层实现的多样性与变化性让可靠性验证更加困难。传统的经验性验证因状态空间过大而成本极高。

Method: 利用并发过程演算（如Communicating Sequential Processes），本文提出一种形式化建模与验证方法，自动识别并修正可能导致系统错误元状态的设计，并保证在时序变化条件下元状态的稳定性。

Result: 在存在已知故障的机器人群体系统上，无论是仿真还是现实实验，方法均可有效发现并修正系统非法元状态，使其在修正后表现出一致的正确行为，验证了方法有效性。

Conclusion: 方法能跨越不同的设计与实现底层，提升机器人群体系统的可靠性与稳定性，是可移植的形式化方法，有助于丰富机器人研究领域的工具箱。

Abstract: Emergent properties in distributed systems arise due to timing
unpredictability; asynchronous state evolution within each sub-system may lead
the macro-system to faulty meta-states. Empirical validation of correctness is
often prohibitively expensive, as the size of the state-space is too large to
be tractable. In robotic swarms this problem is exacerbated, when compared to
software systems, by the variability of the implementation substrate across the
design, or even the deployment, process. We present an approach for formally
reasoning about the correctness of robotic swarm design in a
substrate-timing-independent way. By leveraging concurrent process calculi
(namely, Communicating Sequential Processes), we introduce a methodology that
can automatically identify possible causes of faulty meta-states and correct
such designs such that meta-states are consistently stable, even in the
presence of timing variability due to substrate changes. We evaluate this
approach on a robotic swarm with a clearly identified fault, realized in both
simulation and reality. Results support the research hypothesis, showing that
the swarm reaches an illegal meta-state before the correction is applied, but
behaves consistently correctly after the correction. Our techniques are
transferable across different design methodologies, contributing to the toolbox
of formal methods for roboticists.

</details>


### [363] [No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning](https://arxiv.org/abs/2509.16532)
*Run Yu,Yangdi Liu,Wen-Da Wei,Chen Li*

Main category: cs.RO

TL;DR: 该论文提出了一种新框架NoReal3D，通过3DStructureFormer模块从单目图片生成具有几何意义的伪点云特征，并与2D编码器输出融合，实现无需真实点云数据即可获得与3D点云方法相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于3D点云的方法在机器人操作任务中具有优越的策略表现和泛化能力，但高昂的数据采集成本限制了其实际应用的规模和落地能力。为解决这个问题，亟需一种既能获取3D空间信息、又能减少数据成本的方法。

Method: 论文提出了一种可学习的3D感知模块3DStructureFormer，将单目图像转换为几何和拓扑结构保留的伪点云特征，并设计了伪点云编码器进行特征保留。实验研究了不同特征融合策略，在2D编码器输出与伪点云特征间进行有效结合。

Result: 该方法在多项任务上进行实验，结果显示，不依赖真实点云数据的NoReal3D框架性能可以与基于3D点云的方法相媲美。

Conclusion: 通过该框架，机器人可以消除3D点云采集带来的高额成本，同时实现对空间结构的有效理解，为实际大规模应用提供了有效解决方案。

Abstract: Recently,vision-based robotic manipulation has garnered significant attention
and witnessed substantial advancements. 2D image-based and 3D point cloud-based
policy learning represent two predominant paradigms in the field, with recent
studies showing that the latter consistently outperforms the former in terms of
both policy performance and generalization, thereby underscoring the value and
significance of 3D information. However, 3D point cloud-based approaches face
the significant challenge of high data acquisition costs, limiting their
scalability and real-world deployment. To address this issue, we propose a
novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable
3D perception module capable of transforming monocular images into
geometrically meaningful pseudo-point cloud features, effectively fused with
the 2D encoder output features. Specially, the generated pseudo-point clouds
retain geometric and topological structures so we design a pseudo-point cloud
encoder to preserve these properties, making it well-suited for our framework.
We also investigate the effectiveness of different feature fusion
strategies.Our framework enhances the robot's understanding of 3D spatial
structures while completely eliminating the substantial costs associated with
3D point cloud acquisition.Extensive experiments across various tasks validate
that our framework can achieve performance comparable to 3D point cloud-based
methods, without the actual point cloud data.

</details>


### [364] [TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation](https://arxiv.org/abs/2509.16550)
*Yinghao Wu,Shuhong Hou,Haowen Zheng,Yichen Li,Weiyi Lu,Xun Zhou,Yitian Shao*

Main category: cs.RO

TL;DR: TranTac是一种低成本且数据高效的机器人触觉感知与控制框架，利用集成在夹持器指尖的6轴惯性测量单元(IMU)传感器，通过Transformer和扩散策略模仿人类插入行为，实现精细插入任务中的动态6自由度控制，取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人插入任务在视觉感知不足时容易失败，触觉感知虽重要，但现有方案要么灵敏度不足，要么对数据量要求过高，因此亟需一种高灵敏、低数据需求且低成本的新型触觉感知与控制方案。

Method: TranTac在机器人夹持器柔性指尖内集成单颗6轴IMU传感器，能检测微米级平移和扭转微小形变；利用Transformer编码器和扩散策略，模仿人类插入物品时通过短暂触觉信号动态修正物体6自由度位姿，实现高效的力-感知插入控制。

Result: TranTac与视觉结合下，物品抓取和插入平均成功率为79%，优于单视觉方案及内置力/力矩增强方案；在仅靠触觉情况下，插入任务平均成功率为88%；泛化性实验表明即使在未见过的新物品任务上成功率也达70%。

Conclusion: TranTac框架以低成本和高数据效率实现了高灵敏触觉感知与精细控制，为机器人在精细操作任务中提供新型触觉系统，有望激发更多相关研究。

Abstract: Robotic manipulation tasks such as inserting a key into a lock or plugging a
USB device into a port can fail when visual perception is insufficient to
detect misalignment. In these situations, touch sensing is crucial for the
robot to monitor the task's states and make precise, timely adjustments.
Current touch sensing solutions are either insensitive to detect subtle changes
or demand excessive sensor data. Here, we introduce TranTac, a data-efficient
and low-cost tactile sensing and control framework that integrates a single
contact-sensitive 6-axis inertial measurement unit within the elastomeric tips
of a robotic gripper for completing fine insertion tasks. Our customized
sensing system can detect dynamic translational and torsional deformations at
the micrometer scale, enabling the tracking of visually imperceptible pose
changes of the grasped object. By leveraging transformer-based encoders and
diffusion policy, TranTac can imitate human insertion behaviors using transient
tactile cues detected at the gripper's tip during insertion processes. These
cues enable the robot to dynamically control and correct the 6-DoF pose of the
grasped object. When combined with vision, TranTac achieves an average success
rate of 79% on object grasping and insertion tasks, outperforming both
vision-only policy and the one augmented with end-effector 6D force/torque
sensing. Contact localization performance is also validated through
tactile-only misaligned insertion tasks, achieving an average success rate of
88%. We assess the generalizability by training TranTac on a single prism-slot
pair and testing it on unseen data, including a USB plug and a metal key, and
find that the insertion tasks can still be completed with an average success
rate of nearly 70%. The proposed framework may inspire new robotic tactile
sensing systems for delicate manipulation tasks.

</details>


### [365] [Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly](https://arxiv.org/abs/2509.16611)
*Xiwei Zhao,Yiwei Wang,Yansong Wu,Fan Wu,Teng Sun,Zhonghua Miao,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种基于视频和行为树的新型机器人装配系统，能够将人类示范视频分解为可执行的任务结构，并在实际装配中表现出高度的灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人装配方式需要针对每种产品由专家手工编程，缺乏对于产品变化和实际工况扰动的适应性和鲁棒性，限制了自动化装配系统在现代制造业中的应用。为此，需要开发更加灵活和可适应变化的机器人装配方法。

Method: 该工作提出了Video-to-BT层次化框架，将高层的视频-语言模型（VLM）用于解析人类装配示范视频，自动识别并分解为子任务，再自动生成相应的行为树（BT）结构。该行为树不仅作为装配计划输出，也是实际执行的控制结构；系统在执行过程中根据场景实时解释做出反应，一旦失败则触发基于VLM的重新规划，实现闭环控制。

Result: 通过在真实装配任务上的实验验证，该方法在长时任务规划的可靠性、鲁棒性，以及面对复杂扰动时的泛化能力上表现优越。

Conclusion: Video-to-BT框架实现了高层认知规划与低层反应控制的无缝结合，有效提升了机器人装配系统的灵活性、可靠性和适应动态变化的能力。

Abstract: Modern manufacturing demands robotic assembly systems with enhanced
flexibility and reliability. However, traditional approaches often rely on
programming tailored to each product by experts for fixed settings, which are
inherently inflexible to product changes and lack the robustness to handle
variations. As Behavior Trees (BTs) are increasingly used in robotics for their
modularity and reactivity, we propose a novel hierarchical framework,
Video-to-BT, that seamlessly integrates high-level cognitive planning with
low-level reactive control, with BTs serving both as the structured output of
planning and as the governing structure for execution. Our approach leverages a
Vision-Language Model (VLM) to decompose human demonstration videos into
subtasks, from which Behavior Trees are generated. During the execution, the
planned BTs combined with real-time scene interpretation enable the system to
operate reactively in the dynamic environment, while VLM-driven replanning is
triggered upon execution failure. This closed-loop architecture ensures
stability and adaptivity. We validate our framework on real-world assembly
tasks through a series of experiments, demonstrating high planning reliability,
robust performance in long-horizon assembly tasks, and strong generalization
across diverse and perturbed conditions. Project website:
https://video2bt.github.io/video2bt_page/

</details>


### [366] [ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks](https://arxiv.org/abs/2509.16614)
*Bojan Derajić,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 本文提出了一种基于Hamilton-Jacobi可达性分析的观测条件神经控制障碍函数（CBFs），可有效提升自动系统的安全控制性能，并在仿真和实物实验中优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 尽管CBF在安全关键自动化控制中应用广泛，但其设计困难且传统学习方法存在安全集次优、对部分可观环境适应性差及安全性缺乏理论保证等问题，因此亟需新方法提升可靠性和泛化能力。

Method: 作者提出了一种结合Hamilton-Jacobi (HJ) 可达性分析的神经CBF，通过对观测信息条件化，用超网络结构对安全滤波器进行建模，确保预测出的安全集不会与已观测的失败集相交，理论上提升安全性。

Result: 该方法在仿真及地面机器人、四旋翼硬件试验中表现优越，有效提升安全过滤、成功率和对未知环境的泛化能力，相较现有方法有明显改进。

Conclusion: 观测条件神经CBF结合HJ分析和超网络结构，能够突破传统CBF设计和机器学习方法的局限，在安全关键自动控制系统中具备更强的理论保障和实际适用性。

Abstract: Control barrier functions (CBFs) have been demonstrated as an effective
method for safety-critical control of autonomous systems. Although CBFs are
simple to deploy, their design remains challenging, motivating the development
of learning-based approaches. Yet, issues such as suboptimal safe sets,
applicability in partially observable environments, and lack of rigorous safety
guarantees persist. In this work, we propose observation-conditioned neural
CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately
recover the maximal safe sets. We exploit certain mathematical properties of
the HJ value function, ensuring that the predicted safe set never intersects
with the observed failure set. Moreover, we leverage a hypernetwork-based
architecture that is particularly suitable for the design of
observation-conditioned safety filters. The proposed method is examined both in
simulation and hardware experiments for a ground robot and a quadcopter. The
results show improved success rates and generalization to out-of-domain
environments compared to the baselines.

</details>


### [367] [LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning](https://arxiv.org/abs/2509.16615)
*Jelle Luijkx,Runyu Ma,Zlatan Ajanović,Jens Kober*

Main category: cs.RO

TL;DR: 本论文提出了一种结合大语言模型（LLM）与强化学习的新框架LLM-TALE，显著提升了机器人操作任务中的学习效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在机器人操作任务上样本效率低，且大状态-动作空间的探索需要大量数据。虽然引入LLM能基于常识指导探索，但LLM生成的计划常常存在物理不可行问题，导致行为不可靠。需要兼顾高效探索与计划可行性的问题。

Method: 提出LLM-TALE框架，将LLM的任务级与可供性级（affordance-level）规划结合到强化学习的探索中，通过语义引导探索、在线修正LLM产生的次优计划，并实现多模态的无监督可供性计划探索。

Result: 在标准RL基准的pick-and-place任务和真实机器人实验中，LLM-TALE在样本效率和成功率上均优于其它强基线方法，支持零样本仿真到真实的迁移。

Conclusion: LLM-TALE利用LLM的规划能力，增强了RL在机器人操作任务中的探索效率与实际可靠性，展示了大模型与RL结合的新范式的应用潜力。

Abstract: Reinforcement learning (RL) is a promising approach for robotic manipulation,
but it can suffer from low sample efficiency and requires extensive exploration
of large state-action spaces. Recent methods leverage the commonsense knowledge
and reasoning abilities of large language models (LLMs) to guide exploration
toward more meaningful states. However, LLMs can produce plans that are
semantically plausible yet physically infeasible, yielding unreliable behavior.
We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer
RL exploration. LLM-TALE integrates planning at both the task level and the
affordance level, improving learning efficiency by directing agents toward
semantically meaningful actions. Unlike prior approaches that assume optimal
LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and
explores multimodal affordance-level plans without human supervision. We
evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing
improvements in both sample efficiency and success rates over strong baselines.
Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code
and supplementary material are available at https://llm-tale.github.io.

</details>


### [368] [KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control](https://arxiv.org/abs/2509.16638)
*Jinrui Han,Weiji Xie,Jiakun Zheng,Jiyuan Shi,Weinan Zhang,Ting Xiao,Chenjia Bai*

Main category: cs.RO

TL;DR: 本论文提出了一种名为VMS的统一全身控制器，使得人形机器人能在单一策略下学习多样且动态的全身技能，并表现出良好的泛化能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 推动人形机器人具备通用能力，关键在于能够通过模仿各种人类动作，学习广泛的全身技能，同时保证动作的长期稳定性。然而，单一策略同时掌握多种动作技能且保持稳定性，这一任务难度极大。

Method: 作者提出VMS框架，包含：1）混合追踪目标，权衡局部动作精确性与全局轨迹一致性；2）正交专家混合（OMoE）网络结构，实现动作技能的专项化且提升对新动作的泛化能力；3）分段追踪奖励，让模仿任务对瞬时误差和全局位移更具鲁棒性。

Result: 在模拟与现实实验中，VMS能够准确模仿动态技能，长时间（数分钟）维持动作稳定，并对未见过的动作表现出强泛化能力。

Conclusion: VMS为实现通用、灵活的人形机器人全身控制奠定了可扩展且坚实的基础，在学术与应用上具有广泛潜力。

Abstract: Learning versatile whole-body skills by tracking various human motions is a
fundamental step toward general-purpose humanoid robots. This task is
particularly challenging because a single policy must master a broad repertoire
of motion skills while ensuring stability over long-horizon sequences. To this
end, we present VMS, a unified whole-body controller that enables humanoid
robots to learn diverse and dynamic behaviors within a single policy. Our
framework integrates a hybrid tracking objective that balances local motion
fidelity with global trajectory consistency, and an Orthogonal
Mixture-of-Experts (OMoE) architecture that encourages skill specialization
while enhancing generalization across motions. A segment-level tracking reward
is further introduced to relax rigid step-wise matching, enhancing robustness
when handling global displacements and transient inaccuracies. We validate VMS
extensively in both simulation and real-world experiments, demonstrating
accurate imitation of dynamic skills, stable performance over minute-long
sequences, and strong generalization to unseen motions. These results highlight
the potential of VMS as a scalable foundation for versatile humanoid whole-body
control. The project page is available at
https://kungfubot2-humanoid.github.io.

</details>


### [369] [HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos](https://arxiv.org/abs/2509.16757)
*Haoyang Weng,Yitang Li,Nikhil Sobanbabu,Zihan Wang,Zhengyi Luo,Tairan He,Deva Ramanan,Guanya Shi*

Main category: cs.RO

TL;DR: 提出了HDMI框架，实现了从单目RGB视频中学习整个人形机器人与物体的交互技能，并成功在仿真和真实世界任务中验证了其鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人与物体的整身交互因动作数据稀缺和丰富接触难以建模，导致鲁棒性和泛化性差，难以直接从人类示范数据中学习。

Method: HDMI方法分三步：1）从非结构化视频自动提取并重定向人体与物体轨迹制作结构化数据集；2）利用强化学习，结合统一物体表示、残差动作空间和通用交互奖励，训练机器人协同跟踪自身与物体状态；3）在物理机器人上零样本部署训练好的策略，并在仿真与现实中评估。

Result: 在Unitree G1人形机器人上的仿真到现实实验显示，HDMI能连续完成67次开门任务，并在现实完成6个、仿真完成14个复杂的整体移动-操作任务，验证了方法在不同任务和转移场景下的鲁棒性和通用性。

Conclusion: HDMI框架可作为直接从人类视频习得人形机器互动技能的简洁且通用方法，为未来人形机器人自监督或模仿人类展开复杂交互任务提供基础。

Abstract: Enabling robust whole-body humanoid-object interaction (HOI) remains
challenging due to motion data scarcity and the contact-rich nature. We present
HDMI (HumanoiD iMitation for Interaction), a simple and general framework that
learns whole-body humanoid-object interaction skills directly from monocular
RGB videos. Our pipeline (i) extracts and retargets human and object
trajectories from unconstrained videos to build structured motion datasets,
(ii) trains a reinforcement learning (RL) policy to co-track robot and object
states with three key designs: a unified object representation, a residual
action space, and a general interaction reward, and (iii) zero-shot deploys the
RL policies on real humanoid robots. Extensive sim-to-real experiments on a
Unitree G1 humanoid demonstrate the robustness and generality of our approach:
HDMI achieves 67 consecutive door traversals and successfully performs 6
distinct loco-manipulation tasks in the real world and 14 tasks in simulation.
Our results establish HDMI as a simple and general framework for acquiring
interactive humanoid skills from human videos.

</details>


### [370] [Improve bounding box in Carla Simulator](https://arxiv.org/abs/2509.16773)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.RO

TL;DR: 本文对CARLA自动驾驶仿真平台生成边界框过程进行了改进，旨在减少“幽灵框”和误检，提高物体检测准确性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶训练和测试中，利用数据集中的边界框进行物体检测非常重要，但传统方法会误标被遮挡物体，产生“幽灵框”，影响下游任务表现。

Method: 在CARLA仿真环境中，不仅采集原始的物体坐标和边界框，还对检测结果进行进一步过滤，通过新方法有效剔除遮挡等导致的误检边界框。

Result: 经过性能分析，改进方法明显提升了边界框检测的准确性，有效减少了幽灵框现象。

Conclusion: 提出的改进方法优化了CARLA生成数据集边界框的流程，更适应真实的自动驾驶场景，有助于后续算法开发和测试。

Abstract: The CARLA simulator (Car Learning to Act) serves as a robust platform for
testing algorithms and generating datasets in the field of Autonomous Driving
(AD). It provides control over various environmental parameters, enabling
thorough evaluation. Development bounding boxes are commonly utilized tools in
deep learning and play a crucial role in AD applications. The predominant
method for data generation in the CARLA Simulator involves identifying and
delineating objects of interest, such as vehicles, using bounding boxes. The
operation in CARLA entails capturing the coordinates of all objects on the map,
which are subsequently aligned with the sensor's coordinate system at the ego
vehicle and then enclosed within bounding boxes relative to the ego vehicle's
perspective. However, this primary approach encounters challenges associated
with object detection and bounding box annotation, such as ghost boxes.
Although these procedures are generally effective at detecting vehicles and
other objects within their direct line of sight, they may also produce false
positives by identifying objects that are obscured by obstructions. We have
enhanced the primary approach with the objective of filtering out unwanted
boxes. Performance analysis indicates that the improved approach has achieved
high accuracy.

</details>


### [371] [SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree](https://arxiv.org/abs/2509.16812)
*Priyanshu Agrawal,Shalabh Gupta,Zongyuan Shen*

Main category: cs.RO

TL;DR: SMART-3D是一种面向三维动态环境的自适应重规划算法，能够实时调整路径以应对快速移动的障碍物，具备高成功率与低重规划时间。


<details>
  <summary>Details</summary>
Motivation: 随着无人机、机器人等应用场景向三维空间扩展，动态环境下的实时路径规划需求愈发突出。传统SMART算法局限于二维且受限于网格分解，难以高效拓展到三维空间，因此需要设计一种更高效且适用于3D环境的路径重规划算法。

Method: SMART-3D在SMART算法的基础上进行扩展，采用树状结构以适应三维动态环境。当当前路径被障碍物阻挡时，通过对树结构进行实时形变（morphing）寻找新的安全路径。创新点在于用“热节点（hot-nodes）”替代了原有的“热区（hot-spots）”，避免了网格分解，提高了算法在三维环境下的效率及可扩展性。

Result: 通过大量二维和三维环境下的仿真实验，SMART-3D在包含随机移动动态障碍物的场景中表现出较高的成功率和较低的重规划时间。

Conclusion: SMART-3D有效解决了动态3D环境中路径实时重规划问题，兼具高成功率和实时性，适合实际应用中对机载等实时性要求高的场景。

Abstract: This paper presents SMART-3D, an extension of the SMART algorithm to 3D
environments. SMART-3D is a tree-based adaptive replanning algorithm for
dynamic environments with fast moving obstacles. SMART-3D morphs the underlying
tree to find a new path in real-time whenever the current path is blocked by
obstacles. SMART-3D removed the grid decomposition requirement of the SMART
algorithm by replacing the concept of hot-spots with that of hot-nodes, thus
making it computationally efficient and scalable to 3D environments. The
hot-nodes are nodes which allow for efficient reconnections to morph the
existing tree to find a new safe and reliable path. The performance of SMART-3D
is evaluated by extensive simulations in 2D and 3D environments populated with
randomly moving dynamic obstacles. The results show that SMART-3D achieves high
success rates and low replanning times, thus highlighting its suitability for
real-time onboard applications.

</details>


### [372] [Factorizing Diffusion Policies for Observation Modality Prioritization](https://arxiv.org/abs/2509.16830)
*Omkar Patil,Prabin Rath,Kartikay Pangaonkar,Eric Rosen,Nakul Gopalan*

Main category: cs.RO

TL;DR: 提出了一种新的因子化扩散策略（FDP），使机器人学习中不同观测模态对决策的影响可以被个性化调控，从而提升在少数据和环境分布变换下的表现与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的机器人技能学习方法，往往将所有观测模态（如本体感受、视觉、触觉）等量齐观地融入策略决策，未能区分不同任务中各观测信息的重要性差异，导致应对多变环境时鲁棒性不足。

Method: 本文提出了因子化扩散策略（FDP），在扩散模型的条件化过程中对不同观测模态进行因子化处理，使每种模态对动作生成的影响能够按任务需求有针对性地优先。有别于以往对多模态简单拼接输入，FDP结构性赋予各模态不同权重，从而优化学习过程。

Result: 与传统联合模态扩散策略相比，在数据稀缺情况下，FDP在模拟任务中成功率提升15%。实际及基准测试中，在视觉干扰、摄像头遮挡等分布扰动下，成功率提高40%，明显增强了策略的稳健性和安全性。

Conclusion: FDP通过因子化观测模态，设计上支持了策略对信息源优先级的自适应选择，显著提升了机器人扩散决策策略在现实环境中的任务成功率和鲁棒性，特别适用于分布偏移和动态干扰场景。

Abstract: Diffusion models have been extensively leveraged for learning robot skills
from demonstrations. These policies are conditioned on several observational
modalities such as proprioception, vision and tactile. However, observational
modalities have varying levels of influence for different tasks that diffusion
polices fail to capture. In this work, we propose 'Factorized Diffusion
Policies' abbreviated as FDP, a novel policy formulation that enables
observational modalities to have differing influence on the action diffusion
process by design. This results in learning policies where certain observations
modalities can be prioritized over the others such as $\texttt{vision>tactile}$
or $\texttt{proprioception>vision}$. FDP achieves modality prioritization by
factorizing the observational conditioning for diffusion process, resulting in
more performant and robust policies. Our factored approach shows strong
performance improvements in low-data regimes with $15\%$ absolute improvement
in success rate on several simulated benchmarks when compared to a standard
diffusion policy that jointly conditions on all input modalities. Moreover, our
benchmark and real-world experiments show that factored policies are naturally
more robust with $40\%$ higher absolute success rate across several visuomotor
tasks under distribution shifts such as visual distractors or camera
occlusions, where existing diffusion policies fail catastrophically. FDP thus
offers a safer and more robust alternative to standard diffusion policies for
real-world deployment. Videos are available at
https://fdp-policy.github.io/fdp-policy/ .

</details>


### [373] [Robot Learning with Sparsity and Scarcity](https://arxiv.org/abs/2509.16834)
*Jingxi Xu*

Main category: cs.RO

TL;DR: 本文关注机器人学习领域的数据稀疏和稀缺问题，分别以触觉传感和康复机器人为例，提出利用强化学习与生成型AI等方法高效利用有限数据。


<details>
  <summary>Details</summary>
Motivation: 与自然语言或视觉领域相比，机器人学习普遍缺乏大量可用数据资源。触觉传感数据稀疏且局部，康复机器人领域的生物信号数据更是极其稀缺，因此亟需能高效利用有限数据的学习方法，以推动机器人智能发展。

Method: 1. 针对触觉传感，采用无视觉，仅靠触觉信息的模型无关强化学习，学习触觉探索与操作策略，提升有限触觉数据的利用效率。
2. 针对康复机器人，与医学院合作，通过实验收集卒中患者手部生物信号，借助半监督学习、元学习和生成式AI方法，实现小样本情形下的意图推断。

Result: 在触觉传感领域，提出的基于强化学习的方法能有效利用稀疏的触觉信息，实现无需视觉的操作。在康复机器人领域，提出的方法可用极少量数据准确推断患者意图，并辅助提供对应的物理帮助。

Conclusion: 通过面向数据稀疏与稀缺问题的创新机器学习方法，不仅推动了机器人在复杂场景中的实用性，还为数据受限的医疗康复等领域带来新的AI解决思路。

Abstract: Unlike in language or vision, one of the fundamental challenges in robot
learning is the lack of access to vast data resources. We can further break
down the problem into (1) data sparsity from the angle of data representation
and (2) data scarcity from the angle of data quantity. In this thesis, I will
discuss selected works on two domains: (1) tactile sensing and (2)
rehabilitation robots, which are exemplars of data sparsity and scarcity,
respectively. Tactile sensing is an essential modality for robotics, but
tactile data are often sparse, and for each interaction with the physical
world, tactile sensors can only obtain information about the local area of
contact. I will discuss my work on learning vision-free tactile-only
exploration and manipulation policies through model-free reinforcement learning
to make efficient use of sparse tactile information. On the other hand,
rehabilitation robots are an example of data scarcity to the extreme due to the
significant challenge of collecting biosignals from disabled-bodied subjects at
scale for training. I will discuss my work in collaboration with the medical
school and clinicians on intent inferral for stroke survivors, where a hand
orthosis developed in our lab collects a set of biosignals from the patient and
uses them to infer the activity that the patient intends to perform, so the
orthosis can provide the right type of physical assistance at the right moment.
My work develops machine learning algorithms that enable intent inferral with
minimal data, including semi-supervised, meta-learning, and generative AI
methods.

</details>


### [374] [Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics](https://arxiv.org/abs/2509.16858)
*Soon Jynn Chu,Raju Gottumukkala,Alan Barhorst*

Main category: cs.RO

TL;DR: 本文提出基于线下强化学习的数据驱动方法，使社交机器人能够适应人类情感，在数据受限场景下实现安全且高效的人机协作。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在真实环境下需要对人类情感做出适应性反应，以获得信任和接受，但在线强化学习存在数据收集昂贵和潜在不安全行为的风险，亟需更高效的替代方法。

Method: 作者设计了一套集成多模态情感感知、决策和自适应反馈的系统结构，利用已有的人机交互数据，比较了多种线下强化学习算法（BCQ、CQL、NFQ、DQN、DDQN）的表现。

Result: 实验结果表明，在数据稀疏情况下，BCQ和CQL算法比NFQ、DQN和DDQN更加稳健，获得更高的状态-动作价值。该研究为情感自适应机器人线下强化学习算法的基准测试提供了基础。

Conclusion: 线下强化学习方法能在数据受限且安全性要求高的实际人机协作中提升机器人对情感适应能力，为未来在对话、教育、个人助理等真实应用场景中的部署提供了经验和理论依据。

Abstract: The ability of social robots to respond to human emotions is crucial for
building trust and acceptance in human-robot collaborative environments.
However, developing such capabilities through online reinforcement learning is
sometimes impractical due to the prohibitive cost of data collection and the
risk of generating unsafe behaviors. In this paper, we study the use of offline
reinforcement learning as a practical and efficient alternative. This technique
uses pre-collected data to enable emotion-adaptive social robots. We present a
system architecture that integrates multimodal sensing and recognition,
decision-making, and adaptive responses. Using a limited dataset from a
human-robot game-playing scenario, we establish a benchmark for comparing
offline reinforcement learning algorithms that do not require an online
environment. Our results show that BCQ and CQL are more robust to data
sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.
This work establishes a foundation for benchmarking offline RL in
emotion-adaptive robotics and informs future deployment in real-world HRI. Our
findings provide empirical insight into the performance of offline
reinforcement learning algorithms in data-constrained HRI. This work
establishes a foundation for benchmarking offline RL in emotion-adaptive
robotics and informs its future deployment in real-world HRI, such as in
conversational agents, educational partners, and personal assistants, require
reliable emotional responsiveness.

</details>


### [375] [HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness](https://arxiv.org/abs/2509.16871)
*Yitian Shi,Zicheng Guo,Rosa Wolf,Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: 本文提出了一种基于affordance的手-物体交互抓取生成方法HOGraspFlow，实现了无需物体几何先验，在单张RGB图像下生成可执行的多模态平行夹爪抓取姿态。实验效果领先，抓取成功率高。


<details>
  <summary>Details</summary>
Motivation: 现有研究在实现手-物体交互抓取合成时，通常依赖于目标物体的几何信息或接触点先验，限制了方法的通用性和实际应用场景。因此，亟需一种无需物体几何先验即可高效合成可执行抓取的方案。

Method: 该方法基于手重建和视觉基础模型，运用去噪流匹配(FM)网络生成SE(3)抓取姿态。模型结合三类条件信息：RGB图像语义、HOI接触重建、抓取类型先验，无需显式的物体几何或接触输入。

Result: 实验表明，HOGraspFlow在无显式HOI接触或物体几何输入的条件下，也能高保真地生成抓取合成，并且在SE(3)分布和优化稳定性上优于基于扩散模型的变体HOGraspDiff。真实场景实验中抓取成功率超过83%。

Conclusion: HOGraspFlow证明了无需物体几何先验即可由单张RGB和手部动作生成高质量、多模态抓取，具有出色的泛化能力和实际应用价值。

Abstract: We propose Hand-Object\emph{(HO)GraspFlow}, an affordance-centric approach
that retargets a single RGB with hand-object interaction (HOI) into multi-modal
executable parallel jaw grasps without explicit geometric priors on target
objects. Building on foundation models for hand reconstruction and vision, we
synthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned
on the following three complementary cues: RGB foundation features as visual
semantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.
Our approach demonstrates high fidelity in grasp synthesis without explicit HOI
contact input or object geometry, while maintaining strong contact and taxonomy
recognition. Another controlled comparison shows that \emph{HOGraspFlow}
consistently outperforms diffusion-based variants (\emph{HOGraspDiff}),
achieving high distributional fidelity and more stable optimization in $SE(3)$.
We demonstrate a reliable, object-agnostic grasp synthesis from human
demonstrations in real-world experiments, where an average success rate of over
$83\%$ is achieved.

</details>


### [376] [End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing](https://arxiv.org/abs/2509.16894)
*Zhijie Qiao,Haowei Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: End2Race是一种新提出的端到端模仿学习算法，针对F1Tenth自动驾驶赛车平台，在高动态竞速环境下展现出优异的效率与性能，显著提升超车成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶算法难以满足高速度、动态环境和赛车对抗情景的需求，尤其是在F1Tenth平台这样对决跑道中，如何快速且安全决策，仍面临模型容量与实时性的严峻挑战。作者希望提出有效方案，兼顾极速决策与可靠策略规划。

Method: 提出End2Race算法，采用GRU结构提取时序信息，实现短期响应性与长期策略规划融合。同时引入sigmoid归一化，将LiDAR原始数据转化为空间压力token，便于高效训练收敛。该模型端到端设计，并优化至消费级GPU上推理时间小于0.5ms。

Result: 在F1Tenth仿真环境中，End2Race在2400组超车测试场景中达到了94.2%安全率，59.2%的超车成功率，均显著优于已有方法。

Conclusion: End2Race成为F1Tenth赛车平台端到端超车与决策的领先解决方案，在保障安全的同时显著提升了超车能力，对实际自动赛车算法发展具有重要意义。

Abstract: F1Tenth is a widely adopted reduced-scale platform for developing and testing
autonomous racing algorithms, hosting annual competitions worldwide. With high
operating speeds, dynamic environments, and head-to-head interactions,
autonomous racing requires algorithms that diverge from those in classical
autonomous driving. Training such algorithms is particularly challenging: the
need for rapid decision-making at high speeds severely limits model capacity.
To address this, we propose End2Race, a novel end-to-end imitation learning
algorithm designed for head-to-head autonomous racing. End2Race leverages a
Gated Recurrent Unit (GRU) architecture to capture continuous temporal
dependencies, enabling both short-term responsiveness and long-term strategic
planning. We also adopt a sigmoid-based normalization function that transforms
raw LiDAR scans into spatial pressure tokens, facilitating effective model
training and convergence. The algorithm is extremely efficient, achieving an
inference time of less than 0.5 milliseconds on a consumer-class GPU.
Experiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%
safety rate across 2,400 overtaking scenarios, each with an 8-second time
limit, and successfully completes overtakes in 59.2% of cases. This surpasses
previous methods and establishes ours as a leading solution for the F1Tenth
racing testbed. Code is available at
https://github.com/michigan-traffic-lab/End2Race.

</details>


### [377] [SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms](https://arxiv.org/abs/2509.16920)
*Ettilla Mohiuddin Eumi,Hussein Abbass,Nadine Marcus*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的多模态人-群体机器人交互系统——SwarmChat，实现了通过文本、语音或遥操作等多种方式以自然语言控制机器人群体。该系统包含上下文生成、意图识别、任务规划、模态选择四个模块，并通过三层架构优化用户体验和认知负担。初步评估表明该系统具备高准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 传统的人-群体机器人交互方式缺乏直观、实时、自适应的界面，导致决策效率低下、用户认知负担大且控制灵活性差。文章旨在解决这些痛点。

Method: 作者设计并实现了SwarmChat系统，使用四个基于LLM的子模块（上下文生成、意图识别、任务规划、模态选择）处理自然语言输入，并结合三层架构支持多模态与灵活命令。

Result: 初步实验结果显示，SwarmChat在上下文理解、意图识别和命令执行准确性方面表现良好，且提升了用户满意度。

Conclusion: 论文验证了基于LLM的多模态交互方式能有效提升机器人群体控制体验，优化用户认知负担，并具备良好的实际应用前景。

Abstract: Traditional Human-Swarm Interaction (HSI) methods often lack intuitive
real-time adaptive interfaces, making decision making slower and increasing
cognitive load while limiting command flexibility. To solve this, we present
SwarmChat, a context-aware, multimodal interaction system powered by Large
Language Models (LLMs). SwarmChat enables users to issue natural language
commands to robotic swarms using multiple modalities, such as text, voice, or
teleoperation. The system integrates four LLM-based modules: Context Generator,
Intent Recognition, Task Planner, and Modality Selector. These modules
collaboratively generate context from keywords, detect user intent, adapt
commands based on real-time robot state, and suggest optimal communication
modalities. Its three-layer architecture offers a dynamic interface with both
fixed and customizable command options, supporting flexible control while
optimizing cognitive effort. The preliminary evaluation also shows that the
SwarmChat's LLM modules provide accurate context interpretation, relevant
intent recognition, and effective command delivery, achieving high user
satisfaction.

</details>


### [378] [A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination](https://arxiv.org/abs/2509.16963)
*Chengjin Wang,Yanmin Zhou,Zhipeng Wang,Zheng Yan,Feng Luan,Shuo Jiang,Runjie Shen,Hongrui Sang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种受“动作想象”启发的机器人运动规划器（I-MP），通过在复杂环境中先对动作结果进行想象，提高机器人运动的可靠性和环境适应性。


<details>
  <summary>Details</summary>
Motivation: 受到人类和动物能通过想象预测动作结果来避免未知环境中意外失败的启发，解决传统机器人在复杂、不确定环境下运动规划的可靠性与适应性差的问题。

Method: 1. 工作空间拓扑化。2. 构建立体的感知-动作闭环，让机器人通过与环境交互自主构建接触模型。3. 应用不动点理论和Hausdorff距离，在实际互动约束下计算收敛的空间状态。4. 统一将多维环境特性通过物理“功”表示，并用能量梯度实时优化运动。

Result: 实验证明，I-MP能让机器人在复杂、拥挤的环境中具有更强的实用性和鲁棒性，提升了对未知干扰的适应能力。

Conclusion: 动作想象机制能显著提升机器人运动规划的可靠性和适应性，为未来机器人在复杂未知环境下自适应运动提供了有效框架。

Abstract: Humans and animals can make real-time adjustments to movements by imagining
their action outcomes to prevent unanticipated or even catastrophic motion
failures in unknown unstructured environments. Action imagination, as a refined
sensorimotor strategy, leverages perception-action loops to handle physical
interaction-induced uncertainties in perception and system modeling within
complex systems. Inspired by the action-awareness capability of animal
intelligence, this study proposes an imagination-inspired motion planner (I-MP)
framework that specifically enhances robots' action reliability by imagining
plausible spatial states for approaching. After topologizing the workspace,
I-MP build perception-action loop enabling robots autonomously build contact
models. Leveraging fixed-point theory and Hausdorff distance, the planner
computes convergent spatial states under interaction characteristics and
mission constraints. By homogenously representing multi-dimensional
environmental characteristics through work, the robot can approach the imagined
spatial states via real-time computation of energy gradients. Consequently,
experimental results demonstrate the practicality and robustness of I-MP in
complex cluttered environments.

</details>


### [379] [Geometric Interpolation of Rigid Body Motions](https://arxiv.org/abs/2509.16966)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文研究了刚体运动的轨迹插值问题，提出了两种插值变体：k阶初值型和k阶边值型，并分别给出了k=1到4的初值型解法，以及一种新的给定初终速度的三次插值方法，展示了数值实验效果。


<details>
  <summary>Details</summary>
Motivation: 刚体在空间中的轨迹插值对机器人、动画等场景至关重要，但在已知多阶导数信息的前提下进行插值的方法尚不完善，因此作者旨在系统地提出更高阶刚体轨迹插值的方法，满足实际应用的需求。

Method: 论文定义了k阶初值轨迹插值问题（k-IV-TIP）和k阶边值轨迹插值问题（k-BV-TIP），构造出在指定导数条件下的插值曲线。对于k=1到4的初值型插值问题给出了解析解，并提出了给定初末速度的三次插值新方法。作者还展示了一般性的高阶求解策略，并通过数值实验进行了验证。

Result: 论文得到了k=1~4阶的刚体初值轨迹插值解析解，并提出了一个新三次插值方法，满足刚体的初末速度条件；数值实验验证了所提方法的有效性和优良性质。

Conclusion: 本文系统化解决了高阶刚体运动插值难题，提出方法能满足更严格的导数边界条件，在机器人运动规划等领域具有重要实际意义。

Abstract: The problem of interpolating a rigid body motion is to find a spatial
trajectory between a prescribed initial and terminal pose. Two variants of this
interpolation problem are addressed. The first is to find a solution that
satisfies initial conditions on the k-1 derivatives of the rigid body twist.
This is called the kth-order initial value trajectory interpolation problem
(k-IV-TIP). The second is to find a solution that satisfies conditions on the
rigid body twist and its k-1 derivatives at the initial and terminal pose. This
is called the kth-order boundary value trajectory interpolation problem
(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and
up to the 4th time derivative are prescribed. Further, a solution to the
1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The
latter is a novel cubic interpolation between two spatial configurations with
given initial and terminal twist. This interpolation is automatically identical
to the minimum acceleration curve when the twists are set to zero. The general
approach to derive higher-order solutions is presented. Numerical results are
shown for two examples.

</details>


### [380] [IDfRA: Self-Verification for Iterative Design in Robotic Assembly](https://arxiv.org/abs/2509.16998)
*Nishka Khendry,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 本文提出了一种新的面向机器人装配的迭代式设计框架（IDfRA），通过实际环境中的自我评估和迭代优化，逐步提升产品设计与装配计划，实现自动化和高质量的机器人装配。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在制造业中的普及，如何让产品设计适合机器人的高效自动化装配（DfRA）愈发重要。现有方法多依赖手工规划和死板的物理模拟，难以应对复杂和真实的装配场景，因此亟需更智能和灵活的方法。

Method: 提出IDfRA框架，采用规划-执行-验证-再规划的自我迭代流程。输入为目标结构和部分环境信息，通过多轮自适应修正与优化，在真实物理环境中自我评估设计可行性，无需传统物理仿真。

Result: 实验证明，在语义识别准确率（73.3%）和物理装配成功率（86.9%）等指标上，IDfRA显著优于现有基线方法，且设计质量多次迭代后持续提升。人类评测也支持该方法在多方面表现更佳。

Conclusion: IDfRA通过结合自我验证和上下文自适应，展现出在非结构化制造场景中实际应用的巨大潜力，有望推动机器人自动化装配走向更高智能与实用化。

Abstract: As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),
which is designing products for efficient automated assembly, is increasingly
important. Traditional approaches to DfRA rely on manual planning, which is
time-consuming, expensive and potentially impractical for complex objects.
Large language models (LLM) have exhibited proficiency in semantic
interpretation and robotic task planning, stimulating interest in their
application to the automation of DfRA. But existing methodologies typically
rely on heuristic strategies and rigid, hard-coded physics simulators that may
not translate into real-world assembly contexts. In this work, we present
Iterative Design for Robotic Assembly (IDfRA), a framework using iterative
cycles of planning, execution, verification, and re-planning, each informed by
self-assessment, to progressively enhance design quality within a fixed yet
initially under-specified environment, thereby eliminating the physics
simulation with the real world itself. The framework accepts as input a target
structure together with a partial environmental representation. Through
successive refinement, it converges toward solutions that reconcile semantic
fidelity with physical feasibility. Empirical evaluation demonstrates that
IDfRA attains 73.3\% top-1 accuracy in semantic recognisability, surpassing the
baseline on this metric. Moreover, the resulting assembly plans exhibit robust
physical feasibility, achieving an overall 86.9\% construction success rate,
with design quality improving across iterations, albeit not always
monotonically. Pairwise human evaluation further corroborates the advantages of
IDfRA relative to alternative approaches. By integrating self-verification with
context-aware adaptation, the framework evidences strong potential for
deployment in unstructured manufacturing scenarios.

</details>


### [381] [Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems](https://arxiv.org/abs/2509.17010)
*Rajpal Singh,Aditya Singh,Chidre Shravista Kashyap,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 本文提出了一种基于Koopman算子的全新欧拉-拉格朗日动力学建模方法，利用广义动量隐式状态空间分离，极大降低了模型复杂度、提高了数据效率，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 欧拉-拉格朗日动力学传统的状态空间建模方法中，输入与状态的关系高度耦合，导致基于Koopman算子的线性建模难度大、参数量大、数据需求高。因此，作者希望找到一种能够简化建模过程、减少参数与提高效率的方法。

Method: 作者提出了一种依靠广义动量的隐式状态空间表示，能将已知的线性驱动通道从状态相关动力学中分离出来，仅需学习无驱动部分的动力学，简化了Koopman建模。提出两种神经网络结构以适用于有驱动或无驱动数据。同时，集成线性广义扩展状态观测器（GESO）增强对外部干扰的鲁棒性。

Result: 通过对机器人操作臂的轨迹跟踪仿真和实验，结果显示该方法相比传统的Koopman方法具有更高的预测精度、更强鲁棒性和更高的学习效率。与主流方法比较，展现出更优的性能与模型简洁性。

Conclusion: 广义动量基础的Koopman建模结合GESO能有效促进动力学系统的精确、高效和鲁棒建模，在实际机器人系统中展示了优越性，具有良好的工程应用前景。

Abstract: This paper presents a novel Koopman operator formulation for Euler Lagrangian
dynamics that employs an implicit generalized momentum-based state space
representation, which decouples a known linear actuation channel from state
dependent dynamics and makes the system more amenable to linear Koopman
modeling. By leveraging this structural separation, the proposed formulation
only requires to learn the unactuated dynamics rather than the complete
actuation dependent system, thereby significantly reducing the number of
learnable parameters, improving data efficiency, and lowering overall model
complexity. In contrast, conventional explicit formulations inherently couple
inputs with the state dependent terms in a nonlinear manner, making them more
suitable for bilinear Koopman models, which are more computationally expensive
to train and deploy. Notably, the proposed scheme enables the formulation of
linear models that achieve superior prediction performance compared to
conventional bilinear models while remaining substantially more efficient. To
realize this framework, we present two neural network architectures that
construct Koopman embeddings from actuated or unactuated data, enabling
flexible and efficient modeling across different tasks. Robustness is ensured
through the integration of a linear Generalized Extended State Observer (GESO),
which explicitly estimates disturbances and compensates for them in real time.
The combined momentum-based Koopman and GESO framework is validated through
comprehensive trajectory tracking simulations and experiments on robotic
manipulators, demonstrating superior accuracy, robustness, and learning
efficiency relative to state of the art alternatives.

</details>


### [382] [Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning](https://arxiv.org/abs/2509.17042)
*Zengqi Peng,Yusen Xie,Yubin Wang,Rui Yang,Qifeng Chen,Jun Ma*

Main category: cs.RO

TL;DR: 本论文提出了OGR（Orchestrate, Generate, Reflect）框架，通过利用视觉-语言模型（VLM）的多智能体协作，自动化地提升自动驾驶中的策略学习，减少手动设计奖励函数和训练流程的工作量。实验在CARLA和真实环境中展现了其卓越性能和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶中的奖励函数和训练流程设计高度依赖人工，过程繁琐且耗时，限制了强化学习策略在复杂动态驾驶任务中的规模与效率。如何自动化该流程亟需解决。

Method: 作者构建了一个多模态、分层的智能体系统：中央协调器负责高层训练目标规划，生成模块通过“两步分析-生成”流程高效产生奖励与训练任务对，反思模块以在线评估结果为基础优化策略，记忆模块为VLM代理赋能长期记忆。还增设并行生成和人类参与机制以提高生成多样性和健壮性。

Result: 在CARLA仿真器和真实世界测试中，OGR框架表现出优越的强化学习政策性能、对多种城市场景的强泛化能力，并与多种RL算法高度兼容。

Conclusion: OGR框架自动化了复杂自动驾驶场景中的奖励函数和训练流程设计，提升了RL策略学习的效率与泛化，实现了高效、互动感知的自动驾驶控制，具有实际推广应用价值。

Abstract: The advancement of foundation models fosters new initiatives for policy
learning in achieving safe and efficient autonomous driving. However, a
critical bottleneck lies in the manual engineering of reward functions and
training curricula for complex and dynamic driving tasks, which is a
labor-intensive and time-consuming process. To address this problem, we propose
OGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning
framework that leverages vision-language model (VLM)-based multi-agent
collaboration. Our framework capitalizes on advanced reasoning and multimodal
understanding capabilities of VLMs to construct a hierarchical agent system.
Specifically, a centralized orchestrator plans high-level training objectives,
while a generation module employs a two-step analyze-then-generate process for
efficient generation of reward-curriculum pairs. A reflection module then
facilitates iterative optimization based on the online evaluation. Furthermore,
a dedicated memory module endows the VLM agents with the capabilities of
long-term memory. To enhance robustness and diversity of the generation
process, we introduce a parallel generation scheme and a human-in-the-loop
technique for augmentation of the reward observation space. Through efficient
multi-agent cooperation and leveraging rich multimodal information, OGR enables
the online evolution of reinforcement learning policies to acquire
interaction-aware driving skills. Extensive experiments in the CARLA simulator
demonstrate the superior performance, robust generalizability across distinct
urban scenarios, and strong compatibility with various RL algorithms. Further
real-world experiments highlight the practical viability and effectiveness of
our framework. The source code will be available upon acceptance of the paper.

</details>


### [383] [FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.17053)
*Haizhou Ge,Yufei Jia,Zheng Li,Yue Li,Zhixing Chen,Ruqi Huang,Guyue Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的机器人操作学习框架FILIC，通过力引导和阻抗力矩控制，实现了安全、柔顺和灵活的接触丰富操作任务。特别适合缺少安装力/力矩传感器的协作机器人。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习方法主要关注位置控制，缺乏力感知，且给机器人安装力/力矩传感器成本高、设计复杂。为解决这些局限，有必要开发无需额外硬件且具有明确力感知能力的控制策略。

Method: 1) 提出FILIC框架，将基于Transformer的模仿学习策略与阻抗控制器结合，形成双环结构，实现力知觉与执行；2) 对无力传感器的机器人，通过关节力矩和Jacobian反算估算末端力，同时用数字孪生补偿模型预测误差；3) 利用手持触觉设备与VR视觉设计示范辅助，提升演示质量。

Result: 实验表明，FILIC在接触丰富任务中，比仅视觉或简单力矩估算的方法表现更优，能实现更安全、柔顺及适应性强的操作。

Conclusion: FILIC展现了在无需昂贵力传感器时实现高质量力控制机器人操作的可行性和优势，为机器人在复杂接触场景下广泛应用提供了新思路。

Abstract: Contact-rich manipulation is crucial for robots to perform tasks requiring
precise force control, such as insertion, assembly, and in-hand manipulation.
However, most imitation learning (IL) policies remain position-centric and lack
explicit force awareness, and adding force/torque sensors to collaborative
robot arms is often costly and requires additional hardware design. To overcome
these issues, we propose FILIC, a Force-guided Imitation Learning framework
with impedance torque control. FILIC integrates a Transformer-based IL policy
with an impedance controller in a dual-loop structure, enabling compliant
force-informed, force-executed manipulation. For robots without force/torque
sensors, we introduce a cost-effective end-effector force estimator using joint
torque measurements through analytical Jacobian-based inversion while
compensating with model-predicted torques from a digital twin. We also design
complementary force feedback frameworks via handheld haptics and VR
visualization to improve demonstration quality. Experiments show that FILIC
significantly outperforms vision-only and joint-torque-based methods, achieving
safer, more compliant, and adaptable contact-rich manipulation. Our code can be
found in https://github.com/TATP-233/FILIC.

</details>


### [384] [RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments](https://arxiv.org/abs/2509.17057)
*Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Hanbit Oh,Koshi Makihara,Keisuke Shirai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: RoboManipBaselines是一个用于机器人模仿学习的开源框架，兼顾了数据采集、训练和评估，并支持仿真与真实机器人，强调可集成性、一般性、可扩展性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 目前机器人模仿学习领域缺乏一个统一的平台，能对不同任务、机器人和策略进行系统性基准测试。为推动领域发展，需要一个集成且易于扩展的工具。

Method: 该论文提出了RoboManipBaselines框架，设计了统一的接口来实现数据采集、训练与评估，适配了仿真和现实中的多种机器人与任务，支持各种多模态策略的集成和扩展。

Result: RoboManipBaselines平台能够支持多样任务、机器人、模态策略的无缝集成，且具备良好的通用性和可扩展性，便于进行系统性实验和基准测试。

Conclusion: RoboManipBaselines为机器人模仿学习研究提供了高效、通用且可复现的基准平台，有助于推动领域的标准化和技术进步。

Abstract: RoboManipBaselines is an open framework for robot imitation learning that
unifies data collection, training, and evaluation across simulation and real
robots. We introduce it as a platform enabling systematic benchmarking of
diverse tasks, robots, and multimodal policies with emphasis on integration,
generality, extensibility, and reproducibility.

</details>


### [385] [CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving](https://arxiv.org/abs/2509.17080)
*Ruiguo Zhong,Ruoyu Yao,Pei Liu,Xiaolong Chen,Rui Yang,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种名为CoPlanner的联合多智能体轨迹生成和应急感知运动规划的统一框架，以提升自动驾驶在复杂场景下的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有生成-评估框架在轨迹预测与规划上只采用最可能的一条路径，导致决策过度自信且在罕见关键场景下缺乏备用方案，且预测与规划解耦会产生不符合社会约束的联合轨迹。

Method: 提出了pivot-conditioned diffusion机制，实现以验证的短期段为锚点进行轨迹采样，同时生成多样化的长时分支。此外，设计了应急感知多场景评分策略，对多种长远演化情景下的自车轨迹进行评估，综合考虑安全、进展和舒适性。

Result: 在nuPlan基准的封闭环实验中，CoPlanner在Val14和Test14两个数据集上均超越现有方法，在反应性和非反应性条件下显著提升了安全与舒适表现。

Conclusion: 整合式设计不仅保留了可行的备选轨迹，提高了系统在不确定性下的鲁棒性，也实现了更真实、社交感知的自动驾驶决策。

Abstract: Accurate trajectory prediction and motion planning are crucial for autonomous
driving systems to navigate safely in complex, interactive environments
characterized by multimodal uncertainties. However, current
generation-then-evaluation frameworks typically construct multiple plausible
trajectory hypotheses but ultimately adopt a single most likely outcome,
leading to overconfident decisions and a lack of fallback strategies that are
vital for safety in rare but critical scenarios. Moreover, the usual decoupling
of prediction and planning modules could result in socially inconsistent or
unrealistic joint trajectories, especially in highly interactive traffic. To
address these challenges, we propose a contingency-aware diffusion planner
(CoPlanner), a unified framework that jointly models multi-agent interactive
trajectory generation and contingency-aware motion planning. Specifically, the
pivot-conditioned diffusion mechanism anchors trajectory sampling on a
validated, shared short-term segment to preserve temporal consistency, while
stochastically generating diverse long-horizon branches that capture multimodal
motion evolutions. In parallel, we design a contingency-aware multi-scenario
scoring strategy that evaluates candidate ego trajectories across multiple
plausible long-horizon evolution scenarios, balancing safety, progress, and
comfort. This integrated design preserves feasible fallback options and
enhances robustness under uncertainty, leading to more realistic
interaction-aware planning. Extensive closed-loop experiments on the nuPlan
benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art
methods on both Val14 and Test14 datasets, achieving significant improvements
in safety and comfort under both reactive and non-reactive settings. Code and
model will be made publicly available upon acceptance.

</details>


### [386] [Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation](https://arxiv.org/abs/2509.17125)
*Liang Heng,Jiadong Xu,Yiwen Wang,Xiaoqi Li,Muhe Cai,Yan Shen,Juan Zhu,Guanghui Ren,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了Imagine2Act框架，将语义和几何约束结合到机器人高精度操纵任务的策略学习中，通过假想目标点云和一致性监督提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖于演示数据但难以处理复杂几何约束，要么只生成目标状态观测，未能准确关联物体变换与动作预测，导致动作误差。

Method: Imagine2Act首先根据语言指令生成目标图像，重建三维点云，作为策略模型的额外输入，并采用带软姿态监督的物体-动作一致性策略，将末端执行器的运动与物体变换进行对齐。

Result: 在仿真和现实测试中，Imagine2Act在多种任务上均优于先前的SOTA方法。

Conclusion: Imagine2Act成功结合了语义和几何知识，加强了物体间关系理解，提高了高精度操纵任务的动作预测准确性，有较强泛化能力。

Abstract: Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)
require a robot to manipulate objects with precise semantic and geometric
reasoning. Existing approaches either rely on pre-collected demonstrations that
struggle to capture complex geometric constraints or generate goal-state
observations to capture semantic and geometric knowledge, but fail to
explicitly couple object transformation with action prediction, resulting in
errors due to generative noise. To address these limitations, we propose
Imagine2Act, a 3D imitation-learning framework that incorporates semantic and
geometric constraints of objects into policy learning to tackle high-precision
manipulation tasks. We first generate imagined goal images conditioned on
language instructions and reconstruct corresponding 3D point clouds to provide
robust semantic and geometric priors. These imagined goal point clouds serve as
additional inputs to the policy model, while an object-action consistency
strategy with soft pose supervision explicitly aligns predicted end-effector
motion with generated object transformation. This design enables Imagine2Act to
reason about semantic and geometric relationships between objects and predict
accurate actions across diverse tasks. Experiments in both simulation and the
real world demonstrate that Imagine2Act outperforms previous state-of-the-art
policies. More visualizations can be found at
https://sites.google.com/view/imagine2act.

</details>


### [387] [History-Aware Visuomotor Policy Learning via Point Tracking](https://arxiv.org/abs/2509.17141)
*Jingjing Chen,Hongjie Fang,Chenxi Wang,Shiquan Wang,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种基于点跟踪的面向对象的历史表示方法，用于提升机器人操作任务中的记忆能力，在多种需要长期记忆的场景下表现优异，超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多机器人操作任务需要超过当前观测的记忆，而现有视觉运动策略大多基于马尔可夫假设，难以处理重复状态或长时依赖，因而有必要研发更加有效的记忆机制。

Method: 作者提出通过点跟踪将历史观测抽象为结构化且紧凑的对象级历史表示，仅保留任务相关的关键信息。具体做法是：跟踪关键点，将其在目标对象层面进行编码和聚合，并将该历史表示无缝集成至不同的视觉运动控制策略中。

Result: 实验涵盖多种操作任务，结果显示该方法能有效地应对任务阶段识别、空间记忆、动作计数及更长期需求（如连续/预加载记忆）等多种类型的记忆需求，并持续优于马尔可夫基线和现有的基于历史的模型。

Conclusion: 利用面向对象的点跟踪历史表示能够高效地增强操作策略的历史感知能力，在复杂任务中显著提升决策准确性和总体表现。

Abstract: Many manipulation tasks require memory beyond the current observation, yet
most visuomotor policies rely on the Markov assumption and thus struggle with
repeated states or long-horizon dependencies. Existing methods attempt to
extend observation horizons but remain insufficient for diverse memory
requirements. To this end, we propose an object-centric history representation
based on point tracking, which abstracts past observations into a compact and
structured form that retains only essential task-relevant information. Tracked
points are encoded and aggregated at the object level, yielding a compact
history representation that can be seamlessly integrated into various
visuomotor policies. Our design provides full history-awareness with high
computational efficiency, leading to improved overall task performance and
decision accuracy. Through extensive evaluations on diverse manipulation tasks,
we show that our method addresses multiple facets of memory requirements - such
as task stage identification, spatial memorization, and action counting, as
well as longer-term demands like continuous and pre-loaded memory - and
consistently outperforms both Markovian baselines and prior history-based
approaches. Project website: http://tonyfang.net/history

</details>


### [388] [MAST: Multi-Agent Spatial Transformer for Learning to Collaborate](https://arxiv.org/abs/2509.17195)
*Damian Owerko,Frederic Vatnsdal,Saurav Agarwal,Vijay Kumar,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了多智能体空间变换器（MAST）架构，用于在大规模分布式协作多机器人系统中学习去中心化通信策略。MAST通过新型位置编码和注意力机制，在局部通讯约束下实现高效协作，并在多项任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，由于每个机器人只能获得部分信息、通信范围有限且无中心服务器，如何设计高效的去中心化协作策略成为难题。本文旨在解决这些由于局部感知和有限通信导致的协作障碍。

Method: 提出了一个去中心化的变换器结构MAST，通过新颖的位置编码与窗口化注意力机制，仅在局部感知内传递与处理抽象信息，从而实现高效的智能体间通信与协作。政策通过模仿学习在中心化设置下训练，然后在去中心化环境中部署。

Result: MAST在分布式调度导航与覆盖控制任务中表现出色，能够有效扩展到大规模团队，对通信延迟具备鲁棒性，性能优于现有基线和其他学习方法。

Conclusion: MAST为多机器人分布式协作提供了新范式，能够在仅局部感知和有限通信条件下实现高水平去中心化协作，为多智能体系统部署提供理论与实践上的支持。

Abstract: This article presents a novel multi-agent spatial transformer (MAST) for
learning communication policies in large-scale decentralized and collaborative
multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:
(i) partial observable states as robots make only localized perception, (ii)
limited communication range with no central server, and (iii) independent
execution of actions. The robots need to optimize a common task-specific
objective, which, under the restricted setting, must be done using a
communication policy that exhibits the desired collaborative behavior. The
proposed MAST is a decentralized transformer architecture that learns
communication policies to compute abstract information to be shared with other
agents and processes the received information with the robot's own
observations. The MAST extends the standard transformer with new positional
encoding strategies and attention operations that employ windowing to limit the
receptive field for MRS. These are designed for local computation,
shift-equivariance, and permutation equivariance, making it a promising
approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized
assignment and navigation (DAN) and decentralized coverage control. Efficiently
trained using imitation learning in a centralized setting, the decentralized
MAST policy is robust to communication delays, scales to large teams, and
performs better than the baselines and other learning-based approaches.

</details>


### [389] [Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites](https://arxiv.org/abs/2509.17198)
*Baoshan Song,Weisong Wen,Qi Zhang,Bing Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 该论文提出了一种基于LEO卫星多普勒效应信号的定位方法，可作为GNSS系统的备份。本文首次通过凸优化方法实现了高精度、无需初始值的全局最优定位，并通过仿真和实测有效验证了该方案的大幅优越性。


<details>
  <summary>Details</summary>
Motivation: 在GNSS不可用或不可靠的环境下，需要寻找新的定位信号来源。LEO卫星由于轨道低、遍布全球，其多普勒频移信号具有作为定位辅助的潜力。然而，多普勒定位是非凸问题，传统优化方法易陷入局部最优，且依赖较精确初值，实用中很难保证。因此，亟需一种无需初始、具有全局最优保障的定位方法。

Method: 作者将问题转化为凸优化，通过毕业权重逼近（GWA）和半正定松弛（SDP）设计了可证明全局最优的算法。同时在理论上推导了无噪声最优性条件和有噪声情况下的充分条件。

Result: 通过仿真和Iridium-NEXT卫星的实测，所提方法在无需初值情况下3D定位误差为140m，显著优于传统高斯-牛顿和Dog-Leg等方法（初值误差大时易陷局部最优）。同时，该方法也可以为局部搜索法提供高精度初值，误差降至130m。

Conclusion: 所研发的凸优化LEO多普勒定位方法无需初值，能保证全局最优，极大提升了LEO信号在PNT领域的应用价值，在GNSS失效场景下展示了强大优势。

Abstract: To provide backup and augmentation to global navigation satellite system
(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as
signals of opportunity (SOP) for position, navigation and timing (PNT). Since
the Doppler positioning problem is non-convex, local searching methods may
produce two types of estimates: a global optimum without notice or a local
optimum given an inexact initial estimate. As exact initialization is
unavailable in some unknown environments, a guaranteed global optimization
method in no need of initialization becomes necessary. To achieve this goal, we
propose a certifiably optimal LEO Doppler positioning method by utilizing
convex optimization. In this paper, the certifiable positioning method is
implemented through a graduated weight approximation (GWA) algorithm and
semidefinite programming (SDP) relaxation. To guarantee the optimality, we
derive the necessary conditions for optimality in ideal noiseless cases and
sufficient noise bounds conditions in noisy cases. Simulation and real tests
are conducted to evaluate the effectiveness and robustness of the proposed
method. Specially, the real test using Iridium-NEXT satellites shows that the
proposed method estimates an certifiably optimal solution with an 3D
positioning error of 140 m without initial estimates while Gauss-Newton and
Dog-Leg are trapped in local optima when the initial point is equal or larger
than 1000 km away from the ground truth. Moreover, the certifiable estimation
can also be used as initialization in local searching methods to lower down the
3D positioning error to 130 m.

</details>


### [390] [Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation](https://arxiv.org/abs/2509.17204)
*James R. Han,Mithun Vanniasinghe,Hshmat Sahak,Nicholas Rhinehart,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本文提出了名为Ratatouille的新模型和流水线，极大提升了社交机器人在真实环境下的离线模仿学习导航表现，无需额外数据即可显著减少碰撞和提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 在真实社交环境中利用强化学习训练导航策略既危险又耗时（如碰撞风险），而离线模仿学习虽然安全但用传统方法表现有限。因此，作者试图通过优化IL架构和训练过程提升其实际表现。

Method: 作者提出Ratatouille方法，在不改变训练数据的情况下，通过改进网络架构和训练细节，对比朴素的行为克隆（BC），大幅提升离线IL的社交机器人导航能力。

Result: Ratatouille将每米碰撞次数降低了6倍，导航成功率提升了3倍。实验在仿真和现实世界（大学校园，11小时数据），并有食品广场场景的定性结果展示。

Conclusion: 结果显示，相比增加数据量，精心设计的IL方法和架构选择对提升现实社交导航安全性与可靠性更为关键。

Abstract: Scaling Reinforcement Learning to in-the-wild social robot navigation is both
data-intensive and unsafe, since policies must learn through direct interaction
and inevitably encounter collisions. Offline Imitation learning (IL) avoids
these risks by collecting expert demonstrations safely, training entirely
offline, and deploying policies zero-shot. However, we find that naively
applying Behaviour Cloning (BC) to social navigation is insufficient; achieving
strong performance requires careful architectural and training choices. We
present Ratatouille, a pipeline and model architecture that, without changing
the data, reduces collisions per meter by 6 times and improves success rate by
3 times compared to naive BC. We validate our approach in both simulation and
the real world, where we collected over 11 hours of data on a dense university
campus. We further demonstrate qualitative results in a public food court. Our
findings highlight that thoughtful IL design, rather than additional data, can
substantially improve safety and reliability in real-world social navigation.
Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.

</details>


### [391] [Combining Performance and Passivity in Linear Control of Series Elastic Actuators](https://arxiv.org/abs/2509.17210)
*Shaunak A. Mehta,Dylan P. Losey*

Main category: cs.RO

TL;DR: 该论文探讨串联弹性驱动器（SEA）在提升机器人与人类物理交互安全性的同时，如何权衡安全性与运动性能，并系统分析了不同控制方式和机械配置的影响。


<details>
  <summary>Details</summary>
Motivation: 在人机交互中，机器人既要保证安全又要有优良性能。SEA通过增加弹性元件提升安全性，但同时带来了执行精度下降和振荡问题。因此，需要权衡弹性带来的安全与性能损失。

Method: 论文系统枚举了SEA的不同线性控制算法及机械结构，并比较每种方案对合成顺应性、耗散性和跟踪性能的影响，重点对比了负载侧控制和执行器侧控制，并通过仿真和真实实验验证。

Result: 研究发现，相较于以往普遍关注的负载侧控制，执行器侧控制结合简单PD算法与弹性传动中的阻尼器，可以实现在低物理刚度和高控制增益下，既保证了碰撞安全性又可获得高运动性能。

Conclusion: 通过合理设计低刚度结构和高控制增益的系统，可以在不牺牲人机交互安全的前提下，实现机器人高精度和高性能的控制。

Abstract: When humans physically interact with robots, we need the robots to be both
safe and performant. Series elastic actuators (SEAs) fundamentally advance
safety by introducing compliant actuation. On the one hand, adding a spring
mitigates the impact of accidental collisions between human and robot; but on
the other hand, this spring introduces oscillations and fundamentally decreases
the robot's ability to perform precise, accurate motions. So how should we
trade off between physical safety and performance? In this paper, we enumerate
the different linear control and mechanical configurations for series elastic
actuators, and explore how each choice affects the rendered compliance,
passivity, and tracking performance. While prior works focus on load side
control, we find that actuator side control has significant benefits. Indeed,
simple PD controllers on the actuator side allow for a much wider range of
control gains that maintain safety, and combining these with a damper in the
elastic transmission yields high performance. Our simulations and real world
experiments suggest that, by designing a system with low physical stiffness and
high controller gains, this solution enables accurate performance while also
ensuring user safety during collisions.

</details>


### [392] [Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles](https://arxiv.org/abs/2509.17213)
*Yassine Kebbati,Naima Ait-Oufroukh,Vincent Vigneron,Dalil Ichala*

Main category: cs.RO

TL;DR: 本文提出了一种结合神经网络和自适应模糊推理系统（ANFIS）的自适应模型预测控制（MPC）方法，提升自动驾驶汽车在复杂场景下的路径跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在不断变化的环境中面临多种不确定性和干扰，传统控制器尤其在横向控制方面表现有限，因此需要更智能、自适应的控制方法。

Method: 设计了一种自适应MPC控制器，采用改进粒子群优化算法来调优。控制器通过神经网络和ANFIS实现参数在线自适应更新。实验在三变道和轨迹跟踪场景下进行。

Result: 与标准MPC相比，该自适应控制器在多个挑战性场景下表现更优，路径跟踪精度和鲁棒性均有明显提升。

Conclusion: 结合神经网络与ANFIS的自适应MPC方法为自动驾驶汽车的路径跟踪提供了更高效、鲁棒的解决方案，具有实际应用潜力。

Abstract: Self-driving cars operate in constantly changing environments and are exposed
to a variety of uncertainties and disturbances. These factors render classical
controllers ineffective, especially for lateral control. Therefore, an adaptive
MPC controller is designed in this paper for the path tracking task, tuned by
an improved particle swarm optimization algorithm. Online parameter adaptation
is performed using Neural Networks and ANFIS. The designed controller showed
promising results compared to standard MPC in triple lane change and trajectory
tracking scenarios. Code can be found here:
https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC

</details>


### [393] [Scalable Multi Agent Diffusion Policies for Coverage Control](https://arxiv.org/abs/2509.17244)
*Frederic Vatnsdal,Romina Garcia Camargo,Saurav Agarwal,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的新方法MADP，用于去中心化的机器人群协作，显著提升了多机器人覆盖控制任务中的性能，比现有方法效果更佳。


<details>
  <summary>Details</summary>
Motivation: 随着机器人群体规模的扩大及分布式应用需求的增加，如何高效建模多机器人间的协作决策变得极为重要。传统方法难以捕捉高维、复杂的动作分布及 agent 间的相互影响，因此作者希望通过更强大的生成模型解决这一难题。

Method: 作者提出了MADP，将扩散模型用于生成复杂高维的联合动作分布。每个机器人通过混合自身观察和同伴感知嵌入的信息进行决策采样，决策网络由空间变换器参数化，使各机器人能在去中心化场景下独立推理。模型以专家演示为训练目标采用模仿学习。

Result: 在覆盖控制这一多机器人导航标准任务中，MADP在不同数量、分布和变化密度下，都展现出更好的泛化能力和鲁棒性，实验表现持续优于当前主流方法。

Conclusion: 利用扩散模型，MADP成功捕获了机器人间复杂的协作关系，提升了协作效率和泛化能力，为去中心化多机器人系统提供了高效的新思路。

Abstract: We propose MADP, a novel diffusion-model-based approach for collaboration in
decentralized robot swarms. MADP leverages diffusion models to generate samples
from complex and high-dimensional action distributions that capture the
interdependencies between agents' actions. Each robot conditions policy
sampling on a fused representation of its own observations and perceptual
embeddings received from peers. To evaluate this approach, we task a team of
holonomic robots piloted by MADP to address coverage control-a canonical multi
agent navigation problem. The policy is trained via imitation learning from a
clairvoyant expert on the coverage control problem, with the diffusion process
parameterized by a spatial transformer architecture to enable decentralized
inference. We evaluate the system under varying numbers, locations, and
variances of importance density functions, capturing the robustness demands of
real-world coverage tasks. Experiments demonstrate that our model inherits
valuable properties from diffusion models, generalizing across agent densities
and environments, and consistently outperforming state-of-the-art baselines.

</details>


### [394] [Learning and Optimization with 3D Orientations](https://arxiv.org/abs/2509.17274)
*Alexandros Ntagkas,Constantinos Tsakonas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文系统梳理了各类3D方向的表示方法，并在机器人常见任务中全面对比评价，为相关学习或优化场景选择最佳表示法提供实证指导和参考。


<details>
  <summary>Details</summary>
Motivation: 3D方向的表示方法众多，各有优缺点，且相关文献分散，缺乏统一、清晰的梳理，对于具体任务该如何选择也无定论，尤其在需学习或优化3D方向相关函数时更复杂。

Method: 1) 统一整理现有的3D方向表示及相关技巧（含Lie群等数学工具）；2) 在四类机器人常用场景下（直接优化、神经网络监督学习、强化学习、用微分动态规划的轨迹优化）对各表示法进行实验性对比。

Result: 通过系统实验，揭示了不同表示法在各类机器人场景下的表现差异，优缺点及适用性，同时提供了详细的基准测试数据和公共代码实现。

Conclusion: 本文为3D方向表示的选择提供了实证推荐与决策指南，并填补了机器人领域相关知识系统性不足的空白。

Abstract: There exist numerous ways of representing 3D orientations. Each
representation has both limitations and unique features. Choosing the best
representation for one task is often a difficult chore, and there exist
conflicting opinions on which representation is better suited for a set of
family of tasks. Even worse, when dealing with scenarios where we need to learn
or optimize functions with orientations as inputs and/or outputs, the set of
possibilities (representations, loss functions, etc.) is even larger and it is
not easy to decide what is best for each scenario. In this paper, we attempt to
a) present clearly, concisely and with unified notation all available
representations, and "tricks" related to 3D orientations (including Lie Group
algebra), and b) benchmark them in representative scenarios. The first part
feels like it is missing from the robotics literature as one has to read many
different textbooks and papers in order have a concise and clear understanding
of all possibilities, while the benchmark is necessary in order to come up with
recommendations based on empirical evidence. More precisely, we experiment with
the following settings that attempt to cover most widely used scenarios in
robotics: 1) direct optimization, 2) imitation/supervised learning with a
neural network controller, 3) reinforcement learning, and 4) trajectory
optimization using differential dynamic programming. We finally provide
guidelines depending on the scenario, and make available a reference
implementation of all the orientation math described.

</details>


### [395] [Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation](https://arxiv.org/abs/2509.17287)
*Gokul B. Nair,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文提出了首个基于事件相机的视觉教-重复（teach-and-repeat）系统，实现了超过300Hz的处理速率，大幅提升了机器人在已示范路径上的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的摄像头局限于30-60Hz的固定帧率，导致感知与控制响应存在延迟，限制了机器人在快速变化环境中的反应速度。本文旨在解决这一瓶颈。

Method: 提出了基于频域互相关的事件流匹配框架，将匹配问题转化为高效的傅里叶空间操作，并充分利用事件帧的二值性及图像压缩技术，提升计算速度且不降低定位精度。

Result: 在Prophesee EVK4 HD事件相机与AgileX Scout Mini机器人上的室内外4000米实验证明，系统在自动导航时ATE（绝对轨迹误差）低于24厘米，能持续保持高频率控制更新，速度远超传统帧基系统。

Conclusion: 事件摄像机及其优化的匹配算法极大提升了教-重复导航的实时性与准确性，充分展示了事件视觉在实时机器人导航中的应用前景。

Abstract: Visual teach-and-repeat navigation enables robots to autonomously traverse
previously demonstrated paths by comparing current sensory input with recorded
trajectories. However, conventional frame-based cameras fundamentally limit
system responsiveness: their fixed frame rates (typically 30-60 Hz) create
inherent latency between environmental changes and control responses. Here we
present the first event-camera-based visual teach-and-repeat system. To achieve
this, we develop a frequency-domain cross-correlation framework that transforms
the event stream matching problem into computationally efficient Fourier space
multiplications, capable of exceeding 300Hz processing rates, an order of
magnitude faster than frame-based approaches. By exploiting the binary nature
of event frames and applying image compression techniques, we further enhance
the computational speed of the cross-correlation process without sacrificing
localization accuracy. Extensive experiments using a Prophesee EVK4 HD event
camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous
navigation across 4000+ meters of indoor and outdoor trajectories. Our system
achieves ATEs below 24 cm while maintaining consistent high-frequency control
updates. Our evaluations show that our approach achieves substantially higher
update rates compared to conventional frame-based systems, underscoring the
practical viability of event-based perception for real-time robotic navigation.

</details>


### [396] [Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)](https://arxiv.org/abs/2509.17299)
*Dorian Tsai,Christopher A. Brunner,Riki Lamont,F. Mikaela Nordborg,Andrea Severati,Java Terry,Karen Jackel,Matthew Dunbabin,Tobias Fischer,Scarlett Raine*

Main category: cs.RO

TL;DR: 本文提出了一种自动化的珊瑚产卵计数系统（CSLICS），通过计算机视觉技术大幅减轻了人工计数负担，提高了珊瑚养殖和珊瑚礁修复的效率。


<details>
  <summary>Details</summary>
Motivation: 珊瑚养殖及珊瑚礁修复需要持续准确地计数孵化幼体，以便分配资源和监测健康状况，但目前的方法高度依赖人工，效率低且成为生产瓶颈。

Method: 构建了低成本、模块化摄像头系统CSLICS，并结合人工参与标注的训练方法，实现了自动检测与计数。系统包括系统工程设计、数据集采集和计算机视觉算法（目标检测与分类）。

Result: 系统在大型产卵事件中表面产卵检测F1得分为82.4%，水下产卵检测F1得分为65.3%。每次产卵事件比人工采样节省5,720小时劳动力。与人工计数对比，CSLICS能准确监测受精成功率和水下产卵数量。

Conclusion: CSLICS大幅提升了珊瑚养殖自动化与效率，有助于珊瑚礁修复规模化，应对诸如大堡礁等生态系统面临的气候变化威胁。

Abstract: Coral aquaculture for reef restoration requires accurate and continuous spawn
counting for resource distribution and larval health monitoring, but current
methods are labor-intensive and represent a critical bottleneck in the coral
production pipeline. We propose the Coral Spawn and Larvae Imaging Camera
System (CSLICS), which uses low cost modular cameras and object detectors
trained using human-in-the-loop labeling approaches for automated spawn
counting in larval rearing tanks. This paper details the system engineering,
dataset collection, and computer vision techniques to detect, classify and
count coral spawn. Experimental results from mass spawning events demonstrate
an F1 score of 82.4\% for surface spawn detection at different embryogenesis
stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720
hours of labor per spawning event compared to manual sampling methods at the
same frequency. Comparison of manual counts with CSLICS monitoring during a
mass coral spawning event on the Great Barrier Reef demonstrates CSLICS'
accurate measurement of fertilization success and sub-surface spawn counts.
These findings enhance the coral aquaculture process and enable upscaling of
coral reef restoration efforts to address climate change threats facing
ecosystems like the Great Barrier Reef.

</details>


### [397] [Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing](https://arxiv.org/abs/2509.17308)
*Kazutoshi Tanaka,Tomoya Takahashi,Masashi Hamaya*

Main category: cs.RO

TL;DR: 提出了一种轻量化的线驱蛇形机械臂，并运用基于物理储备计算的方法进行姿态估算，取得比传统分析法和LSTM方法更优的精度。


<details>
  <summary>Details</summary>
Motivation: 由于蛇形机械臂灵活性和轻量化，被期望在复杂环境中具备避障、多向施力等优势。在实际应用中，灵活结构引入的线缆松弛、变形等问题导致分析模型与实际姿态存在较大误差，提升姿态估算精度成为一个关键难题。

Method: 设计了一款9自由度、全长545mm、重量仅308g的线驱蛇形臂。针对分析模型与实际误差大问题，提出基于物理储备计算的姿态估算方法，利用机械臂固有的非线性动力学作为高维‘储层’进行状态映射，对机械臂姿态进行高精度预测，并与LSTM神经网络和传统分析方法进行对比验证。

Result: 实验表明，所提储备计算方法在姿态估算上的平均误差为4.3mm，优于LSTM方法的4.4mm以及分析法的39.5mm。

Conclusion: 基于物理储备计算的姿态估算为轻量化线驱蛇形机械臂的控制和感知提供了新的思路，有效利用了结构本身的非线性动力学优势，展示了该方法的潜力。

Abstract: Cable-driven serpentine manipulators hold great potential in unstructured
environments, offering obstacle avoidance, multi-directional force application,
and a lightweight design. By placing all motors and sensors at the base and
employing plastic links, we can further reduce the arm's weight. To demonstrate
this concept, we developed a 9-degree-of-freedom cable-driven serpentine
manipulator with an arm length of 545 mm and a total mass of only 308 g.
However, this design introduces flexibility-induced variations, such as cable
slack, elongation, and link deformation. These variations result in
discrepancies between analytical predictions and actual link positions, making
pose estimation more challenging. To address this challenge, we propose a
physical reservoir computing based pose estimation method that exploits the
manipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.
Experimental results show a mean pose error of 4.3 mm using our method,
compared to 4.4 mm with a baseline long short-term memory network and 39.5 mm
with an analytical approach. This work provides a new direction for control and
perception strategies in lightweight cable-driven serpentine manipulators
leveraging their intrinsic dynamics.

</details>


### [398] [OpenGVL - Benchmarking Visual Temporal Progress for Data Curation](https://arxiv.org/abs/2509.17321)
*Paweł Budzianowski,Emilia Wiśnios,Gracjan Góral,Igor Kulakov,Viktor Petrenko,Krzysztof Walas*

Main category: cs.RO

TL;DR: 作者提出了OpenGVL，一个用于评估多样化操纵任务中任务进展预测能力的基准，并将其开源。


<details>
  <summary>Details</summary>
Motivation: 由于现实中可用的机器人数据正在快速增长，但如何高效利用和标注这些数据成为瓶颈。有效的任务完成进展预测能助力自动化数据标注和质量评估。

Method: 构建OpenGVL基准，基于GVL方法评估公开可用的开源基础模型与闭源模型在不同操纵任务上预测时间进展的能力，并验证其对自动数据筛选与质量评估的应用价值。

Result: 开源模型在时间进展预测任务上明显落后于闭源模型，仅达到后者约70%的性能。OpenGVL可作为自动化数据筛选和质量评估工具。

Conclusion: OpenGVL填补了多样化操纵任务进展预测的评测空白，并能实际辅助大规模机器人数据集的自动化处理，促进数据驱动型机器人研究的发展。

Abstract: Data scarcity remains one of the most limiting factors in driving progress in
robotics. However, the amount of available robotics data in the wild is growing
exponentially, creating new opportunities for large-scale data utilization.
Reliable temporal task completion prediction could help automatically annotate
and curate this data at scale. The Generative Value Learning (GVL) approach was
recently proposed, leveraging the knowledge embedded in vision-language models
(VLMs) to predict task progress from visual observations. Building upon GVL, we
propose OpenGVL, a comprehensive benchmark for estimating task progress across
diverse challenging manipulation tasks involving both robotic and human
embodiments. We evaluate the capabilities of publicly available open-source
foundation models, showing that open-source model families significantly
underperform closed-source counterparts, achieving only approximately $70\%$ of
their performance on temporal progress prediction tasks. Furthermore, we
demonstrate how OpenGVL can serve as a practical tool for automated data
curation and filtering, enabling efficient quality assessment of large-scale
robotics datasets. We release the benchmark along with the complete codebase at
\href{github.com/budzianowski/opengvl}{OpenGVL}.

</details>


### [399] [AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation](https://arxiv.org/abs/2509.17340)
*Xin Chen,Rui Huang,Longbin Tang,Lin Zhao*

Main category: cs.RO

TL;DR: AERO-MPPI提出了一种完全基于GPU的三维无图导航系统，使用多MPPI优化器并通过新颖的锚点引导机制，实现了在复杂环境中的无人机敏捷导航，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂三维拥挤环境下实现无人机高效、可靠、敏捷飞行存在巨大挑战。传统的建图—规划—控制流程不仅计算开销大，还易受累积误差影响，难以满足实际需求。需要一种能实时处理感知和规划且可靠性更高的新方法。

Method: 提出AERO-MPPI框架，充分利用GPU能力，通过多分辨率的激光雷达点云快速提取空间锚点，并以此引导多模型预测路径积分（MPPI）优化器并行搜索多类可行路径，每步规划中使用多目标代价函数平衡避障与到达目标。核心全部用NVIDIA GPU进行加速。

Result: 在森林、起伏和陡坡等仿真环境中测试，AERO-MPPI实现了高于7 m/s的稳定飞行，任务成功率超过80%，轨迹更加平滑，优于主流方法。实机测试表明，该方法能稳定在NVIDIA Jetson Orin NX 16G平台上实时运行，实现安全、鲁棒、敏捷的飞行。

Conclusion: AERO-MPPI不仅显著提升了无人机在复杂环境中的敏捷性与可靠性，还具备实时运行能力，有望推动无人自主飞行技术发展。代码将在论文接收后开源，便于社区应用与改进。

Abstract: Agile mapless navigation in cluttered 3D environments poses significant
challenges for autonomous drones. Conventional mapping-planning-control
pipelines incur high computational cost and propagate estimation errors. We
present AERO-MPPI, a fully GPU-accelerated framework that unifies perception
and planning through an anchor-guided ensemble of Model Predictive Path
Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR
point-cloud representation that rapidly extracts spatially distributed
"anchors" as look-ahead intermediate endpoints, from which we construct
polynomial trajectory guides to explore distinct homotopy path classes. At each
planning step, we run multiple MPPI instances in parallel and evaluate them
with a two-stage multi-objective cost that balances collision avoidance and
goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI
achieves real-time onboard operation and mitigates the local-minima failures of
single-MPPI approaches. Extensive simulations in forests, verticals, and
inclines demonstrate sustained reliable flight above 7 m/s, with success rates
above 80% and smoother trajectories compared to state-of-the-art baselines.
Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX
16G confirm that AERO-MPPI runs in real time onboard and consistently achieves
safe, agile, and robust flight in complex cluttered environments. The code will
be open-sourced upon acceptance of the paper.

</details>


### [400] [DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception](https://arxiv.org/abs/2509.17350)
*Haoran Zhou,Yangwei You,Shuaijun Wang*

Main category: cs.RO

TL;DR: 本文提出了DyDexHandover框架，实现了仿人双臂机器人在空中递接物体（抛接）的高成功率，仅利用RGB视觉输入。


<details>
  <summary>Details</summary>
Motivation: 动态空中递接是双臂机器人面临的重要挑战，需在感知、协调和动作自然性方面具备高精度。然而，现有方法依赖动态模型、强先验或深度感知，导致泛化能力和动作自然性不足。

Method: 本文提出基于多智能体强化学习的端到端RGB视觉策略，训练双臂机器人完成仿人抛接动作。引入人类策略正则化，提升其动作流畅性和泛化能力，并在Isaac Sim仿真环境中进行了实验。

Result: DyDexHandover在训练集物体上抛接成功率接近99%，对未见物体也有75%的成功率，同时动作更加仿人自然。

Conclusion: 本研究首次实现了仅用RGB视觉输入的双臂机器人空中递接任务，为仿人自然运动与高泛化性的双臂机器人交互提供了新方法。

Abstract: Dynamic in air handover is a fundamental challenge for dual-arm robots,
requiring accurate perception, precise coordination, and natural motion. Prior
methods often rely on dynamics models, strong priors, or depth sensing,
limiting generalization and naturalness. We present DyDexHandover, a novel
framework that employs multi-agent reinforcement learning to train an end to
end RGB based policy for bimanual object throwing and catching. To achieve more
human-like behavior, the throwing policy is guided by a human policy
regularization scheme, encouraging fluid and natural motion, and enhancing the
generalization capability of the policy. A dual arm simulation environment was
built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly
99 percent success on training objects and 75 percent on unseen objects, while
generating human-like throwing and catching behaviors. To our knowledge, it is
the first method to realize dual-arm in-air handover using only raw RGB
perception.

</details>


### [401] [Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators](https://arxiv.org/abs/2509.17381)
*Yongliang Wang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: 本文提出一种结合视觉和强化学习的新型机械臂轨迹规划系统，实现在复杂杂乱环境下的高效无障碍运动，并通过算法优化提升了执行精度、稳定性和跨任务的适应性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化和拥挤环境中为机械臂生成无障碍轨迹极具挑战，现有方法通常依赖复杂的动力学求解，计算量大、效率低。因此，探索无需动力学模型的强化学习在此任务上的潜力，并提升其实际应用效果是本文的核心动机。

Method: 本文方法框架分为两部分：第一部分在任务空间引入创新的视觉轨迹规划器，采用大规模FSA模型和B样条优化的动静态混合路径搜索；第二部分针对联合空间障碍规避，提出结合动作集成（AE）和策略反馈（PF）机制增强的PPO强化学习方法，实现精准、高效的轨迹执行与避障。

Result: 实验验证了所提PPO改进的有效性，提升了模型在仿真到仿真和仿真到现实任务中的鲁棒性，有效增强了轨迹规划器在复杂场景下的性能，包括实时障碍规避和目标到达准确性。

Conclusion: 结合视觉路径与强化学习的新系统显著提升了机械臂在动态、复杂环境下的轨迹生成效率与避障能力，同时具备良好的泛化性和现实部署潜力。

Abstract: Generating obstacle-free trajectories for robotic manipulators in
unstructured and cluttered environments remains a significant challenge.
Existing motion planning methods often require additional computational effort
to generate the final trajectory by solving kinematic or dynamic equations.
This paper highlights the strong potential of model-free reinforcement learning
methods over model-based approaches for obstacle-free trajectory planning in
joint space. We propose a fast trajectory planning system for manipulators that
combines vision-based path planning in task space with reinforcement
learning-based obstacle avoidance in joint space. We divide the framework into
two key components. The first introduces an innovative vision-based trajectory
planner in task space, leveraging the large-scale fast segment anything (FSA)
model in conjunction with basis spline (B-spline)-optimized kinodynamic path
searching. The second component enhances the proximal policy optimization (PPO)
algorithm by integrating action ensembles (AE) and policy feedback (PF), which
greatly improve precision and stability in goal-reaching and obstacle avoidance
within the joint space. These PPO enhancements increase the algorithm's
adaptability across diverse robotic tasks, ensuring consistent execution of
commands from the first component by the manipulator, while also enhancing both
obstacle avoidance efficiency and reaching accuracy. The experimental results
demonstrate the effectiveness of PPO enhancements, as well as
simulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)
transfer, in improving model robustness and planner efficiency in complex
scenarios. These enhancements allow the robot to perform obstacle avoidance and
real-time trajectory planning in obstructed environments. Project page
available at: https://sites.google.com/view/ftp4rm/home

</details>


### [402] [High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics](https://arxiv.org/abs/2509.17387)
*Ziqing Zou,Cong Wang,Yue Hu,Xiao Liu,Bowen Xu,Rong Xiong,Changjie Fan,Yingfeng Chen,Yue Wang*

Main category: cs.RO

TL;DR: 提出了EfficientTrack方法，能够高效高精度地追踪液压挖掘机轨迹，显著减少与环境的交互次数，并在仿真和真实实验中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 液压挖掘机的复杂非线性动态（如时延和控制耦合）导致高精度轨迹追踪困难。传统控制方法无法有效处理这些问题，而基于学习的方法又需大量实际交互，效率低下。

Method: 提出EfficientTrack轨迹追踪方法，结合模型驱动学习管理非线性动态，并利用闭环动态提升学习效率，实现最小化的追踪误差。

Result: 在仿真和真实挖掘机实验中均进行了系统验证。仿真中，EfficientTrack超越其他基于学习的方法，实现了最高的追踪精度和平滑度，并且所需交互次数最少。真实环境下在负载下依然表现出色，并具备持续学习能力。

Conclusion: EfficientTrack方法在处理复杂非线性、追踪精度和学习效率方面均表现优异，具有高度的实际应用前景。

Abstract: The complex nonlinear dynamics of hydraulic excavators, such as time delays
and control coupling, pose significant challenges to achieving high-precision
trajectory tracking. Traditional control methods often fall short in such
applications due to their inability to effectively handle these nonlinearities,
while commonly used learning-based methods require extensive interactions with
the environment, leading to inefficiency. To address these issues, we introduce
EfficientTrack, a trajectory tracking method that integrates model-based
learning to manage nonlinear dynamics and leverages closed-loop dynamics to
improve learning efficiency, ultimately minimizing tracking errors. We validate
our method through comprehensive experiments both in simulation and on a
real-world excavator. Comparative experiments in simulation demonstrate that
our method outperforms existing learning-based approaches, achieving the
highest tracking precision and smoothness with the fewest interactions.
Real-world experiments further show that our method remains effective under
load conditions and possesses the ability for continual learning, highlighting
its practical applicability. For implementation details and source code, please
refer to https://github.com/ZiqingZou/EfficientTrack.

</details>


### [403] [3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks](https://arxiv.org/abs/2509.17389)
*Lois Liow,Jonty Milford,Emre Uygun,Andre Farinha,Vinoth Viswanathan,Josh Pinskier,David Howard*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法，利用3D打印技术制造高灵敏度软体“物理孪生体”，用于更安全、高效地操作和研究脆弱生态系统中的敏感样本（如珊瑚），并通过实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 生态保护等领域常需操作易碎样本（如珊瑚），但现有方法容易造成损伤且采集数据困难。此外，利用机器学习进行操作策略训练，需要安全获取数据，而实物试验存在伦理和风险问题。本文旨在解决这两大难题。

Method: 作者开发了一套自动化设计流程，可根据3D扫描或模型，按需打印复杂可定制的软体传感结构。这些结构内嵌液态金属软体传感器，可以复制自然界复杂几何形态。通过3D打印软体“传感珊瑚”，作为真实珊瑚的物理孪生体，用于实验和数据采集。

Result: 实验显示，所构建的传感珊瑚能精准检测小于0.5N的轻微接触力，能够反映操作易碎物体时的精细力学交互。作者分别在自动化珊瑚标签识别和机器人珊瑚养殖等两项任务中展示了孪生体的应用效果，验证了其实用性和优于传统传感技术的反馈能力。

Conclusion: 物理孪生体融合高灵敏软体传感器，为易碎样本的自动化操作与数据采集提供了一种伦理、安全、可扩展的新方案。这一技术可提升机器人在生态保护等领域的敏感操作能力，也能为强化学习等学习型操作提供高质量实验反馈。

Abstract: Robotics and automation are key enablers to increase throughput in ongoing
conservation efforts across various threatened ecosystems. Cataloguing,
digitisation, husbandry, and similar activities require the ability to interact
with delicate, fragile samples without damaging them. Additionally,
learning-based solutions to these tasks require the ability to safely acquire
data to train manipulation policies through, e.g., reinforcement learning. To
address these twin needs, we introduce a novel method to print free-form,
highly sensorised soft 'physical twins'. We present an automated design
workflow to create complex and customisable 3D soft sensing structures on
demand from 3D scans or models. Compared to the state of the art, our soft
liquid metal sensors faithfully recreate complex natural geometries and display
excellent sensing properties suitable for validating performance in delicate
manipulation tasks. We demonstrate the application of our physical twins as
'sensing corals': high-fidelity, 3D printed replicas of scanned corals that
eliminate the need for live coral experimentation, whilst increasing data
quality, offering an ethical and scalable pathway for advancing autonomous
coral handling and soft manipulation broadly. Through extensive bench-top
manipulation and underwater grasping experiments, we show that our sensing
coral is able to detect grasps under 0.5 N, effectively capturing the delicate
interactions and light contact forces required for coral handling. Finally, we
showcase the value of our physical twins across two demonstrations: (i)
automated coral labelling for lab identification and (ii) robotic coral
aquaculture. Sensing physical twins such as ours can provide richer grasping
feedback than conventional sensors providing experimental validation of prior
to deployment in handling fragile and delicate items.

</details>


### [404] [FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR](https://arxiv.org/abs/2509.17390)
*Junzhe Wu,Yufei Jia,Yiyi Yan,Zhixing Chen,Tiao Tan,Zifan Wang,Guangyu Wang*

Main category: cs.RO

TL;DR: 该论文提出了FGGS-LiDAR框架，实现了三维高斯Splatting（3DGS）资产向高性能激光雷达仿真的无缝转换。方法能将任意3DGS模型转换为高保真封闭网格，并以高帧率进行LiDAR仿真。


<details>
  <summary>Details</summary>
Motivation: 3DGS广泛应用于高保真可视化，但其资产无法直接用于机器人与自动驾驶所依赖的高性能LiDAR仿真。该差距阻碍了3DGS资产在几何精确深度感知等多模态仿真任务中的应用。

Method: FGGS-LiDAR通过体素离散化与Truncated Signed Distance Field（TSDF）提取，将任何预训练的3DGS模型转换为水密网格。结合GPU加速的射线投射模块，实现了高帧率的LiDAR模拟，无需LiDAR监督或架构改动。

Result: 在室内和室外场景验证中，该方法生成的网格几何精度高，LiDAR仿真速度达500 FPS以上。展示了与真实深度信息的高度一致性，能够准确反映环境几何结构。

Conclusion: FGGS-LiDAR将3DGS资产直接用于高精度深度感知，突破了先前只能用于可视化的限制，显著拓展了其应用范围，为大规模多模态仿真提供了新工具。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic
rendering, its vast ecosystem of assets remains incompatible with
high-performance LiDAR simulation, a critical tool for robotics and autonomous
driving. We present \textbf{FGGS-LiDAR}, a framework that bridges this gap with
a truly plug-and-play approach. Our method converts \textit{any} pretrained
3DGS model into a high-fidelity, watertight mesh without requiring
LiDAR-specific supervision or architectural alterations. This conversion is
achieved through a general pipeline of volumetric discretization and Truncated
Signed Distance Field (TSDF) extraction. We pair this with a highly optimized,
GPU-accelerated ray-casting module that simulates LiDAR returns at over 500
FPS. We validate our approach on indoor and outdoor scenes, demonstrating
exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for
geometrically accurate depth sensing, our framework extends their utility
beyond visualization and unlocks new capabilities for scalable, multimodal
simulation. Our open-source implementation is available at
https://github.com/TATP-233/FGGS-LiDAR.

</details>


### [405] [GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera](https://arxiv.org/abs/2509.17435)
*Xiaoyu Wang,Yan Rui Tan,William Leong,Sunan Huang,Rodney Teo,Cheng Xiang*

Main category: cs.RO

TL;DR: 本文提出了一种只基于RGB相机的图像视觉伺服（IBVS）框架，实现了无人机自主导航与避障，无需依赖外部设备。


<details>
  <summary>Details</summary>
Motivation: 尽管无人机导航已广泛研究，但在处理多目标和避障任务时，基于IBVS的方法仍存在较大挑战，尤其是在无需额外传感器的情况下。

Method: 该方法通过AI驱动的单目深度估计，从RGB图像中获取深度信息，实现避障，并利用多AprilTag实现多目标导航。整个系统部署在Jetson平台上，完全自主运行，无需外部路径规划或额外硬件。

Result: 实验结果表明，在无GPS环境下，无人机能够穿越多个AprilTag并有效避开障碍物，系统表现出良好的适应性和实用性。

Conclusion: 本文方法有效实现了无人机基于单一RGB相机的自主导航和避障，具备部署简便、独立性强等优点，为相关场景的应用提供了新的解决方案。

Abstract: This paper proposes an image-based visual servoing (IBVS) framework for UAV
navigation and collision avoidance using only an RGB camera. While UAV
navigation has been extensively studied, it remains challenging to apply IBVS
in missions involving multiple visual targets and collision avoidance. The
proposed method achieves navigation without explicit path planning, and
collision avoidance is realized through AI-based monocular depth estimation
from RGB images. Unlike approaches that rely on stereo cameras or external
workstations, our framework runs fully onboard a Jetson platform, ensuring a
self-contained and deployable system. Experimental results validate that the
UAV can navigate across multiple AprilTags and avoid obstacles effectively in
GPS-denied environments.

</details>


### [406] [Learning Dexterous Manipulation with Quantized Hand State](https://arxiv.org/abs/2509.17450)
*Ying Feng,Hongjie Fang,Yinong He,Jingjing Chen,Chenxi Wang,Zihao He,Ruonan Liu,Cewu Lu*

Main category: cs.RO

TL;DR: 提出了一种新的方法（DQ-RISE），通过将灵巧机械手的手部状态量化，并与手臂动作协同，实现了高效且更平衡的学习，解决了手部与手臂动作耦合导致控制困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的灵巧操作方法中，臂与手动作通常被统一表示在同一个高维空间，这导致手部动作主导了整个动作空间，对手臂控制造成影响，妨碍了高度协调的灵巧操作。

Method: 提出DQ-RISE方法，将高维的手部状态进行量化以简化预测，同时对手臂动作采用连续松弛方式，使手与臂可以在一个更结构化的空间中协同扩散，防止手部动作主导，促进协调学习。

Result: 实验显示，DQ-RISE方法相比传统的联合空间策略，在学习灵巧操作过程中取得了更加平衡、高效的表现。

Conclusion: DQ-RISE的方法结构化了手臂与手部协同控制，提升了灵巧机械手泛化性和操作能力，为未来可推广的灵巧操作打下基础。

Abstract: Dexterous robotic hands enable robots to perform complex manipulations that
require fine-grained control and adaptability. Achieving such manipulation is
challenging because the high degrees of freedom tightly couple hand and arm
motions, making learning and control difficult. Successful dexterous
manipulation relies not only on precise hand motions, but also on accurate
spatial positioning of the arm and coordinated arm-hand dynamics. However, most
existing visuomotor policies represent arm and hand actions in a single
combined space, which often causes high-dimensional hand actions to dominate
the coupled action space and compromise arm control. To address this, we
propose DQ-RISE, which quantizes hand states to simplify hand motion prediction
while preserving essential patterns, and applies a continuous relaxation that
allows arm actions to diffuse jointly with these compact hand states. This
design enables the policy to learn arm-hand coordination from data while
preventing hand actions from overwhelming the action space. Experiments show
that DQ-RISE achieves more balanced and efficient learning, paving the way
toward structured and generalizable dexterous manipulation. Project website:
http://rise-policy.github.io/DQ-RISE/

</details>


### [407] [Morphologies of a sagging elastica with intrinsic sensing and actuation](https://arxiv.org/abs/2509.17572)
*Vishnu Deo Mishra,S Ganga Prasath*

Main category: cs.RO

TL;DR: 本文研究了细长软体机器人通过内部传感器感知形状并由内部驱动器施加力矩改变形态的过程，提出用简单的基于曲率的反馈策略优化其形态控制效果，并揭示了控制误差与传感和驱动受限之间的关系。


<details>
  <summary>Details</summary>
Motivation: 细长软体机器人在实现复杂形态变换时，计算所需驱动力矩存在较大难度，主要由于结构的非线性、实验模型误差、传感与驱动能力有限。因此，研究简单有效的反馈控制策略对解决实际控制难题具有重要意义。

Method: 作者将软体机器人建模为弹性曲线（elastica），采用传感器感知曲率、驱动器按照曲率成比例施加力矩的反馈方式，并考虑有限数量传感器与驱动器所带来的空间滤波效应（通过滤波器宽度建模）。分析设备在重力作用下校直过程中的动力学表现，并探索传感与驱动配置对形态控制精度的影响。

Result: 通过数值模拟与理论分析，揭示了在不同重力-弯曲数、反馈增益、滤波器宽度下系统表现出不同的形态不稳定性。发现传感器间距、驱动器尺寸等参数影响设备对长、短波形态特征的响应精度。给定滤波宽度时，选择适当的驱动增益（与滤波宽度的平方成正比）可最小化形态变换误差。

Conclusion: 本文提出的反馈调节模型能够定量分析有限传感与驱动能力下软体机器人的形态调控问题，对其在实际复杂运动应用中的设计与性能优化具有指导意义。

Abstract: The morphology of a slender soft-robot can be modified by sensing its shape
via sensors and exerting moments via actuators embedded along its body. The
actuating moments required to morph these soft-robots to a desired shape are
often difficult to compute due to the geometric non-linearity associated with
the structure, the errors in modeling the experimental system, and the
limitations in sensing and feedback/actuation capabilities. In this article, we
explore the effect of a simple feedback strategy (actuation being proportional
to the sensed curvature) on the shape of a soft-robot, modeled as an elastica.
The finite number of sensors and actuators, often seen in experiments, is
captured in the model via filters of specified widths. Using proportional
feedback, we study the simple task of straightening the device by compensating
for the sagging introduced by its self-weight. The device undergoes a hierarchy
of morphological instabilities defined in the phase-space given by the
gravito-bending number, non-dimensional sensing/feedback gain, and the scaled
width of the filter. For complex shape-morphing tasks, given a perfect model of
the device with limited sensing and actuating capabilities, we find that a
trade-off arises (set by the sensor spacing & actuator size) between capturing
the long and short wavelength features. We show that the error in
shape-morphing is minimal for a fixed filter width when we choose an
appropriate actuating gain (whose magnitude goes as a square of the filter
width). Our model provides a quantitative lens to study and design slender soft
devices with limited sensing and actuating capabilities for complex maneuvering
applications.

</details>


### [408] [GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots](https://arxiv.org/abs/2509.17582)
*Vassil Atanassov,Wanming Yu,Siddhant Gangapurwala,James Wilson,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 本文提出了一种称为GeCCo的通用接触条件策略，通过深度强化学习训练，可用于四足机器人在不同任务和场景中的通用控制，无需每次从零开始训练新策略。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人运动主要依赖端到端的深度强化学习，但每次遇到新任务都需要耗费大量时间进行奖励函数设计和调优，影响了方法的可扩展性和通用性。

Method: 作者提出了一种低层次通用控制器GeCCo，该策略通过深度强化学习训练，可以跟踪机器人任意设定的接触点。通过结合任务相关的高层次接触规划与预训练的通用控制器，实现不同任务的高效适应，无需反复训练。

Result: 实验在统一的框架下，使用单一通用策略，验证了在多种运动与操作任务（如多种步态、复杂地形、物体交互等）中的可扩展性和鲁棒性。系统能够更高效地获得新行为。

Conclusion: GeCCo策略可为各种高层次任务提供通用的模块化低层控制能力，显著提升了四足机器人任务扩展的效率和适应性，无需每次重新训练低层控制。

Abstract: Most modern approaches to quadruped locomotion focus on using Deep
Reinforcement Learning (DRL) to learn policies from scratch, in an end-to-end
manner. Such methods often fail to scale, as every new problem or application
requires time-consuming and iterative reward definition and tuning. We present
Generalist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained
with Deep Reinforcement Learning that is capable of tracking arbitrary contact
points on a quadruped robot. The strength of our approach is that it provides a
general and modular low-level controller that can be reused for a wider range
of high-level tasks, without the need to re-train new controllers from scratch.
We demonstrate the scalability and robustness of our method by evaluating on a
wide range of locomotion and manipulation tasks in a common framework and under
a single generalist policy. These include a variety of gaits, traversing
complex terrains (eg. stairs and slopes) as well as previously unseen
stepping-stones and narrow beams, and interacting with objects (eg. pushing
buttons, tracking trajectories). Our framework acquires new behaviors more
efficiently, simply by combining a task-specific high-level contact planner and
the pre-trained generalist policy. A supplementary video can be found at
https://youtu.be/o8Dd44MkG2E.

</details>


### [409] [Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery](https://arxiv.org/abs/2509.17666)
*Mimo Shirasaka,Cristian C. Beltran-Hernandez,Masashi Hamaya,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 本文提出了一种利用被动柔性软手腕进行物体插入任务的方法，通过提供合适的柔顺性实现对失败的恢复，无需高频控制或力传感器，极大增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 物体插入任务在存在位姿不确定性和环境变化时，容易失败，且以往常需人工微调或重新训练控制器。作者希望实现一种更鲁棒、无需复杂感知和控制的插入方法，还能自动恢复失败。

Method: 作者设计了一种被动柔顺的软手腕，能够吸收接触带来的冲击，通过序列化的接触形成逐步限制物体自由度。同时引入自动故障恢复机制，即手腕柔顺性允许多次安全地尝试恢复。与此同时，使用预训练的视觉-语言模型来评估任务执行终态，识别失败模式并给出恢复建议（选择合适的技能、更新目标）。

Result: 在仿真中，该方法在多种失配和扰动（最大抓取误差5度、插孔误差20mm、摩擦力提升5倍、不同形状插头）下恢复插入，成功率达到了83%。另外还在真实机器人上进行了验证。

Conclusion: 本文方法无需高频控制和复杂感知，实现了高鲁棒性和自动化的插入失败恢复，对实际工业和服务机器人具有应用前景。

Abstract: Object insertion tasks are prone to failures under pose uncertainties and
environmental variations, traditionally requiring manual finetuning or
controller retraining. We present a novel approach for robust and resilient
object insertion using a passively compliant soft wrist that enables safe
contact absorption through large deformations, without high-frequency control
or force sensing. Our method structures insertion as compliance-enabled contact
formations, sequential contact states that progressively constrain degrees of
freedom, and integrates automated failure recovery strategies. Our key insight
is that wrist compliance permits safe, repeated recovery attempts; hence, we
refer to it as compliance-enabled failure recovery. We employ a pre-trained
vision-language model (VLM) that assesses each skill execution from terminal
poses and images, identifies failure modes, and proposes recovery actions by
selecting skills and updating goals. In simulation, our method achieved an 83%
success rate, recovering from failures induced by randomized
conditions--including grasp misalignments up to 5 degrees, hole-pose errors up
to 20mm, fivefold increases in friction, and previously unseen
square/rectangular pegs--and we further validate the approach on a real robot.

</details>


### [410] [Towards Learning Boulder Excavation with Hydraulic Excavators](https://arxiv.org/abs/2509.17683)
*Jonas Gruetter,Lorenzo Terenzi,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: 本论文提出了一种利用强化学习训练的控制策略，使标准挖掘机无需更换专用夹具，也能在户外复杂环境下自动搬运不规则大型岩石。


<details>
  <summary>Details</summary>
Motivation: 建筑工地经常需要在施工前清理大型岩石，但目前常用方法要么效率低（仅用常规铲斗由人工操作），要么需频繁更换专用夹具，影响施工效率。且自动化方案大多聚焦于松散介质或依赖精细几何规划，难以适应不规则大石块。

Method: 作者在仿真中基于刚体动力学和解析土壤模型训练强化学习策略，输入仅包括通过视觉分割获得的稀疏LiDAR点云（每个岩石仅20个点）及自身位置信息，直接控制常规挖掘机铲斗，根据不同土壤阻力自适应调整策略。

Result: 在真实12吨级挖掘机上实地测试，不同尺寸（0.4-0.7米）和不同土质条件下，该自动驾驶方案岩石搬运成功率为70%，人类操作员为83%。

Conclusion: 标准挖掘设备在感知受限和环境复杂的情况下，借助强化学习可以学会复杂的物体操作任务，无需专用工具，显示出其在实际工程自动化领域的应用潜力。

Abstract: Construction sites frequently require removing large rocks before excavation
or grading can proceed. Human operators typically extract these boulders using
only standard digging buckets, avoiding time-consuming tool changes to
specialized grippers. This task demands manipulating irregular objects with
unknown geometries in harsh outdoor environments where dust, variable lighting,
and occlusions hinder perception. The excavator must adapt to varying soil
resistance--dragging along hard-packed surfaces or penetrating soft
ground--while coordinating multiple hydraulic joints to secure rocks using a
shovel. Current autonomous excavation focuses on continuous media (soil,
gravel) or uses specialized grippers with detailed geometric planning for
discrete objects. These approaches either cannot handle large irregular rocks
or require impractical tool changes that interrupt workflow. We train a
reinforcement learning policy in simulation using rigid-body dynamics and
analytical soil models. The policy processes sparse LiDAR points (just 20 per
rock) from vision-based segmentation and proprioceptive feedback to control
standard excavator buckets. The learned agent discovers different strategies
based on soil resistance: dragging along the surface in hard soil and
penetrating directly in soft conditions. Field tests on a 12-ton excavator
achieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to
83% for human operators. This demonstrates that standard construction equipment
can learn complex manipulation despite sparse perception and challenging
outdoor conditions.

</details>


### [411] [EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering](https://arxiv.org/abs/2509.17750)
*Inkyu Jang,Jonghae Park,Chams E. Mballo,Sihyun Cho,Claire J. Tomlin,H. Jin Kim*

Main category: cs.RO

TL;DR: 论文提出了一种名为EigenSafe的新方法，通过算子理论框架，提高在随机系统中实现安全关键控制的能力。该方法利用动态规划原理和主特征对来度量系统安全性，并结合学习机制实现安全策略筛选。验证显示该方法在仿真任务中有效。


<details>
  <summary>Details</summary>
Motivation: 现有诸如Hamilton-Jacobi可达集和控制屏障函数等安全分析工具难以全面度量带有随机性的机器人系统的安全性，尤其面对感知噪声和环境扰动时更是如此，因此需要更系统性的方法评估和保障安全。

Method: 作者导出了一个描述安全概率动态规划原理的线性算子，并发现其主特征对能量化单一状态和整体闭环系统的安全信息。基于这一算子，提出名为EigenSafe的学习框架，线下联合学习主特征对及安全备选策略。训练好的特征函数可在实际控制中用于实时安全筛查，一旦检测风险自动切换到安全策略。

Result: 作者在三个模拟的随机安全关键控制任务中验证了EigenSafe方法，证明了框架在保障安全和灵活应对随机因素方面的有效性和适用性。

Conclusion: EigenSafe框架通过算子理论与学习策略结合，为复杂随机系统提供了一种新型、安全性更高且可实际部署的控制方法，有望在实际机器人和自动控制领域推广应用。

Abstract: We present EigenSafe, an operator-theoretic framework for learning-enabled
safety-critical control for stochastic systems. In many robotic systems where
dynamics are best modeled as stochastic systems due to factors such as sensing
noise and environmental disturbances, it is challenging for conventional
methods such as Hamilton-Jacobi reachability and control barrier functions to
provide a holistic measure of safety. We derive a linear operator governing the
dynamic programming principle for safety probability, and find that its
dominant eigenpair provides information about safety for both individual states
and the overall closed-loop system. The proposed learning framework, called
EigenSafe, jointly learns this dominant eigenpair and a safe backup policy in
an offline manner. The learned eigenfunction is then used to construct a safety
filter that detects potentially unsafe situations and falls back to the backup
policy. The framework is validated in three simulated stochastic
safety-critical control tasks.

</details>


### [412] [MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies](https://arxiv.org/abs/2509.17759)
*Chengbo Yuan,Rui Zhou,Mengzhen Liu,Yingdong Hu,Shengjie Wang,Li Yi,Chuan Wen,Shanghang Zhang,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出MotionTrans框架，通过结合人类与机器人数据，多任务协同训练机器人操作策略，实现人类动作向机器人转移并获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习因真实机器人数据的难以大规模采集而受限，迫切需要利用辅助数据。相比图像和语言，动作知识从互联网上获取更难。人类在操作任务中动作多样，因此人类数据有望为机器人动作学习带来突破。

Method: 提出MotionTrans，包括数据采集系统、人类数据转化流程和带权重的协同训练方法。框架支持30个多任务人机协同训练，直接将人类任务动作迁移到可部署的机器人端到端策略。

Result: 在13项任务中成功实现动作迁移，9项任务零样本下也达到显著成功率；总体预训练-微调环节成功率提升40%。消融实验发现：与机器人数据协同训练和广泛的相关动作覆盖是成功的关键。

Conclusion: MotionTrans验证了人类动作数据可直接为机器人提供有效动作策略，对机器学习利用人类操作数据提出了高效实践方案，为机器人操作政策训练带来新视角。所有数据与代码已开源。

Abstract: Scaling real robot data is a key bottleneck in imitation learning, leading to
the use of auxiliary data for policy training. While other aspects of robotic
manipulation such as image or language understanding may be learned from
internet-based datasets, acquiring motion knowledge remains challenging. Human
data, with its rich diversity of manipulation behaviors, offers a valuable
resource for this purpose. While previous works show that using human data can
bring benefits, such as improving robustness and training efficiency, it
remains unclear whether it can realize its greatest advantage: enabling robot
policies to directly learn new motions for task completion. In this paper, we
systematically explore this potential through multi-task human-robot
cotraining. We introduce MotionTrans, a framework that includes a data
collection system, a human data transformation pipeline, and a weighted
cotraining strategy. By cotraining 30 human-robot tasks simultaneously, we
direcly transfer motions of 13 tasks from human data to deployable end-to-end
robot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot
manner. MotionTrans also significantly enhances pretraining-finetuning
performance (+40% success rate). Through ablation study, we also identify key
factors for successful motion learning: cotraining with robot data and broad
task-related motion coverage. These findings unlock the potential of
motion-level learning from human data, offering insights into its effective use
for training robotic manipulation policies. All data, code, and model weights
are open-sourced https://motiontrans.github.io/.

</details>


### [413] [Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research](https://arxiv.org/abs/2509.17760)
*Austin Wilson,Sahar Kapasi,Zane Greene,Alexis E. Block*

Main category: cs.RO

TL;DR: 本文介绍了一种对NAO机器人进行硬件和软件升级的增强方案，使其在失去原厂支持后仍具备现代的感知与交互能力。实验证明新系统大幅提升了对话质量和用户偏好。该思路可延伸至其他退役机器人平台，延长其使用寿命。


<details>
  <summary>Details</summary>
Motivation: 许多研究团队面临机器人硬件过时、厂商停止支持、无法适应现代感知与交互需求等问题。为维护已有机器人的科研价值和延长平台寿命，急需可扩展的升级方法。

Method: 对Aldebaran NAO机器人进行硬件升级（包括麦克风阵列、RGB-D与热成像相机、计算资源），并结合本地及云端感知与对话模型（软件升级）；在保证原有机体表达动作的基础上，开展对比实验验证性能提升。

Result: 增强型NAO在对话质量和用户偏好上均显著优于官方AI版，响应延迟无增加。新麦克风与低延迟音频处理有效减少自听与多人交谈混淆。新增视觉和热传感奠定了未来互动的基础。

Conclusion: 该增强方案不但极大提升了NAO及类似遗留机器人的先进性和互动能力，还提供了平台无关的升级思路，助力老旧机器人持续用于人机交互研究。

Abstract: Many research groups face challenges when legacy (unsupported) robotic
platforms lose manufacturer support and cannot accommodate modern sensing,
speech, and interaction capabilities. We present the Enhanced NAO, a
revitalized version of Aldebaran's NAO robot that uses upgraded microphones,
RGB-D and thermal cameras, and additional compute resources in a fully
self-contained package. This system combines cloud and local models for
perception and dialogue, while preserving the NAO's expressive body and
behaviors. In a pilot validation study, the Enhanced NAO delivered
significantly higher conversational quality and stronger user preference
compared to the NAO AI Edition, without increasing response latency. Key
upgrades, such as beamforming microphones and low-latency audio processing,
reduced artifacts like self-hearing and improved multi-party separation.
Expanded visual and thermal sensing established a foundation for future
interaction capabilities. Beyond the NAO, our framework provides a
platform-agnostic strategy for extending the lifespan and research utility of
legacy robots, ensuring they remain valuable tools for human-robot interaction.

</details>


### [414] [RoboSeek: You Need to Interact with Your Objects](https://arxiv.org/abs/2509.17783)
*Yibo Peng,Jiahao Yang,Shenhao Yan,Ziyu Huang,Shuang Li,Shuguang Cui,Yiming Zhao,Yatong Han*

Main category: cs.RO

TL;DR: RoboSeek提出了一种基于交互经验优化动作执行的机器人操控框架，通过仿真-现实循环转移，实现多个平台和任务上的高效泛化与稳定性，平均成功率大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交互驱动的机器人学习在长期任务（如序贯决策、物理约束及感知不确定性下）方面仍存在巨大挑战。受具身认知理论启发，作者希望通过优化机器人对环境的感知和动作经验，提高在复杂操控任务中的实用性、泛化能力和稳定性。

Method: 提出RoboSeek框架：1）以具身认知理论为基础，利用3D重建将现实环境仿真复刻，保证视觉和物理一致性；2）在仿真中结合视觉先验，采用强化学习与交叉熵方法训练策略；3）通过real2sim2real迁移方案，最终将学到的策略部署到各类真实机器人平台执行。整个流程与硬件无关，可拓展至多平台和多任务场景。

Result: RoboSeek在八种涉及序列交互、工具使用和物体操作的长期操控任务（跨多个机器人平台）上，平均成功率达到79%，远高于传统基线（低于50%），显示出出色的任务泛化能力和执行鲁棒性。实验结果证明所提框架在复杂动态的现实环境中的优越性和迁移机制的稳定性。

Conclusion: RoboSeek显著提升了具身机器人在复杂长期操控任务中的通用性与稳定性，其仿真－现实循环转移机制为实现更广泛的具身机器人学习奠定了基础。

Abstract: Optimizing and refining action execution through
  exploration and interaction is a promising way for robotic
  manipulation. However, practical approaches to interaction driven robotic
learning are still underexplored, particularly for
  long-horizon tasks where sequential decision-making, physical
  constraints, and perceptual uncertainties pose significant chal lenges.
Motivated by embodied cognition theory, we propose
  RoboSeek, a framework for embodied action execution that
  leverages interactive experience to accomplish manipulation
  tasks. RoboSeek optimizes prior knowledge from high-level
  perception models through closed-loop training in simulation
  and achieves robust real-world execution via a real2sim2real
  transfer pipeline. Specifically, we first replicate real-world
  environments in simulation using 3D reconstruction to provide
  visually and physically consistent environments., then we train
  policies in simulation using reinforcement learning and the
  cross-entropy method leveraging visual priors. The learned
  policies are subsequently deployed on real robotic platforms
  for execution. RoboSeek is hardware-agnostic and is evaluated
  on multiple robotic platforms across eight long-horizon ma nipulation tasks
involving sequential interactions, tool use, and
  object handling. Our approach achieves an average success rate
  of 79%, significantly outperforming baselines whose success
  rates remain below 50%, highlighting its generalization and
  robustness across tasks and platforms. Experimental results
  validate the effectiveness of our training framework in complex,
  dynamic real-world settings and demonstrate the stability of the
  proposed real2sim2real transfer mechanism, paving the way for
  more generalizable embodied robotic learning. Project Page:
  https://russderrick.github.io/Roboseek/

</details>


### [415] [Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation](https://arxiv.org/abs/2509.17812)
*Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: 本文提出了Tac2Motion框架，实现了基于触觉感知的强化学习方法，用于复杂的手内操作任务，比如开盖，获得了更高的数据效率和更好的泛化能力，并在多指机器人上做了实机验证。


<details>
  <summary>Details</summary>
Motivation: 当前手内操作尤其是涉及多个接触点的操作任务（如开盖）依赖于精确的感知和控制。传统方法在数据效率和泛化能力上存在不足，缺乏对接触信息的有效利用，因此需要一种能有效利用触觉信息的强化学习框架。

Method: 提出了Tac2Motion框架：1）将基于触觉的奖励函数引入强化学习过程中，使奖励与抓持稳定性和手指运动平滑性相关；2）将触觉传感器的观测结果，经嵌入处理后纳入强化学习观测空间。该方法在模拟中进行训练，并迁移至实际多指机器人平台实现。

Result: 在模拟环境下的开盖任务中，Tac2Motion相比基线方法数据效率和鲁棒性均有提升。并展示了所学策略可泛化到不同的物体和动力学场景（如不同的摩擦力）。最终在Shadow多指机器人平台上完成了实物操作测试，验证了策略的现实可迁移性。

Conclusion: Tac2Motion显著提升了复杂手内操作任务中的数据利用效率和泛化能力，并通过有效融合触觉信息实现了从仿真到现实的策略迁移。该方法对推进多指机器人操作能力具有实际意义。

Abstract: This paper proposes Tac2Motion, a contact-aware reinforcement learning
framework to facilitate the learning of contact-rich in-hand manipulation
tasks, such as removing a lid. To this end, we propose tactile sensing-based
reward shaping and incorporate the sensing into the observation space through
embedding. The designed rewards encourage an agent to ensure firm grasping and
smooth finger gaiting at the same time, leading to higher data efficiency and
robust performance compared to the baseline. We verify the proposed framework
on the opening a lid scenario, showing generalization of the trained policy
into a couple of object types and various dynamics such as torsional friction.
Lastly, the learned policy is demonstrated on the multi-fingered robot, Shadow
Robot, showing that the control policy can be transferred to the real world.
The video is available: https://youtu.be/poeJBPR7urQ.

</details>


### [416] [SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model](https://arxiv.org/abs/2509.17850)
*Xiao Zhou,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种名为SocialTraj的轨迹预测框架，以提高自动驾驶系统对周围车辆轨迹的预测精度，尤其在动态复杂场景下。该方法融合了社会心理学的社会价值取向（SVO），并结合贝叶斯逆向强化学习和条件扩散模型实现多模态轨迹生成，并通过大量实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶场景中，准确预测周围车辆未来轨迹对于安全决策至关重要。当前主流方法难以有效刻画驾驶者多样化行为模式，导致轨迹预测偏差，特别是在高动态和复杂交通场景下。

Method: 作者提出SocialTraj框架，首先通过贝叶斯逆向强化学习估算周围车辆的社会价值取向（SVO），获得关键社会交互上下文信息。然后将SVO嵌入条件去噪扩散模型中，实现与个体历史驾驶风格一致的轨迹生成。同时，将本车未来计划轨迹显式纳入建模以增强交互效果。

Result: 在NGSIM和HighD数据集上的实验表明，SocialTraj能够适应复杂交互场景，预测的轨迹更具社会合规性和行为一致性，性能超越现有基线模型。消融实验显示，动态SVO估算和明确的自车规划能提升精度并显著降低推理时间。

Conclusion: SocialTraj通过引入社会心理学因素和改进的轨迹生成模型，有效提升了自动驾驶环境下多模态、行为一致的轨迹预测能力，为安全智能决策提供了支持。

Abstract: Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for
autonomous driving systems to avoid misguided decisions and potential
accidents. However, achieving reliable predictions in highly dynamic and
complex traffic scenarios remains a significant challenge. One of the key
impediments lies in the limited effectiveness of current approaches to capture
the multi-modal behaviors of drivers, which leads to predicted trajectories
that deviate from actual future motions. To address this issue, we propose
SocialTraj, a novel trajectory prediction framework integrating social
psychology principles through social value orientation (SVO). By utilizing
Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we
obtain the critical social context to infer the future interaction trend. To
ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are
embedded into a conditional denoising diffusion model that aligns generated
trajectories with historical driving styles. Additionally, the planned future
trajectory of the ego vehicle (EV) is explicitly incorporated to enhance
interaction modeling. Extensive experiments on NGSIM and HighD datasets
demonstrate that SocialTraj is capable of adapting to highly dynamic and
interactive scenarios while generating socially compliant and behaviorally
consistent trajectory predictions, outperforming existing baselines. Ablation
studies demonstrate that dynamic SVO estimation and explicit ego-planning
components notably improve prediction accuracy and substantially reduce
inference time.

</details>


### [417] [Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection](https://arxiv.org/abs/2509.17877)
*Richard Kuhlmann,Jakob Wolfram,Boyang Sun,Jiaxu Xing,Davide Scaramuzza,Marc Pollefeys,Cesar Cadena*

Main category: cs.RO

TL;DR: 本文提出一种关注目标可见性的机器人自主巡检方法，通过端到端强化学习，实现更高效的检测任务。


<details>
  <summary>Details</summary>
Motivation: 传统的巡检通常简化为导航任务，只要求机器人到达预定位置并避障，忽略了实际任务中只需机器人处于能观测目标的视角即可。以此为背景，作者希望提升巡检效率，减少不必要的移动和耗时。

Method: 提出了一个端到端强化学习框架，将目标可见性作为主要目标，通过结合感知和自身状态信息，无需地图导航，训练机器人走出最短且保证视觉接触的轨迹。整个策略训练在仿真环境中完成，并迁移到真实机器人。还开发了计算地面真实最短巡检路径的算法，便于评估。

Result: 大量实验表明，该方法在仿真和真实环境中均优于传统和已有学习导航方案，产生更高效的巡检路径。

Conclusion: 综合实验结果验证了以目标可见性为核心目标进行巡检的有效性，展现了无地图强化学习方案在实际场景的应用潜力。

Abstract: Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/

</details>


### [418] [The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control](https://arxiv.org/abs/2509.17884)
*Arun L. Bishop,Juan Alvarez-Padilla,Sam Schoedel,Ibrahima Sory Sow,Juee Chandrachud,Sheitej Sharma,Will Kraus,Beomyeong Park,Robert J. Griffin,John M. Dolan,Zachary Manchester*

Main category: cs.RO

TL;DR: 本论文提出了一种基于线性时不变近似的全身模型预测控制器，在复杂的多足机器人上无需在线非线性动力学计算即可完成基本行走等运动任务。


<details>
  <summary>Details</summary>
Motivation: 传统的全身模型预测控制通常需要处理复杂的非线性动力学，这增加了计算负担和实现难度。研究动机在于探索是否可以利用更简化的动力学近似，降低控制复杂度而不损失性能。

Method: 采用了全身线性时不变动力学近似，制定模型预测控制器，无需实时求解非线性动力学或进行矩阵求逆。实验在四足机器人和液压人形机器人上进行，包括行走、干扰抑制和无需脚步规划的目标导航。

Result: 在线性时不变近似下，控制器实现了各种动态运动任务，包括在四足和复杂人形机器人上的稳定行走和干扰抑制，且无需传统脚步规划器或复杂的动力学建模。

Conclusion: 即使对于动力学结构和执行器复杂的机器人，采用简单的线性时不变近似也能实现高效鲁棒的基本运动控制，从而减少非线性因素对控制算法实时性和工程实现的影响。

Abstract: When do locomotion controllers require reasoning about nonlinearities? In
this work, we show that a whole-body model-predictive controller using a simple
linear time-invariant approximation of the whole-body dynamics is able to
execute basic locomotion tasks on complex legged robots. The formulation
requires no online nonlinear dynamics evaluations or matrix inversions. We
demonstrate walking, disturbance rejection, and even navigation to a goal
position without a separate footstep planner on a quadrupedal robot. In
addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with
significant limb inertia, complex actuator dynamics, and large sim-to-real gap.

</details>


### [419] [DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](https://arxiv.org/abs/2509.17940)
*Shuyao Shang,Yuntao Chen,Yuqi Wang,Yingyan Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: DriveDPO提出了一种结合人类模仿和规则安全的端到端自动驾驶策略优化方法，实现了新的安全与可靠性SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶多采用模仿学习，易生成看似人类但实际不安全的轨迹，安全性受限。部分新方法通过多重规则驱动，但监督和策略优化分离，性能不佳。

Method: 提出DriveDPO框架：(1)结合人类模仿相似性与规则安全分数，提炼出统一的策略分布，直接用于策略优化；(2)引入基于轨迹偏好对齐的Direct Preference Optimization，迭代增强安全偏好的策略。

Result: 在NAVSIM基准上DriveDPO获得90.0的PDMS新SOTA，对比实验和多场景定性分析显示其能生成更安全、可靠的驾驶行为。

Conclusion: DriveDPO提升了端到端自动驾驶的安全与可靠性，为安全敏感场景下的行为规划提供了有效、先进的方法。

Abstract: End-to-end autonomous driving has substantially progressed by directly
predicting future trajectories from raw perception inputs, which bypasses
traditional modular pipelines. However, mainstream methods trained via
imitation learning suffer from critical safety limitations, as they fail to
distinguish between trajectories that appear human-like but are potentially
unsafe. Some recent approaches attempt to address this by regressing multiple
rule-driven scores but decoupling supervision from policy optimization,
resulting in suboptimal performance. To tackle these challenges, we propose
DriveDPO, a Safety Direct Preference Optimization Policy Learning framework.
First, we distill a unified policy distribution from human imitation similarity
and rule-based safety scores for direct policy optimization. Further, we
introduce an iterative Direct Preference Optimization stage formulated as
trajectory-level preference alignment. Extensive experiments on the NAVSIM
benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of
90.0. Furthermore, qualitative results across diverse challenging scenarios
highlight DriveDPO's ability to produce safer and more reliable driving
behaviors.

</details>


### [420] [ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion](https://arxiv.org/abs/2509.17941)
*Zichao Hu,Chen Tang,Michael J. Munje,Yifeng Zhu,Alex Liu,Shuijing Liu,Garrett Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: 該論文提出了一種名為ComposableNav的新方法，使機器人在動態環境中能靈活按照複雜指令組合導航。其關鍵在於將指令拆解為多個運動規範並可組合，顯著優於傳統方法。


<details>
  <summary>Details</summary>
Motivation: 隨著機器人技能增多，單一指令可能包含多個運動規範，這導致規範組合呈指數增長，給機器人按照多規範複合指令行動帶來巨大挑戰。傳統方法難以靈活適應這種高複雜度需求。

Method: 作者提出ComposableNav，利用擴散模型（diffusion models）分別學習每個運動原語，部署時將這些原語並行組合，從而滿足多規範組合指令。訓練過程分為兩階段：第一階段進行有監督學習獲得基礎擴散模型，第二階段則用強化學習將基礎模型細化成不同運動原語，無須為每個原語單獨蒐集示範。

Result: 通過模擬與實驗，ComposableNav能生成符合各種全新複合規範（即訓練時未見過的指令組合）的導航軌跡，效果遠勝VLN等非組合基線方法和傳統costmap方法。

Conclusion: ComposableNav實現了機器人在動態環境下靈活響應並完成複雜組合指令的導航，具有較強泛化能力和應用前景，尤其在規範組合激增的現實場景下優勢明顯。

Abstract: This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/

</details>


### [421] [Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins](https://arxiv.org/abs/2509.17952)
*Mahdi Nobar,Jürg Keller,Alessandro Forino,John Lygeros,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种融合校正数字孪生与真实数据的多保真贝叶斯优化框架，实现了高效的数据驱动控制器调优。通过自适应代价感知采集策略，动态平衡了探索改进、模型保真度和采样代价，在多个实验中均超过了传统与现有多保真方法。


<details>
  <summary>Details</summary>
Motivation: 现实中控制器调优常依赖高保真的仿真或真实设备实验，两者成本或精度各有限；数字孪生模型虽能高效模拟，但存在模型偏差，如何融合不同保真度资源进行高效调优，是提升数据利用效率与工程可操作性的关键难题。

Method: 建立一个多保真代理模型，利用学习到的纠正模型修正数字孪生的预估，并与真实观测结合。提出自适应代价感知采集函数，综合考虑期望改进、保真度和采样成本，随着新观测动态更新代理相关性与采集策略，使高准确性数字孪生优先使用，低准确性的权重降低。

Result: 在机器人驱动等实际硬件与数值实验中，此方法较标准贝叶斯优化及其它多保真方法，明显提升了调优效率与效果。

Conclusion: 融合校正数字孪生和真实数据的多保真贝叶斯优化框架，可实现高效且适应性强的控制器调优，为未来复杂系统的自适应优化提供了新的工具和思路。

Abstract: We propose a \textit{guided multi-fidelity Bayesian optimization} framework
for data-efficient controller tuning that integrates corrected digital twin
(DT) simulations with real-world measurements. The method targets closed-loop
systems with limited-fidelity simulations or inexpensive approximations. To
address model mismatch, we build a multi-fidelity surrogate with a learned
correction model that refines DT estimates from real data. An adaptive
cost-aware acquisition function balances expected improvement, fidelity, and
sampling cost. Our method ensures adaptability as new measurements arrive. The
accuracy of DTs is re-estimated, dynamically adapting both cross-source
correlations and the acquisition function. This ensures that accurate DTs are
used more frequently, while inaccurate DTs are appropriately downweighted.
Experiments on robotic drive hardware and supporting numerical studies
demonstrate that our method enhances tuning efficiency compared to standard
Bayesian optimization (BO) and multi-fidelity methods.

</details>


### [422] [M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer](https://arxiv.org/abs/2509.18005)
*Yanxin Zhang,Liang He,Zeyi Kang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新型轻量级多模态学习模型M3ET，提升了在移动与资源受限环境下的推理速度，为机器人视觉与人类行为理解带来了高效、低资源消耗的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法对文本信息的利用不充分，严重依赖于有监督预训练模型，难以适应无监督和模态丢失的情况，同时计算资源消耗过高，限制了在真实机器人环境中的应用。

Method: 提出了一种名为M3ET的多模态增强变换器，通过集成Mamba模块和基于语义的自适应注意力机制，实现高效的特征融合、对齐和模态重构，专为移动和资源受限平台优化。

Result: 实验表明，M3ET在多任务跨域性能上有显著提升，预训练推理速度提升2.3倍，VQA核心任务的准确率达到0.74，同时模型参数量减少67%。但在EQA任务表现有限。

Conclusion: M3ET兼顾了性能和资源消耗，尽管在部分任务上表现有限，但其轻量化设计非常适合部署于资源受限的机器人平台，推动多模态学习实际应用。

Abstract: In recent years, multimodal learning has become essential in robotic vision
and information fusion, especially for understanding human behavior in complex
environments. However, current methods struggle to fully leverage the textual
modality, relying on supervised pretrained models, which limits semantic
extraction in unsupervised robotic environments, particularly with significant
modality loss. These methods also tend to be computationally intensive, leading
to high resource consumption in real-world applications. To address these
challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a
lightweight model designed for efficient multimodal learning, particularly on
mobile platforms. By incorporating the Mamba module and a semantic-based
adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and
modality reconstruction. Our experiments show that M3ET improves cross-task
performance, with a 2.3 times increase in pretraining inference speed. In
particular, the core VQA task accuracy of M3ET remains at 0.74, while the
model's parameter count is reduced by 0.67. Although performance on the EQA
task is limited, M3ET's lightweight design makes it well suited for deployment
on resource-constrained robotic platforms.

</details>


### [423] [Prepare Before You Act: Learning From Humans to Rearrange Initial States](https://arxiv.org/abs/2509.18043)
*Yinlong Dai,Andre Keyser,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文提出ReSET算法，使机器人在执行特定任务前，能先自主调整环境，将环境结构变得更接近训练时的分布，从而提升模仿学习策略在异常初始状态下的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法在遇到未见过的初始状态（如目标物体位置新奇或被遮挡）时表现不佳，需要大量演示才能获得稳健表现。人类在实际操作中，常常会优先调整环境，再进行任务操作。受此启发，研究者希望赋予机器人类似能力。

Method: 提出ReSET算法，分两步操作：首先识别并调整异常环境，将其转化为类似训练数据的结构；之后再执行已有模仿策略以完成任务。方法整合了无动作标签的人类视频和与任务无关的遥操作数据，通过模型决定是否调整环境、预测简化动作，并将结果转化为机器人可执行的基本动作。

Result: 实验证明，ReSET能更好地应对异常初始环境，相较扩散策略、视觉语言模型等多种基线方法，仅用同等训练数据，实现了更强的任务执行鲁棒性和泛化性。

Conclusion: 赋予机器人自主整理环境的能力，可有效提升其模仿学习任务中的表现，特别是在面对分布外输入时，减少泛化性能差距。

Abstract: Imitation learning (IL) has proven effective across a wide range of
manipulation tasks. However, IL policies often struggle when faced with
out-of-distribution observations; for instance, when the target object is in a
previously unseen position or occluded by other objects. In these cases,
extensive demonstrations are needed for current IL methods to reach robust and
generalizable behaviors. But when humans are faced with these sorts of atypical
initial states, we often rearrange the environment for more favorable task
execution. For example, a person might rotate a coffee cup so that it is easier
to grasp the handle, or push a box out of the way so they can directly grasp
their target object. In this work we seek to equip robot learners with the same
capability: enabling robots to prepare the environment before executing their
given policy. We propose ReSET, an algorithm that takes initial states -- which
are outside the policy's distribution -- and autonomously modifies object poses
so that the restructured scene is similar to training data. Theoretically, we
show that this two step process (rearranging the environment before rolling out
the given policy) reduces the generalization gap. Practically, our ReSET
algorithm combines action-agnostic human videos with task-agnostic
teleoperation data to i) decide when to modify the scene, ii) predict what
simplifying actions a human would take, and iii) map those predictions into
robot action primitives. Comparisons with diffusion policies, VLAs, and other
baselines show that using ReSET to prepare the environment enables more robust
task execution with equal amounts of total training data. See videos at our
project website: https://reset2025paper.github.io/

</details>


### [424] [HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2509.18046)
*Yinuo Wang,Yuanyang Qi,Jinzhao Zhou,Gavin Tao*

Main category: cs.RO

TL;DR: 本文提出了一种面向全身机器人行走的端到端强化学习方法HuMam，采用Mamba编码器进行高效特征融合，相比传统方法，在学习效率、稳定性与能耗等方面均表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有端到端强化学习在类人机器人步态控制中存在训练不稳定、特征融合低效和高能耗等问题，限制了实际应用。急需一种高效稳定的特征融合及控制方法。

Method: 提出HuMam框架，基于单层Mamba编码器融合机器人状态、步伐目标和连续相位时钟，策略网络输出关节目标位置，由低级PD环跟踪；设计六项奖励函数平衡稳定性、平滑性、节能等，并用PPO优化。

Result: 在mc-mujoco的JVRC-1类人机器人上，HuMam在学习效率、训练稳定性、整体任务表现上均优于传统前馈方法，且减少能耗与峰值力矩，实现性能、稳定性与经济性的统一提升。

Conclusion: HuMam是首个采Mamba融合主干的类人端到端RL控制器，有效提升控制性能、节能与训练稳定性。方法为端到端类人机器人控制器设计提供了新的思路和技术方案。

Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing
for its compact perception-action mapping, yet practical policies often suffer
from training instability, inefficient feature fusion, and high actuation cost.
We present HuMam, a state-centric end-to-end RL framework that employs a
single-layer Mamba encoder to fuse robot-centric states with oriented footstep
targets and a continuous phase clock. The policy outputs joint position targets
tracked by a low-level PD loop and is optimized with PPO. A concise six-term
reward balances contact quality, swing smoothness, foot placement, posture, and
body stability while implicitly promoting energy saving. On the JVRC-1 humanoid
in mc-mujoco, HuMam consistently improves learning efficiency, training
stability, and overall task performance over a strong feedforward baseline,
while reducing power consumption and torque peaks. To our knowledge, this is
the first end-to-end humanoid RL controller that adopts Mamba as the fusion
backbone, demonstrating tangible gains in efficiency, stability, and control
economy.

</details>


### [425] [V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts](https://arxiv.org/abs/2509.18053)
*Hsu-kuang Chiu,Ryo Hachiuma,Chien-Yi Wang,Yu-Chiang Frank Wang,Min-Hung Chen,Stephen F. Smith*

Main category: cs.RO

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）的合作式自动驾驶“思维图”框架，提升了车辆在视觉遮挡等复杂环境下的感知、预测和规划能力，实验表明方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶在传感器被大型物体遮挡时安全性存在隐患，V2V合作虽能缓解但现有方法未充分利用MLLM的图式推理潜力。

Method: 提出一种面向MLLM的“思维图”框架，包含感知遮挡感知、规划感知预测等新想法；构建V2V-GoT-QA数据集，开发相应V2V-GoT模型进行训练和测试。

Result: 在合作感知、预测与规划任务上，所提方法的性能优于其他基线方法。

Conclusion: 思维图推理方法显著提升了合作式自动驾驶系统在复杂道路环境中的综合决策能力。

Abstract: Current state-of-the-art autonomous vehicles could face safety-critical
situations when their local sensors are occluded by large nearby objects on the
road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed
as a means of addressing this problem, and one recently introduced framework
for cooperative autonomous driving has further adopted an approach that
incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative
perception and planning processes. However, despite the potential benefit of
applying graph-of-thoughts reasoning to the MLLM, this idea has not been
considered by previous cooperative autonomous driving research. In this paper,
we propose a novel graph-of-thoughts framework specifically designed for
MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our
proposed novel ideas of occlusion-aware perception and planning-aware
prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for
training and testing the cooperative driving graph-of-thoughts. Our
experimental results show that our method outperforms other baselines in
cooperative perception, prediction, and planning tasks.

</details>


### [426] [RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds](https://arxiv.org/abs/2509.18068)
*Bin Zhao,Nakul Garg*

Main category: cs.RO

TL;DR: 本文提出了一种无需合成孔径雷达和多帧融合的毫米波雷达点云重建方法RadarSFD，仅用一帧数据即可生成高密度类似LiDAR的点云，在端侧机器视觉场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波雷达成像方法依赖合成孔径（SAR）或多帧叠加来提升分辨率，这对于体积、重量和功耗受限的小型机器人平台（如无人机、穿戴设备等）并不现实，因此需要能在单帧数据下有效提升点云质量的方法。

Method: 提出了RadarSFD方法，基于条件潜变量扩散模型，并引入单目深度估计算法的几何先验到扩散模型，通过通道级潜变量拼接与雷达输入结合，同时通过包含潜空间和像素空间损失的双重目标正则化提升输出质量。

Result: 在RadarHD基准测试上，RadarSFD取得了35cm Chamfer距离、28cm修改Hausdorff距离，远优于单帧RadarHD基线（分别为56cm和45cm），也与使用5到41帧的多帧方法性能相当。

Conclusion: RadarSFD是首个实用的单帧、无需SAR的毫米波雷达高密度点云感知管线，特别适合轻量化机器人系统，具备良好的泛化能力和精细结构恢复效果。

Abstract: Millimeter-wave radar provides perception robust to fog, smoke, dust, and low
light, making it attractive for size, weight, and power constrained robotic
platforms. Current radar imaging methods, however, rely on synthetic aperture
or multi-frame aggregation to improve resolution, which is impractical for
small aerial, inspection, or wearable systems. We present RadarSFD, a
conditional latent diffusion framework that reconstructs dense LiDAR-like point
clouds from a single radar frame without motion or SAR. Our approach transfers
geometric priors from a pretrained monocular depth estimator into the diffusion
backbone, anchors them to radar inputs via channel-wise latent concatenation,
and regularizes outputs with a dual-space objective combining latent and
pixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer
Distance and 28 cm Modified Hausdorff Distance, improving over the single-frame
RadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame
methods using 5-41 frames. Qualitative results show recovery of fine walls and
narrow gaps, and experiments across new environments confirm strong
generalization. Ablation studies highlight the importance of pretrained
initialization, radar BEV conditioning, and the dual-space loss. Together,
these results establish the first practical single-frame, no-SAR mmWave radar
pipeline for dense point cloud perception in compact robotic systems.

</details>


### [427] [ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces](https://arxiv.org/abs/2509.18084)
*Jiawen Tian,Liqun Huang,Zhongren Cui,Jingchao Qiao,Jiafeng Xu,Xiao Ma,Zeyu Ren*

Main category: cs.RO

TL;DR: 该论文提出了一种新型高度柔性和类人化的并联机械手腕ByteWrist，针对狭窄空间操作中的传统机械手腕体积大、灵活性差的问题，提出了更紧凑且高效的设计，并在各种未结构化环境下验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有串联或并联型的机械手腕在狭窄空间复杂操作中，往往面临着体积过大、精确灵活控制难和结构强度不足等瓶颈，限制了服务、医疗、装配等场景下机器人的应用。因此，急需一种兼顾紧凑性、多自由度灵活操作及结构刚性的创新机械手腕。

Method: 设计了一种三级嵌套并联驱动结构，通过弧形末端连杆优化力传递和运动范围，引入中部球体支撑作为球形关节提升整体刚性。提出完整运动学建模方法，包括正逆运动学和数值Jacobian，用于精确控制。

Result: 实验证明ByteWrist在狭窄空间和双臂协作抓取任务中表现优异，机动性和操作灵活性显著优于现有Kinova系统。该设计在紧凑性、刚性和效率方面较传统机械手腕有明显提升。

Conclusion: ByteWrist有效解决了狭窄空间下高自由度机械手腕的体积与性能矛盾，是下一代机器人操作在受限环境下的有前景解决方案。

Abstract: This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic
parallel wrist for robotic manipulation. ByteWrist addresses the critical
limitations of existing serial and parallel wrists in narrow-space operations
through a compact three-stage parallel drive mechanism integrated with
arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)
motion while maintaining exceptional compactness, making it particularly
suitable for complex unstructured environments such as home services, medical
assistance, and precision assembly. The key innovations include: (1) a nested
three-stage motor-driven linkages that minimize volume while enabling
independent multi-DOF control, (2) arc-shaped end linkages that optimize force
transmission and expand motion range, and (3) a central supporting ball
functioning as a spherical joint that enhances structural stiffness without
compromising flexibility. Meanwhile, we present comprehensive kinematic
modeling including forward / inverse kinematics and a numerical Jacobian
solution for precise control. Empirically, we observe ByteWrist demonstrates
strong performance in narrow-space maneuverability and dual-arm cooperative
manipulation tasks, outperforming Kinova-based systems. Results indicate
significant improvements in compactness, efficiency, and stiffness compared to
traditional designs, establishing ByteWrist as a promising solution for
next-generation robotic manipulation in constrained environments.

</details>
