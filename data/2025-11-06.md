<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 42]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cropland Mapping using Geospatial Embeddings](https://arxiv.org/abs/2511.02923)
*Ivan Zvonkov,Gabriel Tseng,Inbal Becker-Reshef,Hannah Kerner*

Main category: cs.CV

TL;DR: 地理空间嵌入技术可简化农田分类流程，并显著提升制图精度，有助于评估土地利用变化及其气候影响。


<details>
  <summary>Details</summary>
Motivation: 目前对土地覆盖的准确制图对于理解土地利用变化和气候变化至关重要，但将地理空间嵌入用于现实世界地图制作的实际探索较少。该论文旨在填补地理空间嵌入应用于作物地制图领域的研究空白。

Method: 作者使用Presto和AlphaEarth生成的地理空间嵌入，对多哥地区进行作物地分类制图，并评估其制图效果。

Result: 实验结果表明，地理空间嵌入不仅能够简化制图流程，还能实现高精度的农田分类。

Conclusion: 地理空间嵌入为制图工作流程带来高效与准确，并有助于更好地评估土地利用变化及其对气候的影响。

Abstract: Accurate and up-to-date land cover maps are essential for understanding land
use change, a key driver of climate change. Geospatial embeddings offer a more
efficient and accessible way to map landscape features, yet their use in
real-world mapping applications remains underexplored. In this work, we
evaluated the utility of geospatial embeddings for cropland mapping in Togo. We
produced cropland maps using embeddings from Presto and AlphaEarth. Our
findings show that geospatial embeddings can simplify workflows, achieve
high-accuracy cropland classification and ultimately support better assessments
of land use change and its climate impacts.

</details>


### [2] [Generative Hints](https://arxiv.org/abs/2511.02933)
*Andy Dimnaku,Abdullah Yusuf Kavranoğlu,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 本文提出了一种全新的生成性提示（generative hints）方法，通过利用生成模型生成虚拟样本，直接在整个输入空间上强化模型已知的不变性（如空间不变性），在视觉领域超越了传统的数据增强技术。


<details>
  <summary>Details</summary>
Motivation: 传统的数据增强只能在训练数据的变换上学习不变性，不能覆盖整个输入空间。作者希望找到一种能在更广泛范围内直接强化特定不变性的训练方法。

Method: 作者提出了生成性提示方法：首先用生成模型基于训练集学习输入分布，然后生成未标记的虚拟样本，这些虚拟样本用于训练时联合监督分类目标和提示目标（即已知的不变性），即使所有训练数据都有标签，也采用半监督的方式使用虚拟样本指导模型学习。

Result: 在多种数据集、网络结构和损失函数上，生成性提示方法针对同一不变性学习相比标准数据增强有更佳表现。在细粒度视觉分类和医疗影像（CheXpert X-ray）数据集上，top-1准确率分别最高提升1.78%、平均提升0.63%，在CheXpert数据集上平均提升1.286%。

Conclusion: 生成性提示方法能更有效地将已知不变性注入到模型中，提升模型泛化能力，优于传统数据增强技术。

Abstract: Data augmentation is widely used in vision to introduce variation and
mitigate overfitting, through enabling models to learn invariant properties,
such as spatial invariance. However, these properties are not fully captured by
data augmentation alone, since it attempts to learn the property on
transformations of the training data only. We propose generative hints, a
training methodology that directly enforces known invariances in the entire
input space. Our approach leverages a generative model trained on the training
set to approximate the input distribution and generate unlabeled images, which
we refer to as virtual examples. These virtual examples are used to enforce
functional properties known as hints. In generative hints, although the
training dataset is fully labeled, the model is trained in a semi-supervised
manner on both the classification and hint objectives, using the unlabeled
virtual examples to guide the model in learning the desired hint. Across
datasets, architectures, and loss functions, generative hints consistently
outperform standard data augmentation when learning the same property. On
popular fine-grained visual classification benchmarks, we achieved up to 1.78%
top-1 accuracy improvement (0.63% on average) over fine-tuned models with data
augmentation and an average performance boost of 1.286% on the CheXpert X-ray
dataset.

</details>


### [3] [ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology](https://arxiv.org/abs/2511.02946)
*Srikumar Sastry,Subash Khanal,Aayush Dhakal,Jiayu Lin,Dan Cher,Phoenix Jarosz,Nathan Jacobs*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态嵌入模型ProM3E，可用于生态学领域的多模态任意互转与检索，表现出优越的检索与表示学习能力。


<details>
  <summary>Details</summary>
Motivation: 多模态生态学数据丰富，但不同模态间信息互补，缺乏灵活融合与互转的通用模型，限制了跨模态分析与应用效果。

Method: 提出基于概率掩码的多模态嵌入模型ProM3E，在嵌入空间通过重建缺失模态进行学习。模型具备概率性特征，能评估模态融合可行性，并实现嵌入空间中的模态反演。提出结合模态内与模态间相似性的创新型跨模态检索方法，同时用线性探查验证其表示学习能力。

Result: ProM3E在所有检索任务中均取得了优越性能，且其隐藏表示在下游线性探查中也表现出较强能力。

Conclusion: ProM3E实现了多模态数据间任意互转，有效提升了多模态检索与表示学习效果，为生态学多模态数据分析提供了新思路。

Abstract: We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.

</details>


### [4] [EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出并发布了EvtSlowTV这一大规模事件相机数据集，并验证其对基于事件信号的深度估计的促进作用。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机深度估计方法受限于小规模标注数据集，难以泛化到真实复杂场景，因此亟需大规模、自然环境下的事件数据集。

Method: 从YouTube等公开平台收集多种环境和运动（登山、飞行、驾驶、水下探险等）下的事件数据，构建超过130亿事件的大规模数据集EvtSlowTV，并在此基础上采用自监督学习框架进行模型训练。

Result: EvtSlowTV数据集的规模远超现有，且自然性优良。在自监督框架下，用EvtSlowTV训练的模型在复杂场景和动态中的泛化能力增强，无需帧级标注，保留事件数据的异步特性。

Conclusion: EvtSlowTV为基于事件的深度学习提供了更广泛、自然的数据基础，推动事件相机在实际复杂场景中的应用和发展。

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [5] [Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification](https://arxiv.org/abs/2511.02992)
*Mikhael Djajapermana,Moritz Reiber,Daniel Mueller-Gritschneder,Ulf Schlichtmann*

Main category: cs.CV

TL;DR: 本文提出了一种用于神经网络结构搜索（NAS）的混合CNN-ViT搜索空间，可自动发现适用于tinyML的高效混合架构。


<details>
  <summary>Details</summary>
Motivation: 混合型CNN和ViT虽然效果优异，但参数量大、计算成本高，难以部署于资源受限的tinyML场景，因此需要寻找更高效的混合架构。

Method: 提出了一种覆盖CNN、ViT及可搜索池化层的混合网络结构搜索空间，让NAS在局部与全局信息建模和特征图高效降维方面均可优化，从而自动发现合适的网络架构。

Result: 实验证明，在CIFAR10数据集上，所提出的搜索空间找到的混合CNN-ViT架构比传统ResNet基tinyML模型在模型规模受限条件下有更好的准确率和推理速度。

Conclusion: 新提出的混合CNN-ViT搜索空间能够为tinyML任务发现更高效且性能更优的模型架构，兼顾精度与推理速度。

Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT)
have outperformed pure CNN or ViT architecture. However, since these
architectures require large parameters and incur large computational costs,
they are unsuitable for tinyML deployment. This paper introduces a new hybrid
CNN-ViT search space for Neural Architecture Search (NAS) to find efficient
hybrid architectures for image classification. The search space covers hybrid
CNN and ViT blocks to learn local and global information, as well as the novel
Pooling block of searchable pooling layers for efficient feature map reduction.
Experimental results on the CIFAR10 dataset show that our proposed search space
can produce hybrid CNN-ViT architectures with superior accuracy and inference
speed to ResNet-based tinyML models under tight model size constraints.

</details>


### [6] [SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics](https://arxiv.org/abs/2511.02996)
*Ailar Mahdizadeh,Puria Azadi Moghadam,Xiangteng He,Shahriar Mirabbasi,Panos Nasiopoulos,Leonid Sigal*

Main category: cs.CV

TL;DR: SCALE-VLP提出了一种创新的视觉-语言预训练框架，结合体积空间语义和医学本体知识，显著提升了CT等三维医学数据的跨模态理解与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型主要聚焦在2D数据与二元监督，忽略了例如CT在内的体积数据的空间与结构特性，以及临床知识语义，从而影响了在医学影像的实际应用。作者希望突破2D切片处理带来的限制，充分利用体积空间信息和领域知识，以提升模型表现。

Method: 提出了SCALE-VLP框架，通过软加权对比学习，将体积的空间语义与领域知识（如放射学本体）注入视觉-语言预训练过程，从而获得结构一致、语义扎实的联合表示。该方法有效结合了解剖结构与知识引导对齐，适用于多种医学任务。

Result: SCALE-VLP在多个任务上表现优异，CT-报告检索top-1提升4.3倍，异常分类提升10分，报告生成任务ROUGE-L 0.44、BERT-F1 0.89。在跨域外部数据的零样本测试中也取得了持续性的提升，展示了强大的跨任务、跨域泛化能力。

Conclusion: SCALE-VLP能够充分利用体积语义和医疗知识，实现了结构化、语义化的医学影像理解。该方法不仅性能领先且泛化能力强，有望拓展医学影像AI的应用边界。

Abstract: Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.

</details>


### [7] [Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning](https://arxiv.org/abs/2511.03004)
*Dakota Hester,Vitor S. Martins,Lucas B. Ferreira,Thainara M. A. Lima*

Main category: cs.CV

TL;DR: 本文提出一种基于自监督学习的高分辨率（1米级）地表覆盖分类方法，仅需1,000个标注样本，实现了密西西比州范围内的八类地表覆盖自动制图，精度达到87.14%。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法能够实现高精度地表覆盖分类，但需要大量标注数据，获取困难且成本高，限制了其在大范围实际应用中的推广。本文旨在降低标注需求，提高实用性。

Method: 采用“Bootstrap Your Own Latent”自监督预训练方法，利用约38万张无标注航拍图像对ResNet-101编码器进行预训练。预训练权重迁移到多种主流语义分割网络（如U-Net等），最后用极少量有标注切片（250、500、750）进行微调和集成，进行8分类地图制作。

Result: 集成最佳U-Net模型，获得87.14%的总体精度和75.58%的宏F1分数，实现超过1230亿像素、覆盖密西西比州的1米级地表制图。林地和水体识别准确，耕地、草地与裸地类型区分仍存一定挑战。

Conclusion: 自监督学习能显著减少高分辨率地表覆盖制图对标注数据的依赖，为大范围、细粒度地表分类提供现实可行的解决方案。

Abstract: Deep learning semantic segmentation methods have shown promising performance
for very high 1-m resolution land cover classification, but the challenge of
collecting large volumes of representative training data creates a significant
barrier to widespread adoption of such models for meter-scale land cover
mapping over large areas. In this study, we present a novel label-efficient
approach for statewide 1-m land cover classification using only 1,000 annotated
reference image patches with self-supervised deep learning. We use the
"Bootstrap Your Own Latent" pre-training strategy with a large amount of
unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to
pre-train a ResNet-101 convolutional encoder. The learned encoder weights were
subsequently transferred into multiple deep semantic segmentation architectures
(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then
fine-tuned using very small training dataset sizes with cross-validation (250,
500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall
accuracy and 75.58% macro F1 score using an ensemble of the best performing
U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more
than 123 billion pixels over the state of Mississippi, USA. Detailed
qualitative and quantitative analysis revealed accurate mapping of open water
and forested areas, while highlighting challenges in accurate delineation
between cropland, herbaceous, and barren land cover types. These results show
that self-supervised learning is an effective strategy for reducing the need
for large volumes of manually annotated data, directly addressing a major
limitation to high spatial resolution land cover mapping at scale.

</details>


### [8] [A Foundation Model for Brain MRI with Dynamic Modality Integration](https://arxiv.org/abs/2511.03014)
*Minh Sao Khue Luu,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: 本文提出了一个适用于多种MRI成像序列输入的脑部MRI基础模型，实现了不同模态兼容、缺失模态自适应，具备较强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的脑部MRI分析模型通常需针对不同成像模态分别训练，且在缺失或未知模态下性能大幅下降，实际应用受限。该研究旨在解决上述问题，开发出能兼容多种成像序列并应对模态缺失的统一模型。

Method: 方法上，提出了包含可学习模态嵌入、条件层归一化和掩码自编码损失的新型编码器，并引入方差-协方差正则项以提升特征稳定性和多样性。模型以大规模多中心数据进行自监督学习，实现自适应模态重建和补全功能。

Result: 初步实验结果表明，所提方法在处理脑肿瘤与多发性硬化分割以及病变分类等任务中能够胜任多种模态组合，并兼容模态缺失情境。

Conclusion: 该模型无需为每种模态单独训练，可适应部分或全部模态缺失，具备较好泛化和实用性。后续将进行更深入的评估和优化，所有代码及模型已开源。

Abstract: We present a foundation model for brain MRI that can work with different
combinations of imaging sequences. The model uses one encoder with learnable
modality embeddings, conditional layer normalization, and a masked autoencoding
objective that accounts for missing modalities. A variance-covariance
regularizer is applied to stabilize feature learning and improve representation
diversity. This design removes the need for separate models for each modality
and allows the network to adapt when some sequences are missing or unseen. It
is trained on about 60,000 multi-center MRIs using self-supervised
reconstruction and modality imputation to learn flexible representations. A
learnable modality embedding guides feature extraction so the encoder can
adjust to different inputs. We describe our planned evaluation on brain tumor
and multiple sclerosis segmentation, as well as lesion classification, under
various modality settings. Preliminary results show that the method works
feasibly, and further experiments are planned to study its performance in more
detail. All code and pretrained models are available at
https://github.com/BrainFM/brainfm

</details>


### [9] [SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment](https://arxiv.org/abs/2511.03019)
*Wenbo Lu*

Main category: cs.CV

TL;DR: 提出一种结构感知的视觉-语言预训练方法（SLIP），通过引入结构对比损失，利用实体之间的关系来提升跨模态检索和分类的性能，并在新的大规模多模态图数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言预训练主要依赖于大规模的图文配对数据，但忽略了图文间更丰富的关系结构（比如商品共购、社交关系等），且人类认知知识时往往借助关系认知图谱。因此，提升多模态预训练模型的结构建模能力具重要意义。

Method: 提出SLIP方法，在常规的对比损失外，新增结构对比损失，显式建模实体邻接关系，并在大规模Amazon商品共购多模态图数据集上实现结构化跨模态监督训练。

Result: 在零样本和小样本的跨模态检索与分类任务中，SLIP均明显优于CLIP等主流方法，展示了结构化监督在跨模态对齐中的价值。

Conclusion: 通过引入和建模关系结构，视觉-语言预训练模型可以更好地进行跨模态对齐，结构感知的训练范式为相关任务带来显著提升。

Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.

</details>


### [10] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 本文提出了一种基于学习的方法来评估移动激光扫描（MLS）点云的不确定性，绕开了以往依赖昂贵且难以获取的真值数据。实验表明，通过几何特征提取和机器学习模型可有效预测点级不确定性。


<details>
  <summary>Details</summary>
Motivation: 在许多对精度要求极高的应用（如Scan-to-BIM、变形分析和三维建模）中，评估MLS点云的不确定性至关重要。但获取真实的标定数据成本高昂、实际应用中难以实现，因此有必要寻找无需真值的新评估方法。

Method: 提出了一种集成最优邻域估计与几何特征提取的学习型框架，利用XGBoost和Random Forest等机器学习模型，基于真实数据集进行实验验证。

Result: 框架在真实数据集实验中取得了与Random Forest相当的准确率，但XGBoost效率提升显著（快约3倍）。证明了基于几何特征能有效预测点级的不确定性。

Conclusion: MLS点云不确定性具备可学习性，本文开创性地提出基于学习方法进行不确定性评估，为相关研究提供了新的视角。

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


### [11] [A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction](https://arxiv.org/abs/2511.03093)
*Yi Gong,Xinyuan Zhang,Jichen Chai,Yichen Ding,Yifei Lou*

Main category: cs.CV

TL;DR: 提出了一种结合压缩感知与光片显微镜的新型高效计算成像框架，用于高效、低光毒的心脏成像，并在斑马鱼心脏实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统光学成像在捕捉快速跳动心脏的动态细胞结构时，空间分辨率和时间分辨率难以兼顾，成像效率低且易产生光毒性，难以满足生物医学成像需求。

Method: 方法综合了压缩感知与光片显微镜，利用数字微镜器件随机二值编码荧光信号完成压缩采集。图像重建采用Plug-and-Play框架，在ADMM算法基础上集成Tikhonov、全变分（TV）和BM3D等高级去噪方法，并通过时序正则化保持相邻z切片之间的结构连续性。

Result: 在斑马鱼心脏高压缩成像实验中，所提方法在大幅压缩数据量的情况下，实现了出色的去噪和图像清晰度，能够成功重建出细胞结构。

Conclusion: 该方法在高速低光生物成像中表现出高效性和鲁棒性，显著提升了动态心脏成像的空间和时间分辨率，并降低了光毒性，为高动态生物组织成像技术提供了有力工具。

Abstract: Cardiac contraction is a rapid, coordinated process that unfolds across
three-dimensional tissue on millisecond timescales. Traditional optical imaging
is often inadequate for capturing dynamic cellular structure in the beating
heart because of a fundamental trade-off between spatial and temporal
resolution. To overcome these limitations, we propose a high-performance
computational imaging framework that integrates Compressive Sensing (CS) with
Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The
system performs compressed acquisition of fluorescence signals via random
binary mask coding using a Digital Micromirror Device (DMD). We propose a
Plug-and-Play (PnP) framework, solved using the alternating direction method of
multipliers (ADMM), which flexibly incorporates advanced denoisers, including
Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in
dynamic imaging, we further introduce temporal regularization enforcing
smoothness between adjacent z-slices. Experimental results on zebrafish heart
imaging under high compression ratios demonstrate that the proposed method
successfully reconstructs cellular structures with excellent denoising
performance and image clarity, validating the effectiveness and robustness of
our algorithm in real-world high-speed, low-light biological imaging scenarios.

</details>


### [12] [ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly](https://arxiv.org/abs/2511.03098)
*Miftahur Rahman,Samuel Adebayo,Dorian A. Acevedo-Mejia,David Hester,Daniel McPolin,Karen Rafferty,Debra F. Laefer*

Main category: cs.CV

TL;DR: 本文介绍了ISC-Perception，这是首个专为钢连接（ISC）组件检测构建的混合型数据集，结合了合成、游戏引擎和真实照片数据，大幅减少人工标注工作量，并显著提升了检测模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对建筑机器人用于ISC组件识别的数据集，采集真实工地照片存在实际和隐私难题，制约了相关机器感知系统的发展。

Method: 作者提出了一种混合数据集制作方法，将自动生成的CAD图像、游戏引擎渲染的高真实感场景与少量人工采集真实照片结合，实现了大规模合成部分的自动标注。整个数据集的制作过程严格统计了人力投入，显著减少了人工作业量。

Result: 使用ISC-Perception数据集训练的检测器在IoU 0.50下获得了0.756的平均精度，远超仅用合成或仅用高真实感数据训练的模型。在1200帧的测试集上，mAP@0.50为0.943，mAP@[0.50:0.95]为0.823。人工标注1万张图片需166.7小时，自动流程则仅需30.5小时，节省了81.7%的人力。

Conclusion: ISC-Perception填补了建筑机器人感知领域数据集空白，大幅提升了目标检测的开发效率和效果。该数据集对学术和工业界均开放，对相关领域研究有较强促进作用。

Abstract: The Intermeshed Steel Connection (ISC) system, when paired with robotic
manipulators, can accelerate steel-frame assembly and improve worker safety by
eliminating manual assembly. Dependable perception is one of the initial stages
for ISC-aware robots. However, this is hampered by the absence of a dedicated
image corpus, as collecting photographs on active construction sites is
logistically difficult and raises safety and privacy concerns. In response, we
introduce ISC-Perception, the first hybrid dataset expressly designed for ISC
component detection. It blends procedurally rendered CAD images, game-engine
photorealistic scenes, and a limited, curated set of real photographs, enabling
fully automatic labelling of the synthetic portion. We explicitly account for
all human effort to produce the dataset, including simulation engine and scene
setup, asset preparation, post-processing scripts and quality checks; our total
human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for
manual labelling at 60,s per image (-81.7%). A manual pilot on a representative
image with five instances of ISC members took 60,s (maximum 80,s), anchoring
the manual baseline. Detectors trained on ISC-Perception achieved a mean
Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained
on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we
report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for
construction-robotics perception, ISC-Perception facilitates rapid development
of custom object detectors and is freely available for research and industrial
use upon request.

</details>


### [13] [DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs](https://arxiv.org/abs/2511.03099)
*Yiyi Miao,Taoyu Wu,Tong Chen,Sihao Li,Ji Jiang,Youpeng Yang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 本文提出了一种名为DentalSplat的新框架，实现了在极稀疏正畸口内照片条件下的高质量3D重建与新视角合成，有效提升了远程牙科诊疗的可行性与效果。


<details>
  <summary>Details</summary>
Motivation: 正畸远程医疗常常只有三张口内照片（前、左、右），多视角三维重建任务极为困难，现有3DGS方法依赖密集多视角影像和高精度相机参数，不适用于该场景。因此亟需突破性方法提升稀疏图像下的三维重建质量。

Method: 作者提出DentalSplat框架，首先利用先验引导的稠密立体重建模型初始化点云，并通过自适应剪枝策略提升高斯点的训练效率和重建质量。在极稀疏视角下，进一步引入光流几何约束和梯度正则化增强渲染质量。方法在大规模临床数据和模拟远程拍摄数据集上进行评估验证。

Result: 在950个案例的临床大数据集和195个远程视频模拟数据集上测试，DentalSplat在极稀疏输入下重建效果和新视角可视化均显著优于现有SOTA方法。

Conclusion: 本文方法能有效解决远程正畸稀疏影像三维重建难题，对于提升在线正畸诊疗和可视化有重要意义，展现了广阔的实际应用前景。

Abstract: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.

</details>


### [14] [Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning](https://arxiv.org/abs/2511.03120)
*Botong. Zhao,Xubin. Wang,Shujing. Lyu,Yue. Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像内在先验的无监督集成电路（IC）缺陷检测和新类别发现方法，不依赖人工标注，能够稳定检测和分类罕见及新型缺陷。


<details>
  <summary>Details</summary>
Motivation: 集成电路制造流程复杂，易产生多种缺陷，影响良率和可靠性。传统有监督方法依赖大量标注且难以应对新型和稀有缺陷，无监督聚类方法又容易因缺乏先验导致不稳定。因此，亟需一种无需人工标注且能适应罕见/新型缺陷的检测与分类方法。

Method: 作者提出IC DefectNCD框架，利用集成电路扫描电镜（SEM）图像的内在先验信息进行缺陷检测与新类别发现：(1) 通过自归一信息引导的缺陷检测，学习正常特征，利用重建残差初步定位缺陷；(2) 采用自适应二值化策略，稳定聚焦于核心缺陷区域；(3) 设计自缺陷信息引导的分类方法，引入软掩码关注机制，将空间缺陷先验融入教师-学生模型中，提高对新颖缺陷的灵敏度并 suppress 背景干扰。

Result: 在包含三大工艺阶段、15类缺陷的真实工业集成电路数据集上验证，实验结果显示提出方法在缺陷检测与新类别缺陷分类任务上具有稳健、优异的性能。

Conclusion: IC DefectNCD方法克服了传统有监督和无监督方法的局限，能够无需人工标注地高效、鲁棒地检测和区分类别罕见或新出现的集成电路缺陷，有助于提升制造可靠性和效率。

Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of
process steps. Defects can arise at any stage, causing yield loss and
ultimately degrading product reliability. Supervised methods require extensive
human annotation and struggle with emergent categories and rare, data scarce
defects. Clustering-based unsupervised methods often exhibit unstable
performance due to missing priors. We propose IC DefectNCD, a support set free
framework that leverages Image Intrinsic Priors in IC SEM images for defect
detection and novel class discovery. We first develop Self Normal Information
Guided IC Defect Detection, aggregating representative normal features via a
learnable normal information extractor and using reconstruction residuals to
coarsely localize defect regions. To handle saliency variations across defects,
we introduce an adaptive binarization strategy that produces stable subimages
focused on core defective areas. Finally, we design Self Defect Information
Guided IC Defect Classification, which incorporates a soft mask guided
attention mechanism to inject spatial defect priors into the teacher student
model. This enhances sensitivity to defective regions, suppresses background
interference, and enables recognition and classification of unseen defects. We
validate the approach on a real world dataset spanning three key fabrication
stages and covering 15 defect types. Experiments demonstrate robust performance
on both defect detection and unseen defect classification.

</details>


### [15] [Accelerating Physical Property Reasoning for Augmented Visual Cognition](https://arxiv.org/abs/2511.03126)
*Hongbo Lan,Zhenlin An,Haoyu Li,Vaibhav Singh,Longfei Shangguan*

Main category: cs.CV

TL;DR: 本文提出了\sysname系统，通过一系列算法和系统优化，大大加快了基于视觉的物理属性推理速度，实现了秒级推理并在多个任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导的物理属性推理方法运算慢，难以实时应用于增强现实等场景，需要提升其推理速度以支持实际使用。

Method: \sysname结合了快速几何3D重建、高效语义特征融合、并行视角编码等多项优化技术，显著减少了整个推理流程的延迟，还与注视跟踪结合，实现杂乱环境中目标物体的快速识别与推理。

Result: 该系统将推理延迟从10-20分钟缩短到不足6秒，在ABO数据集上的速度提升达62.9至287.2倍，并在物体物理属性估算、材料分割和体素推断任务上达到或超过当前最优方法。

Conclusion: \sysname实现了低延迟、高准确率的视觉物理属性推理，具备实际增强现实场景中的应用潜力，能在复杂的真实环境中稳定高效地工作。

Abstract: This paper introduces \sysname, a system that accelerates vision-guided
physical property reasoning to enable augmented visual cognition. \sysname
minimizes the run-time latency of this reasoning pipeline through a combination
of both algorithmic and systematic optimizations, including rapid geometric 3D
reconstruction, efficient semantic feature fusion, and parallel view encoding.
Through these simple yet effective optimizations, \sysname reduces the
end-to-end latency of this reasoning pipeline from 10--20 minutes to less than
6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname
achieves this 62.9$\times$--287.2$\times$ speedup while not only reaching
on-par (and sometimes slightly better) object-level physical property
estimation accuracy(e.g. mass), but also demonstrating superior performance in
material segmentation and voxel-level inference than two SOTA baselines. We
further combine gaze-tracking with \sysname to localize the object of interest
in cluttered, real-world environments, streamlining the physical property
reasoning on smart glasses. The case study with Meta Aria Glasses conducted at
an IKEA furniture store demonstrates that \sysname achives consistently high
performance compared to controlled captures, providing robust property
estimations even with fewer views in real-world scenarios.

</details>


### [16] [Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response](https://arxiv.org/abs/2511.03132)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文首次将AI/ML系统实际部署于联邦灾害响应，利用无人机航拍影像实现自动化建筑损毁评估，大幅提升灾后响应速度。


<details>
  <summary>Details</summary>
Motivation: 近年灾害中，利用无人机采集了大量灾区图像，但图像数据量巨大，人工传输与分析极为耗时，严重延迟一线响应，需要自动化方法解决数据过载并加速评估流程。

Method: 开发并训练了基于计算机视觉与机器学习的建筑损毁评估模型，使用当前最大规模的灾后无人机图像数据集（含21,716个标签），并对91位相关从业者进行了操作培训。

Result: 该系统已在Debby及Helene飓风灾害响应中部署，18分钟内自动评估415栋建筑，效率显著提升。

Conclusion: 该研究首次实现AI/ML灾害实战用于损毁评估，提供了宝贵的实地经验与教训，为相关领域研究和实际应用提供了参考。

Abstract: This paper presents the first AI/ML system for automating building damage
assessment in uncrewed aerial systems (sUAS) imagery to be deployed
operationally during federally declared disasters (Hurricanes Debby and
Helene). In response to major disasters, sUAS teams are dispatched to collect
imagery of the affected areas to assess damage; however, at recent disasters,
teams collectively delivered between 47GB and 369GB of imagery per day,
representing more imagery than can reasonably be transmitted or interpreted by
subject matter experts in the disaster scene, thus delaying response efforts.
To alleviate this data avalanche encountered in practice, computer vision and
machine learning techniques are necessary. While prior work has been deployed
to automatically assess damage in satellite imagery, there is no current state
of practice for sUAS-based damage assessment systems, as all known work has
been confined to academic settings. This work establishes the state of practice
via the development and deployment of models for building damage assessment
with sUAS imagery. The model development involved training on the largest known
dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage
labels, and the operational training of 91 disaster practitioners. The best
performing model was deployed during the responses to Hurricanes Debby and
Helene, where it assessed a combined 415 buildings in approximately 18 minutes.
This work contributes documentation of the actual use of AI/ML for damage
assessment during a disaster and lessons learned to the benefit of the AI/ML
research and user communities.

</details>


### [17] [Finetuning-Free Personalization of Text to Image Generation via Hypernetworks](https://arxiv.org/abs/2511.03156)
*Sagar Shrestha,Gopal Sharma,Luowei Zhou,Suren Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调即可实现文本-图像扩散模型个性化的超网络方法，通过直接从主体图像预测LoRA权重，并结合了新型推理引导手段，实现高效且精准的个性化生成。


<details>
  <summary>Details</summary>
Motivation: 现有个性化技术（如DreamBooth）虽然效果好，但计算昂贵、推理慢。后续适配器与编码器方法虽有所改进，但依然依赖微调或大模型，成本高亟需更高效可扩展的解决方案。

Method: 本文采用超网络（Hypernetwork），能从主体图像直接生成LoRA自适应权重，实现免微调的个性化。为稳定训练引入输出正则化，并提出Hybrid-Model Classifier-Free Guidance（HM-CFG），结合基础模型与个性化模型的优势提升推理泛化能力。

Result: 实验证明，在CelebA-HQ、AFHQ-v2、DreamBench等数据集上，该方法无需每个主体单独优化，显著提升了个性化效果和推理效率。HM-CFG进一步提升了生成图像的组合泛化能力，保持了主体保真度和文本一致性。

Conclusion: 该方法不仅高效可靠，实现了开放类别下的出色个性化，同时展示了超网络用于个性化的巨大潜力，有望成为扩散模型个性化领域可扩展且有效的方向。

Abstract: Personalizing text-to-image diffusion models has traditionally relied on
subject-specific fine-tuning approaches such as
DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and
slow at inference. Recent adapter- and encoder-based methods attempt to reduce
this overhead but still depend on additional fine-tuning or large backbone
models for satisfactory results. In this work, we revisit an orthogonal
direction: fine-tuning-free personalization via Hypernetworks that predict
LoRA-adapted weights directly from subject images. Prior hypernetwork-based
approaches, however, suffer from costly data generation or unstable attempts to
mimic base model optimization trajectories. We address these limitations with
an end-to-end training objective, stabilized by a simple output regularization,
yielding reliable and effective hypernetworks. Our method removes the need for
per-subject optimization at test time while preserving both subject fidelity
and prompt alignment. To further enhance compositional generalization at
inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),
which combines the compositional strengths of the base diffusion model with the
subject fidelity of personalized models during sampling. Extensive experiments
on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves
strong personalization performance and highlights the promise of hypernetworks
as a scalable and effective direction for open-category personalization.

</details>


### [18] [Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation](https://arxiv.org/abs/2511.03163)
*Yun-Chen Lin,Jiayuan Huang,Hanyuan Zhang,Sergi Kavtaradze,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本文提出了一种深度引导的肝脏手术解剖结构分割框架，通过RGB和深度特征融合，采用高效的低秩梯度投影方法（SRFT-GaLore）对大模型高效微调，实现了更精确、泛化性更强的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在腹腔镜肝脏手术中，二维视频限制了深度感知，使得解剖标志物的精确定位变得困难。现有方法在融合RGB和深度特征以及将大规模视觉模型高效适配于手术场景方面存在挑战。

Method: 作者提出结合语义和几何信息的双分支编码框架，分别使用Segment Anything Model V2 (SAM2) 提取RGB特征，Depth Anything V2 (DA2) 提取深度特征。为高效适配SAM2，设计了SRFT-GaLore低秩梯度投影方法，替代传统计算量大的SVD操作。再利用交叉注意力融合模块进行RGB和深度信息融合。

Result: 在公开L3D数据集上，提出方法在Dice系数上提升4.85%，在平均对称表面距离上减少11.78分。并在自建的LLSD外部手术数据集上，模型保持高性能，显著优于SAM系列基线方法，显示出很强的泛化和适应能力。

Conclusion: SRFT-GaLore增强的双编码器框架能够在实时、深度受限的手术场景中，实现可扩展且精确的解剖结构分割，具备优异的跨数据集和未知环境鲁棒性。

Abstract: Accurate detection and delineation of anatomical structures in medical
imaging are critical for computer-assisted interventions, particularly in
laparoscopic liver surgery where 2D video streams limit depth perception and
complicate landmark localization. While recent works have leveraged monocular
depth cues for enhanced landmark detection, challenges remain in fusing RGB and
depth features and in efficiently adapting large-scale vision models to
surgical domains. We propose a depth-guided liver landmark segmentation
framework integrating semantic and geometric cues via vision foundation
encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB
features and Depth Anything V2 (DA2) encoder to extract depth-aware features.
To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient
projection method that replaces the computationally expensive SVD with a
Subsampled Randomized Fourier Transform (SRFT). This enables efficient
fine-tuning of high-dimensional attention layers without sacrificing
representational power. A cross-attention fusion module further integrates RGB
and depth cues. To assess cross-dataset generalization, we also construct a new
Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.
On the public L3D dataset, our method achieves a 4.85% improvement in Dice
Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface
Distance compared to the D2GPLand. To further assess generalization capability,
we evaluate our model on LLSD dataset. Our model maintains competitive
performance and significantly outperforms SAM-based baselines, demonstrating
strong cross-dataset robustness and adaptability to unseen surgical
environments. These results demonstrate that our SRFT-GaLore-enhanced
dual-encoder framework enables scalable and precise segmentation under
real-time, depth-constrained surgical settings.

</details>


### [19] [SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention](https://arxiv.org/abs/2511.03178)
*Shreyas C. Dhake,Jiayuan Huang,Runlong He,Danyal Z. Khan,Evangelos B. Mazomenos,Sophia Bano,Hani J. Marcus,Danail Stoyanov,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本论文提出PitVQA-Anticipation数据集和SurgAnt-ViVQA模型，实现对内镜下垂体瘤手术中未来事件的智能预测和视觉问答，比以往方法更善于前瞻性分析和实时助手。


<details>
  <summary>Details</summary>
Motivation: 内镜下垂体瘤手术过程中视野受限且工作流变化快，实时预测下一步操作和器械需求对辅助医生极为重要。现有视觉问答系统多基于静态帧，局限于描述当前场景，无法有效预测未来事件。

Method: 1）发布PitVQA-Anticipation数据集，包含33.5小时手术视频和73万多组面向未来预测的问题答案对，涵盖阶段、步骤、器械、剩余时间等四个任务。2）提出SurgAnt-ViVQA视频语言模型，基于大语言模型结合GRU门控时序跨模态注意力模块，实现帧间动态编码与细粒度视觉语义融合。并通过高效参数调整，适配手术领域。

Result: SurgAnt-ViVQA在PitVQA-Anticipation和EndoVis等数据集上优于现有图像/视频基线模型。消融实验显示时序递归和门控融合对性能提升作用显著。帧数实验发现：8帧提升语句流畅性，但32帧有利于时间类数字预测。

Conclusion: SurgAnt-ViVQA通过时序编码和门控语视融合，实现从手术视觉描述到前瞻性预测的能力跃升。PitVQA-Anticipation为手术过程预测性问答模型提供全面评测基准，并凸显有针对性的时序建模对于智能手术助手至关重要。

Abstract: Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.

</details>


### [20] [PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research](https://arxiv.org/abs/2511.03194)
*Le Xue,Gang Feng,Wenbo Zhang,Yichi Zhang,Lanlan Li,Shuqi Wang,Liling Peng,Sisi Peng,Xin Gao*

Main category: cs.CV

TL;DR: PETWB-REP是一个公开的全身PET/CT影像与报告数据集，覆盖多种癌症。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据集大多局限于单一模态或癌种，且缺乏详尽临床报告，限制了AI模型开发与研究。

Method: 本研究整理并公开了来自490名多种癌症（如肺癌、肝癌、乳腺癌等）患者的全身18F-FDG PET/CT扫描影像、去标识化放射学报告及结构化临床数据。

Result: 构建了包含影像（三维PET和CT）、报告文本和临床元数据的多模态医学影像数据集。

Conclusion: PETWB-REP将成为医学影像、放射组学、AI和多模态学习等领域的研究基础，有助于促进多癌种相关AI模型的开发与验证。

Abstract: Publicly available, large-scale medical imaging datasets are crucial for
developing and validating artificial intelligence models and conducting
retrospective clinical research. However, datasets that combine functional and
anatomical imaging with detailed clinical reports across multiple cancer types
remain scarce. Here, we present PETWB-REP, a curated dataset comprising
whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed
Tomography (PET/CT) scans and corresponding radiology reports from 490 patients
diagnosed with various malignancies. The dataset primarily includes common
cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and
ovarian cancer. This dataset includes paired PET and CT images, de-identified
textual reports, and structured clinical metadata. It is designed to support
research in medical imaging, radiomics, artificial intelligence, and
multi-modal learning.

</details>


### [21] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型（MLLMs）在多图像场景下的感知和推理能力不足问题，提出了一种全新的零样本prompting方法Question-Guided Chain-of-Captions（QG-CoC），并在多个基准任务中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多图像场景下存在精细感知和信息综合推理能力弱的问题，多数相关研究仅关注单图像或受限场景，缺乏对复杂多图像推理任务的系统性研究。

Method: 作者首先系统性分析了现有prompting方法在多图像感知和处理中的表现，发现其难以有效捕捉所需线索和整合信息。基于此，提出QG-CoC：通过问题引导，生成链式描述，将不同图像的细粒度信息按需整合，支持任意数量图像的推理。

Result: QG-CoC方法在多组公开和闭源MLLMs、多个多图像与单图像基准任务上进行评估。结果表明，QG-CoC在处理复杂多图像任务时提供了较现有方法更强的泛化性和表现，尤其在原有方法失效场景有显著改进。

Conclusion: QG-CoC方法有效提升了MLLMs在多图片复杂推理任务下的表现，显示出更好的细粒度感知和推理能力，为多模态推理场景提供了新思路。

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [22] [MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction](https://arxiv.org/abs/2511.03212)
*Ruting Cheng,Boyuan Feng,Yijiang Zheng,Chuhui Qiu,Aizierjiang Aiersilan,Joaquin A. Calderon,Wentao Zhao,Qing Pan,James K. Hahn*

Main category: cs.CV

TL;DR: 本文提出了一种结合自报医学数据和3D体型扫描的新型人工智能方法，用于预测剖宫产（CS）风险，在资源有限地区也具备可用性。


<details>
  <summary>Details</summary>
Motivation: 剖宫产风险评估在资源有限的医疗环境下十分重要，但现有方法多依赖医院及分娩过程中的参数，难以在家中或医疗资源不足时应用。

Method: 采用31-38周孕妇的3D光学体型扫描与自报医学信息，设计了一种多视角Transformer网络（MvBody），利用度量学习改进模型泛化性和训练效率。还使用Integrated Gradients解释模型决策。

Result: 在独立测试集中，所提方法准确率达84.62%、AUC-ROC达0.724，优于通用机器学习与最新三维分析方法。

Conclusion: 结合自报信息和3D体型特征的AI方法能在数据有限环境下有效预测剖宫产风险，为实际普及与临床决策提供了可行的新工具。

Abstract: Accurately assessing the risk of cesarean section (CS) delivery is critical,
especially in settings with limited medical resources, where access to
healthcare is often restricted. Early and reliable risk prediction allows
better-informed prenatal care decisions and can improve maternal and neonatal
outcomes. However, most existing predictive models are tailored for in-hospital
use during labor and rely on parameters that are often unavailable in
resource-limited or home-based settings. In this study, we conduct a pilot
investigation to examine the feasibility of using 3D body shape for CS risk
assessment for future applications with more affordable general devices. We
propose a novel multi-view-based Transformer network, MvBody, which predicts CS
risk using only self-reported medical data and 3D optical body scans obtained
between the 31st and 38th weeks of gestation. To enhance training efficiency
and model generalizability in data-scarce environments, we incorporate a metric
learning loss into the network. Compared to widely used machine learning models
and the latest advanced 3D analysis methods, our method demonstrates superior
performance, achieving an accuracy of 84.62% and an Area Under the Receiver
Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set.
To improve transparency and trust in the model's predictions, we apply the
Integrated Gradients algorithm to provide theoretically grounded explanations
of the model's decision-making process. Our results indicate that pre-pregnancy
weight, maternal age, obstetric history, previous CS history, and body shape,
particularly around the head and shoulders, are key contributors to CS risk
prediction.

</details>


### [23] [Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation](https://arxiv.org/abs/2511.03219)
*Pengyu Jie,Wanquan Liu,Rui He,Yihui Wen,Deyu Meng,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种结合配对扩散引导与标签保持混合的数据增强方法，用于提升医学分割任务的性能。通过同一掩膜下生成真实与合成配对图像，并引入自适应实数据锚定方法，有效兼顾多样性与监督信号一致性，取得了领先的分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有密集预测的数据增强方法主要依赖样本混合或生成式合成，但两者各有缺陷。样本混合容易因掩膜错位导致标签模糊，生成式合成则存在合成-真实域偏移，且未充分利用结构信息。因此，需要创新方法，在提升多样性的同时，保留像素级监督信号并减小域偏移。

Method: 作者提出“Mask-Consistent Paired Mixing（MCPMix）”：对每个真实图像在相同掩膜下生成一个合成对，配对输入后只混合外观，不影响原始掩膜。训练时监督总是用原始硬掩膜，扩展了样本多样性的同时保证标签一致性。其次，提出“Real-Anchored Learnable Annealing（RLA）”，在训练过程中自适应调整混合强度与混合样本损失权重，逐步将学习重心重新锚定回真实数据，降低分布偏差。

Result: 方法在Kvasir-SEG、PICCOLO、CVC-ClinicDB、NPC-LES数据集和ISIC 2017等医学图像分割任务上，取得了当前最优的分割性能，对比基准方法均有显著提升。

Conclusion: 将标签保持的混合策略与扩散驱动的多样性结合，并配合自适应的实数据锚定机制，能够有效提升端到端内镜图像分割的鲁棒性与泛化能力。

Abstract: Augmentation for dense prediction typically relies on either sample mixing or
generative synthesis. Mixing improves robustness but misaligned masks yield
soft label ambiguity. Diffusion synthesis increases apparent diversity but,
when trained as common samples, overlooks the structural benefit of mask
conditioning and introduces synthetic-real domain shift. We propose a paired,
diffusion-guided paradigm that fuses the strengths of both. For each real
image, a synthetic counterpart is generated under the same mask and the pair is
used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which
mixes only image appearance while supervision always uses the original hard
mask. This produces a continuous family of intermediate samples that smoothly
bridges synthetic and real appearances under shared geometry, enlarging
diversity without compromising pixel-level semantics. To keep learning aligned
with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the
mixing strength and the loss weight of mixed samples over training, gradually
re-anchoring optimization to real data and mitigating distributional bias.
Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC
2017, the approach achieves state-of-the-art segmentation performance and
consistent gains over baselines. The results show that combining
label-preserving mixing with diffusion-driven diversity, together with adaptive
re-anchoring, yields robust and generalizable endoscopic segmentation.

</details>


### [24] [Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2511.03232)
*Sichen Guo,Wenjie Li,Yuanyang Liu,Guangwei Gao,Jian Yang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 本文提出了一种高效的超分辨率方法T-PMambaSR，通过融合窗口自注意力与渐进式Mamba架构，提升了特征表达能力，同时保留了线性计算复杂度。引入的自适应高频细节恢复模块有效弥补了高频信息损失。实验结果显示，本方法性能优于最新Transformer和Mamba方法，计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Mamba的超分辨方法可用线性复杂度处理全局信息，但其在不同尺度特征建模上的过渡不细致，导致特征表达效率受限。因此需要设计同时兼具高效性与表达力的轻量级超分模型。

Method: 提出T-PMambaSR框架，将窗口自注意力机制与渐进式Mamba架构结合，实现多尺度感受野间有效交互，精细化建模各尺度特征。此外，引入自适应高频细节恢复模块（AHFRM），专门用于恢复在Transformer及Mamba处理过程中损失的高频细节。

Result: T-PMambaSR能在保持线性计算复杂度的同时，逐步提升模型感受野和表达能力。大量实验表明，该方法在性能上优于最新的基于Transformer或Mamba的超分辨模型，并具有更低的算力消耗。

Conclusion: T-PMambaSR通过精细化多尺度特征建模与高频细节恢复，有效提升超分辨率任务的表现，并显著降低了计算代价，具有较高的实际应用前景。

Abstract: Recently, Mamba-based super-resolution (SR) methods have demonstrated the
ability to capture global receptive fields with linear complexity, addressing
the quadratic computational cost of Transformer-based SR approaches. However,
existing Mamba-based methods lack fine-grained transitions across different
modeling scales, which limits the efficiency of feature representation. In this
paper, we propose T-PMambaSR, a lightweight SR framework that integrates
window-based self-attention with Progressive Mamba. By enabling interactions
among receptive fields of different scales, our method establishes a
fine-grained modeling paradigm that progressively enhances feature
representation with linear complexity. Furthermore, we introduce an Adaptive
High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
during Transformer and Mamba processing. Extensive experiments demonstrate that
T-PMambaSR progressively enhances the model's receptive field and
expressiveness, yielding better performance than recent Transformer- or
Mamba-based methods while incurring lower computational cost. Our codes will be
released after acceptance.

</details>


### [25] [Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning](https://arxiv.org/abs/2511.03245)
*Liwei Luo,Shuaitengyuan Li,Dongwei Ren,Qilong Wang,Pengfei Zhu,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出了一种新的多预测器优化方法（DMPO），通过结构和训练上的优化，有效提升了大模型早停推理的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着大规模预训练模型的推广，推理效率成为实际部署的关键。早停推理能提升效率，但如何让浅层既服务深层又兼顾自身表现，是未解决的难点。

Method: 提出DMPO方法，设计轻量旁路模块分解浅层特征功能，并用高阶统计预测器增强早期判别力，在训练上采用两阶段损失权重调节，分别优先提升深层与浅层判别能力。

Result: 在多种数据集和不同预训练骨干下，实验证明该方法在减少计算量的同时，性能明显优于同类方法。

Conclusion: DMPO方案能有效解耦浅层的表达与判别能力，适用于提升大模型推理效率，为推理优化提供了新的思路。

Abstract: Recently, remarkable progress has been made in large-scale pre-trained model
tuning, and inference efficiency is becoming more crucial for practical
deployment. Early exiting in conjunction with multi-stage predictors, when
cooperated with a parameter-efficient fine-tuning strategy, offers a
straightforward way to achieve an inference-efficient model. However, a key
challenge remains unresolved: How can early stages provide low-level
fundamental features to deep stages while simultaneously supplying high-level
discriminative features to early-stage predictors? To address this problem, we
propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively
decouple the low-level representative ability and high-level discriminative
ability in early stages. First, in terms of architecture, we introduce a
lightweight bypass module into multi-stage predictors for functional
decomposition of shallow features from early stages, while a high-order
statistics-based predictor is developed for early stages to effectively enhance
their discriminative ability. To reasonably train our multi-predictor
architecture, a decoupled optimization is proposed to allocate two-phase loss
weights for multi-stage predictors during model tuning, where the initial
training phase enables the model to prioritize the acquisition of
discriminative ability of deep stages via emphasizing representative ability of
early stages, and the latter training phase drives discriminative ability
towards earlier stages as much as possible. As such, our DMPO can effectively
decouple representative and discriminative abilities in early stages in terms
of architecture design and model optimization. Experiments across various
datasets and pre-trained backbones demonstrate that DMPO clearly outperforms
its counterparts when reducing computational cost.

</details>


### [26] [Generative deep learning for foundational video translation in ultrasound](https://arxiv.org/abs/2511.03255)
*Nikolina Tomic Roshni Bhatnagar,Sarthak Jain,Connor Lau,Tien-Yu Liu,Laura Gambini,Rima Arnaout*

Main category: cs.CV

TL;DR: 本文提出了一种用于超声心动图不同亚模态（如灰度和彩色多普勒CFD）之间视频转换的生成模型，实现高质量、难以区分真伪的合成影像，有效提升数据集的平衡性和实用性。


<details>
  <summary>Details</summary>
Motivation: 医学超声影像数据存在多模态、失衡和缺失等问题，妨碍深度学习方法在医学影像中的广泛应用。尤其是CFD等亚模态数据稀缺，影响模型泛化和临床实用性。因此，亟需自动化方法生成逼真、均衡的亚模态超声数据。

Method: 作者基于生成对抗网络，融合像素级损失、对抗损失及感知损失，使用双网络结构（一用于重建解剖结构，一用于去噪），在大规模超声视频数据集上（54,975训练，8,368测试）训练和评估，实现CFD与灰度视频转换。

Result: 生成视频与真实视频的平均SSIM为0.91±0.04，深度学习分类与分割表现几乎无差别（F1得分：真实0.9，合成0.89；Dice分割得分0.97）；经临床专家盲审，无显著差异（辨别准确率54±6%），证明合成视频高度真实。该方法还在心脏以外领域表现良好。

Conclusion: 该工作显著提升了医学影像数据的可用性和均衡性，支持深度学习模型训练，为医学影像数据集设计和回顾性研究提供了重要工具，具有较强的普适性和翻译能力。

Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and
interpretation across medicine, however, attention to data imbalance and
missingness is required. Ultrasound data presents a particular challenge
because in addition to different views and structures, it includes several
sub-modalities-such as greyscale and color flow doppler (CFD)-that are often
imbalanced in clinical studies. Image translation can help balance datasets but
is challenging for ultrasound sub-modalities to date. Here, we present a
generative method for ultrasound CFD-greyscale video translation, trained on
54,975 videos and tested on 8,368. The method developed leveraged pixel-wise,
adversarial, and perceptual loses and utilized two networks: one for
reconstructing anatomic structures and one for denoising to achieve realistic
ultrasound imaging. Average pairwise SSIM between synthetic videos and ground
truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real
ones in DL classification and segmentation tasks and when evaluated by blinded
clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice
score between real and synthetic segmentation was 0.97. Overall clinician
accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%),
indicating realistic synthetic videos. Although trained only on heart videos,
the model worked well on ultrasound spanning several clinical domains (average
SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data
expand the utility of retrospectively collected imaging and augment the dataset
design toolbox for medical imaging.

</details>


### [27] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: 本文提出了OmniVLA模型，可融合多种传感器（红外、毫米波雷达、麦克风阵列）输入，提升VLA模型在感知和操作任务上的表现，显著优于仅用RGB的模型。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型虽然受益于大规模视觉-语言预训练，但大多只用RGB相机，导致感知能力有限，不能很好处理真实环境中需要多模态感知的任务。

Method: 提出了sensor-masked image表示，对RGB图像叠加传感器掩码（来自红外、雷达、麦克风等），保持输入统计特性一致，易于训练。设计了轻量化感知融合项目器，在预训练的VLA骨干基础上训练多传感器融合模型。

Result: OmniVLA在实际多模态操作任务上表现优异，任务成功率平均84%，比RGB-only和直接用原始传感器输入的基线分别高出59%和28%；同时具备更高的数据效率和更强的泛化能力。

Conclusion: 通过统一的感知融合方式，OmniVLA提升了机器人在复杂环境下的空间智能和操作表现，证明了多模态融合对VLA模型的重要价值和有效性。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [28] [Enhancing Medical Image Segmentation via Heat Conduction Equation](https://arxiv.org/abs/2511.03260)
*Rong Wu,Yim-Sang Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种结合U-Mamba与热传导方程的新型混合架构，用于医学图像分割，在效率和全局建模能力之间取得更好平衡，并在多模态腹部CT与MRI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型难以在有限计算资源下实现高效的全局上下文建模和长距离依赖建模。本文旨在解决该效率与建模能力之间的矛盾。

Method: 提出了一种U-Mamba与热传导方程（HCOs）结合的混合架构。核心做法是在U-Net结构的bottleneck层引入Mamba状态空间模块进行长距离依赖建模，同时用热传导算子在频域模拟热扩散以提升语义抽象能力。

Result: 在多模态腹部CT和MRI数据集上，所提方法在准确性等评价指标上都超越了强大的基线方法，展示了其有效性和泛化能力。

Conclusion: 将状态空间动力学与基于热扩散的全局信息融合，为医学图像分割提供了一种可扩展且可解释的解决方案。

Abstract: Medical image segmentation has been significantly advanced by deep learning
architectures, notably U-Net variants. However, existing models struggle to
achieve efficient global context modeling and long-range dependency reasoning
under practical computational budgets simultaneously. In this work, we propose
a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.
Our model combines Mamba-based state-space modules for efficient long-range
reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,
simulating frequency-domain thermal diffusion for enhanced semantic
abstraction. Experimental results on multimodal abdominal CT and MRI datasets
demonstrate that the proposed model consistently outperforms strong baselines,
validating its effectiveness and generalizability. It suggest that blending
state-space dynamics with heat-based global diffusion offers a scalable and
interpretable solution for medical segmentation tasks.

</details>


### [29] [IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection](https://arxiv.org/abs/2511.03267)
*Bingyang Guo,Hongjie Li,Ruiyun Yu,Hanzhe Liang,Jinbao Wang*

Main category: cs.CV

TL;DR: 本文提出了一个全新的用于工业设备部件3D异常检测的数据集IEC3D-AD，并基于此提出了新的3D异常检测方法GMANet，提升了实际应用中的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测数据集难以覆盖工业实际场景中复杂且微小的缺陷，无法满足高精度异常检测需求，特别是在工业设备部件上。

Method: 1）采集真实工业产线中的部件点云数据，构建高分辨率和细粒度缺陷标注的IEC3D-AD数据集。2）受2D生成式AD方法启发，提出GMANet，在点云级别做几何形态分析，合成样本并进行空间特征优化，以提升正常与异常样本区分度。

Result: 在IEC3D-AD及其它数据集上进行大量实验，结果显示该数据集和方法显著提升了异常检测的准确性和泛化能力。

Conclusion: IEC3D-AD数据集与GMANet方法为工业3D异常检测领域带来进展，尤其在真实场景下提升了检测能力，对后续研究和实际部署具有促进作用。

Abstract: 3D anomaly detection (3D-AD) plays a critical role in industrial
manufacturing, particularly in ensuring the reliability and safety of core
equipment components. Although existing 3D datasets like Real3D-AD and MVTec
3D-AD offer broad application support, they fall short in capturing the
complexities and subtle defects found in real industrial environments. This
limitation hampers precise anomaly detection research, especially for
industrial equipment components (IEC) such as bearings, rings, and bolts. To
address this challenge, we have developed a point cloud anomaly detection
dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is
directly collected from actual production lines, ensuring high fidelity and
relevance. Compared to existing datasets, IEC3D-AD features significantly
improved point cloud resolution and defect annotation granularity, facilitating
more demanding anomaly detection tasks. Furthermore, inspired by generative
2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This
paradigm generates synthetic point cloud samples based on geometric
morphological analysis, then reduces the margin and increases the overlap
between normal and abnormal point-level features through spatial discrepancy
optimization. Extensive experiments demonstrate the effectiveness of our method
on both IEC3D-AD and other datasets.

</details>


### [30] [Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising](https://arxiv.org/abs/2511.03272)
*Shuangquan Lyu,Steven Mao,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出了一种创新方法，使得能够对任意长度视频实现高质量、可控的视频修补（补全）和拓展编辑，解决了以往视频生成受长度限制和拼接瑕疵等难题。


<details>
  <summary>Details</summary>
Motivation: 现有视频修补和生成方法在处理较长视频片段时，面临可控性差、易产生拼接痕迹或漂移等问题，限制了其在实际长视频编辑中的应用。

Method: 方法上，基于大规模预训练的文生视频扩散模型（如Alibaba Wan 2.1），通过LoRA高效微调模型以适应带掩码区域的视频合成。同时，提出了重叠-融合的时间协同去噪策略和高阶求解器，增强长序列生成时的帧间一致性。

Result: 在大规模帧内编辑或添加目标等复杂视频修补/拓展任务中，方法在视频质量（PSNR/SSIM）和感知真实感（LPIPS）评测指标上均显著优于Wan 2.1、VACE等基线方法。

Conclusion: 本文方法实现了参数效率与性能的良好平衡，能够支持实际长视频范围内的高质量、无缝、高可控编辑，拓展了扩散模型在实际视频编辑领域的应用空间。

Abstract: Generating long videos remains a fundamental challenge, and achieving high
controllability in video inpainting and outpainting is particularly demanding.
To address both of these challenges simultaneously and achieve controllable
video inpainting and outpainting for long video clips, we introduce a novel and
unified approach for long video inpainting and outpainting that extends
text-to-video diffusion models to generate arbitrarily long, spatially edited
videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a
large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked
region video synthesis, and employs an overlap-and-blend temporal co-denoising
strategy with high-order solvers to maintain consistency across long sequences.
In contrast to prior work that struggles with fixed-length clips or exhibits
stitching artifacts, our system enables arbitrarily long video generation and
editing without noticeable seams or drift. We validate our approach on
challenging inpainting/outpainting tasks including editing or adding objects
over hundreds of frames and demonstrate superior performance to baseline
methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and
perceptual realism (LPIPS). Our method enables practical long-range video
editing with minimal overhead, achieved a balance between parameter efficient
and superior performance.

</details>


### [31] [Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2511.03317)
*Minghao Fu,Guo-Hua Wang,Tianyu Cui,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的Diffusion-SDPO方法，有效提升了基于人类偏好的文本到图像扩散模型的对齐能力，并在各类基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管当前基于扩散的文本到图像模型生成质量很高，但如何让生成结果更好地对齐人类偏好仍是难题。现有Diffusion-DPO方法在扩大偏好差距时会导致最终生成质量并未提升，甚至可能同时损害更优和更劣输出的质量。

Method: 作者发现标准Diffusion-DPO目标在优化时可能同时增加赢家（preferred）和输家（less preferred）分支的误差。为解决此问题，提出Diffusion-SDPO——一种自适应调整输家梯度规模、保证赢家误差不升高的更新规则，并给出了一阶精确的缩放系数公式。

Result: Diffusion-SDPO方法简单、高兼容性，基本不增加计算开销。在多个文本到图像数据集和偏好对齐基准下，显著优于现有的偏好学习方法。

Conclusion: Diffusion-SDPO能更好地实现基于人类偏好的文本到图像生成模型训练，对现有框架高度兼容，并具备良好的实际应用前景。

Abstract: Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.

</details>


### [32] [SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding](https://arxiv.org/abs/2511.03325)
*Mauro Orazio Drago,Luca Carlini,Pelinsu Celebi Balyemez,Dennis Pierantozzi,Chiara Lena,Cesare Hassan,Danail Stoyanov,Elena De Momi,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频问答模型SurgViVQA及其配套的专用数据集，专门用于手术领域以提升AI模型对手术视频的时序和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频问答方法主要依赖静态图像特征，缺乏对手术场景中动态信息（比如动作、器械与组织的交互等）的理解，且相关数据集缺乏详细的时序标注，限制了模型对实际手术流程的解释能力。

Method: 作者构建了SurgViVQA模型，采用Masked Video–Text Encoder将手术视频和问题信息融合，捕捉动态时序线索，之后由大型语言模型（LLM）生成答案。模型在新建的REAL-Colon-VQA数据集和公开的EndoVis18-VQA数据集上进行训练与测试。此外，REAL-Colon-VQA涵盖了与运动相关的问题、诊断属性以及格式变化的问题，以测试模型的鲁棒性。

Result: 与现有基于静态图像的VQA模型相比，SurgViVQA在关键词准确性指标上性能更优，在REAL-Colon-VQA上比PitVQA提升11%；在EndoVis18-VQA上提升9%。扰动实验表明SurgViVQA在不同问题表述情况下具有更强的泛化能力和鲁棒性。

Conclusion: SurgViVQA及其数据集为手术视频问答提供了动态时序理解框架，有效提升了AI模型在手术场景下的综合语义和时序推理能力，对推动智能手术辅助系统的发展具有重要意义。

Abstract: Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (LLM) then
decodes into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.

</details>


### [33] [Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge](https://arxiv.org/abs/2511.03332)
*Yi Yang,Yiming Xu,Timo Kaiser,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: 本文提出了一个结合FastTracker和LLaVA-Video的两阶段零样本方法，在MOT25-StAG多目标时空动作定位挑战赛中取得了第二名。


<details>
  <summary>Details</summary>
Motivation: 如何在复杂真实场景下，利用视频数据，实现根据自由描述语言查询下的多目标精准定位与跟踪，是视觉与语言结合领域的关键难题。

Method: 本方法将该任务建模为视频检索问题，采用了两阶段的零样本方案：第一阶段利用SOTA跟踪模型FastTracker进行目标提取与跟踪，第二阶段结合多模态大语言模型LLaVA-Video进行语义检索和匹配。

Result: 在MOT25-StAG测试集上，该方法取得了m-HIoU 20.68和HOTA 10.73的成绩，位列竞赛第二名。

Conclusion: 结合SOTA跟踪与多模态大模型的两阶段方法能较好地解决复杂场景下基于自然语言的多目标定位和跟踪任务，有效提升了任务表现。

Abstract: In this report, we present our solution to the MOT25-Spatiotemporal Action
Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately
localize and track multiple objects that match specific and free-form language
queries, using video data of complex real-world scenes as input. We model the
underlying task as a video retrieval problem and present a two-stage, zero-shot
approach, combining the advantages of the SOTA tracking model FastTracker and
Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our
method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which
won second place in the challenge.

</details>


### [34] [UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions](https://arxiv.org/abs/2511.03334)
*Guozhen Zhang,Zixiang Zhou,Teng Hu,Ziqiao Peng,Youliang Zhang,Yi Chen,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: 现有的音视频生成方法在口型同步和语义一致性方面表现不足。本文提出了UniAVGen，一个统一的音视频联合生成框架，通过双分支扩散Transformer架构和创新的跨模态交互机制，实现高质量的音视频协同生成，并在少量数据下取得了优越效果。


<details>
  <summary>Details</summary>
Motivation: 当前开源音视频生成方法在处理模态间的协同表达时，面临口型与声音不同步、语义一致性差等问题，限制了生成音视频内容的质量。因此亟需更加有效的跨模态建模方法。

Method: 提出了UniAVGen框架，包括双分支扩散Transformer（DiTs）构建统一的跨模态潜在空间，设计了非对称跨模态交互机制，实现模态间的双向时序对齐跨注意力；引入人脸感知调制模块，动态增强重要区域；并在推理时用模态感知无分类引导方法加强信号相关性。

Result: 在仅用130万训练样本的情况下，UniAVGen在音视频同步、音色一致性、情感一致性等方面明显优于以往方法（对比数据集为3010万），验证了其高效、优越的跨模态生成能力。

Conclusion: UniAVGen提供了有效且统一的音视频联合生成解决方案，在核心生成任务上表现突出，为音视频生成领域提供了更强、更通用的工具，并展示了在数据有限场景下的应用潜力。

Abstract: Due to the lack of effective cross-modal modeling, existing open-source
audio-video generation methods often exhibit compromised lip synchronization
and insufficient semantic consistency. To mitigate these drawbacks, we propose
UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
anchored in a dual-branch joint synthesis architecture, incorporating two
parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
enables bidirectional, temporally aligned cross-attention, thus ensuring
precise spatiotemporal synchronization and semantic consistency. Furthermore,
this cross-modal interaction is augmented by a Face-Aware Modulation module,
which dynamically prioritizes salient regions in the interaction process. To
enhance generative fidelity during inference, we additionally introduce
Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
synthesis design enables seamless unification of pivotal audio-video tasks
within a single model, such as joint audio-video generation and continuation,
video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
UniAVGen delivers overall advantages in audio-video synchronization, timbre
consistency, and emotion consistency.

</details>


### [35] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉与语言模型提示学习方法AAPL，通过引入属性级图像增强和对抗性token嵌入提升模型在零样本任务中的泛化能力，并在多个基准数据集上实现了优越表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基于提示学习的视觉语言模型在零样本任务上有很大进步，但现有方法主要关注文本提示，对于图片层面的属性可变性增强关注不足，导致泛化到全新类别时性能有限。本文希望解决这一问题，提升提示学习方法在视觉特征泛化上的能力。

Method: 作者提出AAPL方法，结合属性驱动的图片增强与对抗性token嵌入，将无关的表象变化与类别相关的语义表征分离，从而引导提示向更具视觉判别性的特征聚焦。并在软提示框架下验证增强—提示的协同。

Result: 在11个基准数据集的few-shot、zero-shot、跨数据集和领域泛化设置下，AAPL的表现均优于现有主流方法（如CoOp、CoCoOp等）。

Conclusion: 本文证明了属性级图像增强与对抗性嵌入结合提示学习可以显著提升模型泛化性，AAPL简单有效，有望推动视觉语言模型在现实世界复杂环境下的应用。

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


### [36] [Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort](https://arxiv.org/abs/2511.03416)
*Nikolai Herrmann,Marcella C. Zijta,Stefan Klein,Régine P. M. Steegers-Theunissen,Rene M. H. Wijnen,Bernadette S. de Bakker,Melek Rousian,Wietske A. P. Bastiaansen*

Main category: cs.CV

TL;DR: 该论文提出了一种利用PCA和多种自动方法，实现胚胎在三维超声图像中标准化对齐的流程，提高早孕阶段胚胎可重复分析的准确性，可用于临床和科研。


<details>
  <summary>Details</summary>
Motivation: 目前三维超声胚胎图像的分析受限于手工对齐的低效率与一致性差异，标准化对齐有助于生长监测和跨扫描对比，但自动化方法尚不完善。

Method: 首先通过分割获得胚胎掩膜，应用主成分分析(PCA)提取主轴，生成四个候选方向。接着结合Pearson相关性启发式、和标准图谱的归一化互相关图像匹配，以及随机森林分类器三种方法，自动选择标准方向。

Result: 在1043例孕妇2166张三维超声图像中，PCA在99.0%图像上准确获取主轴，三种方向选择法正确率分别为97.4%、95.8%、98.4%，三者多数投票达98.5%。

Conclusion: 该方法在早孕超声图像中，可实现高准确率自动化、标准化对齐，提升胚胎分析效率和一致性，对科研和临床均有推广价值，代码已开源。

Abstract: Standardized alignment of the embryo in three-dimensional (3D) ultrasound
images aids prenatal growth monitoring by facilitating standard plane
detection, improving visualization of landmarks and accentuating differences
between different scans. In this work, we propose an automated method for
standardizing this alignment. Given a segmentation mask of the embryo,
Principal Component Analysis (PCA) is applied to the mask extracting the
embryo's principal axes, from which four candidate orientations are derived.
The candidate in standard orientation is selected using one of three
strategies: a heuristic based on Pearson's correlation assessing shape, image
matching to an atlas through normalized cross-correlation, and a Random Forest
classifier. We tested our method on 2166 images longitudinally acquired 3D
ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional
Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,
PCA correctly extracted the principal axes of the embryo. The correct candidate
was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,
95.8%, and 98.4% of images, respectively. A Majority Vote of these selection
methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline
enables consistent embryonic alignment in the first trimester, enabling
scalable analysis in both clinical and research settings. The code is publicly
available at:
https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.

</details>


### [37] [Generalizing Shape-from-Template to Topological Changes](https://arxiv.org/abs/2511.03459)
*Kevin Manogue,Tomasz M Schang,Dilara Kuş,Jonas Müller,Stefan Zachow,Agniva Sengupta*

Main category: cs.CV

TL;DR: 本文提出了一种能够处理中拓扑变化情况下，基于模板的三维表面重建方法。实验结果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 以往的形状从模板重建（SfT）方法在遇到如撕裂、切割等拓扑变化时会失效，实际应用中这类问题非常常见，因此需要一种对拓扑变化敏感且鲁棒的解决方案。

Method: 方法以传统SfT为初始，通过分割三维模板的空间域并联合最大化物理可行性和重投影一致性的能量函数，不断自适应调整模板，实现对拓扑变化的处理。

Result: 方法能稳健地处理各种常见的二维表面拓扑变化（如撕裂和切割），并在合成和真实数据集上均优于基线方法。

Conclusion: 文章提出了首个通用的支持拓扑变化的SfT框架，实验证明其优越性和广泛适用性。

Abstract: Reconstructing the surfaces of deformable objects from correspondences
between a 3D template and a 2D image is well studied under Shape-from-Template
(SfT) methods; however, existing approaches break down when topological changes
accompany the deformation. We propose a principled extension of SfT that
enables reconstruction in the presence of such changes. Our approach is
initialized with a classical SfT solution and iteratively adapts the template
by partitioning its spatial domain so as to minimize an energy functional that
jointly encodes physical plausibility and reprojection consistency. We
demonstrate that the method robustly captures a wide range of practically
relevant topological events including tears and cuts on bounded 2D surfaces,
thereby establishing the first general framework for topological-change-aware
SfT. Experiments on both synthetic and real data confirm that our approach
consistently outperforms baseline methods.

</details>


### [38] [Human Mesh Modeling for Anny Body](https://arxiv.org/abs/2511.03589)
*Romain Brégier,Guénolé Fiche,Laura Bravo-Sánchez,Thomas Lucas,Matthieu Armando,Philippe Weinzaepfel,Grégory Rogez,Fabien Baradel*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Anny的人体参数模型，无需3D扫描，基于开源数据和人类测量学知识，能够高效生成不同形态的人体模型，并开源发布。


<details>
  <summary>Details</summary>
Motivation: 现有的人体参数模型依赖昂贵的3D扫描及受限、专有的形状空间，难以广泛代表全球及不同人群的形态变化，需要一种成本低且更具代表性的解决方案。

Method: Anny模型利用MakeHuman社区的开源人体测量学知识，结合WHO人口统计数据，通过连续可解释的形状空间参数（如性别、年龄、身高、体重）进行人体建模，实现无需扫描的形态变换，模型完全可微分，并支持统一表达人类各种形态。

Result: Anny模型实现了毫米级精度的扫描拟合、可控的高质量合成人体数据生成，并支持HMR任务。同时开发了Anny-One数据集，内含80万人体模型，实验显示基于Anny训练的HMR模型性能可与扫描数据驱动的模型媲美。

Conclusion: Anny提供了一种开放、可解释且具代表性的人体建模基础，便于学术和工业界广泛应用。其开源发布将促进3D人体建模领域的发展和创新。

Abstract: Parametric body models are central to many human-centric tasks, yet existing
models often rely on costly 3D scans and learned shape spaces that are
proprietary and demographically narrow. We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community. Anny defines a continuous,
interpretable shape space, where phenotype parameters (e.g. gender, age,
height, weight) control blendshapes spanning a wide range of human forms --
across ages (from infants to elders), body types, and proportions. Calibrated
using WHO population statistics, it provides realistic and demographically
grounded human shape variation within a single unified model. Thanks to its
openness and semantic control, Anny serves as a versatile foundation for 3D
human modeling -- supporting millimeter-accurate scan fitting, controlled
synthetic data generation, and Human Mesh Recovery (HMR). We further introduce
Anny-One, a collection of 800k photorealistic humans generated with Anny,
showing that despite its simplicity, HMR models trained with Anny can match the
performance of those trained with scan-based body models, while remaining
interpretable and broadly representative. The Anny body model and its code are
released under the Apache 2.0 license, making Anny an accessible foundation for
human-centric 3D modeling.

</details>


### [39] [Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals](https://arxiv.org/abs/2511.03645)
*Vittal L. Rao*

Main category: cs.CV

TL;DR: 作者提出了一种基于信号强度加权的坐标表示方法，用于提升生物医学数据的定位任务表现，并在一维与二维数据上均取得了较优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的坐标通道方法（如CoordConv）仅加入单纯的坐标信息，未利用信号强度与位置之间的关联性，而生物医学数据常需要模型捕捉复杂空间或时序特征。作者希望引入信号强度与坐标的耦合，从而提供更有意义的输入表征，提升模型定位能力。

Method: 提出将输入信号的坐标通道乘以本地信号强度，形成一种信号强度加权的坐标表示，赋予模型强度-位置的先验关系，该方法为简洁且适用于不同模态的归纳偏置。该方法在两个任务上进行了验证：(1) 基于心电图（ECG）信号预测形态转变的时间点；(2) 在细胞图像中回归核中心坐标。

Result: 新方法在心电图时序定位和细胞核定位这两个任务中，相较传统坐标通道方法表现出更快的收敛速度和更强的泛化能力。

Conclusion: 信号强度加权的坐标表示法可有效提升模型在一维和二维生物医学定位任务中的表现，具有良好的跨模态泛化性和实际应用潜力。

Abstract: Localisation tasks in biomedical data often require models to learn
meaningful spatial or temporal relationships from signals with complex
intensity distributions. A common strategy, exemplified by CoordConv layers, is
to append coordinate channels to convolutional inputs, enabling networks to
learn absolute positions. In this work, we propose a signal intensity-weighted
coordinate representation that replaces the pure coordinate channels with
channels scaled by local signal intensity. This modification embeds an
intensity-position coupling directly in the input representation, introducing a
simple and modality-agnostic inductive bias. We evaluate the approach on two
distinct localisation problems: (i) predicting the time of morphological
transition in 20-second, two-lead ECG signals, and (ii) regressing the
coordinates of nuclear centres in cytological images from the SiPaKMeD dataset.
In both cases, the proposed representation yields faster convergence and higher
generalisation performance relative to conventional coordinate-channel
approaches, demonstrating its effectiveness across both one-dimensional and
two-dimensional biomedical signals.

</details>


### [40] [A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential](https://arxiv.org/abs/2511.03665)
*Mehdi Sefidgar Dilmaghani,Francis Fowley,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出了一种轻量级的三维卷积神经网络（3DCNN）用于基于事件视觉数据的人体活动识别，兼具高精度和隐私保护，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 当前人类监测系统使用的传统相机易泄露个人隐私，而事件相机仅记录像素强度变化，本身具有隐私保护优势。如何在保持隐私的前提下，实现高效准确的人体活动识别，是一个重要挑战。

Method: 设计了紧凑型3DCNN架构，充分建模空间和时间动态；通过焦点损失结合类别重加权和有针对性的数据增强方法，缓解类别不均衡并提升模型泛化能力。在Toyota Smart Home和ETRI数据集组合上进行训练与测试。

Result: 实验获得F1分数为0.9415，总体准确率为94.17%，优于C3D、ResNet3D及MC3_18等主流3D-CNN架构，提升高达3%。

Conclusion: 该方法在保证隐私的基础上，实现了高效且高精度的人体活动识别，适用于实际边缘计算应用，展示了基于事件视觉深度学习在人体动作识别领域的巨大应用潜力。

Abstract: This paper presents a lightweight three-dimensional convolutional neural
network (3DCNN) for human activity recognition (HAR) using event-based vision
data. Privacy preservation is a key challenge in human monitoring systems, as
conventional frame-based cameras capture identifiable personal information. In
contrast, event cameras record only changes in pixel intensity, providing an
inherently privacy-preserving sensing modality. The proposed network
effectively models both spatial and temporal dynamics while maintaining a
compact design suitable for edge deployment. To address class imbalance and
enhance generalization, focal loss with class reweighting and targeted data
augmentation strategies are employed. The model is trained and evaluated on a
composite dataset derived from the Toyota Smart Home and ETRI datasets.
Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy
of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,
and MC3_18 by up to 3%. These results highlight the potential of event-based
deep learning for developing accurate, efficient, and privacy-aware human
action recognition systems suitable for real-world edge applications.

</details>


### [41] [Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection](https://arxiv.org/abs/2511.03666)
*Dongkeun Kim,Minsu Cho,Suha Kwak*

Main category: cs.CV

TL;DR: 本论文提出了一种以身体部位感知为基础的自下而上群体推理框架，用于细粒度社会互动检测，并在NVI数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的社会互动检测方法忽视了面部表情、目光和手势等细致线索，主要依赖整体表征，且直接检测社交群体而没有显式建模个体之间的底层互动，导致不能有效捕捉局部社交信号，群体划分存在歧义。

Method: 该方法首先检测个体，并利用身体部位信息增强其特征，然后通过基于相似性的推理将个体关联为群体。这个推理过程综合考虑空间关系与细微社交线索，实现更准确的群体配置推断。

Result: 在NVI数据集上的实验显示，该方法优于以往方法，达到了新的性能最高水平（SOTA）。

Conclusion: 身体部位感知和细粒度信号的建模能够提升社交群体推断的精度，所提框架在实际数据集上有效验证了其优势。

Abstract: Social interactions often emerge from subtle, fine-grained cues such as
facial expressions, gaze, and gestures. However, existing methods for social
interaction detection overlook such nuanced cues and primarily rely on holistic
representations of individuals. Moreover, they directly detect social groups
without explicitly modeling the underlying interactions between individuals.
These drawbacks limit their ability to capture localized social signals and
introduce ambiguity when group configurations should be inferred from social
interactions grounded in nuanced cues. In this work, we propose a part-aware
bottom-up group reasoning framework for fine-grained social interaction
detection. The proposed method infers social groups and their interactions
using body part features and their interpersonal relations. Our model first
detects individuals and enhances their features using part-aware cues, and then
infers group configuration by associating individuals via similarity-based
reasoning, which considers not only spatial relations but also subtle social
cues that signal interactions, leading to more accurate group inference.
Experiments on the NVI dataset demonstrate that our method outperforms prior
methods, achieving the new state of the art.

</details>


### [42] [Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition](https://arxiv.org/abs/2511.03725)
*Jongseo Lee,Wooil Lee,Gyeong-Moon Park,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本论文提出一种名为DANCE的新框架，能将视频中动作与场景内容解释有效解耦，从而更清晰地解释视频动作识别模型的判别逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有的视频动作识别解释方法，将动作与空间场景混在一起，导致难以判断模型是在关注动作本身还是场景背景，解释性不佳。基于语言的方法又难以精准描述动作动态。为了解决这两个问题，作者提出该工作。

Method: DANCE框架通过将动作动力学（以姿态序列形式）、物体概念和场景概念三种概念类型进行解耦，构建输入。从而要求模型只通过这些明确定义的概念进行分类。动作概念通过姿态序列表达，物体和场景信息则通过大语言模型自动提取。整个体系采用主动（ante-hoc）概念瓶颈方法实现。

Result: 在KTH、Penn Action、HAA500、UCF-101四个数据集上的实验表明，DANCE能显著提升模型解释的清晰度，同时保持与传统模型相当的分类性能。用户调研验证了其解释的优越性，且其结构便于模型调试、编辑和失效分析。

Conclusion: DANCE框架通过解耦动作和场景，提高了视频动作识别模型的可解释性，且具备良好的实用性和推广前景。

Abstract: Effective explanations of video action recognition models should disentangle
how movements unfold over time from the surrounding spatial context. However,
existing methods based on saliency produce entangled explanations, making it
unclear whether predictions rely on motion or spatial context. Language-based
approaches offer structure but often fail to explain motions due to their tacit
nature -- intuitively understood but difficult to verbalize. To address these
challenges, we propose Disentangled Action aNd Context concept-based
Explainable (DANCE) video action recognition, a framework that predicts actions
through disentangled concept types: motion dynamics, objects, and scenes. We
define motion dynamics concepts as human pose sequences. We employ a large
language model to automatically extract object and scene concepts. Built on an
ante-hoc concept bottleneck design, DANCE enforces prediction through these
concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101
-- demonstrate that DANCE significantly improves explanation clarity with
competitive performance. We validate the superior interpretability of DANCE
through a user study. Experimental results also show that DANCE is beneficial
for model debugging, editing, and failure analysis.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [43] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为ARC的新型缓存机制，用于高效管理和动态更新RAG驱动的LLM代理的高价值小型语料库，大幅提升检索效率与效果。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG技术提升了LLM代理的性能，但如何为每个代理动态构建并维护一套精简且高相关的语料库（缓存）尚未得到充分研究。

Method: 提出ARC（Agent RAG Cache Mechanism）机制：该框架无需人工标注，通过结合历史查询分布和缓存项的嵌入空间几何特征，自动动态管理每个代理的小型高相关缓存。

Result: 在三个检索数据集上实验，ARC将存储需求降至原始的0.015%，has-answer率最高可达79.8%，平均检索延迟减少80%。

Conclusion: ARC显著提升了RAG驱动LLM代理的效率与效果，实现了高效的缓存智能管理。

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [44] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian García-Romero,Miquel Esplà-Gomis,Felipe Sánchez-Martínez*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过利用多语种机器翻译模型的内部表达，高效区分人类翻译与机器翻译句子，并优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译系统依赖于大量互联网平行语料，但大量语料为机器翻译结果，过度使用合成内容会降低翻译质量，因此过滤非人工翻译成为必要步骤。

Method: 作者提出直接利用代理多语种MT模型的内部表示做判别，根据内部特征区分人类翻译与机器翻译，构建相应判别算法。

Result: 实验表明，该方法在检测人译/机译句子上显著优于SOTA，尤其是非英文语言对至少提升5个百分点准确率。

Conclusion: 本文方法更有效区分人类与机器翻译，可提升高质量MT系统数据预处理能力。

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [45] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架LEGO-Eval和详细指令基准LEGO-Bench，以提升对细粒度指令生成3D场景的对齐度评估，并发现现有方法在此任务上的表现很有限。


<details>
  <summary>Details</summary>
Motivation: 当前大模型自动生成3D场景在空间布局和物体属性方面与真实环境有较大差距，主要因为输入指令过于粗糙。此外，现有的评估方法难以准确判断生成场景与精细指令的匹配度，影响对体感智能体的训练效果。因此急需更细致的评估和基准。

Method: 作者提出了LEGO-Eval评估框架，引入多样化工具显式锚定场景组件，从而提升场景-指令对齐度的评估准确性。同时构建了LEGO-Bench基准，收集了描述复杂布局和属性的精细指令，用于系统评价各种3D场景生成方法。方法通过实验对LEGO-Eval与VLM类模型进行了比较。

Result: 实验结果表明，LEGO-Eval在场景-指令对齐度评估任务中，F1得分比主流VLM-as-a-judge方法高0.41。利用LEGO-Bench对当前主流方法进行了基准测试，结果显示，所有方法在精细指令完全对齐的3D场景生成任务上的成功率最高不超过10%。

Conclusion: 本文的LEGO-Eval显著提升了3D场景与细粒度指令的对齐评估能力，但现有3D生成技术在真实细致场景构建方面还存在很大不足，未来需进一步推动高对齐度生成方法的研究。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [46] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: 提出了一种ARF（分析-修改-微调）流程，让小型开源大模型在客户服务摘要任务上超过了更大的闭源模型。


<details>
  <summary>Details</summary>
Motivation: 目前大部分高性能摘要任务依赖参数量大且昂贵的闭源模型（如GPT-3.5），这带来高成本和数据隐私等问题。

Method: ARF流程包含三步：先分析GPT-3.5生成摘要的常见错误类型，然后用较小的编辑模型（Llama 3.1 70B）针对性修正摘要数据，最后用这些高质量数据微调一个更小的学生模型（Llama 3.1 8B）。

Result: 微调后的小模型（Llama 3.1 8B）在客户服务摘要任务上的效果优于GPT-3.5。

Conclusion: ARF流程可提升开源模型性能，同时降低费用并改善数据隐私，具备在多种下游任务中泛化应用的潜力。

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [47] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 本文针对细粒度观点挖掘（ABSA）在教育等低资源领域的应用挑战，提出了新的评估方法（FTS-OBP）、系统性探索了小型生成式模型的训练并发布了领域资源。


<details>
  <summary>Details</summary>
Motivation: ABSA方法及资源主要集中于商业领域，教育和医疗等高需求但低资源领域被忽略；现有方法需大量资源且适应新领域困难，传统评价方式对生成模型不够灵活，低估其能力。

Method: （1）提出了FTS-OBP评估方法，能灵活接受边界变化且提供细致诊断；（2）首次针对参数小于7B的小型生成式模型（SLMs）做ABSA研究，通过教育评论领域案例探索无数据与少数据训练方法，并提出多任务微调策略；（3）公开教育评论ABSA资源。

Result: 多任务微调大幅提升SLMs性能，使1.5-3.8B规模模型用200-1000样本即可超越专有大模型并逼近基准结果；FTS-OBP与传统指标高度相关，更适合生成模型；丰富了低资源领域的可用ABSA数据。

Conclusion: 新评估法和微调策略有效推动了低资源领域ABSA发展，小型生成模型在少量样本下即可高效应用，为教育等领域提供了实用工具和公共数据支持。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [48] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 该论文介绍了ROBOTO2，一个用于辅助临床试验偏倚风险（ROB）评估的开源网页平台，实现了LLM辅助的自动化与人工结合流程。作者还发布了数据集与代码，并对多种大模型自动化评估性能进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 临床试验的偏倚风险评估过程耗时费力，且需要专家参与。作者希望通过自动化和大模型辅助，降低工作量、提升标准化水平。

Method: 作者开发了一个网页平台ROBOTO2，将PDF解析、检索增强型大模型问答与人工审核流程结合，实现半自动化的偏倚风险评估。用户可上传报告、获得大模型生成的答案并实时修正。此外，作者还构建并发布了标注数据集，并用该数据集对多个大模型进行基准评估。

Result: ROBOTO2平台上线并开源，数据集和代码公开。通过新构建的数据集，作者评估了4种主流LLM在ROB2任务上的表现，发现大模型已能辅助部分流程，但在自动化全面评估上仍存在挑战。

Conclusion: ROBOTO2有助于提升临床试验偏倚风险评估的效率和一致性。虽然大模型表现提升，但目前仍需人机协作。发布的数据集和工具将促进领域研究与实际应用。

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [49] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文提出了只记录对话单方内容情景下的人工智能对话理解与生成问题，并构建了相关任务与解决方案，结果显示在缺失信息下AI模型依然可以实现有效内容重构和总结。


<details>
  <summary>Details</summary>
Motivation: 在实际应用，如远程医疗、呼叫中心和智能眼镜等场景中，受隐私等约束往往只能获得一方的对话内容，如何让人工智能模型仅凭单边信息理解、重建完整对话并做出总结成为一个亟需解决的问题。

Method: 作者正式提出"one-sided conversation problem (1SC)"，并设计了两个任务：一是根据记录方内容实时重建对方说话内容，二是直接基于单边内容生成对话摘要。在MultiWOZ、DailyDialog和Candor数据集上，采用了多种提示（prompting）及微调模型，通过人工A/B测试和大语言模型评估，考察不同信息补充方式、模型规模、提示策略对表现的影响。

Result: （1）若可获得未来一句和话语长度信息，可显著提升内容重构效果；（2）使用占位符提示（placeholder prompting）能减轻幻觉现象；（3）大模型基于提示即可生成高质量重建内容，而小模型需微调才具备该能力；（4）不需恢复所有轮次也可直接获得较高质量摘要。

Conclusion: 一方面，作者首次系统性提出并实验验证了1SC问题，并发现AI模型在单方对话数据基础上依然可以实现重建与有效摘要，为隐私保护下的对话AI带来希望。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [50] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: 本文提出PolyNorm，一种基于大语言模型的提示式文本规范化方法，在八种语言上实现了更低的词错误率，并同时发布了多语种基准数据集，降低了人工规则依赖。


<details>
  <summary>Details</summary>
Motivation: 传统的文本规范化系统虽然准确，但需要大量人工工程，难以扩展到多语种和低资源环境。本文试图解决手工规则依赖和扩展性差的问题。

Method: 提出了PolyNorm，即利用大语言模型和prompt（提示词）驱动，将文本规范化任务交由模型完成，减少手工规则设计，同时开发了通用化的数据整理及评估流程。

Result: 在八种不同语言的数据集上实验，PolyNorm系统显著降低了与传统系统对比的词错误率。

Conclusion: PolyNorm具有可扩展性和多语言适应力，有望减少人力成本、拓宽语言覆盖。对应基准数据集的发布也为后续相关研究提供了便利。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [51] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 本文利用计算语言学方法（surprisal与语义连贯性）量化精神分裂症患者语言障碍特点，并分析其与症状严重程度的关系。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症常见症状之一为语言紊乱，包括言语组织混乱和语篇连贯性受损。量化并客观评估这些特征对于诊断和衡量病情有重要价值。

Method: 通过使用surprisal和语义连贯性这两种计算语言学指标，分析精神分裂症患者与健康对照组的自发表达数据，探讨指标与症状严重程度的对应关系。

Result: 结果显示精神分裂症患者在surprisal和语义连贯性指标上与健康人存在差异，这些语言指标随症状严重度变化。

Conclusion: surprisal和语义连贯性等计算语言指标可作为精神分裂症语言障碍的客观度量，有助于症状评估和疾病诊断。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [52] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: 本文介绍了首个大规模自动注释的阿拉伯语Reddit心理健康数据集（CARMA），覆盖六种心理健康状况及对照组，并验证了其用于心理健康检测的有效性。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题在全球范围内普遍存在，早期检测十分困难，阿拉伯语人群因资源和文化原因更为显著。现有大多数心理健康检测研究聚焦英语领域，阿拉伯语却因缺乏注释数据集而被严重忽视。

Method: 构建CARMA数据集，包含六类心理健康状况和对照组。对用户的词汇和语义特征进行定性和定量分析，通过浅层分类器到大型语言模型进行分类实验，评估该数据集在心理健康检测任务中的价值。

Result: CARMA成为当前规模和多样性最优秀的阿拉伯语心理健康数据集。分析揭示了各心理健康状况的语言特征。分类实验表明该数据集有助于提升阿拉伯语心理健康检测的效果。

Conclusion: 阿拉伯语心理健康自动检测领域将因CARMA数据集获得极大推动，有望改善目前阿拉伯语弱势群体心理健康早期发现的困境。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [53] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 提出一种基于控制屏障函数（CBF）的LLM对齐方法，无需微调，通过安全过滤器实时干预生成文本，提升生成文本的正向性。


<details>
  <summary>Details</summary>
Motivation: 当前对齐大语言模型（LLM）通常依赖于微调，但微调成本高、效率低，且缺乏灵活的在线干预机制。该研究动机在于设计一种无需微调、可模块化添加、并能确保模型输出符合用户期望的方法。

Method: 采用控制屏障函数（CBF）作为安全过滤器，对LLM生成的预测token进行筛查和干预，实时阻止或引导输出不符合/符合期望标准的文本。该过滤器可以与任何基础LLM搭配使用，并且能结合有现成评估模型时的对齐目标。

Result: 利用开源语言模型实现了该系统，用于生成正面积极的文本，实验结果显示系统可以有效提升文本生成的对齐和安全性。

Conclusion: 基于CBF安全过滤器的框架能高效实现LLM对齐，并具备无需微调、易于集成和灵活应用等优势，是控制和引导大语言模型输出的新途径。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [54] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出了一个针对多模态大语言模型（MLLMs）认知能力的评测基准MME-CC，系统评估了当前模型在视觉中心推理任务中的表现，并揭示现有模型在空间和几何推理方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准过于侧重文本推理，缺乏对以视觉为核心的认知推理行为的系统考察，因此无法充分评估MLLMs的认知能力。

Method: 作者提出MME-CC基准，将11个代表性推理任务归为空间、几何和知识三类视觉信息，并对16个主流MLLMs进行了细致评测和误差分析。

Result: 实验发现闭源模型（如Gemini-2.5-Pro）整体表现优于开源模型，但空间和几何推理普遍较弱（最高达30%），常见错误包括方向感混淆、跨视角身份保持脆弱、反事实指令理解不佳。此外，推理过程一般分三步：提取-推理-验证，对视觉提取依赖较大。

Conclusion: MME-CC揭示了MLLMs在视觉认知推理上的短板，呼吁研究社区把认知能力作为评估与模型设计的核心方向。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [55] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本论文提出了一种以利害关系人为核心的AI系统风险评估框架，利用大语言模型（LLM）来预测并解释不同群体的风险感知，实现对多方观点的透明展示。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的部署常涉及医疗、自动驾驶、反欺诈等高风险领域，利害关系人对风险的认知差异显著，缺乏透明和可解释的多方风险评估工具，影响了AI的负责任使用。

Method: 作者结合Risk Atlas Nexus和GloVE解释方法，使用LLM模拟评判者，对AI系统在特定场景下的风险进行多利益相关方的细致评估，并用可视化手段展示利害关系人之间的风险认知分歧与冲突成因。

Result: 通过在医疗AI、自动驾驶和欺诈检测三个真实应用案例中的验证，结果发现利害关系人的视角对风险认知和冲突模式有显著影响。提出的方法能够生成各方可解释、针对性的政策建议和分歧展示。

Conclusion: 论文强调，基于利害关系人的风险解释方法对于实现透明、可解释和以人为本的AI治理至关重要，有助于提升LLM在风险评估场景中的实际应用价值。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [56] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLM）输出结果的不确定性估计（UE）方法进行了全面的实证分析，比较了多种UE方法在不同任务和数据分布下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用的普及，其输出结果的可信度变得极为重要。不确定性估计能够帮助识别模型输出的可靠程度，但现有方法众多、各自优劣未明，因此需要系统性地评估各种UE方法的有效性和稳健性。

Method: 作者选取了12种不确定性估计方法，结合4种生成质量评价指标，在问答任务下针对分布内（ID）和分布外（OOD）两类数据，系统测试了这些方法的表现，并重点考察信息型、密度型与语义一致性等主流UE方法。

Result: 信息型方法（利用token和序列概率）在分布内任务表现最佳；密度型方法和P(True)指标在分布外情境下效果突出，能更好反映模型的认知不确定性。语义一致性方法在不同数据集和评测指标下表现稳健，但未必总是最佳方案。

Conclusion: 不同类型的不确定性估计方法在不同场景下各有优势，需针对具体应用选择合适方法。综合使用多种UE方式可提升对LLM输出可信度的判断能力，但应避免一种方法通用化的误区。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [57] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: 该论文提出了BengaliMoralBench，这是第一个面向孟加拉语及其社会文化背景的大规模伦理基准，用以评估多语种大模型在本地伦理适应性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的伦理评测大多以英语和西方框架为主，忽视了本地文化和语言（如使用人口众多的孟加拉语）的伦理差异，这使得大模型的实际部署可能存在文化不适应和伦理偏差的问题。

Method: 设计BengaliMoralBench，涵盖五大伦理领域（如日常活动、习惯、亲子关系等）及50个子话题。每个情景由母语者依据“德性伦理”、“常识伦理”、“正义伦理”三重视角标注。对主流多语种LLM（Llama、Gemma、Qwen、DeepSeek）进行zero-shot评测，使用统一提示和标准评估指标。

Result: 主流多语种大模型在BengaliMoralBench上的准确率差异显著（50%-91%），定性分析发现模型在文化贴合、常识推理和伦理公平性方面普遍存在不足。

Conclusion: BengaliMoralBench为评估和提升多语种大模型在文化及伦理适应性上提供了重要基础，有助于在如孟加拉这样的多语言、低资源环境中，实现更具本地伦理价值观的AI应用部署。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [58] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新方法LGM，通过构建语言概念之间的关系图，提升大语言模型在面对用户含糊或概念不一致指令时的理解能力。实验结果显示LGM优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在遇到指令中存在歧义或概念用词不一致的情况时，理解和响应能力有限，需要有机制加强其概念区分和语义理解能力。

Method: 提出Language Graph Model（LGM），能从自然语言中提取元关系（如继承、别名、构成），并通过反思机制验证关系准确性。利用概念迭代检索算法，动态向LLM提供相关关系和概念描述以辅助理解和生成答案。与常规RAG不同，LGM不依赖于扩展上下文窗口，因此可以处理任意长度文本。

Result: 实验证明，在标准基准数据集上，LGM整体优于现有的RAG增强生成方法。

Conclusion: LGM通过元关系建模和反思机制，显著提升了大语言模型对复杂和模糊指令的理解能力，且突破了上下文长度的限制。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [59] [Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification](https://arxiv.org/abs/2511.03217)
*Shaghayegh Kolli,Richard Rosenbaum,Timo Cavelius,Lasse Strothe,Andrii Lata,Jana Diesner*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型、知识图谱与实时搜索代理的混合事实核查系统，有效提升了事实核查的准确性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在生成流畅文本方面表现优异，但经常缺乏可靠的事实依据。而基于知识图谱的事实核查器虽然证据精确且可解释，但覆盖率和实时性有限。该论文旨在融合二者优势，提升自动化事实核查的性能和可用性。

Method: 该系统流程分为三步：1）通过知识图谱（以DBpedia为例）进行快速一次跳检索；2）基于大语言模型的分类，结合任务定制化的提示词和内置规则逻辑输出判定结果；3）若知识图谱信息覆盖不足，则调用在线网页搜索代理进行补充查证。

Result: 在FEVER基准数据集的支持/反驳任务上，该系统在无任务特定微调的情况下获得了0.93的F1分数。此外，针对原标注为"信息不足"的样本进行复注，发现系统常常能够发现有效证据，且被专家与LLM审核确认。

Conclusion: 本文提出的模块化开源事实核查流程，具备覆盖不足时的回退机制，可跨数据集泛化，显著提升了自动化事实核查的可靠性与灵活性。

Abstract: Large language models (LLMs) excel in generating fluent utterances but can
lack reliable grounding in verified information. At the same time,
knowledge-graph-based fact-checkers deliver precise and interpretable evidence,
yet suffer from limited coverage or latency. By integrating LLMs with knowledge
graphs and real-time search agents, we introduce a hybrid fact-checking
approach that leverages the individual strengths of each component. Our system
comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid
one - hop lookups in DBpedia, 2) an LM-based classification guided by a
task-specific labeling prompt, producing outputs with internal rule-based
logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.
Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the
Supported/Refuted split without task- specific fine - tuning. To address Not
enough information cases, we conduct a targeted reannotation study showing that
our approach frequently uncovers valid evidence for claims originally labeled
as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
reviewers. With this paper, we present a modular, opensource fact-checking
pipeline with fallback strategies and generalization across datasets.

</details>


### [60] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: 本报告介绍了ISI团队在IARPA MATERIAL项目中的SARAL系统，提出了一种新的跨语言信息检索方法，强调可检索与查询相关的文档集合。该方法在Farsi、Kazakh和Georgian三种语言的多项评测中取得领先。


<details>
  <summary>Details</summary>
Motivation: 跨语言信息检索（CLIR）是多语种信息获取领域的重要难题。许多现有方法只关注排名结果，而没有关注查询相关文档集合的获取，因此需要新的方法来提升CLIR的效果和适应性。

Method: 团队开发了一种新的CLIR方法，重点是能检索与查询高度相关的文档集合，而不仅仅是返回一个排序的文档列表，并在MATERIAL项目框架下进行了实验。

Result: 在MATERIAL第三阶段的评测中，该团队的方法在Farsi、Kazakh和Georgian三种语言的六个评测条件中的五个表现超过了其他队伍。

Conclusion: SARAL系统在多语言、跨语言检索任务中取得了显著的效果，证明了其新的集合检索方法的有效性，并推动了CLIR领域的发展。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [61] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 该论文提出了IndicSuperTokenizer，一种专为印度多语种大型语言模型设计的新型分词器，结合了子词和多词分词策略，有效提升了分词质量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的分词器（如BPE）虽然广泛应用于多语种模型，但在支持不同文字脚本和丰富形态变化的印度语言时效果有限，亟需针对多语言特性设计更有效的分词方法。

Method: 提出IndicSuperTokenizer，结合子词分词、多词分词以及针对语言的预分词策略，使得分词更符合语言学规律。并在英语、22种印度语言及代码数据上设计实验，系统评估其效果。同时，对训练数据规模、词表大小、合并技术、预分词策略等设计进行了详细消融分析。

Result: IndicSuperTokenizer在fertility score上相较LLaMA4提升39.5%，相较Sutra提升18%。在推理效率上，相较LLaMA4提升44%，且在英印语基准任务上性能趋近。

Conclusion: IndicSuperTokenizer能显著提升印度多语种LLM的分词质量和推理速度，其设计策略在多层面上表现出较强的稳健性，为多语种模型分词器设计提供了新方向。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [62] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 本文比较了四个开源大模型与GPT-3.5在RAG增强下的问答表现，并发现Mistral-7b-instruct表现最佳，Orca-mini-v3-7b响应延迟最低。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）可以减少大模型幻觉，提升生成式AI的可靠性。伴随RAG和LLM的流行，研究者希望了解不同大模型在专业领域QA任务中的实际水平。

Method: 选取Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b四个开源模型及GPT-3.5，结合RAG进行计算机科学领域问答测试。评估指标包括二元问答的准确率与精确率、长答题的人工和Gemini模型排名、余弦相似度，以及平均响应延迟。

Result: GPT-3.5+RAG在二元与长答案任务上表现优异。开源模型中，Mistral-7b-instruct+RAG整体表现最佳，Orca-mini-v3-7b生成延迟最低，LLaMa2-7b-chat延迟最高。

Conclusion: 开源大模型结合RAG后性能与GPT-3.5相当，合理的基础设施可使其更具竞争力。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [63] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: 提出了一种新的大模型结构扩展方法SCALE，通过冻结模块参数并增加轻量级宽度扩展，在保持旧知识的同时持续预训练更好地获得新知识，实现了忘记与学习的新平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的持续预训练，单纯扩大参数已难以解决知识遗忘和新知识吸收间的矛盾，因此需探索如何结构性扩展模型能力以更好应对任务持续变化。

Method: 提出SCALE架构，在预训练模型的线性模块中插入轻量级扩展，并全部冻结原有权重，同时有三种实现方式（SCALE-Preserve、SCALE-Adapt和SCALE-Route），以不同策略在保持原模型功能的基础上扩展容量并引入新知识。

Result: 在合成传记基准测试中，SCALE有效减缓了模型深度扩展时的遗忘问题，并能学习到新知识。在韩语持续预训练任务上，SCALE各变体对英文任务遗忘最小，对韩语任务提升显著，取得稳定性与可塑性的最佳折中。

Conclusion: SCALE方法能在持续学习场景下，以结构化扩展和选择性训练方式较好地平衡知识保持与新知识适应，有效减缓遗忘，适合于大语言模型的后续持续预训练优化。

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [64] [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)
*Mauro Cettolo,Marco Gaido,Matteo Negri,Sara Papi,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文系统性研究了语音到文本翻译系统在无源文本的情况下面向源信息的自动评测方法，并提出了新的跨语言重分割算法，有效提升了评测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的语音翻译系统自动评测往往只依赖目标文本参考，无法充分利用源语音信息。机器翻译领域已有研究证明源文本有助于更准确地反映系统质量，但语音翻译因源为音频且源文本常缺失，使得源感知评测面临挑战。

Method: 一方面，利用自动语音识别(ASR)生成的转写文本和目标回译(back-translation)作为源语音的文本代理；另一方面，提出跨语言两步式重分割算法，解决合成源和参考翻译之间对齐不一致问题。

Result: 在涵盖79个语对、六种不同架构与水平的ST系统上实验表明，当ASR词错误率低于20%时，ASR转写比回译作为合成源更可靠；而回译则始终为高效可用的备选方案。所提重分割算法让源感知MT指标能稳健应用于ST评测。

Conclusion: 本文提出的源感知自动评测框架、两种合成源策略及重分割算法，有助于实现更准确、科学的语音翻译系统自动评测。

Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically
performed by comparing translation hypotheses with one or more reference
translations. While effective to some extent, this approach inherits the
limitation of reference-based evaluation that ignores valuable information from
the source input. In machine translation (MT), recent progress has shown that
neural metrics incorporating the source text achieve stronger correlation with
human judgments. Extending this idea to ST, however, is not trivial because the
source is audio rather than text, and reliable transcripts or alignments
between source and references are often unavailable. In this work, we conduct
the first systematic study of source-aware metrics for ST, with a particular
focus on real-world operating conditions where source transcripts are not
available. We explore two complementary strategies for generating textual
proxies of the input audio, automatic speech recognition (ASR) transcripts, and
back-translations of the reference translation, and introduce a novel two-step
cross-lingual re-segmentation algorithm to address the alignment mismatch
between synthetic sources and reference translations. Our experiments, carried
out on two ST benchmarks covering 79 language pairs and six ST systems with
diverse architectures and performance levels, show that ASR transcripts
constitute a more reliable synthetic source than back-translations when word
error rate is below 20%, while back-translations always represent a
computationally cheaper but still effective alternative. Furthermore, our
cross-lingual re-segmentation algorithm enables robust use of source-aware MT
metrics in ST evaluation, paving the way toward more accurate and principled
evaluation methodologies for speech translation.

</details>


### [65] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 该论文评估了多模态大模型(MLLMs)的推理（思维）模式在医学任务中的表现，发现推理模式提升有限，模型在复杂医学任务上表现仍较差。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型推出"推理模式"，人们希望对模型思维过程有更明确控制，从而提升在如医疗等复杂领域的表现。然而，尚不清楚该特性的实际价值，因此亟需系统评估。

Method: 作者选择了两个具有推理模式的领先MLLMs（Seed1.5-VL与Gemini-2.5-Flash），并用VQA-RAD和ROCOv2数据集，对它们在四项可视化医学任务中的表现进行了对比性测试，关注其推理模式和非推理模式下的结果差异。

Result: 实验证明：启用推理（思维）模式只带来边际性能提升，且在开放式医学VQA和医学图像解释等复杂任务表现依然较差。

Conclusion: 目前MLLMs仅凭推理模式无法显著提升医学领域表现。后续需要引入领域专有的医学数据以及更加先进的医学知识融合方法，才能有效提升多模态医学AI的效果。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [66] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 本文系统综述了生成式人工智能（GenAI）在生物信息学中的应用与进展，涵盖基因组学、蛋白质组学、结构生物学等领域，并提出了当前存在的问题及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着生物数据的爆炸式增长，传统生物信息学方法已难以应对其复杂性和规模。生成式AI技术因其强大的数据建模与生成能力，在复杂生物数据的分析、预测和创新应用中展现出巨大潜力，因此亟需系统梳理该领域的发展现状与挑战。

Method: 本文采用系统综述和元分析方法，围绕六个研究问题，对GenAI在生物信息学各子领域的应用案例、模型架构、数据集和评估方法进行系统梳理和对比分析。

Result: 1）GenAI在序列分析、分子设计和多模态数据建模等领域显著优于传统方法；2）针对特定任务预训练和定制架构的专用模型优于通用模型；3）分子分析和数据整合领域受益明显，复杂分析准确性提升，误差减少；4）结构建模、功能预测、合成数据生成表现良好并通过标准基准验证；5）主要约束为模型可扩展性与数据偏差，需强化多尺度评估和生物驱动的数据建模；6）丰富的分子、细胞和文本数据集支持GenAI模型的训练与泛化。

Conclusion: 生成式AI极大助力生物信息学领域的创新和效率提升，尤其在模式识别与复杂数据处理方面。但模型扩展性、泛化能力与数据偏差问题尚待解决，未来需发展更具鲁棒性和生物学解释性的GenAI方法。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [67] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了一种评估大型语言模型（LLMs）安全对齐后仍然存在的深层偏见的新方法，强调了传统公平性评估方案存在的盲区，并提出Silenced Bias Benchmark（SBB）以揭示被安全对齐掩盖的隐藏偏见。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs广泛应用于对公平性和无偏见有高要求的场景，但现有的公平性评估多通过问答（QA）方式，当模型拒绝回答时便被认为是公平的，这导致真实隐藏偏见未被发现和评估。认知到这种表面公平可能导致对模型实际表现的误判，驱动作者寻找更有效的深度评估方法。

Method: 作者提出了Silenced Bias Benchmark（SBB），通过激活引导技术减少模型在QA过程中的拒答，从而揭示被安全对齐掩盖的潜在不公平偏见。此基准能方便扩展到新的群体和议题，同时避免了以往方法中因提示工程或手工隐式查询带来的局限和额外偏见。

Result: 作者在多个主流大模型上应用SBB方法，发现模型表面上通过安全对齐拒答表现出公平，实际潜在空间中仍然保留着明显的群体偏见，表明现有的安全对齐策略未能真正消除模型内部的不公。

Conclusion: 该研究揭示了当前安全对齐LLMs在公平性上的严重盲区，SBB为深入发现和衡量这些掩盖偏见提供了有效工具，有助于推动更真实、更深入的AI公平性研究和模型改进。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [68] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: 本文提出了一种名为EQ-Negotiator的新框架，结合博弈论和隐马尔可夫模型，显著提升了小模型在信用谈判等情感交互中的智能与表现，并可在隐私受限的场景中运行。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因高昂的算力和数据隐私问题，不适用于隐私敏感、端侧的自动化谈判场景，而小模型虽具可行性却在处理复杂情感角色时远逊于大模型，因此亟需提升小模型相关能力。

Method: 设计EQ-Negotiator框架，将博弈论与隐马尔可夫模型融合，用于在线学习和追踪谈判中对方的情感状态，无需大规模的预训练，使小模型具备策略和情感识别能力来对抗操控、防止冲突升级，并保证伦理。

Result: 在各种信用谈判模拟中，7B参数的小模型经EQ-Negotiator增强后，实现了优于10倍参数规模以上LLM基线的债务回收与谈判效率，能有效应对如欺骗、威胁、装受害者等多种对抗策略。

Conclusion: 论文推动了情感角色建模从静态描述向动态情感结构转变，并证明了自动谈判中的关键在于策略性情感智能而非模型规模，推动了高效、伦理且兼顾隐私的AI谈判代理的发展。

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [69] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的符号逻辑控制的数据增强方法LFC-DA，能够在保证逻辑严密性的前提下生成多样化的自然语言问题，并显著提升逻辑推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前复杂逻辑数据增强依赖人工标注成本高昂，而直接用大语言模型生成数据则存在生成样本同质且缺乏逻辑可解释性的问题。作者希望解决如何有效生成兼具多样性和逻辑严密性的逻辑文本数据。

Method: 提出LFC-DA符号逻辑控制流程，包括将自然语言逻辑文本转为命题逻辑表达式、用规则库生成合法公式，并通过状态空间搜索确保公式多样性和有效性，最后再转回自然语言问题，实现了可控的逻辑数据增强。

Result: 在ReClor和LogiQA数据集上的实验显示，采用LFC-DA增强后的数据能显著提升预训练模型在逻辑推理任务上的准确率。

Conclusion: LFC-DA能够有效提升大模型指导下的逻辑数据增强质量，兼具生成多样性与逻辑严密性，对提升预训练模型逻辑推理能力有明显作用。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [70] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 论文研究了机器翻译中Byte Pair Encoding (BPE)分词的设置对翻译性能的影响，发现‘非对称BPE’（源语言和目标语言使用不同数量的合并操作）在不同语料规模和语言对上，尤其在低资源设置下，明显优于传统的‘对称BPE’方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译中分词模型参数通常采用源语言和目标语言相同的设置（对称BPE），但作者认为这种统一做法无法保证最佳翻译性能。论文动机是探索不同分词参数（特别是非对称设置）对多语言、不同语料量下机器翻译结果的影响。

Method: 作者系统性地比较了多种语料规模和多种语言对上BPE分词的参数配置，主要考察了对称BPE和非对称BPE的机器翻译效果差异，并通过具体实验量化了性能，并在多个语言对（包括低资源语言）上进行统计显著性测试。

Result: 结果显示，相比对称BPE，非对称BPE在英-印地语以及另外6对语言（如特卢古语、绍纳语、海萨语等）上均带来了统计学意义上的提升，特别是在低资源场景下，英-印地语语料量分别为50K、100K、500K时平均CHRF++提升分别为5.32、4.46和0.7。共12组实验中10组得到显著提升。作者发现最佳设定是源语言较高NMO（4K-32K），目标语言较低NMO（0.5K-2K）。

Conclusion: 建议未来机器翻译工作中，尤其在低资源场景下，优先采用分词参数非对称（源高、目标低）的BPE方案，以获得最优效果。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [71] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本文探索了小型语言模型（SLM）在利用SHACL形状指导下抽取RDF图中数据属性和对象属性的能力，发现稀有属性的长尾分布是主要瓶颈，并提出和对比了多种缓解数据不平衡的方法。


<details>
  <summary>Details</summary>
Motivation: 当前SLM在抽取RDF三元组、尤其是涉及不同属性类型时表现有限，主要因为数据不平衡导致对少见属性的提取能力不足。作者希望找出有效的训练方法，提升SLM对所有属性类型的抽取能力。

Method: 作者分析了属性分布长尾问题，通过多种策略（分层采样、加权损失、扩充数据规模、使用模板生成合成数据）来平衡训练集属性分布，并对比这些方法对模型性能的提升效果。

Result: 实验表明，最佳策略是在训练集中确保每种属性至少出现一定次数，这样模型在抽取不平衡属性时也表现优异。

Conclusion: 保持训练集各属性足够出现频次是缓解长尾效应、提升SLM抽取RDF图完整性的有效策略。相关数据集和代码已公开，有助于未来研究和实际应用。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [72] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 提出了3TF框架，通过在训练中显式引入推理过程，但推理时避免显式推理步骤，从而在保持输出简洁的同时提升模型内隐推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法提升大模型推理能力大多依赖显式推理输出（如Chain-of-Thought），推理时输出繁杂，压缩后效率虽上升，但仍需逐步推理。如何既提高推理准确率，又保证推理输出简洁，是未解决的问题。

Method: 提出3TF框架。第一步，训练模型同时具备推理模式与非推理模式。第二步，利用带有Chain-of-Thought注释的数据进一步训练，使模型在内部学会结构化推理。推理时则采用无推理的简洁输出模式，用训练中获得的能力隐式完成复杂推理。

Result: 实验显示，经过3TF训练的模型，在没有显式推理步骤地前提下，在多项推理基准上表现有大幅提升，说明模型已学会在内部完成高质量推理。

Conclusion: 3TF证明了无需显式推理输出，模型也能在内部完成复杂推理并保持高准确率，有望提升自动推理效率和输出质量。

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [73] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: 本文提出了QuestionRAG框架，以解决大型语言模型（LLM）在问答系统中因输入错误导致的误解和过度修正问题。该方法通过引入外部知识丰富输入，并利用强化学习（RL）实现更精准的纠错目标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在问答系统中处理用户输入错误时，容易产生误解用户意图或不必要地改写问题结构，影响模型的回答准确性。因此，提出新方法以提升问句纠错能力。

Method: QuestionRAG框架包含两个核心策略：1）利用外部知识（如搜索结果、相关实体）增强有问题的问题输入，帮助模型理解用户真实意图；2）采用强化学习对模型进行纠错目标对齐，避免仅进行同义改写，强调纠错的准确性，相比传统的监督微调（SFT）更为有效。

Result: 实验结果显示，利用知识增强可以显著提升模型对错误问题的理解能力；RL对齐方法相比SFT在提升指令跟随能力和模型泛化能力上更有效。

Conclusion: 融合知识增强与RL纠错对齐策略的QuestionRAG框架，显著提升了LLM在问句纠错任务中的性能，释放了大模型在该任务上的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [74] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: 本文介绍了CareMedEval数据集，用以评估LLMs在医学领域批判性解读和推理能力方面的表现，发现现有模型的表现仍有限，尤其在研究局限性和统计分析相关问题上较弱。


<details>
  <summary>Details</summary>
Motivation: 批判性解读科学文献是医学的重要技能，但现有大型语言模型在该领域的可靠性和推理能力仍然不足。因此，亟需一个专门的数据集来系统评估和促进LLMs的进步。

Method: 作者基于法国医学生的真实考试，构建了CareMedEval数据集，包含37篇科学论文的534道题，覆盖批判性解读与推理能力评估。然后对现有通用及医学专用LLMs，在不同上下文条件下进行基准测试，并分析成绩。

Result: 无论是开源还是商业模型，在CareMedEval上的最高“逐字匹配率”都未超过0.5。虽然引入中间推理步骤能提升一定表现，但在研究局限性和统计分析问题上模型依然表现不佳。

Conclusion: CareMedEval成为评估LLMs医学领域推理能力的新基准，揭示了现有模型的局限性，也为未来发展自动化批判性解读助手指明了方向。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [75] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 本文提出了一种基于RDF模式的关系抽取方法，并推出了Kastor框架，通过优化SHACL属性组合选择，有效提升小型语言模型在特定领域知识库补全和精炼任务上的效果。


<details>
  <summary>Details</summary>
Motivation: 受限于数据和计算资源，在专业领域知识库构建中，需要高效的小模型能在有限数据下完成高效、精准的信息抽取。传统SHACL校验方法仅验证单一属性结构，无法满足复杂知识抽取需求。

Method: Kastor框架将传统的SHACL单一形状验证任务，转化为对形状中所有可能属性组合的评估，并为每个训练样本选择最优组合。同时引入迭代学习机制，不断修正带噪声的知识库，从而持续优化模型能力。

Result: 该框架显著提升了模型的泛化性能与抽取准确率，在针对特定领域知识库的补全和精炼任务中表现优异，并有效发现新颖相关事实。

Conclusion: Kastor为基于RDF的关系抽取和小模型微调提供了新的高效方法，能够在专业知识库建设中提升自动化与智能化水平，尤其适用于数据有限场景。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [76] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 本文提出了BanglaSTEM数据集和T5翻译模型，有效提升技术领域（如STEM）中孟加拉语与英语之间的翻译质量，从而帮助孟加拉语使用者更好地利用英文模型进行代码生成和数学问题求解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理英语技术问题时表现良好，但在处理孟加拉语（Bangla）类似问题时表现较差。直接使用翻译系统将Bangla问题转换为英文的方法由于技术术语翻译不准确，经常导致问题含义改变，进而影响解答正确性。

Method: 作者构建了BanglaSTEM数据集，包含来自计算机科学、数学、物理、化学和生物等STEM领域的5,000组高质量孟加拉语-英语句对。句对初步由大语言模型翻译，再由人工评估筛选以确保技术术语准确。随后，作者基于T5模型在该数据集上进行训练，并在代码生成和数学解题任务上评测效果。

Result: 该方法在技术内容翻译准确性上取得显著提升，使孟加拉语用户能更有效地使用以英文为主的大语言模型解决专业问题。

Conclusion: 通过公开数据集和T5翻译模型，BanglaSTEM为STEM领域孟加拉语与英语间的高质量翻译提供了强大支持，有望促进孟加拉语用户在技术领域的学术和实践交流。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [77] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本论文提出了HaluMem基准，实现对AI记忆系统操作级别幻觉的系统性评测，并用真实交互数据揭示当前系统主要在“提取”和“更新”阶段积累幻觉错误。


<details>
  <summary>Details</summary>
Motivation: 当前AI记忆系统（如大模型的长期交互与学习）出现记忆幻觉问题，但现有评测多为端到端问答（QA），难以定位幻觉的具体操作环节。需有更细致的评测基准来诊断、提升系统可靠性。

Method: 作者提出了HaluMem基准，包括记忆提取、更新、问答三类任务，能在操作层面评估幻觉。构建了HaluMem-Medium和HaluMem-Long两个真人与AI多轮对话数据集，分别包含约1.5万条记忆点、3.5千问、百万级别Tokens用于大规模实验。

Result: 实验发现，当今主流AI记忆系统在记忆提取和更新阶段较易产生并积累幻觉，最终在QA阶段表现为错误。目前系统还难以抑制幻觉传播和积累。

Conclusion: 论文指出，未来应关注提升记忆操作可解释性和绘制约束机制，以系统性抑制幻觉、提升AI记忆系统的可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [78] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一个可以动态评估大语言模型多轮指令跟随能力的框架，并基于此构建了新的基准EvolIF，测试显示GPT-5表现最好，优于其它主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多轮对话评测数据集对轮次有限制，容易饱和，无法真实反映用户交互体验，因此需要更全面灵活的评估方法。

Method: 提出一个三层机制框架，分别追踪对话中的约束、指令和话题，能够模拟用户与模型的真实交互（包括耐心耗尽情况），并综合定义多项衡量指标，构建EvolIF基准，涵盖九种不同的约束类型。

Result: GPT-5在该框架下表现最优，能够维持平均18.54轮对话，指令跟随鲁棒性达70.31%，比Gemini-2.5-Pro高11.41%，其余模型明显落后。

Conclusion: 所提出的评测框架和基准能更好地反映多轮对话中模型的真实指令跟随能力，GPT-5在该场景下展现出领先优势。所有数据与代码会公开。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [79] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med系统通过多个小型专科语言模型协作，显著提升医学问答的准确性和实用性，并支持本地部署。


<details>
  <summary>Details</summary>
Motivation: 当前医学问答系统存在幻觉、偏见、计算资源消耗大、隐私安全和跨领域专业要求高等现实难题。作者希望提出新方法应对这些问题。

Method: 提出了SOLVE-Med多智能体架构，包括一个Router Agent（动态选择专科模型）、十个专科小语言模型（均为1B参数，针对特定医学领域微调）以及一个Orchestrator Agent（综合各模型输出）。通过多代理协作分治复杂医学问题。

Result: 在意大利医疗论坛数据的十个专业领域上评测，SOLVE-Med获得ROUGE-1分数0.301，BERTScore F1为0.697，性能优于单一大模型（可比至14B参数），同时支持本地部署。

Conclusion: SOLVE-Med多智能体架构能够提升专业医学问答的质量和实用性，降低计算资源消耗并增强隐私保护，对医学AI应用部署具有实际意义。

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [80] [Bearing Syntactic Fruit with Stack-Augmented Neural Networks](https://arxiv.org/abs/2511.03547)
*Brian DuSell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文首次展示了一类无需依赖句法监督、大规模预训练或长期训练即可在人类语言习得任务中具有人类类泛化能力的神经网络架构——Stack-augmented Neural Networks。


<details>
  <summary>Details</summary>
Motivation: 人类儿童在学习语言时能在缺乏消歧样本的情况下，自然偏好基于层级句法规则的假说。而现有研究发现，常规神经网络只有在特定条件下（如句法监督、海量语料预训练、超长训练）才具有人类类似的偏好。因此，探索无这些特殊条件下能否出现人类类泛化的神经架构，是理解语言习得机制和改进神经网络的重要问题。

Method: 作者对三种主流神经网络架构（Transformer、简单RNN、LSTM）进行了栈机制增强，包括采用Joulin & Mikolov提出的叠加栈和由DuSell & Chiang提出的非确定性栈，并在经典的问题生成任务（classical question formation task）上系统评测了各自的泛化能力。此外，还提出了一种改进Stack RNN层级泛化能力的方法。

Result: 实验显示，加入非确定性栈的Transformer在层级泛化任务上表现优异，优于其他架构。提出的Stack RNN改进方法也增强了层级泛化能力。

Conclusion: Stack增强型神经网络能在没有特殊训练条件的情况下展现出类人的层级泛化能力，有望成为更符合人类语言习得机制的建模工具，对心理语言学研究具有参考价值。

Abstract: Any finite set of training data is consistent with an infinite number of
hypothetical algorithms that could have generated it. Studies have shown that
when human children learn language, they consistently favor hypotheses based on
hierarchical syntactic rules without ever encountering disambiguating examples.
A recent line of work has inquired as to whether common neural network
architectures share this bias, finding that they do so only under special
conditions: when syntactically supervised, when pre-trained on massive corpora,
or when trained long past convergence. In this paper, we demonstrate, for the
first time, neural network architectures that are able to generalize in
human-like fashion without any of the aforementioned requirements:
stack-augmented neural networks. We test three base architectures (transformer,
simple RNN, LSTM) augmented with two styles of stack: the superposition stack
of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed
by DuSell & Chiang (2023). We find that transformers with nondeterministic
stacks generalize best out of these architectures on a classical question
formation task. We also propose a modification to the stack RNN architecture
that improves hierarchical generalization. These results suggest that
stack-augmented neural networks may be more accurate models of human language
acquisition than standard architectures, serving as useful objects of
psycholinguistic study. Our code is publicly available.

</details>


### [81] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 本论文提出了多语言、多主题、高难度的斑马谜题数据集，用于评估大语言模型（LLMs）的逻辑推理能力，并公开了相关生成代码。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理能力的基准有限，特别是在多语言和合适难度方面。本研究旨在填补这一空白，开发适用于不同LLM推理能力的大规模、高质量、多语言逻辑推理评测数据集。

Method: 作者设计并生成了不同语言、主题、大小的斑马谜题，包括14种不同提示类型和8种无效提示类型（迷惑性线索），并通过调节谜题规模和添加红鲱鱼以增加推理难度。随后，采用两种模型（GPT-4o mini与o3-mini）进行了系统测试，并比较了不同语言、主题和提示组合对模型表现的影响。

Result: 实验发现，2x3和4x5规模的谜题对不同模型具有足够挑战性；增加5个迷惑性线索会显著降低o3-mini模型的解题正确率（下降约15%）；模型在不同语言（英语vs丹麦语）或不同主题任务上的表现差异不显著；谜题难度与选择的提示类型无相关性。最终，研究发布了覆盖九种日耳曼语言（2x3和4x5规模）的斑马谜题数据集MultiZebraLogic。

Conclusion: 该研究为评测LLM推理能力提供了大规模、多样化、高难度、多语言的公开数据集及生成工具，将促进跨语言、跨主题的推理研究和模型能力进步。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [82] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文首次实验证明了Transformer语言模型中可控的局部性，通过可调节的参数实现表征的本地化与分布式表征之间的连续切换。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型主要依赖分布式表征，缺乏对模型解释性的动态调节能力。本研究旨在填补可控局部性在语言模型中的空白，满足对可解释性和透明性有高要求的领域需求。

Method: 在两层Transformer结构基础上，通过局部性参数λ的连续调节，实现了从完全本地化(λ=1.0)到完全分布式(λ=0.0)的动态表征。实验在WikiText数据集上，系统性比较了不同λ下的表现。

Result: 结果显示，局部性高的设置（λ=1.0）带来更低注意力熵（5.36 bits对7.18 bits），并拥有更高的指针保真度，表现出较强的对规则目标的对齐能力；而中间值（λ=0.6）则在解释性和性能之间取得最优平衡，验证困惑度和准确率分别达到4.65与84.7%。

Conclusion: 局部化语言模型架构可为需要兼顾透明性与能力的受监管应用领域提供理论与实践支持，通过精确可控的参数调节，实现了模型可解释性和性能的理性权衡。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [83] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 本研究通过微调大型语言模型（LLM）并结合检索增强生成（RAG）方法，为政策制定者提供高效法律文本理解与起草支持。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在法律领域的应用尚不能充分满足政策制定者在理解、分析、制定法律法规等方面的需求，需提升模型对复杂法律文本的处理和实时法规信息的获取能力。

Method: 研究团队构建了专门针对法律领域的监督数据集，对LLM进行了定向微调。同时，引入了RAG方法，使模型能够检索外部最新法律知识并融合到生成结果中，增强其实时性和专业性。

Result: 实验结果证明，结合微调与RAG方法的LLM显著提升了法律信息处理和辅助法规解读与起草的能力。

Conclusion: 该方法为法律研究及法规制定提供了重要工具，提升了政策制定过程的效率与准确性，对动态变化的法律领域具备良好适用性。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [84] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: 本文提出了Step-Audio-EditX，这是首个开源的基于大语言模型（LLM）且支持高度表现力和迭代音频编辑的模型，包括情感、说话风格和副语言特征，同时具备强大的零样本语音合成功能。


<details>
  <summary>Details</summary>
Motivation: 现有的声音编辑和语音合成模型在复杂表达和细粒度调控（如情感、风格等）方面存在局限，多依赖嵌入先验或额外模块，缺乏灵活性和通用性。

Method: 核心创新是仅通过大幅度合成数据驱动的学习方式，避免使用嵌入先验或辅助模块，从而实现跨声音的高度可控和表现力强的音频编辑。这一方法与传统关注表征解耦方式不同。

Result: 实验证明Step-Audio-EditX在情感编辑和其他精细化控制任务上优于MiniMax-2.6-hd和Doubao-Seed-TTS-2.0。

Conclusion: Step-Audio-EditX突破了传统方法的限制，实现了更强的表达力与控制能力，为语音合成和编辑领域带来新的思路和工具。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [85] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 本文系统性回顾了自Transformer模型出现以来关系抽取（RE）的研究进展，对相关综述、数据集和模型进行了详细分析，并总结了当前趋势与挑战。


<details>
  <summary>Details</summary>
Motivation: 关系抽取在信息抽取、知识图谱等领域有广泛应用。Transformer模型的出现推动了该领域的快速发展，因此有必要系统梳理近年主要工作，为后续研究指明方向。

Method: 作者构建自动化框架，对2019至2024年间的文献进行收集与标注，涵盖34篇综述、64个数据集及104个模型。分析内容包括方法进展、基准资源和语义网技术的整合。

Result: 文章总结了RE领域的主要进展，梳理了最新的方法和资源，指出了当前的研究趋势，以及现存的局限性和尚未解决的问题。

Conclusion: 本综述为研究者和从业者提供了全面的参考，帮助大家把握关系抽取领域的演变历程和未来发展方向。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [86] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 本文提出了一个新的可解释性零样本立场检测（ZSSD）框架IRIS，通过内隐和外显推理提升模型泛化能力和可解释性，并在多个基准数据集上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前ZSSD方法普遍存在泛化能力不足或文本与目标之间联系不紧密的问题，且现有基于大模型的方法过于依赖显式推理，解释粗糙，不利于预测结果的理解。作者希望提升模型可解释性和泛化能力。

Method: 提出IRIS框架，将立场检测建模为信息检索排序任务。通过识别文本中的隐式推理线索（implicit rationales）并结合基于语言特征的外显推理（explicit rationales），无需推理真值即可引导模型识别不同立场相关性，实现内在解释与立场判定相结合。

Result: 在VAST、EZ-STANCE、P-Stance和RFD等数据集上，使用50%、30%、10%标注数据均取得了优异、具有泛化能力的实验结果，证明IRIS架构和解释型设计的效用。

Conclusion: IRIS不仅提升了ZSSD模型的泛化能力，还通过显式和隐式推理机制增强了预测解释性，为立场检测任务提供了新的思路。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [87] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 提出了一个高质量的中文多文档问答数据集ChiMDQA，横跨六大领域，涵盖多类细粒度问题，旨在支持中文文档问答任务的发展。


<details>
  <summary>Details</summary>
Motivation: 当前NLP技术飞速发展，但高质量的中文文档问答数据集稀缺，无法很好地支撑相关下游应用，特别是在实际业务领域。

Method: 系统筛选六大领域的长文本材料，通过精心设计问题并分为十类，构建了6068对高质量问答对，并配以细致的评估体系，涵盖学术、教育、金融、法律、医疗和新闻等领域。

Result: 构建了一个多样化、高质量的问答数据集ChiMDQA，并公开了代码和数据，便于社区进一步研究和应用。

Conclusion: ChiMDQA为中文文档理解和问答研究提供了坚实的数据基础，有助于推动相关NLP任务的进展和实际落地。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [88] [Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models](https://arxiv.org/abs/2511.03699)
*Francesco Corso,Francesco Pierri,Gianmarco De Francisci Morales*

Main category: cs.CL

TL;DR: 该论文研究大语言模型（LLMs）在阴谋论倾向、社会人口偏见以及易于被诱导形成阴谋论观点等方面的表现。结果发现LLMs部分同意阴谋信念，可被特定提示轻易操控，且社会人口属性调节下展现出不均一的偏见，提示LLMs存在心理风险。


<details>
  <summary>Details</summary>
Motivation: 阴谋信念对误信息传播与公众对机构信任有重大影响。LLMs作为人类行为研究代理广受关注，但其是否复现如阴谋思维等更高阶心理特质尚不清楚，因此有必要系统分析其社会心理属性及风险。

Method: 作者通过对多种LLMs施加心理学验证过的阴谋心态调查问卷，并采用不同的提示词与社会人口条件设定，检验模型在阴谋信念领域的表现及偏见。

Result: LLMs在阴谋信念部分维度表现出一致性，且受社会人口条件影响，其倾向可能出现不平衡的偏见。同时，特定提示词可轻易诱导模型朝阴谋论方向生成内容，显示较高的可操控性。

Conclusion: LLMs在社会心理层面存在潜在风险；研究者需严肃评估其嵌入的心理维度，以推进计算社会科学，亦为防范其负面应用及制定缓解策略提供参考。

Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit
conspiratorial tendencies, whether they display sociodemographic biases in this
domain, and how easily they can be conditioned into adopting conspiratorial
perspectives. Conspiracy beliefs play a central role in the spread of
misinformation and in shaping distrust toward institutions, making them a
critical testbed for evaluating the social fidelity of LLMs. LLMs are
increasingly used as proxies for studying human behavior, yet little is known
about whether they reproduce higher-order psychological constructs such as a
conspiratorial mindset. To bridge this research gap, we administer validated
psychometric surveys measuring conspiracy mindset to multiple models under
different prompting and conditioning strategies. Our findings reveal that LLMs
show partial agreement with elements of conspiracy belief, and conditioning
with socio-demographic attributes produces uneven effects, exposing latent
demographic biases. Moreover, targeted prompts can easily shift model responses
toward conspiratorial directions, underscoring both the susceptibility of LLMs
to manipulation and the potential risks of their deployment in sensitive
contexts. These results highlight the importance of critically evaluating the
psychological dimensions embedded in LLMs, both to advance computational social
science and to inform possible mitigation strategies against harmful uses.

</details>


### [89] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出了一种全新的注释方案，用于追踪对话中参与者对于指代理解的一致性与偏差，并揭示即使表面上一致，也可能存在实际误解。


<details>
  <summary>Details</summary>
Motivation: 在合作对话中，参与者通过逐步建立共同认知来达成理解。然而，在信息不对称环境下，即使双方看似达成了一致，实际可能各自指涉不同对象而产生误解。缺少细致反映各方视角的资源和分析框架，难以深入研究这种现象。

Method: 作者基于HCRC MapTask语料库，提出了一种视角主义注释方案，分别标记说话人和听者对每个指代表达的理解。通过受该方案约束的大语言模型（LLM）注释流程，获得了13,000个带有可靠性估计的注释，并系统分析了由此获得的理解状态演变过程。

Result: 即使在词汇变体统一后，完全误解的情况也很少见；但当指代对象存在多重可能性时，参与者理解经常出现系统性分歧。这揭示了表面上的共同认知可能掩盖了真实的指代误对齐问题。

Conclusion: 该框架不仅为分析协作对话中基于共同认知的误解提供了新资源和角度，也可作为评价（多模态）大语言模型建模视角依赖型指代理解能力的重要工具。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [90] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 本文提出了针对农业自动化系统的专用运作域(ODD)描述与验证框架——Ag-ODD Framework，提升了农业自主系统环境定义的一致性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 农业自动化面临环境复杂多变且作业与驾驶过程交织的问题，现有通用ODD架构难以满足农业特殊需求，需要一种更契合农业领域的环境描述与验证方法。

Method: 提出Ag-ODD Framework，包括三大核心组成：1) 以ASAM Open ODD与CityGML为基础的环境与运作参数描述结构；2) 基于PEGASUS 6层模型，扩展增加一个流程层形成7层模型以适应动态农业作业；3) 基于7层模型的逻辑场景，设计迭代验证过程确保ODD的完整性和一致性。

Result: Ag-ODD Framework提供了清晰、可验证的农业自主系统运作域描述方案，通过示例展示了框架对环境描述标准化和可扩展性的促进作用。

Conclusion: Ag-ODD Framework为自主农业系统的运作环境边界描述与验证提供了一致、透明的流程，有助于行业标准化与广泛应用。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [91] [Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data](https://arxiv.org/abs/2511.02994)
*Syed Mostaquim Ali,Taufiq Rahman,Ghazal Farhani,Mohamed H. Zaki,Benoit Anctil,Dominique Charlebois*

Main category: cs.RO

TL;DR: 本论文针对无人驾驶系统（ADS）的安全性验证，提出了一种利用虚拟测试环境（VTE）模拟和评估LiDAR扫描的有效方法，并通过实验比较了多种评估指标，最终发现Density Aware Chamfer Distance（DCD）在各方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统物理测试昂贵且存在安全隐患，全面测试几乎不可能。为保证无人驾驶系统上路前的安全性，需要有效的替代测试方法。虚拟测试环境（VTE）作为一种替代方案，只有当其输出高度还原真实环境时，才具备参考价值，因此亟需高效的虚实比对评价指标。

Method: 作者系统评估了多种用于比较真实与模拟LiDAR扫描数据的评价指标，考察了它们在不同噪声、点云密度、畸变、传感器朝向和通道设定下的敏感性与准确性。选定最佳指标后，构建了基于真实LiDAR数据的虚拟环境，采用仪表化车辆采集数据并生成模拟LiDAR扫描，进一步通过模型感知输出和几何相似度比较虚拟与真实扫描。

Result: 实验表明，Density Aware Chamfer Distance（DCD）在所有测试场景下表现最优。模拟与真实LiDAR扫描的mIoU为21%（矫正强度后），平均DCD为0.63，显示几何属性有轻微差异，但在模型输出上有显著差距。DCD与感知方法的相关性也最强。

Conclusion: 虚拟测试环境下LiDAR数据与现实存在一定差异，DCD作为评估指标具有较高参考价值，有助于提升虚拟仿真在无人驾驶测试中的可靠性及准确性。

Abstract: For developing safe Autonomous Driving Systems (ADS), rigorous testing is
required before they are deemed safe for road deployments. Since comprehensive
conventional physical testing is impractical due to cost and safety concerns,
Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing
VTE-generated sensor outputs against their real-world analogues can be a strong
indication that the VTE accurately represents reality. Correspondingly, this
work explores a comprehensive experimental approach to finding evaluation
metrics suitable for comparing real-world and simulated LiDAR scans. The
metrics were tested in terms of sensitivity and accuracy with different noise,
density, distortion, sensor orientation, and channel settings. From comparing
the metrics, we found that Density Aware Chamfer Distance (DCD) works best
across all cases. In the second step of the research, a Virtual Testing
Environment was generated using real LiDAR scan data. The data was collected in
a controlled environment with only static objects using an instrumented vehicle
equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from
the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans
were compared in terms of model perception and geometric similarity. Actual and
simulated LiDAR scans have a similar semantic segmentation output with a mIoU
of 21\% with corrected intensity and an average density aware chamfer distance
(DCD) of 0.63. This indicates a slight difference in the geometric properties
of simulated and real LiDAR scans and a significant difference between model
outputs. During the comparison, density-aware chamfer distance was found to be
the most correlated among the metrics with perception methods.

</details>


### [92] [A Collaborative Reasoning Framework for Anomaly Diagnostics in Underwater Robotics](https://arxiv.org/abs/2511.03075)
*Markus Buchholz,Ignacio Carlucho,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 提出AURA系统，将人类专家、AI分析、数字孪生以及人机协作结合，实现对机器人系统异常的实时诊断与处理，持续提高系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键型环境中，自动化系统可能面临预料之外的异常，仅依赖AI或人类都难以保证安全，需要新范式实现高可靠性与适应性。

Method: 设计AURA框架，结合大语言模型（LLMs）、高保真数字孪生、和人机交互，引入两个智能体：一是低层状态异常表征智能体，将遥测数据转化为结构化自然语言问题描述；二是高层诊断推理智能体，与操作员对话，结合外部知识定位根因。最终将人类验证的诊断反馈用于微调低层模型，实现知识迭代。

Result: 实现了AURA框架的原型，并证明其能在机器人应用中检测并应对异常，实现知识持续积累与能力提升。

Conclusion: AURA奠定了可信赖、人机协作、持续进化的自动化系统模式，为安全关键型机器人团队提供了新标准。

Abstract: The safe deployment of autonomous systems in safety-critical settings
requires a paradigm that combines human expertise with AI-driven analysis,
especially when anomalies are unforeseen. We introduce AURA (Autonomous
Resilience Agent), a collaborative framework for anomaly and fault diagnostics
in robotics. AURA integrates large language models (LLMs), a high-fidelity
digital twin (DT), and human-in-the-loop interaction to detect and respond to
anomalous behavior in real time. The architecture uses two agents with clear
roles: (i) a low-level State Anomaly Characterization Agent that monitors
telemetry and converts signals into a structured natural-language problem
description, and (ii) a high-level Diagnostic Reasoning Agent that conducts a
knowledge-grounded dialogue with an operator to identify root causes, drawing
on external sources. Human-validated diagnoses are then converted into new
training examples that refine the low-level perceptual model. This feedback
loop progressively distills expert knowledge into the AI, transforming it from
a static tool into an adaptive partner. We describe the framework's operating
principles and provide a concrete implementation, establishing a pattern for
trustworthy, continually improving human-robot teams.

</details>


### [93] [WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models](https://arxiv.org/abs/2511.03077)
*R. Khorrambakht,Joaquim Ortiz-Haro,Joseph Amigo,Omar Mostafa,Daniel Dugas,Franziska Meier,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的机器人任务解决方法，通过少量无结构的'游戏'数据训练视觉世界模型和扩散式动作采样器，并结合蒙特卡洛树搜索与零阶MPC进行机器人控制，实验证明方案优于传统行为克隆方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于行为克隆的机器人策略学习方法依赖于复杂的人类演示，难以泛化到新任务且数据采集成本高，因此需要更具通用性且降低数据收集难度的方法。

Method: 作者采用少量易采集的无结构'游戏'数据训练动作条件视觉世界模型、扩散式动作采样器以及可选的奖励模型。随后结合世界模型、动作采样器及奖励模型，使用蒙特卡洛树搜索进行动作规划，并通过零阶MPC在机器人上执行。

Result: 实验在三个具备不同规划与建模复杂度的真实机器人任务上进行，结果显示本文方法能够缓解世界模型在规划时产生的幻觉问题，并在标准操控任务环境下显著优于基线的行为克隆方法。

Conclusion: 基于模型的规划结合高效的数据采集方式，可以显著提升机器人任务执行表现，为机器人泛化与实际应用提供更优解。

Abstract: Robots must understand their environment from raw sensory inputs and reason
about the consequences of their actions in it to solve complex tasks. Behavior
Cloning (BC) leverages task-specific human demonstrations to learn this
knowledge as end-to-end policies. However, these policies are difficult to
transfer to new tasks, and generating training data is challenging because it
requires careful demonstrations and frequent environment resets. In contrast to
such policy-based view, in this paper we take a model-based approach where we
collect a few hours of unstructured easy-to-collect play data to learn an
action-conditioned visual world model, a diffusion-based action sampler, and
optionally a reward model. The world model -- in combination with the action
sampler and a reward model -- is then used to optimize long sequences of
actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are
executed on the robot via a zeroth-order Model Predictive Controller (MPC). We
show that the action sampler mitigates hallucinations of the world model during
planning and validate our approach on 3 real-world robotic tasks with varying
levels of planning and modeling complexity. Our experiments support the
hypothesis that planning leads to a significant improvement over BC baselines
on a standard manipulation test environment.

</details>


### [94] [3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors](https://arxiv.org/abs/2511.03078)
*Rohan Kota,Kaival Shah,J. Edward Colgate,Gregory Reardon*

Main category: cs.RO

TL;DR: 提出了一个将廉价3D打印机转变为自动化探测设备的开源库，可高效生成大规模标注数据，用于触觉传感器的自动校准，并在实际传感器应用中取得了优异的校准效果。


<details>
  <summary>Details</summary>
Motivation: 触觉传感器对于机器手灵巧操作至关重要，但将原始读数转换为有意义的物理量，需要复杂且繁琐的校准工作，目前校准流程普遍缺乏规范且非常耗时。

Method: 开发了一套开源软件，能够让廉价3D打印机自动化地采集和标注触觉传感器校准数据；利用收集到的数据，结合定制的卷积神经网络，对DIGIT和GelSight Mini等商用视觉型触觉传感器进行深入校准实验。

Result: 成功用该系统完成了对DIGIT和GelSight Mini的校准，能精确重建高质量深度图。通过数据消融实验提出了数据量需求的实用建议，并对训练模型的泛化能力进行了基准测试，证明了其准确性和适应性。

Conclusion: 该开源库实现了触觉传感器校准流程的自动化，大幅简化了触觉传感器的部署流程，加速了触觉感知研究，有助于推动触觉传感器在机器人平台的实际应用。

Abstract: Tactile sensing plays a key role in enabling dexterous and reliable robotic
manipulation, but realizing this capability requires substantial calibration to
convert raw sensor readings into physically meaningful quantities. Despite its
near-universal necessity, the calibration process remains ad hoc and
labor-intensive. Here, we introduce \libname{}, an open-source library that
transforms a low-cost 3D printer into an automated probing device capable of
generating large volumes of labeled training data for tactile sensor
calibration. We demonstrate the utility of \libname{} by calibrating two
commercially available vision-based tactile sensors, DIGIT and GelSight Mini,
to reconstruct high-quality depth maps using the collected data and a custom
convolutional neural network. In addition, we perform a data ablation study to
determine how much data is needed for accurate calibration, providing practical
guidelines for researchers working with these specific sensors, and we
benchmark the trained models on previously unseen objects to evaluate
calibration accuracy and generalization performance. By automating tactile
sensor calibration, \libname{} can accelerate tactile sensing research,
simplify sensor deployment, and promote the practical integration of tactile
sensing in robotic platforms.

</details>


### [95] [SENT Map - Semantically Enhanced Topological Maps with Foundation Models](https://arxiv.org/abs/2511.03165)
*Raj Surya Rajendran Kathirvel,Zach A Chavis,Stephen J. Guy,Karthik Desingh*

Main category: cs.RO

TL;DR: 本文提出了一种名为SENT-Map的语义增强型拓扑地图，用于室内机器人导航与操控，通过结合基础模型(FM)提升机器人对环境的理解、规划和操作能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在室内环境导航往往依赖几何信息，缺乏灵活的语义理解，难以与人类意图以及自然语言交互对接，因此需要一种兼容人类和机器人双向理解的新式地图表示。

Method: SENT-Map采用两阶段流程：第一阶段，通过Vision-FM与操作员联合采集环境并构建拓扑及语义地图，形成可被机器和人类理解的JSON文本表达；第二阶段，融合自然语言查询和FM，辅助机器人基于SENT-Map进行规划决策。整个流程确保机器人始终基于已有节点规划，避免不可行状态。

Result: 实验证明，基于SENT-Map的语义增强让即便是较小的、本地部署的基础模型，也能成功完成室内环境中的自动规划任务。

Conclusion: 语义增强后的地图表示能够显著提升机器人在室内环境的自主规划与任务执行能力，为人机协作和高效交互提供了新途径。

Abstract: We introduce SENT-Map, a semantically enhanced topological map for
representing indoor environments, designed to support autonomous navigation and
manipulation by leveraging advancements in foundational models (FMs). Through
representing the environment in a JSON text format, we enable semantic
information to be added and edited in a format that both humans and FMs
understand, while grounding the robot to existing nodes during planning to
avoid infeasible states during deployment. Our proposed framework employs a two
stage approach, first mapping the environment alongside an operator with a
Vision-FM, then using the SENT-Map representation alongside a natural-language
query within an FM for planning. Our experimental results show that
semantic-enhancement enables even small locally-deployable FMs to successfully
plan over indoor environments.

</details>


### [96] [Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.03167)
*Xin Liu,Jinze Wu,Yinghui Li,Chenkun Qi,Yufei Xue,Feng Gao*

Main category: cs.RO

TL;DR: 本文提出了一种基于动作先验（motion prior）的深度强化学习方法，使六足机器人能够在复杂地形上实现自然且鲁棒的步态。研究首次将强化学习控制器应用于真实六足机器人，显著提升了其运动性能。


<details>
  <summary>Details</summary>
Motivation: 多足机器人的步态协调是提升其跨越复杂地形能力的关键，但多足导致动作探索空间巨大，传统方法难以高效生成自然且鲁棒的运动。因此需要新的学习方法来实现更高效的步态控制。

Method: 作者生成高质量的动作先验数据集，训练对抗判别器（adversarial discriminator），利用其指导六足机器人通过深度强化学习算法学习自然步态。所学策略最终被迁移到真实机器人上进行验证。

Result: 所提策略能够在复杂地形上实现六足机器人的有效行走，无需视觉信息，步态自然且表现出显著的鲁棒性。

Conclusion: 这是首次在真实六足机器人上实现强化学习控制器的复杂地形行走，验证了动作先验结合深度强化学习在多足机器人运动控制领域的有效性和实用价值。

Abstract: Multi-legged robots offer enhanced stability to navigate complex terrains
with their multiple legs interacting with the environment. However, how to
effectively coordinate the multiple legs in a larger action exploration space
to generate natural and robust movements is a key issue. In this paper, we
introduce a motion prior-based approach, successfully applying deep
reinforcement learning algorithms to a real hexapod robot. We generate a
dataset of optimized motion priors, and train an adversarial discriminator
based on the priors to guide the hexapod robot to learn natural gaits. The
learned policy is then successfully transferred to a real hexapod robot, and
demonstrate natural gait patterns and remarkable robustness without visual
information in complex terrains. This is the first time that a reinforcement
learning controller has been used to achieve complex terrain walking on a real
hexapod robot.

</details>


### [97] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型任务规划、高低层混合模仿与强化学习的新型框架，实现了机器人在复杂长序列包裹任务中的高效协作与操作。核心贡献是引入子任务感知Transformer，实现了端到端的复杂动作学习，在真实场景中包裹任务成功率达97%。


<details>
  <summary>Details</summary>
Motivation: 在仓库、零售等场景，人机协作时常需处理如纸张、袋子等可变形物体，但因变形动态难以预测和力控制复杂，高效协作极具挑战性。为此，选用具有代表性的长时间动作任务——礼品包裹，作为研究切入点。

Method: 提出一种分层学习框架：高层利用大语言模型进行任务分解规划，低层融合模仿学习和强化学习，通过Sub-task Aware Robotic Transformer（START）统一建模所有子任务，借助子任务ID实现动作序列的时间锚定。

Result: 在真实机器人包裹任务中，该方法实现了97%的成功率。统一模型减少了对各类专用策略的依赖，提升了对人类监督和高低层意图到动作的衔接能力。证明了该方法在复杂可变形物体操控中的有效性和实用性。

Conclusion: 该框架为人机协作中操作复杂、可变形物体提供了通用且高效的解决方案。核心Transformer能跨越长时间依赖，实现灵活子目标学习，有望推广至更多要求精细力控和长序列动作规划的任务。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


### [98] [Collaborative Assembly Policy Learning of a Sightless Robot](https://arxiv.org/abs/2511.03189)
*Zeqing Zhang,Weifeng Lu,Lei Yang,Wei Jing,Bowei Tang,Jia Pan*

Main category: cs.RO

TL;DR: 本文针对机器人与人协作共同插入木板的任务，提出结合专家设计的顺应控制与强化学习的方法，提升协作效率，降低人力成本，实验显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统顺应控制法在物理人机协作中需要准确测量人施加的力/力矩，但实际测量较难，且强化学习在稀疏奖励和安全约束下也不适用，限制了机器人有效辅助人类完成插板任务的能力。

Method: 提出了一种结合人类设计的顺应控制器与强化学习的新方法，让机器人学习更主动地协助人类，降低人类操作时的物理负担。该方法通过仿真和真实场景实验进行验证。

Result: 实验表明，该方法在任务完成率和完成时间上均优于传统顺应控制，同时机器人施加的人受力/力矩明显减少。

Conclusion: 结合顺应控制与强化学习的策略能提升复杂人机协作任务的效率与安全性，具有实际应用前景。

Abstract: This paper explores a physical human-robot collaboration (pHRC) task
involving the joint insertion of a board into a frame by a sightless robot and
a human operator. While admittance control is commonly used in pHRC tasks, it
can be challenging to measure the force/torque applied by the human for
accurate human intent estimation, limiting the robot's ability to assist in the
collaborative task. Other methods that attempt to solve pHRC tasks using
reinforcement learning (RL) are also unsuitable for the board-insertion task
due to its safety constraints and sparse rewards. Therefore, we propose a novel
RL approach that utilizes a human-designed admittance controller to facilitate
more active robot behavior and reduce human effort. Through simulation and
real-world experiments, we demonstrate that our approach outperforms admittance
control in terms of success rate and task completion time. Additionally, we
observed a significant reduction in measured force/torque when using our
proposed approach compared to admittance control. The video of the experiments
is available at https://youtu.be/va07Gw6YIog.

</details>


### [99] [GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement](https://arxiv.org/abs/2511.03400)
*Minquan Gao,Xinyi Li,Qing Yan,Xiaojian Sun,Xiaopan Zhang,Chien-Ming Huang,Jiachen Li*

Main category: cs.RO

TL;DR: 提出一种称为GUIDES的轻量级框架，可为机器人已有策略增加来自大模型的语义指导，无需更换原有模型即可提升其在实际任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 许多机器人已经具备经过广泛训练和验证的策略，但这些策略缺乏类似大模型的语义认知能力。完全替换现有策略代价高昂且不现实，因此需要能增强而不是替换原有策略的方法。

Method: GUIDES框架利用微调的视觉-语言模型（Instructor）生成任务相关的语义指令，通过辅助手段编码为指导嵌入，然后注入到已有策略的潜空间中。仅需针对新语义输入对旧模型进行简短、局部微调。推理阶段由大语言模型（Reflector）监控Instructor的置信度，遇到低置信时进行回溯、检索与上下文增强，提升整体鲁棒性。

Result: 在RoboCasa仿真环境中对多种策略架构进行大量验证，GUIDES均能明显提高完成任务的成功率。在实际UR5机械臂部署时，也提升了抓取等关键子任务的运动精度。

Conclusion: GUIDES为增强现有机器人策略提供了实际且高效的途径，可通过引入语义指导实现策略升级，无需损失已积累的知识或承担高昂替换成本。

Abstract: Pre-trained robot policies serve as the foundation of many validated robotic
systems, which encapsulate extensive embodied knowledge. However, they often
lack the semantic awareness characteristic of foundation models, and replacing
them entirely is impractical in many situations due to high costs and the loss
of accumulated knowledge. To address this gap, we introduce GUIDES, a
lightweight framework that augments pre-trained policies with semantic guidance
from foundation models without requiring architectural redesign. GUIDES employs
a fine-tuned vision-language model (Instructor) to generate contextual
instructions, which are encoded by an auxiliary module into guidance
embeddings. These embeddings are injected into the policy's latent space,
allowing the legacy model to adapt to this new semantic input through brief,
targeted fine-tuning. For inference-time robustness, a large language
model-based Reflector monitors the Instructor's confidence and, when confidence
is low, initiates a reasoning loop that analyzes execution history, retrieves
relevant examples, and augments the VLM's context to refine subsequent actions.
Extensive validation in the RoboCasa simulation environment across diverse
policy architectures shows consistent and substantial improvements in task
success rates. Real-world deployment on a UR5 robot further demonstrates that
GUIDES enhances motion precision for critical sub-tasks such as grasping.
Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,
rather than replace, validated robot policies.

</details>


### [100] [Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach](https://arxiv.org/abs/2511.03444)
*Vesna Poprcova,Iulia Lefter,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 本研究通过与心理健康研究者的参与式设计工作坊，探讨了社交辅助机器人在支持社交焦虑方面应具备的核心价值和设计要素。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑广泛存在且严重影响生活质量，但现有支持和治疗措施往往不足。社会机器人技术的发展为心理健康支持提供了新机遇。为有效设计相关机器人，必须了解目标用户认为有意义、可接受和有帮助的价值观。

Method: 进行了参与式设计工作坊，邀请心理健康学术研究者参与。通过创造性、反思性和前瞻性的活动，让参与者探索场景和设计可能性，系统性归纳与机器人支持干预相关的价值观、需求和期望。

Result: 结果揭示了多项与设计相关的重要价值观，包括适应性、接受度以及效果性（疗效），这些都是为社交焦虑人士提供有效支持的关键。

Conclusion: 研究凸显了以研究为主导的价值观挖掘方法的重要性，强调以用户为中心、注重情境相关性的设计思路对于开发社交辅助机器人至关重要。

Abstract: Social anxiety is a prevalent mental health condition that can significantly
impact overall well-being and quality of life. Despite its widespread effects,
adequate support or treatment for social anxiety is often insufficient.
Advances in technology, particularly in social robotics, offer promising
opportunities to complement traditional mental health. As an initial step
toward developing effective solutions, it is essential to understand the values
that shape what is considered meaningful, acceptable, and helpful. In this
study, a participatory design workshop was conducted with mental health
academic researchers to elicit the underlying values that should inform the
design of socially assistive robots for social anxiety support. Through
creative, reflective, and envisioning activities, participants explored
scenarios and design possibilities, allowing for systematic elicitation of
values, expectations, needs, and preferences related to robot-supported
interventions. The findings reveal rich insights into design-relevant
values-including adaptivity, acceptance, and efficacy-that are core to support
for individuals with social anxiety. This study highlights the significance of
a research-led approach to value elicitation, emphasising user-centred and
context-aware design considerations in the development of socially assistive
robots.

</details>


### [101] [Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control](https://arxiv.org/abs/2511.03481)
*Jianbo Yuan,Haohua Zhu,Jing Dai,Sheng Yi*

Main category: cs.RO

TL;DR: 本文介绍了一种新型高性能五指灵巧机械手Dex-Hand 021，兼具轻量化与高自由度，显著提升了机械手的灵巧性和力感知能力，对灵巧机械手的设计和智能制造具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 尽管人手在运动、感知和协调操作等方面表现卓越，但在机器人系统中复制这些功能仍面临复杂性、结构尺寸、耐用性及力感知等工程难题。本文旨在解决如何兼顾人手灵巧与工程可行性的问题。

Method: 作者设计了一种基于钢索驱动的五指机械手Dex-Hand 021，具有12个主动和7个被动自由度（总共19个）。此外，提出了一种基于本体力感知的从动控制（admittance control）方法，用于增强对物体的操作表现。

Result: 实验显示，每根手指负载能力超过10N，指尖重复定位误差小于0.001米，力估计误差小于0.2N。与传统PID控制相比，多物体抓取时关节力矩降低了31.19%，有效提升了力感知能力，同时防止了过载。该手在动力抓握和精密拾取中表现优越，成功完成了33种GRASP动作及复杂操作任务。

Conclusion: Dex-Hand 021为实现轻量化、工业级灵巧机械手和本体力感知控制提供了新进展，有助于提升机器人操作能力和智能制造水平。

Abstract: The human hand plays a vital role in daily life and industrial applications,
yet replicating its multifunctional capabilities-including motion, sensing, and
coordinated manipulation-with robotic systems remains a formidable challenge.
Developing a dexterous robotic hand requires balancing human-like agility with
engineering constraints such as complexity, size-to-weight ratio, durability,
and force-sensing performance. This letter presents Dex-Hand 021, a
high-performance, cable-driven five-finger robotic hand with 12 active and 7
passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight
1 kg design. We propose a proprioceptive force-sensing-based admittance control
method to enhance manipulation. Experimental results demonstrate its superior
performance: a single-finger load capacity exceeding 10 N, fingertip
repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared
to PID control, joint torques in multi-object grasping are reduced by 31.19%,
significantly improves force-sensing capability while preventing overload
during collisions. The hand excels in both power and precision grasps,
successfully executing 33 GRASP taxonomy motions and complex manipulation
tasks. This work advances the design of lightweight, industrial-grade dexterous
hands and enhances proprioceptive control, contributing to robotic manipulation
and intelligent manufacturing.

</details>


### [102] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本文提出了一种基于MCP协议的服务器，实现对ROS和ROS2数据包的分析和可视化，结合大语言模型（LLM/VLM）支持机器人数据的自然语言处理，并评估了多种SOTA模型的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI和Embodied AI的兴起，二者结合的跨界研究较少，而MCP协议作为关键使能技术，亟需相关工具提升机器人数据交互和分析能力。

Method: 作者开发了针对移动机器人领域的MCP服务器，支持自然语言分析robot数据包，包括轨迹、激光扫描、变换和时间序列，同时提供接口与ROS2 CLI工具集成，并可自定义topic和时间点筛选。配套开发了轻量级UI，可对多种LLM/VLM工具调用能力进行基准测评。

Result: 实验分析了8种SOTA的专有及开源LLM/VLM模型的工具调用表现，发现不同模型间差距明显，Kimi K2与Claude Sonnet 4表现最好。工具描述方式、参数数量及可调用工具数量等因素显著影响调用效果。

Conclusion: MCP服务器和配套工具有效提升了ROS和ROS2数据包的自然语言处理能力，不同LLM/VLM模型存在明显差异，未来应关注工具设计与模型能力的适配。相关代码已开源。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [103] [Indicating Robot Vision Capabilities with Augmented Reality](https://arxiv.org/abs/2511.03550)
*Hong Wang,Ridhima Phatak,James Ocampo,Zhao Han*

Main category: cs.RO

TL;DR: 人类常误以为机器人与自己有相同视野，易导致协作失误。作者提出四种用于AR的视野指示器，并测试其对准确性、信心、效率与负担的影响。分配在任务空间的外部指示器效果最佳。


<details>
  <summary>Details</summary>
Motivation: 因为人类往往不了解机器人的实际视觉能力，容易产生误解，导致在人机协作中提出机器人无法完成的任务。本研究致力于减少此类误解，提高协作效率。

Method: 设计了四种AR视野指示器（涵盖机器人视角到任务空间），并通过41名受试者实验，依次评估它们对准确性、自信、任务效率及负荷的影响。

Result: 实验显示，在任务空间的allocentric（外部型）指示器准确率最高，但需更多时间理解。egocentric（机器人视角）指示器也提升了准确性。所有方案下，参与者信心高、认知负荷低。

Conclusion: 提出了六项设计建议，指导在AR或物理层面采用这些视野指示器，从而帮助人类更好理解机器人的视觉能力，优化协作体验。

Abstract: Research indicates that humans can mistakenly assume that robots and humans
have the same field of view (FoV), possessing an inaccurate mental model of
robots. This misperception may lead to failures during human-robot
collaboration tasks where robots might be asked to complete impossible tasks
about out-of-view objects. The issue is more severe when robots do not have a
chance to scan the scene to update their world model while focusing on assigned
tasks. To help align humans' mental models of robots' vision capabilities, we
propose four FoV indicators in augmented reality (AR) and conducted a user
human-subjects experiment (N=41) to evaluate them in terms of accuracy,
confidence, task efficiency, and workload. These indicators span a spectrum
from egocentric (robot's eye and head space) to allocentric (task space).
Results showed that the allocentric blocks at the task space had the highest
accuracy with a delay in interpreting the robot's FoV. The egocentric indicator
of deeper eye sockets, possible for physical alteration, also increased
accuracy. In all indicators, participants' confidence was high while cognitive
load remained low. Finally, we contribute six guidelines for practitioners to
apply our AR indicators or physical alterations to align humans' mental models
with robots' vision capabilities.

</details>


### [104] [OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera](https://arxiv.org/abs/2511.03571)
*Hao Shi,Ze Wang,Shangwei Guo,Mengfei Duan,Song Wang,Teng Chen,Kailun Yang,Lin Wang,Kaiwei Wang*

Main category: cs.RO

TL;DR: 本文提出了OneOcc，一种专为腿型或人形机器人设计的全景视觉语义场景补全框架，通过多种创新模块实现了对仿生机器人360°视野的稳健3D语义占据推理，并发布了两个新数据集，实验效果优于多项主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的语义场景补全（SSC）系统多针对装有前向传感器的轮式机器人，难以适应腿型/人形机器人因行走引入的抖动与需要360°全方位感知的需求。现有方法缺乏对全景、身体抖动和环境空间特性的系统性应对。

Method: OneOcc采用全视觉输入，提出：1）Dual-Projection fusion（DP-ER），结合环形全景与等距展开，保持360°连续性与网格对齐；2）Bi-Grid Voxelization（BGV），在笛卡尔和柱面两种空间内推理，以减少离散误差、突出边界；3）轻量级解码器结合分层AMoE-3D实现动态多尺度融合与远距离/遮挡环境推理；4）可插拔式步态位移补偿（GDC）模块实现免传感器运动纠正。

Result: OneOcc在新发布的QuadOcc（真实四足机器人360°全景）和Human360Occ（CARLA仿真含多模态标签）两个基准数据集上，显著超越了视觉和主流LiDAR基线：在H3O数据集内城市场景提升3.83 mIoU，跨城市提升8.08 mIoU。所有模块均为轻量级，可实际部署。

Conclusion: OneOcc首创性为腿型/人形机器人提供了高效、高精度的全景视觉语义场景补全技术，并开源了高质量数据集。其模块可移植性强，具有实际部署价值，有望拓展机器人在复杂环境下的应用。

Abstract: Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most
semantic scene completion (SSC) systems target wheeled platforms with
forward-facing sensors. We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.
OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular
panorama and its equirectangular unfolding, preserving 360{\deg} continuity and
grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and
cylindrical-polar spaces, reducing discretization bias and sharpening
free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D
for dynamic multi-scale fusion and better long-range/occlusion reasoning; and
(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level
motion correction without extra sensors. We also release two panoramic
occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and
Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic
occupancy; standardized within-/cross-city splits). OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city). Modules are lightweight, enabling deployable full-surround
perception for legged/humanoid robots. Datasets and code will be publicly
available at https://github.com/MasterHow/OneOcc.

</details>


### [105] [Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution](https://arxiv.org/abs/2511.03576)
*Aniol Civit,Antonio Andriella,Carles Sierra,Guillem Alenyà*

Main category: cs.RO

TL;DR: 本文提出了MUP-QBAF多用户个性化框架，使机器人能在多用户（例如护理场景）中动态、透明地协调各方偏好冲突。


<details>
  <summary>Details</summary>
Motivation: 当前HRI个性化研究主要关注单用户，忽略多用户、偏好冲突场景。实际应用中如护理任务常涉及多方，需解决偏好冲突。

Method: 基于QBAF提出MUP-QBAF，将多用户正负偏好建模为argument，结合用户输入与机器人对环境的动态观察，迭代更新argument强度，通过论证框架推理最终偏好。以护理机器人在衰弱评估中的多方偏好协调为案例进行验证。

Result: 实验表明该框架可动态整合用户输入与情境变化，透明且结构化地处理偏好冲突，敏感性分析展示了基线参数如何影响结果。

Conclusion: MUP-QBAF为多用户HRI偏好冲突提供了结构化、可解释解决方案，优于传统静态或纯数据驱动方法，有助于机器人在实际环境中有效解决多方偏好矛盾。

Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced
significantly, most existing approaches focus on single-user adaptation,
overlooking scenarios involving multiple stakeholders with potentially
conflicting preferences. To address this, we propose the Multi-User Preferences
Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user
personalisation framework based on Quantitative Bipolar Argumentation
Frameworks (QBAFs) that explicitly models and resolves multi-user preference
conflicts. Unlike prior work in Argumentation Frameworks, which typically
assumes static inputs, our approach is tailored to robotics: it incorporates
both users' arguments and the robot's dynamic observations of the environment,
allowing the system to adapt over time and respond to changing contexts.
Preferences, both positive and negative, are represented as arguments whose
strength is recalculated iteratively based on new information. The framework's
properties and capabilities are presented and validated through a realistic
case study, where an assistive robot mediates between the conflicting
preferences of a caregiver and a care recipient during a frailty assessment
task. This evaluation further includes a sensitivity analysis of argument base
scores, demonstrating how preference outcomes can be shaped by user input and
contextual observations. By offering a transparent, structured, and
context-sensitive approach to resolving competing user preferences, this work
advances the field of multi-user HRI. It provides a principled alternative to
data-driven methods, enabling robots to navigate conflicts in real-world
environments.

</details>


### [106] [Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning](https://arxiv.org/abs/2511.03591)
*Qingyi Chen,Ruiqi Ni,Jun Kim,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种在任务约束流形（manifold-constrained）下，实现分布式多智能体安全运动规划的新方法。该方法在处理高维、多任务约束的场景下，优于现有方案，具有良好的扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 目前多智能体运动规划难以同时满足任务引导下的复杂约束（如服务机器人保持杯子直立同时避免碰撞），现有分布式方案在高维和流形约束下效果有限，因此需要新方法应对实际机器人中的复杂约束和动态环境。

Method: 作者提出基于流形约束的Hamilton-Jacobi可达性（HJR）学习框架，解决流形约束下的HJR问题以刻画与任务相关的安全条件，并将这些条件集成到分布式轨迹优化规划器中，实现无须假设其他智能体策略的、安全且任务可行的运动规划。

Result: 方法在多种流形任务约束和高维多智能体操作任务上测试，性能优于现有受限运动规划器，并可在真实场景下实现足够快的运算速度。

Conclusion: 新方法有效解决了高维、多任务约束下多智能体安全运动规划难题，具备良好的通用性和实际应用潜力。

Abstract: Safe multi-agent motion planning (MAMP) under task-induced constraints is a
critical challenge in robotics. Many real-world scenarios require robots to
navigate dynamic environments while adhering to manifold constraints imposed by
tasks. For example, service robots must carry cups upright while avoiding
collisions with humans or other robots. Despite recent advances in
decentralized MAMP for high-dimensional systems, incorporating manifold
constraints remains difficult. To address this, we propose a
manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for
decentralized MAMP. Our method solves HJR problems under manifold constraints
to capture task-aware safety conditions, which are then integrated into a
decentralized trajectory optimization planner. This enables robots to generate
motion plans that are both safe and task-feasible without requiring assumptions
about other agents' policies. Our approach generalizes across diverse
manifold-constrained tasks and scales effectively to high-dimensional
multi-agent manipulation problems. Experiments show that our method outperforms
existing constrained motion planners and operates at speeds suitable for
real-world applications. Video demonstrations are available at
https://youtu.be/RYcEHMnPTH8 .

</details>


### [107] [Multi-robot searching with limited sensing range for static and mobile intruders](https://arxiv.org/abs/2511.03622)
*Swadhin Agrawal,Sujoy Bhore,Joseph S. B. Mitchell,P. B. Sujit,Aayush Gohil*

Main category: cs.RO

TL;DR: 本文研究了在几何域内利用多个搜索机器人寻找入侵者的问题，提出了几种高效鲁棒的搜索算法，并分析了机器人数量与搜索时间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，使用机器人主动搜索入侵物体（如安防、救援等）具有实际应用意义，但受限于机器人感知能力与任务空间的复杂性，需要设计有效的搜索策略。

Method: 作者在正交多边形空间（边平行于坐标轴、无孔）内，分别针对静止和移动的入侵者，基于机器人感知范围有限性，提出了三类算法：空间填充曲线搜索、随机搜索及合作式随机搜索，并分析其性能。

Result: 问题本身，即使对于静止入侵者，也是NP难问题。通过仿真和理论分析，得到了不同算法在机器人数量与搜索时长之间的具体权衡，并探讨了空间形状特性对搜索效果的影响。

Conclusion: 尽管寻找入侵者问题极具复杂性，作者提出的高效算法（尤其是合作随机搜索）能有效提升搜索效率，并为多机器人协同搜索实际应用提供理论与方法基础。

Abstract: We consider the problem of searching for an intruder in a geometric domain by
utilizing multiple search robots. The domain is a simply connected orthogonal
polygon with edges parallel to the cartesian coordinate axes. Each robot has a
limited sensing capability. We study the problem for both static and mobile
intruders. It turns out that the problem of finding an intruder is NP-hard,
even for a stationary intruder. Given this intractability, we turn our
attention towards developing efficient and robust algorithms, namely methods
based on space-filling curves, random search, and cooperative random search.
Moreover, for each proposed algorithm, we evaluate the trade-off between the
number of search robots and the time required for the robots to complete the
search process while considering the geometric properties of the connected
orthogonal search area.

</details>


### [108] [Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural](https://arxiv.org/abs/2511.03651)
*Andrei A. Korigodskii,Oleg D. Kalachev,Artem E. Vasiunik,Matvei V. Urvantsev,Georgii E. Bondar*

Main category: cs.RO

TL;DR: 本论文展示了一种创新的无人机自主喷绘系统，成功实现了世界最大规模的无人机壁画创作，突破了室外环境下的精确导航与喷绘难题。


<details>
  <summary>Details</summary>
Motivation: 当前，利用无人机在户外进行大规模艺术创作面临极高的精度和可靠性要求，尤其在风力、阳光等复杂环境下，传统方法难以胜任，因此需要开发专门针对艺术应用的高精度导航与控制系统。

Method: 作者结合红外运动捕捉摄像与激光雷达（LiDAR）技术，设计了专为大规模壁画设计的高精度定位系统，并创新性地采用切向与法向分开调节的轨迹跟踪结构，结合自研抗气流喷涂机构与复杂轨迹路径优化算法，实现了无人机在复杂环境下的高精度自主喷画。

Result: 实验证明，无人机系统在不同环境条件下均能稳定完成复杂曲线与区域填充的绘画任务，整体表现出较高的鲁棒性和精度。

Conclusion: 本系统不仅实现了世界最大无人机壁画创作，展示了无人机艺术应用的巨大潜力，也为户外大规模自主艺术创作及机器人在创意领域的应用开辟了新的方向。

Abstract: This paper presents the innovative design and successful deployment of a
pioneering autonomous unmanned aerial system developed for executing the
world's largest mural painted by a drone. Addressing the dual challenges of
maintaining artistic precision and operational reliability under adverse
outdoor conditions such as wind and direct sunlight, our work introduces a
robust system capable of navigating and painting outdoors with unprecedented
accuracy. Key to our approach is a novel navigation system that combines an
infrared (IR) motion capture camera and LiDAR technology, enabling precise
location tracking tailored specifically for largescale artistic applications.
We employ a unique control architecture that uses different regulation in
tangential and normal directions relative to the planned path, enabling precise
trajectory tracking and stable line rendering. We also present algorithms for
trajectory planning and path optimization, allowing for complex curve drawing
and area filling. The system includes a custom-designed paint spraying
mechanism, specifically engineered to function effectively amidst the turbulent
airflow generated by the drone's propellers, which also protects the drone's
critical components from paint-related damage, ensuring longevity and
consistent performance. Experimental results demonstrate the system's
robustness and precision in varied conditions, showcasing its potential for
autonomous large-scale art creation and expanding the functional applications
of robotics in creative fields.

</details>


### [109] [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](https://arxiv.org/abs/2511.03652)
*Azizollah Taheri,Derya Aksaray*

Main category: cs.RO

TL;DR: 本文提出了一种面向不确定环境下空间-时间-逻辑任务（以scLTL表达）的小车运动规划方法，融合概率知识与自动机理论，并通过奖励机制与价值迭代实现在线重规划。仿真验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中，目标区域的精确位置往往未知，只能得到概率性语义标签信息。因此，需要克服环境标签不确定下，满足复杂空间-时间逻辑任务的运动规划挑战。

Method: 提出了一种新的自动机理论方法，构造了特殊的乘积自动机，能够处理语义标签的不确定性，对自动机边设计奖励函数，并用价值迭代算法实现在线动态重规划。

Result: 理论分析证明了该方法的有效性，并通过仿真和实验展示了提出方法在不确定语义标签环境下的优良表现。

Conclusion: 本文方法能在概率性已知环境标签下，实现基于scLTL任务的高效、鲁棒运动规划，是自动机理论在机器人规划领域的一项创新应用。

Abstract: This paper addresses a motion planning problem to achieve
spatio-temporal-logical tasks, expressed by syntactically co-safe linear
temporal logic specifications (scLTL\next), in uncertain environments. Here,
the uncertainty is modeled as some probabilistic knowledge on the semantic
labels of the environment. For example, the task is "first go to region 1, then
go to region 2"; however, the exact locations of regions 1 and 2 are not known
a priori, instead a probabilistic belief is available. We propose a novel
automata-theoretic approach, where a special product automaton is constructed
to capture the uncertainty related to semantic labels, and a reward function is
designed for each edge of this product automaton. The proposed algorithm
utilizes value iteration for online replanning. We show some theoretical
results and present some simulations/experiments to demonstrate the efficacy of
the proposed approach.

</details>


### [110] [Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design](https://arxiv.org/abs/2511.03676)
*Taito Tashiro,Tomoko Yonezawa,Hirotake Yamazoe*

Main category: cs.RO

TL;DR: 本研究通过分析人类在棋类游戏中的动作特征，探索如何将这些动作线索用于设计更具表现力的机器人手臂动作。


<details>
  <summary>Details</summary>
Motivation: 设计能有效传达意图与情感的机器人动作，对于人与机器协作与交流至关重要。然而，如何借鉴人类动作中的表达性来提升机器人动作的可解释性，仍缺乏系统方法。

Method: 研究以Geister棋类游戏为例，分析了人类自然下棋动作和有意识表达动作，并据此分析动作速度与停顿等时序特征，进而设计了机器人在不同阶段的动作（如提取和撤回阶段），通过实体机器人和录像两种方式展示给观察者，并收集他们的印象评价。

Result: 实验结果显示，机器人动作的后期阶段，尤其是“撤回”时动作时序，对观察者形成动作印象作用显著。同时，实体机器人展示比视频更能增强动作线索的可解释性和表现力。

Conclusion: 基于人类动作中的时序线索（如运动速度和停顿时长）设计机器人动作，有助于提升机器人动作的表现力和可被理解性，对人机互动中的动作设计具有实际指导意义。

Abstract: This study investigates how human motion cues can be used to design
expressive robot-arm movements. Using the imperfect-information game Geister,
we analyzed two types of human piece-moving motions: natural gameplay
(unconscious tendencies) and instructed expressions (intentional cues). Based
on these findings, we created phase-specific robot motions by varying movement
speed and stop duration, and evaluated observer impressions under two
presentation modalities: a physical robot and a recorded video. Results
indicate that late-phase motion timing, particularly during withdrawal, plays
an important role in impression formation and that physical embodiment enhances
the interpretability of motion cues. These findings provide insights for
designing expressive robot motions based on human timing behavior.

</details>


### [111] [Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping](https://arxiv.org/abs/2511.03691)
*Zhihang Qin,Yueheng Zhang,Wan Su,Linxin Hou,Shenghao Zhou,Zhijun Chen,Yu Jun Tan,Cecilia Laschi*

Main category: cs.RO

TL;DR: 该论文提出了一种无需外部能量源、体积固定且自包含的软体夹持器。它通过内部三腔液体重分布，实现源头自由的流体驱动抓取，具备尺寸选择性和压力自适应等优点。


<details>
  <summary>Details</summary>
Motivation: 现有流体驱动软夹持器依赖外部流体源，影响其便携性和长时自主操作，因此需要设计一种无源、自适应的软夹持方案。

Method: 设计了由三个互连的双稳态快跳腔组成的软夹持器，通过物体接触导致的传感腔变形引发液体内部重分配，从而驱动抓取腔实现抓取，无需持续能量输入。

Result: 该夹持器能稳定且有选择性地基于目标尺寸完成抓握，同时内置液压反馈机制可根据物体刚度自适应调整夹持压力。

Conclusion: 该无源紧凑设计为轻量化、刚度自适应的软体流体操作提供了新思路，可用于特定尺寸的目标采样和水下、野外等环境操作。

Abstract: Conventional fluid-driven soft grippers typically depend on external sources,
which limit portability and long-term autonomy. This work introduces a
self-contained soft gripper with fixed size that operates solely through
internal liquid redistribution among three interconnected bistable snap-through
chambers. When the top sensing chamber deforms upon contact, the displaced
liquid triggers snap-through expansion of the grasping chambers, enabling
stable and size-selective grasping without continuous energy input. The
internal hydraulic feedback further allows passive adaptation of gripping
pressure to object stiffness. This source-free and compact design opens new
possibilities for lightweight, stiffness-adaptive fluid-driven manipulation in
soft robotics, providing a feasible approach for targeted size-specific
sampling and operation in underwater and field environments.

</details>
