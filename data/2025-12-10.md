<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 110]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.RO](#cs.RO) [Total: 33]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

TL;DR: 本论文提出了一种利用深度学习模型检测社交媒体中GIF图片网络欺凌的方法，并构建了包含4100余个GIF的相关数据集，检测准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 目前网络欺凌在社交媒体平台日益严重，虽然已有大量针对文本和部分针对图片的网络欺凌检测研究，但针对GIF/贴纸的检测研究极为稀少，因此有必要填补这一研究空白。

Method: 作者首先通过Twitter提取与网络欺凌相关的hashtags，并利用GIPHY的公开API爬取含有这些标签的GIF，最后获得包括网络欺凌和非网络欺凌的GIF共计4100余个。检测方法上，采用了深度学习预训练模型VGG16进行GIF中的网络欺凌识别。

Result: 基于VGG16模型的网络欺凌检测在所构建GIF数据集上的准确率达到了97%。

Conclusion: 本研究为GIF/贴纸的网络欺凌检测领域提供了研究数据集，并验证了深度学习方法在GIF欺凌检测中的有效性，为后续相关研究奠定基础。

Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [2] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 本文提出了一种利用深度学习与卫星影像对苏丹战争冲突火灾损毁进行近实时监测的方法，并在五个案例中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 苏丹持续战争带来的复杂局势对冲突事件的快速监测与分析提出了需求。传统方法存在时效性与准确性上的不足，因此迫切需要新的技术手段提升监控效率。

Method: 作者基于Planet Labs提供的4波段卫星影像，结合深度学习模型，对冲突中的火情和烧毁区域进行自动检测，并与基线方法进行对比，同时测试了增加到8波段或利用时间序列影像的效果。

Result: 基于4波段影像的自动化模型能够更为准确和及时地捕捉冲突中的火灾与焦土区域。与基线方法相比，在检测性能上显著提升，而提升为8波段或者利用时间序列数据的增益有限。

Conclusion: 采用深度学习与4波段卫星影像可以实现对战争火灾的高效、低延迟自动化监测，并且相较于更复杂数据，成本和效果平衡较好，适合实际应用。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [3] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频参考引导的面部替换方法LivingSwap，在长期视频序列中实现了高保真和时序一致的换脸效果。


<details>
  <summary>Details</summary>
Motivation: 影视制作中对高质量、时序一致的视频换脸有强需求，而现有方法在复杂场景和长序列上难以兼顾保真度和时序一致性。近期参考引导图像编辑的进展启发作者将此思路应用到视频换脸以提升效果。

Method: 提出了LivingSwap模型，利用关键帧作为条件信号注入目标身份，并采用视频参考引导，实现灵活可控的视频换脸。结合关键帧和时序引导，进行时序拼接以保证身份和画面稳定。为解决训练数据不足，作者构建了Face2Face数据集并进行成对反转提升监督效果。

Result: 广泛实验表明，LivingSwap在身份保持、表情、光照和运动等融合方面达到业界领先水平，大大减少了人工后期工作量。

Conclusion: LivingSwap实现了高保真、时序一致的视频换脸，能有效提升影视制作的自动化与效果，为相关领域提供了新的技术手段。

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [4] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种显式嵌入解剖结构层次的语义分割新框架，通过递归分级预测、输出头限制和特征调制提升牙科影像分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有面向解剖结构层次的分割方法主要通过损失函数弱、间接引入结构信息，难以精确反映实际解剖关系，增加精细结构识别难度。

Method: 提出基于递归、逐层预测架构，通过原图与前一级logits联合输入，利用Feature-wise Linear Modulation调制子类别特征，并引入概率合成规则与分级损失，确保解剖结构一致性。

Result: 在自建的牙科全景影像数据集上进行验证，基于UNet和HRNet实现，层次模型在IoU、Dice和召回率等核心指标上优于传统模型，尤其在精细结构表现和解剖一致性方面显著提升。

Conclusion: 显式引入解剖层次结构的方法不仅提升了分割性能，还增强了对于临床实际应用的解释力，尤其适用于低样本牙科影像场景，但也会带来假阳性增加的副作用。

Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [5] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 本论文提出了FRIEDA基准，用于测试大视觉语言模型（LVLMs）在复杂地图推理任务中的能力。研究发现，现有顶尖模型在该任务上的表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管地图推理技能对于现实中的地图解读和各类重要应用至关重要，但在AI领域几乎未被系统性评估。现有相关研究往往将地图与其他图表类视觉任务混为一谈，忽略了地图独有的复杂多层符号与空间关系解析难点。

Method: 作者构建了FRIEDA基准，从实际文档和报告中收集多领域、多地理区域的真地图，围绕GIS文献中定义的三类空间关系设计多步推理题目：拓扑关系（如边界、包含、相交）、度量关系（如距离）、方向关系（如方位）。共测评了11个主流LVLM，在“直接提供相关地图”与“需模型自行识别相关地图”两种设置下评估其能力。

Result: 即使是目前最强模型Gemini-2.5-Pro和GPT-5-Think，在FRIEDA任务上的准确率也仅为38.20%和37.20%，大大低于人类平均84.87%的表现，显示多步地图推理仍存在显著差距。

Conclusion: FRIEDA基准揭示了当前LVLM在复杂空间推理任务上的不足，为推动模型空间智能的发展和更真实地图理解能力的提升提供了重要参考。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [6] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSplain的可解释性方法，可以为ROP视网膜图像分类的神经网络模型生成结构保留（平滑且稀疏）的像素级解释。该方法优于现有常用解释器，在准确性和图像平滑性上表现更好，并能识别临床可理解的判别特征。


<details>
  <summary>Details</summary>
Motivation: 黑盒神经网络模型在医学诊断中的广泛应用，但缺乏可解释性，限制了临床医生的信任和理解，尤其现有解释方法存在无法兼顾结构平滑和稀疏的问题。

Method: 该研究提出SSplain方法，通过对优化问题施加组合约束并利用ADMM算法求解，实现了对输入图像结构的平滑和稀疏保留，生成具有临床相关性的像素级解释。

Result: 实验证明，SSplain优于主流解释方法（在后验准确性与平滑度上均有提升），并能提取与临床医生认知一致的判别性特征。同时，SSplain在其他公开数据集上的泛化能力也得以验证。

Conclusion: SSplain为医学图像AI模型带来了结构保留、解释性更强的解释方法，有助于医生理解和信任AI决策，在医学诊断等领域具有潜在推广价值。

Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [7] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,Gül Varol,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本论文提出了一个统一的手语理解模型，可同时实现手语翻译（SLT）和手语字幕对齐（SSA），在多个主流数据集上取得了领先效果，并具备良好的跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将手语翻译与手语字幕对齐作为独立任务处理，导致效率低、对实际应用支持有限。为此，作者提出将二者统一建模，以服务于手语与口语转换、字幕生成和教育等多种实际需求。

Method: 该方法包含三大核心组件：(i) 一种轻量化视觉骨干网络，从人体关键点和唇部区域图像中提取丰富的手部和非手部特征，并保护手语者隐私；(ii) Sliding Perceiver映射网络，将连续视频的视觉特征聚合为词级嵌入，有效连接视觉与文本信息；(iii) 多任务可扩展训练策略，联合优化SLT和SSA，增强模型的语言和时序对齐能力。同时，模型在BSL和ASL的大规模手语文本语料上进行多语言预训练，提升泛化性。

Result: 在BOBSL（英式手语）数据集上，该模型在手语翻译和手语字幕对齐任务上均实现了最新最优结果。模型还表现出对How2Sign（美式手语）数据集的强零样本泛化能力和微调性能，支持手语间的可扩展性。

Conclusion: 该统一模型不仅处于业界领先水平，还展示了优越的跨语言适应能力，为手语翻译、字幕制作及手语自动化处理提供了有效解决方案，在实际交流和教育等领域具有广阔应用前景。

Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [8] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 该论文提出了一种基于频域掩码的深度伪造检测方法，能更好地泛化到新型深度伪造图像，并兼顾高效能与绿色计算需求，达到当前最优的检测效果。


<details>
  <summary>Details</summary>
Motivation: 目前深度伪造生成模型更新迅速，检测系统需具备良好的泛化能力以应对未见过的新伪造类型。此外，大规模检测还需低计算成本，迎合绿色AI趋势，现有方法往往依赖大量数据和计算，对实时和大规模应用不友好。

Method: 作者提出在训练阶段采用频域随机掩码以及几何变换，重点通过频域掩码迫使检测器注意更具泛化能力的高维特征，无需依赖大型预训练模型。同时该方法可结合模型剪枝，进一步降低计算资源消耗。

Result: 该方法在针对多种GAN与扩散模型生成的图像测试中实现了最优的泛化性能，并在进行模型结构剪枝后依然保持较高检测率，体现了方法的实用性和可扩展性。

Conclusion: 频域掩码作为训练策略在提高深度伪造检测广泛适应性和资源效率方面表现突出，为实现可持续、通用的深度伪造检测提供了有效途径。

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [9] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: 本文提出了一种简单的持续测试时自适应（CTTA）方法，随机对输入图像做掩码（空间或频率）来提升模型在分布变化环境下的表现，省去了复杂的定制掩码机制，实验显示该方法效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法在应对测试阶段分布变化时，多依赖于复杂的掩码设计、校准的不确定性或稳定的注意力得分，增加了额外复杂性。作者希望探究简单随机掩码能否达到同样甚至更好的自适应效果。

Method: 提出Mask to Adapt（M2A）方法，通过对图像施加空间或频率域的随机掩码，生成多个视图，使用掩码一致性损失（对不同视图输出保持一致）和熵最小化损失（鼓励模型输出更自信），驱动模型自适应。详细对比不同掩码类型。

Result: 在CIFAR10C、CIFAR100C、ImageNetC等数据集高强度扰动测试下，M2A（空间掩码）优于或匹配其它CTTA基线，而M2A（频率掩码）效果稍逊。消融实验显示，简单的随机掩码同样非常有效和鲁棒。

Conclusion: 无需依靠不确定性或注意力机制，仅用随机掩码与一致性、熵目标即可实现强大的测试时自适应，降低实现复杂性，是一种既简单又有效的CTTA方法。

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [10] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: 本文基于PRODES遥感数据，评估和改进多种机器学习森林变化检测模型，重点提升亚马逊等巴西重要生态区的森林砍伐监测精度。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林的保护对全球气候、生物多样性和原住民文化至关重要。现有基于PRODES数据的自动化遥感监测模型效果有限，方法难以直接对比，因此亟需系统评估和改进。

Method: 作者在统一数据集上系统比较了多种变化检测模型，包括全卷积网络和引入Transformer自注意力机制的网络。还分析了多种前后处理技术（如分量面积过滤、纹理替换和图像增强）对提升检测精度的作用，并探索模型集成策略优化最终结果。

Result: 多样化的前后处理手段能大幅提升单模型的检测效果。多模型集成后取得了F1分数80.41%的成绩，达到业界较高水准。

Conclusion: 采用统一评测方法和引入现代深度学习模型，可显著提升遥感森林砍伐检测效果，对亚马逊区域森林保护具有重要意义。

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [11] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一种受中央-周边视觉启发的多模态框架（CVP），通过模拟人类视觉系统提升模型的空间推理能力，并在多个3D场景理解任务取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有的空间推理方法通常仅依赖于无结构的数据表示，并通过坐标嵌入隐式加入场景上下文，导致缺乏对场景结构的高层次理解，空间推理能力受限。

Method: 作者提出两大创新组件：中心倾向性token（模拟中央视觉）聚焦于与查询相关的目标；异中心网格（模拟周边视觉）负责编码全局场景上下文和空间关系。这两者被集成到大型多模态模型架构中，实现互补。

Result: CVP模型在多个3D场景理解基准测试中实现了业界领先水平，显示出优越的空间结构推理与全局上下文感知能力。

Conclusion: 通过引入受人类视觉启发的结构化表示，CVP不仅提升了多模态模型在复杂3D场景下的空间推理水平，也为后续相关研究提供了新方向。

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [12] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种高效的新型图像去雾方法Fourier-RWKV，兼具高性能和低计算量。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像去雾中表现优异，但计算复杂度高，不适合实时应用。论文旨在开发一种在复杂雾环境下既高效又高性能的去雾方法。

Method: 提出基于Multi-State Perception范式的Fourier-RWKV框架，通过：1）DQ-Shift实现空间感知以适应局部雾变化；2）Fourier Mix块用Fourier域扩展WKV注意力捕捉长距离依赖；3）SBM模块对齐编码器与解码器特征，抑制伪影。整个方法复杂度为线性，显著降低计算消耗。

Result: 在多个基准数据集上，方法不仅在各种雾环境下实现了最先进的性能，同时较大幅降低了计算开销。

Conclusion: Fourier-RWKV方法实现了去雾质量与效率的优良平衡，适用于实际部署。

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [13] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: 本文系统性分析了69种单目深度估计算法在KITTI数据集上的表现，发现提升模型准确率并不一定让其表现更接近人类感知，强调了评估指标应超越传统准确率，纳入人类感知相似性。


<details>
  <summary>Details</summary>
Motivation: 虽然深度神经网络在单目深度估计任务上取得了极高的物理准确度，但模型的表征与人类感知是否一致、二者间的权衡关系还缺乏深入理解。这对提升模型的鲁棒性与可解释性具有重要意义。

Method: 作者对69个深度估计算法在KITTI数据集进行测试，将模型输出与人的感知深度进行对比。通过仿射拟合（affine fitting）方法，将预测误差分解为可解释的多个成分，细致分析模型与人类在不同因素上的误差结构和关联。

Result: 实验结果显示，人类和DNN在深度估计上某些地方具有相似偏差（正误差相关），但在模型准确性与人类相似度之间存在权衡关系，即模型准确性提升不等于更类似人类表现。

Conclusion: 仅追求准确率无法使模型更具人类感知特性，评估与发展单目深度估计模型时，需要引入多维评价标准，包括与人类感知的一致性。

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [14] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: 本文提出了GeoLoom，一个将自然语言几何描述自动转化为高精度几何图形的系统，在结构准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的几何图生成要求空间精度高，同时自然语言描述又有良好的约束条件。现有基于形式化和符号求解的方法在正确性和可解释性上有所进步，但如何自动完成文本到图形的精确生成仍有挑战。

Method: 提出了GeoLoom框架，包括两个核心部分：（1）自动形式化模块，将自然语言转成面向生成的几何形式语言GeoLingua；（2）坐标求解器，利用高效的蒙特卡洛优化方法，根据形式化约束推算精确坐标。还提出GeoNF数据集，对齐自然语言与GeoLingua描述，并设计了基于约束的评价指标用于结构偏差度量。

Result: 实验证明，GeoLoom在结构一致性和图形准确性方面显著优于现有主流方法。

Conclusion: GeoLoom为可解释、可扩展的几何图自动生成提供了新的理论和实践基础，推动了文本到几何图生成领域的发展。

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [15] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种在低功耗微控制器(MCU)等资源受限设备上运行的动物个体重识别(Animal Re-ID)方案，并证明了其在多个数据集上的高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 动物重识别对于野生动物监测和精准畜牧管理具有重要价值，但现有模型多面向高性能计算环境，无法直接部署到内存和计算力有限的MCU设备，限制了其在实际野外大规模使用。

Method: （1）首先分析了当前主流Animal Re-ID模型与MCU硬件之间的差距，发现传统知识蒸馏方法在受限条件下效果有限。 （2）据此，基于MobileNetV2骨干网络，针对低分辨率输入系统性缩放，设计专用的高准确率轻量化模型架构。 （3）利用真实世界数据集评估该框架，并引入了只需三张图片即可快速微调以适应新环境的数据高效微调策略。

Result: 所提出的精简模型在六个公开Animal Re-ID数据集上，检索准确率与大型模型相当，但模型体积下降两个数量级。在自采牛只数据集上，模型可完全在设备端推理，Top-1准确率保持不变，仅有极小精度损失。

Conclusion: 论文首次展示了MCU级硬件上可行和适应性强的动物重识别系统，为野外环境中大规模、低成本应用铺平了道路。

Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [16] [Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement](https://arxiv.org/abs/2512.08215)
*Chia-Hern Lai,I-Hsuan Lo,Yen-Ku Yeh,Thanh-Nguyen Truong,Ching-Chun Huang*

Main category: cs.CV

TL;DR: Blur2Sharp提出了一种结合3D感知神经渲染和扩散模型的新方法，用于从单视图生成几何一致、高清晰的人体多视图图像。实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人体虚拟化方法，要么多视角几何不一致，要么牺牲真实感导致模糊、失真。在复杂动作和视角切换下，这成为构建逼真人体数字分身的主要难点。研究驱动源于对高质量、多角度且结构连贯的拟真人体渲染的实际需求。

Method: Blur2Sharp提出双重条件架构。首先，利用Human NeRF对目标姿态生成结构一致的多视图渲染，明确编码3D结构引导。然后，通过扩散模型对这些渲染结果进行细化，进一步提升细节与结构保真度。同时，通过分层特征融合，引入从SMPL参数化模型提取的纹理、法线与语义先验，以兼顾整体一致性和局部细节精度。

Result: 大量实验（包括穿着宽松服装、遮挡等复杂情形）显示，Blur2Sharp在新姿态和新视角下的图像生成质量明显优于现有同类方法，图像更加清晰且几何一致。

Conclusion: Blur2Sharp为逼真、多视角人体渲染提供了有效新范式，特别适合高难度场景，在数字人等应用中具备显著优势。

Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

</details>


### [17] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一个多模态视觉知识库，通过系统地整合图像和文本信息提升对物体类别的深入理解，并以动物类别为例构建了AnimalKB知识库，验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的物体识别只能提供类别标签，难以支持深层次的理解，例如组成部分、属性和与背景知识的关联。现有多模态数据不成体系，难以支持如推理和问答等高级任务，因此需要一个结构化的多模态知识库以弥补这一不足。

Method: 作者提出了视觉知识库Visual Knowledge Base并开发了构建框架VisKnow，将图像和文本知识以图结构组织起来。VisKnow通过专家手工设计与大模型自动化抽取结合，提取和对齐物体和部件层级的区域标注，并整合百科类文本三元组和大规模图片数据。具体以动物类别出发，构建AnimalKB涵盖406种动物、2.2万文本知识三元组、42万图片及标注。

Result: 实验显示，AnimalKB在零样本识别、细粒度视觉问答等任务中提升了视觉智能表现，并为知识图谱补全和部件分割等任务提供了有挑战性的基准数据集。

Conclusion: 结果表明，自动化多模态视觉知识库的构建能有效推动对物体的深入视觉理解，有助于推动相关实际应用的发展。

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [18] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文探索了以提示调优（prompt tuning）方法用于3D目标检测的有效性，并提出场景导向的提示池，显著提升了不同场景下的检测效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在NLP领域通过提示调优取得了良好迁移效果，但3D目标检测领域对类似方法的研究较少。作者想验证在3D目标检测任务中，预训练模型（如在Waymo大规模数据集训练）是否也能通过提示调优适应新场景。

Method: 先考察提示token与提示生成器对3D目标检测的影响，再提出场景导向的提示池（Scene-Oriented Prompt Pool, SOP$^2$），通过在不同场景下调用不同提示以提高检测自适应性和效果。

Result: 实验表明提示池方法能显著提升模型在新场景下的检测表现，验证了提示调优在3D目标检测领域的可行性与潜力。

Conclusion: 提示调优方法及场景导向提示池显著提升了3D目标检测任务中的泛化能力，为后续研究提示方法在3D领域的应用奠定了基础。

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [19] [New VVC profiles targeting Feature Coding for Machines](https://arxiv.org/abs/2512.08227)
*Md Eimran Hossain Eimon,Ashan Perera,Juan Merlos,Velibor Adzic,Hari Kalva*

Main category: cs.CV

TL;DR: 本文针对分割推理系统中神经网络中间特征的压缩，分析了现有视频编解码方法在此场景下的表现，并提出了三种高效的VVC压缩配置。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码标准以人眼感知为中心进行优化，但在AI推理场景下传输的是抽象稀疏的神经网络特征，感知相关性失效。因此需要针对机器使用场景优化特征压缩方法，提升下游任务精度及压缩效率。

Method: 作者对符合MPEG-AI FCM标准下，基于VVC的视频压缩算法进行了细致的工具级分析，研究各组件对特征压缩率和下游计算机视觉任务表现的影响。基于分析结果，设计了三种‘Fast’系列轻量配置，分别在压缩效率与编码速度上做权衡。

Result: 提出的Fast配置在提升2.96%压缩率的同时，编码速度提升21.8%；Faster配置压缩率提升1.85%，编码速度提升51.5%；Fastest配置编码速度提升95.6%，压缩率仅微降1.71%。

Conclusion: 传统感知驱动的视频编解码并不适合AI特征传输，基于深入分析后设计的轻量VVC配置能更好地服务机器学习推理场景，在兼顾压缩性能和速度的同时，保证下游任务结果。

Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.

</details>


### [20] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出了一个新基准MM-CoT，用于评测多模态模型在视觉-推理链条中的视觉与逻辑一致性，发现当前主流模型在真实推理能力上明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型虽能生成复杂视觉推理链条（CoT），但缺乏对其推理链条是否真正基于视觉证据并逻辑自洽的有效评测工具。多数基准关注生成能力，而忽略了推理链条的核查，即推理过程的可验证性。

Method: 作者设计了MM-CoT基准，要求模型从限定选项中选出唯一同时满足视觉一致性和逻辑自洽性的事件链。基准中还设计了有针对性的对抗干扰选项，分别故意破坏视觉或逻辑一致性，以检测模型在两方面的具体失误。

Result: 在对主流视觉-语言模型进行评测后发现，即便是最先进的模型在该基准上的表现也很有限，说明其生成流畅性和真实推理一致性之间存在巨大差距；MM-CoT得分与其他基准几乎无关，表明其确实测量的是不同能力。

Conclusion: MM-CoT为多模态模型推理的真实一致性评测提供了新基础，有助于推动未来模型不仅提供似是而非的推理，更能做到视觉与逻辑上的忠实和自洽。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [21] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种逐步重参数化策略，通过用更紧凑的PHM层逐渐替换多模态语言模型中的密集前馈网络块，实现模型压缩并显著降低模型参数量和计算量，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型通常需要庞大的参数量来对齐视觉特征与文本表示，导致计算开销大，难以高效部署，因此亟需更高效的模型压缩和部署方法。

Method: 作者采用一种逐步的方法，将密集前馈网络块以阶段性方式替换为参数更少的PHM层。训练过程中引入残差插值、轻量重建损失和知识蒸馏损失，确保替换后的模块能继承原有功能。

Result: 在多个视觉-语言模型上验证，该方法在大幅压缩模型参数量和加速推理速度的同时，性能基本不降，达到与原模型相当的多模态对齐效果。

Conclusion: 逐步PHM替换方法为多模态模型的高效推理和部署提供了一条兼容现有架构的新路径，并可与低比特量化等技术互补。

Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [22] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于法向量引导的稀疏深度采样策略，有效提升了深度补全模型的准确性和贴近现实传感器行为。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏深度补全方法大多随机采样稀疏像素，忽略了真实传感器的空间非均匀和几何相关特性，导致训练环境与现实不符，从而影响模型泛化和实用性。

Method: 作者使用PCA方法估算RGB-D点云的表面法向量，以获得每像素的深度可靠性分布，再根据该分布选取稀疏深度样本。该采样方法被集成到Marigold-DC扩散型深度补全模型中，并在NYU Depth v2数据集上进行实验证明其有效性。

Result: 实验显示，基于几何信息的稀疏深度采样策略不仅提升了深度补全准确性，还减少了在边缘及不连续区域的伪影，并赋予了训练过程更高的现实感。

Conclusion: 该方法更好地模拟了实际传感器行为，为深度补全研究提供了更接近真实场景的训练数据生成方式，有助于提升机器人系统的三维感知能力。

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [23] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

TL;DR: 本文提出了一种基于姿态关键点的手语发现（Sign Language Spotting）新任务和方法，用于在连续手语视频中检索特定手语。


<details>
  <summary>Details</summary>
Motivation: 尽管自动手语识别在连接聋人与听人之间起着重要作用，但在连续手语流中检索和定位特定手语（即“手语发现”）问题仍未被充分研究。对此需求进行探索，有助于推动手语检索相关技术的发展。

Method: 作者提出一种端到端的模型，直接利用从手语视频中提取的姿态关键点特征，不依赖中间的gloss识别或文本匹配。采用仅编码器结构（encoder-only backbone）和二分类输出，用于判断查询手语是否在目标序列中出现。此方法着重姿态表示，减少了计算量及视觉噪声。

Result: 在WSLP 2025的Word Presence Prediction数据集上进行评测，取得了61.88%的准确率和60.00%的F1分数。

Conclusion: 基于姿态关键点的方法在手语发现任务上表现可靠，为后续自动手语检索和验证提供了研究基础和方向。

Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [24] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: FastBEV++提出了一种无需定制插件、兼具高精度和易部署性的摄像头俯视图感知框架，在保持实时性的同时取得了新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有摄像头BEV感知方法虽精度高，但因依赖复杂的视图变换和定制化算子，难以高效部署到车辆端；需解决性能与部署之间无法兼得的瓶颈。

Method: 提出FastBEV++框架，采用“通过算法加速”和“可部署性优先”两大原则：（1）引入Index-Gather-Reshape分解的新型视图变换，无需特制CUDA算子，完全用原生运算实现，便于移植到TensorRT等平台；（2）基于该结构集成端到端的深度感知特征融合、时间聚合和数据增强，提升BEV几何精准度。

Result: 在nuScenes数据集上，FastBEV++达到新SOTA的0.359 NDS，同时在Tesla T4等车规级硬件上实现超过134FPS的推理速度，兼具高效与准确。

Conclusion: FastBEV++无需定制插件即可实现极高性能和平台适配性，展现出成熟、可扩展的自动驾驶量产系统设计理念。

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [25] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: HTC-VLM提出了一种混合视觉语言模型，在实现高效表示的同时兼顾了语义和细节，显著提升了多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量视觉patch token时，计算与内存消耗巨大，而传统压缩方法难以兼顾高层语义信息与细节信息。需要一种新方法提高效率的同时不损失关键信息。

Method: 提出HTC-VLM，采用连续通道传递视觉细节（ViT patch），离散通道通过MGVQ量化捕获高层语义锚点，将两者通过特殊注意力机制和瓶颈压缩为一个voco token，从而实现高效混合表示。

Result: 在7个主流多模态基准上，HTC-VLM在580倍压缩率下准确率达87.2%，显著超过领先的连续模型（81.0%），消融验证了离散锚点对语义引导作用。

Conclusion: HTC-VLM以极简的混合设计有效平衡效率与表示保真度，推动了更大规模视觉语言模型的发展。

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [26] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的深度混合型残差SwinCA-Net分割框架，并在BUSI乳腺肿瘤超声数据集上实现了超过99%的分割准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 乳腺肿瘤超声影像分割任务因为噪声、组织连续性差和结构细节复杂而面临巨大挑战，现有CNN和Transformer方法在特征提取和全局依赖学习方面各有不足，因此亟需新方法提升分割鲁棒性与准确性。

Method: 提出了融合残差CNN和定制Swin Transformer的混合架构，通过区域Laplacian-of-Gaussian算子增强组织连续和边界细节，采用逐步收缩策略和多尺度通道注意力（MSCAS）提升特征表达，同时引入像素注意力模块抑制背景干扰。

Result: 在公开的BUSI乳腺肿瘤超声图像数据集上，所提方法平均分割准确率达99.29%，IoU为98.74%，Dice系数为0.9041，全面优于现有CNN和ViT方法。

Conclusion: 所提出的Residual-SwinCA-Net框架有效提升了乳腺超声肿瘤分割与诊断性能，可辅助临床早期、精准决策，具备较高的实际应用价值。

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [27] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于稀疏查询的未来时序知识蒸馏方法（FTKD），用于提升在线3D目标检测模型的性能，能有效利用离线模型的未来帧信息，在不增加推理成本的前提下明显提升精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于摄像头的时间序列3D目标检测，离线模型通过利用未来帧可以提升精度。知识蒸馏有望将离线模型的信息迁移到在线模型中，但现有蒸馏方法主要关注空间特征蒸馏或时序关系，未能有效利用未来知识，导致在线模型学习未来信息受限。

Method: FTKD（未来时序知识蒸馏）方法通过设计未来感知特征重建策略，使学生模型在无须严格帧对齐的情况下学习未来特征。同时，引入未来引导的logit蒸馏，有效利用教师模型在前景和背景的语境信息。该方法可直接应用于高性能3D目标检测基线模型。

Result: 在高性能3D检测基线模型上，FTKD方法在nuScenes数据集上实现了最高1.3 mAP和1.3 NDS的提升，并在速度估计精度方面也取得最佳结果，同时没有增加推理成本。

Conclusion: FTKD方法能有效将离线模型的未来时序知识迁移到在线模型，不增加部署开销的情况下，大幅提升3D目标检测精度和速度估计表现。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [28] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: 本文针对小样本3D点云语义分割任务，提出了一种新的查询感知原型学习方法，通过充分建模支持集和查询集的语义相关性，有效提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本点云分割方法通常仅基于支持集生成原型，忽略了与查询数据的相关性，导致原型容易过拟合支持集特征，在分布偏移时泛化能力差，影响分割效果。

Method: 提出Query-aware Hub Prototype (QHP) 学习方法，包含两个核心模块：1）Hub Prototype Generation (HPG) 模块，在支持集和查询集之间构建二部图，识别高频连接的“枢纽”支持点并生成更具查询关联性的原型；2）Prototype Distribution Optimization (PDO)模块，采用纯度重加权对比损失，优化原型分布，使错误或边界模糊原型更贴近类中心。

Result: 在S3DIS和ScanNet数据集上，所提QHP方法在小样本点云分割任务中取得了显著优于现有方法的性能提升，有效缩小了原型与查询集之间的语义鸿沟。

Conclusion: QHP通过联合支持集和查询集信息生成高质量原型，并优化原型分布，极大提升了小样本3D点云分割的泛化能力和精度，对FS-3DSeg方向具有较大推动作用。

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [29] [SFP: Real-World Scene Recovery Using Spatial and Frequency Priors](https://arxiv.org/abs/2512.08254)
*Yun Liu,Tao Li,Cosmin Ancuti,Wenqi Ren,Weisi Lin*

Main category: cs.CV

TL;DR: 该论文提出融合空间和频率先验（SFP）的方法，以提升真实场景下图像恢复的表现，针对多种退化类型提供了有效恢复方案。


<details>
  <summary>Details</summary>
Motivation: 现有场景恢复方法要么仅依赖单一先验，难以应对复杂的多重图像退化，要么依赖复杂网络及合成数据，泛化性差。为此，作者希望提出一种结合多种先验、能更好适用于多样真实退化场景的方法。

Method: 作者分别在空间域和频率域提出两种先验：（1）空间域借助降质图像反演后与场景透射特性的关联，估算透射图进而恢复场景；（2）频率域构建自适应增强掩模，提出两个新颖频率先验：一是DC分量各通道的均值与清晰图像接近；二是清晰图中低于0.001的低径向频率幅度占总谱约1%。最后设计加权融合策略整合空间、频率恢复结果及输入图像显著特征。

Result: 广泛实验验证了所提SFP方法在应对多种退化条件下，具备优越的场景恢复能力。

Conclusion: SFP方法能有效融合空间与频率先验，显著提升多退化场景下的图像恢复效果，优于传统单一先验或复杂深度模型。

Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.

</details>


### [30] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

TL;DR: 本文提出了RLCNet，一种端到端可训练的深度学习框架，实现LiDAR、RADAR和摄像头的在线联合标定，具备高精度、强鲁棒性和实时动态调整能力，在实际环境和数据集上效果优异，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶依赖多传感器感知，传感器如LiDAR、RADAR和摄像头的精确外参标定直接影响车辆感知结果，但在动态环境下，机械振动和时间积累的传感器漂移导致在线标定极具挑战性，难以满足实际场景需求。

Method: 提出了RLCNet，一个可端到端训练的深度神经网络，用于多模态传感器（LiDAR、RADAR、摄像头）同时在线标定。框架集成加权滑动平均和异常值剔除等机制，降低预测噪声和漂移影响，实现参数动态调整。论文还通过消融实验分析了网络结构设计的影响。

Result: RLCNet在真实数据集上进行了验证，表现出优于现有方法的标定精度和鲁棒性，能够适应各类复杂环境。实时在线标定算法能够显著减小噪声及漂移，提高了实用性。

Conclusion: 论文证明了RLCNet在多传感器协同在线标定中的优越性，为自动驾驶感知系统部署稳定、精确的标定方案提供了有效工具，并具有现实应用前景。

Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [31] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出了EgoX框架，可将第三人称（exocentric）视频生成第一人称（egocentric）视频，并在几何一致性和视觉保真度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前将第三人称视频转化为第一人称视频面临着视角极大变化和可见区域重叠极小的挑战，难以兼顾内容保留与未见区域的合成。

Method: EgoX框架利用预训练大规模视频扩散模型的时空特征，结合轻量化的LoRA适配，并通过宽度和通道拼接的方式统一引入第三人称与第一人称先验，此外引入几何引导的自注意力机制，提升空间相关区域的关注能力。

Result: EgoX能够生成连贯且真实的第一人称视频，在未见场景及真实世界视频测试中展现了良好的可扩展性与鲁棒性。

Conclusion: EgoX为实现高质量第三人称到第一人称视频转换提供了可行方案，对提升沉浸式视频理解具有重要意义。

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [32] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了一种结合物理推理的可生成更符合真实物理属性声音的视频转音频（V2A）方法。


<details>
  <summary>Details</summary>
Motivation: 当前V2A方法主要关注视觉与声学之间的相关性，忽略了影响真实世界声音的物理因素，因此生成的音频缺乏物理合理性。作者希望通过引入物理信息，提升生成音频的逼真度。

Method: 提出物理感知视频转音频合成方法（PAVAS），关键是引入Physics-Driven Audio Adapter（Phy-Adapter），接受物理参数估算器（PPE）输出的物体质量和运动轨迹等参数。这些参数分别通过视觉-语言模型估算物体质量，通过分割驱动3D重建获得运动轨迹和速度信息，最后用于声源合成。

Result: 实验中，提出了用于评测物理真实感的新基准数据集VGG-Impact和新指标APCC。实验结果显示，所提方法在物理合理性和听感一致性上，均优于现有V2A模型。

Conclusion: 该研究证明，将物理信息引入视频转音频生成，能够有效提升生成音频的物理合理性和主观听感，推动V2A模型朝向更加真实和自然的方向发展。

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [33] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出了OpenSubject，这是一个包含250万样本和435万图像的大规模视频衍生数据集，用于支持主体驱动的图像生成与编辑，有效提升了多主体复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的主体驱动图像生成模型在复杂或多主体场景下容易偏离参考身份，生成效果不理想。因此亟需更大规模、高质量、能增强模型泛化能力的数据集支撑模型训练。

Method: 作者设计了四阶段数据集构建管线：一是筛选高质量视频片段；二是通过视觉-语言模型筛选并配对具有代表性的图像；三是结合分割图与盒引导的生成方法，以及几何增强与边界处理，合成参考图像；四是用视觉-语言模型校验并为样本生成多种描述。此外，为评估模型效果，作者还提出了新的基准，利用VLM自动评判多项核心指标。

Result: 实验证明，使用OpenSubject数据集训练能显著提升主体一致性、提示词遵循、编辑及背景一致性等多方面指标，尤其在复杂多主体场景中效果更突出。

Conclusion: OpenSubject为主体驱动图像生成和编辑提供了优质多样的数据资源，实验表明可有效提升模型在复杂任务下的表现，为后续相关研究提供了基础和新方向。

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [34] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 本文提出了一种新的地形生成方法Terrain Diffusion，将扩散模型引入程序化世界生成，实现了高真实感、大规模一致性和无限制生成。


<details>
  <summary>Details</summary>
Motivation: 传统的程序化地形噪声（如Perlin噪声）虽然高效、可无限扩展，但在逼真度和大范围一致性上存在局限性。作者希望突破这些限制，实现既真实又无缝、可控且高效的世界生成。

Method: 提出InfiniteDiffusion算法，它用分层扩散模型结合行星级上下文和局部细节，通过紧凑的拉普拉斯编码和一致性蒸馏，支持实时、无缝、无限大地形生成。还开发了支持常量内存操作的无限张量框架。

Result: 方法实现了可控、高效且大规模的一致性世界生成，能够合成具有真实细节和范围的行星级地形。

Conclusion: 扩散模型可成为程序化世界生成的新基础，未来可用于无缝、逼真且可控的虚拟环境及地形合成。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [35] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed Doğan,Nicolas R. -Peña,Alfred Schoettl*

Main category: cs.CV

TL;DR: 本文提出了一种全新的基于深度数据的6D位姿估算方法，能够在密集、遮挡严重的工业抓取场景下高效识别物体位姿，并在公开数据集上取得了有竞争力的效果。


<details>
  <summary>Details</summary>
Motivation: 在工业抓取环境中，物体密集堆叠、遮挡严重，表面缺乏纹理，导致现有6D位姿估算方法难以准确恢复物体姿态，亟需更高效鲁棒的新方法。

Method: 作者提出了一种多视角深度信息融合框架，可生成高分辨率稠密点云或稀疏TSDF体积分布。核心方法包括分阶段热力图机制实现适应性关注与显存控制，以及密度感知稀疏Transformer块处理自遮挡和3D数据非均匀分布。通过全稀疏处理策略和创新的体素投票方法，对场景内任意数量物体同时预测6D位姿。

Result: 在IPD和MV-YCB多视角数据集上测试，方法在工业及家居环境下密集堆叠场景实现了高分辨率、细粒度几何特征捕捉，取得了有竞争力的结果。

Conclusion: 该方法有效缓解了遮挡、反射和无纹理物体带来的挑战，实现了高效、高精度的6D位姿估算，对密集工业抓取和相关机器人应用具有较大潜力。

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [36] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: 论文提出GeoDM，一种几何感知的数据集蒸馏方法，通过在欧式-双曲-球面乘积空间中进行分布匹配，更好地保持原始数据流形几何，超越现有线性方法。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集蒸馏方法通常局限于欧氏空间，无法捕捉高维数据的真实流形结构（如曲率），导致提取的数据子集缺乏几何代表性。高维数据通常分布在低维流形上，因此需对蒸馏的子集流形结构进行对齐，以更好地聚合数据的本质特征。

Method: 提出了GeoDM（Geometry-aware Distribution-Matching）框架，将数据嵌入欧氏、双曲、球面三类空间的笛卡尔乘积，统一捕捉平坦、层级、周期等多种几何结构。方法引入了可学习的几何曲率和权重参数，并设计了最优传输损失，以提升分布拟合度。

Result: 理论上证明了在乘积空间进行的几何感知分布匹配能获得比单一欧氏空间更小的泛化误差上界。实验上，GeoDM在多个标准基准数据集上的表现超越了现有最优数据蒸馏方法，在单一几何空间的各种分布拟合策略下依然有效。

Conclusion: GeoDM通过显式建模复杂流形几何，实现更精确的数据集蒸馏，在理论和实验中均优于传统欧氏分布拟合方法，为分布匹配和数据压缩任务提供了新的路径。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [37] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出了一种自动化多摄像头手术视频对齐与选择的方法，提升了外科手术视频的连贯性和观看体验。


<details>
  <summary>Details</summary>
Motivation: 开放性手术录像对于教学和科研十分重要，但拍摄时常因外科医生遮挡造成视频画面被挡，且频繁调整摄像头位置和角度极为繁琐，人工后期对齐难以满足自动化和高效需求。

Method: 在手术灯上安装多台摄像头以环绕拍摄，每当手术灯移动导致摄像头配置发生变化时，自动检测移动帧、重新对齐并从各路摄像头中选择遮挡最少的画面，最终生成保持一致视角且无遮挡的手术视频。

Result: 用户研究表明，该方法生成的视频在外科手术区域可确认性和观感舒适度上均优于传统方法，并且视频质量有显著提升。此外，研究还实现了多种新的视频合成选项，并基于外科医生偏好开展了用户调研。

Conclusion: 提出的方法为自动化、高质量的手术视频录制提供了新途径，能有效提升外科教学与科研所需的视频资料质量和观看体验。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [38] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 本论文介绍了3DTeethLand挑战赛，这是首个面向3D牙齿标志点检测的公开数据集及竞赛，旨在推动深度学习方法在三维牙齿标志点检测领域的发展。


<details>
  <summary>Details</summary>
Motivation: 由于牙齿结构复杂且个体差异大，临床正畸对牙齿标志点的精准识别需求增长，而现有技术在准确性和稳定性方面存在瓶颈。因此，亟需基于深度学习的先进方法来提升3D牙齿标志点检测的效果。

Method: 通过与MICCAI 2024合作，举办了一个全球竞赛，发布首个公开3D牙齿标志点数据集，吸引研究人员开发和比较不同的算法，专注于利用深度学习方法从口内3D扫描中检测牙齿标志点。

Result: 挑战赛为牙齿标志点检测提供了一个统一且高质量的三维数据评测平台，促进了领域内各种前沿检测算法的开发与性能评估，并推动了社区对该问题的关注。

Conclusion: 此次挑战赛构建并发布了首个公开可用的3D牙齿标志点检测数据集，有效推动了相关深度学习方法的进步，对临床口腔医学具有重要意义。

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [39] [Accelerated Rotation-Invariant Convolution for UAV Image Segmentation](https://arxiv.org/abs/2512.08888)
*Manduhu Manduhu,Alexander Dow,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 提出了一种GPU优化的旋转不变卷积方法，显著提升了细粒度目标分割任务的效率和性能，尤其适用于目标有任意朝向的无人机航拍影像。


<details>
  <summary>Details</summary>
Motivation: 无人机航拍图像中的目标通常具有任意朝向和细小结构，对高精度分割提出更高要求。现有的分割模型（如U-Net）不具备旋转不变性，容易因观测角度变化导致性能下降。传统实现旋转不变的方法（如多方向卷积）虽然有效，但会带来较大计算和存储开销，因此亟需高效的旋转不变分割方法。

Method: 本文提出了一种GPU优化的旋转不变卷积框架，去除了常规数据展开（im2col）步骤，利用对称旋转滤波器间的数据共享，实现多方向卷积，显著减少内存和计算冗余。同时，该框架可以推广至任意非对称角度的旋转卷积，兼容性强。

Result: 在广泛基准测试下，所提卷积方法比CUDNN快20-55%，能耗低15-45%，准确率与主流旋转不变方法相当。在8方向设置下，对于256×256图像速度提升45%、节能41%，1024×1024图像速度提升32%、节能23%。结合U-Net，精度比无旋转感知基线提升6%。

Conclusion: 该方法为旋转不变CNN提供了高效且有效的新选择，可在保证分割精度的基础上大幅提升训练速度和能耗表现，适合实际部署在要求高效的场景（如无人机航拍分析等）。

Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.

</details>


### [40] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的拉格朗日视频运动放大方法GeoDiffMM，通过使用光流作为几何线索，实现结构一致的运动放大，并抑制放大过程中产生的噪声。该方法在多套真实和合成数据集上性能优越，显著提升了微小运动可视化的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的欧拉法视频运动放大虽能让不可见的细小运动可视化，但在处理极微小运动时，难以分离光子噪声与真实微运动，造成噪声放大。如何消除放大过程中的噪声，获得更结构化、真实的运动放大效果，成为亟需解决的问题。

Method: 作者提出了基于扩散模型的拉格朗日VMM框架GeoDiffMM，其核心创新有：1）利用无噪声的光流增强策略，生成多样化非刚性运动场，作为监督信号，提升光流估计的准确性和泛化能力；2）提出扩散运动放大器，将去噪过程依赖于由几何先验的光流和可学习的放大系数，选择性地放大与场景结构一致的运动分量，抑制无关扰动；3）最后基于放大后的光流做视频合成，实现图像域的高保真还原。

Result: 在多个真实与合成数据集的实验中，GeoDiffMM方法在运动放大效果和保真度上均优于现有方法，有效提升了运动的可感知度，并且噪声显著减少。

Conclusion: GeoDiffMM为视频运动放大提供了一种更结构一致、抗噪性能强的新范式，解决了微小运动与噪声难以分离的问题，为高质量运动放大应用奠定基础。

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [41] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: LiDAS系统通过主动调控汽车前灯照明，自适应强化夜间相机感知效果，无需重新训练模型即可大幅提升夜间目标检测与分割性能，并节能降耗。


<details>
  <summary>Details</summary>
Motivation: 夜间环境光线不足，严重影响基于相机的自动驾驶感知系统。现有方法仅依赖场景光照，导致感知性能下降，亟需高效、通用且经济的夜间感知增强方案。

Method: 提出了Lighting-driven Dynamic Active Sensing（LiDAS）系统，将高分辨率前照灯与视觉感知模型闭环结合。LiDAS动态预测和控制照明分布，自动减少无用区域光照，将光能集中投射至感兴趣对象区域，实现主动智能照明。模型训练于合成数据，并直接应用于真实自动驾驶场景，无需夜间数据或再训练。

Result: 在真实驾驶场景下，无需夜间再训练，LiDAS系统相较传统低灯功率下mAP50提升18.7%，mIoU提升5.0%，并节省40%能耗。与领域泛化方法结合，可进一步增强稳健性。

Conclusion: LiDAS能通过智能控制现有车辆前灯，实现低成本、高效且泛化性强的夜间视觉感知增强，为自动驾驶等场景下夜间安全感知提供了新的解决路径。

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [42] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的彩色图像分类方法LSQMM，通过将RGB通道视为四元数，有效保持颜色通道间的联系，并利用四元数核范数正则化提升低秩结构，实验显示该方法在分类精度、鲁棒性和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统彩色图像分类方法往往只将特征表示为实数域的向量、矩阵等，未能有效保持RGB通道之间的耦合关系。已有研究表明四元数建模在图像恢复和去噪任务中有显著优势，启发作者在分类任务中使用更强的通道耦合模型。

Method: 提出LSQMM（低秩支持四元数矩阵机）方法，将三通道RGB数据用纯四元数表示，通过四元数代数保留通道内在联系。模型在损失函数中加入四元数核范数正则项以鼓励低秩。为求解四元数优化问题，设计了基于ADMM的迭代算法。

Result: 在多个彩色图像分类基准数据集上，该方法在分类准确率、鲁棒性及计算效率方面均优于支持向量机、支持矩阵机及支持张量机等多种先进方法。

Conclusion: LSQMM可有效提升彩色图像分类任务中对颜色通道相关信息的建模能力，为基于四元数的模式分析方法提供了新思路。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [43] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 本文系统性分析了如Glaze和Nightshade这样的图像保护机制，通过多种可解释AI方法揭示其扰动的内部结构和可检测特征。结果显示，这些保护手段造成的是有结构、与图像内容相关的扰动，而非随机噪声，其信号在多域表现出一致的可检测性特征。


<details>
  <summary>Details</summary>
Motivation: 虽然近期的图像保护机制（如Glaze和Nightshade）在防止生成式AI滥用方面效果显著，但它们的内部工作机理、扰动结构和可检测性尚不清楚，难以为将来的防护和检测方法提供理论依据。

Method: 作者提出了一套统一的分析框架，结合了白盒特征空间检查与黑盒信号层级探测。具体方法包括潜在空间聚类、特征通道激活分析、基于遮挡的空间敏感性映射、以及频域特征分析，对当前主流保护机制的图像扰动进行多角度深入剖析。

Result: 研究发现，保护机制导致的扰动是与图像内容高度耦合的低熵结构扰动，而非无序变化。受保护图像在内容特征的基础上呈现保护相关的亚结构，未出现全局特征漂移。扰动的可检测性与其熵值、空间布局及频谱特性的相互作用有关，并且多次保护反而增加可检测信号。Glaze和Nightshade通过沿主要图像频率轴重分配能量，而不是简单加入噪声。

Conclusion: 当前的图像保护机制实质上是对特征级进行结构化扰动，并非语义层面的位移，因此这些信号即使视觉上不明显也易被检测。该研究增进了对对抗性图像保护手段的可解释性认知，为未来防护与检测方法设计提供了理论基础。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [44] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: 提出了一种结合扩散模型和对比学习的新方法PointDico，在3D数据表征中取得了当前最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有自监督3D表示学习方法（对比型和生成型）存在各自劣势：对比模型易过拟合，3D掩码自编码器难以处理点云无序特性。为提升3D点云表示能力，需要融合不同范式的优点。

Method: 提出PointDico模型，将扩散生成建模与跨模态对比学习结合，通过知识蒸馏让扩散模型引导对比模型。设计了金字塔递阶条件生成器捕捉多尺度几何特征，双通道结构整合局部与全局上下文信息。

Result: PointDico在3D表征学习任务中取得了新SOTA成绩，如在ScanObjectNN上94.32%分类准确率，在ShapeNetPart上86.5%实例mIoU。

Conclusion: 通过整合扩散与对比学习，PointDico有效提升了3D点云的表示能力，为3D自监督学习提供了新范式。

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [45] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: 本文提出一种高效的自适应卷积方法（Bi^2MAC），以提升遥感图像融合性能，同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的全色锐化（pansharpening）方法难以有效适应图像中的区域异质性，且现有自适应卷积方案计算开销大、对异质区域建模能力有限。

Method: 提出Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC)，采用轻量模块生成软掩码与硬掩码，分别进行特征调制及区域分支引导。冗余特征进入低成本分支处理，异质特征进入高资源分支实现精细建模。

Result: 在多个基准数据集上的实验表明，Bi^2MAC在保持最小计算开销和参数量的情况下，取得了现有最优性能，并显著缩短训练时间。

Conclusion: Bi^2MAC不仅有效融合不同区域信息，还实现了高效资源分配，是遥感图像融合任务中兼具性能和效率的先进方法。

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [46] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 提出了一种新的Hybrid Splatting方法，通过反射烘焙和高斯剪枝，大幅提升了3D高斯splatting在复杂反射场景渲染的速度和存储效率，且能保持高质量渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯splatting在真实复杂反射场景新视图合成中渲染速度慢、内存占用大，成为进一步提升光真实渲染的瓶颈。

Method: 1. 提出反射烘焙高斯追踪，将视角相关的反射信息集成到每个高斯基元中；2. 利用基于tile的高斯splatting方式进行反射渲染；3. 通过统一的hybrid splatting框架集成基础与反射高斯基元；4. 引入管线级加速和基于反射敏感度的高斯剪枝，有效降低模型规模。

Result: 在Ref-NeRF、NeRF-Casting等复杂反射场景上，HybridSplat相较于基于光线追踪的高斯splatting，渲染速度提升约7倍，同时高斯基元数量减少4倍，渲染质量依然出色。

Conclusion: HybridSplat成为复杂反射场景新视角渲染新的SOTA方法，不仅显著提升了速度和存储效率，也保证了高质量的反射渲染。

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [47] [DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation](https://arxiv.org/abs/2512.08337)
*Jianwei Wang,Qing Wang,Menglan Ruan,Rongjun Ge,Chunfeng Yang,Yang Chen,Chunming Xie*

Main category: cs.CV

TL;DR: 本文提出DINO-BOLDNet模型，能直接从T1w结构像生成BOLD功能像，解决BOLD数据缺失问题，模型基于DINOv3自监督transformer，结果优于现有条件GAN方法。


<details>
  <summary>Details</summary>
Motivation: BOLD影像对于脑功能分析至关重要，但实际中常因噪声、缺失导致BOLD像不可用。当前缺乏高效精准的从结构像（T1w）恢复BOLD像的解决方案，因此作者希望开发一种能够在T1w缺失BOLD情况下重建可信BOLD像的方法，推动后续功能分析任务。

Method: 提出DINO-BOLDNet框架。该方法包含冻结的DINOv3自监督编码器提取切片内结构特征，结合可训练解码器；并通过切片注意力模块将相邻切片的上下文信息融合，多尺度解码器细致恢复功能对比度。损失函数中特别引入DINO感知损失，保证输出结构和纹理与真实BOLD像在transformer特征空间上的一致性。

Result: 在包含248名受试者的临床数据集上实验显示，DINO-BOLDNet在PSNR和MS-SSIM两项指标上都超过了条件GAN方法。

Conclusion: DINO-BOLDNet首次实现了直接从T1w生成均值BOLD影像，证实了自监督transformer在结构到功能映射中的巨大潜力，为处理BOLD影像缺失等问题提供了新方向。

Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

</details>


### [48] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的单目三维跟踪方法TrackingWorld，能够在世界坐标系中实现几乎所有像素的密集三维跟踪，并解决了相机运动与前景动态运动分离不足及新出现物体难以跟踪的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单目三维跟踪方法难以有效区分相机运动和前景动态，同时无法对视频中新出现的动态目标进行密集跟踪，限制了三维重建和视觉理解的应用。

Method: 作者提出了TrackingWorld方法，包括：1）引入tracking upsampler，将稀疏二维跟踪提升为密集二维跟踪；2）对所有帧应用upsampler，并去除重叠区域中的冗余跟踪，实现对新目标的广泛覆盖；3）通过优化框架将这些密集二维跟踪反投影到世界坐标系下，联合估计相机位姿及轨迹的三维坐标，实现准确的三维跟踪。

Result: 通过大量的合成与真实数据集实验证明，该系统能在世界坐标系下实现准确且密集的三维跟踪，表现优于现有技术。

Conclusion: TrackingWorld大幅提升了单目视频中像素级三维跟踪的准确度和全面性，为相关三维视觉任务（如运动分析、三维建模等）提供坚实基础。

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [49] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出一种名为SCU-CGAN的新方法，用于生成高质量的火灾图像，用于提升家庭火灾检测模型的数据集，显著提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 目前家庭火灾检测系统受到火灾数据不充足的限制，导致检测模型性能有限。生成更多高质量火灾图像成了提升检测系统表现的需求。

Method: 提出SCU-CGAN模型，结合U-Net结构、CBAM注意力机制以及额外判别器，将非火灾图像转化为真实感强的火灾图像，并和已有生成模型进行对比评估。

Result: SCU-CGAN生成的火灾图像质量优于现有模型（KID得分比CycleGAN提升41.5%），并通过数据增强，大幅提升了火灾检测模型的准确性（以YOLOv5 nano为例，mAP@0.5:0.95提升56.5%）。

Conclusion: SCU-CGAN能生成高质量火灾图像，有效解决数据不足问题，为提升家庭火灾检测系统准确率提供新思路。

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [50] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: MLLM中视觉与文本特征的范数失衡影响多模态融合，简单加LayerNorm可大幅提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM采用Pre-Norm结构，导致视觉token范数大、文本token范数小，引发特征融合效率低、表现受限的问题。

Method: 理论分析并实证范数失衡问题带来的不对称更新动态。提出在视觉投影后插入特定初始化的LayerNorm，强制规范视觉与文本特征范数一致。

Result: 实验在LLaVA-1.5上验证：单层LayerNorm即可显著提升多模态和纯文本任务性能。

Conclusion: 解决视觉与文本范数失衡问题有助于MLLM更有效融合信息，提升整体能力，提出的方法简单实用效果显著。

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [51] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种用于复杂光照下图像增强和降噪的新型框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像增强方法在提升图像质量时，要么容易放大噪声，要么只能适用于特定的光照条件，难以应对实际应用中复杂多变的环境。

Method: 通过引入梯度域加权引导滤波（GDWGIF）进行精确光照估计，结合Retinex模型将图像分为光照层和反射层，对各层并行处理：修正光照层照明、增强反射层细节，最后结合多曝光融合和线性拉伸优化动态范围。

Result: 在实际应用采集的真实数据集上测试，所提方法在对比度增强和噪声抑制方面均优于现有主流方法。

Conclusion: 新框架能在复杂光照下高效增强图像质量并抑制噪声，提升视觉应用的性能，具有实际应用价值。

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [52] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 本文探讨了面部美颜（修图）对生物识别系统的干扰，提出并验证了基于AI的特征提取与美颜感知提升检测美颜图像的准确率，单张图片下D-EER仅为1.1%。


<details>
  <summary>Details</summary>
Motivation: 随着修图技术在社交媒体和广告中的广泛应用，越来越多的面部图片经过美颜处理。如果这些修图照片被作为生物识别系统的样本，会对安全性带来威胁。已有研究显示修图会影响人脸识别的准确性，因此检测修图图片显得尤为重要。

Method: 本研究分析了美颜评估算法在修图图像上的表现，比较了多种基于人工智能的特征提取方法用于提升修图检测能力，同时探索了人脸美观度是否有助于检测修图图像。特别地，该方法在面对未知的修图算法时，也能对单张图片进行有效检测。

Result: 在攻击性修图算法未知的情况下，该方法在单张图片检测上的D-EER仅为1.1%，显示出极高的检测准确率。

Conclusion: 人脸美颜检测对于提升生物识别系统的安全性至关重要，结合AI特征提取和美颜评估有助于显著提升修图检测的性能。该方法为应对未知修图算法提供了新思路。

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [53] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: 本文提出了一套用于自动化鱼儿重新识别（Re-ID）的深度学习流水线，借助AutoFish数据集针对渔业电子监控海量视频中鱼类识别问题，利用Swin-T模型与优化策略取得了明显优于传统卷积网络的效果。


<details>
  <summary>Details</summary>
Motivation: 随着电子监控系统的普及，监控视频数据量激增，人工审核已无法满足需求，因此亟需自动、高效、准确的鱼儿个体识别工具来支持渔业科学管理。

Method: 构建AutoFish数据集（六种外观相似鱼），提出用于鱼儿Re-ID的深度学习流水线。采用Swin-T（视觉变换器）架构，并结合hard triplet mining技巧及自定义的数据规范化图像增强方法，与ResNet-50对比评测。

Result: Swin-T结合定制训练策略，在Re-ID评估指标Rank-1准确率和mAP@k上都超过ResNet-50，分别达到90.43%和41.65%。详细分析发现，同种个体区分困难，主要受视角变化影响大于部分遮挡。

Conclusion: Swin-T为代表的变换器模型借助优化策略，可显著提升渔业视频中鱼儿Re-ID的准确性，有助于后续管理自动化。视角一致性对提升识别尤其关键。代码已开源，可便于实际应用和研究扩展。

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [54] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练即可提升视频中3D人体恢复时序一致性和遮挡鲁棒性的框架 SAM-Body4D。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的人体3D恢复方法在实际视频应用中，容易因逐帧推理而出现时序不一致和受遮挡影响。

Method: 利用可提示的分割模型生成时序一致的“masklet”，采用遮挡感知模块修复缺失区域，再用这些masklet引导原有的3D人体模型，实现训练外的鲁棒时序3D恢复。支持多人体并行推理。

Result: 在实际野外视频数据上，无需重新训练即显著增强了3D恢复的时间稳定性与遮挡鲁棒性。

Conclusion: SAM-Body4D实现了高效、训练外的视频3D人体重建，提升了实际应用价值。

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [55] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频增强检索方法OneClip-RAG，可高效处理长视频，大幅提升多模态大语言模型（MLLMs）对长视频的理解能力和效率。


<details>
  <summary>Details</summary>
Motivation: 当前多数多模态大语言模型因内存开销过大，往往只能处理有限帧数的视频，难以高效理解长视频。如何提升长视频处理效率和理解质量，是亟需解决的问题。

Method: 提出OneClip-RAG框架，通过有效利用视频片段，兼顾知识完整性和语义连贯性。还创新性地引入基于查询引导的视频切块算法，将切块与跨模态检索整合为一步式处理，减少冗余计算。为进一步提升指令跟随能力，还构建了新数据集SynLongVideo，并采用递进式训练策略。

Result: OneClip-RAG集成到五种主流MLLMs中，并在多个长视频基准集上实验。结果显示，模型性能明显提升，如使InternLV2 8B和Qwen2-VL 7B在MLVU任务上达到GPT-4o水准，同时显著提升处理长视频的效率，如LLaVA-Video可在一块4090 GPU上2.2分钟内解析1小时视频。

Conclusion: OneClip-RAG有效提升了多模态大模型对长视频的处理能力和效率，为长视频理解任务设立了新标杆。

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [56] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: 提出LapFM模型，通过构建层次化知识结构和自进化标注策略，实现对大规模无标注腹腔镜图像的高效泛化分割，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景分割受限于标注稀缺与语义不一致，且常用的自然领域基础模型微调方式难以应对多变的手术目标，泛化能力弱，亟需具备更强场景适应性的手术基础分割模型。

Method: 1）提出了层次化概念进化预训练范式，建立腹腔镜概念层次结构（LCH），通过父子查询嵌入统一不同类别的实体，实现语义一致性；2）引入置信度驱动的进化标注机制，基于层次一致性，从无标注数据中不断筛选高可靠伪标签，构建了大规模图像掩膜对数据集（LapBench-114K）。

Result: 在多个腹腔镜分割基准上，LapFM显著优于现有主流方法，在细粒度自适应泛化能力上确立了新标准。

Conclusion: LapFM为手术分割任务，特别是腹腔镜通用分割，提供了新颖且有效的基础解决方案，推动了大规模手术场景一致性理解的进展。

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [57] [Leveraging Multispectral Sensors for Color Correction in Mobile Cameras](https://arxiv.org/abs/2512.08441)
*Luca Cogo,Marco Buzzelli,Simone Bianco,Javier Vazquez-Corral,Raimondo Schettini*

Main category: cs.CV

TL;DR: 本文提出了一种结合高分辨率RGB传感器和低分辨率多光谱(MS)传感器的统一学习型色彩校正框架，实现从端到端的高精度色彩校正，并在多个公开数据集和两种主流架构上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的色彩校正流程通常将多光谱数据在早期阶段舍弃，并将流程分割成多个环节，难以充分利用多光谱信息，导致色彩准确性有限。作者旨在突破这一瓶颈，以提升消费级及移动设备成像的色彩质量。

Method: 作者提出了一个端到端、学习驱动的统一色彩校正框架，能够联合输入高分辨率RGB和低分辨率多光谱(MS)数据，通过单一模型处理全流程，并对现有两种主流image-to-image架构进行了适配。同时，构建了一个结合多种公开多光谱数据集并针对不同相机灵敏度重渲染的新数据集。

Result: 实验显示，该方法能显著提升色彩准确率和稳定性，相较于仅用RGB或传统MS驱动方法，色彩误差降低可达50%。

Conclusion: 提出的统一端到端色彩校正框架有效提升了成像色彩的准确性与稳定性，具有灵活适配不同架构的优势，并为后续研究提供了公开的数据、模型与代码资源。

Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.

</details>


### [58] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 现有的子集选择法对深度视觉模型的解释在分布外（OOD）环境下表现不佳，作者提出结合不确定性估计的新方法，不仅提升OOD场景下解释的健壮性与可信度，也改善了分布内（ID）表现。


<details>
  <summary>Details</summary>
Motivation: 虽然子集选择法在分布内场景下能有效突出重要区域并支持对象级解释，但其在分布外（OOD）场景下是否可靠仍缺乏系统研究。实际应用中，模型解释在OOD情境下常常不稳定、不精确，这影响了解释方法的实用性和可信度。

Method: 通过在多组ID-OOD数据集上大量实验，系统性评估现有方法的表现。作者提出将子模集合选择法与基于梯度的不确定性层级估计结合，通过自适应的权重扰动计算不确定性，再用该不确定性值引导子模集合优化过程，实现选取的信息丰富且多样的区域。该方法无需额外训练或辅助模型。

Result: 实验显示，现有基于子集选择的解释方法在OOD条件下表现出冗余、不稳定、对不确定性敏感等缺陷；提出的新框架克服了这些问题，在OOD和ID场景下都提升了解释的健壮性和准确性。

Conclusion: 当前子集选择法在OOD环境下有明显不足，结合不确定性驱动的优化可以显著提升解释的可靠性和可解释性。因此，该方法有助于视觉AI模型在实际复杂环境中更加透明和可信。

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [59] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了一种结合SAM和CSRT追踪器及球衣颜色外观模型的高效足球运动员追踪方法，兼顾精度与实时性，能应对轻度和部分重度遮挡，但对长时间消失的目标再识别能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的运动员追踪方法在频繁遮挡、外观相似及快速运动的场景下表现受限，尤其是在足球等人员密集场景下，实现高效、准确且资源友好的追踪更具挑战。本文针对足球场景提出更精确、鲁棒和高效的追踪方法。

Method: 方法结合Segment Anything Model (SAM)实现精确初始化，以CSRT追踪器作实时跟踪，并利用HSV直方图的球衣颜色辅助外观再识别，设计了团队感知的追踪系统以改善遮挡恢复能力。

Result: 在足球视频上实验证明，该方法能以7.6-7.7FPS、约1880MB内存稳定运行，轻度遮挡下100%追踪成功率，拥挤区域（5人以上）有90%成功率。在重度遮挡中通过外观再识别恢复50%个体，但对完全离开画面后再返回者，仅有8.66%再获取率。

Conclusion: 结合SAM与CSRT的轻量级方法适合资源有限环境下部署，对持续可见目标表现优异。但在长时间遮挡/消失时，需进一步强化再识别机制。研究为实际赛事视频分析系统提供性能-资源权衡指南。

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [60] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出了一种新的基于拖拽的图像编辑方法ContextDrag，通过上下文特征注入与位置一致注意力机制显著提升编辑的连贯性和细节保真度，且无需微调模型或进行反演，在多个标准测试上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法难以有效利用参考图像的上下文信息和细粒度纹理，导致生成结果在一致性和真实度方面表现有限。作者希望解决这一问题，实现更加高质量、细致的交互式图像编辑。

Method: ContextDrag方法首先利用VAE对参考图像进行特征编码。通过“上下文保持Token注入（CTI）”和“潜空间反向映射（LRM）”实现无噪声地将参考特征注入到目标位置，保留细节与一致性。同时采用位置一致注意力机制（PCA），对参考特征进行位置重编码，并用重叠感知掩码来剔除无关信息，从而进一步保证编辑区域的有效性和精准性。

Result: 在DragBench-SR和DragBench-DR数据集上的大量实验表明，ContextDrag在编辑的一致性、细节保留和整体效果方面均优于当前所有SOTA方法。

Conclusion: ContextDrag无需微调和反演，即可通过有效利用参考图像的上下文特征实现更高质量的图像拖拽编辑，为交互式图像编辑提供了新的思路和技术路径。

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [61] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: 本文提出了Visionary，这是一个基于Web的实时高效3D高斯投影与网格渲染平台，支持多种生成与更新算法，并显著提升了Web端3DGS渲染效率。


<details>
  <summary>Details</summary>
Motivation: 虽然3D高斯投影（3DGS）等神经渲染技术发展迅速，但现有的Web端可视化工具分散、臃肿且难以支持动态图像与生成式模型，导致部署困难，限制了3DGS的应用和实验推广。

Method: 作者开发了Visionary平台，基于WebGPU实现高效渲染，并通过每帧ONNX推理支持神经网络的动态运算。平台引入统一的高斯生成器标准接口，允许不同算法灵活接入，支持推理生成和后处理。同时提供TypeScript API，便于与现有Web应用整合。

Result: 实验表明，Visionary在相同3DGS资源下，因基于GPU的原语排序而实现比当前主流Web端查看器更高的渲染效率，并已支持多种方法（如MLP-based 3DGS、4DGS、神经头像、风格迁移等）。

Conclusion: Visionary统一了Web端的神经推理与3DGS渲染，极大降低了重现实验、对比和部署门槛，为重建和生成式世界模型方法提供了通用的浏览器平台。

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [62] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 论文提出PCI方法分析扩散模型在生成过程中概念的形成和锁定时机，不仅揭示了不同扩散模型和概念类型在时序上的动态行为，还提升了文本驱动的图像编辑效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型被普遍用于图像生成，评价通常集中在最终结果，但实际生成是渐进的动态过程。理解概念形成的时序对模型的可控性、可靠性和可预测性具有重要意义，尤其在图像编辑等应用场景下可为干预提供理论支持。

Method: 提出了一种无需额外训练、无需模型内部结构信息的分析框架PCI，通过插入特定时刻的概念干预，定义并测量概念插入成功率（CIS），进而研究在扩散过程中不同时刻概念被模型保留并体现在最终图像中的概率，横跨多种主流文本生成图像扩散模型和丰富的概念类型进行了实证。

Result: 实验证明，不同扩散模型、不同类型甚至同一类型的不同概念在扩散过程中的形成和锁定时机存在显著差异。PCI方法识别出部分阶段更适合插入特定概念，且基于PCI的干预可实现兼顾语义准确和内容保持的更强编辑效果，优于现有主流方法。

Conclusion: PCI为理解和解释扩散模型的动态生成过程提供了系统化工具，有助于提升文本驱动图像编辑的可控性且无需侵入式修改模型，方法通用且易于应用，为未来相关研究和落地应用提供了重要参考。

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [63] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了首个针对多相机系统的实时3D重建框架，能够高效无漂移地从多路RGB视频流增量性地重建大规模场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的实时单目重建存在视场有限导致的3D覆盖不全问题，多相机装置可从根本上解决，但缺乏有效的实时重建框架。

Method: 框架采用分层相机初始化方案实现无标定情况下的粗配准，结合精简的多相机束调整优化，实现轨迹稳定；引入无冗余采样和频率感知调度，减少高斯原语和优化迭代次数，提升效率和精度。

Result: 采用原始多路视频输入，2分钟内即可对百米级3D场景完成高效、高保真、稳健的实时重建。

Conclusion: 该方法首次实现多相机系统下的在线高效大规模3D重建，速度与质量均优于现有技术。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [64] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 本文提出了一种新型的针对多模态大推理模型（MLRM）的隐私防护方法ReasonBreak，能够通过概念感知扰动有效阻断模型的层级推理过程，实现对地理信息隐私的保护。实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的MLRM可以通过分析个人图片中的环境线索，层层推理出精确的地理位置，带来严重的隐私风险。而现有的隐私防护方法主要针对感知级模型，对具备多步推理能力的MLRM效果甚微，因此亟需更有针对性的隐私保护手段。

Method: 提出ReasonBreak框架，通过概念感知扰动手段，有选择性地破坏推理链条中的关键概念依赖，从而让模型无法顺利推理出地理位置。同时，作者构建了包含6,341张高分辨率图片及其层级概念标注的GeoPrivacy-6K数据集。

Result: 在包括GPT-4o、GPT-5、Gemini 2.5 Pro等七个先进MLRM上的实验证明，ReasonBreak可使区块级保护率由19.4%提升到33.8%，街区级保护近乎翻倍（由16.8%到33.5%），显著优于现有方法。

Conclusion: ReasonBreak为针对推理型威胁的隐私保护提出了新范式，为今后多模态大模型时代的隐私安全奠定了基础。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [65] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: 本文提出了一种名为NoisyCLIP的方法，可以在扩散生成模型的去噪早期阶段实时检测文本与图像的不对齐情况，在维持语义对齐的前提下大大降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的条件扩散模型虽然能够依据文本生成语义相关的图像，但经常出现语义不对齐或错误解释（幻觉）现象。检测这些不对齐通常在生成结束后进行，效率很低，因此需要一种实时且高效的检测方法。

Method: 作者提出NoisyCLIP方法，在图像生成的去噪过程中，利用双编码器方法在潜空间测量文本/图像的语义对齐，能够在生成尚未完成时就判断是否存在对齐问题。

Result: NoisyCLIP在保持98% CLIP对齐性能的同时，将评估计算成本缩减了50%。实验证明其在BoN（Best-of-N）场景下的应用效果良好。

Conclusion: NoisyCLIP实现了在图像生成过程中实时的对齐检测，有效降低计算成本，为未来优化生成质量、节省资源提供了新思路。

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [66] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: 作者提出了一种基于潜变量扩散模型的新方法OCCDiff，用于从LiDAR点云精准重建建筑表面，显著提升了对不同分辨率和噪声干扰下的重建效果。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云重建建筑表面时，常面临点密度不均和噪声干扰的挑战。传统方法难以兼顾高质量采样与鲁棒性。

Method: 提出OCCDiff方法，将潜变量扩散过程与函数自编码器结构相结合，在占据函数空间中生成可在任意位置评估的连续占据函数。同时，设计点编码器提取特征并辅助扩散学习，多任务训练提升鲁棒性与特征表达能力。

Result: 实验证明，OCCDiff能生成与目标分布高度一致、物理上合理的样本，并对噪声数据具有较强鲁棒性。

Conclusion: OCCDiff能够灵活、高质量地重建建筑表面，提高了点云重建的精度和抗干扰能力，具有实际应用价值。

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [67] [Thinking with Images via Self-Calling Agent](https://arxiv.org/abs/2512.08511)
*Wenxi Yang,Yuzhong Zhao,Fang Wan,Qixiang Ye*

Main category: cs.CV

TL;DR: 本文提出了Self-Calling Chain-of-Thought (sCoT)，一种无需真实跨模态交互即可高效增强视觉推理能力的新范式，大幅提升了多模态推理训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前将视觉信息动态融入推理链（CoT）的think-with-images范式已显示出强大的视觉推理能力，但要用强化学习优化交错的多模态CoT（iMCoT）面临优质推理数据稀缺的挑战，严重限制了实际应用和扩展。

Method: 作者提出sCoT，将视觉推理任务语言化，主智能体将复杂任务分解为原子型子任务，并由自身参数共享的虚拟子智能体（subagents）分别在独立上下文中解决，过程中不需显式模态交错。同时，利用群体相对策略优化（group-relative policy optimization）以强化有效的推理行为。

Result: 实验在HR-Bench 4K数据集上进行，sCoT方法在整体推理性能上提升了1.9%，同时所需GPU时间减少了约75%，效果显著优于现有强基线。

Conclusion: sCoT通过任务分解与参数共享子智能体实现了视觉推理的高效训练，无需真实跨模态交互，显著提升了推理表现与训练效率，具有良好的扩展性和实际应用价值。

Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.

</details>


### [68] [MVP: Multiple View Prediction Improves GUI Grounding](https://arxiv.org/abs/2512.08529)
*Yunzhu Zhang,Zeyu Pan,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MVP（Multi-View Prediction）的无训练改进方法，通过多视角推理提升GUI定位稳定性，显著增强现有模型对高分辨率、小UI元素的坐标预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding模型在预测像素坐标时对视觉微小变化高度敏感，导致结果不稳定且易错，尤其在高分辨率和小UI元素下影响突出，亟需提升预测鲁棒性。

Method: MVP包含(1) 基于指令-图像注意力的视角生成，对原始图像进行多样裁剪获得不同视角；(2) 多坐标聚类，将多视角的预测结果做空间聚类，最终选取最密集簇的中心作为输出。整个过程无须重新训练模型。

Result: 在多个基准和模型上实验，MVP显著提升了准确率。例如在ScreenSpot-Pro任务上，UI-TARS-1.5-7B提高到56.1%，GTA1-7B提高到61.7%，Qwen3VL-8B-Instruct提升到65.3%，Qwen3VL-32B-Instruct提升到74.0%。

Conclusion: MVP方法有效缓解了GUI grounding坐标预测的不稳定性，对多种主流模型均有一致增益，具有良好通用性和实用价值。

Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.

</details>


### [69] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出了一个统一的多模态框架，实现了油画生成与编辑的高度交互与创意表达，可细致编辑并保持统一风格。


<details>
  <summary>Details</summary>
Motivation: 现有的油画生成与编辑技术主要依赖于真实照片改动，受限于训练数据分布，难以保持油画独特的笔触和风格，限制了创意实现。因此需要新方法实现更精细、统一风格的油画生成与编辑。

Method: 1. 训练阶段采用空间对齐与语义增强条件策略，将掩码和手绘草图映射为空间约束，引用图片和文本信息编码为上下文特征，实现对象级的语义对齐；2. 为解决数据稀缺问题，提出基于笔触渲染的自监督风格迁移流程，通过模拟油画修复，把真实图片转为带笔触纹理的油画，生成大规模配对训练数据；3. 推理阶段用AdaIN算子融合特征，保证风格一致性。

Result: 大规模实验表明，该系统能实现精细化编辑操作，同时保持油画艺术质感，在风格化油画生成和编辑上实现了前所未有的创意表达能力。

Conclusion: 所提出的多模态油画生成与编辑系统，突破了现有限制，实现了细粒度编辑与风格统一，为油画数字化创作和编辑带来了新的可能性。

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [70] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Photo3D提出了一套利用GPT-4o-Image生成图片推动3D高真实感生成的新方法，实现了外观和结构的一致性，大幅提升了3D生成效果，达到了最优的真实感表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D本地生成器尽管在几何生成上取得了进步，但在真实感外观表现上仍存在不足，主要原因是缺乏高质量、细节丰富的3D真实资产数据，原始数据采集难度较大。

Method: 本方法提出了Photo3D框架，通过利用GPT-4o-Image生成图片，并设计结构对齐多视图合成流程，构建了细节增强的多视图数据集与3D几何绑定。在此基础上，提出基于感知特征自适应和语义结构匹配的外观一致性增强方法，确保3D几何的结构一致性和真实细节的外观一致性，并给出了适配不同生成范式的训练策略。

Result: Photo3D在多种3D本地生成模型下都能获得优异的泛化效果，实现了业界最优的高真实感3D生成表现。实验结果验证了方法的有效性。

Conclusion: Photo3D能够大幅提升3D生成外观的真实细节和鲁棒性，为3D生成领域带来新的技术突破，并具有良好的通用性和扩展性。

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [71] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种加速自回归-扩散（AR-diffusion）混合生成范式的新方法Fast-ARDiff，通过联合优化自回归和扩散模块，在保证生成质量的同时极大提升解码速度，在主流任务上实现了3-4.3倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 当前AR-diffusion混合模型结合了自回归的结构建模能力和扩散模型的高保真合成能力，但由于自回归生成的顺序性和扩散模型的多步去噪迭代，导致整体推理延迟高，影响实际应用。作者希望解决该效率瓶颈，实现高质量且低延迟的生成。

Method: 作者提出了Fast-ARDiff框架，包括：（1）基于熵推测的草稿模型加速生成，减缓由于熵失配带来的高拒绝率；（2）扩散解码端采用动态调度，将自回归优化信息用于指导后续扩散步骤，并通过联合蒸馏优化扩散模块，实现稳定训练和高质量合成。在推理端，利用自回归模块提取的浅层熵特征提前筛除低熵草稿，减少冗余计算，进一步提升速度。

Result: Fast-ARDiff在ImageNet 256×256数据集上的TransDiff模型实现了4.3倍无损加速，在文本条件生成场景下的NextStep-1也实现了3倍加速，且合成质量无显著损失，优于当前同类方法。

Conclusion: 通过统一优化和推理策略，Fast-ARDiff成功解决了AR-diffusion混合范式高延迟的问题，大幅提升了推理效率，能广泛应用于多种高质量生成任务。

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [72] [A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation](https://arxiv.org/abs/2512.08542)
*Zhigang Jia,Duan Wang,Hengkai Wang,Yajun Xie,Meixiang Zhao,Xiaoyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的四元数Wasserstein距离及其在生成对抗网络中的应用，有效提升了彩色图像生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有彩色图像生成模型忽视了颜色通道间的相关性，导致色差问题，并且缺少对彩色图像数据分布及数据集之间度量的理论体系。

Method: 定义了新的四元数Wasserstein距离，发展了对应的对偶理论，借助四元数凸集分离定理和四元数Farkas引理，推导了强对偶形式；基于此距离，设计了Wasserstein四元数生成对抗网络（Wasserstein Quaternion GAN）。

Result: 所提新模型在生成效率和图像质量上均优于传统（四元数）生成对抗网络和Wasserstein生成对抗网络。

Conclusion: 四元数Wasserstein距离及其应用显著提升了生成模型处理彩色图像的理论基础和应用效果，推动了彩色图像生成相关理论及方法的发展。

Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.

</details>


### [73] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 本文提出了一种无迭代的固定点估计器，用于扩散逆过程中的噪声恢复任务，相较于主流方法在不需多次迭代或重新训练的情况下，实现了更好的重建效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于固定点迭代的扩散逆方法由于需要多次迭代及超参数选择，计算成本高且应用受限，因此有必要提出一种高效且易于实现的新方法。

Method: 作者推导了理想固定点的显式表达式，但其中包含未知的数据预测误差。为此，提出用前一步已知误差近似当前步骤未知误差，得到了可计算、近似的固定点表达式，实现了无迭代的固定点估计。

Result: 在 NOCAPS 和 MS-COCO 两个文本-图像数据集上的重建实验显示，该方法在不需额外训练或多步迭代的情况下，相较于 DDIM 逆过程以及其他固定点迭代方法表现更优且一致。

Conclusion: 所提基于误差近似的无迭代固定点估计方法，克服了传统固定点迭代法的计算瓶颈和参数难题，在扩散逆重建任务中表现突出，具有实际应用价值。

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [74] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 该论文提出SSCATeR算法，可在处理连续扫描的LiDAR点云时，仅关注发生变化的区域，跳过静止区域，大幅提升检测效率，且精度不降。


<details>
  <summary>Details</summary>
Motivation: 现有点云检测方法在每一帧都需对全部区域进行卷积计算，浪费大量算力，尤其是在大多数区域未发生变化时，亟需高效只处理变化区域的机制以提升实时性和节省计算资源。

Method: 作者利用LiDAR的连续扫描特性，采用滑动时间窗口和短步长，检测点云数据随时间变化的部分。引入了卷积结果的跨帧存储，只在检测到数据变化的区域进行计算。此外，扩展了散射式卷积方法（scatter-based convolution），提出支持数据重复利用的极度稀疏卷积算法SSCATeR，只针对变化点云进行处理。

Result: 与传统稀疏卷积方法相比，SSCATeR在实验中将处理时间最多减少了6.61倍，且输出的特征图和传统方法完全一致。

Conclusion: SSCATeR通过只对变化区域进行高效卷积运算，实现了点云目标检测在速度和算力上的显著提升，不降低原有检测精度，适合实时应用。

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [75] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: 本文提出了一种大规模、自动化的分析框架，用于在全脑范围内发现和解释大脑中与视觉概念相关的fMRI活动模式。通过无监督分解方法挖掘可解释信号，并结合自动化流程，对每种模式进行图像与自然语言描述，展现了数千种与视觉概念相关的可解释大脑信号。


<details>
  <summary>Details</summary>
Motivation: 之前的研究多为小规模，手工分析，且只关注特定脑区或属性，研究结果很难系统推广。本研究旨在突破手动和局部分析的局限，实现对大脑视觉表示的全面、自动发现与解释。

Method: 采用了无监督数据驱动的fMRI信号分解方法，自动发现有解释性的信号模式。随后，通过筛选自然图片来激活这些信号，结合自然语言生成技术，自动为每种模式生成可读的视觉意义描述。为提高效率与可靠性，设计了自动化流程对候选解释进行打分和选择。

Result: 这一方法发现了覆盖全脑范围、与多种不同视觉概念相关的大量可解释信号模式。其中包括许多以往未被报道的细粒度视觉表征。

Conclusion: 该自动化框架推动了对于人类大脑中视觉概念表征的系统性和规模化研究，为探索人脑视觉编码机制提供了重要的新工具。

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [76] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 该论文提出了一个模块化的神经图像信号处理（ISP）框架，实现了高质量图像渲染，并在中间阶段提供极高的可控性和编辑灵活性。作者还开发了相关的交互式照片编辑工具。


<details>
  <summary>Details</summary>
Motivation: 现有的神经ISP方法多为端到端设计，缺乏对中间处理环节的灵活控制，难以扩展、调试或适应不同风格及新相机数据，用户定制化需求难以满足。

Method: 提出模块化设计的神经ISP，将整个渲染流程分为多个可控模块，实现高度定制与中间结果控制；并开发了一个交互式照片编辑工具，结合该ISP支持多样化的编辑操作和风格匹配。该框架完全基于学习，有不同规模的变体，参数量适中。

Result: 该方法在多组测试集上，展现出良好的定量和定性表现。其交互式工具能高质量渲染并支持无限次后期可编辑再渲染。

Conclusion: 模块化神经ISP不仅在渲染质量上表现优异，还大幅提升了可扩展性、可调试性、对新相机的泛化能力以及用户定制化的灵活性。

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [77] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: 本文提出了一种在继续测试时自适应调整伪标签的方法，从而提升语义分割模型对不断变化领域的适应能力，在多个实验中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA方法在鲁棒性上有提升，但常依赖固定阈值，无法应对不同类别和样本的难度变化，特别是在需要密集多类预测的语义分割场景下表现有限。

Method: 该方法根据每张图像内的置信度分布自适应调整伪标签，并动态平衡受领域转移影响较大的类别学习，实现更细粒度的类别和实例适应，从而提供更可靠的监督信息。

Result: 在包括合成到真实、长期转移在内的8个CTTA和TTA场景中，所提方法在所有情况下都优于当前最优技术。

Conclusion: 该方法为应对不断变化领域下的语义分割任务设立了新的性能标杆，有效缓解了持续适应中的错误积累问题。

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [78] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren Schüllerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,Nataša Sladoje*

Main category: cs.CV

TL;DR: 本文提出了一种基于分层图神经网络（HiGINE）的方法，结合多模态信息，从多重免疫荧光图像中的肿瘤微环境进行分析，有效预测肺癌患者的生存期（短/长），并提升风险分层表现。


<details>
  <summary>Details</summary>
Motivation: 肿瘤微环境（TME）中不同细胞类型间的复杂相互作用为预后生物标志物提供了新机会，但现有方法难以充分捕捉这些交互关系，因此需要一种更精准的分析方法，提高对患者风险分类和生存期预测的能力。

Method: 提出HiGINE分层图神经网络，既编码细胞邻域的局部关系，又刻画全局细胞间联系，综合细胞类型、形态信息。通过多模态融合，将癌症分期数据与mIF图像特征整合，用于预测患者生存期。

Result: 在两个公开数据集上测试，HiGINE在风险分层中表现出更高的准确性、稳健性与泛化能力。

Conclusion: HiGINE方法能够更有效地分析TME并预测肺癌患者的生存期，在临床风险分层领域具有应用前景。

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [79] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法提升和自动化花粉颗粒的检测与分类，特别关注兽医细胞学中的应用，在光学与全息显微镜图像中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 目前花粉检测和分类多依赖人工费时费力，而现有的自动化手段在不同成像方式中表现不佳，特别是在低成本全息显微镜图像下，准确率远不如传统光学图像。研究希望结合深度学习提高检测效率并降低硬件成本，方便实际使用。

Method: 检测任务采用YOLOv8s模型，分类任务用MobileNetV3L模型，并分别在光学和全息显微镜图像上评估表现。针对全息图像表现低问题，通过自动化标注扩展数据集及扩大标注框面积等技术手段提升了模型性能。

Result: 在光学图像上，检测mAP50为91.3%，分类准确率为97%；初始全息图像检测mAP50仅2.49%，分类准确率42%。通过改进后，全息图像检测性能提升至13.3% mAP50，分类提升至54%。

Conclusion: 深度学习方法可显著提高光学与全息显微镜下的花粉粒检测分类自动化水平，尤其在成本较低的无透镜数字全息设备上也可获得较好分类效果，显示其在实际应用中的潜力。

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [80] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: 本论文发现CLIP模型在few-shot学习中，模板-样本相似性（TSS）引入了偏置，影响模型表现。作者提出利用“空白提示”及双阶段校准方法，有效消除模板偏置，提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前CLIP模型利用文本模板与图像对齐实现高性能few-shot分类，但过度依赖模板与样本间的表面相似性（TSS），而非真实的类别语义，导致精度和鲁棒性下降。急需方法识别并校正这种偏置。

Method: 提出一种用“空白提示”（无类别语义信息的提示语）消除TSS偏置的框架：预训练阶段用空白提示暴露与消减模板偏置，微调阶段引入偏置校准损失（bias calibration loss），确保图像与类别正确对齐。

Result: 在多个公开基准测试上，该方法有效减少了由TSS引发的精度波动，提升了分类准确率和模型鲁棒性。

Conclusion: 通过空白提示和两阶段校正，实现了对CLIP模板偏置的有效解耦，显著提升了few-shot学习的性能与可靠性。

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [81] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: 本文提出OpenMonoGS-SLAM，这是首个将3D高斯泼洒技术（3D Gaussian Splatting, 3DGS）与开放集语义能力相融合的单目SLAM系统，无需深度或语义真值输入，利用视觉基础模型实现开放环境下的高性能定位、建图与语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有的SLAM与语义理解结合工作往往依赖深度传感器或封闭类别的语义模型，导致其在开放世界环境中的适应性和可扩展性受限。为推动空间智能的发展，亟需设计能够应对开放世界且无需额外传感器或标签的单目SLAM系统。

Method: 该方法提出OpenMonoGS-SLAM框架，将3D高斯泼洒表示融入单目SLAM pipeline，并采用MASt3R、SAM与CLIP等视觉基础模型，分别用于视觉几何和开放语义理解。方法完全依赖自监督学习，不需深度输入或3D语义标签，并设计了新颖的内存机制，有效管理高维语义特征，生成语义高斯特征地图。

Result: 实验显示，该方法在无需深度图或语义标注的前提下，在封闭集与开放集语义分割任务上均达到或超过现有基线方法的性能。

Conclusion: OpenMonoGS-SLAM证明了在无辅助传感器、无语义标签的情况下，通过结合3DGS与大型视觉基础模型，可实现高效的单目定位、建图和开放语义感知，有望推动空间AI在开放世界的落地。

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [82] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频模糊模式和稠密轨迹联合光学设计的新型深度估计算法，通过现成视觉编码器与点追踪器提取视频信息，再结合多窗口嵌入与聚合、视觉-语言模型稠密化轨迹，实现高精度、多场景下鲁棒的深度还原。


<details>
  <summary>Details</summary>
Motivation: 传统手持拍摄因缺少机械稳定器，易产生随景深变化的透视模糊，影响成像质量。而这种模糊随物体空间位置而异，给深度估计带来了新的可能性。作者希望利用这一物理现象，通过分析模糊模式辅助实现metric级别的深度恢复，提升现有光学与计算算法的深度重建效果。

Method: 方法上，作者通过视觉编码器和点追踪器提取视频帧信息，利用windowed embedding和multi-window aggregation估计深度图。同时，借助视觉-语言模型对光学算法获得的稀疏运动轨迹进行稠密化，最终联合获得更加高精度且稠密的深度估计结果。

Result: 在多个公开深度数据集上实验验证，该方法表现出良好的大范围深度适应能力和泛化能力。与实际手持拍摄的真实轨迹对比，光学算法的轨迹定位与稠密重建都展现出更高的精度与准确性。

Conclusion: 综上，该方法创新性地结合模糊分析和光学轨迹，突破了无机械稳定条件下深度估计的难题，在泛化性和精度上取得显著进步，有望为实际视频拍摄及三维重建带来应用价值。

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [83] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 论文提出了一种仅依赖单目RGB图像和自然语言指令的无人机航拍视觉-语言导航（VLN）框架，显著提升了导航性能，简化了实际应用部署。


<details>
  <summary>Details</summary>
Motivation: 目前主流的空中VLN方法依赖全景、深度或里程计等额外输入，导致系统成本高、集成复杂，不适用于轻量级无人机。为降低部署门槛，亟需无依赖、轻量化的解决方案。

Method: 该方法仅使用无人机前视单目RGB图像和语言指令，将导航任务建模为下一个动作token预测，结合提示引导的多任务学习优化空间感知、轨迹推理和动作决策。此外，通过关键帧筛选减少冗余、动作合并及标签加权缓解样本分布不均，实现更稳健的多任务协同训练。

Result: 在Aerial VLN基准测试中，该方法在仅用单目RGB的极限条件下，于已见和未见场景均取得优异效果，显著优于同类RGB-only方法，并极大缩小与最先进的全景RGB-D方法之间的性能差距。消融实验验证了方案设计和结构的有效性。

Conclusion: 本文提出的轻量级空中VLN新框架，极大降低了系统复杂度、提升了性能，为无人机实际部署提供了有效方向。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [84] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: 本文提出了链式图像生成（CoIG）框架，通过将图像生成过程拆解为连续的语义步骤，使生成过程更加透明和可监控。


<details>
  <summary>Details</summary>
Motivation: 当前主流的图像生成模型虽然生成效果卓越，但内部生成过程如“黑箱”，不易观察、干预且难以解释，影响了模型的可靠性、安全性和可控性。这种不可解释性阻碍了更广泛的应用和信任。

Method: 提出了CoIG框架，借鉴语言模型中的Chain-of-Thought思路，使用大语言模型将复杂的生成指令分解为一系列简明的逐步指令。图像生成模型按照这些指令逐步生成和编辑图像，每一步只处理一个语义实体，并通过设定两种新指标（可读性和因果相关性）来定量评估各环节的清晰性和影响。

Result: 实验证明，CoIG在提升模型监控性方面表现显著，同时保持与主流基线模型相当的合成鲁棒性，并能有效缓解实体坍缩等问题。该框架具有模型无关性，可集成于任何图像生成模型。

Conclusion: CoIG框架通过分步生成过程提高了图像生成模型的可监督性和解释性，有助于提升安全性与可靠性，并为未来图像生成的可控化和人机协同打下基础。

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [85] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: 本文提出了一种高效、轻量化的驾驶员分心行为识别方法C-DIRA，利用动态感兴趣区域路由和域自适应对抗学习机制，在不牺牲精度的情况下提升了端侧推理效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有车载摄像头驾驶员分心行为识别模型难以在边缘设备实现高效实时推理。轻量模型难以捕捉细粒度行为特征，且传统ROI策略计算量大，难以兼顾准确率与效率。亟需兼具高效与泛化能力的新架构。

Method: 提出C-DIRA框架：（1）基于视觉显著性实现Top-K ROI池化，提取局部特征并融合分类；（2）动态ROI路由，仅对困难样本做ROI推理以节省计算；（3）采用伪域标签与对抗学习，实现对驾驶员与场景变化具有鲁棒性的域不变特征学习。

Result: 在State Farm驾驶员分心行为检测数据集上，C-DIRA以更低的FLOPs和更低延迟，保持了比现有轻量化模型更高的准确率；且在恶劣视觉条件及未见领域下表现出较强的鲁棒性与泛化能力。

Conclusion: C-DIRA实现了高效、紧凑且具备强泛化能力的驾驶员分心行为识别，为边缘设备端推理提供了切实可行的方案。

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [86] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需外部编码器的对比学习框架 ({\mname})，显著提升了生成模型（如扩散模型）的训练效率与表现。


<details>
  <summary>Details</summary>
Motivation: 当前去噪生成模型虽然在视觉合成方面表现出色，但训练成本高、特征表达效率低。尽管通过辅助判别表征可提升效果，但引入外部预训练编码器带来计算开销大和领域不匹配问题。

Method: 作者提出{\mname}，通过引入记忆库机制维持大量高质量负样本，使负样本数量独立于batch size，提高对比学习效果。此外，采用低维投影头减少内存和带宽消耗。整个方法无需外部编码器，且在推理时无额外参数和计算成本。

Result: 在ImageNet-256数据集上，{\mname}以400k步数达到最新的FID 2.40，显著优于现有同类方法，收敛速度快且生成质量高。

Conclusion: {\mname}摆脱了对外部视觉大模型的依赖，同时实现了更快收敛和优异生成效果，对高效视觉生成具有重要意义。

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [87] [Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds](https://arxiv.org/abs/2512.08673)
*Shaofeng Zhang,Xuanqi Chen,Xiangdong Zhang,Sitong Wu,Junchi Yan*

Main category: cs.CV

TL;DR: 该论文提出了一种用于3D点云自监督学习的新型对比学习方法CSCon，通过中心-周围掩码和分支设计，有效提升了高层语义和局部细节的表示能力，并在多项评测协议下超越了主流生成式方法。


<details>
  <summary>Details</summary>
Motivation: 目前3D点云自监督方法主要以生成式为主，但这类方法难以学习判别性特征，导致下游任务表现不佳，而2D对比学习方法直接应用于3D又无法捕获局部细节。因此作者希望探索3D点云任务下更优的对比学习框架。

Method: 作者提出了CSCon框架，通过分别对点云的中心和周围部分做掩码处理，构建双分支输入，分支分别侧重中心区域和周围区域的信息表征。同时，设计了patch级别的对比损失，兼顾全局和局部特征学习。

Result: 在FULL、ALL等协议下，CSCon与生成式方法表现相当；在MLP-LINEAR等协议下，CSCon取得了目前最优的结果，部分场景甚至超过了跨模态方法，并相较基线Point-MAE有显著提升（最高提升10.3%）。

Conclusion: 面向3D点云的CSCon对比学习框架可有效弥补生成式和传统对比学习在判别性和局部感知上的不足，显著提升下游任务表现。

Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.

</details>


### [88] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: 提出了MoSAIC-ReID框架，通过专家模型和定量分析，揭示了行人再识别模型依赖的语义属性，实现了可解释性增强。


<details>
  <summary>Details</summary>
Motivation: 当前行人再识别模型虽准确率高，但缺乏可解释性，不清楚模型实际依赖哪些高层语义属性。作者希望系统性、定量地分析和解释属性对识别性能的贡献。

Method: 提出Mixture-of-Experts方法（MoSAIC-ReID），每个基于LoRA的专家关注一个特定属性，引入oracle路由器进行归因分析。结合广义线性模型、统计检验和特征重要性分析，评估不同属性在识别任务中的作用。

Result: 在Market-1501和DukeMTMC数据集上取得了有竞争力的性能。实验证明，衣服颜色等属性贡献最大，而罕见属性如配饰影响有限。同时首次进行大规模属性重要性量化分析。

Conclusion: MoSAIC-ReID提供了行人再识别领域可解释性分析框架，强调引入显式语义知识的价值，为后续研究实用可解释ReID方法提供基础。

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [89] [Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth](https://arxiv.org/abs/2512.08700)
*Kyumin Hwang,Wonhyeok Choi,Kiljoon Han,Wonjoon Choi,Minwoo Choi,Yongcheon Na,Minwoo Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的知识蒸馏方法，使大型单目深度估计基础模型的知识有效转移到轻量级的全环绕单目深度估计算法中，从而提升实时性和尺度一致性。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度基础模型虽然泛化能力强，但应用到全环绕场景时，受限于高运算成本和尺度难以度量的问题，难以满足实际应用中对实时性和尺度精确度的需求。

Method: 作者设计了融合回归与知识蒸馏的混合框架，通过引入尺度不变的深度分箱和交互式知识蒸馏，将基础模型对深度概率的理解迁移到学生网络中，并利用ground-truth指导其推断绝对深度；此外，还提出了视图关系蒸馏，将多摄像头间的结构关系编码后传递，提升跨视角一致性。

Result: 在DDAD和nuScenes数据集实验结果表明，提出的方法优于传统监督方法以及已有知识蒸馏技术，在准确度和效率之间取得良好平衡，满足实时应用需求。

Conclusion: 本文方法能高效将基础模型的优势知识压缩进轻量网络，显著提升了全环绕单目深度估计的性能与尺度一致性，为相关实际场景提供了有效的解决方案。

Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.

</details>


### [90] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本论文将Segment Anything Model 3（SAM 3）首次应用于遥感零样本语义分割（OVSS）任务，提出了无需训练、融合多输出头的简单方法，显著提升了遥感小目标的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的无训练OVSS方法在遥感场景下难以精准定位小目标且流程复杂，迫切需要统一高效的新方案。SAM 3作为新一代分割和识别模型，提供了跨类别和区域的强大能力，值得探索在遥感OVSS的潜力。

Method: 方法包括两点：1）设计mask fusion策略，融合SAM 3语义分割头与实例分割头（Transformer decoder）的输出，集两种特征优势于一体；2）利用presence head的presence score过滤实际不存在的类别，有效抑制大词汇表导致的假阳性。全程零训练。

Result: 在多个遥感数据集上做了大量实验，结果显示该方法适配性好、性能优越，特别在密集和小目标任务中表现突出。

Conclusion: SAM 3无需训练的简单适配已在遥感开放词汇语义分割展现出良好前景，为遥感自动分析和下游任务提供了有效的新思路。

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [91] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 该论文针对皮肤病检测中的皮肤色公平性，提出基于连续分布而非分组性质的新评价和缓解偏差方法，有效提升了个体层面的公平性。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像AI常以粗略的群体分组（如肤色分类别）来衡量公平性，忽略个体间的微小差异，导致对少数或极端个体的系统性忽视。这在皮肤病检测中尤为突出，因肤色差异明显影响诊断表现，因此急需细粒度的公平性评估与优化方法。

Method: 作者提出将肤色视为连续属性，用核密度估计（KDE）建模肤色分布。对比了12种分布距离度量表征肤色分布间的偏差，并设计了基于距离的重加权（DRW）损失函数，用以平衡数据中肤色分布，缓解少数肤色数据的代表性不足。用CNN和Transformer等模型在皮肤病分类任务中进行了实验验证。

Result: 实验显示，传统基于类别加权的公平性方法难以捕捉个体层次的偏差，而所提分布式重加权策略（特别是Fidelity Similarity、Wasserstein Distance、Hellinger Metric和Harmonic Mean Similarity等度量）表现更优，有效提升了皮肤色个体公平性。

Conclusion: 提出的以肤色分布为核心的个体公平性评价与缓解框架，可为皮肤病AI公平性研究奠定基础，对医学图像包含的连续敏感属性带来广泛影响。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [92] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*Artúr I. Károly,Péter Galambos*

Main category: cs.CV

TL;DR: 本文提出了一种结合Blender三维渲染与受限扩散模型的全自动合成蘑菇图像生成流程，用于提升工业蘑菇检测与分割模型的训练数据质量与数量，实验证明仅用合成数据训练的模型即取得了优异的真实世界分割效果。


<details>
  <summary>Details</summary>
Motivation: 蘑菇自动化种植与采收依赖于高质量的视觉感知模型，而训练此类模型需要大量精确标注的图像数据，获取真实数据成本高，因此急需低成本、高质量的替代方案。

Method: 作者设计了一个创新流程，将Blender三维场景配置与受限扩散模型相结合，全自动生成具备高真实感和自动标注的Agaricus Bisporus蘑菇图像数据集，无需专业的三维建模知识。

Result: 发布了两个包含共12000张图片、超25万蘑菇实例的合成数据集；在仅用合成数据训练Mask R-CNN模型情况下，在两个独立真实数据集（包含新采集基准数据集）上实现了F1=0.859（M18K数据集）的最新分割水平。

Conclusion: 该方法不仅适用于蘑菇，也可扩展到水果、叶片等其他农作物检测领域，实现合成数据驱动的高质量视觉模型训练。

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [93] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于偏度引导的剪枝方法，用于压缩多模态Swin Transformer模型，支持在边缘设备上的高效医学影像智能诊断，并结合联邦学习保障隐私。


<details>
  <summary>Details</summary>
Motivation: 当前高精度的医学影像识别模型多体积庞大计算量大，难以部署在资源有限的边缘设备上；同时医学数据隐私限制了集中式数据管理，因此需要能兼顾隐私保护与高效部署的新方法。

Method: 提出一种利用输出分布偏度的剪枝算法，针对多模态Swin Transformer中的多头自注意力和多层感知机层进行选择性剪枝，并在联邦学习框架下对模型进行优化与训练。

Result: 在联邦学习环境下验证了方法有效性，在压缩Swin Transformer模型体积约36%的前提下，模型精度未受影响。

Conclusion: 该方法在维持诊断准确率的同时，显著降低了模型复杂度，证明了在边缘医疗AI设备上实现高效模型压缩与分布式隐私保护学习的可行性。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [94] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: Wan-Move框架让视频生成模型具备了更精细、高质量的动作控制能力，可以生成长时长、高分辨率、动作精确可控的视频，并且实现简单、易于扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在动作控制上粒度较粗，不够灵活，且扩展性有限，难以满足实际应用的高质量视频合成需求。

Method: Wan-Move采用密集点轨迹来精细描述物体运动，并将轨迹信息投影到隐空间后，沿着轨迹传播首帧特征，得到对齐的时空特征图作为新的动作控制条件，无需更改原有架构或引入额外运动编码器，即可用于主流图像转视频模型。

Result: Wan-Move生成了动作可控性堪比商业化产品（如Kling 1.5 Pro Motion Brush），时长最长5秒、分辨率高达480p的视频。用户调研和公开数据集上的实验均显示其动作质量优于同类方法。

Conclusion: Wan-Move显著提升了视频生成模型的动作可控性和可扩展性，且方法简单高效，在多个基准测试中表现优异，并公开了代码和数据，有望推动视频生成领域发展。

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [95] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出自我修正扩散模型，利用XAI高亮图有效检测和修复图像合成中的伪影与不真实区域，大幅提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成上取得了巨大成功，但依然存在伪影和不真实区域，影响图像质量，需要有效方法加以检测和改进。

Method: 提出一种自我修正扩散框架，通过可解释AI（XAI）生成的缺陷激活图（FAMs）检测伪影区域。在扩散前向过程中加大这些区域的噪声，逆向过程中则重点修复，从而提升整体图像重建质量。

Result: 该方法在多种扩散模型和不同数据集上，Fréchet inception distance指标最高提升27.3%，在图像生成、文本到图像、修补等任务中展现强大稳定效果。

Conclusion: 可解释AI不仅用于解释模型，还能直接参与并提升图像生成质量。该方法具备通用性和有效性，显著推动了扩散模型图像合成领域的发展。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [96] [LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models](https://arxiv.org/abs/2512.08785)
*Yiming Hao,Mutian Xu,Chongjie Ye,Jie Qin,Shunlin Lu,Yipeng Qin,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种新方法LoFA，用于高效预测个人化先验，实现视觉生成模型的快速自定制，显著提升了效率并超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉生成模型的个性化自适应方法（如LoRA）需要大量特定任务数据和长时间优化，实用性受限。现有基于超网络的方法虽尝试直接预测适应权重，却难以处理复杂的用户请求，严重限制实际应用。因此，亟需一种能够快速高效个性化模型适配的新方法。

Method: 作者首先发现LoRA与基础模型参数间的相对变化中存在结构化分布模式，提出采用两阶段超网络：第一步预测关键适应区域的相对分布模式，第二步由此指导最终的LoRA权重预测，从而实现快速精确适配。

Result: 实验表明，该方法能够在多任务、多用户请求下于数秒内预测出高质量的个性化先验效果，性能优于常规需要数小时优化的LoRA方法。

Conclusion: LoFA极大提升了个性化视觉生成模型适配效率和质量，为相关实际应用和后续研究提供了有力支持。

Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.

</details>


### [97] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出了一种用于文档阴影去除的新型MatteViT模型，结合空间和频率信息，同时高效保留文本细节，在多个基准数据集上优于现有方法，并提升了OCR等应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文档阴影去除方法在消除阴影时，易损失文本边缘和线条等高频细节，导致文档可读性降低。因此，如何在去除阴影的同时保留这些关键细节，是提升文档数字化质量的重要难题。

Method: 提出了一种基于视觉Transformer的MatteViT模型，融合空间域和频率域信息。具体包括两个关键策略：（1）设计轻量级的高频放大模块（HFAM），用于分解并增强高频分量以保留细节；（2）提出基于连续亮度的阴影matte，通过自建数据集和matte生成器提前引入空间指导信息，从处理初期就精确定位阴影并引导修复。

Result: 在RDD和Kligler等公开基准数据集上进行大量实验，MatteViT模型在阴影去除效果及细节保留上均达到最新最好水平。同时，该方法在OCR等下游任务中的文本识别准确率也有明显提升。

Conclusion: MatteViT方法能在有效去除文档阴影的同时高质量保留关键细节，为实际文档处理提供了强有力且实用的解决方案，并能增强后续应用如OCR的性能。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [98] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新型适配方法——训练自由双重双曲适配器（T-DHA），有效提升了大规模视觉-语言模型在跨领域与小样本任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言模型在领域变化时表现下降明显，且在新领域微调需要较高计算资源。因此亟需开发更高效的适配方法。

Method: 创新性地用双曲空间（Poincaré球模型）来表示语义层次结构，配合负学习策略，通过训练自由的双重适配器实现对模型的无损适配与增强，无需额外训练就能嵌入层次知识并提升判别能力。

Result: 在多个数据集上的实验表明，T-DHA在小样本图像识别和领域泛化任务中的表现，显著超过现有先进方法。

Conclusion: T-DHA为视觉-语言模型的领域适应和小样本学习提供了高效、训练自由的通用解决方案，兼具准确性与鲁棒性，具有广泛应用前景。

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [99] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: InfiniteVL是一种新型的视觉语言模型架构，结合滑动窗口注意力和门控DeltaNet，实现了高效的线性复杂度和长序列处理能力，同时在性能和效率上都优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 目前视觉语言模型如果用窗口注意力会在序列长度超过窗口时表现下降，用线性注意力则在需要大量信息的任务上表现不佳。现有方案难以同时兼顾效率、内存和性能。

Method: 提出了InfiniteVL架构，将滑动窗口注意力（SWA）与门控DeltaNet结合，达到线性复杂度。采用三阶段训练策略：蒸馏式预训练、指令微调和长序列SFT，大大降低了训练数据需求。

Result: InfiniteVL在使用不到主流模型2%训练数据的情况下，显著超越过往线性复杂度VLM，并能媲美主流Transformer型VLM。速度方面，比使用FlashAttention-2的Transformer VLM快3.6倍，支持更长的序列和实时视频流推理。

Conclusion: InfiniteVL综合提升了视觉语言模型在资源受限条件下的效率与性能，为长序列和流式任务提供了优异的解决方案，同时极大节省训练资源，具有很强的工程应用和研究意义。

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [100] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: 本文比较了生成式方法和非生成式方法在人类视觉感知代表性特征——组合式泛化方面的能力，发现生成式方法借助合适的归纳偏置，能够更有效地实现组合式泛化。


<details>
  <summary>Details</summary>
Motivation: 人类水平的视觉感知被认为需要生成式推理（如通过解码器反演获得内部表征），而现今主流视觉模型多为非生成式，这引发了生成是否为实现人类级视觉感知的必要条件的问题。

Method: 作者在形式化组合式数据生成过程中，对生成式（解码器反演）与非生成式（编码器）方法需要的归纳偏置进行了理论分析。并证明对于编码器来说，无法仅靠正则化或架构约束实现必要的归纳偏置，而生成式方法则可直接通过约束解码器实现。提出了基于梯度搜索或生成式重放的解码器反演策略，并在真实图像数据集上对比训练两类方法，进行实验验证。

Result: 理论上显示生成式方法更易实现组合式泛化。实验证明：非生成式方法缺乏必要归纳偏置情况下，组合泛化能力较弱，且需大规模预训练或额外监督。而生成式方法无需额外数据，仅通过适当归纳偏置即可显著提升组合泛化。

Conclusion: 生成式方法在实现人类视觉感知中的组合泛化上具有天然优势，非生成式方法很难通过常规方式达到同等泛化能力，因此为实现类人视觉，生成式模型是更可行的路径。

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [101] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: 本论文提出用于评测几何推理能力的基准Tri-Bench，并发现在现实变化下，当前视觉-语言模型(VLM)对几何推理准确率较低，尤其在相机姿态变化和识别少数三角形类别时表现极差。


<details>
  <summary>Details</summary>
Motivation: 近年来，视觉-语言模型在理解和推理任务上表现突出，但其对几何推理的可验证性和可控性在真实环境下仍存不足。现有评测缺乏对几何推理核心能力的有效隔离和验证。因此，论文希望提出一个新基准，专注于三角形几何推理，旨在深入分析VLM在真实场景变化（如相机视角、背景干扰）下的表现极限，并检测其可验证性。

Method: 作者设计了Tri-Bench基准，包括平面三角形问题，控制了相机姿态（平面与倾斜）和场景干扰（10种常见物体），以此隔离和考察相对几何推理能力。选取了四种最新VLM，采用统一带“护栏”的提示（明确定义参考方框），用单一提示词评估六类任务（二元与连续目标），并比较模型输出与真实3D和图像2D投影的准确性。

Result: 四种VLM在六项任务中的平均3D准确率约为69%（最高75%，最低64%），与2D投影对齐度略高（均值72%）。在识别少数三角形类型时（如等边、等腰、直角三角形），四种模型准确率降为近0%。相机倾斜会令整体准确率下降约4.1%。场景干扰对准确率无显著影响。

Conclusion: 当前VLM虽能部分处理几何推理任务，但对参考系线索利用不足，倾向依赖2D图像提示，尤其在变换视角和复杂几何类别下表现不可靠，显示VLM距离可验证和可控的几何智能尚有较大差距。

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [102] [Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning](https://arxiv.org/abs/2512.08873)
*Jing Jie Tan,Anissa Mokraoui,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SOLI的新方法，通过轻量级的Siamese网络优化低分辨率图片的潜在嵌入，从而实现高效准确的图像描述生成，尤其适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽然提升了图像描述的性能，但对低分辨率图片和资源受限情况下应用不友好。此问题在辅助视觉障碍人士等实际场景中尤为突出，因此亟需一种既高效又轻量化的低分辨率图片描述方法。

Method: 提出SOLI方法，将Siamese网络结构应用于低分辨率图片描述，通过优化潜在嵌入表示，利用网络的双路径结构在减少计算消耗的同时提升图像到文本翻译的效率和准确率。

Result: SOLI在保证性能的前提下显著降低了计算资源消耗，实验表明其在低分辨率图片描述任务中表现出较高的效率与精度，适用于资源受限的训练环境。

Conclusion: SOLI为低分辨率图片描述提供了轻量高效的新方案，兼顾性能与资源消耗，对实际应用有重要意义。

Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.

</details>


### [103] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种用于卫星图像视觉定位的结构化定位机制，通过微调预训练的视觉-语言模型（VLM）并引入专用控制标记，有效提升模型在复杂卫星场景中精准定位目标的能力，实验结果在多个遥感基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在遥感领域展现出强大通用性，但复杂卫星场景下的精准视觉定位依然存在挑战。作者希望提升VLM在卫星图像中的视觉定位能力，推进其在实际遥感应用中的可靠性和实用性。

Method: 方法包括在多种指令跟随任务上微调预训练VLM，并通过专用控制标记（control tokens）联动一个专门的定位模块，实现对语言和空间信息的联合推理，从而提升视觉定位效果。

Result: 在多个遥感视觉定位基准上测试了所提框架，相较于已有方法，视觉定位相对提升达24.8%，在多项指标上均超越了最新方法。

Conclusion: 结构化空间推理的引入显著提升了VLM处理复杂卫星场景的能力，有助于实现更可靠的遥感数据分析并推动相关实际应用发展。

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [104] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了一种无需标注的新训练框架，通过结合大语言模型（LLM）和视觉语言模型（VLM）验证器，从而提升视觉推理与目标定位能力，在多个空间推理任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法要么依赖大量标注数据（链式思维方法），要么完全依赖预训练但逻辑和定位都不理想（程序合成方法）。两者都有显著的局限性，亟需一种能够提升推理与视觉定位且无需大量标注的新方法。

Method: 作者设计了一个无标注训练框架，包括两个AI驱动的验证器：一个LLM验证器通过强化学习优化推理过程；一个VLM验证器通过自动困难负样本挖掘强化视觉定位。这种方式无需人工标注，结合了强大的语言和视觉AI模型能力。

Result: 该方法在多种空间推理任务上进行了评估，结果显示本方法不仅提升了视觉推理水平，还超越了其他公开或专有模型，改善了目标定位精度，并优于最新的仅基于文本的视觉推理方法。

Conclusion: 提出的无标注AI验证训练框架有效结合了语言和视觉模型优势，显著提高了视觉推理和精准对象定位能力，为相关应用提供了新思路和性能提升。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [105] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种统一的内容感知布局生成模型UniLayDiff，能够用单一模型解决多种受约束的布局生成任务，并在多个任务上取得了最新最好表现。


<details>
  <summary>Details</summary>
Motivation: 现有的内容感知布局生成方法只能解决部分特定输入约束的任务，或者在不同约束条件下需分别训练不同参数的模型，无法实现任务统一，限制了实际应用。

Method: 提出UniLayDiff，一种基于多模态扩散Transformer的模型，将布局约束视为独立模态，并捕捉背景图像、布局元素及不同约束的复杂关系。模型还通过LoRA微调集成关系约束，实现预训练后对特殊任务的适应。

Result: UniLayDiff在无条件及多种条件布局生成任务中，均取得了最优表现，首次实现了内容感知布局生成任务的全覆盖和统一。

Conclusion: UniLayDiff是一种端到端可训练的统一模型，能够适应不同复杂约束下的内容感知布局生成任务，具有实际应用和推广价值。

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [106] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本论文提出了一个无需训练的3D场景生成框架EvoScene，能够从单张图片自适应地逐步重建完整、高质量的三维场景。


<details>
  <summary>Details</summary>
Motivation: 目前基于单张图像生成3D场景的方法，大多仅限于物体级别，对复杂和大规模的场景、细致的结构与纹理泛化能力不足。如何从单张图片恢复大规模、真实感强的三维场景仍是一项重大挑战。

Method: 作者提出EvoScene框架，结合了3D生成模型的几何推理和视频生成模型的视觉知识。方法包含三个递进阶段：空间先验初始化、视觉引导的3D网格生成、空间引导的新视角合成，2D和3D之间反复交替迭代优化，逐步改善结构与外观。

Result: 在多种复杂场景上，EvoScene在几何稳定性、视角一致性纹理以及未见区域的补全能力上都超过了多个强基线方法，并能够直接生成实用的3D网格。

Conclusion: EvoScene可无需训练，一步步将单张图片自适应地重建为高质量的3D场景，在结构和纹理表现、泛用性以及实际应用价值方面都优于现有方法。

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [107] [Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration](https://arxiv.org/abs/2512.08922)
*Jin Hyeon Kim,Paul Hyunbin Cho,Claire Kim,Jaewon Min,Jaeeun Lee,Jihye Park,Yeji Choi,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新型文本感知图像修复框架UniT，显著提升了含文本图像的修复质量，有效减少了文字幻觉问题，并在主流数据集上实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在通用图像修复任务上表现优异，但在与文本相关的场景容易生成与原文不符的内容（即“文字幻觉”），原因在于缺乏明确的语言知识指导。为了解决这一问题，研究者希望将视觉-语言知识与扩散模型融合，提升文本恢复质量。

Method: UniT框架结合了Diffusion Transformer（DiT）、视觉-语言模型（VLM）和文本识别模块（TSM）。具体做法是：VLM负责从退化图像中提取文本并提供修复指导，TSM基于扩散特征在每一步去噪中生成文字识别结果，供VLM持续优化提示，而DiT则综合以上信息，还原清晰且逼真的文本细节。

Result: 在SA-Text和Real-Text两个基准数据集上，UniT能够高保真地重建退化文本，显著减少文字幻觉现象，并获得了TAIR任务上的最新最优F1分数。

Conclusion: UniT框架通过有效融合视觉-语言知识和扩散模型，推动了含文本图像修复技术的发展，为相关实际应用场景（如文本水印修复、文档还原等）带来了更高质量的解决方案。

Abstract: Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.

</details>


### [108] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,Joëlle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: 本文提出了D4RT，一种高效的前馈Transformer模型，能够从视频中联合推断深度、时空对应关系和摄像机参数，实现动态图像的4D重建，各项任务性能均优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 动态图像的几何结构与运动估计是计算机视觉的重要难题。现有方法在算法复杂性、解码计算量及多任务管理等方面存在效率和实用性不足。因此，迫切需要一种统一、简洁且高效的方案。

Method: D4RT是一种统一的Transformer模型。其核心创新是提出了新型的查询机制，既绕开了逐帧密集解码的高计算成本，也避免了为不同任务设计多套解码器的复杂性。该架构允许模型独立灵活地查询任意时空三维点的相关信息，从而联合推理场景中的深度、时空对应和摄像机参数。

Result: 实验表明，D4RT在多类4D重建任务中均超越了以往方法，模型更轻量、训练和推理效率更高，具有很好的泛化和可扩展性。

Conclusion: D4RT提供了一种有效的解决方案，显著提升了动态图像场景4D重建的效率与精度，有望为实际视觉应用带来广泛影响。

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [109] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: 本文提出Selfi方法，通过特征对齐提升了VGGT这类模型在新视角合成和相机位姿估计中的3D一致性，实现了高保真的三维重建。


<details>
  <summary>Details</summary>
Motivation: 当前NVS模型常依赖已知相机参数和显式3D归纳偏置，但数据驱动的VGGT模型虽灵活却缺乏多视点下的几何一致性。提高3D特征一致性可同时改善NVS与位姿估计。

Method: 作者提出Selfi方法，基于VGGT输出，通过重投影一致性损失训练轻量级特征适配器，实现伪真值特征自对齐，从而获得几何一致的三维特征表征。

Result: Selfi方法在新视角合成和相机位姿估计上达到了业界领先性能。

Conclusion: 特征几何对齐对于下游三维推理任务非常有益，Selfi可显著提升无监督数据驱动NVS系统的三维一致性和重建质量。

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


### [110] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了Astra模型，一种面向多场景和多类型动作的通用交互式世界模型，大幅提升了视频长期预测的交互性、一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器强的视频生成方法主要关注视觉质量，但针对输入过去观测和动作来预测未来、支持多类型交互和多场景泛化的世界模型研究仍不充分，存在对长时序动作预测和多样化场景交互能力不足的问题。

Method: 提出Astra模型。具体方法包括：1) 自回归去噪架构，融合时间因果注意力机制以聚合历史观测并实现流式输出；2) 在历史记忆中引入噪声防止模型过度依赖过去帧，达到响应性与时序一致性的平衡；3) 设计action-aware adapter把动作信号直接注入去噪流程，实现精确动作控制；4) 采用action experts混合体，动态路由异构动作模态，提升模型在不同任务（如自动驾驶、机器人操作等）中的适应性和泛化。

Result: 在多个数据集实验中，Astra在视频预测的保真度、长时序预测能力及动作对齐上都优于现有世界模型SOTA。

Conclusion: Astra实现了面向多场景、多类型动作的高质量、交互式长期视频预测，为通用世界建模和复杂任务提供了新工具。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [111] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出大部分序列中的下一个 token 主要依赖于较短的上下文，并系统性地测量了实现高精度预测所需的最小上下文长度(MCL)。结果显示绝大多数 token 只需少量前缀即可预测。论文还提出了可行的 MCL 代理方法（DaMCL），并基于此提高了对长距离依赖的识别与预测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力受上下文长度限制，多数预测任务可能无需长距离依赖，仅靠局部上下文即可完成。本工作旨在验证短上下文主导假说，并改进对那些确实需要长距离依赖的任务的检测和建模。

Method: 作者利用大语言模型作为统计 oracle，测量在不同数据集上预测下一个 token 所需的最小上下文长度（MCL），并提出一种无需真实下一个 token 的新型代理（DaMCL），兼容非贪婪采样方法。再依据此设计新的解码算法，动态增强真正需要长距离上下文的 token 预测。

Result: 实验证明，75-80%的序列token只需最后96个token的信息即可准确预测。提出的DaMCL度量与阈值法，能够有效检测需长距离依赖的序列。改进的解码算法能提升涉及长上下文依赖任务的性能。

Conclusion: 短上下文主导是普遍现象，具备代价小、兼容性的MCL代理工具（DaMCL）可以辅助识别和优化长距离依赖预测，从而显著提升大模型处理长文档的综合表现。

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [112] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

TL;DR: 该论文提出了一种可扩展的检索模型训练方案，通过使用通用嵌入模型和无标签语料，实现金融等专用领域对话AI高效信息检索，显著提升了准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然强大，但在金融等专用领域的检索任务上，由于计算与延迟限制及专业相关性需求，难以直接应用。嵌入检索模型虽然缓解了部分问题，但在领域检索表现不足。该研究希望构建成本低、性能强、无需人工标注的数据驱动专用检索模型。

Method: 作者提出了一条新的训练流水线：以通用检索嵌入模型为基础，从无标签语料中自动训练专用模型。方法核心是：学生模型与教师模型交互，通过检索反复地从无标签数据中挖难例（正/负），不断优化学生检索器。整个过程无需人工标注，而用模型与模型之间的反馈及难例训练不断提升模型表现。

Result: 在21,800个金融问答对、14种金融文件类型评测下，MRR@5提升27.7%，DCG@5提升44.6%。在FinanceBench基准的4类文件中，有3类NDCG显著提升。

Conclusion: 本文方法可高效自动地从无标签语料中优化特定领域的对话AI检索，不依赖人工标注，成本低，效果显著。为通用模型向专用领域迁移提供了实用新思路。

Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [113] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 本文提出了一种通用方法SEA，用于将带时间戳的字幕与连续手语视频高效对齐，在多个数据集上达到最优表现，并具备良好的通用性及扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖特定语言或数据集的端到端训练，缺乏通用性，影响在不同语言和场景下的应用。该工作旨在解决字幕与手语视频对齐时的通用性与高效率问题。

Method: SEA方法结合了两个预训练模型：首先分割视频帧序列得到单个手语动作；然后将每个手语片段嵌入到与文本共享的潜在空间。对齐部分采用高效的动态规划算法，可在CPU上于一分钟内处理小时级别视频。

Result: 在四个手语数据集上进行了实验，SEA方法取得了对齐任务的最新最优表现，证明了其跨语言与跨领域的强大适应性。

Conclusion: SEA为字幕与手语视频的通用对齐提供了有效方案，有望推动手语处理技术的发展。代码和模型已开源，便于社区进一步研究和应用。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [114] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种通用对抗后缀方法，通过在任意输入后附加短序列（4-10个token）来显著降低多种任务和模型的准确率，实现高效、可迁移的攻击。该方法在情感分析、自然语言推理等多任务和多模型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型作为零样本或少样本分类器时，容易受到对抗提示的影响。此前工作多为针对特定任务或模型的优化，通用性和可比性差。作者希望寻找一种简单、通用且可迁移的攻击手段，从而统一各类任务与模型上的攻击研究。

Method: 作者提出了“通用对抗后缀”——一段4-10 token的短序列，可直接附加到任意输入后，导致模型整体性能下降。方法上，使用Gumbel-Softmax松弛技术以可微方式学习后缀，并在推理阶段离散化。训练时最大化校准后的交叉熵，并屏蔽标签token防止信息泄露，同时引入熵正则防止退化。

Result: 仅需在一个模型上训练出的单个后缀，即可在不同任务（如情感分析、自然语言推理、同义句识别等）、不同主流小模型（Qwen2-1.5B、Phi-1.5、TinyLlama-1.1B）之间迁移，对准确率和模型置信度均有明显打击效果。

Conclusion: 所提出的对抗后缀方法能够以统一简便的方式对不同模型和任务构成有效威胁，表明当前主流小型语言模型在泛化性攻击下仍较为脆弱。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [115] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的新方法，通过添加对抗性后缀，能有效且广泛地降低多种主流语言模型在不同任务上的预测准确率，其对抗效果和泛化性优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性后缀生成方法主要依赖梯度或规则，往往局限于特定模型或任务，缺乏泛化能力且易受破解。作者希望探索一种更通用且高效的方法以提升对抗攻击的有效性和可迁移性。

Method: 将对抗性后缀视作强化学习中的策略（policy），采用PPO算法，利用冻结的大模型作为奖励oracle，奖励函数基于校准交叉熵以去除标签偏置，并在多种表面形式上聚合奖励，从而提升对抗样本的迁移能力。方法在五大NLP基准数据集和三种主流小型语言模型上进行了验证。

Result: 实验结果显示，RL训练得到的对抗后缀可以显著且一致地降低不同模型在多种任务上的准确率，且相较于以往同类对抗方法，其在跨任务、跨模型上的攻击迁移性表现更优。

Conclusion: 通过强化学习生成的对抗性后缀是一种更为稳健且通用的攻击手段，对未来改进模型鲁棒性和设计更强对抗防御具有重要参考价值。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [116] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub是一个整合ClinicalTrials.gov和PubMed文章临床试验信息的检索平台，使用大语言模型提升结构化数据获取能力，结构化信息比单用ClinicalTrials.gov提升了83.8%。


<details>
  <summary>Details</summary>
Motivation: 目前ClinicalTrials.gov等数据库中临床试验结构化数据有限，难以满足患者、临床医生、研究者与决策者获取高质量证据的需求，需要提升数据获取的全面性和可访问性。

Method: 平台自动抓取ClinicalTrials.gov数据并结合大语言模型（如GPT-5.1和Gemini-3-Pro）对PubMed原始全文进行解析，抽取和结构化临床试验信息。通过自然语言查询转换为结构化检索，并实现证据溯源的问答系统。系统效能通过用户研究及自动化评测两方面验证。

Result: 比单靠ClinicalTrials.gov平台，结构化临床试验数据访问率提升83.8%。用户（临床医生、科研人员、药学及护理领域博士生）参与后实用性获验证，信息抽取与问答能力在自动化测试中表现良好。

Conclusion: ClinicalTrialsHub极大提升了结构化临床试验数据的获取能力，为多类用户（患者、临床医生、研究人员、政策制定者）提供便捷、可溯源的证据型医学工具，有助于推动循证医学发展。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [117] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 本文通过复现Boukes（2024）的人工注释工作，评估GLLM在文本注释中的偏差。多种GLLM模型对五个概念进行标注，结果显示GLLM虽F1分数尚可，但与人工注释结果偏差明显，相互之间更一致，且存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: GLLM被大量应用于自动化文本标注，然而其与人工注释的一致性、潜在系统性偏见问题尚未充分探究。本研究旨在系统分析GLLM注释结果的偏差和可靠性。

Method: 选择四个主流GLLM（Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b），结合五种不同提示词，在五个概念维度（政治内容、互动性、理性、非文明、意识形态）上复现人工注释任务。比较不同GLLM与人工注释的F1分数、结果分布和偏差。

Result: GLLM在F1分数上表现尚可，但在结果分布上与人工注释明显不同，下游任务输出也有显著差异，且GLLM之间的结果比与人工注释更加一致，显示出系统性偏见。F1分数的差异无法解释偏见程度。

Conclusion: GLLM虽然标注准确率可接受，但其结果易产生系统性偏见，与人工标准差异较大，影响下游分析，F1分数不足以评价其有效性。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [118] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 本文旨在揭示机器翻译或大型语言模型中性别偏见的根源，通过解释性方法分析模型性别决策的输入影响，并与人类认知进行对比。结果表明模型决策与人类认知存在较大重合，有助于减缓性别偏见。


<details>
  <summary>Details</summary>
Motivation: 虽然机器翻译和大语言模型的性别偏见被广泛关注，但相关的解释性研究较少。本研究试图超越对偏见的简单量化，探索偏见产生的原因，深入理解模型的性别决策过程。

Method: 使用性别模糊的自然语料，结合对比解释法和显著性归因分析，检测源码句中的哪些输入词影响模型选择目标语言中的具体性别。此外，将显著性输入词与人类对性别的感知进行比对，并做语言学分析。

Result: 显著性的源码词与人类对性别感知的重合度高，表明模型做出的性别决策与人类认知有一致性。同时，对影响性别决策的词汇做出了语言学解释。

Conclusion: 对模型性别决策过程的解释性分析有助于理解和缓解机器翻译中的性别偏见。应当利用这些解释性信息优化现有模型，减少性别相关的偏差。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [119] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 提出一种具有推理约束（soft inductive bias）的训练方法，用于提升韩语大模型在不当言论检测中的表现，并取得了明显准确率提升。


<details>
  <summary>Details</summary>
Motivation: 在匿名在线社区和游戏中，不当言论易演变为辱骂甚至犯罪，社会关注度高。目前虽有韩语大模型和链式推理技术，但应用于检测不当言论的相关研究较少。急需有效检测和抑制此类言论的方法，保护网络环境安全。

Method: 提出一种“soft inductive bias”方法，明确限定推理视角，引导模型决策过程，减少推理失误。采用该方法微调韩语大语言模型（Kanana-1.5），并在不同训练策略下进行定量和定性对比实验。

Result: 采用该方法训练的Kanana-1.5模型平均准确率达87.00%，比传统监督学习提升了约3.89%。

Conclusion: 该方法能引导模型进行更精确、一致的推理判断，提升对不当言论的检测能力，显示出超越常规知识模仿的效果，对于构建安全交流环境有实际意义。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [120] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种分层多智能体系统，通过64*64的网格分布决策，结合选择性oracle和空间化课程学习机制，提升长时序推理能力，并降低计算消耗和对oracle的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和多智能体系统在复杂任务分解中有应用前景，但面对长时序推理时面临推理链过长和资源消耗大的挑战，有必要提升推理能力并优化计算资源分配。

Method: 提出一种层级化多智能体架构，包含64*64网格轻量智能体，各智能体先解决中央简单任务再扩展到外围难题。引入NLL信心度指标，指引课程学习关注表现好和信心高区域。用Thompson采样机制根据智能体能力和NLL奖励自适应分配训练区域。

Result: 在空间化汉诺塔（Tower of Hanoi）等具备长时序结构的基准上，提出方法表现出更好的系统稳定性、更少对oracle的依赖，以及更强的分布式协作推理能力。

Conclusion: 分层多智能体网络结合课程学习和信心度机制，有望提升分布式系统在长时序任务中的推理能力与可靠性，同时降低外部辅助需求。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [121] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出了一场关于医疗健康领域自然语言处理（HealthcareNLP）的综述型教程，涵盖数据合成、可解释性、检索增强生成等最新进展，并提供动手实践环节，适合无背景参与者。


<details>
  <summary>Details</summary>
Motivation: 现有医疗NLP综述往往遗漏合成数据生成、可解释性、检索增强生成、神经符号集成等前沿主题，未能全面涵盖面向患者和资源的应用，亟需系统介绍领域主要子方向，降低入门门槛。

Method: 设计三级层次结构：1）数据/资源层（注释准则、伦理审批、治理、合成数据）；2）NLP任务评估层（实体识别、关系抽取、情感分析、编码与方法分类等）；3）患者层（公众参与、健康素养、翻译、简化、支持决策）。并设置动手实践环节供学员操作实际工具。

Result: 形成了面向初学者的HealthcareNLP整体教程，系统梳理了领域关键问题、方法与未来挑战，降低了相关领域研究和应用的入门难度。

Conclusion: 该教程为医疗NLP领域研究者和应用开发者提供了前沿、系统、实用的知识框架与工具实践，有助于推动医疗NLP的普及和深化应用。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [122] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个开源Python框架，用于自动生成问卷风格的响应，以支持基于大语言模型（LLM）的调查和标注任务，并配有无代码界面。


<details>
  <summary>Details</summary>
Motivation: 众多基于LLM的问卷和标注任务尚缺少方便、系统、高效的生成和评测工具。

Method: 开发了QSTN框架，支持问卷展现、提示语扰动、应答生成的系统化实验及评估，并提供无代码界面。

Result: 通过超过4000万条模拟问卷数据的实验，发现问卷结构和生成方法显著影响LLM与人类答案的一致性，且计算成本很低。

Conclusion: QSTN提升了LLM相关调查/标注任务的可复现性和可靠性，为LLM研究提供有力支撑。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [123] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: 本文提出了一种多框架结构化智能体AI系统（MOSAIC），用于提升临床沟通中的注释自动化，提升准确性和可扩展性，并取得了优异实验结果。


<details>
  <summary>Details</summary>
Motivation: 临床医患沟通质量对患者结局至关重要，但大规模人工注释耗时费力且一致性差。现有大语言模型方法多为单任务，缺乏跨不同框架和临床领域的适应性、可解释性和可靠性。

Method: 设计了MOSAIC系统，基于LangGraph架构，编排了四个智能体：计划智能体（负责选择注释规则和流程）、更新智能体（维护最新检索数据库）、注释智能体（基于检索增强生成和动态few-shot实现具体注释）、验证智能体（进行一致性检查和反馈）。用26份人工标注转录本训练，50份测试，覆盖风湿科和妇产科。

Result: 在50份测试集上，MOSAIC整体F1分数为0.928，其中风湿科领域表现最佳（F1=0.962），尤其擅长患者行为类注释。消融实验和基线对比显示MOSAIC效果优于现有方法。

Conclusion: MOSAIC系统在临床沟通自动化注释任务上展现了高准确性和可扩展性，在多个领域均有良好适应性，为临床会话大规模自动分析提供了新方案。

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [124] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 本论文推出了首个面向巴斯克语、用于自动作文评分与反馈生成的公开数据集，并训练了开源模型，实现了成绩与反馈质量超过现有闭源系统。


<details>
  <summary>Details</summary>
Motivation: 巴斯克语为低资源语言，缺乏用于自动作文评分（AES）和反馈生成的公开数据集，对教育应用和NLP研究造成限制。

Method: 收集并整理了HABE机构3200篇CEFR C1等级作文，专家依据多个评分标准（如正确性、丰富性等）逐一打分并详细给出反馈。使用RoBERTa-EusCrawl及Latxa等开源模型微调，并提出结合自动一致性指标和专家人工验证的新评价方法。

Result: 微调后的Latxa模型在作文评分和反馈生成上表现优异，不仅在评分一致性和反馈质量上超过GPT-5和Claude Sonnet 4.5等闭源SoTA系统，还能识别更广泛的错误类型。

Conclusion: 本研究为低资源语言的作文自动评分和反馈生成奠定了公开、透明、可复现的基础，推动教育NLP在此类语言的发展。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [125] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 本文提出了一种用于低资源语言的语言模型后训练方法，在通过可能不流畅的奖励模型对齐时，仍能保证流畅性。该方法无需目标语言的指令微调数据，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 以往偏好优化主要专注于英语和中文等高资源语言，低资源语言缺乏高质量数据与流畅生成模型。作者希望解决低资源语言在指令对齐中流畅性难以保持和数据获取困难的问题。

Method: 采用基于策略（on-policy）的训练方法，将其与两种常见方法（基于机器翻译数据的有监督微调和多语种微调）进行比较，无需目标语言的人工指令微调数据。以挪威博克马尔语为例，通过母语者主观评分评估模型流畅性。

Result: 基于策略的训练方法在保持语言流畅性方面优于机器翻译微调和多语种微调，且不依赖难以获得的本地数据。

Conclusion: 提出的基于策略的后训练方法可在缺乏指令微调数据的低资源语言中，显著提升偏好对齐模型流畅性，为相关语言模型研究与应用提供了新途径。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [126] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出了一种在联邦学习环境下，兼顾LLM与多样人类偏好对齐质量和公平性的评估框架，并创新地引入了自适应聚合机制，实验验证了其在问答任务中对公平性的提升。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，不同群体存在多样化的偏好，传统的人类偏好对齐机制往往无法兼顾各方利益，导致公平性不足。因此，亟需评估不同聚合策略下的对齐与公平权衡，并寻求更优方案。

Method: 提出系统化评估框架，比较常见的奖励聚合方法（最小、最大、平均）和新颖的自适应机制（根据历史对齐表现调整偏好权重），所有操作在无原始数据交互前提下，在PPO框架下完成。

Result: 实验显示，自适应聚合方法在保持对齐分数竞争力的同时，能显著提升各群体间的公平性。

Conclusion: 该方法为多元群体下LLM公平对齐提供了实用方案和评估标准，推动了开发更加包容、公正的生成式模型。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [127] [Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts](https://arxiv.org/abs/2512.08814)
*Yifan Lyu,Liang Zhang*

Main category: cs.CL

TL;DR: 本文提出了ROME框架，将心理测评知识显式引入到个性识别中，利用大语言模型模拟问卷答题，改善了传统基于标签稀缺和语义映射不明问题，实现了更优的个性预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体个性检测方法依赖标签但面临标签稀缺及用户语言与心理特质间的语义映射不足，导致模型预测性能受限。

Method: ROME框架借鉴标准化自我评估问卷，通过大语言模型进行角色扮演，生成用户针对心理问卷的问题级答案，把原始用户发帖转化为可解释的问卷证据。此外，引入由问题驱动的专家网络（Mixture-of-Experts）模块，联合建模帖文和问卷答案，并采用多任务学习，将问答作为辅助任务提升个性标签预测。

Result: 在两个真实数据集上，ROME均显著优于当前主流方法，在Kaggle数据集上达到了15.41%的性能提升。

Conclusion: ROME通过将心理学知识融入个性检测流程，缓解了标签稀缺及语义链路不明的问题，实现了理论和性能上的双重提升。

Abstract: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -> user vector -> labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).

</details>


### [128] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 本文探讨了在Transformer训练过程中逐步增加网络深度，对模型性能和训练效率的影响，同时提出并改进了一种深度渐增方法。作者发现该策略不仅能提升推理能力，还能更好地利用模型深度，并缓解传统Transformer中后半层“深度诅咒”问题。


<details>
  <summary>Details</summary>
Motivation: 在现有Transformer架构中，后半部分层对最终输出的贡献显著低于前半部分（即“深度诅咒”），这影响了模型深度的有效利用。近期MIDAS方法显示渐增深度具有成本和性能优势，但背后的机制尚不清楚，本文旨在系统分析和解释这些现象。

Method: 作者采用逐步中间堆叠的深度增长策略，通过层级分析探究深度渐增对残差结构、推理流程和模型内部模块化的影响。此外，针对MIDAS提出了一种轻量级修改，用于提升推理基准测试的表现。

Result: 实验证明，逐步增加模型深度能更充分发挥Transformer的层数作用，改变残差流结构，形成易于交换的计算模块，并在多项推理任务中提升性能。改进后的MIDAS方案也带来了进一步的下游任务提升。

Conclusion: 通过逐步增加Transformer深度，不仅能够克服传统模型层利用率不足的问题，还可以促使模型自发形成有效的计算电路，从而改善推理效果并降低训练成本。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [129] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为RAGLens的新方法，利用LLM内部表征通过稀疏自编码器检测RAG生成中的幻觉，提高了检测准确性且更加高效。


<details>
  <summary>Details</summary>
Motivation: RAG虽提升了LLM的事实性，但仍受限于幻觉（生成内容与证据相矛盾或超越），现有检测方法要么依赖大规模人工标注，要么使用外部LLM判断，成本高且效果有限。

Method: 用稀疏自编码器（SAE）拆解LLM的内部激活，识别被RAG幻觉特定激活的特征。基于信息驱动特征筛选和加性特征建模，设计了轻量级的RAGLens检测器，用于内部表征上的幻觉检测，并可解释其判断依据。

Result: RAGLens在检测性能和效率上优于现有方法，能够准确发现不忠实的RAG输出，并为后续修正提供可解释推理。

Conclusion: RAGLens是一种高效且可解释的RAG幻觉检测工具，推动了内部机制分析在增强LLM输出事实性上的应用，同时对幻觉相关信号分布有了新认识。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [130] [Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization](https://arxiv.org/abs/2512.07969)
*Alan Papalia,Nikolas Sanderson,Haoyu Han,Heng Yang,Hanumant Singh,Michael Everett*

Main category: cs.RO

TL;DR: 本论文提出一种结合分离性和稀疏性的非线性最小二乘（NLS）问题求解方法，在包含规范对称性的机器人感知任务中实现了高效求解。通过创新的VarPro方案作为预处理步骤，极大提高了SLAM、SNL、SfM任务的求解速度，同时保证精度。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏性已被广泛利用于大规模NLS问题的求解，但变量分离性这一结构还未被充分挖掘，尤其是在机器人感知领域。特别是因全局位姿等规范对称性带来的计算挑战，阻碍了更高效方法的应用。

Method: 论文提出专为含规范对称性问题设计的VarPro方案，将线性变量解析消元，构建矩阵自由的Schur补操作符，作为一次性预处理步骤，能高效地计算成本、梯度和海森矩阵-向量积，并兼容现有NLS迭代求解器。论文还详细给出了方法适用的条件及部分条件下的扩展策略。

Result: 在合成数据和现实SLAM、SNL、SfM基准测试中，该方法相比当前先进方法实现了2至35倍的运行速度提升，并保持相同的求解精度。

Conclusion: 所提方法显著提升了包含规范对称性的大型NLS问题的求解效率与适用性，易于与常用迭代NLS求解器集成，适用于多类视觉与机器人问题。已开源C++实现及全部实验数据集。

Abstract: Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.

</details>


### [131] [VLD: Visual Language Goal Distance for Reinforcement Learning Navigation](https://arxiv.org/abs/2512.07976)
*Lazar Milikic,Manthan Patel,Jonas Frey*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Vision-Language Distance (VLD)的视觉-语言导航学习框架，通过将感知与策略解耦，并借助大规模自监督学习提升机器人导航的能力，实现了更高的灵活性和可扩展性。VLD能够支持多模态目标表达，并在多项实验中超过了现有的主流方法。


<details>
  <summary>Details</summary>
Motivation: 端到端的图像到动作导航训练在现实中面临sim-to-real落差或动作数据不足等难题，现有方法在泛化能力和数据需求之间存在权衡。该论文旨在既能大规模利用无需标签数据，又能支持多模态目标，提高导航策略的泛化性和可靠性。

Method: 作者提出先通过互联网视频进行大规模自监督训练，学习图像/文本到目标距离的预测器（VLD）；然后用几何距离信号和噪声在仿真中训练强化学习策略，最后部署时策略直接消费VLD输出，实现感知与动作决策解耦。并提出了等级一致性评估距离函数优劣。

Result: VLD方法在多项仿真实验中达到了与主流方法相当甚至更优的导航效果，显著优于ViNT等基于时间距离的旧方法，并且兼容多种目标输入（图像、文字），验证了其灵活性与扩展性。

Conclusion: VLD为机器人导航策略学习提供了一条全新且可扩展的技术路线，通过自监督感知学习和策略解耦，克服了传统Sim-to-real和数据短缺问题，促进了更加可靠且泛化能力强的多模态导航策略的实现。

Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.

</details>


### [132] [DIJIT: A Robotic Head for an Active Observer](https://arxiv.org/abs/2512.07998)
*Mostafa Kamali Tabrizi,Mingshi Chi,Bir Bikram Dey,Yu Qing Yuan,Markus D. Solbach,Yiqian Liu,Michael Jenkin,John K. Tsotsos*

Main category: cs.RO

TL;DR: 该论文介绍了一种新型双目机器人头部DIJIT，专为移动主动观察的机器人设计，用于研究人类视觉中的眼-头运动及其与视觉能力的关联，并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 主动视觉及人类眼-头-颈协同运动对于理解和提升视觉任务表现至关重要，但目前缺乏既能模拟人类运动又能支持光学自主调节的实验平台。此外，当前计算机视觉系统在运动规划及视觉任务解决方式上与人类存在差异。

Method: 设计并实现了一套具有9个机械自由度和4个光学自由度的双目机器人头部DIJIT，运动速度和范围接近人类。特别设计了能实现汇聚立体视觉（如集合、转动、扭转等）的运动模式。提出了新的摄像头快速转动（扫视）方法，建立了摄像头方向与电机值之间的直接关系。

Result: DIJIT的运动范围和速度与人类相当。新提出的摄像头扫视方法实现的运动精度接近人类眼睛的扫视运动。

Conclusion: DIJIT为主动视觉、眼头联动研究以及类人运动与计算机视觉方法对比提供了创新实验平台，其运动与精度接近人类，推进了人机视觉协同理解和仿真研究。

Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.

</details>


### [133] [Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations](https://arxiv.org/abs/2512.08028)
*Lampis Papakostas,Aristeidis Geladaris,Athanasios Mastrogeorgiou,Jim Sharples,Gautier Hattenberger,Panagiotis Chatzakos,Panagiotis Polygerinos*

Main category: cs.RO

TL;DR: 该论文提出了一种无人机集群系统，通过分布式传感与协同导航，提升灾害情景下（如森林火灾）应急救援的效能，重点解决了避障和区域覆盖优化问题。


<details>
  <summary>Details</summary>
Motivation: 灾难应急场景（如野火）中，对大范围、实时高效的数据采集有强烈需求，单一无人机航时和能力有限，集群协同与避障、覆盖优化成为关键挑战。

Method: 系统采用多无人机分布式传感，利用本地欧氏符号距离场（ESDF）进行自主避障，在保持队形的同时减少路径偏离。还将TSP变体引入规划过程，根据环境行为和关键基础设施预先分配点的优先级，实现兴趣点的高效覆盖。

Result: 通过不同规模集群的仿真实验，系统能够在保证无人机与障碍物及彼此之间有效避碰的前提下，最大化区域覆盖率。

Conclusion: 所提出的无人机集群系统能够提升灾害环境下的应急数据采集能力，通过高效避障与覆盖规划，有效降低任务失败风险，实现更高的任务可靠性与数据利用率。

Abstract: This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.

</details>


### [134] [An Introduction to Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2512.08052)
*Pedro Santana*

Main category: cs.RO

TL;DR: 本文介绍了应用于机器人和虚拟角色等具身体代理的深度强化学习（DRL）和深度模仿学习（DIL）基础算法，关注其实现任务时如何解决序贯决策问题。


<details>
  <summary>Details</summary>
Motivation: 由于手动设计复杂控制器非常困难，研究者希望通过学习方法，让智能体自动学习决策策略。

Method: 文章采用自成体系、深入浅出的方式，介绍与具身体智能体相关的核心数学和机器学习知识。主要涵盖DRL中的马尔可夫决策过程（MDP）、REINFORCE算法和PPO算法，以及DIL中的行为克隆、DAgger和GAIL算法。

Result: 系统阐述了深度强化学习和深度模仿学习在具身体智能体中的关键算法及其原理，实现了理论知识与实际应用的对接。

Conclusion: 学习型方法为解决复杂的序贯决策问题提供了可行方案。本文着重培养对代表性方法的深入理解，而非广泛综述，适合初学者和研究人员理解核心机制。

Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.

</details>


### [135] [Chat with UAV -- Human-UAV Interaction Based on Large Language Models](https://arxiv.org/abs/2512.08145)
*Haoran Wang,Zhuohang Chen,Guang Li,Bo Ma,Chuanghuang Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的双智能体人机无人机（HUI）交互框架，通过任务规划和执行两个独立智能体提升无人机任务规划的个性化和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用普及，传统预定义交互方法难以满足多样、个性化需求，尤其是在用户与无人机间缺乏共通语言的情况下。利用LLM理解自然语言和无人机指令，有望实现更智能的人机交互。

Method: 设计了两个独立的LLM智能体（任务规划智能体和执行智能体），通过差异化的提示工程分别处理任务的理解、规划与执行，并在涵盖四类典型场景的任务数据库上，用三种指标进行性能评估。同时对不同LLM模型控制无人机的效果进行了对比测试。

Result: 实验结果显示，新框架提升了交互过程的流畅性和任务执行的灵活性，更好满足了用户个性化需求。

Conclusion: 该双智能体HUI框架有效促进了基于LLM的人机无人机交互，适应复杂应用场景，为无人机个性化任务规划与执行提供了新思路。

Abstract: The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.

</details>


### [136] [RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features](https://arxiv.org/abs/2512.08170)
*Haoxin Zhang,Shuaixin Li,Xiaozhou Zhu,Hongbo Chen,Wen Yao*

Main category: cs.RO

TL;DR: 本文提出了一种面向用户且兼容多种LiDAR和相机传感器的标定工具包，仅需激光点集和相机图像各一对，实现了目标无关环境下的高精度标定，且具有更强的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-相机标定方法通常依赖目标物或初始变换，对大范围外参变化不稳定，且不够用户友好。本研究旨在简化使用流程，支持多传感器类型，并提升标定的准确性和鲁棒性。

Method: 采用Gluestick流程自动建立摄像机2D特征点与LiDAR 3D特征点和线的对应关系，给出鲁棒的初始外参估计。引入特征分布对标定质量的影响分析，基于该分析自适应调整各特征的代价权重，并优化外参参数，通过过滤不良特征提升最终精度。

Result: 在室内外多种不同类型的LiDAR-相机组合和场景下进行了系统实验，结果显示该方法相比当前主流（SOTA）技术在准确性和鲁棒性上均有明显提升。

Conclusion: 本文方法为LiDAR-相机标定提供了一个高效、通用且操作简单的工具，极大降低了实际应用门槛，并以优越的性能验证了方法有效性，相关代码也已开源，便于业界和学术界采用。

Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.

</details>


### [137] [Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/abs/2512.08186)
*Meng Wei,Chenyang Wan,Jiaqi Peng,Xiqian Yu,Yuqiang Yang,Delin Feng,Wenzhe Cai,Chenming Zhu,Tai Wang,Jiangmiao Pang,Xihui Liu*

Main category: cs.RO

TL;DR: 本文提出DualVLN，采用双系统结构，将高层次推理（全局规划）与低层次行动执行（局部导航）结合，提升视觉-语言导航（VLN）系统在动态复杂环境下的实时性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航方法端到端地将视觉与语言映射为离散短时序动作，导致动作碎片化、延迟高，且在真实动态环境下表现不佳。

Method: 提出DualVLN，包含两个系统：系统2基于大视觉-语言模型（VLM）进行全局推理，预测中期路标目标；系统1为轻量级多模态扩散Transformer策略，结合系统2的中期目标与显式像素目标，实现平滑高效的本地导航。两系统训练分离，增强泛化与可解释性。

Result: DualVLN在所有VLN基准测试中均优于现有方法，并在真实环境中表现出强大的长时规划能力和动态适应能力。

Conclusion: DualVLN通过结合高层全局推理与低层局部行动，成功提升VLN系统在复杂动态环境下的综合表现，展示了对未来实际部署的巨大潜力。

Abstract: While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.

</details>


### [138] [Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model](https://arxiv.org/abs/2512.08188)
*Wenjiang Xu,Cindy Wang,Rui Fang,Mingkang Zhang,Lusong Li,Jing Xu,Jiayuan Gu,Zecui Zeng,Rui Chen*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的机器人操作规划方法EToT（Embodied Tree of Thoughts），结合物理引擎和视觉语言模型，提高了预测物理动态和应对故障的能力，在多项任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成世界模型在机器人操作中容易出现不准确和长时物理约束失效的问题，因此需要一种更具物理基础、能适应复杂动态的世界模型。

Method: EToT方法通过引入基于物理引擎的数字孪生体作为世界模型，将操作规划问题转化为树搜索问题，并结合两种分支机制：1）事先分支（Priori Branching）根据语义与空间分析生成多样操作路径；2）反思分支（Reflective Branching）利用视觉语言模型在仿真失败时进行诊断并修正操作树。该方法保证了高层规划与物理一致。

Result: EToT在多个短期和长期操作任务中进行验证，在预测物理动态和自适应故障方面，相比主流方法表现更优。

Conclusion: EToT框架有效提升了机器人操作规划的物理合理性和适应性，为基于物理约束的智能操作规划提供了新的解决思路。

Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .

</details>


### [139] [High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement](https://arxiv.org/abs/2512.08206)
*Duo Zhang,Junshan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的双臂同步重排规划框架SDAR，在复杂的桌面物体重排任务中实现两机械臂高效协作，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 桌面重排任务中，物体之间常存在紧密的依赖和缠绕关系，单臂规划难以高效解决，而双臂协作规划复杂度极高，亟需突破性的规划框架。

Method: SDAR框架结合了依赖驱动的任务规划器（SDAR-T）和双臂同步运动规划器（SDAR-M）。任务规划器通过分解全局依赖图获得更优的任务拆分，运动规划器利用GPU并行运动规划工具，在众多方案中筛选最优的双臂同步运动计划。

Result: SDAR在复杂的非单调、长序列桌面重排任务上达到100%成功率，解决质量远超此前最优方法，并且能在真实机器人（UR-5e机械臂）上直接、可靠运行。

Conclusion: SDAR展示出卓越的复杂桌面双臂同步重排能力，为实际多机器人协作和高级智能体任务规划提供了有效方案。

Abstract: We propose Synchronous Dual-Arm Rearrange- ment Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal config- urations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR- M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state- of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.

</details>


### [140] [Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior](https://arxiv.org/abs/2512.08233)
*Timothy Chen,Marcus Dominguez-Kuhne,Aiden Swann,Xu Liu,Mac Schwager*

Main category: cs.RO

TL;DR: 该论文提出了一种通过利用人类演示和视觉语言模型（VLM）常识，来提取人类隐式风险模型的贝叶斯框架。它能输出与人类风险感知一致的像素级风险图，在机器人规划等下游任务中展示了优良表现。


<details>
  <summary>Details</summary>
Motivation: 人类对风险的理解是连续且与环境和空间相关的，但目前大多数自动化系统难以实现这种类似人类的风险感知。因此，作者希望让机器人能更好地理解和执行符合人类风险直觉的动作选择。

Method: 作者提出了一种基于贝叶斯公式的风险建模框架：先验由预训练的视觉语言模型提供，似然函数采用训练后的ViT视觉变换器，用于预测像素级风险图。系统输入为RGB图片及待查询目标，输出为与目标相关的风险热力图。该方法直接从安全的人类演示视频进行监督学习，能泛化到新物体和场景。

Result: 实验表明，该方法预测的风险与人类偏好高度吻合。该框架还能作为机器人视觉运动规划的价值预测器，或与经典轨迹优化结合，生成更具人类感的动作路径。此外，贝叶斯公式使模型可以快速适应更多观测或常识规则。

Conclusion: 该研究展示了用贝叶斯风险模型以人类视角学习和预测风险的可行性，为自主系统实现更符合人类意图的决策迈出了重要一步。

Abstract: Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.

</details>


### [141] [Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks](https://arxiv.org/abs/2512.08248)
*Ahan Basu,Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种基于时空通道（STT）的控制框架，结合物理信息神经网络实现对具有未知动力学的非线性纯反馈系统的任务约束控制，并在典型案例中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决通用控制仿射多输入多输出（MIMO）非线性纯反馈系统在未知动力学和外部干扰下，如何满足时间受控的“到达-避障-停留”（T-RAS）任务问题。传统方法难以应对未知复杂动力学和高维约束。

Method: 将时空通道（STT）建模为随时间变化的球体，其中心和半径通过物理信息神经网络(PINN)联合逼近。STT的约束被设计为PINN的损失函数，并通过算法训练，使PINN在选定采样点上最小化约束违背。同时，采用基于Lipschitz条件的形式化验证，确保在整个时间连续域上的约束满足。基于学习到的STT，提出了无近似的解析型控制器。

Result: 在移动机器人和飞行器穿越障碍场景中进行了案例研究，验证了所提出框架的有效性和可扩展性。结果显示该方法能够在外部扰动和动力学未知情况下有效实现T-RAS任务。

Conclusion: 该方法能在未知复杂动力学和外部干扰情形下，保证非线性多输入多输出系统高效、可验证地实现复杂时空约束任务，具有推广到更复杂场景的潜力。

Abstract: This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.

</details>


### [142] [Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation](https://arxiv.org/abs/2512.08271)
*Srijan Dokania,Dharini Raghavan*

Main category: cs.RO

TL;DR: 本文提出了Zero-Splat TeleAssist，一种零样本传感器融合管线，利用普通CCTV视频实时构建6自由度世界模型，实现多机器人遥操作协同。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人遥操作往往依赖特定硬件（如深度传感器或标记点），而普通环境缺乏这些条件，亟需一种无需特殊传感器的鲁棒、低成本解决方案。

Method: 系统整合了视觉-语言分割、单目深度估计、加权主成分分析（PCA）姿态提取和3D高斯体素化技术（3D Gaussian Splatting），通过对普通CCTV流进行多模块融合，实时生成机器人及操作环境的6-DoF全局模型。

Result: 该方法可在不使用标记点或深度传感器的情况下，为每位操作员实时提供多个机器人在操作环境中的全局位置和方向，实现高效的多边协同遥操作。

Conclusion: Zero-Splat TeleAssist能够以低成本和高通用性，将现成CCTV转化为多机器人协同遥操作的感知基础，有效推动遥操作系统在人机交互场景的落地。

Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.

</details>


### [143] [Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making](https://arxiv.org/abs/2512.08280)
*Haldun Balim,Na Li,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种离线决策生成模型MPDiffuser，通过引入动态一致性和任务相关性，提升了生成轨迹的可行性和表现，并在多项基准测试和真实机器人控制中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法在离线决策中，常常生成动态不可行的行为轨迹，难以满足实际系统的动态约束和任务目标。急需研究能够兼顾任务对齐和动态一致性的合成方法，以增强生成行为的现实可用性。

Method: MPDiffuser为组合式基于模型的扩散框架，包含：1）规划器——生成多样且与任务对齐的轨迹，2）动力学模型——确保轨迹与系统动力学一致，3）排序器——挑选与任务目标最相关的行为。采用交替扩散采样，将规划与动力学校正步步结合，使轨迹逐步同时优化任务对齐和动力学可行性。此外，理论分析证明该框架可在数据先验与动力学一致性间取得平衡。

Result: 实验显示，MPDiffuser在D4RL和DSRL离线决策基准上均优于现有方法，具备更高的样本效率，即使在低质量数据下也能学习动力学，并能适应新动力学。此外，初步实验展现了其在高维视觉控制任务上的潜力，并成功部署于真实四足机器人。

Conclusion: MPDiffuser有效提升离线决策的轨迹可行性和任务表现，理论和实验验证其优越性，具备扩展至复杂感知和真实机器人控制场景的潜力，为实际决策生成提供了有力工具。

Abstract: Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.

</details>


### [144] [Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging](https://arxiv.org/abs/2512.08333)
*Yajat Yadav,Zhiyuan Zhou,Andrew Wagenmaker,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出通过模型权重插值的方法，在对通用机器人策略模型进行细调时，同时保持其通用性与新任务能力。该方法在模拟及真实环境中表现优越，可实现多技能的持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人策略虽能广泛泛化，但对新任务有限数据细调时易过拟合，失去原有能力并且新任务泛化性差。

Method: 将经过新任务细调后的模型与原有通用模型进行权重插值，合并二者特性，从而保持通用能力并学会新任务。

Result: 插值后的模型兼具原有通用任务能力和新任务的鲁棒泛化能力，在新任务分布外测试上优于仅预训练或仅细调模型；同时适用于持续、多技能学习。

Conclusion: 权重插值是一种简单有效的实现通用机器人策略持续进化与泛化能力保留的方法，有助于实现真正泛化与拓展能力的单一机器人策略模型。

Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.

</details>


### [145] [Learning Robot Manipulation from Audio World Models](https://arxiv.org/abs/2512.08405)
*Fan Zhang,Michael Gienger*

Main category: cs.RO

TL;DR: 本文提出了一种生成式潜在流匹配模型，能够预测未来音频观测，提升机器人在需要声音感知的任务中的表现。实验表明，该方法优于不考虑未来预测的传统方法。


<details>
  <summary>Details</summary>
Motivation: 许多机器人学习任务需要多模态推理，特别是在仅靠视觉信息无法有效判断任务状态时（如倒水时需感知声音）。因此，准确预测和利用音频信息成为提升机器人智能的重要方向。

Method: 作者提出了一种生成式潜在流匹配模型（generative latent flow matching model），该模型能够基于当前观测预测未来音频信号，并将该预测能力集成到机器人决策策略中，使其能够进行长时序的推理和决策。

Result: 在两个需要在真实环境中理解音频或音乐信号的操作任务中，提出的方法在准确感知和推理未来状态上明显优于不具备未来音频预测能力的方法。

Conclusion: 机器人在需要多模态感知的任务中，仅有多模态输入还不够，关键在于对体现节奏等内在规律的未来音频状态的准确预测。提出的方法显著提升了相关任务中的机器人性能。

Abstract: World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.

</details>


### [146] [A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems](https://arxiv.org/abs/2512.08476)
*Po-An Shih,Shao-Hua Wang,Yung-Che Li,Chia-Heng Tu,Chih-Han Chang*

Main category: cs.RO

TL;DR: 本论文提出了一种基于多智能体大语言模型（LLM）的设计空间探索（DSE）框架，用于自动驾驶系统的硬件/软件联合优化，能自动分析多模态执行结果，不依赖人工参与，提升探索效率和结果质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在多变环境下高效探索大量硬件/软件配置，传统方法在处理复杂结果和多目标权衡时低效且劳动密集，需新的自动化、智能化探索方法。

Method: 提出一个多智能体LLM驱动DSE框架，结合3D仿真与性能剖析工具，通过不同专职LLM智能体自动完成用户输入解析、配置生成、调度运行及多模态（视觉、文本）输出分析，实现全流程自动化探索瓶颈和优选设计方案。

Result: 在机器人出租车（SAE L4自动驾驶）案例中，原型系统在相同探索预算下，相较于遗传算法基线，找到更多帕累托最优、低成本且导航时间更短的解决方案，实验展示LLM方法的探索效率优势。

Conclusion: 多智能体LLM驱动的设计空间探索框架能高效自动识别和优化自动驾驶系统设计，显著降低人工参与，展示出推进自动驾驶系统自动化设计的巨大潜力。

Abstract: Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.

</details>


### [147] [Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception](https://arxiv.org/abs/2512.08481)
*Yixiang Lin,Tiancheng Yang,Jonathan Eden,Ying Tan*

Main category: cs.RO

TL;DR: 本论文通过实验证明，人类在与机器人进行物理交互时，对不确定性的反应具有高度个体差异，传统的最优控制理论难以完全解释这些行为。


<details>
  <summary>Details</summary>
Motivation: 当前物理人机交互（pHRI）需要保证用户的安全与舒适，但人类在不确定环境下的行为常与最优控制假设不同。为了更好地设计安全有效的pHRI系统，必须理解人在不确定性下的真实反应机制。

Method: 研究通过设计一个人类与机器人身体耦合的目标到达实验，系统性地改变机器人施加助力或干扰的概率（10%到90%），并记录分析参与者的力量输入和决策策略。

Result: 实验结果显示存在两类主要的人类行为：一类是根据干扰概率调整物理反应的“权衡型”，另一类则是无论概率如何都倾向规避风险的“始终补偿型”。此外，发现人类对概率的感知和实际概率存在偏差。

Conclusion: 研究验证了人类在pHRI中的决策行为具有高度个体化，概率感知存在误差。应采用如累积前景理论（CPT）这样的解释性更强的行为模型，以更准确建模人类行为，指导未来自适应机器人控制器的设计。

Abstract: Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\% to 90\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a "trade-off" group that modulated their physical responses according to disturbance likelihood, and an "always-compensate" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.

</details>


### [148] [SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking](https://arxiv.org/abs/2512.08518)
*Nadezhda Kushina,Ko Watanabe,Aarthi Kannan,Ashita Ashok,Andreas Dengel,Karsten Berns*

Main category: cs.RO

TL;DR: 本研究探讨了用户在不同距离下与机器人Ameca互动时的舒适度，并利用眼动追踪和主观报告进行数据收集，利用不同机器学习模型进行舒适度建模。发现决策树模型表现最佳，最小瞳孔直径是关键预测因子，表明人机互动中的生理舒适阈值与人际互动有所不同，可通过可解释模型有效预测。


<details>
  <summary>Details</summary>
Motivation: 尽管眼动追踪在估测人际互动舒适度方面表现出色，其在人-机器人互动场景下的适用性尚未被研究。鉴于社会机器人需要适应人类的空间规范以提升舒适感，探讨其适用性有助于优化机器人在社交互动中的行为。

Method: 招募19名受试者，与机器人Ameca在四种不同距离下互动（0.5至2.0米），采用移动式眼动追踪仪收集注视数据，并结合主观舒适度报告。利用多种机器学习和深度学习模型（包括Transformer和决策树）进行舒适度建模和预测。

Result: 在所有评估的模型中，决策树分类器在舒适度预测任务中取得最高性能（F1分数0.73），并发现最小瞳孔直径是最重要的预测指标。Transformer模型未能像在人际互动场景一样表现出色。

Conclusion: 人机互动中的生理舒适阈值与人际互动存在差别，使用解释性强的简单模型（如决策树）能够有效预测用户舒适度。对于社会机器人设计应考虑这些差异，以更好地适应人类的空间与心理需求。

Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

</details>


### [149] [vEDGAR - Can CARLA Do HiL?](https://arxiv.org/abs/2512.08541)
*Nils Gehrke,David Brecht,Dominik Kulmer,Dheer Patel,Frank Diermeyer*

Main category: cs.RO

TL;DR: 本文提出了一个针对自动驾驶功能开发流程的开源仿真测试框架，扩展了CARLA仿真器以支持在专用硬件上的实时传感—执行闭环测试，实现端到端的自动驾驶算法评估。


<details>
  <summary>Details</summary>
Motivation: 目前主流开源仿真器如CARLA多用于算法训练和虚拟测试，但缺乏完整的实时传感器和执行器闭环、基于实际硬件的测试能力，不能全面验证自动驾驶系统在真实硬件上的表现。作者希望为开源车用自动驾驶软件开发流程提供统一的、高可信的评估工具和平台。

Method: 首先，作者根据自动驾驶功能硬件在环（HiL）测试的需求，提出并实现了一个新仿真架构。将CARLA进行功能拓展，形成支持端到端传感—执行闭环的仿真系统vEDGAR，并开放源码。

Result: 基于提出的需求，对vEDGAR系统进行了测试和评估。结果显示，该系统能够满足自动驾驶自动化软件在真实硬件上的HiL实时评估要求，验证了其作为开发流程统一测试工具的可行性。

Conclusion: 该研究通过扩展CARLA和开发vEDGAR，显著提升了开源自动驾驶仿真系统的能力，为自动驾驶功能开发流程提供了全流程支撑的测试平台，加强了开源开发的实用性和一致性。CARLA及vEDGAR适合自动驾驶车辆的HiL测试，相关代码已全部开源。

Abstract: Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR

</details>


### [150] [Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations](https://arxiv.org/abs/2512.08548)
*Yuchi Zhang,Churui Sun,Shiqi Liang,Diyuan Liu,Chao Ji,Wei-Nan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 本文提出了一种结合了语义的动作表征方式，可以减少机器人操作任务中数据分布漂移带来的影响，从而提升大模型在机器人操作任务中的迁移和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作任务中常用端到端、大语言模型启发的架构，但由于不同平台和任务间动作数据的尺度差异，分布漂移严重，影响预训练知识的迁移效果。

Method: 提出了一种基于语义的动作表征方式，将动作表示为忽略数值尺度，仅关注方向的抽象运动表征，避免了常规离散化表征对数值尺度的敏感性。通过把动作表示向普通词汇靠近，缩小异模态特征距离，从而提升预训练的泛化和迁移能力。

Result: 在两个多任务机器人基准测试中，新提出的方法显著提升了泛化性能和在不同机器人操作任务间的迁移能力。

Conclusion: 基于运动方向的语义动作表征，可以有效缓解不同机器人平台、任务间的分布漂移，提升机器人操作领域中预训练模型的泛化和迁移性能。

Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.

</details>


### [151] [RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight](https://arxiv.org/abs/2512.08574)
*Vit Kratky,Robert Penicka,Parakh M. Gupta,Ondrej Prochazka,Martin Saska*

Main category: cs.RO

TL;DR: 提出了一种基于非线性模型预测控制（NMPC）和时变互易速度约束（RVCs）的多无人机避碰方法，无需过多通讯，仅基于可观察信息，效率高、适用于敏捷飞行，在仿真和实测中均实现了更快且安全的避碰性能。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机避碰方法大多依赖机器人间的通信和较为理想化的动力学建模，难以兼顾实时性和非线性动态下的敏捷飞行需求。因此，作者希望设计一种更加高效、通信需求低、适用于高动态场景（如敏捷无人机飞行）的避碰方案。

Method: 本方法基于非线性模型预测控制（NMPC）并引入时变的互易速度约束（RVCs），通过对约束条件高效运算并嵌入控制器级实时解决，依赖于环境中可观测数据，实现全流程100 Hz运行，适配多无人机复杂动力学。

Result: 在最多10架无人机、速度最高25 m/s、加速度最高30 m/s²的极端仿真与现实实验下，方案均实现避碰零失误。与现有方法相比，飞行时间在复杂场景下平均缩短31%。

Conclusion: 该方法无需大量通信，能高效处理多无人机动态避碰，提升了飞行效率，验证了其实用性和优越性，对敏捷无人机群体协同具有较大推动作用。

Abstract: This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.

</details>


### [152] [Mind to Hand: Purposeful Robotic Control via Embodied Reasoning](https://arxiv.org/abs/2512.08580)
*Peijun Tang,Shangjin Xie,Binyan Sun,Baifu Huang,Kuncheng Luo,Haotian Yang,Weiqi Jin,Jianan Wang*

Main category: cs.RO

TL;DR: Lumo-1是一种将视觉、语言、动作三者结合的通用机器人模型，实现了“思考”与“行动”的统一，并在多项机器人任务上优于现有方案，展现出优秀的泛化与推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然AI系统借助大规模互联网数据获得了推理能力，但如何将这种能力落地到物理行动中仍是难题。该研究旨在让机器人拥有类似人类的基于语境和意图的推理及行动能力。

Method: 该方法基于多模态视觉-语言模型，并通过三阶段的预训练流程扩展到具身推理和动作预测：(1) 利用精选视觉-语言数据继续预训练，增强规划、空间理解等能力；(2) 在机器人跨平台数据与视觉-语言数据上共同训练；(3) 在双臂移动机器人上进行带推理过程的动作训练，并结合强化学习提升推理与行动一致性。

Result: 实验证明Lumo-1在具身视觉-语言推理方面取得了显著提升，尤其在应对新物体、新环境、复杂长任务和需要推理的自然人类指令时，较现有强基线模型表现更佳。

Conclusion: Lumo-1实现了推理与机器人动作的高效融合，为通用型机器人在复杂任务中的应用打下基础，推动了机器人朝“思考+行动”一致的方向发展。

Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

</details>


### [153] [Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems](https://arxiv.org/abs/2512.08630)
*Marta Manzoni,Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本论文提出采用多任务贝叶斯优化（MTBO）方法，用于多无人机系统中分布式轨迹生成算法的调优，并对比了全任务平均优化与单任务独立优化两种策略。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统中，随着无人机数量增多，轨迹生成算法的调优变得复杂且成本高，急需高效的方法提升优化效率和任务完成时间。

Method: 将不同轨迹生成场景（由无人机之间的交互数量定义）作为不同的任务，采用多任务高斯过程模型任务间的关系，应用多任务贝叶斯优化方法进行参数调优，并对比平均任务优化和单任务优化策略。

Result: 仿真结果表明，单任务优化可随着规模扩大带来更短的任务时间，但在优化时间上消耗显著高于平均任务优化。

Conclusion: 多任务贝叶斯优化为多无人机轨迹生成算法调优提供了高效框架，权衡了优化效率与任务完成时间。

Abstract: This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.

</details>


### [154] [A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation](https://arxiv.org/abs/2512.08653)
*Doumegna Mawuto Koudjo Felix,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.RO

TL;DR: 论文提出了一种新颖的、多样化可控的雷达点云降质仿真平台，用于评估SLAM系统在各种真实物理降质条件下的鲁棒性，支持可解释和可复现的压力测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于激光雷达的SLAM系统容易受遮挡、噪声、视场范围下降等影响，现有的鲁棒性评估方法往往缺乏物理基础，或者无法反映传感器特性。因此需要一种更真实、可解释的点云降质测试方法。

Method: 论文提出了一个与传感器特性相关的现象学仿真框架，可以在原始点云上直接模拟各种典型降质（如结构化丢失、视场减少、高斯噪声、遮挡、稀疏化与运动失真），并保持点云几何、强度与时间结构不变。系统支持自动话题和传感器检测，模块化配置，有四级严重度，在ROS中实时运行。

Result: 作者在三种不同激光雷达和五个主流SLAM系统上进行了实验，发现不同传感器设计和环境会导致SLAM表现出不同的鲁棒性特征。

Conclusion: 该开源仿真工具为基于激光雷达的SLAM提供了物理意义强、场景可控、可再现的降质压力测试基准，为后续SLAM算法研究和传感器选择决策提供实用参考。

Abstract: Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.

</details>


### [155] [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](https://arxiv.org/abs/2512.08656)
*Lauritz Rismark Fosso,Herman Biørn Amundsen,Marios Xanthidis,Sveinung Johan Ohrem*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）的零样本迁移（zero-shot sim2real）速度控制器Sim2Swim，实现了水下机器人（AUV）的敏捷六自由度（6DOF）路径跟踪和机动，训练时间仅需3分钟，且无需后处理或再调参，在多种配置下均表现出强健性能。


<details>
  <summary>Details</summary>
Motivation: 现有的全动型AUV虽然具备高机动能力，但由于复杂的动力学、不确定性和载荷变化，控制难度大，通常需要针对特定硬件精细调整控制器。一般难以在无需重新调参的情况下，实现强健的时变参考轨迹跟踪与敏捷机动。实际应用中，AUV的全6DOF高机动性能很少被真正利用。

Method: 作者提出Sim2Swim方法，借鉴先进的DRL位置控制方案，通过大规模并行训练与域随机化，使策略能泛化到不同载荷与动力学条件的AUV。训练过程完全在仿真中进行，且时间极短，实现了端到端的速度控制策略，无需后续调参。

Result: Sim2Swim经过多种配置的池中实地测试，结果表明其能够在面临多样动力学和载荷的AUV上，维持高效、敏捷且鲁棒的6DOF控制性能，展现出了很好的泛化能力。

Conclusion: 本文首次实现了AUV零样本迁移的DRL速度控制器，具备极短的训练周期、极强的泛化能力与实用价值，为AUV全6DOF敏捷机动和时变路径跟踪在实际应用中的推广提供了可能。

Abstract: Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.

</details>


### [156] [Ergodic Trajectory Planning with Dynamic Sensor Footprints](https://arxiv.org/abs/2512.08661)
*Ziyue Zheng,Yongce Liu,Hesheng Wang,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 本文提出了针对动态且分辨率可变的传感器覆盖范围的信息获取轨迹规划新方法，通过更真实的传感器模型，实现更优的区域扫描和信息采集。


<details>
  <summary>Details</summary>
Motivation: 现有的遍历性（ergodic）轨迹规划通常假设传感器范围恒定，无法适应实际中传感器随运动变化的情况，如无人机相机随着高度与姿态变化而变化视野，导致现有方法与实际应用脱节。

Method: 作者提出一种新的度量指标，能够考虑动态变化的传感器覆盖范围，并分析了理论上的局部最优条件，设计了数值轨迹优化算法来联合优化轨迹和传感器覆盖范围。

Result: 实验结果表明，该方法相比传统方法在遍历性方面达到数量级的提升，并成功应用于多无人机三维空间对象的遍历覆盖任务。

Conclusion: 本文所提方法能显著提升带动态传感器的自主机器人在信息采集任务中的表现，推动了现实场景中更有效的轨迹规划与区域覆盖技术发展。

Abstract: This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.

</details>


### [157] [Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility](https://arxiv.org/abs/2512.08688)
*Mark Pustilnik,Francesco Borrelli*

Main category: cs.RO

TL;DR: 提出了一种人-机器人合作导航的新动态博弈方法，支持在共享空间内共同满足安全约束，允许人和机器人分担不同程度的安全责任，并嵌入到预测控制框架中。


<details>
  <summary>Details</summary>
Motivation: 现实协作场景中，人和机器人需要在共享空间安全共处，但目前大多方法默认各方在安全约束中承担相等责任，缺乏灵活性。该工作旨在解决不同参与方在安全保障责任分配上的实际需求。

Method: 提出非归一化均衡结构，让人类和机器人按照实际能力和意愿分担不同的安全任务（如避障、维持距离），并将该结构嵌入有限预测时域（MPC）最优控制框架实现动态决策。

Result: 方法成功支持合作主体按照设定不同程度分担安全约束，能在安全性满足和任务完成之间实现灵活权衡，比传统均等分担机制更具适应性。

Conclusion: 引入非归一化均衡结构提升了人-机器人协作导航的现实性与灵活性，为多主体安全协作提供了有效的新工具。

Abstract: This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.

</details>


### [158] [A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models](https://arxiv.org/abs/2512.08754)
*Jason Hughes,Marcel Hussing,Edward Zhang,Shenbagaraj Kannapiran,Joshua Caswell,Kenneth Chaney,Ruichen Deng,Michaela Feehery,Agelos Kratimenos,Yi Fan Li,Britny Major,Ethan Sanchez,Sumukh Shrote,Youkang Wang,Jeremy Wang,Daudi Zein,Luying Zhang,Ruijun Zhang,Alex Zhou,Tenzi Zhouga,Jeremy Cannon,Zaffir Qasim,Jay Yelon,Fernando Cladera,Kostas Daniilidis,Camillo J. Taylor,Eric Eaton*

Main category: cs.RO

TL;DR: 该报告提出了一个用于大规模伤亡事件远程初级分诊的异构机器人系统，通过空地协作的无人机和地面机器人对伤员进行定位、伤情评估及医疗优先级划分，减少救援人员风险。


<details>
  <summary>Details</summary>
Motivation: 在大规模伤亡事件中，传统分诊需要救援人员冒险进入危险现场，存在较大安全风险，因此，开发无需人身涉险的自动化分诊系统，有助于提升救援效率和保障救援人员安全。

Method: 该系统由无人机（UAV）和地面机器人（UGV）协作构成：无人机负责识别伤员并提供现场鸟瞰图；地面机器人配备专用传感器，可测量生命体征、检测伤情并定位伤员。系统覆盖定位、生命体征测量、伤情分级、精神状态评估及数据整合全分诊流程。

Result: 该方法完善了以往只关注探索或有限医学评估的方案，实现了完整的远程分诊功能，并在DARPA分诊挑战中进行了开发和展示。

Conclusion: 多机器人系统能够扩展人类在灾难应急中的能力，实现无风险、高效的远程分诊，有助于最大程度挽救生命。

Abstract: This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.

</details>


### [159] [Data-Driven Dynamic Parameter Learning of manipulator robots](https://arxiv.org/abs/2512.08767)
*Mohammed Elseiagy,Tsige Tadesse Alemayoh,Ranulfo Bezerra,Shotaro Kojima,Kazunori Ohno*

Main category: cs.RO

TL;DR: 本论文提出了一种基于Transformer的动力学参数估计方法，通过自动化生成多样化机器人物理模型和丰富的轨迹数据，结合注意力机制，有效提升参数估计精度，促进机器人仿真到现实（sim-to-real）转移。


<details>
  <summary>Details</summary>
Motivation: 现有的分析方法在面对复杂机器人结构时动力学参数估计精度有限，传统神经网络如RNN难以捕捉长距离依赖。从而导致机器人从仿真到现实部署存在巨大鸿沟，需要更有效的数据驱动方法来提升参数估计能力。

Method: 提出一种基于Transformer的估计模型，通过自动生成包含多样惯性及摩擦特性的8192种机器人数据集，并利用Jacobian派生特征丰富模型输入，结合注意力机制同时建模时序和空间依赖。

Result: 实验显示，最佳配置（序列长度64、频率64Hz、四层、32 heads）下，模型的验证R2为0.8633，对质量和惯性参数取得几乎完美估计，对库仑摩擦有中高精度，对粘性摩擦和远端质心难度较大。

Conclusion: 将Transformer与自动化数据集生成及运动学特征融合，有效提升动力学参数的可扩展和精确估计，为提升机器人仿真到现实的迁移性能提供了新方案。

Abstract: Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems

</details>


### [160] [Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks](https://arxiv.org/abs/2512.08813)
*Connor York,Zachary R Madin,Paul O'Dowd,Edmund R Hunt*

Main category: cs.RO

TL;DR: 该论文研究多机器人系统在面对持续任务和突发紧急任务时的性能权衡，探索行为和感知异质性对任务平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，机器人常常需要在区域巡逻等持续任务与突发、紧急子任务之间做取舍，如何优化团队结构以实现任务平衡尚未有明确定论。

Method: 本文采用仿真方法，设计了“巡逻者”和“搜索者”两类角色（即行为异质性）以及仅搜索者具备特定感知能力（感知异质性），对比分析不同团队组合下的性能表现，并寻找最优权衡点。

Result: 结果显示，在大多数情况下，具备行为异质性的团队（即有专门巡逻和搜索角色）具有最平衡的任务权衡。此外，若限制部分机器人感知能力，拥有一半感知能力的异质性团队与全感知同质性团队性能相当，但成本更低。

Conclusion: 预先设计行为和感知上的异质性是多机器人系统应对时间冲突任务的有效手段，改变异质化程度可按照需求调控系统性能侧重，实现任务优先级的灵活调整。

Abstract: Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization ("patrollers" and "searchers") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.

</details>


### [161] [IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams](https://arxiv.org/abs/2512.08877)
*Ryan LeRoy,Jack Kolb*

Main category: cs.RO

TL;DR: 本文探讨在异质多智能体场景下，自玩PPO方法与引入政策旋转训练（RPT）方法在泛化能力上的表现。结果表明，即便未引入同伴多样性，经典自玩方法也具备较好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 主流多智能体强化学习（MARL）常在同质、参数共享、自玩的设定下训练智能体。这使得智能体可能更倾向于过拟合搭档行为，而非学习到真正适用于环境的协调策略。因此，作者希望了解自玩训练智能体在面对新同伴算法时的泛化能力。

Method: 作者在HeMAC环境（具有能力互补的Observer与Drone两类智能体）下，提出并测试了Rotating Policy Training（RPT）方法，即在训练过程中轮换智能体使用不同学习算法的策略，以引入更多同伴多样性。随后，与传统单一PPO策略共享（IPPO）自玩方法进行对比，特别考察两种方法与未知DDQN策略搭档时的表现。

Result: 实验发现，引入RPT方法的智能体与传统IPPO自玩训练的智能体，在与DDQN策略的搭档协作时，表现具有可比性。即使没有在训练阶段见到同伴的策略多样性，IPPO训练方法依然展现较强的泛化能力。

Conclusion: 在异质多智能体任务中，简单的自玩PPO方法（IPPO）在泛化到新同伴策略方面的能力，可能与为提升多样性而设计的复杂训练方法（如RPT）相当。

Abstract: Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.

</details>


### [162] [OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer](https://arxiv.org/abs/2512.08920)
*Jessica Yin,Haozhi Qi,Youngsun Wi,Sayantan Kundu,Mike Lambeta,William Yang,Changhao Wang,Tingfan Wu,Jitendra Malik,Tess Hellebrekers*

Main category: cs.RO

TL;DR: 本文提出了一种开源可穿戴触觉手套OSMO，实现了更精准的人类到机器人操作技能迁移，显著优于仅用视觉的方法，并开放软硬件资料供社区使用。


<details>
  <summary>Details</summary>
Motivation: 仅依靠视频学习机器人操作技能，会缺失关于接触力的重要信息，而这些信息是完成复杂操控任务的关键。

Method: 设计并实现了一款配备12个三轴触觉传感器的手套（OSMO），可配合最新手部追踪技术用于采集丰富的人类操作演示数据，并实现人机之间触觉信号的同步采集和应用。

Result: 在无任何真实机器人数据参与下，仅用OSMO手套采集的人类演示数据训练出的策略，机器人能完成需要持续接触压力的复杂擦拭任务，成功率达72%，并明显优于仅靠视觉的算法。

Conclusion: OSMO大幅减小了人机之间的感知差距，使机器人学习到更加鲁棒的操控技能。硬件及软件已开源，有望推动机器人技能迁移研究社区的发展。

Abstract: Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.

</details>
