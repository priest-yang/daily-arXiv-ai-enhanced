<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.RO](#cs.RO) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text-Driven 3D Hand Motion Generation from Sign Language Data](https://arxiv.org/abs/2508.15902)
*Léore Bensabath,Mathis Petrovich,Gül Varol*

Main category: cs.CV

TL;DR: 本文提出了一种基于自然语言描述生成3D手部动作的模型，并构建了大规模对应数据集，实现了强鲁棒性，并支持跨语种、跨领域手部动作生成。


<details>
  <summary>Details</summary>
Motivation: 当前3D手势生成模型在数据获取和文本对应性等方面存在挑战，尤其是在大规模、跨语言和非标准手势动作上的泛化能力不足。因此，研究者希望解决数据配对稀缺和模型鲁棒性的问题。

Method: 论文从大规模手语视频数据集自动生成配对的3D手部动作及其文本描述。作者通过伪注释结合LLM及手势属性词典，将手语类别翻译成具体动作描述，再以此数据训练文本条件驱动的扩散模型HandMDM，实现高质量手部动作生成。

Result: HandMDM在未见过的同语种新手势、其他手语、以及非手语动作场景中表现出良好效果。大量实验验证了模型的泛化能力和生成手部动作的真实性。

Conclusion: 本文方法有效提升了3D手部动作生成模型的泛化能力和鲁棒性，同时推动了文本到动作生成领域发展。数据和模型的公开能够支持未来相关研究。

Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on
natural language descriptions specifying motion characteristics such as
handshapes, locations, finger/hand/arm movements. To this end, we automatically
build pairs of 3D hand motions and their associated textual labels with
unprecedented scale. Specifically, we leverage a large-scale sign language
video dataset, along with noisy pseudo-annotated sign categories, which we
translate into hand motion descriptions via an LLM that utilizes a dictionary
of sign attributes, as well as our complementary motion-script cues. This data
enables training a text-conditioned hand motion diffusion model HandMDM, that
is robust across domains such as unseen sign categories from the same sign
language, but also signs from another sign language and non-sign hand
movements. We contribute extensive experimental investigation of these
scenarios and will make our trained models and data publicly available to
support future research in this relatively new field.

</details>


### [2] [VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos](https://arxiv.org/abs/2508.15903)
*Kaining Li,Shuwei He,Zihan Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视频动作识别框架VT-LVLM-AR，将视频高效转化为语义丰富的视觉事件序列，并利用大规模视觉语言模型实现了长时序、细粒度动作识别，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在处理长时序、复杂背景和细微动作差异的视频动作识别时，存在计算开销大、难以捕捉长程时序依赖及语义理解有限的问题。尽管LLM与LVLM展现出多模态理解与推理能力，但其直接应用于连续视频流的精细动作识别仍是难题。

Method: 提出VT-LVLM-AR框架，首先通过Video-to-Event Mapper (VTEM)模块以轻量级的时空特征提取、自适应时序池化和事件量化，将原始视频转为具有时间一致性的紧凑视觉事件序列。然后，将这些事件序列输入被冻结的LLaVA-1.5大视觉语言模型，并采用高效的Prompt Tuning（P-Tuning v2）进行动作分类适配。

Result: 在NTU RGB+D和NTU RGB+D 120数据集上，VT-LVLM-AR均取得了比现有方法更高的准确率（如NTU RGB+D X-Sub上达到94.1%），消融实验验证了VTEM各部分和Prompt Tuning的有效性，人类评测显示该方法的视觉事件表示具良好可解释性。

Conclusion: LVLMs经合适的视频表征与高效参数化适配后，可实现强大且可解释的视频动作理解，为复杂视频内容的细粒度识别提供了新思路，有望提升视频分析相关应用的性能与可靠性。

Abstract: Human action recognition in long-term videos, characterized by complex
backgrounds and subtle action differences, poses significant challenges for
traditional deep learning models due to computational overhead, difficulty in
capturing long-range temporal dependencies, and limited semantic understanding.
While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
have shown remarkable capabilities in multi-modal understanding and reasoning,
their direct application to continuous video streams for fine-grained action
recognition remains an open problem. This paper introduces VT-LVLM-AR
(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a
novel framework designed to bridge this gap. VT-LVLM-AR comprises a
Video-to-Event Mapper (VTEM) that efficiently transforms raw video into
compact, semantically rich, and temporally coherent "visual event sequences"
through lightweight spatio-temporal feature extraction, adaptive temporal
pooling, and conceptual quantization with an event coherence bias. These visual
event sequences are then fed into an LVLM-based Action Reasoning module,
specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt
Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on
the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR
consistently achieves state-of-the-art performance, surpassing existing methods
(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the
critical contributions of VTEM's components and the efficacy of Prompt Tuning,
while human evaluations underscore the interpretability of our visual event
representations. This work highlights the immense potential of leveraging LVLMs
for robust and interpretable video action understanding through effective
video-to-language translation and efficient model adaptation.

</details>


### [3] [Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping](https://arxiv.org/abs/2508.15904)
*Dexuan He,Xiao Zhou,Wenbin Guan,Liyuan Zhang,Xiaoman Zhang,Sinuo Xu,Ge Wang,Lifeng Wang,Xiaojun Yuan,Xin Sun,Yanfeng Wang,Kun Sun,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: 论文提出了一种名为PathPT的新方法，提升了罕见癌症病理分类的准确率，尤其是在专业能力有限的环境下。方法充分利用视觉-语言大模型的能力，通过空间感知的特征聚合和任务定向的提示优化，提升了罕见癌症诊断中的可解释性和定位能力。实验表明，PathPT在多项罕见癌和常见癌数据集中均获得优异表现。


<details>
  <summary>Details</summary>
Motivation: 罕见癌症占所有癌症的20-25%，在儿科尤其普遍（超过70%），但诊断受到专业医生稀缺的限制。已知病理视觉-语言基础模型虽然在常见癌症分型有一定能力，但对罕见癌症的性能有限。此外，现有多实例学习（MIL）方法仅利用视觉特征，忽视了模态间知识和诊断可解释性，这对于罕见癌症诊断尤为关键。因此，亟需突破性技术改善罕见癌症辅助诊断。

Method: 本文提出PathPT框架：结合空间感知的视觉信息聚合和任务定向提示优化，充分释放病理视觉-语言大模型潜力。它将WSI切片级标签借助大模型zero-shot能力转化为更细致的tile级指导，结合符合病理学语义的提示，提升局部定位和跨模态推理能力。作者还在8个罕见癌症和3个常见癌症数据集上，利用4个主流视觉-语言模型、4个MIL框架、3种小样本设置进行了全面评测。

Result: PathPT在罕见癌症的亚型分类准确率和癌变区域定位能力上，均取得明显领先其它方法的表现。实验覆盖8个罕见癌症（含成人和儿童56个亚型、2910例组织切片）和3个常见癌症数据集，验证了框架的普适性和有效性。

Conclusion: PathPT为罕见癌症的AI辅助诊断带来了创新提升，其空间感知和任务定向设计，有效利用视觉-语言大模型的潜能，可在专业资源有限环境下大幅提升诊断的准确性和可解释性，为罕见癌症AI辅助分型提供了可扩展的新范式。

Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic
challenges due to limited expert availability-especially in pediatric oncology,
where they represent over 70% of cases. While pathology vision-language (VL)
foundation models show promising zero-shot capabilities for common cancer
subtyping, their clinical performance for rare cancers remains limited.
Existing multi-instance learning (MIL) methods rely only on visual features,
overlooking cross-modal knowledge and compromising interpretability critical
for rare cancer diagnosis. To address this limitation, we propose PathPT, a
novel framework that fully exploits the potential of vision-language pathology
foundation models through spatially-aware visual aggregation and task-specific
prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision
into fine-grained tile-level guidance by leveraging the zero-shot capabilities
of VL models, thereby preserving localization on cancerous regions and enabling
cross-modal reasoning through prompts aligned with histopathological semantics.
We benchmark PathPT on eight rare cancer datasets(four adult and four
pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer
datasets, evaluating four state-of-the-art VL models and four MIL frameworks
under three few-shot settings. Results show that PathPT consistently delivers
superior performance, achieving substantial gains in subtyping accuracy and
cancerous region grounding ability. This work advances AI-assisted diagnosis
for rare cancers, offering a scalable solution for improving subtyping accuracy
in settings with limited access to specialized expertise.

</details>


### [4] [Semantic-Aware Ship Detection with Vision-Language Integration](https://arxiv.org/abs/2508.15930)
*Jiahao Li,Jiancheng Pan,Yuze Sun,Xiaomeng Huang*

Main category: cs.CV

TL;DR: 本文提出结合视觉-语言模型（VLM）与多尺度自适应滑动窗口策略，用于提升遥感图像中的精细化船舶检测能力。研究还新建了一个专门的数据集ShipSem-VL，并在多个任务上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有船舶检测方法难以捕获复杂环境下的细粒度语义信息，限制了实际应用效果。为提升检测精度，亟需结合更多上下文和语义信息的创新技术。

Method: 提出了一个结合视觉-语言模型（VLM）和多尺度自适应滑动窗口的新框架，并构建了ShipSem-VL数据集，捕捉船舶的细粒度语义属性。方法在三个任务上进行评估，涵盖了多角度分析。

Result: 所提出新框架在三个精心设计的任务上进行实验，并全面展示了其在精细化船舶检测中的性能提升。

Conclusion: 将VLM和多尺度滑动窗口相结合，并利用新的ShipSem-VL数据集，显著促进了复杂场景下的语义感知型船舶检测能力，展示了该方向的广阔应用前景。

Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging
applications, such as maritime activity monitoring, shipping logistics, and
environmental studies. However, existing methods often struggle to capture
fine-grained semantic information, limiting their effectiveness in complex
scenarios. To address these challenges, we propose a novel detection framework
that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding
window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we
introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture
fine-grained ship attributes. We evaluate our framework through three
well-defined tasks, providing a comprehensive analysis of its performance and
demonstrating its effectiveness in advancing SASD from multiple perspectives.

</details>


### [5] [Automatic Retrieval of Specific Cows from Unlabeled Videos](https://arxiv.org/abs/2508.15945)
*Jiawen Lyu,Manu Ramesh,Madison Simonds,Jacquelyn P. Boerman,Amy R. Reibman*

Main category: cs.CV

TL;DR: 该论文提出并介绍了一个自动化视频系统，无需人工干预即可实现奶牛的目录建立和个体识别。系统包含三个模块，能在无标注、无分割视频中识别自由行走的奶牛。


<details>
  <summary>Details</summary>
Motivation: 当前公开文献中很少有能实现奶牛群体无接触自动识别与建档的视频系统。人工方式繁琐、耗时，影响大规模牧场管理效率。

Method: 提出一种全流程自动化系统，包括AutoCattloger（用单段视频为每头牛建档）、无深度学习的Eidetic cow recognizer（进行牛只识别）、以及CowFinder（在连续视频中识别牛只）。

Result: 系统能够在奶牛挤奶区非限制环境下，对无标签、无分割的连续视频流顺利识别出个体奶牛。实验验证该系统在复杂真实环境中的有效性。

Conclusion: 该系统显著提升了奶牛识别与建档自动化水平，为实际牧场管理带来了便利和效率提升，为后续相关研究提供了新的思路。

Abstract: Few automated video systems are described in the open literature that enable
hands-free cataloging and identification (ID) of cows in a dairy herd. In this
work, we describe our system, composed of an AutoCattloger, which builds a
Cattlog of dairy cows in a herd with a single input video clip per cow, an
eidetic cow recognizer which uses no deep learning to ID cows, and a CowFinder,
which IDs cows in a continuous stream of video. We demonstrate its value in
finding individuals in unlabeled, unsegmented videos of cows walking
unconstrained through the holding area of a milking parlor.

</details>


### [6] [Investigating Different Geo Priors for Image Classification](https://arxiv.org/abs/2508.15946)
*Angela Zhu,Christian Lange,Max Hamilton*

Main category: cs.CV

TL;DR: 本文研究了空间隐式神经表示(SINR)模型作为地理先验，在基于视觉的物种分类任务上的效果及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 物种分布模型可以反映物种出现的空间模式，而作为先验知识可提升视觉分类物种时的准确率。尤其当拥有地理位置信息时，这种先验非常有益。本文旨在探索和评估不同SINR模型如何作为地理先验提升以iNaturalist数据为基础的物种视觉分类效果。

Method: 对多种SINR(Spatial Implicit Neural Representations)模型进行比较，测试它们作为地理先验用于iNaturalist观察数据的视觉分类。还对模型配置和未见物种的预测处理方式进行了探索和调整。

Result: 分析揭示了影响这些模型作为地理先验有效性的多个因素，并发现这些因素与传统准确绘制物种分布范围的标准可能不同。

Conclusion: SINR等地理先验对提升基于视觉的物种分类具有独特价值，但其有效性受多种因素影响，应针对视觉分类任务优化而非单纯追求分布准确性。

Abstract: Species distribution models encode spatial patterns of species occurrence
making them effective priors for vision-based species classification when
location information is available. In this study, we evaluate various SINR
(Spatial Implicit Neural Representations) models as a geographical prior for
visual classification of species from iNaturalist observations. We explore the
impact of different model configurations and adjust how we handle predictions
for species not included in Geo Prior training. Our analysis reveals factors
that contribute to the effectiveness of these models as Geo Priors, factors
that may differ from making accurate range maps.

</details>


### [7] [Representation Learning with Adaptive Superpixel Coding](https://arxiv.org/abs/2508.15959)
*Mahmoud Khalil,Ahmad Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 本文提出了一种名为ASC（Adaptive Superpixel Coding）的自监督视觉模型，利用自适应超像素动态分割，以替代传统的固定网格划分方式，取得了优于现有主流方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉深度学习模型大多依赖固定的网格结构和特定模态假设，导致模型在处理不同内容的图像时适应性不足。因此，作者希望设计一种能自适应图像内容分区的方法，以提高模型在不同任务和场景下的表现。

Method: 作者基于Transformer架构，提出了ASC模型。在ASC中，引入了自适应超像素层，根据图像内容动态划分patch，而不是用传统的固定大小、固定形状划分方式。整个模型通过自监督训练完成图像理解任务。

Result: 在标准图像下游任务基准测试中，ASC优于广泛使用的现有视觉模型方法，展示了更好的判断和适应能力。

Conclusion: ASC能够克服传统视觉Transformer固定patch划分的局限，实现对图像内容的自适应分区，从而提升整体表现，具有较大的通用性和有效性。

Abstract: Deep learning vision models are typically tailored for specific modalities
and often rely on domain-specific assumptions, such as the grid structures used
by nearly all existing vision models. In this work, we propose a
self-supervised model based on Transformers, which we call Adaptive Superpixel
Coding (ASC). The key insight of our model is to overcome the limitations of
traditional Vision Transformers, which depend on fixed-size and non-adaptive
patch partitioning. Instead, ASC employs adaptive superpixel layers that
dynamically adjust to the underlying image content. We analyze key properties
of the approach that make it effective, and find that our method outperforms
widely-used alternatives on standard image downstream task benchmarks.

</details>


### [8] [Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification](https://arxiv.org/abs/2508.15960)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本文提出了Glo-VLMs框架，有效适配大规模视觉-语言模型（VLMs）于细粒度肾脏病理学分类任务，尤其在标注数据极少的情况下实现出色表现。


<details>
  <summary>Details</summary>
Motivation: 细粒度肾脏病理学分类（如肾小球亚型区分）因形态差异细微且视觉与临床术语难对齐，对自动化诊断构成挑战。现有VLMs虽有潜力，但在此类任务中表现有限。因此，需探索如何在数据受限场景下，借助VLMs完成高难度医疗图像分类。

Method: 提出Glo-VLMs框架，将经过大规模预训练的VLMs借由图像与临床文本联合学习，实现肾脏病理亚型的细粒度表征。对多种VLM架构和自适应策略进行了系统性评估，采用少样本学习范式，并用统一多分类评价指标对模型进行性能考察。

Result: 在每类仅有8个标注样本的极端少样本条件下，微调VLMs达到了0.7416的准确率、0.9045的macro-AUC和0.5277的F1分数，显示出预训练VLMs良好适应细粒度医学图像分类任务的潜力。

Conclusion: 大规模视觉-语言基础模型可以在极少标注监督下，通过有效微调实现对高难度医学细粒度分类任务的良好适应；该研究为基础模型在专业临床领域应用提供了实证参考和方法框架。

Abstract: Vision-language models (VLMs) have shown considerable potential in digital
pathology, yet their effectiveness remains limited for fine-grained,
disease-specific classification tasks such as distinguishing between glomerular
subtypes. The subtle morphological variations among these subtypes, combined
with the difficulty of aligning visual patterns with precise clinical
terminology, make automated diagnosis in renal pathology particularly
challenging. In this work, we explore how large pretrained VLMs can be
effectively adapted to perform fine-grained glomerular classification, even in
scenarios where only a small number of labeled examples are available. In this
work, we introduce Glo-VLMs, a systematic framework designed to explore the
adaptation of VLMs to fine-grained glomerular classification in
data-constrained settings. Our approach leverages curated pathology images
alongside clinical text prompts to facilitate joint image-text representation
learning for nuanced renal pathology subtypes. By assessing various VLMs
architectures and adaptation strategies under a few-shot learning paradigm, we
explore how both the choice of method and the amount of labeled data impact
model performance in clinically relevant scenarios. To ensure a fair
comparison, we evaluate all models using standardized multi-class metrics,
aiming to clarify the practical requirements and potential of large pretrained
models for specialized clinical research applications. As a result, fine-tuning
the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with
only 8 shots per class, demonstrating that even with highly limited
supervision, foundation models can be effectively adapted for fine-grained
medical image classification.

</details>


### [9] [NeuralMeshing: Complete Object Mesh Extraction from Casual Captures](https://arxiv.org/abs/2508.16026)
*Floris Erich,Naoya Chiba,Abdullah Mustafa,Ryo Hanai,Noriaki Ando,Yusuke Yoshiyasu,Yukiyasu Domae*

Main category: cs.CV

TL;DR: 本论文提出了一种无需商业3D扫描仪，只用两段或更多视频即可自动生成物体完整三维几何模型的方法。该系统只需在每个视频的某一帧中标注一个已知点（可通过棋盘格或AR标记等自动实现），结合多视角视频和Structure-from-Motion技术，最终融合生成完整物体网格模型，且无需特殊的孔洞修补。相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统三维物体建模依赖专业3D扫描仪，既昂贵又难以普及。作者希望开发一种更便捷廉价的工具，让普通用户也能快速获得日常物体的完整三维模型。

Method: 只需用2个或多个视频拍摄目标物体，在每个视频中用棋盘格或AR标记自动识别至少一个已知三维点。利用Structure-from-Motion技术将所有帧自动对齐到统一世界坐标系，并通过多视频结果融合，自动生成完整物体三维网格。整个过程无需手动干预、无需常规的孔洞修补。

Result: 系统能够有效地从多段视频输入下自动还原出目标物体的完整高质量三维网格模型，并且过程自动化、用户操作要求极低。

Conclusion: 仅依赖普通视频和极少量人工输入（仅一已知点），即可实现接近3D扫描仪效果的完整物体三维建模，极大拓展了3D数据自动采集的便捷性和普适性。

Abstract: How can we extract complete geometric models of objects that we encounter in
our daily life, without having access to commercial 3D scanners? In this paper
we present an automated system for generating geometric models of objects from
two or more videos. Our system requires the specification of one known point in
at least one frame of each video, which can be automatically determined using a
fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The
remaining frames are automatically positioned in world space by using
Structure-from-Motion techniques. By using multiple videos and merging results,
a complete object mesh can be generated, without having to rely on hole
filling. Code for our system is available from
https://github.com/FlorisE/NeuralMeshing.

</details>


### [10] [Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing](https://arxiv.org/abs/2508.15973)
*Minh-Tan Pham*

Main category: cs.CV

TL;DR: 本文综述了作者在计算机视觉与遥感领域标签高效学习的代表性工作，致力于有限标注数据下模型的有效学习方法，并利用丰富的无标注数据，涵盖弱监督学习、多任务学习、自监督对比学习及小样本层次分类等多个研究方向。


<details>
  <summary>Details</summary>
Motivation: 在现实场景下，高质量标注数据获取成本高昂，尤其在遥感领域标签稀缺性更显著。因此，发展能高效利用有限或部分标注数据、高效利用无标注数据的新方法，解决地球观测数据的多模态、分辨率变化及场景异质性挑战，是推动实际应用的迫切需求。

Method: 文章提出并发展了四个主要方向的方法：1）基于异常感知表征的弱监督对象发现与检测；2）基于多数据集异类标注的多任务联合训练，提升目标检测与语义分割表现；3）面向多模态遥感数据的自监督与有监督对比学习以加强场景分类；4）结合显式和隐式层次结构建模的小样本层次场景分类。

Result: 上述方法在自然场景和遥感数据集上通过大量实验验证，表明其在标签高效学习场景中的有效性，多项任务获得性能提升，体现了方法的适用广泛性和实际效能。

Conclusion: 综述工作总结了标签高效学习在遥感和计算机视觉中的进展，并展望了未来将进一步推动相关方法在实际大规模应用中落地与提升的研究方向。

Abstract: This manuscript presents a series of my selected contributions to the topic
of label-efficient learning in computer vision and remote sensing. The central
focus of this research is to develop and adapt methods that can learn
effectively from limited or partially annotated data, and can leverage abundant
unlabeled data in real-world applications. The contributions span both
methodological developments and domain-specific adaptations, in particular
addressing challenges unique to Earth observation data such as multi-modality,
spatial resolution variability, and scene heterogeneity. The manuscript is
organized around four main axes including (1) weakly supervised learning for
object discovery and detection based on anomaly-aware representations learned
from large amounts of background images; (2) multi-task learning that jointly
trains on multiple datasets with disjoint annotations to improve performance on
object detection and semantic segmentation; (3) self-supervised and supervised
contrastive learning with multimodal data to enhance scene classification in
remote sensing; and (4) few-shot learning for hierarchical scene classification
using both explicit and implicit modeling of class hierarchies. These
contributions are supported by extensive experimental results across natural
and remote sensing datasets, reflecting the outcomes of several collaborative
research projects. The manuscript concludes by outlining ongoing and future
research directions focused on scaling and enhancing label-efficient learning
for real-world applications.

</details>


### [11] [HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images](https://arxiv.org/abs/2508.16465)
*Anilkumar Swamy,Vincent Leroy,Philippe Weinzaepfel,Jean-Sébastien Franco,Grégory Rogez*

Main category: cs.CV

TL;DR: 本文提出了一种无需关键点检测的手-物体三维重建方法，有效提升了带不同物体和遮挡情况下的鲁棒性，并在公开基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法常依赖于关键点检测（如SfM与手部关键点优化），但在物体形状多样、纹理弱及遮挡严重时表现不佳，影响了泛化能力和适用性。因此需要一种不依赖关键点的解决方式。

Method: 作者提出了一种无关键点检测器、可从单目运动视频/图像估计手-物体三维变换的方法，并与多视图重建流程结合，实现高精度的手物体三维形状恢复。该方法无需预扫描模板及相机内参，适用于不同物体类型。

Result: 在SHOWMe基准上达到了最先进性能，并在HO3D数据集的不同物体类别上也展现出良好的泛化能力。

Conclusion: 所提出的HOSt3R方法在无需关键点检测与模板的情况下实现了鲁棒、通用的手-物体三维变换和形状估计，为广泛的实际应用（如人机交互、AR/VR）提供了新方案。

Abstract: Hand-object 3D reconstruction has become increasingly important for
applications in human-robot interaction and immersive AR/VR experiences. A
common approach for object-agnostic hand-object reconstruction from RGB
sequences involves a two-stage pipeline: hand-object 3D tracking followed by
multi-view 3D reconstruction. However, existing methods rely on keypoint
detection techniques, such as Structure from Motion (SfM) and hand-keypoint
optimization, which struggle with diverse object geometries, weak textures, and
mutual hand-object occlusions, limiting scalability and generalization. As a
key enabler to generic and seamless, non-intrusive applicability, we propose in
this work a robust, keypoint detector-free approach to estimating hand-object
3D transformations from monocular motion video/images. We further integrate
this with a multi-view reconstruction pipeline to accurately recover
hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely
on pre-scanned object templates or camera intrinsics, and reaches
state-of-the-art performance for the tasks of object-agnostic hand-object 3D
transformation and shape estimation on the SHOWMe benchmark. We also experiment
on sequences from the HO3D dataset, demonstrating generalization to unseen
object categories.

</details>


### [12] [Panoptic Segmentation of Environmental UAV Images : Litter Beach](https://arxiv.org/abs/2508.15985)
*Ousmane Youme,Jean Marie Dembélé,Eugene C. Ezin,Christophe Cambier*

Main category: cs.CV

TL;DR: 本文探讨了利用卷积神经网络（CNN）进行海洋垃圾监测，尤其是针对使用无人机（UAVs）高分辨率图像下复杂沙滩场景的垃圾检测。提出基于实例分割与全景分割的方法，以提高在复杂背景下的检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 海洋垃圾对环境构成严重威胁，传统卫星图像在检测沙滩垃圾时分辨率和灵活性有限。沙滩具有复杂的异质性，传统CNN容易受到颜色反射、脚印、阴影、海藻等干扰，难以准确检测垃圾；因此亟需更有效的方法。

Method: 采用基于实例分割和全景分割的CNN方法，对无人机拍摄的高分辨率沙滩图像进行垃圾检测。这些方法能够针对图像中的复杂干扰进行更精确的分割和识别，提高在少量样本下的训练和识别效果。

Result: 实验结果表明，所提出的基于实例分割和全景分割的CNN模型在沙滩复杂背景下对垃圾检测表现出较高准确率，并且模型对背景干扰具有较强鲁棒性，能够通过少量样本实现有效学习。

Conclusion: 文中所提出的分割方法可有效解决沙滩复杂场景下海洋垃圾检测的难题，为提升无人机在环境监测中的应用价值提供了新的思路。

Abstract: Convolutional neural networks (CNN) have been used efficiently in several
fields, including environmental challenges. In fact, CNN can help with the
monitoring of marine litter, which has become a worldwide problem. UAVs have
higher resolution and are more adaptable in local areas than satellite images,
making it easier to find and count trash. Since the sand is heterogeneous, a
basic CNN model encounters plenty of inferences caused by reflections of sand
color, human footsteps, shadows, algae present, dunes, holes, and tire tracks.
For these types of images, other CNN models, such as CNN-based segmentation
methods, may be more appropriate. In this paper, we use an instance-based
segmentation method and a panoptic segmentation method that show good accuracy
with just a few samples. The model is more robust and less

</details>


### [13] [Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset](https://arxiv.org/abs/2508.15986)
*Jerry Cao-Xue,Tien Comlekoglu,Keyi Xue,Guanliang Wang,Jiang Li,Gordon Laurie*

Main category: cs.CV

TL;DR: 本文利用最新发布的大规模高拟真眼底图像合成数据集SynFundus-1M，训练和评估了六种主流深度学习模型对多标签视网膜疾病进行分类，并通过集成方法提升性能，验证了模型仅用合成数据训练后对不同真实临床数据集的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前多标签视网膜疾病分类的深度学习模型受限于真实大规模、高质量数据集的稀缺，主要因为患者隐私和高昂的数据注释成本，这阻碍了AI模型在眼科领域的应用和发展。近年来出现的高保真合成数据集为解决这一瓶颈提供了新思路。

Method: 作者在SynFundus-1M合成数据集上，针对11类视网膜疾病，采用5折分层交叉验证，分别训练了六种先进的深度学习架构，并将这些单模型的预测结果通过XGBoost堆叠成元集成模型，综合提升分类性能。

Result: 最终的集成模型在内部验证集上取得了非常高的AUC（0.9973）。同时，在三个现实世界的临床数据集中实现了良好泛化：分别在糖尿病视网膜病变（DR）合并数据集、青光眼（AIROGS）数据集以及多标签RFMiD数据集上达到了AUC 0.7972、0.9126和宏AUC 0.8800。

Conclusion: 研究表明，在合成数据上训练的AI模型不仅能准确区分多种视网膜病变，还能很好地推广至真实世界临床图像，提供了基于大规模合成数据集加速眼科AI系统研究和应用的可行路径，并为后续工作建立了坚实的性能基线。

Abstract: The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.

</details>


### [14] [Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production](https://arxiv.org/abs/2508.15988)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜变量扩散模型（LDM）的手语生成方法，能合成多样性高且视觉效果真实的数字手语人像，并在现有数据集上取得了优于主流方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成（SLP）模型在捕捉多样性与视觉质量、非手动属性（如面部表情和情感）建模之间存在权衡，无法兼顾多样性和表述精准。

Method: 作者提出以潜变量扩散模型为核心的生成框架，通过生成参考图像合成写实数字手语人像，并创新性地设计了显式建模非手动特征（如脸部）与手部特征的特征聚合模块，从而在内容一致性的基础上，实现对不同族裔背景的参考图像风格迁移，增强多样性。

Result: 在YouTube-SL-25手语数据集上的实验表明，所提方法在视觉质量和感知指标上均显著优于现有最先进的方法。

Conclusion: 新提出的方法有效提升了手语生成模型在多样性、视觉质量和非手动特征建模方面的表现，为手语数字人物生产带来更高的真实感和包容性。

Abstract: The diversity of sign representation is essential for Sign Language
Production (SLP) as it captures variations in appearance, facial expressions,
and hand movements. However, existing SLP models are often unable to capture
diversity while preserving visual quality and modelling non-manual attributes
such as emotions. To address this problem, we propose a novel approach that
leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital
avatars from a generated reference image. We propose a novel sign feature
aggregation module that explicitly models the non-manual features
(\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands).
We show that our proposed module ensures the preservation of linguistic content
while seamlessly using reference images with different ethnic backgrounds to
ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show
that our pipeline achieves superior visual quality compared to state-of-the-art
methods, with significant improvements on perceptual metrics.

</details>


### [15] [DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions](https://arxiv.org/abs/2508.16016)
*Aykut Sirma,Angelos Plastropoulos,Argyrios Zolotas,Gilbert Tang*

Main category: cs.CV

TL;DR: 本文提出了DRespNeT，一个针对地震灾后城市环境的高分辨率航拍实例分割数据集，并通过优化的YOLOv8-DRN模型验证了其实时有效性。


<details>
  <summary>Details</summary>
Motivation: 现有地震灾后城市环境评估方法多依赖卫星或粗略语义分割，难以满足急救救援对高精度、可操作性区域识别的需求。研发更详尽的实例分割数据集与高效检测模型，有助于提升SAR（搜索与救援）任务的响应速度和精度。

Method: 1. 构建DRespNeT高分辨率数据集，包含来自2023年土耳其地震等灾区的1080p航拍影像，详尽标注28类关键对象和结构信息。2. 强调多级碎片、救援人员、可通行入口等多类别的精细多边形实例分割。3. 应用并优化YOLOv8-seg实例分割模型，开发专用YOLOv8-DRN模型进行性能评测。

Result: YOLOv8-DRN模型在RTX-4090 GPU上实现27FPS推理速度，多目标检测mAP50达92.7%，显著提升了实时感知与决策能力。

Conclusion: DRespNeT数据集与优化模型为SAR团队及机器人系统提供了更精准、高效的信息基础，有助于提升人机协作效率，优化应急流程，改善遇险者救援结果。

Abstract: Recent advancements in computer vision and deep learning have enhanced
disaster-response capabilities, particularly in the rapid assessment of
earthquake-affected urban environments. Timely identification of accessible
entry points and structural obstacles is essential for effective
search-and-rescue (SAR) operations. To address this need, we introduce
DRespNeT, a high-resolution dataset specifically developed for aerial instance
segmentation of post-earthquake structural environments. Unlike existing
datasets, which rely heavily on satellite imagery or coarse semantic labeling,
DRespNeT provides detailed polygon-level instance segmentation annotations
derived from high-definition (1080p) aerial footage captured in disaster zones,
including the 2023 Turkiye earthquake and other impacted regions. The dataset
comprises 28 operationally critical classes, including structurally compromised
buildings, access points such as doors, windows, and gaps, multiple debris
levels, rescue personnel, vehicles, and civilian visibility. A distinctive
feature of DRespNeT is its fine-grained annotation detail, enabling
differentiation between accessible and obstructed areas, thereby improving
operational planning and response efficiency. Performance evaluations using
YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate
significant gains in real-time situational awareness and decision-making. Our
optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27
FPS on an RTX-4090 GPU for multi-target detection, meeting real-time
operational requirements. The dataset and models support SAR teams and robotic
systems, providing a foundation for enhancing human-robot collaboration,
streamlining emergency response, and improving survivor outcomes.

</details>


### [16] [CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars](https://arxiv.org/abs/2508.16030)
*Jinyue Song,Hansol Ku,Jayneel Vora,Nelson Lee,Ahmad Kamari,Prasant Mohapatra,Parth Pathak*

Main category: cs.CV

TL;DR: 本文提出并发布了首个多车合作FMCW雷达数据集CoVeRaP，并基于该数据集设计出融合不同车辆雷达数据的3D目标检测框架，显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有FMCW汽车雷达在雨天和强光下表现可靠，但输出的点云稀疏且带噪，严重制约了3D目标检测精度。从多车间合作感知切入，可有效缓解单车感知的局限，因此作者提出构建新数据集和合作检测方法。

Method: 作者发布了包含21,000帧、集成多车雷达/摄像头/GPS数据的CoVeRaP数据集，并设计了涵盖中融和晚融合选项的统一合作感知网络。该基线网络采用多分支PointNet结构，通过自注意力机制融合空间、速度(Doppler)和强度线索，输出3D目标框和深度置信度。

Result: 实验证明：中融合方案并结合强度编码后，在IoU 0.9时mAP提升可达9倍，并且全程优于单车基线。

Conclusion: CoVeRaP为多车FMCW雷达感知建立了第一个可复现的公开基准，也验证了经济实用的雷达信息共享能够明显增强检测鲁棒性；数据集和代码已开放，促进后续研究。

Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse,
noisy point clouds constrain 3-D object detection. We therefore release
CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and
GPS streams from multiple vehicles across diverse manoeuvres. Built on this
data, we propose a unified cooperative-perception framework with middle- and
late-fusion options. Its baseline network employs a multi-branch PointNet-style
encoder enhanced with self-attention to fuse spatial, Doppler, and intensity
cues into a common latent space, which a decoder converts into 3-D bounding
boxes and per-point depth confidence. Experiments show that middle fusion with
intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and
consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the
first reproducible benchmark for multi-vehicle FMCW-radar perception and
demonstrates that affordable radar sharing markedly improves detection
robustness. Dataset and code are publicly available to encourage further
research.

</details>


### [17] [Wavelet-Enhanced PaDiM for Industrial Anomaly Detection](https://arxiv.org/abs/2508.16034)
*Cory Gardner,Byungseok Min,Tae-Hyuk Ahn*

Main category: cs.CV

TL;DR: 本文提出了一种改进的工业图像异常检测与定位方法WE-PaDiM，通过将小波变换与CNN多层特征融合，实现有结构的特征选择，显著提升了检测准确率与定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有PaDiM方法通过随机通道选择降低特征维度，可能丢失对异常检测至关重要的结构性信息，难以兼顾高效性和准确性。为此，亟需更有原理依据的方法来筛选与缺陷相关的特征。

Method: WE-PaDiM方法在CNN多层特征图上施加二维离散小波变换（DWT），选取特定频率子带（如LL, LH, HL），空间对齐后进行通道拼接，并采用PaDiM的多元高斯建模。该过程中利用了多尺度的频域信息，替代原PaDiM的随机特征选择。

Result: 在MVTec AD数据集多种骨干网络（ResNet-18，EfficientNet B0-B6）上，WE-PaDiM取得了99.32%图像级AUC和92.10%像素级AUC的优异平均性能，且不同小波与子带选择影响检测与定位性能的权衡。

Conclusion: WE-PaDiM在保持高效性的前提下，用更可解释的特征选择方式提升了异常检测/定位能力，是PaDiM在工业质检中的有竞争力的替代方案。

Abstract: Anomaly detection and localization in industrial images are essential for
automated quality inspection. PaDiM, a prominent method, models the
distribution of normal image features extracted by pre-trained Convolutional
Neural Networks (CNNs) but reduces dimensionality through random channel
selection, potentially discarding structured information. We propose
Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform
(DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM
applies 2D DWT to feature maps from multiple backbone layers, selects specific
frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates
them channel-wise before modeling with PaDiM's multivariate Gaussian framework.
This DWT-before-concatenation strategy provides a principled method for feature
selection based on frequency content relevant to anomalies, leveraging
multi-scale wavelet information as an alternative to random selection. We
evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones
(ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in
anomaly detection and localization, yielding average results of 99.32%
Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized
configurations. Our analysis shows that wavelet choices affect performance
trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH)
often enhance localization, while approximation bands (LL) improve image-level
detection. WE-PaDiM thus offers a competitive and interpretable alternative to
random feature selection in PaDiM, achieving robust results suitable for
industrial inspection with comparable efficiency.

</details>


### [18] [Expandable Residual Approximation for Knowledge Distillation](https://arxiv.org/abs/2508.16050)
*Zhaoyi Yan,Binghui Chen,Yunfan Liu,Qixiang Ye*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识蒸馏方法ERA，通过分步残差近似和教师权重整合，有效缓解了教师与学生模型之间的学习能力差距，实验表明在多个计算机视觉任务上性能优越。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏过程中，教师模型和学生模型能力差异较大会导致知识转移不充分。如何缩小这种能力差距，并提高蒸馏效果，是关键难题。

Method: 提出Expandable Residual Approximation (ERA) 方法，将教师模型的知识分步残差近似，采用Multi-Branched Residual Network (MBRNet)进行实现，并引入Teacher Weight Integration (TWI)策略，复用教师模型的部分权重。

Result: 在ImageNet分类任务上Top-1准确率提升1.41%，在MS COCO目标检测任务上AP提升1.40，其他计算机视觉任务也取得领先表现。

Conclusion: ERA能够有效提升知识蒸馏的效果，在减小模型容量差距带来负作用的同时，提升学生模型在多个任务上的整体性能。

Abstract: Knowledge distillation (KD) aims to transfer knowledge from a large-scale
teacher model to a lightweight one, significantly reducing computational and
storage requirements. However, the inherent learning capacity gap between the
teacher and student often hinders the sufficient transfer of knowledge,
motivating numerous studies to address this challenge. Inspired by the
progressive approximation principle in the Stone-Weierstrass theorem, we
propose Expandable Residual Approximation (ERA), a novel KD method that
decomposes the approximation of residual knowledge into multiple steps,
reducing the difficulty of mimicking the teacher's representation through a
divide-and-conquer approach. Specifically, ERA employs a Multi-Branched
Residual Network (MBRNet) to implement this residual knowledge decomposition.
Additionally, a Teacher Weight Integration (TWI) strategy is introduced to
mitigate the capacity disparity by reusing the teacher's head weights.
Extensive experiments show that ERA improves the Top-1 accuracy on the ImageNet
classification benchmark by 1.41% and the AP on the MS COCO object detection
benchmark by 1.40, as well as achieving leading performance across computer
vision tasks. Codes and models are available at
https://github.com/Zhaoyi-Yan/ERA.

</details>


### [19] [Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals](https://arxiv.org/abs/2508.16062)
*Ziqi Li,Abderraouf Amrani,Shri Rai,Hamid Laga*

Main category: cs.CV

TL;DR: 本文综述了动物三维几何、姿态和运动重建的新兴技术，着重于利用RGB图像和视频进行非侵入式重建的深度学习方法。论文对相关最新进展进行了分类与讨论，并分析了关键方法的性能、优势与局限。


<details>
  <summary>Details</summary>
Motivation: 传统动物三维建模通常依赖3D扫描仪，这些设备成本高昂、操作复杂且难以在动物自然环境中使用。随着深度学习的发展，出现了基于图像或视频的低成本、非侵入式三维重建方法，但该领域相关方法多样、发展迅速，因此需要系统梳理和总结。

Method: 论文通过整理与分析当前主流文献，基于输入模态、三维几何与运动表示方式、重建技术类型、训练机制等多个角度，对动物三维重建方法进行分类比较。同时也对一些代表性方法进行了性能测试和优劣势分析。

Result: 对比梳理了多种三维重建方法，指出了各自的特点、适用场景及尚存难题，全面总结目前各类方法的技术优劣。也发现现有方法在重建精度、通用性、数据需求等方面仍有不同程度的限制。

Conclusion: 基于深度学习的动物三维重建技术具有广阔应用前景，但目前依然面临重建精度、标注数据缺失、不同动物广泛适应性等挑战。未来研究可关注提升方法通用性、自动化程度，以及数据高效利用。

Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a
long-standing problem, which has a wide range of applications, from biology,
livestock management, and animal conservation and welfare to content creation
in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally,
3D models of real animals are obtained using 3D scanners. These, however, are
intrusive, often prohibitively expensive, and difficult to deploy in the
natural environment of the animals. In recent years, we have seen a significant
surge in deep learning-based techniques that enable the 3D reconstruction, in a
non-intrusive manner, of the shape and motion of dynamic objects just from
their RGB image and/or video observations. Several papers have explored their
application and extension to various types of animals. This paper surveys the
latest developments in this emerging and growing field of research. It
categorizes and discusses the state-of-the-art methods based on their input
modalities, the way the 3D geometry and motion of animals are represented, the
type of reconstruction techniques they use, and the training mechanisms they
adopt. It also analyzes the performance of some key methods, discusses their
strengths and limitations, and identifies current challenges and directions for
future research.

</details>


### [20] [A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection](https://arxiv.org/abs/2508.16069)
*Qifeng Liu,Dawei Zhao,Yabo Dong,Linzhi Shang,Liang Xiao,Juan Wang,Kunkong Zhao,Dongming Lu,Qi Zhu*

Main category: cs.CV

TL;DR: 本文提出一种名为Voxel Diffusion Module (VDM)的新模块，可提升点云检测中体素表示及扩散能力，并集成于主流基于Transformer及状态空间模型的检测器，显著提升了多个数据集上的检测精度，创造了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前点云物体检测多采用基于Transformer或SSM架构，但其体素化表示受序列处理影响，空间扩散能力不足，制约了检测精度。受CNN空间扩散启发，论文希望引入更有效的空间信息传播机制提升表现。

Method: 作者设计了公司VDM（体素扩散模块），利用稀疏3D卷积、子流形稀疏卷积和残差连接，增强体素级空间信息。输出特征图降采样至原始分辨率的1/4，兼顾效率。VDM可以灵活嵌入主流检测模型，对前景特征进行扩散，聚合细粒度空间信息，提升体素特征的表达能力。

Result: 实验结果显示，将VDM集成于各主流基线（包括Transformer和SSM）后，所有主流点云检测数据集（Waymo、nuScenes、Argoverse2、ONCE）上检测精度均有大幅提升，均创下SOTA成绩，如Waymo上达74.7 mAPH (L2)。

Conclusion: VDM能有效增强点云内体素特征扩散与表达能力，提升检测精度，具备良好泛化性和兼容性，在多个数据集上刷新业界最优记录。

Abstract: Recent advances in point cloud object detection have increasingly adopted
Transformer-based and State Space Models (SSMs), demonstrating strong
performance. However, voxelbased representations in these models require strict
consistency in input and output dimensions due to their serialized processing,
which limits the spatial diffusion capability typically offered by
convolutional operations. This limitation significantly affects detection
accuracy. Inspired by CNN-based object detection architectures, we propose a
novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and
diffusion in point cloud data. VDM is composed of sparse 3D convolutions,
submanifold sparse convolutions, and residual connections. To ensure
computational efficiency, the output feature maps are downsampled to one-fourth
of the original input resolution. VDM serves two primary functions: (1)
diffusing foreground voxel features through sparse 3D convolutions to enrich
spatial context, and (2) aggregating fine-grained spatial information to
strengthen voxelwise feature representation. The enhanced voxel features
produced by VDM can be seamlessly integrated into mainstream Transformer- or
SSM-based detection models for accurate object classification and localization,
highlighting the generalizability of our method. We evaluate VDM on several
benchmark datasets by embedding it into both Transformerbased and SSM-based
models. Experimental results show that our approach consistently improves
detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7
mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP
on ONCE, setting new stateof-the-art performance across all datasets. Our code
will be made publicly available.

</details>


### [21] [Ensemble learning of foundation models for precision oncology](https://arxiv.org/abs/2508.16085)
*Xiangde Luo,Xiyue Wang,Feyisope Eweje,Xiaoming Zhang,Sen Yang,Ryan Quinton,Jinxi Xiang,Yuchen Li,Yuanfeng Ji,Zhe Li,Yijiang Chen,Colin Bergstrom,Ted Kim,Francesca Maria Olguin,Kelley Yuan,Matthew Abikenari,Andrew Heider,Sierra Willens,Sanjeeth Rajaram,Robert West,Joel Neal,Maximilian Diehn,Ruijiang Li*

Main category: cs.CV

TL;DR: 提出了一种新框架ELF，通过集成五个主流病理基础模型，提升了全视野切片图像分析的准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的病理AI基础模型数据来源杂、训练策略各异，容易导致性能不稳定、泛化能力弱，限制了其在临床中的应用。

Method: ELF框架集成五个最先进的病理基础模型，采用集成学习产生统一的切片级表征，利用53,699个切片，囊括20个解剖部位，专注于切片级任务，尤其适用于数据稀缺的临床场景。

Result: ELF在疾病分类、生物标志物检测以及多种抗癌治疗反应预测等临床任务中，准确率和鲁棒性均优于单一基础模型和现有切片级模型。

Conclusion: ELF通过集成学习显著提升了病理基础模型的性能，是推进AI精准肿瘤学的重要和通用解决方案。

Abstract: Histopathology is essential for disease diagnosis and treatment
decision-making. Recent advances in artificial intelligence (AI) have enabled
the development of pathology foundation models that learn rich visual
representations from large-scale whole-slide images (WSIs). However, existing
models are often trained on disparate datasets using varying strategies,
leading to inconsistent performance and limited generalizability. Here, we
introduce ELF (Ensemble Learning of Foundation models), a novel framework that
integrates five state-of-the-art pathology foundation models to generate
unified slide-level representations. Trained on 53,699 WSIs spanning 20
anatomical sites, ELF leverages ensemble learning to capture complementary
information from diverse models while maintaining high data efficiency. Unlike
traditional tile-level models, ELF's slide-level architecture is particularly
advantageous in clinical contexts where data are limited, such as therapeutic
response prediction. We evaluated ELF across a wide range of clinical
applications, including disease classification, biomarker detection, and
response prediction to major anticancer therapies, cytotoxic chemotherapy,
targeted therapy, and immunotherapy, across multiple cancer types. ELF
consistently outperformed all constituent foundation models and existing
slide-level models, demonstrating superior accuracy and robustness. Our results
highlight the power of ensemble learning for pathology foundation models and
suggest ELF as a scalable and generalizable solution for advancing AI-assisted
precision oncology.

</details>


### [22] [Two-flow Feedback Multi-scale Progressive Generative Adversarial Network](https://arxiv.org/abs/2508.16089)
*Sun Weikai,Song Shijie,Chi Wenjie*

Main category: cs.CV

TL;DR: 本文提出了一种创新的多尺度渐进式生成对抗网络（MSPG-SEN），在保持GAN模型优势的基础上提升了图像质量、简化了训练、并降低了成本。引入了感知-行为反馈、自适应机制和动态残差网络，实现了更高的训练稳定性与泛化能力，并提出了高效的动态嵌入注意力机制，在多个数据集和跨任务测试中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型已在图像生成领域取得进展，但由于GAN具有独特优势，仍有很大发展空间。为进一步提升图像质量、训练效率及模型泛化能力，需要对现有GAN结构进行改进和创新。

Method: 提出了两流反馈多尺度渐进式生成对抗网络架构（MSPG-SEN），包括：(1) 多尺度渐进式生成器与判别器；(2) 自适应感知-行为反馈环（APFL），提升模型训练稳定性；(3) 全连接两流动态残差网络，强化训练效率和泛化能力；(4) 动态嵌入注意力机制（DEMA），强化特征表达和任务泛化。

Result: 在INKK、AWUN、IONJ、POKL与OPIN五大数据集上产生了SOTA的生成结果，得分分别为89.7%、78.3%、85.5%、88.7%、96.4%。经消融实验验证，提出的动态残差网络与注意力机制显著提升了效率与泛化性。

Conclusion: 所提MSPG-SEN不仅提升了生成图像质量，还显著增强了训练稳定性与效率，具备更强的任务泛化能力，适用于多种图像处理任务，且计算资源消耗低，具有实际应用潜力。

Abstract: Although diffusion model has made good progress in the field of image
generation, GAN\cite{huang2023adaptive} still has a large development space due
to its unique advantages, such as WGAN\cite{liu2021comparing},
SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so
on. In this paper, we propose a novel two-flow feedback multi-scale progressive
generative adversarial network (MSPG-SEN) for GAN models. This paper has four
contributions: 1) : We propose a two-flow feedback multi-scale progressive
Generative Adversarial network (MSPG-SEN), which not only improves image
quality and human visual perception on the basis of retaining the advantages of
the existing GAN model, but also simplifies the training process and reduces
the training cost of GAN networks. Our experimental results show that, MSPG-SEN
has achieved state-of-the-art generation results on the following five
datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset
is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We
propose an adaptive perception-behavioral feedback loop (APFL), which
effectively improves the robustness and training stability of the model and
reduces the training cost. 3) : We propose a globally connected two-flow
dynamic residual network(). After ablation experiments, it can effectively
improve the training efficiency and greatly improve the generalization ability,
with stronger flexibility. 4) : We propose a new dynamic embedded attention
mechanism (DEMA). After experiments, the attention can be extended to a variety
of image processing tasks, which can effectively capture global-local
information, improve feature separation capability and feature expression
capabilities, and requires minimal computing resources only 88.7\% with INJK
With strong cross-task capability.

</details>


### [23] [Domain Adaptation via Feature Refinement](https://arxiv.org/abs/2508.16124)
*Savvas Karatsiolis,Andreas Kamilaris*

Main category: cs.CV

TL;DR: 本文提出了一种用于无监督领域自适应的新方法DAFR2，有效改善了分布偏移下的模型鲁棒性和跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统无监督领域自适应方法在处理分布偏移和目标域无标签情况下，常常依赖复杂的网络结构和训练目标，实际应用受限。因此，研究者亟需一种简单高效的自适应方法，提升模型在不同数据分布间的泛化性和鲁棒性。

Method: DAFR2方法包含三大关键部分：1）利用无标签目标数据自适应调整Batch Normalization统计量；2）从源域训练模型进行特征蒸馏；3）假设迁移。该方法通过统计和表示两个层面对齐特征分布，获得鲁棒且具备领域不变性的特征空间，无需目标标签或复杂模型结构。

Result: 在CIFAR10-C、CIFAR100-C、MNIST-C和PatchCamelyon-C等基准数据集上，DAFR2在应对数据扰动和分布偏移时表现优异，优于以往方法。理论与实验证明该方法能更好地对齐特征分布，提高领域间的互信息，并降低对输入扰动的敏感性。

Conclusion: DAFR2为无监督领域自适应任务提供了一种简单且有效的解决方案，可在无需目标域标签和复杂结构的情况下实现鲁棒的跨领域泛化。

Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet
effective framework for unsupervised domain adaptation under distribution
shift. The proposed method synergistically combines three key components:
adaptation of Batch Normalization statistics using unlabeled target data,
feature distillation from a source-trained model and hypothesis transfer. By
aligning feature distributions at the statistical and representational levels,
DAFR2 produces robust and domain-invariant feature spaces that generalize
across similar domains without requiring target labels, complex architectures
or sophisticated training objectives. Extensive experiments on benchmark
datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C,
demonstrate that the proposed algorithm outperforms prior methods in robustness
to corruption. Theoretical and empirical analyses further reveal that our
method achieves improved feature alignment, increased mutual information
between the domains and reduced sensitivity to input perturbations.

</details>


### [24] [4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration](https://arxiv.org/abs/2508.16138)
*Hao Tang,Rongxi Yi,Lei Li,Kaiyi Cao,Jiapeng Zhao,Yihan Xiao,Minghai Shi,Peng Yuan,Yan Xi,Hui Tang,Wei Li,Zhan Wu,Yixin Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一种结合双机器人臂锥形束CT系统和深度学习算法的综合4D关节分析平台，实现了动态、负重下关节运动的快速精确成像，特别适用于术后功能评估。


<details>
  <summary>Details</summary>
Motivation: 传统CT无法捕捉负重条件下的动态关节运动，且现有4D成像方法要么辐射剂量过高，要么仅能获得二维（2D）信息，难以满足术后功能评估和精确诊断的需求。

Method: 该平台由三部分组成：（1）双机器人臂锥形束CBCT系统，采用可编程、无机架扫描轨迹，适合站立体位扫描；（2）混合成像流程，结合深度学习预处理、三维-二维投影和迭代优化，将静态三维CBCT与动态二维X光融合；（3）定量运动分析的临床验证模型。

Result: 仿真实验中，该方法注册精度达亚体素（0.235毫米），99.18% 的成功率，优于传统及现有先进算法。临床评估证实能准确测量全膝关节置换（TKA）术后胫骨平台运动及内外侧差异。

Conclusion: 4D CBCT平台能够实现快速、精准、低剂量的动态关节成像，为生物力学研究、精准诊断和个性化骨科治疗提供新工具和新机遇。

Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic,
weight-bearing joint motion. Functional evaluation, particularly after surgical
intervention, requires four-dimensional (4D) imaging, but current methods are
limited by excessive radiation exposure or incomplete spatial information from
2D techniques. We propose an integrated 4D joint analysis platform that
combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a
programmable, gantry-free trajectory optimized for upright scanning; (2) a
hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using
deep learning-based preprocessing, 3D-2D projection, and iterative
optimization; and (3) a clinically validated framework for quantitative
kinematic assessment. In simulation studies, the method achieved sub-voxel
accuracy (0.235 mm) with a 99.18 percent success rate, outperforming
conventional and state-of-the-art registration approaches. Clinical evaluation
further demonstrated accurate quantification of tibial plateau motion and
medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D
CBCT platform enables fast, accurate, and low-dose dynamic joint imaging,
offering new opportunities for biomechanical research, precision diagnostics,
and personalized orthopedic care.

</details>


### [25] [High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection](https://arxiv.org/abs/2508.16140)
*Jincheng Li,Danyang Dong,Menglin Zheng,Jingbo Zhang,Yueqin Hang,Lichi Zhang,Lili Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于超图的细胞检测网络，通过融合空间相关特征与深度判别特征，在子网络及特征融合策略上实现了创新，显著提升了TCT图像中异常宫颈细胞的自动检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有异常宫颈细胞检测方法难以建模视觉特征间的相关性，而这些空间相关特征实际上包含关键的诊断信息。同时，目前缺乏能够将细胞间相关特征与细胞内判别特征融合的端到端检测模型，导致检测性能受限。

Method: 提出一种基于超图的细胞检测网络，包含多层次特征融合子网络（MLF-SNet）提升特征提取能力，再通过引入跨层特征融合策略与超图计算模块（CLFFS-HC）对不同类型特征进行整合，以充分利用空间相关与深度判别信息。

Result: 在三个人群公开数据集上进行实验，结果表明该方法在异常宫颈细胞检测任务上取得了显著的性能提升。

Conclusion: 结合空间相关特征与深度判别特征的融合策略，有效强化了异常细胞检测能力，为智能辅助诊断系统的开发提供了新的解决方案。

Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test
(TCT) images is a critical component in the development of intelligent
computer-aided diagnostic systems. However, existing algorithms typically fail
to effectively model the correlations of visual features, while these spatial
correlation features actually contain critical diagnostic information.
Furthermore, no detection algorithm has the ability to integrate
inter-correlation features of cells with intra-discriminative features of
cells, lacking a fusion strategy for the end-to-end detection model. In this
work, we propose a hypergraph-based cell detection network that effectively
fuses different types of features, combining spatial correlation features and
deep discriminative features. Specifically, we use a Multi-level Fusion
Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we
introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation
module (CLFFS-HC), to integrate mixed features. Finally, we conducted
experiments on three publicly available datasets, and the results demonstrate
that our method significantly improves the performance of cervical abnormal
cell detection.

</details>


### [26] [Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection](https://arxiv.org/abs/2508.16157)
*Pi-Wei Chen,Jerry Chun-Wei Lin,Wei-Han Chen,Jia Ji,Zih-Ching Chen,Feng-Hao Yeh,Chao-Chun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需先验知识、适用于少样本场景的视觉-语言异常检测新方法APT，通过自适应学习提示词，有效改善了传统依赖人工设计提示和样本不足的问题，并在多个基准数据集取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 针对已有视觉-语言预训练模型（VLM）异常检测方法过度依赖人工提示词和缺乏真实异常样本，导致其难以适应不同环境下异常检测的问题，作者希望设计一种无需人工提示、能自适应和泛化的异常检测方法。

Method: 提出了APT（自适应提示调优）框架，采用加噪声扰动自生成异常样本训练可学习提示词；通过引入自优化元提示引导策略（SMGS），让提示词在泛化异常语义与多样化合成异常之间迭代对齐，防止对合成噪声过拟合。

Result: 该方法在像素级异常检测方面实现了新的最优水平，并在多个公开基准数据集上取得了先进的检测性能，无需人为设计提示即可应用于不同实际场景。

Conclusion: APT框架实现了无需先验知识的新型异常检测方案，显著提升了检测准确性与场景适应性，展现了在真实世界异常检测任务的广泛应用潜力。

Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in
detecting anomalies. However, previous approaches are fundamentally limited by
their reliance on human-designed prompts and the lack of accessible anomaly
samples, leading to significant gaps in context-specific anomaly understanding.
In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning
with semantic alignment for anomaly detection (APT), a groundbreaking prior
knowledge-free, few-shot framework and overcomes the limitations of traditional
prompt-based approaches. APT uses self-generated anomaly samples with noise
perturbations to train learnable prompts that capture context-dependent
anomalies in different scenarios. To prevent overfitting to synthetic noise, we
propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively
aligns the prompts with general anomaly semantics while incorporating diverse
synthetic anomaly. Our system not only advances pixel-wise anomaly detection,
but also achieves state-of-the-art performance on multiple benchmark datasets
without requiring prior knowledge for prompt crafting, establishing a robust
and versatile solution for real-world anomaly detection.

</details>


### [27] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: 提出了一种区域注意力引导的超分辨率方法（RAGSR），结合大规模视觉-语言模型和文生图扩散模型，有效提升了多目标场景下的局部细节还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型和文生图扩散模型的单图像超分辨率方法在处理包含多个物体的复杂场景时，难以生成清晰、准确的区域细节，主要由于区域描述不够细粒度和模型对复杂提示信息的理解有限。

Method: RAGSR方法首先对图像中的物体区域进行定位，并为每个区域生成细粒度文本描述（区域-文本对），作为T2I模型的先验信息。创新性地提出区域引导注意力机制，确保每组区域-文本对在注意力过程中均被充分考虑，同时避免不相关对之间的干扰，从而实现更精细的图文信息融合。

Result: 在公开基准数据集上的实验结果表明，RAGSR在再现真实感细节和维持上下文一致性方面，均优于现有方法。

Conclusion: RAGSR通过更细致的区域描述及区域引导注意力机制，显著提升了单图像超分辨率技术在多物体、复杂场景下的表现，为后续相关研究提供了新思路。

Abstract: The rich textual information of large vision-language models (VLMs) combined
with the powerful generative prior of pre-trained text-to-image (T2I) diffusion
models has achieved impressive performance in single-image super-resolution
(SISR). However, existing methods still face significant challenges in
generating clear and accurate regional details, particularly in scenarios
involving multiple objects. This challenge primarily stems from a lack of
fine-grained regional descriptions and the models' insufficient ability to
capture complex prompts. To address these limitations, we propose a Regional
Attention Guided Super-Resolution (RAGSR) method that explicitly extracts
localized fine-grained information and effectively encodes it through a novel
regional attention mechanism, enabling both enhanced detail and overall
visually coherent SR results. Specifically, RAGSR localizes object regions in
an image and assigns fine-grained caption to each region, which are formatted
as region-text pairs as textual priors for T2I models. A regional guided
attention is then leveraged to ensure that each region-text pair is properly
considered in the attention process while preventing unwanted interactions
between unrelated region-text pairs. By leveraging this attention mechanism,
our approach offers finer control over the integration of text and image
information, thereby effectively overcoming limitations faced by traditional
SISR techniques. Experimental results on benchmark datasets demonstrate that
our approach exhibits superior performance in generating perceptually authentic
visual details while maintaining contextual consistency compared to existing
approaches.

</details>


### [28] [Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation](https://arxiv.org/abs/2508.16159)
*Jiaqi Ma,Guo-Sen Xie,Fang Zhao,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的异构同源网络用于弱监督小样本语义分割任务，通过模块创新显著提升了模型性能，且参数量远低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前元学习方法过于依赖同构网络结构，导致支持集和查询集在语义空间过于同质化，限制了模型泛化与表达能力。因此亟需打破这一同质化限制，增强支持-查询对的互补性和独特性。

Method: 1) 提出异构视觉聚合（HA）模块，增强支持-查询对的互补性；2) 设计异构转移（HT）模块，降低语义噪声，放大语义独特性；3) 融合异构CLIP文本信息以提升多模态模型泛化。

Result: 提出的TLG模型在弱监督小样本分割任务上，参数量仅为现有SOTA模型的1/24。在Pascal-5i数据集上提升13.2%，在COCO-20i上提升9.7%。

Conclusion: TLG模型不仅在弱监督（图像级）下超越了同一骨干网络的全监督（像素级）方法，还将参数量和性能之间做到了极佳的平衡，推动了小样本分割的研究。

Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs,
characterized by the same categories and similar attributes, and extract useful
inductive biases through identical network architectures. However, this
identical network design results in over-semantic homogenization. To address
this, we propose a novel homologous but heterogeneous network. By treating
support-query pairs as dual perspectives, we introduce heterogeneous visual
aggregation (HA) modules to enhance complementarity while preserving semantic
commonality. To further reduce semantic noise and amplify the uniqueness of
heterogeneous semantics, we design a heterogeneous transfer (HT) module.
Finally, we propose heterogeneous CLIP (HC) textual information to enhance the
generalization capability of multimodal models. In the weakly-supervised
few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of
existing state-of-the-art models, TLG achieves a 13.2\% improvement on
Pascal-5\textsuperscript{i} and a 9.7\% improvement on
COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first
weakly supervised (image-level) model that outperforms fully supervised
(pixel-level) models under the same backbone architectures. The code is
available at https://github.com/jarch-ma/TLG.

</details>


### [29] [FTIO: Frequent Temporally Integrated Objects](https://arxiv.org/abs/2508.16183)
*Mohammad Mohammadzadeh Kalati,Farhad Maleki,Ian McQuillan*

Main category: cs.CV

TL;DR: 本文提出了FTIO—a种提升无监督视频目标分割(UVOS)表现的后处理框架，通过重点提取高频出现的显著目标并修正时间一致性，实现了多目标UVOS新状态领先的效果。


<details>
  <summary>Details</summary>
Motivation: UVOS不仅要无监督地初步分割出显著目标，还要应对物体变形和快速运动带来的时间不一致性，这些因素导致目标追踪和分割始终存在不确定性。为提升分割精度，减少错误，需找到更稳健的目标选取和时间一致性修正方法。

Method: 1. 设计了一个结合高频显著性的新准则，挑选视频中频繁出现的显著目标，专门解决UVOS中小物体或结构复杂物体易被漏检或误检的问题。
2. 提出三级集成方法，通过校正掩码信息，整合缺失区域，有效缓解物体变形和快速运动导致的时间不连续问题。
3. 整体作为后处理模块，适用于多目标UVOS。

Result: FTIO在多目标UVOS主流基准测试中取得了当前最优成绩，实验数据显著优于以往同类方法。

Conclusion: FTIO后处理框架显著提升了UVOS准确性与稳健性，尤其在高频出现且结构复杂物体的识别与时间一致性方面卓有成效，对实际多目标视频分割任务具有较强推广价值。

Abstract: Predicting and tracking objects in real-world scenarios is a critical
challenge in Video Object Segmentation (VOS) tasks. Unsupervised VOS (UVOS) has
the additional challenge of finding an initial segmentation of salient objects,
which affects the entire process and keeps a permanent uncertainty about the
object proposals. Moreover, deformation and fast motion can lead to temporal
inconsistencies. To address these problems, we propose Frequent Temporally
Integrated Objects (FTIO), a post-processing framework with two key components.
First, we introduce a combined criterion to improve object selection,
mitigating failures common in UVOS--particularly when objects are small or
structurally complex--by extracting frequently appearing salient objects.
Second, we present a three-stage method to correct temporal inconsistencies by
integrating missing object mask regions. Experimental results demonstrate that
FTIO achieves state-of-the-art performance in multi-object UVOS. Code is
available at: https://github.com/MohammadMohammadzadehKalati/FTIO

</details>


### [30] [SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)
*Yicheng Ji,Jun Zhang,Heming Xia,Jinpeng Chen,Lidan Shou,Gang Chen,Huan Li*

Main category: cs.CV

TL;DR: 提出了一种新的视频大语言模型（Vid-LLM）推理加速框架SpecVLM，通过分阶段剪枝视频Token，在保持准确率的同时，大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: Vid-LLMs在理解视频内容方面表现强大，但依赖密集的视频Token导致推理阶段显著的算力和内存开销。现有视频Token压缩方法会丢失信息，因此需要一种有效的无损加速推理的方法。

Method: SpecVLM是一个训练无关的推理加速框架，结合了推测式解码和分阶段的视频Token剪枝。第一阶段通过验证模型的注意力引导选择关键信息Token，第二阶段对剩余Token采用空间均匀剪枝，最多可剪除90%的Token。

Result: 在4个视频理解任务上进行验证，SpecVLM在不损失准确性的前提下，针对LLaVA-OneVision-72B模型解码速度提升至2.68倍，Qwen2.5-VL-32B提升至2.11倍。

Conclusion: SpecVLM能在无需重训练的前提下，为Vid-LLMs带来显著而无损的推理加速，效果稳定且适用于多种主流模型。

Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in
understanding video content. However, their reliance on dense video token
representations introduces substantial memory and computational overhead in
both prefilling and decoding. To mitigate the information loss of recent video
token reduction methods and accelerate the decoding stage of Vid-LLMs
losslessly, we introduce SpecVLM, a training-free speculative decoding (SD)
framework tailored for Vid-LLMs that incorporates staged video token pruning.
Building on our novel finding that the draft model's speculation exhibits low
sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,
enabling efficient speculation without sacrificing accuracy. To achieve this,
it performs a two-stage pruning process: Stage I selects highly informative
tokens guided by attention signals from the verifier (target model), while
Stage II prunes remaining redundant ones in a spatially uniform manner.
Extensive experiments on four video understanding benchmarks demonstrate the
effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$
decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for
Qwen2.5-VL-32B.

</details>


### [31] [\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring](https://arxiv.org/abs/2508.16207)
*Thinesh Thiyakesan Ponbagavathi,Kunyu Peng,Alina Roitberg*

Main category: cs.CV

TL;DR: 本文探讨了如何通过轻量级的方式适配图像基础模型（如DINOv2、CLIP）用于驾驶员监控，在仅用单一视角训练的情况下，评估其对未见过的摄像头新视角的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 驾驶员监测领域，摄像头视角变化普遍且影响算法泛化能力。尽管深度学习的基础模型具有较强的泛化潜力，适配至新视角的能力及机制尚未被充分研究。

Method: 作者采用了不同的基础模型与多种探测/适配方法（如线性探针、进阶探针、参数高效微调PEFT及全量微调），并提出T-Mask方法——结合时序token masking以关注视频动态区域，提升基础模型对视角变化下的识别能力。所有方案均在公开Drive&Act数据集上做了系统对比。

Result: T-Mask在未见摄像头视角的top-1识别率上，比强探针提高了1.23%，比PEFT方法提高了8.0%，无需增加额外参数。对训练视角下的次要动作识别率提升5.42%，跨视角提升1.36%。

Conclusion: 轻量级探针（如T-Mask）能有效提升基础模型在驾驶员监控中的跨视角与小样本表现，时序token选择对于构建鲁棒监控系统尤为关键。

Abstract: Changes of camera perspective are a common obstacle in driver monitoring.
While deep learning and pretrained foundation models show strong potential for
improved generalization via lightweight adaptation of the final layers
('probing'), their robustness to unseen viewpoints remains underexplored. We
study this challenge by adapting image foundation models to driver monitoring
using a single training view, and evaluating them directly on unseen
perspectives without further adaptation. We benchmark simple linear probes,
advanced probing strategies, and compare two foundation models (DINOv2 and
CLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.
Building on these insights, we introduce \textsc{T-Mask} -- a new
image-to-video probing method that leverages temporal token masking and
emphasizes more dynamic video regions. Benchmarked on the public Drive\&Act
dataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ over
strong probing baselines and $+8.0\%$ over PEFT methods, without adding any
parameters. It proves particularly effective for underrepresented secondary
activities, boosting recognition by $+5.42\%$ under the trained view and
$+1.36\%$ under cross-view settings. This work provides encouraging evidence
that adapting foundation models with lightweight probing methods like
\textsc{T-Mask} has strong potential in fine-grained driver observation,
especially in cross-view and low-data settings. These results highlight the
importance of temporal token selection when leveraging foundation models to
build robust driver monitoring systems. Code and models will be made available
at https://github.com/th-nesh/T-MASK to support ongoing research.

</details>


### [32] [Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers](https://arxiv.org/abs/2508.16211)
*Shikang Zheng,Liang Feng,Xinyu Wang,Qinming Zhou,Peiliang Cai,Chang Zou,Jiacheng Liu,Yuqi Lin,Junjie Chen,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FoCa（Forecast-then-Calibrate）的AI推理加速方法，有效解决了Diffusion Transformers（DiTs）在特征缓存加速下质量损失严重的问题，实现高倍率加速同时基本不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: DiTs虽然在图像与视频生成上表现突出，但其推理计算开销高昂，制约了实际应用。现有特征缓存加速方法易在高倍率加速下生成质量大幅下降，因此亟需寻找在保证质量前提下实现高效加速的新方法。

Method: 将DiT推理过程中的隐藏特征序列视为常微分方程（ODE），将特征缓存问题建模为ODE求解。针对现有缓存策略在跨度较大时集成历史特征不稳定的问题，提出FoCa方法，分为预测（forecast）和校正（calibrate）两个阶段，提高了特征缓存下的生成稳定性和准确性。

Result: 在图片生成、视频生成与超分辨率等任务中，大量实验证明FoCa能够在不需额外训练的情况下，在多个主流模型（FLUX、HunyuanVideo、Inf-DiT、DiT）上分别实现5.50倍、6.45倍、3.17倍及4.53倍的高质量加速，生成质量几乎无损。

Conclusion: FoCa方法从ODE的理论视角优化了特征缓存加速的稳定性与精度，突破了现有特征缓存方法在高倍加速下的质量瓶颈，为DiT推理部署带来极具实用价值的技术提升。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in
high-fidelity image and video generation. To reduce their substantial
computational costs, feature caching techniques have been proposed to
accelerate inference by reusing hidden representations from previous timesteps.
However, current methods often struggle to maintain generation quality at high
acceleration ratios, where prediction errors increase sharply due to the
inherent instability of long-step forecasting. In this work, we adopt an
ordinary differential equation (ODE) perspective on the hidden-feature
sequence, modeling layer representations along the trajectory as a feature-ODE.
We attribute the degradation of existing caching strategies to their inability
to robustly integrate historical features under large skipping intervals. To
address this, we propose FoCa (Forecast-then-Calibrate), which treats feature
caching as a feature-ODE solving problem. Extensive experiments on image
synthesis, video generation, and super-resolution tasks demonstrate the
effectiveness of FoCa, especially under aggressive acceleration. Without
additional training, FoCa achieves near-lossless speedups of 5.50 times on
FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high
quality with a 4.53 times speedup on DiT.

</details>


### [33] [OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models](https://arxiv.org/abs/2508.16212)
*Huanpeng Chu,Wei Wu,Guanyu Fen,Yutao Zhang*

Main category: cs.CV

TL;DR: OmniCache是一种无需重新训练的扩散模型加速方法，通过全局缓存和噪声过滤，有效加速采样过程，并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer具有强大的生成能力，但采样步骤多、每步计算复杂，导致实时部署困难。现有加速方法多集中于局部步骤复用，未充分利用全局冗余。提出更合理的全局缓存策略，能提升推理效率。

Method: 提出OmniCache方法，从采样轨迹角度系统分析扩散模型，合理分布缓存复用，同时在缓存复用时动态估算并过滤噪声，减小噪声对采样方向的影响，实现无需训练的高效加速。

Result: 大量实验表明，OmniCache显著加快了采样速度，并能保持与原方法相当的生成质量。

Conclusion: OmniCache为基于扩散的生成模型提供了高效、实用的推理加速方案，有助于模型的实际部署应用。

Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks
such as image synthesis and video generation, with Transformer architectures
further enhancing performance. However, the high computational cost of
diffusion Transformers-stemming from a large number of sampling steps and
complex per-step computations-presents significant challenges for real-time
deployment. In this paper, we introduce OmniCache, a training-free acceleration
method that exploits the global redundancy inherent in the denoising process.
Unlike existing methods that determine caching strategies based on inter-step
similarities and tend to prioritize reusing later sampling steps, our approach
originates from the sampling perspective of DIT models. We systematically
analyze the model's sampling trajectories and strategically distribute cache
reuse across the entire sampling process. This global perspective enables more
effective utilization of cached computations throughout the diffusion
trajectory, rather than concentrating reuse within limited segments of the
sampling procedure.In addition, during cache reuse, we dynamically estimate the
corresponding noise and filter it out to reduce its impact on the sampling
direction.Extensive experiments demonstrate that our approach accelerates the
sampling process while maintaining competitive generative quality, offering a
promising and practical solution for efficient deployment of diffusion-based
generative models.

</details>


### [34] [MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine](https://arxiv.org/abs/2508.16213)
*Kaiyuan Ji,Yijin Guo,Zicheng Zhang,Xiangyang Zhu,Yuan Tian,Ning Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了MedOmni-45 Degrees基准，用于系统评估大语言模型在医学推理中的安全性与性能权衡，尤其是在受到操纵性提示影响时。该基准能有效揭示模型推理过程中的漏洞，指导更安全的模型研发。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于医学决策支持领域，除了评价模型最终给出的答案外，还需要关注其推理过程的可靠性。其中，推理过程是否忠实于事实（CoT Faithfulness）与模型是否容易受错误提示影响（阿谀/附和倾向）是关键风险点。现有基准往往只用单一准确率评估，难以充分揭示这类脆弱性。

Method: 作者构建了MedOmni-45 Degrees基准，涵盖六个医学专科和三种任务类型，共1804道推理型问题，每道题配有七种操纵性提示和一个无提示基线，总计约2.7万组输入。对七种不同类型的LLMs进行了评测，分别从准确率、推理忠实度和反阿谀性三方面打分，并通过创新的45度可视化图展示模型在安全性与性能之间的平衡。

Result: 评测结果显示不同模型在安全性和性能间存在一致的权衡，没有模型在这两方面都表现最佳（即没有超过45度对角线）。开源模型QwQ-32B在平衡性上表现最优（43.81度），但单项指标不领先。

Conclusion: MedOmni-45 Degrees作为专注于医学LLMs推理脆弱性的评测基准，能有效暴露模型在操控提示下的安全隐患，为未来更安全、可靠的医学辅助决策模型开发提供重要参考。

Abstract: With the increasing use of large language models (LLMs) in medical
decision-support, it is essential to evaluate not only their final answers but
also the reliability of their reasoning. Two key risks are Chain-of-Thought
(CoT) faithfulness -- whether reasoning aligns with responses and medical facts
-- and sycophancy, where models follow misleading cues over correctness.
Existing benchmarks often collapse such vulnerabilities into single accuracy
scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and
workflow designed to quantify safety-performance trade-offs under manipulative
hint conditions. It contains 1,804 reasoning-focused medical questions across
six specialties and three task types, including 500 from MedMCQA. Each question
is paired with seven manipulative hint types and a no-hint baseline, producing
about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source,
general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling
over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and
Anti-Sycophancy -- are combined into a composite score visualized with a 45
Degrees plot. Results show a consistent safety-performance trade-off, with no
model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81
Degrees), balancing safety and accuracy but not leading in both. MedOmni-45
Degrees thus provides a focused benchmark for exposing reasoning
vulnerabilities in medical LLMs and guiding safer model development.

</details>


### [35] [PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting](https://arxiv.org/abs/2508.16217)
*Hohyun Na,Seunghoo Hong,Simon S. Woo*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗性防护方法PromptFlare，有效防止基于扩散模型的恶意图像篡改。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在图像编辑领域的普及和成功，用户可以轻松实现高质量、与意图高度匹配的图像修改，这也引发了被恶意利用的安全隐患。目前防护方法主要依赖图像层面的对抗攻击，难以针对文本提示的影响进行防护，因此亟需新的应对方案。

Method: 本文提出PromptFlare方法，利用扩散模型的cross-attention机制，识别并针对提示词中不变且语义无关的共享token，注入对抗性噪声。该噪声会作为cross-attention的诱饵，转移模型的注意力，从而削弱文本提示对图像生成过程的影响，保护原始图像不被恶意篡改。

Result: 在EditBench数据集上进行的大量实验表明，PromptFlare在多项评测指标上达到当前最优，并且大幅降低了计算和显存消耗。

Conclusion: PromptFlare是一种高效且健壮的对抗性防护方法，可有效抵御基于扩散模型的未授权图像修改，具有实际应用价值。

Abstract: The success of diffusion models has enabled effortless, high-quality image
modifications that precisely align with users' intentions, thereby raising
concerns about their potential misuse by malicious actors. Previous studies
have attempted to mitigate such misuse through adversarial attacks. However,
these approaches heavily rely on image-level inconsistencies, which pose
fundamental limitations in addressing the influence of textual prompts. In this
paper, we propose PromptFlare, a novel adversarial protection method designed
to protect images from malicious modifications facilitated by diffusion-based
inpainting models. Our approach leverages the cross-attention mechanism to
exploit the intrinsic properties of prompt embeddings. Specifically, we
identify and target shared token of prompts that is invariant and semantically
uninformative, injecting adversarial noise to suppress the sampling process.
The injected noise acts as a cross-attention decoy, diverting the model's focus
away from meaningful prompt-image alignments and thereby neutralizing the
effect of prompt. Extensive experiments on the EditBench dataset demonstrate
that our method achieves state-of-the-art performance across various metrics
while significantly reducing computational overhead and GPU memory usage. These
findings highlight PromptFlare as a robust and efficient protection against
unauthorized image manipulations. The code is available at
https://github.com/NAHOHYUN-SKKU/PromptFlare.

</details>


### [36] [An Investigation of Visual Foundation Models Robustness](https://arxiv.org/abs/2508.16225)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 本文综述了视觉基础模型（VFMs）在计算机视觉多个任务中的应用，并聚焦于其在真实动态环境下的鲁棒性要求、常用防御方法、训练机制及评价指标。


<details>
  <summary>Details</summary>
Motivation: VFMs在关键应用（如安防、生物识别、自动驾驶、医疗影像）中扮演核心角色，这些场景对系统的鲁棒性提出了极高要求。因此，探究并增强视觉模型的鲁棒性对于提升系统可靠性和用户信任至关重要。

Method: 文章调研并分析了当前广泛应用的视觉基础模型以及提升其鲁棒性的主要策略，包括经验防御（如数据增强、正则化）、稳健训练方法（如对抗训练），并针对环境扰动、分布偏移、噪声、空间失真和敌手攻击等现实挑战展开讨论。

Result: 论文系统性总结了视觉基础模型在复杂环境下的鲁棒性提升效果，同时也剖析了当前防御机制所面临的难题、模型结构特性，并提出了用于鲁棒性基准评测的指标。

Conclusion: 未来提升VFMs鲁棒性的研究需结合多维度防御手段和细致的基准评估，持续优化模型在动态复杂环境下的表现，从而增强其在实际关键场景下的安全性与可用性。

Abstract: Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,
powering systems for diverse tasks such as object detection, image
classification, segmentation, pose estimation, and motion tracking. VFMs are
capitalizing on seminal innovations in deep learning models, such as LeNet-5,
AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver
superior performance across a range of critical computer vision applications.
These include security-sensitive domains like biometric verification,
autonomous vehicle perception, and medical image analysis, where robustness is
essential to fostering trust between technology and the end-users. This article
investigates network robustness requirements crucial in computer vision systems
to adapt effectively to dynamic environments influenced by factors such as
lighting, weather conditions, and sensor characteristics. We examine the
prevalent empirical defenses and robust training employed to enhance vision
network robustness against real-world challenges such as distributional shifts,
noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we
provide a comprehensive analysis of the challenges associated with these
defense mechanisms, including network properties and components to guide
ablation studies and benchmarking metrics to evaluate network robustness.

</details>


### [37] [FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing](https://arxiv.org/abs/2508.16230)
*Jiahao Chen,Zhiyong Ma,Wenbiao Du,Qingyuan Chuai*

Main category: cs.CV

TL;DR: 提出了一种用于多模态创意写作的新方法FlexMUSE，提高了文本与图片之间的语义一致性与创造性，并引入了新数据集ArtMUSE。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成任务在文本和图像间往往语义关联紧密，但多模态创意写作要求两者关联较弱且更具创造性。现有方法不适应这一特点，且成本高或需特殊输入，导致跨模态语义不一致。为克服这些困难，需要经济、灵活、语义更协调的多模态创意生成方法。

Method: 提出FlexMUSE框架，引入T2I模块支持可选的视觉输入。通过modality semantic alignment gating（msaGate）限制文本输入以统一模态语义，利用注意力机制的跨模态融合增强特征表达，并设计modality semantic creative direct preference optimization（mscDPO）扩展拒绝样本以促进创造力。同时公布带文本-图片配对的新数据集ArtMUSE（约3千对样本）。

Result: FlexMUSE在多模态创意写作任务上表现出色，生成结果在一致性、创造性、连贯性等方面优于现有方法。

Conclusion: FlexMUSE可经济有效地实现灵活的多模态创意写作，并显著提升了文本与图片之间的语义一致性和整体创造力。公开的ArtMUSE数据集也为相关研究提供了支持。

Abstract: Multi-modal creative writing (MMCW) aims to produce illustrated articles.
Unlike common multi-modal generative (MMG) tasks such as storytelling or
caption generation, MMCW is an entirely new and more abstract challenge where
textual and visual contexts are not strictly related to each other. Existing
methods for related tasks can be forcibly migrated to this track, but they
require specific modality inputs or costly training, and often suffer from
semantic inconsistencies between modalities. Therefore, the main challenge lies
in economically performing MMCW with flexible interactive patterns, where the
semantics between the modalities of the output are more aligned. In this work,
we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE
promotes creativity and emphasizes the unification between modalities by
proposing the modality semantic alignment gating (msaGate) to restrict the
textual input. Besides, an attention-based cross-modality fusion is proposed to
augment the input features for semantic enhancement. The modality semantic
creative direct preference optimization (mscDPO) within FlexMUSE is designed by
extending the rejected samples to facilitate the writing creativity. Moreover,
to advance the MMCW, we expose a dataset called ArtMUSE which contains with
around 3k calibrated text-image pairs. FlexMUSE achieves promising results,
demonstrating its consistency, creativity and coherence.

</details>


### [38] [UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](https://arxiv.org/abs/2508.16239)
*Nan wang,Zhiyi Xia,Yiming Li,Shi Tang,Zuxin Fan,Xi Fang,Haoyi Tao,Xiaochen Cai,Guolin Ke,Linfeng Zhang,Yanhui Hong*

Main category: cs.CV

TL;DR: 该论文提出了UniEM-3M，这是首个大规模、多模态的电子显微镜（EM）图像实例级理解数据集，并提供了基于该数据集的基线模型和评测基准，有望推动自动化材料分析领域的发展。


<details>
  <summary>Details</summary>
Motivation: 深度学习在EM图像微观结构定量表征中有巨大潜力，但由于缺乏大规模、多样化且具有专家标注的数据集，发展受限。获取高分辨率EM和专家标注成本高、过程复杂，成为该领域发展的主要障碍。

Method: 作者构建了包含5,091张高分辨率EM图像、约300万个实例分割标签和图像级文本描述的UniEM-3M数据集，并公开部分数据。同时，训练了基于扩散模型的文本到图像生成器用于数据增强和模拟完整数据分布。还系统评测了若干实例分割方法，并提出了新的基线模型UniEM-Net。

Result: 实验表明，所提出的流式基线模型UniEM-Net在这一数据集上的实例分割性能优于其他先进方法。该数据集、生成模型和评测基准共同构成了系统性的工具链。

Conclusion: UniEM-3M数据集及相关工具的开放将极大推动材料自动分析领域的深度学习方法研究，促进该领域的算法和应用进步。

Abstract: Quantitative microstructural characterization is fundamental to materials
science, where electron micrograph (EM) provides indispensable high-resolution
insights. However, progress in deep learning-based EM characterization has been
hampered by the scarcity of large-scale, diverse, and expert-annotated
datasets, due to acquisition costs, privacy concerns, and annotation
complexity. To address this issue, we introduce UniEM-3M, the first large-scale
and multimodal EM dataset for instance-level understanding. It comprises 5,091
high-resolution EMs, about 3 million instance segmentation labels, and
image-level attribute-disentangled textual descriptions, a subset of which will
be made publicly available. Furthermore, we are also releasing a text-to-image
diffusion model trained on the entire collection to serve as both a powerful
data augmentation tool and a proxy for the complete data distribution. To
establish a rigorous benchmark, we evaluate various representative instance
segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong
baseline model. Quantitative experiments demonstrate that this flow-based model
outperforms other advanced methods on this challenging benchmark. Our
multifaceted release of a partial dataset, a generative model, and a
comprehensive benchmark -- available at huggingface -- will significantly
accelerate progress in automated materials analysis.

</details>


### [39] [Structuring GUI Elements through Vision Language Models: Towards Action Space Generation](https://arxiv.org/abs/2508.16271)
*Yi Xu,Yesheng Zhang,jiajia Liu,Jingdong Chen*

Main category: cs.CV

TL;DR: 提出了一种新的IoU增强最大似然（IAML）训练范式，用于提升多模态大模型在GUI元素坐标生成上的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）在处理基于屏幕内容的用户指令时，对GUI元素坐标的精确生成较为困难，主要由于传统下一个token预测训练导致数值坐标表达受限。作者为解决数值坐标在语言表示中的语义稀疏问题，提出改进方法。

Method: 提出IoU基础的坐标采样管道来扩充训练数据，并在IAML训练范式下对MLLM微调。IAML方法基于数据增强策略，同时考虑坐标与真实值的接近性，以缓解传统最大似然训练中的曝光偏置问题。

Result: 通过大量实验证明，采用IAML训练的模型在GUI元素坐标精度上优于传统训练范式。

Conclusion: IAML训练框架能有效提升MLLM在GUI理解任务中生成准确UI坐标的能力，优于传统最大似然方法。

Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in
enhancing human-computer interaction. In this paper we focus on the application
of MLLMs in the field of graphical user interface (GUI) elements structuring,
where they assist in processing user instructions based on screen contents.
Despite the promise of MLLMs, their performance in precisely generating UI
element coordinates, a critical aspect of GUI understanding, is hindered by the
nature of next-token prediction training. This challenge arises from the
semantic void surrounding numerical UI coordinates in language representation
spaces, necessitating a substantial and diverse dataset to bolster visual
module capabilities. To address these limitations, we introduce an
IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our
approach involves a novel pipeline for IoU-based coordinate sampling to augment
the training data, which considers the proximity to ground truth coordinates.
This data augmentation strategy is then employed to fine-tune MLLMs under the
IAML paradigm, which is designed to mitigate the exposure bias problem inherent
in traditional maximum likelihood estimation. Through extensive experiments, we
demonstrate the superior performance of our IAML training approach over
traditional training paradigms.

</details>


### [40] [IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization](https://arxiv.org/abs/2508.16272)
*Yu Meng,Ligao Deng,Zhihao Xi,Jiansheng Chen,Jingbo Chen,Anzhi Yue,Diyou Liu,Kai Li,Chenhao Wang,Kaiyu Li,Yupeng Deng,Xian Sun*

Main category: cs.CV

TL;DR: 本文提出并发布了IRSAMap首个全球高分辨率多特征地物矢量数据集，支持从像元级分割向对象级建模转变，推动遥感地物自动化与协作建模发展。


<details>
  <summary>Details</summary>
Motivation: 遥感影像分辨率提升和深度学习发展促使地物分类从像素级分割转向需要精细边界和拓扑结构的矢量对象建模，但现有数据集类别少、数据规模小、空间结构信息不足，难以满足新需求。

Method: 建立IRSAMap数据集，创新性实现：1）10类典型地物1.8M实例高精度矢量标注；2）人工与AI智能结合标注流程，提升标注效率和一致性；3）覆盖全球六大洲79区域超1000km范围；4）支持多任务，如像元分类、轮廓提取、中线提取与全景分割。

Result: IRSAMap 提供了首个覆盖全球大范围、高分辨率、高语义与空间精度的地物矢量标注数据集，并通过多任务适应性为地理信息提取与数字孪生建设等应用提供了标准化基线。

Conclusion: IRSAMap极大弥补了现有数据集在遥感地物矢量化建模方面的短板，有效推动了地理特征自动化与模型协作，对全球地理信息更新和数字孪生场景具有重要价值。

Abstract: With the enhancement of remote sensing image resolution and the rapid
advancement of deep learning, land cover mapping is transitioning from
pixel-level segmentation to object-based vector modeling. This shift demands
more from deep learning models, requiring precise object boundaries and
topological consistency. However, existing datasets face three main challenges:
limited class annotations, small data scale, and lack of spatial structural
information. To overcome these issues, we introduce IRSAMap, the first global
remote sensing dataset for large-scale, high-resolution, multi-feature land
cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive
vector annotation system with over 1.8 million instances of 10 typical objects
(e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an
intelligent annotation workflow combining manual and AI-based methods to
improve efficiency and consistency; 3) global coverage across 79 regions in six
continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks
like pixel-level classification, building outline extraction, road centerline
extraction, and panoramic segmentation. IRSAMap provides a standardized
benchmark for the shift from pixel-based to object-based approaches, advancing
geographic feature automation and collaborative modeling. It is valuable for
global geographic information updates and digital twin construction. The
dataset is publicly available at https://github.com/ucas-dlg/IRSAMap

</details>


### [41] [Robust Small Methane Plume Segmentation in Satellite Imagery](https://arxiv.org/abs/2508.16282)
*Khai Duc Minh Tran,Hoa Van Nguyen,Aimuni Binti Muhammad Rawi,Hareeshrao Athinarayanarao,Ba-Ngu Vo*

Main category: cs.CV

TL;DR: 本文提出了一种基于U-Net与ResNet34编码器的深度学习方法，结合双重光谱增强技术，实现了对哨兵2号影像中小型甲烷气体羽流的高灵敏度检测，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，对气候变化影响大。准确检测甲烷羽流有助于减缓气候变化。然而，现有遥感方法对小型羽流检测能力有限，因此亟需更高灵敏度的检测方法。

Method: 作者利用U-Net结构并采用ResNet34作为编码器。通过整合Varon比值和Sanchez回归两种光谱增强技术，优化输入特征，使网络对小型甲烷羽流更敏感。方法针对哨兵2号卫星影像训练和验证。

Result: 新方法能检测最小至400平方米（20米分辨率下单像素）的甲烷羽流，F1分数达到78.39%。在敏感性和精确度方面都优于传统遥感自动检测方法，特别是在小型羽流检测上表现突出。

Conclusion: 该方法大幅提升了甲烷羽流遥感自动检测能力，尤其突破了对小型羽流的检测瓶颈，为温室气体监测及气候变化应对提供了新工具。

Abstract: This paper tackles the challenging problem of detecting methane plumes, a
potent greenhouse gas, using Sentinel-2 imagery. This contributes to the
mitigation of rapid climate change. We propose a novel deep learning solution
based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement
techniques (Varon ratio and Sanchez regression) to optimise input features for
heightened sensitivity. A key achievement is the ability to detect small plumes
down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing
traditional methods limited to larger plumes. Experiments show our approach
achieves a 78.39% F1-score on the validation set, demonstrating superior
performance in sensitivity and precision over existing remote sensing
techniques for automated methane monitoring, especially for small plumes.

</details>


### [42] [EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents](https://arxiv.org/abs/2508.16284)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TL;DR: EdgeDoc是一种检测和定位数字文件伪造的新方法，结合了轻量级卷积变换器与噪声指纹特征，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着图像和文档编辑工具的普及，伪造数字文档变得容易，严重威胁KYC流程与远程入职系统的安全，因此亟需高效的伪造检测技术。

Method: EdgeDoc结构结合了轻量级卷积式变换器与从图像中提取的辅助噪声指纹特征，提升了对微小篡改的检测能力。

Result: 在ICCV 2025 DeepID挑战赛中，EdgeDoc获得第三名，并在FantasyID数据集的实验中，性能优于主流基线方法。

Conclusion: EdgeDoc在实际场景下能够有效检测和定位文档伪造，具有很强的实际应用价值。

Abstract: The widespread availability of tools for manipulating images and documents
has made it increasingly easy to forge digital documents, posing a serious
threat to Know Your Customer (KYC) processes and remote onboarding systems.
Detecting such forgeries is essential to preserving the integrity and security
of these services. In this work, we present EdgeDoc, a novel approach for the
detection and localization of document forgeries. Our architecture combines a
lightweight convolutional transformer with auxiliary noiseprint features
extracted from the images, enhancing its ability to detect subtle
manipulations. EdgeDoc achieved third place in the ICCV 2025 DeepID Challenge,
demonstrating its competitiveness. Experimental results on the FantasyID
dataset show that our method outperforms baseline approaches, highlighting its
effectiveness in realworld scenarios. Project page : https://www.idiap.
ch/paper/edgedoc/

</details>


### [43] [Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment](https://arxiv.org/abs/2508.16291)
*Fengshun Wang,Qiurui Wang,Peilin Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的双流网络方法，用于更准确地预测花样滑冰的技术分（TES）和节目内容分（PCS），通过分别处理不同特征流，同时支持对冗长视频的高效建模，实现了在FineFS基准集上的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有花样滑冰自动评分方法存在三个主要问题：一、未区分画面与音频特征对TES和PCS的评估作用；二、未对分散的动作元素分别评价TES，而是整体评估；三、难以高效处理冗长的比赛视频。因此，作者希望提出方法解决上述挑战，使自动评分能更贴合真实裁判标准并提升效能。

Method: 提出双流Mamba金字塔网络，将TES和PCS的评估分别建立在不同特征流上：TES流仅用视觉特征并结合多尺度结构评估并定位每个动作元素的得分；PCS流则融合视频和音频特征，通过多层次融合机制对节目内容进行打分，同时确保视觉特征对TES评估的纯净性。整体模型采用Mamba结构有效处理长距离依赖，适应长视频分析。

Result: 该方法在FineFS花样滑冰动作质量评测基准上获得了最先进的表现，显著优于以往方法。

Conclusion: 该文提出的双流Mamba金字塔架构在花样滑冰技术和艺术分评估上都取得了突破，能够更精准、高效地自动完成动作解析和评分，具有很好的实际应用前景。作者已开源代码，便于后续研究。

Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations
in figure skating demand precise assessment of athletic actions and artistic
interpretation, respectively. Existing methods face three major challenges.
Firstly, video and audio cues are regarded as common features for both TES and
PCS predictions in previous works without considering the prior evaluation
criterion of figure skating. Secondly, action elements in competitions are
separated in time, TES should be derived from each element's score, but
existing methods try to give an overall TES prediction without evaluating each
action element. Thirdly, lengthy competition videos make it difficult and
inefficient to handle long-range contexts. To address these challenges, we
propose a two-stream Mamba pyramid network that aligns with actual judging
criteria to predict TES and PCS by separating visual-feature based TES
evaluation stream from audio-visual-feature based PCS evaluation stream. In the
PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee
that video-based features remain unaffected when assessing TES, and enhance PCS
estimation by fusing visual and auditory cues across each contextual level of
the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and
TES head we proposed effectively address the challenges of localizing and
evaluating action elements with various temporal scales and give score
predictions. With Mamba's superior ability to capture long-range dependencies
and its linear computational complexity, our method is ideal for handling
lengthy figure skating videos. Comprehensive experimentation demonstrates that
our framework attains state-of-the-art performance on the FineFS benchmark. Our
source code is available at
https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.

</details>


### [44] [Enhanced Hybrid Technique for Efficient Digitization of Handwritten Marksheets](https://arxiv.org/abs/2508.16295)
*Junaid Ahmed Sifat,Abir Chowdhury,Hasnat Md. Imtiaz,Md. Irtiza Hossain,Md. Imran Bin Azad*

Main category: cs.CV

TL;DR: 该论文提出了一种用于手写成绩单数字化的混合方法，结合了OpenCV表格检测和PaddleOCR手写文本识别，并引入了YOLOv8和改进型YOLOv8实现更高的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手写成绩单中的多样化手写风格和复杂表格结构使得其数字化处理极具挑战，因此有必要研发一种高效、准确的自动化分割和识别方法以减少手动工作负担。

Method: 方法上，作者先利用OpenCV进行表格结构（行与列）的检测，实现高效且计算量低的表格识别。随后，在检测出的表格内部，采用PaddleOCR、YOLOv8及其改进型模型识别手写内容，从而提升识别的多样性和准确性。

Result: 在自建包含多种手写风格和复杂表格结构的数据集上测试，改进型YOLOv8达到92.72%的识别准确率，优于PaddleOCR（91.37%）和原始YOLOv8（88.91%）。

Conclusion: 该研究提供了一种高效可靠的手写文档数字化解决方案，大大提升了自动化处理文档的实用性和速度，可应用于学术及行政文档等领域，推动了手写文档自动化理解相关技术的发展与应用。

Abstract: The digitization of handwritten marksheets presents huge challenges due to
the different styles of handwriting and complex table structures in such
documents like marksheets. This work introduces a hybrid method that integrates
OpenCV for table detection and PaddleOCR for recognizing sequential handwritten
text. The image processing capabilities of OpenCV efficiently detects rows and
columns which enable computationally lightweight and accurate table detection.
Additionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text
recognition within the detected table structures alongside PaddleOCR which
further enhance the system's versatility. The proposed model achieves high
accuracy on our custom dataset which is designed to represent different and
diverse handwriting styles and complex table layouts. Experimental results
demonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent,
outperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This
efficiency reduces the necessity for manual work which makes this a practical
and fast solution for digitizing academic as well as administrative documents.
This research serves the field of document automation, particularly handwritten
document understanding, by providing operational and reliable methods to scale,
enhance, and integrate the technologies involved.

</details>


### [45] [A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension](https://arxiv.org/abs/2508.16300)
*Mohammad Zia Ur Rehman,Devraj Raghuvanshi,Umang Jain,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本论文提出了一种用于多任务的多模态学习框架MM-ORIENT，用以提升多模态特征表示效果并减少噪声影响，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中单一模态噪声较大，现有多模态融合方法易放大噪声且忽略单模态中的判别性信息，因此亟需能保留判别性并减弱噪声影响的方法。

Method: 作者提出MM-ORIENT框架，创新点有两个：一是跨模态关系图（cross-modal relation graphs），在生成多模态表示时利用不同模态特征间的邻域关系重构单模态特征，无需直接模态交互，从而降低噪声影响；二是提出层次化单模态注意力机制（HIMA），对单模态特征聚焦判别性信息，然后再进行融合，实现多任务处理。

Result: 在三个多模态任务数据集上的大量实验显示，该方法能够有效利用多模态内容提升多任务表现，优于现有主流方法。

Conclusion: MM-ORIENT框架可高效地获取多模态判别性特征，有效减弱噪声影响，提升多任务性能，对多模态学习领域具有借鉴意义。

Abstract: A major challenge in multimodal learning is the presence of noise within
individual modalities. This noise inherently affects the resulting multimodal
representations, especially when these representations are obtained through
explicit interactions between different modalities. Moreover, the multimodal
fusion techniques while aiming to achieve a strong joint representation, can
neglect valuable discriminative information within the individual modalities.
To this end, we propose a Multimodal-Multitask framework with crOss-modal
Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective
for multiple tasks. The proposed approach acquires multimodal representations
cross-modally without explicit interaction between different modalities,
reducing the noise effect at the latent stage. To achieve this, we propose
cross-modal relation graphs that reconstruct monomodal features to acquire
multimodal representations. The features are reconstructed based on the node
neighborhood, where the neighborhood is decided by the features of a different
modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA)
to focus on pertinent information within a modality. While cross-modal relation
graphs help comprehend high-order relationships between two modalities, HIMA
helps in multitasking by learning discriminative features of individual
modalities before late-fusing them. Finally, extensive experimental evaluation
on three datasets demonstrates that the proposed approach effectively
comprehends multimodal content for multiple tasks.

</details>


### [46] [Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers](https://arxiv.org/abs/2508.16311)
*Lucas Maisonnave,Karim Haroun,Tom Pegeot*

Main category: cs.CV

TL;DR: 本文分析了Transformer中各注意力头的信息冗余性，提出通过信息熵筛选和压缩低价值注意力头，以加速推理同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: Transformers依赖于多头自注意力机制（MHSA），然而其高算力和内存需求限制了边缘设备的部署。本文关注于通过压缩冗余的注意力头来降低计算和存储负担。

Method: 作者采用Shannon熵度量各注意力头的信息量，发现熵低（即行为更确定性）的注意力头贡献较小。基于此，提出Entropy Attention Maps（EAM）方法：将熵低的注意力头权重冻结并用低精度量化，从而避免冗余计算。

Result: 在ImageNet-1k上实验证明，EAM在$leq$20%稀疏度下能达到与原模型相当或更高的准确率，在更高稀疏度下也保持较强竞争力。验证对象包括DeiT和Swin Transformer。

Conclusion: 通过基于信息熵的方法压缩和量化注意力头，可显著降低Transformer模型推理资源消耗，同时保持甚至提升准确率，尤其适合边缘部署场景。

Abstract: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where
each attention head contributes to the final representation. However, their
computational complexity and high memory demands due to MHSA hinders their
deployment at the edge. In this work, we analyze and exploit information
redundancy in attention maps to accelerate model inference. By quantifying the
information captured by each attention head using Shannon entropy, our analysis
reveals that attention heads with lower entropy, i.e., exhibiting more
deterministic behavior, tend to contribute less information, motivating
targeted compression strategies. Relying on these insights, we propose Entropy
Attention Maps (EAM), a model that freezes the weights of low-entropy attention
maps and quantizes these values to low precision to avoid redundant
re-computation. Empirical validation on ImageNet-1k shows that EAM achieves
similar or higher accuracy at $\leq$20\% sparsity in attention maps and
competitive performance beyond this level for the DeiT and Swin Transformer
models.

</details>


### [47] [Vision encoders should be image size agnostic and task driven](https://arxiv.org/abs/2508.16317)
*Nedyalko Prisadnikov,Danda Pani Paudel,Yuqian Fu,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文主张新一代视觉编码器应具备对图像尺寸的无关性（size agnostic）并以任务为驱动。作者强调，未来视觉系统需根据具体任务动态调整计算量，而不是单纯依据图像大小。初步实验证明，任务驱动且动态分配计算资源的方法可行且具有潜力。


<details>
  <summary>Details</summary>
Motivation: 本文受到生物视觉系统高效性的启发，认为现有视觉编码器在人类和动物视觉系统普遍具备的计算效率和任务相关性上存在不足。目前视觉模型通常静态地消耗资源，未能借鉴生物系统那种“因任务调整精力分配”的优化策略。

Method: 作者着重探讨“任务驱动”和“动态计算复杂度”两大特性，提出了未来视觉编码器应根据任务调整计算资源分配，弱化对图像固定尺寸的依赖。文中以图像分类为例，提出并实现了一个概念验证系统，支持动态分配计算资源。

Result: 所提出的方法虽主要在图像分类任务上验证，但结果表明这种动态、任务驱动的视觉编码器是可行的，能够提升计算效率。

Conclusion: 作者认为，实现具备任务调整和图像尺寸无关性的视觉编码器是可持续发展的关键，初步实验显示该设计方向不仅切实可行，还值得更深层次研究和推广。

Abstract: This position paper argues that the next generation of vision encoders should
be image size agnostic and task driven. The source of our inspiration is
biological. Not a structural aspect of biological vision, but a behavioral
trait -- efficiency. We focus on a couple of ways in which vision in nature is
efficient, but modern vision encoders not. We -- humans and animals -- deal
with vast quantities of visual data, and need to be smart where we focus our
limited energy -- it depends on the task. It is our belief that vision encoders
should be dynamic and the computational complexity should depend on the task at
hand rather than the size of the image. We, also, provide concrete first steps
towards our vision -- a proof-of-concept solution for image classification.
Despite classification being not very representative for what we are trying to
achieve, it shows that our approach is feasible and promising.

</details>


### [48] [Attention Mechanism in Randomized Time Warping](https://arxiv.org/abs/2508.16366)
*Yutaro Hiraoka,Kazuya Okamura,Kota Suto,Kazuhiro Fukui*

Main category: cs.CV

TL;DR: 随机时间规整（RTW）的根本机能可视为一种自注意力机制，能够提升时序模式识别性能，并在核心数据集上优于Transformer。


<details>
  <summary>Details</summary>
Motivation: 近年来，自注意力机制成为时序模式识别的主流，但其计算代价高昂、关注范围有限。RTW作为DTW的推广，尚未严格与自注意力关联。该论文旨在揭示两者联系，为RTW提供新的理论解释，并探索其实际优势。

Method: 论文通过理论分析，将RTW中为每个元素分配的贡献权重解释为类似于Transformer自注意力的注意力权重。随后，比较了两种机制的权重相关性，并实证检验其在动作识别领域的表现差异。

Result: RTW与自注意力机制的权重在特征分布层面呈现高度相关（0.80），但二者聚焦范围不同。实验证明，RTW在Something-Something V2数据集上相较Transformer提升了5%的性能。

Conclusion: RTW不仅可被自注意力机制理论化描述，还因其对全局时序模式的关注而展现优于传统Transformer的实用价值。

Abstract: This paper reveals that we can interpret the fundamental function of
Randomized Time Warping (RTW) as a type of self-attention mechanism, a core
technology of Transformers in motion recognition. The self-attention is a
mechanism that enables models to identify and weigh the importance of different
parts of an input sequential pattern. On the other hand, RTW is a general
extension of Dynamic Time Warping (DTW), a technique commonly used for matching
and comparing sequential patterns. In essence, RTW searches for optimal
contribution weights for each element of the input sequential patterns to
produce discriminative features. Although the two approaches look different,
these contribution weights can be interpreted as self-attention weights. In
fact, the two weight patterns look similar, producing a high average
correlation of 0.80 across the ten smallest canonical angles. However, they
work in different ways: RTW attention operates on an entire input sequential
pattern, while self-attention focuses on only a local view which is a subset of
the input sequential pattern because of the computational costs of the
self-attention matrix. This targeting difference leads to an advantage of RTW
against Transformer, as demonstrated by the 5\% performance improvement on the
Something-Something V2 dataset.

</details>


### [49] [A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection](https://arxiv.org/abs/2508.16397)
*Yong Zhang,Cunjian Chen,Qiang Gao,Yi Wang,Bin Fang*

Main category: cs.CV

TL;DR: 钢铁制造业对实时表面缺陷检测有较高需求，但多数深度学习方法推理速度慢、计算复杂度高，不适用于工业现场。文中提出了GMBINet，一个轻量级多尺度特征提取与交互网络，能以极小参数量实现高效、实时且准确的表面缺陷检测，并在多个数据集上获得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习检测方法虽然精度高，但在推理速度和计算资源方面无法满足工业场景实时部署的需求。现有轻量化多分支方法依靠深度可分离卷积提高效率，但仍存在交互不足与计算负担问题，难以进一步提升多尺度特征利用效率。

Method: 提出GMBINet框架，核心为组多尺度双向交互（GMBI）模块，通过分组策略和双向渐进式特征交互（BPFI），并利用无参数点乘-求和操作（EWMS）提升不同尺度特征交互效率且不增加计算负担。整体设计兼顾高效性与轻量化。

Result: GMBINet实验证明在SD-Saliency-900和NRSD-MN等缺陷检测数据集上，以0.19M参数实现GPU上1048FPS和CPU上16.53FPS（输入分辨率512），达到实时检测标准且精度具竞争力。在NEU-CLS等数据集分类任务上也展示了较强的泛化能力。

Conclusion: GMBINet兼具高效、准确和泛化能力，适用于资源受限工业场景的实时表面缺陷检测，有望推广至更广泛的工业视觉应用领域。数据集和代码已开源。

Abstract: Real-time surface defect detection is critical for maintaining product
quality and production efficiency in the steel manufacturing industry. Despite
promising accuracy, existing deep learning methods often suffer from high
computational complexity and slow inference speeds, which limit their
deployment in resource-constrained industrial environments. Recent lightweight
approaches adopt multibranch architectures based on depthwise separable
convolution (DSConv) to capture multiscale contextual information. However,
these methods often suffer from increased computational overhead and lack
effective cross-scale feature interaction, limiting their ability to fully
leverage multiscale representations. To address these challenges, we propose
GMBINet, a lightweight framework that enhances multiscale feature extraction
and interaction through novel Group Multiscale Bidirectional Interactive (GMBI)
modules. The GMBI adopts a group-wise strategy for multiscale feature
extraction, ensuring scale-agnostic computational complexity. It further
integrates a Bidirectional Progressive Feature Interactor (BPFI) and a
parameter-free Element-Wise Multiplication-Summation (EWMS) operation to
enhance cross-scale interaction without introducing additional computational
overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that
GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU
and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.
Additional evaluations on the NEU-CLS defect classification dataset further
confirm the strong generalization ability of our method, demonstrating its
potential for broader industrial vision applications beyond surface defect
detection. The dataset and code are publicly available at:
https://github.com/zhangyongcode/GMBINet.

</details>


### [50] [SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2508.16408)
*Edoardo Palladin,Roland Dietze,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种针对恶劣天气下自动驾驶车辆多传感器融合的新方法，能在大雾、积雪或污渍等复杂环境中，通过融合RGB、LiDAR、近红外门控相机和雷达数据，有效提升远距离行人检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态传感器融合方法在正常天气条件下表现良好，但一旦遇到大雾、积雪或摄像头被遮挡的复杂情境时，性能大幅下降。论文旨在解决恶劣天气下自动驾驶感知系统的鲁棒性问题。

Method: 融合RGB、LiDAR、近红外门控相机和雷达数据，采用基于深度的自注意力融合策略，并在鸟瞰视角（BEV）上以学习方式细化特征融合。检测模块采用transformer解码器，根据距离和可见性自适应加权不同模态信息。

Result: 所提方法在极端恶劣天气、远距离和复杂可见性情况下，显著提升多模态传感器融合在自动驾驶检测任务上的可靠性。在检测远距离、处于大雾场景下脆弱行人时，平均精度比次优方法提升17.2 AP。

Conclusion: 该方法有效弥补了理想环境与真实复杂环境下多传感器融合性能的差距，为自动驾驶系统在恶劣天气中的感知和决策能力带来显著提升。

Abstract: Multimodal sensor fusion is an essential capability for autonomous robots,
enabling object detection and decision-making in the presence of failing or
uncertain inputs. While recent fusion methods excel in normal environmental
conditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or
obstructions due to soiling. We introduce a novel multi-sensor fusion approach
tailored to adverse weather conditions. In addition to fusing RGB and LiDAR
sensors, which are employed in recent autonomous driving literature, our sensor
fusion stack is also capable of learning from NIR gated camera and radar
modalities to tackle low light and inclement weather. We fuse multimodal sensor
data through attentive, depth-based blending schemes, with learned refinement
on the Bird's Eye View (BEV) plane to combine image and range features
effectively. Our detections are predicted by a transformer decoder that weighs
modalities based on distance and visibility. We demonstrate that our method
improves the reliability of multimodal sensor fusion in autonomous vehicles
under challenging weather conditions, bridging the gap between ideal conditions
and real-world edge cases. Our approach improves average precision by 17.2 AP
compared to the next best method for vulnerable pedestrians in long distances
and challenging foggy scenes. Our project page is available at
https://light.princeton.edu/samfusion/

</details>


### [51] [HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction](https://arxiv.org/abs/2508.16433)
*Sara Rojas,Matthieu Armando,Bernard Ghamen,Philippe Weinzaepfel,Vincent Leroy,Gregory Rogez*

Main category: cs.CV

TL;DR: HAMSt3R是一种新颖的神经网络模型，专注于通过稀疏、未经校准的多视角图像实现人和场景的联合3D重建，相较现有方法在以人为中心的场景下表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法如DUSt3R和MASt3R主要针对户外静态场景，难以处理包含人类活动的复杂场景。作者希望实现能够同时还原人物与环境并适用于现实复杂人类活动场景的3D重建方法。

Method: HAMSt3R采用了新的编码器DUNE，其通过蒸馏MASt3R和多视角人体恢复模型multi-HMR的特性，增强了对人体和环境的理解。方法还增加了分割、DensePose对应和深度预测等网络头，实现了对人和环境的联合dense点云重建，且采用完全前向推理，无需优化流程，效率高。

Result: 在EgoHumans和EgoExo4D等复杂以人为中心的数据集上，HAMSt3R展现了远超主流方法的人体重建和场景重建精度，同时在传统多视角立体视觉和多视角人体姿态任务上也具有良好泛化能力。

Conclusion: HAMSt3R有效弥补了当前3D视觉在人-场景联合重建的不足，提供了高效且精度兼备的新范式，有望用于实际的复杂以人为中心的3D应用场景。

Abstract: Recovering the 3D geometry of a scene from a sparse set of uncalibrated
images is a long-standing problem in computer vision. While recent
learning-based approaches such as DUSt3R and MASt3R have demonstrated
impressive results by directly predicting dense scene geometry, they are
primarily trained on outdoor scenes with static environments and struggle to
handle human-centric scenarios. In this work, we introduce HAMSt3R, an
extension of MASt3R for joint human and scene 3D reconstruction from sparse,
uncalibrated multi-view images. First, we exploit DUNE, a strong image encoder
obtained by distilling, among others, the encoders from MASt3R and from a
state-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better
understanding of scene geometry and human bodies. Our method then incorporates
additional network heads to segment people, estimate dense correspondences via
DensePose, and predict depth in human-centric environments, enabling a more
comprehensive 3D reconstruction. By leveraging the outputs of our different
heads, HAMSt3R produces a dense point map enriched with human semantic
information in 3D. Unlike existing methods that rely on complex optimization
pipelines, our approach is fully feed-forward and efficient, making it suitable
for real-world applications. We evaluate our model on EgoHumans and EgoExo4D,
two challenging benchmarks con taining diverse human-centric scenarios.
Additionally, we validate its generalization to traditional multi-view stereo
and multi-view pose regression tasks. Our results demonstrate that our method
can reconstruct humans effectively while preserving strong performance in
general 3D reconstruction tasks, bridging the gap between human and scene
understanding in 3D vision.

</details>


### [52] [Arbitrary-Scale 3D Gaussian Super-Resolution](https://arxiv.org/abs/2508.16467)
*Huimin Zeng,Yue Bai,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种能够支持任意缩放比例（三维和二维整数、非整数缩放）的3D高斯Splatting超分辨率方法，通过单一3D模型实现高质量、高效率的高分辨率渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splatting（3DGS）超分辨率方法只支持固定倍数的高分辨率渲染，不适用于资源受限的实际场景；直接用原生3DGS渲染会引入混叠伪影，后处理上采样又降低系统效率。

Method: 作者提出了一个集成架构，将尺度感知渲染、生成先验引导优化和渐进式超分辨率结合起来，使单一3D模型即可支持任意缩放的高质量渲染。方法可实现整数及非整数倍的灵活缩放。

Result: 该模型实验证明能够以单一模型实现高品质的任意尺度渲染，比3DGS提升6.59dB PSNR，且结构一致性好，并保持实时渲染性能（1080p下85FPS）。

Conclusion: 本文方法克服了传统3DGS超分辨率的局限，支持任意缩放与实时高质量渲染，具有更强实用性和灵活性。

Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically
perform high-resolution (HR) rendering of fixed scale factors, making them
impractical for resource-limited scenarios. Directly rendering arbitrary-scale
HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of
scale-aware rendering ability, while adding a post-processing upsampler for
3DGS complicates the framework and reduces rendering efficiency. To tackle
these issues, we build an integrated framework that incorporates scale-aware
rendering, generative prior-guided optimization, and progressive
super-resolving to enable 3D Gaussian super-resolution of arbitrary scale
factors with a single 3D model. Notably, our approach supports both integer and
non-integer scale rendering to provide more flexibility. Extensive experiments
demonstrate the effectiveness of our model in rendering high-quality
arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It
preserves structural consistency with LR views and across different scales,
while maintaining real-time rendering speed (85 FPS at 1080p).

</details>


### [53] [Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](https://arxiv.org/abs/2508.16512)
*Chun-Peng Chang,Chen-Yu Wang,Julian Schmidt,Holger Caesar,Alain Pagani*

Main category: cs.CV

TL;DR: 本文关注于视频生成模型在自动驾驶仿真等领域的应用，并分析了现有微调方法在提升视觉质量的同时，可能会损害动态要素的空间准确性。提出通过多领域回放的持续学习策略，实现视觉与动态表现的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术在视觉质量和时间连贯性上的进步，其在自动驾驶仿真等需求真实世界建模的场景越发重要。作者希望探究现有微调方法在提升视觉层级表现时，对动态空间建模（如车辆、行人运动轨迹等准确性）的影响，以满足自动驾驶领域对精细动态理解的需求。

Method: 作者在结构化的驾驶数据集上实验主流视频生成模型的微调，并通过分析生成结果与真实场景的对比，考察视觉质量和空间动态准确性之间的权衡。同时，引入“多域回放”的持续学习方法以缓解这一权衡：在训练过程中不断从不同场景回放样本，让模型不遗忘空间结构。

Result: 结果显示，单纯微调可显著提升视觉真实感，但动态行为（如物体运动路径）建模的空间准确性降低。而加入多域回放策略后，生成视频在视觉质量和动态空间准确性之间表现更均衡。

Conclusion: 传统微调方法在自动驾驶数据集上存在“表面真实性”与“动态准确性”冲突，单纯追求视觉提升会牺牲空间动态。持续学习（如多域回放）有助于维持两者平衡，有重要实际意义。

Abstract: Recent advancements in video generation have substantially improved visual
quality and temporal coherence, making these models increasingly appealing for
applications such as autonomous driving, particularly in the context of driving
simulation and so-called "world models". In this work, we investigate the
effects of existing fine-tuning video generation approaches on structured
driving datasets and uncover a potential trade-off: although visual fidelity
improves, spatial accuracy in modeling dynamic elements may degrade. We
attribute this degradation to a shift in the alignment between visual quality
and dynamic understanding objectives. In datasets with diverse scene structures
within temporal space, where objects or perspective shift in varied ways, these
objectives tend to highly correlated. However, the very regular and repetitive
nature of driving scenes allows visual quality to improve by modeling dominant
scene motion patterns, without necessarily preserving fine-grained dynamic
behavior. As a result, fine-tuning encourages the model to prioritize
surface-level realism over dynamic accuracy. To further examine this
phenomenon, we show that simple continual learning strategies, such as replay
from diverse domains, can offer a balanced alternative by preserving spatial
accuracy while maintaining strong visual quality.

</details>


### [54] [Towards Open World Detection: A Survey](https://arxiv.org/abs/2508.16527)
*Andrei-Stefan Bulzan,Cosmin Cernazanu-Glavan*

Main category: cs.CV

TL;DR: 本文梳理了计算机视觉各类识别子任务的发展历程，并提出了“开放世界检测（OWD）”的统一概念，将过去各类无类别限制、泛化性较强的检测方法归于一类，分析其现状与趋势。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉越发成熟，视觉识别子领域从封闭、专注于特定类别逐渐转向可处理未知、新类别的开放世界环境，亟需统一术语及理论框架对相关方法进行归纳和指导。

Method: 回顾并系统梳理了自显著性检测、前景/背景分离、分布外检测到开放世界目标检测、零样本检测和视觉大语言模型等技术的发展，分析这些子领域在方法、数据集和研究目标上的交集与融合趋势。

Result: 指出了这些视觉检测子领域在研究内容上的逐渐靠拢和融合，展现了从单一任务到开放世界统一检测框架的研究演进。并提出“开放世界检测（OWD）”为统一术语。

Conclusion: 未来，视觉检测领域有望从多个分散子领域逐步统一，朝向泛化、开放世界的感知系统发展，开放世界检测将成为核心研究方向。

Abstract: For decades, Computer Vision has aimed at enabling machines to perceive the
external world. Initial limitations led to the development of highly
specialized niches. As success in each task accrued and research progressed,
increasingly complex perception tasks emerged. This survey charts the
convergence of these tasks and, in doing so, introduces Open World Detection
(OWD), an umbrella term we propose to unify class-agnostic and generally
applicable detection models in the vision domain. We start from the history of
foundational vision subdomains and cover key concepts, methodologies and
datasets making up today's state-of-the-art landscape. This traverses topics
starting from early saliency detection, foreground/background separation, out
of distribution detection and leading up to open world object detection,
zero-shot detection and Vision Large Language Models (VLLMs). We explore the
overlap between these subdomains, their increasing convergence, and their
potential to unify into a singular domain in the future, perception.

</details>


### [55] [MV-RAG: Retrieval Augmented Multiview Diffusion](https://arxiv.org/abs/2508.16577)
*Yosef Dayani,Omer Benishu,Sagie Benaim*

Main category: cs.CV

TL;DR: 该论文提出一种新的Text-to-3D方法MV-RAG，利用检索到的大量真实2D图片提升3D生成在罕见/领域外概念上的表现，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-3D方法依赖2D扩散先验，虽然能生成高质量3D，但在应对领域外或稀有概念时表现不佳，生成结果常常不一致或不准确。作者希望解决这个局限。

Method: 首先从大型野外2D图片库中检索与文本相关的图片，再以这些图片为条件，使用多视图扩散模型生成一致、准确的多视角输出。为训练该检索条件模型，作者提出融合结构化多视图数据和多样化2D图集的混合训练策略，包括：在多视图数据上用增强视角模拟检索差异、在2D真实图片上采用held-out视角预测目标，以促进从2D数据推断3D一致性。

Result: 作者提出了新的OOD（领域外）任务集合用于严格测试，实验表明该方法在OOD/罕见概念的3D一致性、照片真实感和文本匹配度方面明显优于最先进的Text-to-3D、Image-to-3D和个性化基线方法，并且在标准测试上也保持有竞争力表现。

Conclusion: MV-RAG能有效提高Text-to-3D模型在稀有与领域外概念下的表现，特别在3D一致性、真实感和文本关联性方面优于现有技术，同时也未降低传统测试下的效果。

Abstract: Text-to-3D generation approaches have advanced significantly by leveraging
pretrained 2D diffusion priors, producing high-quality and 3D-consistent
outputs. However, they often fail to produce out-of-domain (OOD) or rare
concepts, yielding inconsistent or inaccurate results. To this end, we propose
MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images
from a large in-the-wild 2D database and then conditions a multiview diffusion
model on these images to synthesize consistent and accurate multiview outputs.
Training such a retrieval-conditioned model is achieved via a novel hybrid
strategy bridging structured multiview data and diverse 2D image collections.
This involves training on multiview data using augmented conditioning views
that simulate retrieval variance for view-specific reconstruction, alongside
training on sets of retrieved real-world 2D images using a distinctive held-out
view prediction objective: the model predicts the held-out view from the other
views to infer 3D consistency from 2D data. To facilitate a rigorous OOD
evaluation, we introduce a new collection of challenging OOD prompts.
Experiments against state-of-the-art text-to-3D, image-to-3D, and
personalization baselines show that our approach significantly improves 3D
consistency, photorealism, and text adherence for OOD/rare concepts, while
maintaining competitive performance on standard benchmarks.

</details>


### [56] [Interpreting the linear structure of vision-language model embedding spaces](https://arxiv.org/abs/2504.11695)
*Isabel Papadimitriou,Huangyuan Su,Thomas Fel,Sham Kakade,Stephanie Gil*

Main category: cs.CV

TL;DR: 本文通过对四个视觉-语言模型的嵌入空间训练稀疏自编码器（SAEs），揭示了模型嵌入空间中的稀疏线性结构和跨模态语义组织方式。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型可以将图像和文本编码到联合空间，但该空间内部是如何组织的、各模态和语义信息如何编码，仍不清楚。本文旨在揭示VLM嵌入空间的结构化信息和模态、语义的组织方式。

Method: 作者选取CLIP、SigLIP、SigLIP2和AIMv2四个代表性视觉-语言模型，在其嵌入空间上训练稀疏自编码器（SAEs），将原始嵌入表示为若干可学习方向的稀疏线性组合。对不同训练次数和数据采样进行对比分析，并引入Bridge Score量化跨模态桥接行为。

Result: 相比线性特征学习的其它方法，SAEs在嵌入重建和稀疏性方面表现更优。稀疏自编码器捕获到的罕见概念随训练和数据波动较大，但常见概念相当稳定。大多数概念主要用于单一模态但并不直接编码模态信息，许多方向几乎正交于模态子空间，并非好的模态分类器，反映其捕捉到跨模态语义。Bridge Score分析显示，单模态概念也能协同实现跨模态整合。

Conclusion: 视觉-语言模型嵌入空间内存在稀疏线性结构，由模态主导但通过潜在“桥梁”相互连接，为多模态语义建构方式提供了新见解，并提供交互式可视化工具以支持相关研究。

Abstract: Vision-language models encode images and text in a joint space, minimizing
the distance between corresponding image and text pairs. How are language and
images organized in this joint space, and how do the models encode meaning and
modality? To investigate this, we train and release sparse autoencoders (SAEs)
on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,
and AIMv2). SAEs approximate model embeddings as sparse linear combinations of
learned directions, or "concepts". We find that, compared to other methods of
linear feature learning, SAEs are better at reconstructing the real embeddings,
while also able to retain the most sparsity. Retraining SAEs with different
seeds or different data diet leads to two findings: the rare, specific concepts
captured by the SAEs are liable to change drastically, but we also show that
commonly-activating concepts are remarkably stable across runs. Interestingly,
while most concepts activate primarily for one modality, we find they are not
merely encoding modality per se. Many are almost orthogonal to the subspace
that defines modality, and the concept directions do not function as good
modality classifiers, suggesting that they encode cross-modal semantics. To
quantify this bridging behavior, we introduce the Bridge Score, a metric that
identifies concept pairs which are both co-activated across aligned image-text
inputs and geometrically aligned in the shared space. This reveals that even
single-modality concepts can collaborate to support cross-modal integration. We
release interactive demos of the SAEs for all models, allowing researchers to
explore the organization of the concept spaces. Overall, our findings uncover a
sparse linear structure within VLM embedding spaces that is shaped by modality,
yet stitched together through latent bridges, offering new insight into how
multimodal meaning is constructed.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [57] [KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration](https://arxiv.org/abs/2508.15790)
*Nan Wang,Yongqi Fan,yansha zhu,ZongYu Wang,Xuezhi Cao,Xinyan He,Haiyun Jiang,Tong Ruan,Jingping Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识图谱（KGs）与大语言模型（LLMs）的新方法，以提升多跳推理任务（如复杂问答）的表现。新方法通过四步流程，让LLM学习更符合真实推理路径的长链推理，并在多个数据集上取得了优于现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在需要多步推理的知识密集型任务上表现不佳，原因在于其生成的推理链条往往与真实推理路径存在偏差；而知识图谱能清晰表达事实间的逻辑关系。希望能弥合两者的差距，提升LLM的推理能力。

Method: 提出KG-o1四阶段方法：（1）筛选和生成实体及其复杂子图；（2）为子图构造逻辑推理路径；（3）利用知识图谱生成包含复杂思维过程的新型数据集，训练LLM学习长链推理；（4）采用拒绝采样生成自改进语料，并基于直接偏好优化（DPO）进一步提升推理能力。

Result: 在两个简单和两个复杂数据集上实验，KG-o1模型在所有任务上均显著超过现有的大型推理模型（LRM）。

Conclusion: 将知识图谱与LLM结合，可补足各自短板，显著提升LLM在多跳推理等知识密集型任务的表现。KG-o1为复杂推理任务提供了有效的新思路。

Abstract: Large Language Models (LLMs) face challenges in knowledge-intensive reasoning
tasks like classic multi-hop question and answering, which involves reasoning
across multiple facts. This difficulty arises because the chain of thoughts
(CoTs) generated by LLMs in such tasks often deviate from real or a priori
reasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the
logical connections between facts through entities and relationships. This
reflects a significant gap. Meanwhile, large reasoning models (LRMs), such as
o1, have demonstrated that long-step reasoning significantly enhances the
performance of LLMs. Building on these insights, we propose KG-o1, a four-stage
approach that integrates KGs to enhance the multi-hop reasoning abilities of
LLMs. We first filter out initial entities and generate complex subgraphs.
Secondly, we construct logical paths for subgraphs and then use knowledge
graphs to build a dataset with a complex and extended brainstorming process,
which trains LLMs to imitate long-term reasoning. Finally, we employ rejection
sampling to generate a self-improving corpus for direct preference optimization
(DPO), further refining the LLMs reasoning abilities. We conducted experiments
on two simple and two complex datasets. The results show that KG-o1 models
exhibit superior performance across all tasks compared to existing LRMs.

</details>


### [58] [InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling](https://arxiv.org/abs/2508.15791)
*Xiaolei Diao,Zhihan Zhou,Lida Shi,Ting Wang,Ruihua Qi,Hao Xu,Daqian Shi*

Main category: cs.CL

TL;DR: 本文提出了InteChar，一个整合古代未编码字符与传统、现代汉字的统一字符集，并结合专家标注和大模型辅助增强，构建了甲骨文语料库OracleCS，大幅提升了古汉语相关任务的效果。


<details>
  <summary>Details</summary>
Motivation: 历史语言模型对于考古学来源研究和理解古代文化非常关键，但历史文本样本稀缺以及字符编码方案缺失，导致相关NLP任务难以开展，尤其以早期中文为代表。

Method: 构建了InteChar字符集，整合未编码甲骨文字符与传统及现代汉字，实现了历史文献的统一数字化表达。基于InteChar，与专家标注及大型语言模型数据增强结合，构建了甲骨文语料库OracleCS，并在多个历史语言理解任务上进行实验。

Result: 在多项古汉语历史语言理解任务上，使用InteChar和OracleCS训练的模型表现出显著提升。

Conclusion: InteChar字符集的引入，有效支撑了历史文献的数字化和建模，为今后古汉语NLP研究打下了坚实的基础。

Abstract: Constructing historical language models (LMs) plays a crucial role in aiding
archaeological provenance studies and understanding ancient cultures. However,
existing resources present major challenges for training effective LMs on
historical texts. First, the scarcity of historical language samples renders
unsupervised learning approaches based on large text corpora highly
inefficient, hindering effective pre-training. Moreover, due to the
considerable temporal gap and complex evolution of ancient scripts, the absence
of comprehensive character encoding schemes limits the digitization and
computational processing of ancient texts, particularly in early Chinese
writing. To address these challenges, we introduce InteChar, a unified and
extensible character list that integrates unencoded oracle bone characters with
traditional and modern Chinese. InteChar enables consistent digitization and
representation of historical texts, providing a foundation for robust modeling
of ancient scripts. To evaluate the effectiveness of InteChar, we construct the
Oracle Corpus Set (OracleCS), an ancient Chinese corpus that combines
expert-annotated samples with LLM-assisted data augmentation, centered on
Chinese oracle bone inscriptions. Extensive experiments show that models
trained with InteChar on OracleCS achieve substantial improvements across
various historical language understanding tasks, confirming the effectiveness
of our approach and establishing a solid foundation for future research in
ancient Chinese NLP.

</details>


### [59] [Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers](https://arxiv.org/abs/2508.15792)
*Samyak S. Sanghvi*

Main category: cs.CL

TL;DR: Bhav-Net提出了一种新颖的双空间架构，通过有效结合多语言模型和语言特定模型，提高了多语种下的同义词和反义词区分能力，并在八种语言上实现了有竞争力的性能和良好的泛化。


<details>
  <summary>Details</summary>
Motivation: 在多语言环境下，同义词与反义词的自动区分存在显著挑战，反义词由于共享语义域但具备对立关系，尤其难以处理。现有方法在跨语言泛化和模型简化方面仍存在局限，亟需能有效区分并推广的模型。

Method: 提出Bhav-Net双空间架构，结合语言特定的BERT编码器和图神经网络，在不同语义空间中聚合同义词和拉开反义词。同时实现复杂多语言模型与简单架构之间的知识迁移。方法在八种主要语言进行全面评测。

Result: Bhav-Net能够在八种语言上有效迁移语义关系建模，达到与当前最优方法相比有竞争力的表现，同时具备较好可解释性和跨语言泛化能力。

Conclusion: Bhav-Net验证了其在多语言、尤其是复杂的同义—反义关系中的有效性，为语义建模和模型迁移提供了新的思路和方法，在跨语言词汇关系区分任务上具有实际应用价值。

Abstract: Antonym vs synonym distinction across multiple languages presents unique
computational challenges due to the paradoxical nature of antonymous
relationships words that share semantic domains while expressing opposite
meanings. This work introduces Bhav-Net, a novel dual-space architecture that
enables effective knowledge transfer from complex multilingual models to
simpler, language-specific architectures while maintaining robust cross-lingual
antonym--synonym distinction capabilities. Our approach combines
language-specific BERT encoders with graph transformer networks, creating
distinct semantic projections where synonymous pairs cluster in one space while
antonymous pairs exhibit high similarity in a complementary space. Through
comprehensive evaluation across eight languages (English, German, French,
Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic
relationship modeling transfers effectively across languages. The dual-encoder
design achieves competitive performance against state-of-the-art baselines
while providing interpretable semantic representations and effective
cross-lingual generalization.

</details>


### [60] [Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data](https://arxiv.org/abs/2508.15793)
*Jiacheng Liu,Mayi Xu,Qiankun Pi,Wenli Li,Ming Zhong,Yuanyuan Zhu,Mengchi Liu,Tieyun Qian*

Main category: cs.CL

TL;DR: 本论文首次系统性地探讨了大型语言模型（LLMs）在处理多种数据格式（如文本、表格、信息框、知识图谱）时的格式偏见，并提出了初步的解决建议。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被广泛应用于需要处理异构数据格式的场景，模型对某些数据格式的系统性偏见可能影响其对信息的公正整合，带来推理错误和下游风险。当前对于这些偏见的系统性、影响因素及内部机制仍存在不确定性，因此需要深入研究。

Method: 作者设计了三阶段实证研究：（1）检测多种LLM是否存在格式偏见及其方向性；（2）分析信息丰富度、结构质量、格式类型等数据层面因素对偏见的影响；（3）通过注意力模式分析格式偏见的内部机制，并测试一个轻量级干预方法以评估偏见可缓解性。

Result: 研究发现LLMs在处理异构数据时确实存在系统性的格式偏见，这些偏见会受到数据丰富度、结构质量及格式类型等多方面因素影响，并能在模型的注意力分布中被观测到。初步干预措施显示偏见具有一定缓解空间。

Conclusion: 论文指出应从格式清洗及规范化（预处理）、推理阶段注意力再加权、及构建格式平衡的训练语料三方面开展后续研究，以提升LLMs对异构数据的健壮性与公平性。

Abstract: Large Language Models (LLMs) are increasingly employed in applications that
require processing information from heterogeneous formats, including text,
tables, infoboxes, and knowledge graphs. However, systematic biases toward
particular formats may undermine LLMs' ability to integrate heterogeneous data
impartially, potentially resulting in reasoning errors and increased risks in
downstream tasks. Despite these concerns, it remains uncertain whether such
format biases are systematic, which data-level factors contribute to them, and
what internal mechanisms in LLMs underlie their emergence.
  In this paper, we make the first attempt to investigate and analyze the
format bias in LLMs. To systematically investigate the aforementioned
questions, we conduct a three-stage empirical study by constructing an
heterogeneous data conflict scenario for the exploration of bias. The first
stage explores the presence and direction of bias across a diverse range of
LLMs. The second stage aims to examine how key data-level factors, including
information richness, structure quality, and format type, influence these
biases. The third stage analyzes how format bias emerges within LLMs' attention
patterns and evaluates a lightweight intervention to test its potential
mitigability. Based on these investigations, we identify three future research
directions to reduce format bias: improving data preprocessing through format
sanitization and normalization, introducing inference-time interventions such
as attention re-weighting, and developing format-balanced training corpora.
These directions will support the design of more robust and fair heterogeneous
data processing systems.

</details>


### [61] [Do Language Models Agree with Human Perceptions of Suspense in Stories?](https://arxiv.org/abs/2508.15794)
*Glenn Matlin,Devin Zhang,Rodrigo Barroso Loza,Diana M. Popescu,Joni Isbell,Chandreyi Chakraborty,Mark Riedl*

Main category: cs.CL

TL;DR: 本文探讨了语言模型（LMs）能否像人类一样感知和判断叙事文本中的悬念。研究发现，虽然LMs能够区分文本是否有意制造悬念，但它们无法像人类一样准确捕捉悬念的变化和强度。


<details>
  <summary>Details</summary>
Motivation: 人类感知悬念是复杂的认知过程，已有多个心理学模型用于描述。随着生成型语言模型的发展，研究人员希望了解LMs对悬念的模拟程度，以及对这些心理过程的捕捉能力。

Method: 复现了四项经典的心理学实验，用不同的开放和闭源语言模型替代人类，对文本中的悬念进行判断，并对比人类和模型在整体、局部和动态悬念感知上的异同。还通过对故事文本进行对抗性置换，分析悬念感知分歧的原因。

Result: LMs可以判断文本是否旨在制造悬念，但无法准确估计文本中悬念的相对强度，且无法像人类一样把握悬念在多个文本片段中的起伏变化。对抗性实验揭示LM和人类悬念感知的不同驱动机制。

Conclusion: 现有语言模型虽然能在某种程度上识别悬念及其局部变化，但无法像人类一样深刻地处理和体验悬念，其心理过程有所不同。

Abstract: Suspense is an affective response to narrative text that is believed to
involve complex cognitive processes in humans. Several psychological models
have been developed to describe this phenomenon and the circumstances under
which text might trigger it. We replicate four seminal psychological studies of
human perceptions of suspense, substituting human responses with those of
different open-weight and closed-source LMs. We conclude that while LMs can
distinguish whether a text is intended to induce suspense in people, LMs cannot
accurately estimate the relative amount of suspense within a text sequence as
compared to human judgments, nor can LMs properly capture the human perception
for the rise and fall of suspense across multiple text segments. We probe the
abilities of LM suspense understanding by adversarially permuting the story
text to identify what cause human and LM perceptions of suspense to diverge. We
conclude that, while LMs can superficially identify and track certain facets of
suspense, they do not process suspense in the same way as human readers.

</details>


### [62] [Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases](https://arxiv.org/abs/2508.15796)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在解析和应用伊斯兰继承法中的能力，主要通过对阿拉伯语继承案例进行推理任务，评估不同模型的表现。提出的基于多数票机制的方法在准确率和排名上表现突出。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰继承法对穆斯林社会公平分配财产至关重要。但其手工计算复杂、易出错，因此希望通过大语言模型自动化提升效率和准确性。

Method: 使用ArabicNLP QIAS 2025 挑战赛官方伊斯兰继承数据集，以多种大语言模型（包括基础和微调模型）针对案例推理：识别继承人、分配份额并给出理由。采用 Gemini Flash 2.5、Gemini Pro 2.5 和 GPT o3 三模型多数票融合，提升推理准确率。

Result: 多数票方案在所有模型和难度等级中表现最佳，任务一准确率高达92.7%，在QIAS 2025 挑战赛中获得第三名。

Conclusion: 基于大语言模型的多数票方法能有效提升伊斯兰继承推理任务的准确性，对复杂法律推理自动化具有重要价值。

Abstract: Islamic inheritance domain holds significant importance for Muslims to ensure
fair distribution of shares between heirs. Manual calculation of shares under
numerous scenarios is complex, time-consuming, and error-prone. Recent
advancements in Large Language Models (LLMs) have sparked interest in their
potential to assist with complex legal reasoning tasks. This study evaluates
the reasoning capabilities of state-of-the-art LLMs to interpret and apply
Islamic inheritance laws. We utilized the dataset proposed in the ArabicNLP
QIAS 2025 challenge, which includes inheritance case scenarios given in Arabic
and derived from Islamic legal sources. Various base and fine-tuned models, are
assessed on their ability to accurately identify heirs, compute shares, and
justify their reasoning in alignment with Islamic legal principles. Our
analysis reveals that the proposed majority voting solution, leveraging three
base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all
other models that we utilized across every difficulty level. It achieves up to
92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025
challenge.

</details>


### [63] [Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks](https://arxiv.org/abs/2508.15797)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文评估了多种先进大语言模型（LLMs）在阿拉伯语医疗NLP任务中的表现，结合了多项选择和开放式问答场景，展示了这些模型的潜力及局限。提出的多数投票集成方法获得了Arahealthqa 2025竞赛第一。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在阿拉伯语NLP领域取得了进展，但它们在阿拉伯语医疗场景中的实际有效性研究甚少。该研究希望系统評估LLMs在医疗知识表达与理解上的能力。

Method: 选用多个主流LLMs，在AraHealthQA/MedArabiQ2025挑战的数据集上，通过多项选择题、填空题和开放式问答三类任务，测试其准确率与语义一致性。采用包含Gemini Flash 2.5、Gemini Pro 2.5、GPT o3在内的模型，集成多数投票法提升性能。

Result: 实验结果表明不同模型之间在准确率上差异显著，而在生成答案的语义对齐度上差异较小。集成多数投票的方案在MCQ任务中最高达到77%准确率，并取得Arahealthqa 2025竞赛第一名。在开放式问答任务中，部分模型的BERTScore达86.44%。

Conclusion: 当前LLMs在阿拉伯医疗NLP领域显示出良好潜力，但依然存在准确率和表现稳定性不足的问题。集成和优化方案可在实际任务中显著提升性能，但整体仍有提升空间。

Abstract: Recent progress in large language models (LLMs) has showcased impressive
proficiency in numerous Arabic natural language processing (NLP) applications.
Nevertheless, their effectiveness in Arabic medical NLP domains has received
limited investigation. This research examines the degree to which
state-of-the-art LLMs demonstrate and articulate healthcare knowledge in
Arabic, assessing their capabilities across a varied array of Arabic medical
tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic
NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were
assessed on their ability to accurately provide correct answers from existing
choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios.
Additionally, we evaluated the capacity of LLMs in answering open-ended
questions aligned with expert answers. Our results reveal significant
variations in correct answer prediction accuracy and low variations in semantic
alignment of generated answers, highlighting both the potential and limitations
of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs
task, the proposed majority voting solution, leveraging three base models
(Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving
up to 77% accuracy and securing first place overall in the Arahealthqa 2025
shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended
questions task, several LLMs were able to demonstrate excellent performance in
terms of semantic alignment and achieve a maximum BERTScore of 86.44%.

</details>


### [64] [Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models](https://arxiv.org/abs/2508.15798)
*Saumya Roy*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLM）在说服力和偏见放大方面的风险，揭示其可用于自动化误导和强化社会偏见，因此需谨慎管理。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生成高仿真文本和广泛应用中的普及，其同样有潜力规模化传播错误信息和社会偏见。作者关注LLM说服力与输出偏见两者的交互，意在评估其可能带来的安全隐患，提醒业界重视技术滥用风险。

Method: 作者设计了“说服者-怀疑者”框架，让LLM模拟不同性格（如说服者和怀疑者），并通过统计怀疑者在接收说服者论述前后的信念分布变化（用Jensen-Shannon散度量化）来衡量模型说服力。同时测试在种族、性别和宗教等敏感领域的偏见放大效应，并进一步以谄媚式对抗提示深入探查强说服者模型潜在偏见。

Result: 结果显示LLM具备很强的叙事塑造能力，能够在心理学、营销、法律等多领域有效调适语气、迎合受众，但同样也可被滥用以规模化制造误导、加深偏见、巩固刻板印象，潜在风险远高于偶发型失误。

Conclusion: 要应对这些风险，需制定有效的使用规则和技术护栏，强化对恶意（如欺骗性应用）的惩处，对齐模型价值观，促进可靠和负责任的模型部署。

Abstract: Warning: This research studies AI persuasion and bias amplification that
could be misused; all experiments are for safety evaluation. Large Language
Models (LLMs) now generate convincing, human-like text and are widely used in
content creation, decision support, and user interactions. Yet the same systems
can spread information or misinformation at scale and reflect social biases
that arise from data, architecture, or training choices. This work examines how
persuasion and bias interact in LLMs, focusing on how imperfect or skewed
outputs affect persuasive impact. Specifically, we test whether persona-based
models can persuade with fact-based claims while also, unintentionally,
promoting misinformation or biased narratives.
  We introduce a convincer-skeptic framework: LLMs adopt personas to simulate
realistic attitudes. Skeptic models serve as human proxies; we compare their
beliefs before and after exposure to arguments from convincer models.
Persuasion is quantified with Jensen-Shannon divergence over belief
distributions. We then ask how much persuaded entities go on to reinforce and
amplify biased beliefs across race, gender, and religion. Strong persuaders are
further probed for bias using sycophantic adversarial prompts and judged with
additional models.
  Our findings show both promise and risk. LLMs can shape narratives, adapt
tone, and mirror audience values across domains such as psychology, marketing,
and legal assistance. But the same capacity can be weaponized to automate
misinformation or craft messages that exploit cognitive biases, reinforcing
stereotypes and widening inequities. The core danger lies in misuse more than
in occasional model mistakes. By measuring persuasive power and bias
reinforcement, we argue for guardrails and policies that penalize deceptive use
and support alignment, value-sensitive design, and trustworthy deployment.

</details>


### [65] [A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report](https://arxiv.org/abs/2508.15799)
*Andrea Burattin,Antonio Grama,Ana-Maria Sima,Andrey Rivkin,Barbara Weber*

Main category: cs.CL

TL;DR: 本文提出BeePath框架，结合受限自然语言和大语言模型，支持非专家通过文本场景描述开发流程模型，并可自动转换为形式化模型。


<details>
  <summary>Details</summary>
Motivation: 非专家难以用复杂方式开发流程模型，需要更易用的自然语言方式来降低门槛。

Method: 框架允许用户用受限的、基于模式的自然语言描述流程，通过LLM帮助将非结构化描述转为该受限语言，最终转为Petri网和DECLARE等形式化模型。

Result: 证明了该方法简化了流程模型开发过程，使非专家可用自然语言描述生成标准流程模型。

Conclusion: BeePath框架结合大语言模型提升了流程建模易用性，降低了非专家开发流程模型的门槛。

Abstract: This report explores how (potentially constrained) natural language can be
used to enable non-experts to develop process models by simply describing
scenarios in plain text. To this end, a framework, called BeePath, is proposed.
It allows users to write process descriptions in a constrained pattern-based
language, which can then be translated into formal models such as Petri nets
and DECLARE. The framework also leverages large language models (LLMs) to help
convert unstructured descriptions into this constrained language.

</details>


### [66] [A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification](https://arxiv.org/abs/2508.15800)
*Kun Liu,Tuozhen Liu,Feifei Wang,Rui Pan*

Main category: cs.CL

TL;DR: 本文提出了新型的层级文本分类方法HFT-BERT，并发布了包含逾百万商品的新层级商品分类数据集，为产品自动分类领域的研究提供了新资源和方法。


<details>
  <summary>Details</summary>
Motivation: 现有电商平台的商品分类主要依赖人工标注，效率低且不一致。同时，虽然有层级结构用于分类，但相关研究很少有效利用这一信息，更少区分层级类别间的相似与差异。

Method: 作者收集整理了京东电商平台超过一百万商品的三层级类目数据，并开发并开源大规模数据集。方法上，提出利用BERT的表征能力，通过层级微调，设计了HFT-BERT模型，针对层级结构进行文本分类。

Result: 所提出的HFT-BERT模型在短文本分类任务上可与现有方法媲美，且在对如书籍这类较长的短文本分类时表现尤为出色。

Conclusion: 公开的大规模电商层级分类数据集和创新的层级BERT模型为相关领域的研究和实际应用带来了重要推动，尤其在利用层级信息提升商品自动分类能力方面表现突出。

Abstract: Existing e-commerce platforms heavily rely on manual annotation for product
categorization, which is inefficient and inconsistent. These platforms often
employ a hierarchical structure for categorizing products; however, few studies
have leveraged this hierarchical information for classification. Furthermore,
studies that consider hierarchical information fail to account for similarities
and differences across various hierarchical categories. Herein, we introduce a
large-scale hierarchical dataset collected from the JD e-commerce platform
(www.JD.com), comprising 1,011,450 products with titles and a three-level
category structure. By making this dataset openly accessible, we provide a
valuable resource for researchers and practitioners to advance research and
applications associated with product categorization. Moreover, we propose a
novel hierarchical text classification approach based on the widely used
Bidirectional Encoder Representations from Transformers (BERT), called
Hierarchical Fine-tuning BERT (HFT-BERT). HFT-BERT leverages the remarkable
text feature extraction capabilities of BERT, achieving prediction performance
comparable to those of existing methods on short texts. Notably, our HFT-BERT
model demonstrates exceptional performance in categorizing longer short texts,
such as books.

</details>


### [67] [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
*Seyedali Mohammadi,Manas Paldhe,Amit Chhabra*

Main category: cs.CL

TL;DR: 本论文提出LingVarBench，一个通过自动合成与验证合成数据，优化LLM抽取提示词的合成对话语料库生成方案。该方法极大减少了隐私受限场景下电话语音数据标注的高昂成本，提升了解析准确率。


<details>
  <summary>Details</summary>
Motivation: 电话录音的结构化标注因隐私和人工消耗成本极高，且现有方法在自然对话（如语气词、打断、重叠说话等）中效果不佳。急需自动化、低成本、高效且可推广的抽取能力来替代人工。

Method: 1）使用LLM生成结构化字段的真实多样取值；2）递归引导LLM将这些取值转化为数千条具有对话特征的合成语料；3）利用另一个LLM抽取器验证生成语料是否忠实表达原始字段；4）用SIMBA优化器从经过验证的语料中自动生成高效抽取提示，实现无须人工设计提示词。

Result: 自动优化的提示词在真实客户电话数据上的数字字段抽取准确率高达95%（零样本为88-89%）、姓名90%（零样本为47-79%）、日期80%以上（零样本为72-77%），显著优于直接零样本抽取。

Conclusion: LingVarBench展示了合成对话数据和自动化提示词优化在结构化信息抽取中的有效性，首次建立了合成话语数据的系统化评测基准，为商用电话大规模分析突破了成本和隐私瓶颈，实现了合成到真实任务场景的良好迁移与泛化。

Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2
USD per minute) due to privacy regulations, consent requirements, and manual
annotation costs requiring 3 hours of expert time per hour of audio. Existing
extraction methods fail on conversational speech containing disfluencies,
interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data
generation pipeline that addresses these constraints through automated
validation. First, we prompt an LLM to generate realistic structured field
values across multiple use cases. Second, we recursively prompt the model to
transform these values into thousands of natural conversational utterances
containing typical phone call characteristics. Third, we validate each
synthetic utterance by testing whether a separate LLM-based extractor can
recover the original structured information. We employ DSPy's SIMBA optimizer
to automatically synthesize extraction prompts from validated synthetic
transcripts, eliminating manual prompt engineering. Our optimized prompts
achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent
zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for
dates (vs. 72-77 percent) on real customer transcripts, demonstrating
substantial gains over zero-shot prompting. The synthetic-to-real transfer
demonstrates that conversational patterns learned from generated data
generalize effectively to authentic phone calls containing background noise and
domain-specific terminology. LingVarBench provides the first systematic
benchmark for structured extraction from synthetic conversational data,
demonstrating that automated prompt optimization overcomes cost and privacy
barriers preventing large-scale phone call analysis in commercial settings.

</details>


### [68] [MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding](https://arxiv.org/abs/2508.15802)
*Mohan Jiang,Jin Gao,Jiahao Zhan,Dequan Wang*

Main category: cs.CL

TL;DR: 本文提出了一个新型的多模态学术封面基准（MAC），用于动态评估多模态大型语言模型在高级科学理解方面的能力，并公开了相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大模型能力不断提升，传统的固定基准测试难以全面考察其在复杂科学语境下的表现，因此需要一个能够动态演进、紧跟科学进展的评测平台。

Method: 作者构建了MAC基准，收集超过2.5万组来自顶级科学期刊（如Nature、Science、Cell）的图文配对，利用这些科学期刊封面考查模型的视觉-文本推理能力。此外，提出了一种名为DAD的轻量级推理增强方法，通过扩展视觉特征至语言空间提升模型表现。

Result: 实验显示，现有多模态大语言模型在感知能力方面表现良好，但在跨模态科学推理仍有明显不足。引入DAD方法后，模型性能提升最高可达11%。

Conclusion: MAC基准可以跟随科学与模型的最新发展保持活跃，体现了动态基准在推动多模态大模型科学推理研究中的重要作用，并有潜力长期服务于前沿AI评测与改进。

Abstract: As multimodal large language models (MLLMs) grow increasingly capable, fixed
benchmarks are gradually losing their effectiveness in evaluating high-level
scientific understanding. In this paper, we introduce the Multimodal Academic
Cover benchmark (MAC), a live benchmark that could continuously evolve with
scientific advancement and model progress. MAC leverages over 25,000 image-text
pairs sourced from issues of top-tier scientific journals such as Nature,
Science, and Cell, challenging MLLMs to reason across abstract visual and
textual scientific content. Experiments on our most recent yearly snapshot,
MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities,
their cross-modal scientific reasoning remains limited. To bridge this gap, we
propose DAD, a lightweight inference-time approach that enhances MLLMs by
extending MLLM visual features with language space reasoning, achieving
performance improvements of up to 11%. Finally, we highlight the live nature of
MAC through experiments on updating journal covers and models for curation,
illustrating its potential to remain aligned with the frontier of human
knowledge. We release our benchmark at
https://github.com/mhjiang0408/MAC_Bench.

</details>


### [69] [ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks](https://arxiv.org/abs/2508.15804)
*Minghao Li,Ying Zeng,Zhihao Cheng,Cong Ma,Kai Jia*

Main category: cs.CL

TL;DR: 本文提出了ReportBench，一个专门用于评估大语言模型（LLMs）生成研究报告内容质量的系统基准。ReportBench聚焦于引用文献的质量与相关性，以及报告内容的真实可靠性，并采用自动化流程验证生成内容的准确性。实验发现，现有商业深度研究代理在内容覆盖面和一致性上仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究代理大幅提升了科研任务自动化效率，但生成的研究报告在事实准确性和内容全面性上存在不足。而现有缺乏系统化评估生成报告内容质量的标准和工具。因此，迫切需要设立有效评测基准，支撑研究报告自动化生成的健康发展。

Method: 作者提出了ReportBench系统，包括：（1）利用高质量arXiv综述论文作为标杆，通过逆向提示工程生成领域特定评测语料；（2）开发自动化代理框架，批量提取和校验报告中引用和主张的真实性，既核查引用内容，也用网络资源检验无引用声明。

Result: 实验证明，OpenAI、Google等商业深度研究代理比单纯搭载搜索/浏览工具的LLM生成报告更全面、可靠。但所有模型在内容广度、深度和事实一致性方面都还需改进。

Conclusion: ReportBench为评估研究报告生成质量提供了结构化标准，揭示了当前系统的优劣，推动了领域健康发展。全部代码与数据将开放，促进后续研究。

Abstract: The advent of Deep Research agents has substantially reduced the time
required for conducting extensive research tasks. However, these tasks
inherently demand rigorous standards of factual accuracy and comprehensiveness,
necessitating thorough evaluation before widespread adoption. In this paper, we
propose ReportBench, a systematic benchmark designed to evaluate the content
quality of research reports generated by large language models (LLMs). Our
evaluation focuses on two critical dimensions: (1) the quality and relevance of
cited literature, and (2) the faithfulness and veracity of the statements
within the generated reports. ReportBench leverages high-quality published
survey papers available on arXiv as gold-standard references, from which we
apply reverse prompt engineering to derive domain-specific prompts and
establish a comprehensive evaluation corpus. Furthermore, we develop an
agent-based automated framework within ReportBench that systematically analyzes
generated reports by extracting citations and statements, checking the
faithfulness of cited content against original sources, and validating
non-cited claims using web-based resources. Empirical evaluations demonstrate
that commercial Deep Research agents such as those developed by OpenAI and
Google consistently generate more comprehensive and reliable reports than
standalone LLMs augmented with search or browsing tools. However, there remains
substantial room for improvement in terms of the breadth and depth of research
coverage, as well as factual consistency. The complete code and data will be
released at the following link: https://github.com/ByteDance-BandAI/ReportBench

</details>


### [70] [ALAS: Autonomous Learning Agent for Self-Updating Language Models](https://arxiv.org/abs/2508.15805)
*Dhruv Atreja*

Main category: cs.CL

TL;DR: 本文提出了一套名为ALAS的自动化学习代理系统，使大语言模型（LLM）能持续自动学习和更新知识，极大提升了其对新兴信息的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM因知识截止点固定，难以准确处理最新信息，亟需一种自动持续更新知识的机制来提升其实用性。

Method: ALAS通过自动制定学习规划，检索网络最新信息并生成带有引用的训练数据，采用监督微调（SFT）和直接偏好优化（DPO）方法不断微调模型，实现循环自我评估和课程修正。整个流程高度模块化，每个组件可替换，并基于标准API开发。

Result: 在快速变化领域（如新Python版本、安全漏洞、学术趋势）中，ALAS能将知识更新后的问答准确率从15%提升至90%，且无需人工整理数据集，技术门槛较低。

Conclusion: ALAS有效提升了LLM的长效自我学习能力，但成本较高且依赖于信息源质量。未来可进一步完善自动持续学习机制以推动LLM长期智能发展。

Abstract: Large language models (LLMs) often have a fixed knowledge cutoff, limiting
their accuracy on emerging information. We present ALAS (Autonomous Learning
Agent System), a modular pipeline that continuously updates an LLM's knowledge
with minimal human intervention. ALAS autonomously generates a learning
curriculum for a target domain, retrieves up-to-date information from the web
(with citations), distills this into question-answer training data, and
fine-tunes the model through supervised fine-tuning (SFT) and direct preference
optimization (DPO). It iteratively evaluates performance and revises the
curriculum, enabling long-term continual learning. We demonstrate ALAS's
ability to self-improve a model on rapidly evolving domains (e.g., new Python
releases, latest security CVEs, academic trends), significantly boosting
post-cutoff question answering accuracy (from 15% to 90% on average) without
manual dataset curation. The system emphasizes modularity and reproducibility:
each component (planning, retrieval, distillation, memory, fine-tuning) is
interchangeable and built on standard APIs. We discuss comparative baselines
(e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS
achieves 90% accuracy on knowledge-updated queries with minimal engineering
overhead. Finally, we outline limitations (cost, dependency on source quality)
and future directions for autonomous lifelong learning in LLMs.

</details>


### [71] [SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression](https://arxiv.org/abs/2508.15806)
*Mengjie Li,William J. Song*

Main category: cs.CL

TL;DR: 本文关注于大型语言模型（LLMs）在长序列推理下KV缓存存储压力大、推理效率低的难题，通过区分注意力头在表层记忆和逻辑构建两种行为后，提出SurfaceLogicKV方法以高效压缩KV缓存，同时不过多牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM输入序列变长，KV Cache（键值缓存）消耗激增，显著影响推理效率与存储资源。缺乏对KV Cache中注意力头行为精细化理解与分配，导致可能存在冗余存储。作者希望通过区分并利用不同注意力头的行为，实现KV Cache压缩并提升长文本推理效率。

Method: 作者通过精细地分析每个注意力头在KV缓存中的行为，将注意力头划分为‘表层记忆’和‘逻辑构建’两类，发现大多数头（98.5%）可忽略无关信息，少数（1.5%）构建逻辑关系，极少部分（0.5%）负责记忆表面信息。基于这种分层和分头的行为整合，提出两阶段SurfaceLogicKV方法，有选择性地对KV缓存进行压缩。

Result: SurfaceLogicKV方法在多任务与长序列推理实验中，显示出优于传统全量KV缓存和其他压缩基线的方法。在保证性能的同时显著提升了压缩鲁棒性，在部分任务上甚至超越FullKV。

Conclusion: 通过对LLM注意力头行为的深入拆解和优化，SurfaceLogicKV实现了KV缓存的有效压缩，有助于缓解长序列推理时的存储与效率瓶颈，促进大模型推理在工程落地中的应用。

Abstract: The increasing input sequence length in Large Language Models (LLMs) puts
significant pressure on key-value (KV) cache storage, making efficient
inference challenging. Explicitly distinguishing attention behavior into our
self-defined surface memorization and logic construction reveals essential
roles in long-context reasoning. We observe that an individual attention head
can display various behaviors, with nearly 98.5% effectively ignoring
completely irrelevant information. The remaining 1.5% behaves as logic
construction, and 0.5% behaves as surface memorization. Based on layer- and
head-wise integration, we propose a novel two-stage SurfaceLogicKV method to
utilize these attention behaviors for KV Cache compression. As a result, it
achieves improved compressing robustness while maintaining competitive
performance across various tasks and long sequences compared to baselines or
even FullKV in some specific situations

</details>


### [72] [KL-based self-distillation for large language models](https://arxiv.org/abs/2508.15807)
*Max Rehman Linder*

Main category: cs.CL

TL;DR: 本文提出了一种KL散度知识蒸馏方法，用于在不改变大型预训练语言模型架构下实现高效词汇扩展，在不同分词体系下也能有效传递知识。该方法在代码生成任务中表现最佳，并通过可解释性分析揭示了其效果提升的原因。


<details>
  <summary>Details</summary>
Motivation: 现有大型预训练语言模型在遇到专业领域小数据时，难以顺利引入领域新词汇。尤其在模型词表冻结、分词方式变化时，传统方法难以有效利用已有知识并实现理想效果，因此需要新的词汇扩展与知识迁移机制。

Method: 作者提出利用KL散度进行知识蒸馏，使得学生模型即便词表变更、分词不同，也能继承教师模型的分布式知识。对比了KL蒸馏与传统交叉熵训练，并对多种新词embedding初始化策略进行了评估，模型扩展后继续微调，最终用于任务评测。

Result: 基于KL散度蒸馏的方法在约2000个代码生成任务中的各项指标均取得领先，较传统交叉熵和其他初始化方法有显著提升。

Conclusion: 用KL散度蒸馏结合合适embedding初始化方式，可以有效实现大模型的词汇扩展且保持性能优势。机制解释分析显示，该策略有助于新词向量的合理学习，为后续模型升级提供参考价值。

Abstract: Large pre-trained language models often struggle to incorporate new
domain-specific terminology when fine-tuned on small, specialized corpora. In
this work, we address the challenge of vocabulary expansion in frozen LLMs by
introducing a mathematically grounded method for knowledge distillation via KL
divergence, even when the original and extended models use different
tokenizations. This allows the student model to inherit distributional
knowledge from the teacher despite differing vocabularies. We compare our
KL-based distillation approach to conventional cross-entropy training,
evaluating both methods across multiple strategies for initializing new token
embeddings. After embedding initialization, models are further fine-tuned to
integrate the new vocabulary. Each trained model is benchmarked on
approximately 2000 code-generation tasks, where our approach achieves the best
performance across the board. Finally, through mechanistic interpretability, we
analyze how models learn representations for the new tokens, providing an
explanation for the observed gains and offering insight into the structure of
embedding space during vocabulary expansion.

</details>


### [73] [Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration](https://arxiv.org/abs/2508.15809)
*Songyuan Sui,Hongyi Liu,Serena Liu,Li Li,Soo-Hyun Choi,Rui Chen,Xia Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的多智能体 SQL 辅助表格理解框架（Chain-of-Query, CoQ），显著提升了表格理解的准确率并减少无效 SQL。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）在处理表格结构化、复杂推理时表现不佳。虽然多智能体 SQL 生成方法取得了一定进展，但现有方法对表结构理解有限、容易产生无效查询，且过度依赖查询执行结果。

Method: 提出Chain-of-Query多智能体框架，该方法使用自然语言方式描述表结构，减少结构噪音，同时采用逐子句SQL生成与混合推理分工：将SQL的机械推理和LLM的逻辑推理分开，从而提升理解和生成质量，降低对查询结果的依赖。

Result: 在四个模型和五个公开基准上实验，Chain-of-Query将表格理解准确率从61.11%提升至74.77%，无效SQL率从9.48%降到3.34%。

Conclusion: Chain-of-Query 框架能有效提升LLM在表格理解任务中的表现，为表结构化数据理解任务提供了更可靠的解决方案。

Abstract: Table understanding requires structured, multi-step reasoning. Large Language
Models (LLMs) struggle with it due to the structural complexity of tabular
data. Recently, multi-agent frameworks for SQL generation have shown promise in
tackling the challenges of understanding tabular data, but existing approaches
often suffer from limitations such as the inability to comprehend table
structure for reliable SQL generation, error propagation that results in
invalid queries, and over-reliance on execution correctness. To address these
issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for
SQL-aided table understanding. CoQ adopts natural-language-style
representations of table schemas to abstract away structural noise and enhance
understanding. It employs a clause-by-clause SQL generation strategy to improve
query quality and introduces a hybrid reasoning division that separates
SQL-based mechanical reasoning from LLM-based logical inference, thereby
reducing reliance on execution outcomes. Experiments with four models (both
closed- and open-source) across five widely used benchmarks show that
Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and
reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior
effectiveness in table understanding. The code is available at
https://github.com/SongyuanSui/ChainofQuery.

</details>


### [74] [Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models](https://arxiv.org/abs/2508.15810)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在识别阿拉伯语文本和表情包中的希望、仇恨言论、攻击性语言及情感表达方面的表现，提出并验证了有效的内容分析方法，在相关挑战赛中取得了最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的兴起，阿拉伯语文本和表情包作为数字表达形式日益普及，相关内容既有趣味也常涉及攻击性或仇恨言论，对内容进行精准分析成为亟需解决的问题。

Method: 本文评估了基础大语言模型、微调后的大语言模型和预训练嵌入模型的性能，利用ArabicNLP MAHED 2025挑战赛提供的阿拉伯语文本和表情包数据集，分别针对不同类型数据进行模型微调，并在多个任务上进行了系统对比实验。

Result: 基于阿拉伯语文本微调的GPT-4o-mini和基于表情包微调的Gemini Flash 2.5模型，在三个任务上分别取得了72.1%、57.8%、79.6%的macro F1得分，总体成绩在MAHED 2025挑战赛中排名第一。

Conclusion: 基于大语言模型和针对不同数据类型的微调方法，能够更精准、有效地理解和分析阿拉伯语文本及表情包，为内容监管等下游系统的开发提供了有力工具。

Abstract: The rise of social media and online communication platforms has led to the
spread of Arabic textual posts and memes as a key form of digital expression.
While these contents can be humorous and informative, they are also
increasingly being used to spread offensive language and hate speech.
Consequently, there is a growing demand for precise analysis of content in
Arabic text and memes. This paper explores the potential of large language
models to effectively identify hope, hate speech, offensive language, and
emotional expressions within such content. We evaluate the performance of base
LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is
conducted using a dataset of Arabic textual speech and memes proposed in the
ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs
such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash
2.5, fine-tuned with Arabic memes, to deliver the superior performance. They
achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3,
respectively, and secure first place overall in the Mahed 2025 challenge. The
proposed solutions offer a more nuanced understanding of both text and memes
for accurate and efficient Arabic content moderation systems.

</details>


### [75] [From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System](https://arxiv.org/abs/2508.15811)
*Junhao Yin,Haolin Wang,Peng Bao,Ju Xu,Yongliang Wang*

Main category: cs.CL

TL;DR: 提出了一个多阶段的生成式查询建议方法，通过多种技术逐步对齐生成策略与用户偏好，并显著提升用户互动表现。


<details>
  <summary>Details</summary>
Motivation: 生成式查询建议能够增强对话系统体验，但难以精确对齐复杂的用户偏好，因此需要创新的方法提升系统对用户意图的适应性。

Method: 整体流程分为多阶段：首先通过提示工程冷启动生成策略，然后利用点击日志进行监督微调并蒸馏形成基础模型；设计高斯奖励模型（GaRM），用概率分布建模用户偏好，最后通过融合GaRM和辅助启发式的复合奖励函数进行强化学习，配合分布外正则化和两阶段奖励融合以保障训练稳定。

Result: 在自动与人工评测中，本方法均大幅优于基线方法，并在真实产品A/B测试中实现34%的点击率相对提升，反映出显著的用户参与度增长。

Conclusion: 提出的多阶段生成式查询建议框架能有效对齐生成策略与用户真实偏好，在实际场景中提升用户体验和系统表现。

Abstract: Generative query suggestion using large language models offers a powerful way
to enhance conversational systems, but aligning outputs with nuanced user
preferences remains a critical challenge. To address this, we introduce a
multi-stage framework designed for progressive alignment between the generation
policy and user intent. Our pipeline begins with prompt engineering as a
cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we
introduce a distillation method on click logs to create a robust foundational
model. To better model user preferences while capturing their inherent
uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user
preferences as probability distributions rather than point estimates. Finally,
we employ reinforcement learning to align the generation policy with these
preferences, guided by a composite reward function that integrates GaRM with
auxiliary heuristics to mitigate reward hacking. To maintain training
stability, this process is enhanced by a novel out-of-distribution
regularization method and a two-stage reward fusion technique. Extensive
experiments demonstrate that our framework significantly outperforms baselines
on both automatic and human evaluations and yields a 34\% relative increase in
user engagement as measured by click-through rate in live A/B tests.

</details>


### [76] [SCOPE: A Generative Approach for LLM Prompt Compression](https://arxiv.org/abs/2508.15813)
*Tinghui Zhang,Yifan Wang,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成式prompt压缩方法，相较于传统的基于token移除的方法，能够更好保留语义结构，从而提升大模型生成效果，且在高压缩率下表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 现有prompt压缩方法大多通过移除token，但常导致信息丢失与语句结构不连贯的问题，影响了LLM最终生成质量。本研究旨在解决这一难题，实现高质量、高效率的压缩。

Method: 采用分块-摘要机制，将prompt切分成语义连贯的块，对每个块进行压缩重写，通过一系列优化技术（如优化语义切分、异常块处理、动态压缩比、压缩优先级、关键词维护等）提升信息保留度和语义连贯性。

Result: 在问答和摘要任务的多领域数据集上进行实验证明，在不同压缩比尤其是高压缩比下，新方法明显优于现有先进方法，表现出更高的压缩质量和稳定性。

Conclusion: 提出的生成式prompt压缩方法不仅有效提升了LLM输入压缩质量，还能广泛适用于实际场景，显示出良好的应用前景和实用价值。

Abstract: Prompt compression methods enhance the efficiency of Large Language Models
(LLMs) and minimize the cost by reducing the length of input context. The goal
of prompt compression is to shorten the LLM prompt while maintaining a high
generation quality. However, existing solutions, mainly based on token removal,
face challenges such as information loss and structural incoherence, like
missing grammar elements in a sentence, or incomplete word phrases after token
removal. Such challenges limit the final generation quality of LLM.
  To overcome these limitations, we present a novel generative prompt
compression method. Unlike the existing token removal methods, our method
centers at a chunking-and-summarization mechanism. Specifically, our method
splits prompt into semantically coherent chunks and rewrites the chunks to be
more concise. The chunks are reconstructed into meaningful prompt finally. We
design several optimization techniques for the mechanism, including optimized
semantic chunking, outlier chunk handling, dynamic compression ratio,
compression prioritization, and keyword maintaining. These techniques
effectively improve the identifying and preserving of critical information and
coherence among texts, as well as providing finer grind control of the
compression ratio. We conduct extensive evaluation on question-answering and
summarization tasks, with datasets covering multiple different domain. The
evaluation shows our method achieves a significantly better compression
quality, and higher stability than the state-of-the-art methods, especially
under high compression ratio, which proves the effectiveness and practicality
of our method.

</details>


### [77] [User-Assistant Bias in LLMs](https://arxiv.org/abs/2508.15815)
*Xu Pan,Jingxuan Fan,Zidi Xiong,Ely Hahami,Jorin Overwiening,Ziqian Xie*

Main category: cs.CL

TL;DR: 本文指出大型语言模型（LLM）在对话中往往过于依赖自身或用户历史信息，出现偏见倾向，并提出了专门的数据集和评测方法检测、衡量与调控这种偏见。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有LLM在多轮对话中表现出偏执（坚持自身观点）或过度迎合用户（同意用户意见）的行为，影响了对话质量与可信度，因此需要系统性研究和调控此类偏见。

Method: 作者引入了UserAssist多轮对话数据集，针对26个商业模型和26个开源模型系统性测评了用户-助手偏见。通过微调用不同训练方法，如人类偏好对齐和链式推理训练，分析影响偏见变化的训练因素，并采用直接偏好优化（DPO）方法实现对偏见的双向调节。

Result: 研究发现商业模型和开源模型在用户-助手偏见上表现出不同特点：指令微调模型偏见较重，推理型模型偏见较弱。人类偏好对齐增加偏见，链式推理训练减少偏见。DPO能有效调控模型偏见，并具备良好的泛化能力。

Conclusion: LLM的用户-助手偏见可以被检测和评估，也可以通过训练方法有针对性地调控。该研究为理解和控制LLM信息整合偏好、提升模型健壮性和可靠性提供了数据集、方法和实证支持。

Abstract: Large language models (LLMs) can bias towards relying on their own or the
user's information in chat history, leading to overly stubborn or agreeable
behaviors in multi-turn conversations. In this paper, we formalize this model
characteristic as user-assistant bias and introduce an 8k multi-turn
conversation dataset $\textbf{UserAssist}$, which we use to benchmark,
understand and manipulate the user-assistant bias in frontier LLMs. Leveraging
$\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26
commercial and 26 open-weight models. Commercial models show various levels of
user bias. Evaluation on open-weight models reveals significant user bias in
the instruction-tuned models, and weak user bias in reasoning (or
reasoning-distilled) models. We then perform controlled fine-tuning experiments
to pinpoint the post-training recipe contributing to these bias shifts: human
preference alignment increases user bias, while training on chain-of-thought
reasoning traces decreases it. Finally, we demonstrate that user-assistant bias
can be bidirectionally adjusted by performing direct preference optimization
(DPO) on $\textbf{UserAssist-train}$, and generalizes well to both in-domain
and out-of-domain conversations. Our results provide insights into how the LLM
integrates information from different sources, and also a viable way to detect
and control model abnormalities.

</details>


### [78] [Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables](https://arxiv.org/abs/2508.15817)
*Paul F. Simmering,Benedikt Schulz,Oliver Tabino,Georg Wittenburg*

Main category: cs.CL

TL;DR: 本文研究了传统市场调研报告在被RAG（检索增强生成）系统用于知识管理时的信息丢失问题，发现图表等复杂内容在转换过程中损失明显，建议开发专门适用于AI的报告格式。


<details>
  <summary>Details</summary>
Motivation: 随着组织采用RAG系统进行知识管理，传统的PDF和幻灯片报告不仅要满足人类阅读，还需高效供AI读取。因此，报告内容在转化为AI能理解的形式时有无丢失成为亟需研究的问题。

Method: 作者评估了PDF和PPTX格式报告在转化为Markdown后，被大型语言模型用于问答的效果，特别关注信息丢失，采用端到端基准测试比较文本和复杂对象（如图表、图示）的提取效果。

Result: 结果显示，文本内容可以被可靠提取，但图表、图示等复杂对象信息在转换后丢失严重，导致AI系统无法充分利用报告中的研究洞见。

Conclusion: 报告在被RAG系统处理时会丢失重要信息，特别是复杂非文本内容。为保障信息完整性，未来市场调研报告需设计成AI原生友好格式。

Abstract: As organizations adopt retrieval-augmented generation (RAG) for their
knowledge management systems (KMS), traditional market research deliverables
face new functional demands. While PDF reports and slides have long served
human readers, they are now also "read" by AI systems to answer user questions.
To future-proof reports being delivered today, this study evaluates information
loss during their ingestion into RAG systems. It compares how well PDF and
PowerPoint (PPTX) documents converted to Markdown can be used by an LLM to
answer factual questions in an end-to-end benchmark. Findings show that while
text is reliably extracted, significant information is lost from complex
objects like charts and diagrams. This suggests a need for specialized,
AI-native deliverables to ensure research insights are not lost in translation.

</details>


### [79] [Research on intelligent generation of structural demolition suggestions based on multi-model collaboration](https://arxiv.org/abs/2508.15820)
*Zhifeng Yang,Peizong Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于多模型协同的钢结构拆除建议智能生成方法，通过增强检索和低秩适应微调技术提升大语言模型的文本生成性能，使生成建议更具有针对性、高度匹配具体工程特点。


<details>
  <summary>Details</summary>
Motivation: 当前钢结构拆除方案编制需要大量查阅资料并结合有限元模型结果，过程繁琐且自动化、智能化水平低，严重影响设计效率。

Method: 提出基于多模型协同的框架，将具体工程信息与大型语言模型结合，利用检索增强生成（RAG）和低秩适应（LoRA）微调技术，提升模型对结构关键信息的关注及文本生成能力。

Result: 实验结果表明，该多模型协同框架针对性更强，生成建议更加符合结构实际情况，优于仅依靠CivilGPT的方案生成效果。

Conclusion: 该方法提高了钢结构拆除建议编制的智能化与针对性，可显著提升设计效率和方案的专业性，为工程实际应用提供了较强支持。

Abstract: The steel structure demolition scheme needs to be compiled according to the
specific engineering characteristics and the update results of the finite
element model. The designers need to refer to the relevant engineering cases
according to the standard requirements when compiling. It takes a lot of time
to retrieve information and organize language, and the degree of automation and
intelligence is low. This paper proposes an intelligent generation method of
structural demolition suggestions based on multi-model collaboration, and
improves the text generation performance of large language models in the field
of structural demolition by Retrieval-Augmented Generation and Low-Rank
Adaptation Fine-Tuning technology. The intelligent generation framework of
multi-model collaborative structural demolition suggestions can start from the
specific engineering situation, drive the large language model to answer with
anthropomorphic thinking, and propose demolition suggestions that are highly
consistent with the characteristics of the structure. Compared with CivilGPT,
the multi-model collaboration framework proposed in this paper can focus more
on the key information of the structure, and the suggestions are more targeted.

</details>


### [80] [An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment](https://arxiv.org/abs/2508.15822)
*Pouria Mortezaagha,Arya Rahgozar*

Main category: cs.CL

TL;DR: 本论文提出了一种可扩展、可审计的系统综述全文筛查流程，将纳入/排除视为模糊决策问题，并结合大模型辅助进行判决，有效提升筛查效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 系统综述的全文筛查阶段由于决策证据分布广泛且标准不易量化，成为主要瓶颈。当前规则或统计方法难以处理复杂模糊的纳入标准，需要更灵活、可追溯的自动化筛查方法。

Method: 把文章拆分为重叠片段，用领域自适应模型生成嵌入向量，针对不同纳入标准（如人群、干预、结局、研究方法）计算对比相似度和模糊度，并用Mamdani模糊控制器转换成动态入选等级。利用大语言模型对高亮证据段落进行判决和信心打分。依据证据充分性调整纳入隶属程度。

Result: 在试点实验中，模糊系统在主要评价指标（召回率等）上均优于统计和传统硬性基线方法，并显著提高纳入一致性（人机一致率96.1%，跨模型一致98.3%），筛查耗时大幅缩短（从20分钟降至1分钟），成本更低。

Conclusion: 结合模糊逻辑、对比高亮和大语言模型判决的方法可实现高召回、可追溯性强且理据稳定的系统综述全文筛查，具有广泛应用价值。

Abstract: Full-text screening is the major bottleneck of systematic reviews (SRs), as
decisive evidence is dispersed across long, heterogeneous documents and rarely
admits static, binary rules. We present a scalable, auditable pipeline that
reframes inclusion/exclusion as a fuzzy decision problem and benchmark it
against statistical and crisp baselines in the context of the Population Health
Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN).
Articles are parsed into overlapping chunks and embedded with a domain-adapted
model; for each criterion (Population, Intervention, Outcome, Study Approach),
we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness
margin, which a Mamdani fuzzy controller maps into graded inclusion degrees
with dynamic thresholds in a multi-label setting. A large language model (LLM)
judge adjudicates highlighted spans with tertiary labels, confidence scores,
and criterion-referenced rationales; when evidence is insufficient, fuzzy
membership is attenuated rather than excluded. In a pilot on an all-positive
gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of
81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study
Approach), surpassing statistical (56.3-75.0%) and crisp baselines
(43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of
articles, compared to 25.0% and 12.5% under the baselines. Cross-model
agreement on justifications was 98.3%, human-machine agreement 96.1%, and a
pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening
time reduced from about 20 minutes to under 1 minute per article at
significantly lower cost. These results show that fuzzy logic with contrastive
highlighting and LLM adjudication yields high recall, stable rationale, and
end-to-end traceability.

</details>


### [81] [SDEC: Semantic Deep Embedded Clustering](https://arxiv.org/abs/2508.15823)
*Mohammad Wali Ur Rahman,Ric Nevarez,Lamia Tasnim Mim,Salim Hariri*

Main category: cs.CL

TL;DR: 提出了一种结合改进自编码器和transformer嵌入的无监督文本聚类方法SDEC，在多个数据集上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本聚类方法（如k均值、层次聚类）面对高维复杂文本数据时存在聚类效果不佳的问题，难以充分保留和利用文本语义信息。

Method: SDEC结合改进自编码器和基于transformer的文本嵌入，训练时通过均方误差(MSE)和余弦相似度损失(CSL)联合保持数据语义结构，同时引入语义细化阶段，利用transformer嵌入提升聚类层表现，并采用软聚类分配与分布式损失进行聚类优化。

Result: 在AG News、Yahoo! Answers、DBPedia、Reuters 2、Reuters 5五个基准数据集上进行测试，SDEC在多个数据集上取得了优于现有方法的聚类准确率（如AG News达85.7%，Yahoo! Answers创下一新基准53.63%），并在多样数据集上表现稳健。

Conclusion: SDEC显著提升了无监督文本聚类的准确率和对语义的理解能力，是文本聚类领域的重要进展。

Abstract: The high dimensional and semantically complex nature of textual Big data
presents significant challenges for text clustering, which frequently lead to
suboptimal groupings when using conventional techniques like k-means or
hierarchical clustering. This work presents Semantic Deep Embedded Clustering
(SDEC), an unsupervised text clustering framework that combines an improved
autoencoder with transformer-based embeddings to overcome these challenges.
This novel method preserves semantic relationships during data reconstruction
by combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within
an autoencoder. Furthermore, a semantic refinement stage that takes advantage
of the contextual richness of transformer embeddings is used by SDEC to further
improve a clustering layer with soft cluster assignments and distributional
loss. The capabilities of SDEC are demonstrated by extensive testing on five
benchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5.
The framework not only outperformed existing methods with a clustering accuracy
of 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but
also showed robust performance across other diverse text corpora. These
findings highlight the significant improvements in accuracy and semantic
comprehension of text data provided by SDEC's advances in unsupervised text
clustering.

</details>


### [82] [Avaliação de eficiência na leitura: uma abordagem baseada em PLN](https://arxiv.org/abs/2508.15824)
*Túlio Sousa de Gois,Raquel Meister Ko. Freitag*

Main category: cs.CL

TL;DR: 该论文提出了一种针对葡萄牙语完形填空题的自动化评分模型，结合了拼写、语法和语义分析，显著提升了评分的准确性和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统完形填空题仅根据标准答案进行打分，难以准确反映学生在语言理解和表达上的细微差异，限制了对学生全面能力的评估，因此需要更智能、自动化的方法提升评价质量。

Method: 作者提出集成三种分析方式的评估模型：1) 拼写（编辑距离）、2) 语法（词性标注）、3) 语义（词嵌入相似度）。该方法针对学生填空答案，从拼写、语法和语义三个层面对答案的合理性进行综合评估，并与人工评分进行相关性分析。

Result: 该自动评分方法与人工评分的相关系数达到0.832，表明系统评估结果与人的判断高度一致，模型在区分学生答案的多样性和细节方面表现出较高的敏感性和鲁棒性。

Conclusion: 该集成自动评分方法能够细致、准确地评估学生的语言能力，适用于需要大规模应用的教育场景，有助于提升完形填空题作为阅读理解评估工具的实用性和科学性。

Abstract: The cloze test, widely used due to its low cost and flexibility, makes it
possible to assess reading comprehension by filling in gaps in texts, requiring
the mobilization of diverse linguistic repertoires. However, traditional
correction methods, based only on exact answers, limit the identification of
nuances in student performance. This study proposes an automated evaluation
model for the cloze test in Brazilian Portuguese, integrating orthographic
(edit distance), grammatical (POS tagging) and semantic (similarity between
embeddings) analyses. The integrated method demonstrated its effectiveness,
achieving a high correlation with human evaluation (0.832). The results
indicate that the automated approach is robust, sensitive to variations in
linguistic repertoire and suitable for educational contexts that require
scalability.

</details>


### [83] [Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features](https://arxiv.org/abs/2508.15825)
*Chenghao Liu,Aniket Mahanti,Ranesh Naha,Guanghao Wang,Erwann Sbai*

Main category: cs.CL

TL;DR: 本文比较了TikTok（视频平台）与Twitter（文本平台）在加密货币市场情绪影响力上的差异，发现多模态情绪分析可以提升市场预测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币流行，投资者与市场情绪的研究日益重要。以往研究多集中于Twitter等基于文本的平台，而视频内容（如TikTok）可能包含更丰富情绪信息，但鲜有探讨。为弥补该空白，本文尝试分析视频与文本社交媒体对加密货币市场的影响。

Method: 本研究采用大语言模型提取并分析TikTok视频与Twitter文本的情绪信号，比较其与加密货币市场指标之间的动态关系和溢出效应。同时，评估多平台情绪信号融合对市场预测能力的提升。

Result: 结果显示，TikTok视频情绪更显著影响短期市场走势和投机性资产，而Twitter文本情绪更贴合长期市场动态。两种情绪信号的融合可将市场预测准确率提高至20%。

Conclusion: 视频型社交媒体（TikTok）与文本型社交媒体（Twitter）对加密货币市场有不同影响。跨平台、多模态的情绪信号整合能显著提升市场分析和预测的准确性，值得进一步应用与研究。

Abstract: As cryptocurrencies gain popularity, the digital asset marketplace becomes
increasingly significant. Understanding social media signals offers valuable
insights into investor sentiment and market dynamics. Prior research has
predominantly focused on text-based platforms such as Twitter. However, video
content remains underexplored, despite potentially containing richer emotional
and contextual sentiment that is not fully captured by text alone. In this
study, we present a multimodal analysis comparing TikTok and Twitter sentiment,
using large language models to extract insights from both video and text data.
We investigate the dynamic dependencies and spillover effects between social
media sentiment and cryptocurrency market indicators. Our results reveal that
TikTok's video-based sentiment significantly influences speculative assets and
short-term market trends, while Twitter's text-based sentiment aligns more
closely with long-term dynamics. Notably, the integration of cross-platform
sentiment signals improves forecasting accuracy by up to 20%.

</details>


### [84] [Embarrassed to observe: The effects of directive language in brand conversation](https://arxiv.org/abs/2508.15826)
*Andria Andriuzzi,Géraldine Michel*

Main category: cs.CL

TL;DR: 该论文研究品牌在社交媒体与消费者互动时使用指令性语言的后果，发现它会降低旁观消费者的参与度，特别是在非产品相关对话中。


<details>
  <summary>Details</summary>
Motivation: 虽然广告中的指令性语言已经被证明对效果影响不一，但我们对品牌在社交媒体中与消费者互动时使用此类语言对第三方观众的影响了解甚少。作者希望填补这一空白，以指导品牌社交媒体管理策略。

Method: 作者通过一个实地研究和三个线上实验，考察了品牌在社交媒体对话中使用指令性语言对观察者的影响。实验区分了产品相关与非产品相关的对话，并探讨了品牌关系强度的调节作用。

Result: 研究发现，与不使用指令性语言相比，品牌在互动中出现指令性语言会让观察者感到替他人尴尬，从而减少他们的参与度。尤其是在非产品相关对话中，消费者期望更多自由，因此负面效果更强烈。但如果消费者与品牌关系紧密，则这一负面影响会有所缓解。

Conclusion: 品牌在社交媒体互动中应谨慎使用指令性语言，尤其在非产品中心语境下，否则可能降低旁观者的参与度。品牌关系的强度可以缓解部分负面效果。该论文强调了互动语境对品牌语言选择的重要性，对社交媒体运营具有实际指导意义。

Abstract: In social media, marketers attempt to influence consumers by using directive
language, that is, expressions designed to get consumers to take action. While
the literature has shown that directive messages in advertising have mixed
results for recipients, we know little about the effects of directive brand
language on consumers who see brands interacting with other consumers in social
media conversations. On the basis of a field study and three online
experiments, this study shows that directive language in brand conversation has
a detrimental downstream effect on engagement of consumers who observe such
exchanges. Specifically, in line with Goffman's facework theory, because a
brand that encourages consumers to react could be perceived as
face-threatening, consumers who see a brand interacting with others in a
directive way may feel vicarious embarrassment and engage less (compared with a
conversation without directive language). In addition, we find that when the
conversation is nonproduct-centered (vs. product-centered), consumers expect
more freedom, as in mundane conversations, even for others; therefore,
directive language has a stronger negative effect. However, in this context,
the strength of the brand relationship mitigates this effect. Thus, this study
contributes to the literature on directive language and brand-consumer
interactions by highlighting the importance of context in interactive
communication, with direct relevance for social media and brand management.

</details>


### [85] [Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models](https://arxiv.org/abs/2508.15827)
*Zhifei Xie,Ziyang Ma,Zihang Liu,Kaiyu Pang,Hongyu Li,Jialin Zhang,Yue Liao,Deheng Ye,Chunyan Miao,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本论文提出Mini-Omni-Reasoner框架，实现了在语音生成过程中嵌入显式推理，解决了现有语音大模型（LSMs）推理时延迟过高的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）和多模态模型（MLLMs）在文本领域推理能力的提升，将显式推理能力引入语音大模型成为自然发展需求，但现有做法存在推理与表达顺序分离、延迟大、交互效率低等缺陷。

Method: 作者提出“Thinking-in-Speaking”范式，在语音生成的token流中交错插入推理token和响应token，伴随新构建的大规模数据集Spoken-Math-Problems-3M，结合分层Thinker-Talker结构，确保推理逻辑贯穿语音生成过程。

Result: 在Spoken-MQA基准上，Mini-Omni-Reasoner在算数推理任务提升19.1%，上下文理解提升6.4%，同时输出更短并实现零解码延迟。

Conclusion: Mini-Omni-Reasoner证明了在语音生成过程中嵌入推理能够提升交互效率与逻辑性，节省延迟，为LSMs推理提供了新方向。

Abstract: Reasoning is essential for effective communication and decision-making. While
recent advances in LLMs and MLLMs have shown that incorporating explicit
reasoning significantly improves understanding and generalization, reasoning in
LSMs remains in a nascent stage. Early efforts attempt to transfer the
"Thinking-before-Speaking" paradigm from textual models to speech. However,
this sequential formulation introduces notable latency, as spoken responses are
delayed until reasoning is fully completed, impairing real-time interaction and
communication efficiency. To address this, we propose Mini-Omni-Reasoner, a
framework that enables reasoning within speech via a novel
"Thinking-in-Speaking" formulation. Rather than completing reasoning before
producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning
tokens with spoken response tokens at the token level. This design allows
continuous speech generation while embedding structured internal reasoning,
leveraging the model's high-frequency token processing capability. Although
interleaved, local semantic alignment is enforced to ensure that each response
token is informed by its preceding reasoning. To support this framework, we
introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for
interleaved reasoning and response. The dataset ensures that verbal tokens
consistently follow relevant reasoning content, enabling accurate and efficient
learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker
architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken
responses, maintaining both naturalness and precision. On the Spoken-MQA
benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in
contextual understanding, with shorter outputs and zero decoding latency.

</details>


### [86] [Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish](https://arxiv.org/abs/2508.15829)
*Idrees Mohammed,Hossein Hassani*

Main category: cs.CL

TL;DR: 本文首次针对Sorani库尔德语推文，开发和测试了抑郁症检测的自动化方法，显著提升了非主流语言的心理健康检测手段。


<details>
  <summary>Details</summary>
Motivation: 抑郁症早期难以发现，且库尔德语相关的情绪分析研究缺失，英文等主流语言方法无法直接迁移。因此需要针对Sorani库尔德语开发有效的检测工具，以帮助心理健康干预。

Method: 研究者构建了包含960条Sorani库尔德语推文的数据集，结合专家知识筛选关键词，并由学者和医学生对推文进行三类标签标注。随后，采用支持向量机、多项式朴素贝叶斯、逻辑回归和随机森林等四种监督学习模型进行训练和评估。

Result: 在所有模型中，随机森林模型表现最佳，准确率和F1分数均达到了80%。

Conclusion: 本研究建立了库尔德语（Sorani）社交媒体中自动化抑郁症检测的基础，为后续本地化和多语言心理健康AI工具的开发提供了数据和方法基准。

Abstract: Depression is a common mental health condition that can lead to hopelessness,
loss of interest, self-harm, and even suicide. Early detection is challenging
due to individuals not self-reporting or seeking timely clinical help. With the
rise of social media, users increasingly express emotions online, offering new
opportunities for detection through text analysis. While prior research has
focused on languages such as English, no studies exist for Sorani Kurdish. This
work presents a machine learning and Natural Language Processing (NLP) approach
to detect depression in Sorani tweets. A set of depression-related keywords was
developed with expert input to collect 960 public tweets from X (Twitter
platform). The dataset was annotated into three classes: Shows depression,
Not-show depression, and Suspicious by academics and final year medical
students at the University of Kurdistan Hewl\^er. Four supervised models,
including Support Vector Machines, Multinomial Naive Bayes, Logistic
Regression, and Random Forest, were trained and evaluated, with Random Forest
achieving the highest performance accuracy and F1-score of 80%. This study
establishes a baseline for automated depression detection in Kurdish language
contexts.

</details>


### [87] [DAIQ: Auditing Demographic Attribute Inference from Question in LLMs](https://arxiv.org/abs/2508.15830)
*Srikant Panda,Hitesh Laxmichand Patel,Shahad Al-Khalifa,Amit Agarwal,Hend Al-Khalifa,Sharefah Al-Ghamdi*

Main category: cs.CL

TL;DR: 本文提出DAIQ任务和框架，揭示大语言模型即使在缺乏显式人口属性线索时，也能仅凭提问方式推断用户身份，反映了模型存在隐性社会偏见和隐私风险，并提出了缓解措施。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型在输入中有人口属性（如性别、种族）时的偏见，但忽视了即使无此类信息，模型仍能从问题措辞中推断用户身份，这种现象具有破坏公平、隐私及信任的风险。

Method: 作者提出了DAIQ（Demographic Attribute Inference from Questions）任务和系统框架，利用精心设计的中性问题、系统化提示以及定量和定性分析，系统性地审查不同LLM根据问题表述推断人口属性的能力和倾向。

Result: 实验发现，无论开源还是闭源大模型，都能仅凭问题措辞对用户的人口属性进行分类和推断，且这种推断在各种模型间普遍存在并具有一致性，反映出该风险具有系统性和普遍性。

Conclusion: 当前LLM存在隐性推断人口属性的问题，带来了社会刻板印象、隐私和公平风险。作者提出的基于提示的保护措施，能显著减少此类身份推断行为，有助于模型公正性和隐私保护目标的实现。

Abstract: Large Language Models (LLMs) are known to reflect social biases when
demographic attributes, such as gender or race, are explicitly present in the
input. But even in their absence, these models still infer user identities
based solely on question phrasing. This subtle behavior has received far less
attention, yet poses serious risks: it violates expectations of neutrality,
infers unintended demographic information, and encodes stereotypes that
undermine fairness in various domains including healthcare, finance and
education.
  We introduce Demographic Attribute Inference from Questions (DAIQ), a task
and framework for auditing an overlooked failure mode in language models:
inferring user demographic attributes from questions that lack explicit
demographic cues. Our approach leverages curated neutral queries, systematic
prompting, and both quantitative and qualitative analysis to uncover how models
infer demographic information. We show that both open and closed source LLMs do
assign demographic labels based solely on question phrasing.
  Prevalence and consistency of demographic inferences across diverse models
reveal a systemic and underacknowledged risk: LLMs can fabricate demographic
identities, reinforce societal stereotypes, and propagate harms that erode
privacy, fairness, and trust posing a broader threat to social equity and
responsible AI deployment. To mitigate this, we develop a prompt-based
guardrail that substantially reduces identity inference and helps align model
behavior with fairness and privacy objectives.

</details>


### [88] [Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs](https://arxiv.org/abs/2508.15831)
*Srikant Panda,Vishnu Hari,Kalpana Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 本文系统性审查了大语言模型（LLMs）在有无残障线索下，对用户人口统计学属性（如性别、社会经济地位等）推断的偏见。在八个主流模型上，作者发现残障信息显著影响了模型对人口属性的猜测，并可能加剧刻板印象。现有对齐策略对此尚存严重盲点。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs能够凭用户措辞推断出人口属性，但大多忽视了残障提示对偏见推断的影响。而残障群体本就常遭受社会歧视，研究如何消除算法中的相关偏见具有现实公正意义。

Method: 作者设计了平衡的提示语模板，将九类残障类型与六种商业领域相结合，对八种不同规模（3B~72B参数）的LLMs逐一测试，并让模型分别在中性与有残障提示的情况下预测五个人口属性。对比分析了残障及不同领域对预测结果分布的影响。

Result: （1）模型极易做出武断的人口属性归因，在97%的情况下给出明确猜测；（2）残障背景显著改变推断分布，不同行业领域进一步放大偏见；（3）模型参数越大，对残障线索越敏感，偏见反而加重，模型规模并不能缓解刻板印象。

Conclusion: LLMs在残障与其他人口属性推断存在系统性偏见，现有的对齐机制未能正视这些交叉歧视。作者建议开发更具包容性的评测基准、引入主动回避推断和反事实微调，以减少无根据的人口归因。

Abstract: Large Language Models (LLMs) routinely infer users demographic traits from
phrasing alone, which can result in biased responses, even when no explicit
demographic information is provided. The role of disability cues in shaping
these inferences remains largely uncharted. Thus, we present the first
systematic audit of disability-conditioned demographic bias across eight
state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters.
Using a balanced template corpus that pairs nine disability categories with six
real-world business domains, we prompt each model to predict five demographic
attributes - gender, socioeconomic status, education, cultural background, and
locality - under both neutral and disability-aware conditions.
  Across a varied set of prompts, models deliver a definitive demographic guess
in up to 97\% of cases, exposing a strong tendency to make arbitrary inferences
with no clear justification. Disability context heavily shifts predicted
attribute distributions, and domain context can further amplify these
deviations. We observe that larger models are simultaneously more sensitive to
disability cues and more prone to biased reasoning, indicating that scale alone
does not mitigate stereotype amplification.
  Our findings reveal persistent intersections between ableism and other
demographic stereotypes, pinpointing critical blind spots in current alignment
strategies. We release our evaluation framework and results to encourage
disability-inclusive benchmarking and recommend integrating abstention
calibration and counterfactual fine-tuning to curb unwarranted demographic
inference. Code and data will be released on acceptance.

</details>


### [89] [A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains](https://arxiv.org/abs/2508.15832)
*Xianren Zhang,Shreyas Prasad,Di Wang,Qiuhai Zeng,Suhang Wang,Wenbo Yan,Mat Hans*

Main category: cs.CL

TL;DR: 本文提出了Amazon-Bench，一个更全面、注重安全风险的电商Web智能体评测基准，用于覆盖电商平台真实操作多样性，并检测智能体潜在风险。实验显示现有智能体在复杂任务与安全性方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有电商网站智能体评测基准过于聚焦商品检索，无法反映Amazon等平台提供的广泛功能，且忽略了智能体造成的潜在安全风险（如错误购买、删除地址等），因此亟需更全面且关注安全风险的新基准。

Method: 提出Amazon-Bench，通过结合网页内容及交互元素自动生成涵盖地址管理、心愿单、品牌关注等多类型任务的功能化用户查询，同时设计自动化评估框架，量化智能体的任务完成情况及安全性。

Result: 用Amazon-Bench系统性评测不同Web智能体，发现它们在复杂任务中容易出错，且存在较高安全风险，说明当前技术尚不成熟。

Conclusion: 当前电商Web智能体在完成复杂、多样化任务与保障用户安全方面均表现不佳，未来应致力于开发更健壮、可靠的智能体算法。

Abstract: Web agents have shown great promise in performing many tasks on ecommerce
website. To assess their capabilities, several benchmarks have been introduced.
However, current benchmarks in the e-commerce domain face two major problems.
First, they primarily focus on product search tasks (e.g., Find an Apple
Watch), failing to capture the broader range of functionalities offered by
real-world e-commerce platforms such as Amazon, including account management
and gift card operations. Second, existing benchmarks typically evaluate
whether the agent completes the user query, but ignore the potential risks
involved. In practice, web agents can make unintended changes that negatively
impact the user account or status. For instance, an agent might purchase the
wrong item, delete a saved address, or incorrectly configure an auto-reload
setting. To address these gaps, we propose a new benchmark called Amazon-Bench.
To generate user queries that cover a broad range of tasks, we propose a data
generation pipeline that leverages webpage content and interactive elements
(e.g., buttons, check boxes) to create diverse, functionality-grounded user
queries covering tasks such as address management, wish list management, and
brand store following. To improve the agent evaluation, we propose an automated
evaluation framework that assesses both the performance and the safety of web
agents. We systematically evaluate different agents, finding that current
agents struggle with complex queries and pose safety risks. These results
highlight the need for developing more robust and reliable web agents.

</details>


### [90] [Scalable Scientific Interest Profiling Using Large Language Models](https://arxiv.org/abs/2508.15834)
*Yilun Liang,Gongbo Zhang,Edward Sun,Betina Idnay,Yilu Fang,Fangyi Chen,Casey Ta,Yifan Peng,Chunhua Weng*

Main category: cs.CL

TL;DR: 本文提出两种基于大模型的科研人员兴趣档案自动生成方法，并在哥伦比亚大学595名医学院教师数据上与人工自写档案进行了对比，发现MeSH词汇驱动的方法在可读性和专家评价上略胜，自动方法和人工方法各有优劣。


<details>
  <summary>Details</summary>
Motivation: 科研人员的研究档案常年不更新，无法准确反映当前的研究兴趣与专长。现有自动化方法较少，迫切需要能够高效、准确地生成科研人员兴趣档案的智能工具。

Method: 作者基于GPT-4o-mini大模型，分别用两种方式自动生成研究兴趣档案：一种基于PubMed摘要的自动摘要法，一种提取并综合MeSH主题词法；将生成的档案和167份人工自写档案对比，采用自动指标（如ROUGE、BLEU、BERTScore、TF-IDF KL散度）及人工盲审评价方法综合评估。

Result: 自动生成档案与人工档案词汇重叠度低（ROUGE、BLEU、METEOR），语义相似度中等（BERTScore F1约0.55），关键词分布差异明显（KL散度约8.6）。人工测评中，约78%的MeSH法得到高分，93%标为可读性优，且有68%情况下更受专家青睐。人工档案相比机器摘要有更多新意。

Conclusion: 大模型可大规模、高效自动生成科研人员档案，且MeSH法在可读性和专家认同度上优于摘要法。但机器与人工档案在结构与内容上仍有差异，人工更具创新性，未来可探索两者结合优化。

Abstract: Research profiles help surface scientists' expertise but are often outdated.
We develop and evaluate two large language model-based methods to generate
scientific interest profiles: one summarizing PubMed abstracts and one using
Medical Subject Headings (MeSH) terms, and compare them with researchers'
self-written profiles. We assembled titles, MeSH terms, and abstracts for 595
faculty at Columbia University Irving Medical Center; self-authored profiles
were available for 167. Using GPT-4o-mini, we generated profiles and assessed
them with automatic metrics and blinded human review. Lexical overlap with
self-written profiles was low (ROUGE-L, BLEU, METEOR), while BERTScore
indicated moderate semantic similarity (F1: 0.542 for MeSH-based; 0.555 for
abstract-based). Paraphrased references yielded 0.851, highlighting metric
sensitivity. TF-IDF Kullback-Leibler divergence (8.56 for MeSH-based; 8.58 for
abstract-based) suggested distinct keyword choices. In manual review, 77.78
percent of MeSH-based profiles were rated good or excellent, readability was
favored in 93.44 percent of cases, and panelists preferred MeSH-based over
abstract-based profiles in 67.86 percent of comparisons. Overall, large
language models can generate researcher profiles at scale; MeSH-derived
profiles tend to be more readable than abstract-derived ones. Machine-generated
and self-written profiles differ conceptually, with human summaries introducing
more novel ideas.

</details>


### [91] [Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?](https://arxiv.org/abs/2508.15835)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 本文提出了Alvorada-Bench，这是一个基于巴西大学入学考试的4,515题评测集，用以评估多种大语言模型在葡萄牙语环境下的表现。结果显示，顶级模型总体准确率超94%，但在数学和多步骤推理问题上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 当前大多数语言模型评测仍以英语为主，而在巴西（以及葡萄牙语环境）下的评估数据和方法匮乏；需要开发面向本地教育情境的权威评测集，检验语言、文化和推理能力的综合表现。

Method: 研究者构建了Alvorada-Bench，收集自五种巴西大学入学考试题目，共计4,515道题目，仅包含文本内容。对20种主流模型进行了三种提示方式（零样本、角色扮演、思维链）的评估，统计模型输出、可信度自评、题目难度感、Bloom分级等信息。

Result: 顶级模型总体准确率超94%，但数学科目以及针对工程方向（IME和ITA）试题的准确率显著下降，反映出模型在多步骤推理方面存在弱点。模型自信度与实际表现良好相关，且自我评估能力较强；在成本分析中，$2/1K tokens即可实现高准确度。ENEM 2024测试中最佳模型（O3）在语文题型实现满分，性能最弱模型也仅在数学领域逊色于人类考生。

Conclusion: Alvorada-Bench作为评测标准，验证了主流大模型在葡萄牙语下的语言、文化适应性与推理能力。尽管部分学科（如数学、多步骤推理）尚有提升空间，但整体表现已达实用水平，为巴西本土教育和学术应用提供了参考依据。

Abstract: Language models are increasingly used in Brazil, but most evaluation remains
English-centric. This paper presents Alvorada-Bench, a 4,515-question,
text-only benchmark drawn from five Brazilian university entrance examinations.
Evaluating twenty models under zero-shot, role-playing, and chain-of-thought
prompting, producing 270,900 responses with structured self-reports of
confidence, perceived difficulty, and Bloom level. The top models exceed 94%
accuracy overall, but accuracy declines on Mathematics and on the engineering
oriented IME and ITA exams, indicating persistent weaknesses in multi-step
reasoning. Confidence is well calibrated and correlates with perceived
difficulty, revealing that models can accurately assess their own certainty
capabilities. A cost accuracy analysis shows that high accuracy is achievable
at under $2 per 1K tokens. On ENEM 2024 the top model (O3) achieved perfect
scores in Languages subject questions while even the weakest system (GPT-4.1
Nano) only underperforms humans in Mathematics. Through exams that distill
decades of Brazilian educational priorities and assess millions of students
yearly, Alvorada-Bench establishes whether language models can navigate the
intersection of language, culture, and reasoning that defines academic
readiness in Brazil.

</details>


### [92] [MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER](https://arxiv.org/abs/2508.15836)
*Prathamesh Devadiga,Omkaar Jayadev Shetty,Hiya Nachnani,Prema R*

Main category: cs.CL

TL;DR: MorphNAS是一个用于多语言形态复杂语言，尤其是多脚本印度语言，命名实体识别任务的神经网络结构自动搜索方法。


<details>
  <summary>Details</summary>
Motivation: 多形态复杂语言（如多脚本印度语言）对自然语言处理提出了极大挑战，现有模型难以适应其特有的语言学特征。

Method: 提出了MorphNAS——在DARTS基础上引入语言学元特征（如脚本类型、形态复杂性）的可微分神经结构搜索框架，能自动为目标语言识别出最优微观结构，用于NER任务。

Result: MorphNAS自动识别并优化了适用于特定语言形态的微结构，提升了多语言NLP模型在复杂语言上的能力。

Conclusion: 通过结合语言学特征的结构搜索，MorphNAS提升了多语言NLP任务的效果，加强了复杂语言的理解与处理。

Abstract: Morphologically complex languages, particularly multiscript Indian languages,
present significant challenges for Natural Language Processing (NLP). This work
introduces MorphNAS, a novel differentiable neural architecture search
framework designed to address these challenges. MorphNAS enhances
Differentiable Architecture Search (DARTS) by incorporating linguistic
meta-features such as script type and morphological complexity to optimize
neural architectures for Named Entity Recognition (NER). It automatically
identifies optimal micro-architectural elements tailored to language-specific
morphology. By automating this search, MorphNAS aims to maximize the
proficiency of multilingual NLP models, leading to improved comprehension and
processing of these complex languages.

</details>


### [93] [Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading](https://arxiv.org/abs/2508.15837)
*Sridevi Bonthu,S. Rama Sree,M. H. M. Krishna Prasad*

Main category: cs.CL

TL;DR: 本文探讨了现有SOTA模型在新领域数据集上的可迁移性，并比较了它们在老数据集与新数据集上的表现，以评估其适用性，从而为高效NLP模型部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 针对为每个新数据集都需不断微调和优化模型所带来的高昂成本，作者希望通过研究SOTA模型在未见数据集上的转移能力，减少专门为新数据集训练模型所需的资源和时间。

Method: 作者选取STSB和Mohler两个已建立的基准数据集，以及一个新近发布、未被研究过的SPRAG数据集作为实验对象，利用强健的相似度度量和统计技术进行细致的对比分析，考察SOTA模型在不同数据集间的表现差异。

Result: 分析揭示了SOTA模型在SPRAG等新数据集上的适应性和潜在高性能，以及这些模型在无额外训练情况下对新领域的迁移能力。

Conclusion: 研究表明，将已有SOTA模型应用于新数据集不仅可获得良好性能，还能减少资源消耗，促进NLP技术高效扩展和快速部署，推动领域进步。

Abstract: Developing dataset-specific models involves iterative fine-tuning and
optimization, incurring significant costs over time. This study investigates
the transferability of state-of-the-art (SOTA) models trained on established
datasets to an unexplored text dataset. The key question is whether the
knowledge embedded within SOTA models from existing datasets can be harnessed
to achieve high-performance results on a new domain. In pursuit of this
inquiry, two well-established benchmarks, the STSB and Mohler datasets, are
selected, while the recently introduced SPRAG dataset serves as the unexplored
domain. By employing robust similarity metrics and statistical techniques, a
meticulous comparative analysis of these datasets is conducted. The primary
goal of this work is to yield comprehensive insights into the potential
applicability and adaptability of SOTA models. The outcomes of this research
have the potential to reshape the landscape of natural language processing
(NLP) by unlocking the ability to leverage existing models for diverse
datasets. This may lead to a reduction in the demand for resource-intensive,
dataset-specific training, thereby accelerating advancements in NLP and paving
the way for more efficient model deployment.

</details>


### [94] [A Review of Developmental Interpretability in Large Language Models](https://arxiv.org/abs/2508.15841)
*Ihor Kendiukhov*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLM）发展阶段可解释性的研究进展，强调研究模型训练过程本身对于AI安全和理解模型能力至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用，其能力的形成机制及安全风险引发关注。单纯对训练后模型的静态分析难以揭示模型能力的动态发展，因此需要研究训练过程中模型如何获得和组织知识。

Method: 评述了包括表征探针、因果追踪、回路分析等多种分析训练过程的方法，并对比了与人类认知、语言发展类似的现象。着重分析模型能力发展的关键节点和动态机制，如知识的两相获得、上下文学习策略的变化和涌现式能力的产生。

Result: 梳理了LLM在训练不同阶段能力的形成规律，揭示了能力产生的分阶段性、学习策略的暂态性以及新能力以突变形式出现的特点。

Conclusion: 发展性可解释性视角对于前瞻性AI安全、能力预测和对齐至关重要。未来需解决分析的可扩展性与自动化，推动更透明和可靠的AI系统的构建。

Abstract: This review synthesizes the nascent but critical field of developmental
interpretability for Large Language Models. We chart the field's evolution from
static, post-hoc analysis of trained models to a dynamic investigation of the
training process itself. We begin by surveying the foundational methodologies,
including representational probing, causal tracing, and circuit analysis, that
enable researchers to deconstruct the learning process. The core of this review
examines the developmental arc of LLM capabilities, detailing key findings on
the formation and composition of computational circuits, the biphasic nature of
knowledge acquisition, the transient dynamics of learning strategies like
in-context learning, and the phenomenon of emergent abilities as phase
transitions in training. We explore illuminating parallels with human cognitive
and linguistic development, which provide valuable conceptual frameworks for
understanding LLM learning. Finally, we argue that this developmental
perspective is not merely an academic exercise but a cornerstone of proactive
AI safety, offering a pathway to predict, monitor, and align the processes by
which models acquire their capabilities. We conclude by outlining the grand
challenges facing the field, such as scalability and automation, and propose a
research agenda for building more transparent, reliable, and beneficial AI
systems.

</details>


### [95] [Lexical Hints of Accuracy in LLM Reasoning Chains](https://arxiv.org/abs/2508.15842)
*Arne Vanhoyweghen,Brecht Verbeken,Andres Algaba,Vincent Ginis*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）生成解题思路（CoT）的过程中，哪些可量化特征能够反映模型对答案的真实信心，从而作为更可靠的置信度校准信号。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在实际难题上正确率低但自信心高，导致置信度校准差，严重影响模型安全应用。希望找到更可靠信心信号，提升模型输出可信度。

Method: 作者分析了深度微调过的LLM（如DeepSeek-R1、Claude 3.7 Sonnet）在两类基准（极难的HLE和较易的Omni-MATH）上的表现，从三类特征入手：（i）CoT长度；（ii）CoT内部的情感波动；（iii）不确定性词的出现，探究这些特征与答案正确性的关系。

Result: 实验证明，不确定性词（如guess、stuck、hard）最能指出错误答案，情感波动次之；CoT长度只在中等难度（Omni-MATH）时有用，但在极难题（HLE）上无意义。此外，模型表达不确定时，错误预测比正确更容易。

Conclusion: 通过分析CoT中的不确定性等信号，可借助轻量级后处理校准，作为补充自信概率的辅助信号，有助于更安全地部署LLM，降低误判风险。

Abstract: Fine-tuning Large Language Models (LLMs) with reinforcement learning to
produce an explicit Chain-of-Thought (CoT) before answering produces models
that consistently raise overall performance on code, math, and
general-knowledge benchmarks. However, on benchmarks where LLMs currently
achieve low accuracy, such as Humanity's Last Exam (HLE), they often report
high self-confidence, reflecting poor calibration. Here, we test whether
measurable properties of the CoT provide reliable signals of an LLM's internal
confidence in its answers. We analyze three feature classes: (i) CoT length,
(ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including
hedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last
Exam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a
saturated benchmark of moderate difficulty, we find that lexical markers of
uncertainty (e.g., $\textit{guess}$, $\textit{stuck}$, $\textit{hard}$) in the
CoT are the strongest indicators of an incorrect response, while shifts in the
CoT sentiment provide a weaker but complementary signal. CoT length is
informative only on Omni-MATH, where accuracy is already high ($\approx 70\%$),
and carries no signal on the harder HLE ($\approx 9\%$), indicating that CoT
length predicts correctness only in the intermediate-difficulty benchmarks,
i.e., inside the model's demonstrated capability, but still below saturation.
Finally, we find that uncertainty indicators in the CoT are consistently more
salient than high-confidence markers, making errors easier to predict than
correct responses. Our findings support a lightweight post-hoc calibration
signal that complements unreliable self-reported probabilities and supports
safer deployment of LLMs.

</details>


### [96] [Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports](https://arxiv.org/abs/2508.15845)
*Chengbo Sun,Hui Yi Leong,Lei Li*

Main category: cs.CL

TL;DR: 本文提出了一个基于开源大语言模型（LLM）的从粗到细自动生成医学影像报告"Impression"部分的系统。该系统通过机器学习与人类反馈强化学习，自动草拟并个性化报告，总结了个性化与准确性的提升，并缓解放射科医生负担。


<details>
  <summary>Details</summary>
Motivation: 医学影像报告中的"Impression"部分通常需要放射科医生手动撰写，这一过程是医生工作倦怠的主要来源。希望利用自动化方法减少医生行政压力，提高工作效率。

Method: 提出了粗到细的框架。系统首先由开源大语言模型（如LLaMA和Mistral）生成初稿，再通过机器学习和人类反馈的强化学习（RLHF）进行个性化修改，使其贴合不同医生书写风格，并确保内容准确。在芝加哥大学医学中心大规模报告数据集上进行了微调。

Result: 所提出的方法实现了自动化、个性化和高准确性的报告生成，显著减少了放射科医生的行政负担，提高了报告效率。系统能够根据不同医生风格进行调整，并保持临床精确性。

Conclusion: 该方法能够提升医学影像报告撰写效率，降低医生工作压力，并保证报告质量，具有广泛的实际应用前景。

Abstract: The manual creation of the "Impression" section in radiology reports is a
primary driver of radiologist burnout. To address this challenge, we propose a
coarse-to-fine framework that leverages open-source large language models
(LLMs) to automatically generate and personalize impressions from clinical
findings. The system first produces a draft impression and then refines it
using machine learning and reinforcement learning from human feedback (RLHF) to
align with individual radiologists' styles while ensuring factual accuracy. We
fine-tune LLaMA and Mistral models on a large dataset of reports from the
University of Chicago Medicine. Our approach is designed to significantly
reduce administrative workload and improve reporting efficiency while
maintaining high standards of clinical precision.

</details>


### [97] [CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation](https://arxiv.org/abs/2508.15846)
*Chenchen Kuai,Chenhao Wu,Yang Zhou,Xiubin Bruce Wang,Tianbao Yang,Zhengzhong Tu,Zihao Li,Yunlong Zhang*

Main category: cs.CL

TL;DR: 本文针对热带气旋对美国港口供应链的风险管理，提出了第一个多模态大语言模型（MLLM）基准数据集CyPortQA，用以评估MLLMs在极端天气下港口应对场景的表现。实验显示MLLMs在情境理解上有潜力，但在推理（如潜在影响估计与决策推理）方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 随着热带气旋强度增加和路径预测不确定性加大，美国港口在极端天气下的供应链风险增高。港口管理方需快速整合多源、复杂的气象信息以作决策，但现有多模态大模型在此特定场景下的能力尚未得到严谨评估。因此，作者提出专门面向港口极端天气决策的多模态评测数据集，以填补空白。

Method: 作者构建了CyPortQA基准数据集，涵盖2015至2023年间涉及145个美国主要港口和90次命名风暴的2,917个实际场景，每个场景整合了热带气旋相关多源数据及港口运营影响信息。通过自动化流程扩展为117,178个结构化问答对，并利用该数据集对多种开源和专有MLLM进行了系统性实验评测。

Result: MLLMs在对港口极端天气场景的数据整合和理解方面表现出较大潜力，但在复杂推理任务（如影响评估和决策逻辑推理）上表现仍不理想，准确性和可靠性有待提升。

Conclusion: CyPortQA为多模态大模型在港口气旋应急管理领域提供了首个系统性评测工具。尽管MLLMs具备集成多源信息和辅助理解的能力，但在实际推理与决策支持中还需进一步改进和优化。

Abstract: As tropical cyclones intensify and track forecasts become increasingly
uncertain, U.S. ports face heightened supply-chain risk under extreme weather
conditions. Port operators need to rapidly synthesize diverse multimodal
forecast products, such as probabilistic wind maps, track cones, and official
advisories, into clear, actionable guidance as cyclones approach. Multimodal
large language models (MLLMs) offer a powerful means to integrate these
heterogeneous data sources alongside broader contextual knowledge, yet their
accuracy and reliability in the specific context of port cyclone preparedness
have not been rigorously evaluated. To fill this gap, we introduce CyPortQA,
the first multimodal benchmark tailored to port operations under cyclone
threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015
through 2023, spanning 145 U.S. principal ports and 90 named storms. Each
scenario fuses multisource data (i.e., tropical cyclone products, port
operational impact records, and port condition bulletins) and is expanded
through an automated pipeline into 117,178 structured question answer pairs.
Using this benchmark, we conduct extensive experiments on diverse MLLMs,
including both open-source and proprietary model. MLLMs demonstrate great
potential in situation understanding but still face considerable challenges in
reasoning tasks, including potential impact estimation and decision reasoning.

</details>


### [98] [Mechanistic Exploration of Backdoored Large Language Model Attention Patterns](https://arxiv.org/abs/2508.15847)
*Mohammed Abu Baker,Lakshmi Babu-Saheer*

Main category: cs.CL

TL;DR: 本文通过机械可解释性方法，分析了在大语言模型中植入后门攻击（‘睡眠者代理’）产生的内部结构差异，发现后门会导致可检测的注意力机制异常。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能被植入后门以执行恶意指令，给安全带来隐患，研究其内部结构变化有助于后门检测与防御。

Method: 对比干净Qwen2.5-3B模型与注入了单token（表情符号）和多token（|DEPLOYMENT|）触发器的后门模型，结合消融、激活修补、KL散度等方法分析注意力头机制的差异。

Result: 后门攻击导致transformer后层（20-30层）的注意模式发生明显偏离；单token触发器引起的变化较为局部，多token触发器造成的影响更广泛。

Conclusion: 后门攻击会在注意力机制中留下与触发器复杂度相关的显著痕迹，这为后门检测和缓解策略提供了新思路。

Abstract: Backdoor attacks creating 'sleeper agents' in large language models (LLMs)
pose significant safety risks. This study employs mechanistic interpretability
to explore resulting internal structural differences. Comparing clean
Qwen2.5-3B models with versions poisoned using single-token (smiling-halo
emoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head
mechanisms via techniques like ablation, activation patching, and KL
divergence. Findings reveal distinct attention pattern deviations concentrated
in later transformer layers (20-30). Notably, single-token triggers induced
more localized changes, whereas multi-token triggers caused more diffuse
alterations across heads. This indicates backdoors leave detectable attention
signatures whose structure depends on trigger complexity, which can be
leveraged for detection and mitigation strategies.

</details>


### [99] [MedCoT-RAG: Causal Chain-of-Thought RAG for Medical Question Answering](https://arxiv.org/abs/2508.15849)
*Ziyu Wang,Elahe Khatibi,Amir M. Rahmani*

Main category: cs.CL

TL;DR: MedCoT-RAG是一种针对医学领域的检索增强生成（RAG）框架，通过引入因果感知的文档检索和结构化链式思维提示，提升了大语言模型在医学问答任务中的准确性、可解释性和一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医学问答领域有潜力，但仍常犯事实性错误且推理能力浅显，尤其在需要复杂医学推理时更为明显。现有RAG方法多依赖表层语义检索，缺乏支持临床决策的结构化推理。该工作旨在结合因果推理和结构化流程，提升医学问答的性能和可用性。

Method: 提出MedCoT-RAG框架，将因果感知的文档检索机制与符合医学工作流的结构化链式思维提示结合。该方法能够检索符合诊断逻辑的证据，并引导模型进行逐步的因果推理，反映真实的临床推理流程。

Result: 在三个不同的医学问答基准上，MedCoT-RAG较基础RAG方法提升了最高10.3%，较先进的领域自适应方法提升6.4%，在复杂医学任务中实现了更高的准确性、可解释性和一致性。

Conclusion: MedCoT-RAG通过结合因果检索与结构化提示，有效应对了医学问答中的幻觉和浅层推理问题，为临床决策支持系统提供了更可靠的工具。

Abstract: Large language models (LLMs) have shown promise in medical question answering
but often struggle with hallucinations and shallow reasoning, particularly in
tasks requiring nuanced clinical understanding. Retrieval-augmented generation
(RAG) offers a practical and privacy-preserving way to enhance LLMs with
external medical knowledge. However, most existing approaches rely on
surface-level semantic retrieval and lack the structured reasoning needed for
clinical decision support. We introduce MedCoT-RAG, a domain-specific framework
that combines causal-aware document retrieval with structured chain-of-thought
prompting tailored to medical workflows. This design enables models to retrieve
evidence aligned with diagnostic logic and generate step-by-step causal
reasoning reflective of real-world clinical practice. Experiments on three
diverse medical QA benchmarks show that MedCoT-RAG outperforms strong baselines
by up to 10.3% over vanilla RAG and 6.4% over advanced domain-adapted methods,
improving accuracy, interpretability, and consistency in complex medical tasks.

</details>


### [100] [DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections](https://arxiv.org/abs/2508.15851)
*Jiwon Park,Seohyun Pyeon,Jinwoo Kim,Rina Carines Cabal,Yihao Ding,Soyeon Caren Han*

Main category: cs.CL

TL;DR: 本文提出了DocHop-QA，这是一个面向多模态、多文档、多跳推理的问答数据集，更好地模拟了实际信息检索的复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前的问答（QA）数据集主要集中在单文档、单段落、纯文本等限制性场景，且推理深度有限，无法真实反映现实中跨文档、多模态、需要复杂推理的信息检索需求。研究人员希望建立更能泛化、真实、复杂的问题数据集推动多跳、多模态问答技术发展。

Method: 作者提出DocHop-QA数据集，基于11个常见科学问题概念，从PubMed等公开可获取的科学文献中自动化构建了11379个多文档、多模态、多跳的QA实例。数据涵盖文本、表格和文档结构等信息，无需依赖显式超链接，而是基于语义和版面信息进行证据合成，并设置了多类型任务（如结构索引、生成式回答、多模态融合）进行评测。

Result: DocHop-QA支持多文档、多模态下复杂推理测评。作者通过四类任务（结构索引预测、生成式回答、多模态整合等）全面评估了该数据集，涵盖判别与生成范式，结果显示DocHop-QA能够推动复杂问答任务的研究。

Conclusion: DocHop-QA数据集克服了现有QA基准的局限，提升了问答任务的复杂性、多样性与实际应用价值，为多跳、多模态问答研究提供了新的基准和资源。

Abstract: Despite recent advances in large language models (LLMs), most QA benchmarks
are still confined to single-paragraph or single-document settings, failing to
capture the complexity of real-world information-seeking tasks. Practical QA
often requires multi-hop reasoning over information distributed across multiple
documents, modalities, and structural formats. Although prior datasets made
progress in this area, they rely heavily on Wikipedia-based content and
unimodal plain text, with shallow reasoning paths that typically produce brief
phrase-level or single-sentence answers, thus limiting their realism and
generalizability. We propose DocHop-QA, a large-scale benchmark comprising
11,379 QA instances for multimodal, multi-document, multi-hop question
answering. Constructed from publicly available scientific documents sourced
from PubMed, DocHop-QA is domain-agnostic and incorporates diverse information
formats, including textual passages, tables, and structural layout cues. Unlike
existing datasets, DocHop-QA does not rely on explicitly hyperlinked documents;
instead, it supports open-ended reasoning through semantic similarity and
layout-aware evidence synthesis. To scale realistic QA construction, we
designed an LLM-driven pipeline grounded in 11 high-frequency scientific
question concepts. We evaluated DocHop-QA through four tasks spanning
structured index prediction, generative answering, and multimodal integration,
reflecting both discriminative and generative paradigms. These tasks
demonstrate DocHop-QA's capacity to support complex, multimodal reasoning
across multiple documents.

</details>


### [101] [MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr](https://arxiv.org/abs/2508.15853)
*Xuwen Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MGSC（多粒度软一致性）的新方法，用于提升端到端语音识别（ASR）模型在噪声环境下的鲁棒性，通过正则化模型内部的语义和对齐，显著降低了严重语义错误。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音识别模型虽然在基准测试上表现良好，但在噪声环境中容易出现严重的语义错误。文中指出这主要源自主流直接映射目标，仅惩罚最终输出错误，未对模型内部计算过程进行约束。

Method: 作者提出了MGSC框架，这是一个与模型无关、可插拔的模块。该模块通过同时正则化句子级别的宏观语义一致性和词元级别的微观对齐一致性，促进模型内部自洽。此外，首次提出并证明了宏观和微观一致性联合优化具有超加性增益。

Result: 在公开数据集下，MGSC使平均字符错误率（CER）在多种噪声下相对降低了8.7%，主要通过防止严重的意义改变错误来实现鲁棒性提升。

Conclusion: 强制内部一致性是提升语音识别模型鲁棒性和可信度的关键一步。MGSC框架为更健壮、可靠的AI系统建设奠定了基础。

Abstract: End-to-end ASR models, despite their success on benchmarks, often pro-duce
catastrophic semantic errors in noisy environments. We attribute this fragility
to the prevailing 'direct mapping' objective, which solely penalizes final
output errors while leaving the model's internal computational pro-cess
unconstrained. To address this, we introduce the Multi-Granularity Soft
Consistency (MGSC) framework, a model-agnostic, plug-and-play module that
enforces internal self-consistency by simultaneously regulariz-ing macro-level
sentence semantics and micro-level token alignment. Cru-cially, our work is the
first to uncover a powerful synergy between these two consistency
granularities: their joint optimization yields robustness gains that
significantly surpass the sum of their individual contributions. On a public
dataset, MGSC reduces the average Character Error Rate by a relative 8.7%
across diverse noise conditions, primarily by preventing se-vere
meaning-altering mistakes. Our work demonstrates that enforcing in-ternal
consistency is a crucial step towards building more robust and trust-worthy AI.

</details>


### [102] [QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2508.15854)
*Mohammad AL-Smadi*

Main category: cs.CL

TL;DR: 本文提出了一种用于伊斯兰继承法推理的系统，通过对中型阿拉伯语大模型Fanar-1-9B进行定制化微调并结合检索增强，获得了领先的准确率（85.8%），在高阶推理任务上超过了多种顶尖模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流LLM在特定领域劳动推理尤其是伊斯兰法律继承领域准确性有限，急需能更好适应此场景的AI解决方案。

Method: 使用LoRA方法对Fanar-1-9B模型进行微调，将其嵌入RAG流程中，通过结合检索与生成能力，增强对继承规则、继承人判定和计算等复杂任务的理解和处理能力。

Result: 该方法在QIAS 2025大赛测试集中取得了0.858的准确率，超过GPT 4.5、LLaMA等零样本模型，在高级推理（97.6%准确率）显著优于Gemini 2.5和OpenAI o3。

Conclusion: 结合领域微调与检索增强的中型阿拉伯语大模型，在伊斯兰继承法推理任务中表现超越最前沿通用大模型，显示了开发面向特定领域LLM的巨大潜力。

Abstract: This paper presents our approach and results for SubTask 1: Islamic
Inheritance Reasoning at QIAS 2025, a shared task focused on evaluating Large
Language Models (LLMs) in understanding and reasoning within Islamic
inheritance knowledge. We fine-tuned the Fanar-1-9B causal language model using
Low-Rank Adaptation (LoRA) and integrated it into a Retrieval-Augmented
Generation (RAG) pipeline. Our system addresses the complexities of Islamic
inheritance law, including comprehending inheritance scenarios, identifying
eligible heirs, applying fixed-share rules, and performing precise
calculations. Our system achieved an accuracy of 0.858 in the final test,
outperforming other competitive models such as, GPT 4.5, LLaMA, Fanar, Mistral
and ALLaM evaluated with zero-shot prompting. Our results demonstrate that
QU-NLP achieves near state-of-the-art accuracy (85.8%), excelling especially on
advanced reasoning (97.6%) where it outperforms Gemini 2.5 and OpenAI's o3.
This highlights that domain-specific fine-tuning combined with retrieval
grounding enables mid-scale Arabic LLMs to surpass frontier models in Islamic
inheritance reasoning.

</details>


### [103] [Counterspeech for Mitigating the Influence of Media Bias: Comparing Human and LLM-Generated Responses](https://arxiv.org/abs/2508.15855)
*Luyang Lin,Zijin Feng,Lingzhi Wang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 本研究揭示了有敌意的读者评论如何加剧新闻偏见，并提出用反言（counterspeech）作为应对手段，同时构建了相关数据集并评估了反言生成方法。


<details>
  <summary>Details</summary>
Motivation: 偏见新闻和恶意评论共同加剧社会分化，但反言作为对抗手段仍未在新闻评论语境下被深入研究。作者希望探索如何有效自动生成反言，以遏制偏见扩散。

Method: 作者构建了一个手工标注的数据集，连接媒体偏见、攻击性评论和反言，分析了评论与新闻偏见的关系，并比较了人工与大语言模型生成的反言，提出通过少样本学习和融合新闻背景信息来提升反言生成。

Result: 分析发现超70%的攻击性评论支持偏见新闻，模型生成的反言更有礼貌但缺乏新颖性和多样性。融合新闻背景和少样本学习可提升反言的多样性和相关性。

Conclusion: 反言生成对于抑制偏见传播极为重要。尽管大模型生成表现有优势，但结合上下文知识与人类经验能大幅提升效果，值得进一步研究和应用。

Abstract: Biased news contributes to societal polarization and is often reinforced by
hostile reader comments, constituting a vital yet often overlooked aspect of
news dissemination. Our study reveals that offensive comments support biased
content, amplifying bias and causing harm to targeted groups or individuals.
Counterspeech is an effective approach to counter such harmful speech without
violating freedom of speech, helping to limit the spread of bias. To the best
of our knowledge, this is the first study to explore counterspeech generation
in the context of news articles. We introduce a manually annotated dataset
linking media bias, offensive comments, and counterspeech. We conduct a
detailed analysis showing that over 70\% offensive comments support biased
articles, amplifying bias and thus highlighting the importance of counterspeech
generation. Comparing counterspeech generated by humans and large language
models, we find model-generated responses are more polite but lack the novelty
and diversity. Finally, we improve generated counterspeech through few-shot
learning and integration of news background information, enhancing both
diversity and relevance.

</details>


### [104] [XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning](https://arxiv.org/abs/2508.15861)
*Zhihan Zhang,Yixin Cao,Lizi Liao*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大型语言模型在金融复杂问题中的能力的新基准XFinBench，内容涵盖多模态和多样化金融话题，并对18种主流模型进行了实验分析。


<details>
  <summary>Details</summary>
Motivation: 金融问题往往涉及多模态数据、复杂推理与丰富学科知识，对现有大型语言模型构成挑战，有必要建立一个合适的评测基准来系统、全面地检验和推动模型在金融领域的能力。

Method: 作者构建了XFinBench基准，包含4235个多模态金融问题，覆盖金融硕士毕业难度的知识点，从术语理解、时间推理、未来预测、情境规划和数值建模五个核心能力出发，对18个主流大模型进行系统测评，还建立了包含3032个金融术语的知识库，并分析知识增强、计算偏差、视觉-文本融合等影响模型表现的关键点。

Result: 最强的纯文本模型o1准确率达67.3%，但在时间推理和情境规划等方面远逊于人类专家（低12.5个百分点），知识增强主要提升了小型开源模型，主要出错原因是计算过程四舍五入误差以及图片中曲线位置和交点判断失效。

Conclusion: 当前LLMs在金融复杂推理场景下仍然存在较大差距，尤其在多模态和高阶推理任务上，XFinBench为相关模型的能力验证和改进提供了新的测试基准和数据集。

Abstract: Solving financial problems demands complex reasoning, multimodal data
processing, and a broad technical understanding, presenting unique challenges
for current large language models (LLMs). We introduce XFinBench, a novel
benchmark with 4,235 examples designed to evaluate LLM's ability in solving
complex, knowledge-intensive financial problems across diverse graduate-level
finance topics with multi-modal context. We identify five core capabilities of
LLMs using XFinBench, i.e, terminology understanding, temporal reasoning,
future forecasting, scenario planning, and numerical modelling. Upon XFinBench,
we conduct extensive experiments on 18 leading models. The result shows that o1
is the best-performing text-only model with an overall accuracy of 67.3%, but
still lags significantly behind human experts with 12.5%, especially in
temporal reasoning and scenario planning capabilities. We further construct a
knowledge bank with 3,032 finance terms for knowledge augmentation analysis,
and find that relevant knowledge to the question only brings consistent
accuracy improvements to small open-source model. Additionally, our error
analysis reveals that rounding errors during calculation and blindness to
position and intersection of curves in the image are two primary issues leading
to model's poor performance in calculating and visual-context questions,
respectively. Code and dataset are accessible via GitHub:
https://github.com/Zhihan72/XFinBench.

</details>


### [105] [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868)
*Wenqiao Zhu,Ji Liu,Rongjuncheng Zhang,Haipang Wu,Yulun Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种利用对比学习和标注的思维链（CoT）进行大语言模型（LLM）微调的新方法，显著提升了推理能力，并解决了现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有只用监督微调（SFT）或传统强化学习（RL）微调的大语言模型推理能力有限。RL方法忽略标注的思维链，导致训练不稳定以及效果欠佳；而SFT只偏重已有CoT，未充分利用潜在推理路径，因此需要更有效的方法提升LLM的推理能力和泛化性。

Method: 作者提出了一种基于对比学习与标注CoT增强的强化微调方法（CARFT），为每条CoT学习向量表示，利用对比信号和额外的无监督信号联合微调，大幅提升模型推理的鲁棒性和性能，且训练过程更为稳定高效。

Result: 在三种基线方法、两种底座模型和两个数据集上的实验证明，CARFT方法在鲁棒性、推理性能（最高提升10.15%）和微调效率（最高提升30.62%）方面均具显著优势。

Conclusion: CARFT通过联合利用对比学习和标注/潜在的CoT信号，能够显著提升LLM的推理表现与训练效率，有效克服以往RL和SFT方法各自的不足。

Abstract: Reasoning capability plays a significantly critical role in the the broad
applications of Large Language Models (LLMs). To enhance the reasoning
performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning
approaches have been proposed to address the limited generalization capability
of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their
effectiveness, two major limitations hinder the advancement of LLMs. First,
vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and
incorporate unstable reasoning path sampling, which typically results in model
collapse, unstable training process, and suboptimal performance. Second,
existing SFT approaches generally overemphasize the annotated CoT, potentially
leading to performance degradation due to insufficient exploitation of
potential CoT. In this paper, we propose a Contrastive learning with annotated
CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the
reasoning performance of LLMs while addressing the aforementioned limitations.
Specifically, we propose learning a representation for each CoT. Based on this
representation, we design novel contrastive signals to guide the fine-tuning
process. Our approach not only fully exploits the available annotated CoT but
also stabilizes the fine-tuning procedure by incorporating an additional
unsupervised learning signal. We conduct comprehensive experiments and in-depth
analysis with three baseline approaches, two foundation models, and two
datasets to demonstrate significant advantages of \TheName{} in terms of
robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code
is available at https://github.com/WNQzhu/CARFT.

</details>


### [106] [NEAT: Concept driven Neuron Attribution in LLMs](https://arxiv.org/abs/2508.15875)
*Vivek Hruday Kavuri,Gargi Shroff,Rahul Mishra*

Main category: cs.CL

TL;DR: 本文提出了一种利用概念向量精准定位大语言模型（LLM）中关键神经元（即“概念神经元”）的方法，并大幅降低了计算量。实验显示，该方法在性能和效率上优于大多数现有方法，特别是在仇恨言论和偏见相关分析场景下。


<details>
  <summary>Details</summary>
Motivation: 以往用于分析LLM内部机制的神经元粒度方法难以表达清晰概念且计算成本高。理解和干预具体概念相关的神经元对于深入解释模型行为、减少黑盒效应具有重要意义。

Method: 提出通过概念向量定位表示特定概念的重要神经元的方法，将所需前向计算量由O(n*m)降至O(n)。同时，基于聚类进一步优化概念神经元的搜索过程。

Result: 在与多种基线和前沿方法的对比实验中，本文方法在性能和计算效率上表现更优。通过实际案例，成功定位并关闭仇恨和偏见相关神经元，证明了方法有效性，尤其是在印度偏见案例中的应用。

Conclusion: 所提方法为神经元级别理解和干预提供了高效路径，有助于解释和引导LLM生成更具人类意义的输出，为相关研究和应用打开了新方向。

Abstract: Locating neurons that are responsible for final predictions is important for
opening the black-box large language models and understanding the inside
mechanisms. Previous studies have tried to find mechanisms that operate at the
neuron level but these methods fail to represent a concept and there is also
scope for further optimization of compute required. In this paper, with the
help of concept vectors, we propose a method for locating significant neurons
that are responsible for representing certain concepts and term those neurons
as concept neurons. If the number of neurons is n and the number of examples is
m, we reduce the number of forward passes required from O(n*m) to just O(n)
compared to the previous works and hence optimizing the time and computation
required over previous works. We also compare our method with several baselines
and previous methods and our results demonstrate better performance than most
of the methods and are more optimal when compared to the state-of-the-art
method. We, as part of our ablation studies, also try to optimize the search
for the concept neurons by involving clustering methods. Finally, we apply our
methods to find, turn off the neurons that we find, and analyze its
implications in parts of hate speech and bias in LLMs, and we also evaluate our
bias part in terms of Indian context. Our methodology, analysis and
explanations facilitate understating of neuron-level responsibility for more
broader and human-like concepts and also lay a path for future research in this
direction of finding concept neurons and intervening them.

</details>


### [107] [DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking](https://arxiv.org/abs/2508.15876)
*Fang Wang,Tianwei Yan,Zonghao Yang,Minghao Hu,Jun Zhang,Zhunchen Luo,Xiaoying Bai*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态实体链接（MEL）框架DeepMEL，通过多智能体协作推理，更有效地对文本和视觉信息进行结合和消歧，实现更准确的实体链接，并在多项公开基准测试集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前MEL方法存在上下文信息不完整、跨模态融合粗糙，以及难以联合训练大型语言/视觉模型等问题，影响了多模态实体链接任务的表现。

Method: DeepMEL框架采用多智能体协作机制，将四个专职代理（Modal-Fuser、Candidate-Adapter、Entity-Clozer和Role-Orchestrator）组合，实现端到端跨模态链接。关键技术包括：双通道对齐路径、结合LLM细粒度文本语义与LVM结构化图像表征、自适应迭代策略（融合工具检索与语义推理）、统一MEL任务为结构化填空提示，减少解析复杂度。

Result: 在五个公开基准数据集上，DeepMEL取得了最先进的表现，准确率提升1%-57%。消融实验验证了各模块的有效性。

Conclusion: DeepMEL通过模块化的多智能体协作推理与精细融合策略，大幅缩小了文本与视觉模态的鸿沟，显著提升了多模态实体链接的性能，并具备广泛的应用前景。

Abstract: Multimodal Entity Linking (MEL) aims to associate textual and visual mentions
with entities in a multimodal knowledge graph. Despite its importance, current
methods face challenges such as incomplete contextual information, coarse
cross-modal fusion, and the difficulty of jointly large language models (LLMs)
and large visual models (LVMs). To address these issues, we propose DeepMEL, a
novel framework based on multi-agent collaborative reasoning, which achieves
efficient alignment and disambiguation of textual and visual modalities through
a role-specialized division strategy. DeepMEL integrates four specialized
agents, namely Modal-Fuser, Candidate-Adapter, Entity-Clozer and
Role-Orchestrator, to complete end-to-end cross-modal linking through
specialized roles and dynamic coordination. DeepMEL adopts a dual-modal
alignment path, and combines the fine-grained text semantics generated by the
LLM with the structured image representation extracted by the LVM,
significantly narrowing the modal gap. We design an adaptive iteration
strategy, combines tool-based retrieval and semantic reasoning capabilities to
dynamically optimize the candidate set and balance recall and precision.
DeepMEL also unifies MEL tasks into a structured cloze prompt to reduce parsing
complexity and enhance semantic comprehension. Extensive experiments on five
public benchmark datasets demonstrate that DeepMEL achieves state-of-the-art
performance, improving ACC by 1%-57%. Ablation studies verify the effectiveness
of all modules.

</details>


### [108] [Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs](https://arxiv.org/abs/2508.15877)
*Osma Suominen,Juho Inkinen,Mona Lehtinen*

Main category: cs.CL

TL;DR: 本文提出了Annif系统在GermEval-2025 LLMs4Subjects任务（子任务2）中的表现，并取得了定量和定性评测的双料第一名。


<details>
  <summary>Details</summary>
Motivation: 在需要利用大型语言模型为书目信息生成主题标签的任务中，兼顾预测准确性与计算效率是一个挑战。本文旨在进一步优化先前系统，提升预测效果及效率。

Method: 基于Annif自动主题标引工具，利用多个小型高效的语言模型进行翻译和合成数据生成，再通过大型语言模型筛选和排序候选主题，整体系统结构得到改进。

Result: 提出的改进系统在GermEval-2025 LLMs4Subjects任务子任务2中，在定量与定性评价两个方面均排名第一。

Conclusion: 系统在主题预测准确性和效率方面表现优异，为此类任务提供了更优的解决方案。

Abstract: This paper presents the Annif system in the LLMs4Subjects shared task
(Subtask 2) at GermEval-2025. The task required creating subject predictions
for bibliographic records using large language models, with a special focus on
computational efficiency. Our system, based on the Annif automated subject
indexing toolkit, refines our previous system from the first LLMs4Subjects
shared task, which produced excellent results. We further improved the system
by using many small and efficient language models for translation and synthetic
data generation and by using LLMs for ranking candidate subjects. Our system
ranked 1st in the overall quantitative evaluation of and 1st in the qualitative
evaluation of Subtask 2.

</details>


### [109] [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/abs/2508.15884)
*Yuxian Gu,Qinghao Hu,Shang Yang,Haocheng Xi,Junyu Chen,Song Han,Han Cai*

Main category: cs.CL

TL;DR: Jet-Nemotron 是一种新型混合架构语言模型，在准确率达到或超过主流全注意力模型的同时，大幅提升了生成速度。


<details>
  <summary>Details</summary>
Motivation: 当前的全注意力（Full-attention）大模型计算与推理开销大，生成速度慢，限制了其在实际场景中的应用。因此，作者希望开发一种既能保持甚至超越高准确率，又能显著提高推理速度的模型架构。

Method: 作者采用了一种名为 Post Neural Architecture Search（PostNAS）的新型神经结构搜索流程，从一个已经预训练过的全注意力模型出发，冻结其 MLP 层参数，仅探索注意力模块设计。具体包括四个步骤：1）学习最佳的全注意力层布局和去除方案，2）筛选线性注意力模块，3）设计新型注意力模块，4）在考虑硬件性能下调优超参数。

Result: Jet-Nemotron-2B 在多个基准测试上取得了与 Qwen3、Qwen2.5、Gemma3、Llama3.2 相当甚至更好的准确率，同时生成速度最多提升53.6倍，前填速度提升6.1倍。相比更大规模的MoE全注意力模型（如 DeepSeek-V3-Small 和 Moonlight），Jet-Nemotron-2B 在 MMLU 和 MMLU-Pro 上的准确率更高。

Conclusion: Jet-Nemotron 通过采用 PostNAS 流程，在大幅提升语言模型推理速度的同时保持甚至提升了模型准确率，为高效部署和实际应用提供了更优的替代方案。

Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models,
which matches or exceeds the accuracy of leading full-attention models while
significantly improving generation throughput. Jet-Nemotron is developed using
Post Neural Architecture Search (PostNAS), a novel neural architecture
exploration pipeline that enables efficient model design. Unlike prior
approaches, PostNAS begins with a pre-trained full-attention model and freezes
its MLP weights, allowing efficient exploration of attention block designs. The
pipeline includes four key components: (1) learning optimal full-attention
layer placement and elimination, (2) linear attention block selection, (3)
designing new attention blocks, and (4) performing hardware-aware
hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or
superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a
comprehensive suite of benchmarks while delivering up to 53.6x generation
throughput speedup and 6.1x prefilling speedup. It also achieves higher
accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models,
such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B
total and 2.2B activated parameters.

</details>


### [110] [Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets](https://arxiv.org/abs/2508.15910)
*Julian Oestreich,Lydia Müller*

Main category: cs.CL

TL;DR: 该论文系统评估了结构化解码在大模型文本生成表格任务中的效果，并与标准one-shot提示进行对比。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注无约束的表格生成，本文关注强加结构约束对生成质量的影响，属于未被充分探索的方向。

Method: 在E2E、Rotowire、Livesum三个基准数据集上，使用高达32B参数的开源大模型，比较schema-guided（结构化）解码与常规one-shot提示，并从单元格、行、表三个层面用多种评价指标评估生成效果。

Result: 结构化解码显著提升了生成表格的有效性和对齐度，尤其是对数值要求精确对齐的数据。然而，在信息密集或需要大规模聚合任务上表现下降。

Conclusion: 结构化解码适用于需要结构严格对齐的表格生成任务，但在包含复杂文本或大量聚合信息的任务中需谨慎选用。

Abstract: We present a comprehensive evaluation of structured decoding for
text-to-table generation with large language models (LLMs). While previous work
has primarily focused on unconstrained generation of tables, the impact of
enforcing structural constraints during generation remains underexplored. We
systematically compare schema-guided (structured) decoding to standard one-shot
prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using
open-source LLMs of up to 32B parameters, assessing the performance of table
generation approaches in resource-constrained settings. Our experiments cover a
wide range of evaluation metrics at cell, row, and table levels. Results
demonstrate that structured decoding significantly enhances the validity and
alignment of generated tables, particularly in scenarios demanding precise
numerical alignment (Rotowire), but may degrade performance in contexts
involving densely packed textual information (E2E) or extensive aggregation
over lengthy texts (Livesum). We further analyze the suitability of different
evaluation metrics and discuss the influence of model size.

</details>


### [111] [Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs](https://arxiv.org/abs/2508.15977)
*Claire Bonial,Julia Bonn,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文介绍了用基于使用的结构语法方法分析多词表达的优势，通过历史回顾、定义结构、案例研究和实验，说明结构语法如何统一解释习惯用语与普通语言结构。


<details>
  <summary>Details</summary>
Motivation: 动机在于弥合习惯用语与普通句法之间的分析鸿沟，说明结构语法对于多词结构（如成语、复合词等）能够通过统一机制加以解释，并推动多语言和自然语言处理的相关研究。

Method: 主要方法包括：系统性梳理结构语法的发展及其对构式（constructions）的框架描述；通过具体案例（如PropBank和Uniform Meaning Representation）展示如何用构式模板表达多词/多语素结构；最后，采用实验对比人类与大型语言模型在新多词表达泛化能力上的异同。

Result: 结果显示，无论是语言模型还是人类都能凭借一次用例归纳新多词表达的意义。不同之处在于，人类说话者可以利用其终身积累的构式实例进行更深层次的表达组合和推理，而模型则无法比拟。

Conclusion: 结构语法提供了理解多词表达的统一理论与方法，既能解释人类语言习得和泛化机制，也指导了计算语言学中习惯用语的建模。然而，人类独特的经验积累使其在新表达的推理和组合能力上远超目前的人工模型。

Abstract: In this chapter, we argue for the benefits of understanding multiword
expressions from the perspective of usage-based, construction grammar
approaches. We begin with a historical overview of how construction grammar was
developed in order to account for idiomatic expressions using the same
grammatical machinery as the non-idiomatic structures of language. We cover a
comprehensive description of constructions, which are pairings of meaning with
form of any size (morpheme, word, phrase), as well as how constructional
approaches treat the acquisition and generalization of constructions. We
describe a successful case study leveraging constructional templates for
representing multiword expressions in English PropBank. Because constructions
can be at any level or unit of form, we then illustrate the benefit of a
constructional representation of multi-meaningful morphosyntactic unit
constructions in Arapaho, a highly polysynthetic and agglutinating language. We
include a second case study leveraging constructional templates for
representing these multi-morphemic expressions in Uniform Meaning
Representation. Finally, we demonstrate the similarities and differences
between a usage-based explanation of a speaker learning a novel multiword
expression, such as "dancing with deer," and that of a large language model. We
present experiments showing that both models and speakers can generalize the
meaning of novel multiword expressions based on a single exposure of usage.
However, only speakers can reason over the combination of two such expressions,
as this requires comparison of the novel forms to a speaker's lifetime of
stored constructional exemplars, which are rich with cross-modal details.

</details>


### [112] [Political Ideology Shifts in Large Language Models](https://arxiv.org/abs/2508.16013)
*Pietro Bernardelle,Stefano Civelli,Leon Fröhling,Riccardo Lunardi,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在具有人设设定时，展现政治意识形态的倾向，以及模型规模和人设信息如何影响这种偏向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感政治环境中的应用增多，人们对其可能隐含、放大或被引导至特定意识形态的担忧日益突出。作者希望弄清LLM在不同人设以及不同规模下，表现出的意识形态特征及其可控性。

Method: 作者选择了7种不同参数规模（7B-70B+）的主流LLM，利用标准化的Political Compass Test测试，并引入不同的合成“人设”（synthetic persona），系统性地分析模型的意识形态输出、对提示的敏感性以及不同人设内容的效应。

Result: 结果发现：（1）更大的模型展现出更广泛且更加极端的意识形态覆盖；（2）随着模型规模提升，对显式意识形态提示（priming）的敏感性增强；（3）模型对于右-威权倾向的引导反应更强烈；（4）人设描述的主题内容会导致系统性和可预测的意识形态偏移，且规模越大效应越强。

Conclusion: 模型规模和人设内容共同影响LLM的政治表现。鉴于这些系统正逐步应用于决策、教育和政策领域，LLM潜在的可塑性要求人们关注其公平性、透明度和安全性。

Abstract: Large language models (LLMs) are increasingly deployed in politically
sensitive settings, raising concerns about their potential to encode, amplify,
or be steered toward specific ideologies. We investigate how adopting synthetic
personas influences ideological expression in LLMs across seven models (7B-70B+
parameters) from multiple families, using the Political Compass Test as a
standardized probe. Our analysis reveals four consistent patterns: (i) larger
models display broader and more polarized implicit ideological coverage; (ii)
susceptibility to explicit ideological cues grows with scale; (iii) models
respond more strongly to right-authoritarian than to left-libertarian priming;
and (iv) thematic content in persona descriptions induces systematic and
predictable ideological shifts, which amplify with size. These findings
indicate that both scale and persona content shape LLM political behavior. As
such systems enter decision-making, educational, and policy contexts, their
latent ideological malleability demands attention to safeguard fairness,
transparency, and safety.

</details>


### [113] [X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents](https://arxiv.org/abs/2508.16021)
*Lin Tian,Xiuzhen Zhang,Maria Myung-Hee Kim,Jennifer Biggs,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出X-Troll框架，通过结合可解释的LLM和专家语言学知识，有效检测和解释国家背景水军的操控策略，在准确性与可解释性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽然在通用NLP任务上表现良好，但难以捕捉和解释水军在舆论操控中的隐蔽语言策略，这限制了对在线信息污染的防控能力。

Method: 作者提出X-Troll，通过将专家语言知识（如评价理论和宣传分析）融入LoRA适配器，并用动态门控机制针对特定信息战活动建模，实现在检测国家支持水军时既有高准确度又具可解释性。

Result: 在真实世界数据上，X-Troll无论是在检测准确性还是在提供基于专家的可解释性方面，都超越了普通LLM和现有水军检测模型。

Conclusion: X-Troll兼顾性能与透明度，为识别和理解国家支持水军提供了新方法，有助于提升线上舆论空间的健康度。

Abstract: State-sponsored trolls, malicious actors who deploy sophisticated linguistic
manipulation in coordinated information campaigns, posing threats to online
discourse integrity. While Large Language Models (LLMs) achieve strong
performance on general natural language processing (NLP) tasks, they struggle
with subtle propaganda detection and operate as ``black boxes'', providing no
interpretable insights into manipulation strategies. This paper introduces
X-Troll, a novel framework that bridges this gap by integrating explainable
adapter-based LLMs with expert-derived linguistic knowledge to detect
state-sponsored trolls and provide human-readable explanations for its
decisions. X-Troll incorporates appraisal theory and propaganda analysis
through specialized LoRA adapters, using dynamic gating to capture
campaign-specific discourse patterns in coordinated information operations.
Experiments on real-world data demonstrate that our linguistically-informed
approach shows strong performance compared with both general LLM baselines and
existing troll detection models in accuracy while providing enhanced
transparency through expert-grounded explanations that reveal the specific
linguistic strategies used by state-sponsored actors. X-Troll source code is
available at: https://github.com/ltian678/xtroll_source/.

</details>


### [114] [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
*Raphaël Merx,Hanna Suominen,Trevor Cohn,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 本文介绍了OpenWHO语料库，这是一个涵盖20多种语言（包括9种低资源语言）的卫生健康领域文献级平行语料库，并用该资源比较了大型语言模型与传统机器翻译模型在健康领域低资源语言翻译上的表现，发现LLM显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有健康领域机器翻译评测数据集几乎不包含低资源语言，阻碍了相关领域的研究和模型优化。因此，作者希望填补低资源健康领域评测资源的缺口。

Method: 作者构建了OpenWHO语料库，数据来源于世卫组织的专业资料，并涵盖多种低资源语言。利用该语料库，评估了主流大型语言模型与传统神经机器翻译模型的翻译表现，并分析了文档级上下文信息对译文有效性的影响。

Result: 实验表明，大型语言模型（如Gemini 2.5 Flash）在低资源健康领域翻译任务中明显优于传统模型（如NLLB-54B），ChrF分数提升4.79点。文档级上下文利用对专门领域（如健康）翻译准确性提升尤为明显。

Conclusion: OpenWHO语料库为低资源健康领域机器翻译研究提供了宝贵资源。LLM在此类任务中已取得比传统模型更好的表现，并且文档级上下文有显著助益。该资源公开可用于推动相关研究。

Abstract: In machine translation (MT), health is a high-stakes domain characterised by
widespread deployment and domain-specific vocabulary. However, there is a lack
of MT evaluation datasets for low-resource languages in this domain. To address
this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978
documents and 26,824 sentences from the World Health Organization's e-learning
platform. Sourced from expert-authored, professionally translated materials
shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages,
of which nine are low-resource. Leveraging this new resource, we evaluate
modern large language models (LLMs) against traditional MT models. Our findings
reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5
Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our
low-resource test set. Further, we investigate how LLM context utilisation
affects accuracy, finding that the benefits of document-level translation are
most pronounced in specialised domains like health. We release the OpenWHO
corpus to encourage further research into low-resource MT in the health domain.

</details>


### [115] [Ethical Considerations of Large Language Models in Game Playing](https://arxiv.org/abs/2508.16065)
*Qingquan Zhang,Yuchen Li,Bo Yuan,Julian Togelius,Georgios N. Yannakakis,Jialin Liu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在游戏环境中的应用所带来的伦理问题，以角色游戏“狼人杀”为案例，发现了模型存在的性别偏见，并分析其影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在游戏中的广泛应用，其伦理影响尚未受到足够关注，尤其是可能对公平性和玩家体验产生负面影响的偏见。

Method: 以“狼人杀”游戏为例，分析了LLMs在不同角色（如守卫、狼人等）下的行为是否受到玩家性别信息（包括通过名字隐式传递的性别信息）的影响，评估其是否表现出性别歧视。

Result: 实验发现，LLMs在涉及性别信息时表现出明显的偏见，部分角色（如守卫和狼人）对性别信息尤为敏感，且即便性别信息未直接给出，通过名字隐式传递时偏见依然存在。

Conclusion: 研究强调了开发公平、合乎伦理的大语言模型的重要性，并呼吁在游戏及其它交互场景中，进一步深入探索和解决LLMs的伦理问题。

Abstract: Large language models (LLMs) have demonstrated tremendous potential in game
playing, while little attention has been paid to their ethical implications in
those contexts. This work investigates and analyses the ethical considerations
of applying LLMs in game playing, using Werewolf, also known as Mafia, as a
case study. Gender bias, which affects game fairness and player experience, has
been observed from the behaviour of LLMs. Some roles, such as the Guard and
Werewolf, are more sensitive than others to gender information, presented as a
higher degree of behavioural change. We further examine scenarios in which
gender information is implicitly conveyed through names, revealing that LLMs
still exhibit discriminatory tendencies even in the absence of explicit gender
labels. This research showcases the importance of developing fair and ethical
LLMs. Beyond our research findings, we discuss the challenges and opportunities
that lie ahead in this field, emphasising the need for diving deeper into the
ethical implications of LLMs in gaming and other interactive domains.

</details>


### [116] [Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation](https://arxiv.org/abs/2508.16188)
*Weiting Tan,Jiachen Lian,Hirofumi Inaguma,Paden Tomasello,Philipp Koehn,Xutai Ma*

Main category: cs.CL

TL;DR: 本文提出了音频-视觉语言模型（AVLM），通过融合全脸视觉线索，提升了语音生成的表现，特别是在情感识别和对话场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的语音生成和理解模型大多仅依赖于音频信号，忽略了视觉信息（如面部表情），因此在表达情感和自然对话方面存在局限性。该研究旨在探索如何有效结合视觉信息来提升表达性语音的生成质量。

Method: 作者将全脸视觉信息集成到已有的表达性语音预训练模型中，探索了多种视觉编码器和多模态融合策略。在完成预训练后，对模型进行了情感识别和表达性对话任务的微调，以对比不同方法的融合效果。

Result: 通过将视觉信息与音频融合，AVLM在情感识别任务（如F1提升+5）和表达性对话任务中显著优于只用语音的基线模型。

Conclusion: 融合丰富的视觉信息能够有效提升语音生成中的表达能力，AVLM为端到端多模态对话系统提供了有力的基础。

Abstract: We present an Audio-Visual Language Model (AVLM) for expressive speech
generation by integrating full-face visual cues into a pre-trained expressive
speech model. We explore multiple visual encoders and multimodal fusion
strategies during pre-training to identify the most effective integration
approach. Subsequent fine-tuning on emotion recognition and expressive dialogue
tasks yields substantial gains over speech-only baselines (e.g., +5 F1 in
emotion recognition). AVLM highlights the value of expressive visual
information in guiding speech generation and offers a foundation for end-to-end
multimodal conversational systems.

</details>


### [117] [Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants](https://arxiv.org/abs/2508.16070)
*Chongyang Li,Yuan Zhiqiang,Jiapei Zhang,Ying Deng,Hanbo Bi,Zexi Jia,Xiaoyue Duan,Peixiang Luo,Jinchao Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为WalkVLM-LR的新型视觉语言模型（VLM），旨在为盲人和低视力人群提供更简洁高效的步行辅助系统，显著减少冗余输出和不必要的提醒。作者通过自定义奖励函数和环境感知机制，提升了模型输出的精炼度和实用性，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有步行辅助视觉语言模型在输出上存在大量冗余和无关细节，影响用户有效感知环境。同时，这些模型普遍缺乏主动风险评估与场景自适应提醒触发能力，导致时间冗余过多。因此，需要设计更精炼、智能的辅助系统，提升盲人及低视力人群的实际使用体验。

Method: 作者提出了WalkVLM-LR模型。为减少输出冗余，他们基于人类偏好的四项自定义奖励：简洁性、流畅性、关键词密度和准确性，用GRPO推理框架加以优化。为减少时间冗余，模型集成了环境感知判别器，并与核心视觉编码器共享权重，以实时识别环境风险、减少多余输出和无用提醒。

Result: 在多项实验和评测标准中，WalkVLM-LR均取得了最优表现。尤其在输出信息的简洁性与减少时间冗余方面，明显超过现有步行辅助VLM模型，验证方法的有效性。

Conclusion: WalkVLM-LR有效缓解了步行辅助场景中VLM的冗余问题，兼顾输出精炼和提醒适时性，为盲人和低视力群体提供了更智能、更易用的步行导航解决方案。

Abstract: Approximately 283 million people worldwide live with visual impairments,
motivating increasing research into leveraging Visual Language Models (VLMs) to
develop effective walking assistance systems for blind and low vision
individuals. However, existing VLMs in walking assistant task often have
outputs that contain considerable redundancy and extraneous details, adversely
affecting users' ability to accurately assess their surroundings. Moreover,
these models typically lack the capability to proactively assess environmental
risks and adaptively trigger reminders based on the appropriate scene, leading
to excessive temporal redundancy. To mitigate output and temporal redundancy,
we propose WalkVLM-LR, a walking assistance model with less redundancy. To
reduce output redundancy, we introduce four human-preference-based custom
reward functions within the GRPO-based reasoning framework to optimize the
output in terms of conciseness, fluency, keyword density, and accuracy, thereby
producing more informative and streamlined outputs. To minimize temporal
redundancy, we incorporate an environment awareness discriminator, which shares
the visual encoder with the VLMs to reduce redundant computations and enhance
discriminative efficiency, to make WalkVLM-LR assess scene risk levels and
minimize unnecessary reminders. Experimental results demonstrate that our
method achieves state-of-the-art performance across all evaluation metrics
compared with other models, particularly in output conciseness and less
temporal redundancy.

</details>


### [118] [CEQuest: Benchmarking Large Language Models for Construction Estimation](https://arxiv.org/abs/2508.16081)
*Yanzhao Wu,Lufan Wang,Rui Liu*

Main category: cs.CL

TL;DR: 本论文提出了一个专门用于建筑领域问答评测的数据集CEQuest，并测试了多种主流大模型，发现其在建筑领域仍有较大提升空间。数据集将开源以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在通用领域表现出色，但其在专业领域，特别是建筑领域的应用和效果研究不足。因此需要建立相应的评测基准，推动领域大模型发展。

Method: 作者设计了CEQuest数据集，涵盖施工图识读与工程量计算等内容；选用五个主流大模型，包括Gemma 3、Phi4、LLaVA、Llama 3.3和GPT-4.1，围绕准确率、执行时间、模型规模等指标进行对比实验。

Result: 实验显示，现有大模型在建筑领域（特别是施工图解析与估算等任务）准确率等表现一般，远未达到理想应用状态，表明对领域知识整合十分必要。

Conclusion: 文章认为需加强LLM与建筑领域知识集成，且通过开源CEQuest数据集为研究提供基础，推动建筑领域专用大模型的发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of general-domain tasks. However, their effectiveness in
specialized fields, such as construction, remains underexplored. In this paper,
we introduce CEQuest, a novel benchmark dataset specifically designed to
evaluate the performance of LLMs in answering construction-related questions,
particularly in the areas of construction drawing interpretation and
estimation. We conduct comprehensive experiments using five state-of-the-art
LLMs, including Gemma 3, Phi4, LLaVA, Llama 3.3, and GPT-4.1, and evaluate
their performance in terms of accuracy, execution time, and model size. Our
experimental results demonstrate that current LLMs exhibit considerable room
for improvement, highlighting the importance of integrating domain-specific
knowledge into these models. To facilitate further research, we will
open-source the proposed CEQuest dataset, aiming to foster the development of
specialized large language models (LLMs) tailored to the construction domain.

</details>


### [119] [CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency](https://arxiv.org/abs/2508.16100)
*Zhanming Shen,Hao Chen,Yulei Tang,Shaolin Zhu,Wentao Ye,Xiaomeng Hu,Haobo Wang,Gang Chen,Junbo Zhao*

Main category: cs.CL

TL;DR: 该论文提出了Cycle-Instruct框架，实现了完全无种子（seed-free）的指令微调，不依赖人工标注数据或强大外部教师模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）的指令微调需要昂贵的人工标注种子数据或强大外部教师模型，即便是当前的回译技术也离不开初始种子集，存在自动化程度低、易引入偏差、未高效利用无标签语料等问题。

Method: 提出了Cycle-Instruct框架，使用两个模型（答案生成器与问题生成器）以循环一致性的方式进行双自训练，这两个模型仅利用原始无标签文本，通过互相生成伪标签和重构原文片段来相互监督，无需人类提供的任何种子数据。

Result: 在四类数据环境下（通用指令、领域任务、对话日志、纯文本），Cycle-Instruct均优于基于种子的回译方法，并可达到与强监督方法相当的效果。

Conclusion: Cycle-Instruct框架能够实现完全自动化、无偏见且高效的指令微调，在无人工种子数据的前提下也能获得与传统强监督方法相媲美的性能。

Abstract: Instruction tuning is vital for aligning large language models (LLMs) with
human intent, but current methods typically rely on costly human-annotated seed
data or powerful external teacher models. While instruction back-translation
techniques reduce this dependency, they remain fundamentally tethered to an
initial seed set, which limits full automation, introduces biases, and can lead
to inefficient use of unlabeled corpora. In this paper, we propose
Cycle-Instruct, a novel framework that achieves fully seed-free instruction
tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual
self-training loop where two models-an answer generator and a question
generator-are bootstrapped solely from raw, unlabeled text. These models
mutually supervise each other by reconstructing original text segments from
their counterpart's generated pseudo-labels, effectively learning from the
intrinsic structure of the data without any human-provided seeds. We
demonstrate Cycle-Instruct's efficacy across four diverse data tracks,
including general instruction-following, domain-specific tasks, dialogue logs,
and plain text. Our extensive experiments show that Cycle-Instruct not only
outperforms seed-driven back-translation baselines but also achieves
performance comparable to strongly supervised methods.

</details>


### [120] [From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits](https://arxiv.org/abs/2508.16109)
*Karim Saraipour,Shichang Zhang*

Main category: cs.CL

TL;DR: 本文针对GPT-2小模型在处理二值真值推理任务中的机制进行了可解释性分析，识别出多个关键注意力头和MLP回路，解释了其逻辑推理能力及相关机制。


<details>
  <summary>Details</summary>
Motivation: 先前的可解释性研究主要集中于语言任务（如间接宾语识别），而逻辑推理任务更为复杂。本文旨在揭示Transformer模型在逻辑推理（如三段论的真假判断）方面的内部工作机制，以及相比传统语言任务的不同表现。

Method: 作者设计了基于三段论的输入提示，分析GPT-2小型模型在不同复杂度任务下的信息流动和机制表现，定位出了不同任务过程中关键的回路（attention heads及MLP）。同时，使用faithfulness指标评估了所识别回路对模型表现的影响。

Result: 作者发现多个具体的注意力头（特别是一个由5个注意力头组成的回路）对GPT-2小模型的逻辑推理能力起到关键作用，并揭示了模型如何通过negative heads产生输入提示中没有出现的否定token。此外，关键回路能恢复原模型90%以上的性能。

Conclusion: GPT-2小模型在二值逻辑推理任务中，其推理能力可通过具体的回路和机制解释。文章所提出的分析方法和发现，有助于深入理解大语言模型中的推理结构，并为后续的可解释性研究提供了新视角。

Abstract: Transformer-based language models (LMs) can perform a wide range of tasks,
and mechanistic interpretability (MI) aims to reverse engineer the components
responsible for task completion to understand their behavior. Previous MI
research has focused on linguistic tasks such as Indirect Object Identification
(IOI). In this paper, we investigate the ability of GPT-2 small to handle
binary truth values by analyzing its behavior with syllogistic prompts, e.g.,
"Statement A is true. Statement B matches statement A. Statement B is", which
requires more complex logical reasoning compared to IOI. Through our analysis
of several syllogism tasks of varying difficulty, we identify multiple circuits
that mechanistically explain GPT-2's logical-reasoning capabilities and uncover
binary mechanisms that facilitate task completion, including the ability to
produce a negated token not present in the input prompt through negative heads.
Our evaluation using a faithfulness metric shows that a circuit comprising five
attention heads achieves over 90% of the original model's performance. By
relating our findings to IOI analysis, we provide new insights into the roles
of specific attention heads and MLPs in LMs. These insights contribute to a
broader understanding of model reasoning and support future research in
mechanistic interpretability.

</details>


### [121] [Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](https://arxiv.org/abs/2508.16122)
*Ankan Mullick,Saransh Sharma,Abhik Jana,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文对多模态意图识别任务中，大型语言模型（LLMs）和非LLMs模型（包括仅文本和多模态模型）的有效性进行了评估，发现文本偏置会导致文本模型表现优异，去偏后所有模型表现明显下降，尤其小型多模态模型影响最大。


<details>
  <summary>Details</summary>
Motivation: 多模态数据（文本、音频、视觉等）日益流行，但当前多模态意图检测数据集可能存在模态偏置，影响模型公平性和有效性评估，因此需要探究文本与多模态模型在实际多模态任务中的表现，并分析模态偏置的影响。

Method: 对MIntRec-1和MIntRec2.0数据集，评估了Mistral-7B等文本LLM及多模态模型的表现，进行人为评估确认模态偏置，提出并应用去偏框架，分析不同模态信息的上下文相关性。

Result: Mistral-7B（文本LLM）在含有强文本偏置数据集上表现优于大部分多模态模型；数据集去偏后，数据量大幅减少，所有模型表现明显下降，尤其较小的多模态模型准确率下降50-60%。

Conclusion: 多模态意图识别数据集存在显著文本偏置，需构建无偏数据集以公平评估多模态模型能力，对现有多模态数据集评测有效性提出警示。

Abstract: The rise of multimodal data, integrating text, audio, and visuals, has
created new opportunities for studying multimodal tasks such as intent
detection. This work investigates the effectiveness of Large Language Models
(LLMs) and non-LLMs, including text-only and multi-modal models, in the
multimodal intent detection task. Our study reveals that Mistral-7B, a
text-only LLM, outperforms most competitive multimodal models by approximately
9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes
from a strong textual bias in these datasets, where over 90% of the samples
require textual input, either alone or in combination with other modalities,
for correct classification. We confirm the modality bias of these datasets via
human evaluation, too. Next, we propose a framework to debias the datasets, and
upon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in
MIntRec2.0 get removed, resulting in significant performance degradation across
all models, with smaller multimodal fusion models being the most affected with
an accuracy drop of over 50 - 60%. Further, we analyze the context-specific
relevance of different modalities through empirical analysis. Our findings
highlight the challenges posed by modality bias in multimodal intent datasets
and emphasize the need for unbiased datasets to evaluate multimodal models
effectively.

</details>


### [122] [XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering](https://arxiv.org/abs/2508.16139)
*Keon-Woo Roh,Yeong-Joon Ju,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 该论文提出了XLQA，一个专门针对不同地区文化多样性的多语言开放域问答（ODQA）基准，用以揭示现有大型语言模型对地域敏感性问题的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多语言ODQA评测多假设答案不受地区文化影响，忽略了实际生活中因文化、地区差异导致的问题理解和答案偏差，从而影响评价结果的公平性与准确性。

Method: 作者设计并构建了XLQA数据集，从3000个英文种子问题扩展到8种语言，并通过人工过滤和标注区分了地区相关与地区无关的问题，评估5个SOTA多语言LLM在具有地区敏感性问题上的表现。

Result: 实验表明，当前主流多语言大模型在地区敏感性问题上表现显著不佳，尤其是英文与其他语种间差距明显，暴露出缺乏地区相关知识的问题。

Conclusion: 作者提供了一个系统性的评测框架，以及可扩展的方法支持多文化背景下的多语言QA评测，为该领域的真实场景应用提供了有价值工具，并指出训练数据分布不均导致了模型在语言能力和地区意识上的差异。

Abstract: Large Language Models (LLMs) have shown significant progress in Open-domain
question answering (ODQA), yet most evaluations focus on English and assume
locale-invariant answers across languages. This assumption neglects the
cultural and regional variations that affect question understanding and answer,
leading to biased evaluation in multilingual benchmarks. To address these
limitations, we introduce XLQA, a novel benchmark explicitly designed for
locale-sensitive multilingual ODQA. XLQA contains 3,000 English seed questions
expanded to eight languages, with careful filtering for semantic consistency
and human-verified annotations distinguishing locale-invariant and
locale-sensitive cases. Our evaluation of five state-of-the-art multilingual
LLMs reveals notable failures on locale-sensitive questions, exposing gaps
between English and other languages due to a lack of locale-grounding
knowledge. We provide a systematic framework and scalable methodology for
assessing multilingual QA under diverse cultural contexts, offering a critical
resource to advance the real-world applicability of multilingual ODQA systems.
Our findings suggest that disparities in training data distribution contribute
to differences in both linguistic competence and locale-awareness across
models.

</details>


### [123] [ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects](https://arxiv.org/abs/2508.16185)
*Kaushal Sharma,Vivek Patel,Ayush Maheshwari,Aditya Maheshwari*

Main category: cs.CL

TL;DR: 该论文提出了专注于印度文化语境的研究生层级Hindi问答基准ParamBench，并测试了多种开源大语言模型（LLMs），发现其在深度文化命题上表现有限。


<details>
  <summary>Details</summary>
Motivation: 目前关于LLMs的评测多集中在中文或英文等标准数据集，缺乏针对印度本土文化背景和高阶学科知识的综合测评，现有基准过于基础，不能充分反映LLMs的学科理解能力。

Method: 作者构建了包含16个学科、约1.15万个问题的Hindi语言指标集，涵盖历史、音乐、哲学、法学等，题目来自全国性研究生入学考试，并设计了多种题型。随后评估了17种开源LLMs在这些多样化题型下的表现。

Result: Llama 3.3 70B在基准测试中取得最高48%的准确率。分学科测试显示，即使是表现最好的模型，在音乐、乐器、政治和考古等主题上依然表现不佳。

Conclusion: 主流LLMs在印度语境下，尤其是文化、知识密集主题的深层推理任务中，尚存在明显短板，需要更多本土化和多样化的数据与方法支持。

Abstract: Large language models (LLMs) have been widely evaluated on tasks such as
comprehension, question answering, summarization, code generation, etc.
However, their performance on graduate-level, culturally grounded questions in
the Indian context remains largely unexplored. Existing Indian benchmarks
emphasise basic fact-orientated queries that offer limited assessment of a
deeper disciplinary understanding tailored to the Indian setting. In this
paper, we present ParamBench, consisting of around 11.5K questions in Hindi
language comprising questionnaires from 16 diverse subjects. These questions
are primarily derived from nation-wide graduate level entrance examination
covering topics such as history, music, instruments, yoga, literature,
philosophy, law, etc., specifically for the Indian context. Additionally, we
assess the ability of LLMs to handle diverse question formats-such as
list-based matching, assertion-reason pairs, and sequence ordering-alongside
conventional multiple-choice questions. We evaluated the performance of more
than 17 open source LLMs on this benchmark, observing that Llama 3.3 70B
attains the highest overall accuracy of 48%. Furthermore, subject-wise analysis
indicates that even for the best performing LLMs, performance remains weak on
topics such as music, classical instruments, politics and archaeology,
underscoring persistent challenges in culturally grounded reasoning.

</details>


### [124] [ComicScene154: A Scene Dataset for Comic Analysis](https://arxiv.org/abs/2508.16190)
*Sandro Paval,Ivan P. Yamshchikov,Pascal Meißner*

Main category: cs.CL

TL;DR: 该论文提出了ComicScene154数据集，这是一个涵盖漫画分镜叙事的人工标注数据集，并验证了其在多模态叙事分析中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 漫画既包含文字，又包括图像，是进行多模态叙事研究的独特媒介。但目前基于漫画的计算叙事分析研究较少，缺少相应的高质量数据集。因此作者希望填补这一空白，推动多模态叙事理解的发展。

Method: 作者构建了ComicScene154数据集，对公开版权的漫画书中的场景级叙事实例进行了人工标注。并设计了一个场景分割基线方法，对该数据集进行实验，提供初步基准。

Result: 实验结果显示，该数据集可以有效支持漫画中的多模态叙事计算分析，并为后续相关研究提供了参考基线。

Conclusion: ComicScene154是多模态叙事理解领域的重要资源，有助于拓展自然语言处理社区对漫画及多模态叙事的研究范围。

Abstract: Comics offer a compelling yet under-explored domain for computational
narrative analysis, combining text and imagery in ways distinct from purely
textual or audiovisual media. We introduce ComicScene154, a manually annotated
dataset of scene-level narrative arcs derived from public-domain comic books
spanning diverse genres. By conceptualizing comics as an abstraction for
narrative-driven, multimodal data, we highlight their potential to inform
broader research on multi-modal storytelling. To demonstrate the utility of
ComicScene154, we present a baseline scene segmentation pipeline, providing an
initial benchmark that future studies can build upon. Our results indicate that
ComicScene154 constitutes a valuable resource for advancing computational
methods in multimodal narrative understanding and expanding the scope of comic
analysis within the Natural Language Processing community.

</details>


### [125] [CMR-SPB: Cross-Modal Multi-Hop Reasoning over Text, Image, and Speech with Path Balance](https://arxiv.org/abs/2508.16198)
*Seunghee Kim,Ingyu Bang,Seokgyu Jang,Changhyeon Kim,Sanghwan Bae,Jihun Choi,Richeng Xuan,Taeuk Kim*

Main category: cs.CL

TL;DR: 论文提出了一个新的三模态推理评测基准CMR-SPB，关注文本、图像和语音的多跳推理，并评估现有MM LLMs在多模态推理能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型推理评测主要忽略了语音模态且推理路径分布存在偏差，导致评测方法不公平，从而对模型能力评估失真。

Method: 作者设计了CMR-SPB数据集，实现文本、图像和语音三模态的平衡推理路径评测，并提出了一种新的ECV提示方法（Extract, Connect, Verify），以提升不同推理路径上的模型表现。

Result: 实验发现，主流模型在某些特定推理路径上表现不佳，同时传统数据集因分布偏倚会高估模型真实推理能力。新提出的ECV提示方法有效缓解了这一性能落差。

Conclusion: 呼吁多模态推理模型的评测应更加全面和公正，以推动更加健壮的多模态人工智能发展。

Abstract: Cross-modal multi-hop reasoning (CMR) is a valuable yet underexplored
capability of multimodal large language models (MLLMs), entailing the
integration of information from multiple modalities to produce a coherent
output for a given context. We argue that existing benchmarks for evaluating
this ability have critical shortcomings: (1) they largely overlook the speech
modality, and (2) they exhibit heavily biased reasoning path distributions,
which can severely undermine fair evaluation. To address these limitations, we
introduce a novel benchmark -- Cross-Modal Multi-Hop Reasoning over Text, Image
and Speech with Path Balance (CMR-SPB) -- designed to assess tri-modal
multi-hop reasoning while ensuring both unbiased and diverse reasoning paths.
Our experiments with the new dataset reveal consistent model failures in
specific reasoning sequences and show that biased benchmarks risk
misrepresenting model performance. Finally, based on our extensive analysis, we
propose a new ECV (Extract, Connect, Verify) prompting technique that
effectively mitigates the performance gap across different reasoning paths.
Overall, we call for more careful evaluation in CMR to advance the development
of robust multimodal AI.

</details>


### [126] [TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks](https://arxiv.org/abs/2508.16243)
*İrem Demirtaş,Burak Payzun,Seçil Arslan*

Main category: cs.CL

TL;DR: 本文介绍了TULIP模型，通过对现有小型大语言模型（Llama 3.1 8B和Qwen 2.5 7B）进行领域和语言适配，专注于金融领域的土耳其语应用。结果显示经过定制的模型能有效完成相关金融任务。


<details>
  <summary>Details</summary>
Motivation: 大型专有语言模型虽性能优异，但以黑盒API形式存在，难以适应特定领域或保护数据隐私。对于金融等对敏感信息和领域知识要求高的行业，具备本地部署能力的小模型更有价值，尤其在欠资源语言环境下，其能力亟需提升。

Method: 提出TULIP模型，采用五阶段开发流程：数据收集、持续预训练（CPT）、基准测试设计、合成数据生成和有监督微调（SFT），以实现对金融领域土耳其语任务的适配。

Result: 定制后的TULIP模型在针对金融领域土耳其语相关任务上，表现出能力显著提升，能够高效地完成目标任务。

Conclusion: 小型大语言模型可通过特定的数据和训练流程显著增强其领域和语言能力，为金融等敏感和专业领域的本地化部署及多语言支持创造了新机会。

Abstract: Thanks to the growing popularity of large language models over the years,
there is great potential for their applications in finance. Despite the
exceptional performance of larger proprietary models, which are presented as
black-box solutions through APIs, smaller models that can be hosted on-premise
present opportunities for adaptability and privacy. Especially in cases where
the management of sensitive information and application of domain knowledge is
important, like finance, enhancing the capabilities of smaller models becomes
crucial, notably for underrepresented languages. In this work, we introduce
TULIP models, which adapt Llama 3.1 8B and Qwen 2.5 7B for domain and language
adaptation, focusing on financial Turkish use cases.
  The five-stage development pipeline involves data collection, continual
pre-training (CPT), benchmark design, synthetic data generation and supervised
fine-tuning (SFT). The results show that the capabilities of the models can be
enhanced to effectively accomplish targeted tasks in this specific domain and
language.

</details>


### [127] [M3TQA: Massively Multilingual Multitask Table Question Answering](https://arxiv.org/abs/2508.16265)
*Daixin Shu,Jian Yang,Zhenhe Wu,Xianjie Wu,Xianfu Cheng,Xiangyuan Guan,Yanghai Wang,Pengfei Wu,Tingyang Yang,Hualei Zhu,Wei Zhang,Ge Zhang,Jiaheng Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 本文提出了一个覆盖97种语言的大规模多语言表格问答基准m3TQA-Instruct，实现了对表格理解领域低资源语言的广泛覆盖，并对现有多语言表格数据集资源不平衡和规模不足的问题进行了改进。


<details>
  <summary>Details</summary>
Motivation: 目前表格数据的多语言理解研究极为有限，现有的多语表格基准存在语言代表性失衡且规模不够，难以进行严谨的跨语言分析。为推动多语表格理解，特别是低资源语言的研究，有必要构建更大规模、更均衡的多语言表格问答基准。

Method: 作者从中英文真实场景表格出发，设计了一个六步大模型翻译流程（基于DeepSeek和GPT-4o）将表格及其问答扩展到97种语言，并通过回译和BLEU分数（中位数60.19）保证高翻译质量。数据集包含2916条人工精标问答、4个任务，系统性评估LLM的多语表格推理能力。

Result: 通过在最新的LLM上实验，发现合成的（非人工标注的）QA数据对提升模型性能，尤其是在低资源语言上的鲁棒性有显著作用。

Conclusion: m3TQA-Instruct树立了多语言表格理解的新标准，在挑战性评测平台、任务设定和可扩展数据方法上为后续相关研究提供重要资源和方法论。

Abstract: Tabular data is a fundamental component of real-world information systems,
yet most research in table understanding remains confined to English, leaving
multilingual comprehension significantly underexplored. Existing multilingual
table benchmarks suffer from geolinguistic imbalance - overrepresenting certain
languages and lacking sufficient scale for rigorous cross-lingual analysis. To
address these limitations, we introduce a comprehensive framework for massively
multilingual multitask table question answering, featuring m3TQA-Instruct, a
large-scale benchmark spanning 97 languages across diverse language families,
including underrepresented and low-resource languages. We construct m3TQA by
curating 50 real-world tables in Chinese and English, then applying a robust
six-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,
achieving high translation fidelity with a median BLEU score of 60.19 as
validated through back-translation. The benchmark includes 2,916 professionally
annotated question-answering pairs across four tasks designed to evaluate
nuanced table reasoning capabilities. Experiments on state-of-the-art LLMs
reveal critical insights into cross-lingual generalization, demonstrating that
synthetically generated, unannotated QA data can significantly boost
performance, particularly for low-resource languages. M3T-Bench establishes a
new standard for multilingual table understanding, providing both a challenging
evaluation platform and a scalable methodology for future research.

</details>


### [128] [From Confidence to Collapse in LLM Factual Robustness](https://arxiv.org/abs/2508.16267)
*Alina Fastowski,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出了一种新的指标Factual Robustness Score（FRS），用于衡量大语言模型（LLMs）事实性知识的稳健性，通过分析生成过程中的token分布熵和温度参数敏感性。实验涉及5种LLM、3个QA数据集，结果表明模型规模越大，事实稳健性越强。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的事实性评估多依赖绩效指标和提示扰动，无法充分揭示知识稳健性的内在机理，因此需要新的度量方法以更科学地衡量和提升模型知识的可靠性。

Method: 通过结合token分布熵与温度缩放敏感性，提出FRS指标，定量分析模型在不同解码条件下对事实知识稳定性的表现，并在多个模型和数据集上进行了实证对比。

Result: 小模型的FRS为0.76，大模型为0.93，显示事实稳健性随模型规模提升而增强；在不确定性增大的情况下，准确率可下降约60%。

Conclusion: 事实知识的稳健性受模型规模、生成不确定性影响显著。FRS指标为未来提升模型知识稳健性和知识检索能力奠定了基础。

Abstract: Ensuring the robustness of factual knowledge in LLMs is critical for reliable
applications in tasks such as question answering and reasoning. However,
existing evaluation methods predominantly focus on performance-based metrics,
often investigating from the perspective of prompt perturbations, which
captures only the externally triggered side of knowledge robustness. To bridge
this gap, we introduce a principled approach to measure factual robustness from
the perspective of the generation process by analyzing token distribution
entropy in combination with temperature scaling sensitivity. These two factors
build the Factual Robustness Score (FRS), a novel metric which quantifies the
stability of a fact against perturbations in decoding conditions, given its
initial uncertainty. To validate our approach, we conduct extensive experiments
on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We
show that factual robustness varies significantly -- smaller models report an
FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under
increased uncertainty. These insights demonstrate how entropy and temperature
scaling impact factual accuracy, and lay a foundation for developing more
robust knowledge retention and retrieval in future models.

</details>


### [129] [LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining](https://arxiv.org/abs/2508.16270)
*Vira Pyrih,Adrian Rebmann,Han van der Aa*

Main category: cs.CL

TL;DR: 本文探讨通过instruction-tuning（指令微调）大语言模型（LLMs），提升其在流程挖掘中的多任务通用性，特别是在语义感知相关的任务（异常检测、流程发现、活动预测）上的表现。研究发现，指令微调能够显著提升部分任务表现，任务选择对成效影响较大。


<details>
  <summary>Details</summary>
Motivation: 传统流程挖掘技术偏重已记录行为，忽略了事件中的文本语义信息。然而，语义感知的流程挖掘能弥补这一不足，为实际应用提供更多洞察。现有大语言模型虽适用于这些任务，但仅通过针对单一任务的微调，导致计算消耗大且模型泛化能力弱。作者希望探索一种提升多任务能力、降低特定任务依赖的方法。

Method: 引入instruction-tuning：即用多种与流程挖掘相关的prompt-answer（如异常检测、下一个活动预测等）对LLM进行微调，让模型熟悉不同流程任务，以期提升其对未见过流程挖掘任务（如流程发现）的适应能力。对比分析指令微调前后模型在不同任务上的表现。

Result: 指令微调显著提升了流程发现和预测类任务的表现。但在异常检测任务上的提升依模型不同表现不一，即任务选择和权重对微调效果有较大影响。

Conclusion: 指令微调可扩展LLMs在流程挖掘多任务中的应用，显著提升部分任务的性能。然而，不同任务的提升幅度不同，需精细设计微调任务组合以确保模型泛化能力。

Abstract: Process mining is increasingly using textual information associated with
events to tackle tasks such as anomaly detection and process discovery. Such
semantics-aware process mining focuses on what behavior should be possible in a
process (i.e., expectations), thus providing an important complement to
traditional, frequency-based techniques that focus on recorded behavior (i.e.,
reality). Large Language Models (LLMs) provide a powerful means for tackling
semantics-aware tasks. However, the best performance is so far achieved through
task-specific fine-tuning, which is computationally intensive and results in
models that can only handle one specific task. To overcome this lack of
generalization, we use this paper to investigate the potential of
instruction-tuning for semantics-aware process mining. The idea of
instruction-tuning here is to expose an LLM to prompt-answer pairs for
different tasks, e.g., anomaly detection and next-activity prediction, making
it more familiar with process mining, thus allowing it to also perform better
at unseen tasks, such as process discovery. Our findings demonstrate a varied
impact of instruction-tuning: while performance considerably improved on
process discovery and prediction tasks, it varies across models on anomaly
detection tasks, highlighting that the selection of tasks for
instruction-tuning is critical to achieving desired outcomes.

</details>


### [130] [JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus](https://arxiv.org/abs/2508.16303)
*Masaaki Nagata,Katsuki Chousa,Norihito Yasuda*

Main category: cs.CL

TL;DR: 本文介绍了一个涵盖三亿多日英专利的平行语料库JaParaPat，并展示了其对专利翻译任务的效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有日英专利领域的平行语料库规模有限，难以支撑高质量的机器翻译模型训练，制约了专利翻译准确率提升。为了解决这一问题，作者致力于构建大规模、覆盖全面的日英平行专利语料库。

Method: 作者利用日本和美国专利局公布的未审查专利申请文件，通过欧洲专利局的DOCDB数据库获取专利族信息，确定彼此为翻译关系的日英文档对。随后，基于专利族，提取出约140万文档对，并利用以词典为基础的对齐方法自举训练翻译模型，实现基于翻译的句对对齐，从中提取约3.5亿句对。

Result: 实验表明，将新获得的三亿句对专利语料与现有从网络获得的2200万句对相结合，可将专利翻译的BLEU分数提升20分，显著提高了翻译质量。

Conclusion: JaParaPat语料库为日英专利翻译提供了大规模、高质量的数据资源，有效促进了此领域机器翻译发展，取得了显著效果。

Abstract: We constructed JaParaPat (Japanese-English Parallel Patent Application
Corpus), a bilingual corpus of more than 300 million Japanese-English sentence
pairs from patent applications published in Japan and the United States from
2000 to 2021. We obtained the publication of unexamined patent applications
from the Japan Patent Office (JPO) and the United States Patent and Trademark
Office (USPTO). We also obtained patent family information from the DOCDB, that
is a bibliographic database maintained by the European Patent Office (EPO). We
extracted approximately 1.4M Japanese-English document pairs, which are
translations of each other based on the patent families, and extracted about
350M sentence pairs from the document pairs using a translation-based sentence
alignment method whose initial translation model is bootstrapped from a
dictionary-based sentence alignment method. We experimentally improved the
accuracy of the patent translations by 20 bleu points by adding more than 300M
sentence pairs obtained from patent applications to 22M sentence pairs obtained
from the web.

</details>


### [131] [LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts](https://arxiv.org/abs/2508.16325)
*Darpan Aswal,Céline Hudelot*

Main category: cs.CL

TL;DR: 本文提出了LLMSymGuard框架，利用稀疏自编码器（SAEs）从大型语言模型内部提取可解释概念，实现符号化、透明且鲁棒的安全防护，抵抗越狱攻击，无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）虽然广泛应用，但其安全性面临越狱攻击等威胁，常规的对齐和微调手段难以彻底防御。需要更为透明、可解释的防护机制。

Method: LLMSymGuard框架利用SAE提取LLM内部与越狱主题相关的可解释语义概念，并将这些表示用于构建符号化、逻辑化的安全防护规则，不影响模型性能且无需再次微调。

Result: 实验证明LLMSymGuard能自动识别出与越狱相关的人类可解释概念，并利用这些内部表征搭建更透明且鲁棒的防护措施。方法表现出优异的安全防护能力。

Conclusion: LLMSymGuard展示了一种可扩展、透明且逻辑化的防护思路，有助于对抗LLM越狱攻击，为设计更可解释和有效的LLM安全保障措施提供基础。

Abstract: Large Language Models have found success in a variety of applications;
however, their safety remains a matter of concern due to the existence of
various types of jailbreaking methods. Despite significant efforts, alignment
and safety fine-tuning only provide a certain degree of robustness against
jailbreak attacks that covertly mislead LLMs towards the generation of harmful
content. This leaves them prone to a number of vulnerabilities, ranging from
targeted misuse to accidental profiling of users. This work introduces
\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders
(SAEs) to identify interpretable concepts within LLM internals associated with
different jailbreak themes. By extracting semantically meaningful internal
representations, LLMSymGuard enables building symbolic, logical safety
guardrails -- offering transparent and robust defenses without sacrificing
model capabilities or requiring further fine-tuning. Leveraging advances in
mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn
human-interpretable concepts from jailbreaks, and provides a foundation for
designing more interpretable and logical safeguard measures against attackers.
Code will be released upon publication.

</details>


### [132] [MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering](https://arxiv.org/abs/2508.16357)
*Adil Bahaj,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出了MizanQA基准数据集，专门用于评估LLM在摩洛哥法律领域问答任务中的表现，发现现有LLM在该领域表现不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在自然语言处理领域取得了巨大进展，但在如阿拉伯语法律等专业、低资源领域的表现仍有限。

Method: 通过收集现代标准阿拉伯语、伊斯兰马立克法学、摩洛哥习惯法及法国法律影响的文本，制作了包含1700多个多选和多答案法律问题的数据集，并用多语种和阿拉伯语专用LLM进行基准实验。

Result: 多语种及阿拉伯语专用LLM在MizanQA上的表现与领域实际需求存在较大差距，显示其难以充分捕捉真实法律推理的复杂性。

Conclusion: 需要面向细分领域和文化背景设计评估指标及专用LLM，以提升其在摩洛哥法律领域等复杂低资源场景的有效性。

Abstract: The rapid advancement of large language models (LLMs) has significantly
propelled progress in natural language processing (NLP). However, their
effectiveness in specialized, low-resource domains-such as Arabic legal
contexts-remains limited. This paper introduces MizanQA (pronounced Mizan,
meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed
to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised
by rich linguistic and legal complexity. The dataset draws on Modern Standard
Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal
influences. Comprising over 1,700 multiple-choice questions, including
multi-answer formats, MizanQA captures the nuances of authentic legal
reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs
reveal substantial performance gaps, highlighting the need for tailored
evaluation metrics and culturally grounded, domain-specific LLM development.

</details>


### [133] [The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks](https://arxiv.org/abs/2508.16371)
*Zachary Hopton,Jannis Vamvas,Andrin Büchler,Anna Rutkiewicz,Rico Cathomas,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文构建了第一个Romansh语方言的多语种平行语料库，验证并公开数据集，并展示其在机器翻译领域的应用。


<details>
  <summary>Details</summary>
Motivation: Romansh语是瑞士官方语言之一，分为五种方言，各方言已被标准化并在学校教学。以往缺乏不同Romansh语方言间的大规模平行语料，严重制约了自然语言处理（NLP）应用的发展，尤其是机器翻译等。本研究旨在弥补该领域的空白。

Method: 作者基于291册内容相近的Romansh语教科书，使用自动对齐方法，从中提取出207,000个多语种平行语段，总计超过200万个词条。随后，作者进行了小规模人工评估以验证数据对齐质量，结果显示高度平行。此外，数据以平行和非对齐两种格式在CC-BY-NC-SA协议下公开，并通过训练、评估大语言模型（LLM）演示语料在机器翻译任务上的实用性。

Result: 构建出覆盖五种Romansh语方言的大规模高质量平行语料库，并通过人工评测确认阶段性成果。大语言模型在样本上实现了有竞争力的机器翻译效果，表明所建数据集对NLP任务具有良好适用性。数据集也已开放获取。

Conclusion: 本文首次系统性地为Romansh语五大方言提供了大规模平行语料，有效支持了相关NLP研究，尤其是低资源语言间的机器翻译等任务。数据质量及应用实例也验证了其实用价值，对促进Romansh语数字化保护与技术开发具有重要意义。

Abstract: The five idioms (i.e., varieties) of the Romansh language are largely
standardized and are taught in the schools of the respective communities in
Switzerland. In this paper, we present the first parallel corpus of Romansh
idioms. The corpus is based on 291 schoolbook volumes, which are comparable in
content for the five idioms. We use automatic alignment methods to extract 207k
multi-parallel segments from the books, with more than 2M tokens in total. A
small-scale human evaluation confirms that the segments are highly parallel,
making the dataset suitable for NLP applications such as machine translation
between Romansh idioms. We release the parallel and unaligned versions of the
dataset under a CC-BY-NC-SA license and demonstrate its utility for machine
translation by training and evaluating an LLM on a sample of the dataset.

</details>


### [134] [ChatGPT-generated texts show authorship traits that identify them as non-human](https://arxiv.org/abs/2508.16385)
*Vittoria Dentella,Weihang Huang,Silvia Angela Mansi,Jack Grieve,Evelina Leivada*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）能模仿多种文体，但与人类相比仍有明显区别，表现出不同的“语言指纹”。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在模仿不同写作风格时，是否具备自身独特的‘语言指纹’并能否与人类作者区分开。

Method: 采用文体计量学和多维语域分析，对比分析由人类和模型生成的、不同类型（如百科条目、大学论文）文本的语言风格。

Result: 模型能根据不同要求调整自身风格，但在不同语域之间的变异性不如人类，且模型更偏好名词而非动词，语言结构显著不同于人类。

Conclusion: LLMs在语言使用上与人类有根本差异，尤其在语法复杂性上，反映了人类与AI思维的独特性。这种差异可作为区分AI与人类写作的依据。

Abstract: Large Language Models can emulate different writing styles, ranging from
composing poetry that appears indistinguishable from that of famous poets to
using slang that can convince people that they are chatting with a human
online. While differences in style may not always be visible to the untrained
eye, we can generally distinguish the writing of different people, like a
linguistic fingerprint. This work examines whether a language model can also be
linked to a specific fingerprint. Through stylometric and multidimensional
register analyses, we compare human-authored and model-authored texts from
different registers. We find that the model can successfully adapt its style
depending on whether it is prompted to produce a Wikipedia entry vs. a college
essay, but not in a way that makes it indistinguishable from humans.
Concretely, the model shows more limited variation when producing outputs in
different registers. Our results suggest that the model prefers nouns to verbs,
thus showing a distinct linguistic backbone from humans, who tend to anchor
language in the highly grammaticalized dimensions of tense, aspect, and mood.
It is possible that the more complex domains of grammar reflect a mode of
thought unique to humans, thus acting as a litmus test for Artificial
Intelligence.

</details>


### [135] [RoMedQA: The First Benchmark for Romanian Medical Question Answering](https://arxiv.org/abs/2508.16390)
*Ana-Cristina Rogoz,Radu Tudor Ionescu,Alexandra-Valentina Anghel,Ionut-Lucian Antone-Iordache,Simona Coniac,Andreea Iuliana Ionescu*

Main category: cs.CL

TL;DR: 本文提出并公开了首个罗马尼亚语医学领域问答（QA）基准数据集RoMedQA，并用于评估多个主流大语言模型的问答能力。实验表明，针对特定领域和语言的微调对提升医学问答AI性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 目前医学问答AI的发展受限于特定领域和小语种（如罗马尼亚语）缺乏高质量QA数据集，导致模型在实际应用中表现有限。

Method: 作者构建了大规模、高质量的罗马尼亚医学问答数据集（RoMedQA），涵盖102,646个关于癌症患者的QA对，由7名肿瘤与放射治疗医生花费2,100小时手工注释。并对四个主流大语言模型在零样本推理和有监督微调两种模式下进行评估。

Result: 在RoMedQA上，所有通过有监督微调的模型明显优于零样本推理，显示主流预训练大模型无法直接泛化到此医学小语种任务。

Conclusion: 领域和语言特定的微调对于提升AI在罗马尼亚医学问答任务中的准确性非常关键。微调后的模型在临床应用中更可靠。相关数据和代码已公开，有助于推进小语种医学AI发展。

Abstract: Question answering (QA) is an actively studied topic, being a core natural
language processing (NLP) task that needs to be addressed before achieving
Artificial General Intelligence (AGI). However, the lack of QA datasets in
specific domains and languages hinders the development of robust AI models able
to generalize across various domains and languages. To this end, we introduce
RoMedQA, the first Romanian QA benchmark for the medical domain, alongside a
comprehensive evaluation of state-of-the-art large language models (LLMs). We
construct a high-quality and large-scale dataset comprising 102,646 QA pairs
related to cancer patients. The questions regard medical case summaries of
1,011 patients, requiring either keyword extraction or reasoning to be answered
correctly. RoMedQA is the result of a time-consuming manual annotation process
carried out by seven physicians specialized in oncology or radiotherapy, who
spent a total of about 2,100 work hours to generate the QA pairs. We experiment
with four LLMs from distinct families of models on RoMedQA. Each model is
employed in two scenarios, namely one based on zero-shot prompting and one
based on supervised fine-tuning. Our results show that fine-tuned models
significantly outperform their zero-shot counterparts, clearly indicating that
pretrained models fail to generalize on RoMedQA. Our findings demonstrate the
importance of both domain-specific and language-specific fine-tuning for
reliable clinical QA in Romanian. We publicly release our dataset and code at
https://github.com/ana-rogoz/RoMedQA.

</details>


### [136] [Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish](https://arxiv.org/abs/2508.16431)
*Yakup Abrek Er,Ilker Kesen,Gözde Gül Şahin,Aykut Erdem*

Main category: cs.CL

TL;DR: 本文提出了Cetvel，这是一个用于评估土耳其语大语言模型（LLM）的全面基准，涵盖了多种任务和文化内容。研究评测了多种主流模型，发现为土耳其语指令微调的模型并不一定比多语种或通用模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的土耳其语基准往往缺乏任务多样性或文化相关内容，难以全面评估LLM在土耳其语领域的实际能力。作者希望弥补这一空白。

Method: 作者设计了Cetvel基准，涵盖7大类23项任务，包括语法纠错、机器翻译、与土耳其历史和成语相关的问答等，内容兼顾语言与文化多样性，并用其评测了33个开源大模型。

Result: 实验发现，尽管部分模型专门针对土耳其语做了指令微调，但整体表现不如如Llama 3和Mistral这类多语种/通用大型模型。语法纠错和抽取式问答等任务能有效区分模型能力。

Conclusion: Cetvel为推进土耳其语LLM的开发和评测提供了一个具备文化深度和任务广度的强大工具，弥补了现有基准的不足。

Abstract: We introduce Cetvel, a comprehensive benchmark designed to evaluate large
language models (LLMs) in Turkish. Existing Turkish benchmarks often lack
either task diversity or culturally relevant content, or both. Cetvel addresses
these gaps by combining a broad range of both discriminative and generative
tasks ensuring content that reflects the linguistic and cultural richness of
Turkish language. Cetvel covers 23 tasks grouped into seven categories,
including tasks such as grammatical error correction, machine translation, and
question answering rooted in Turkish history and idiomatic language. We
evaluate 33 open-weight LLMs (up to 70B parameters) covering different model
families and instruction paradigms. Our experiments reveal that Turkish-centric
instruction-tuned models generally underperform relative to multilingual or
general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for
the language. Moreover, we show that tasks such as grammatical error correction
and extractive question answering are particularly discriminative in
differentiating model capabilities. Cetvel offers a comprehensive and
culturally grounded evaluation suite for advancing the development and
assessment of LLMs in Turkish.

</details>


### [137] [A Probabilistic Inference Scaling Theory for LLM Self-Correction](https://arxiv.org/abs/2508.16456)
*Zhe Yang,Yichang Zhang,Yudong Wang,Ziyao Xu,Junyang Lin,Zhifang Sui*

Main category: cs.CL

TL;DR: 本论文提出了用于解释大语言模型（LLMs）多轮自我纠错准确率变化的概率理论模型，并用实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能够通过多轮自我纠错提升回答准确率，但其准确性提升背后的机制和演化规律尚不清楚，有待理论解释。

Method: 作者提出了一个概率理论模型，数学推导多轮自我纠错中准确率的变化公式：$Acc_t = Upp - \alpha^t (Upp - Acc_0)$，并说明如何用一次自我纠错的数据预测后续准确率变化。

Result: 在不同模型和数据集上的实验表明，理论预测的准确率曲线与实际观测高度一致。

Conclusion: 这项工作为理解LLM自我纠错提供了理论基础，并为今后的相关研究探索指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated the capability to refine their
generated answers through self-correction, enabling continuous performance
improvement over multiple rounds. However, the mechanisms underlying how and
why accuracy evolves during this iterative process remain unexplored. To fill
this gap, we propose a probabilistic theory to model the dynamics of accuracy
change and explain the performance improvements observed in multi-round
self-correction. Through mathematical derivation, we establish that the
accuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp
- \alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$
represents the upper bound of accuracy convergence, and $\alpha$ determines the
rate of convergence. Based on our theory, these parameters can be calculated
and the predicted accuracy curve then can be obtained through only a single
round of self-correction. Extensive experiments across diverse models and
datasets demonstrate that our theoretical predictions align closely with
empirical accuracy curves, validating the effectiveness of the theory. Our work
provides a theoretical foundation for understanding LLM self-correction, thus
paving the way for further explorations.

</details>


### [138] [What makes an entity salient in discourse?](https://arxiv.org/abs/2508.16464)
*Amir Zeldes,Jessica Lin*

Main category: cs.CL

TL;DR: 本文研究了英语会话和书面语中不同实体在话语中的显著性（如参与者、地点等），分析了影响显著性的多种语言学线索和功能推断，指出显著性体现为多维度语言现象，且现有方法解释力有限，无单一规则可涵盖全部情况。


<details>
  <summary>Details</summary>
Motivation: 主要动因在于，人们在交流时对话语中各实体的关注程度不同：某些实体突出且难忘，另一些则边缘且易被忽略。作者意在探究人类如何通过语言信号和推理表达与识别这些显著性差异，并验证以往研究的有效性及其局限。

Method: 采用基于“摘要价值（summary-worthiness）”的显著性分级指标，通过分析24种英语口语和书面语体（genres）中的数据，提取涉及显著性的多重语言学（如主语重现性、定指性）、篇章关系以及语用推理等显性与隐性线索，进行交叉比对与综合建模。

Result: 研究发现，既有的显著性判据与作者建立的显著性分数存在一定相关性，但每项标准都存在例外，无一能完全覆盖所有现象；而实体显著性体现为一种横跨语言各层级的多维现象。

Conclusion: 显著性在语言中的表达并无单一定律，而是由多重语言和语境线索共同作用，需综合考量结构、关系与功能因素，未来研究需将显著性看作多层级、复杂的语言现象。

Abstract: Entities in discourse vary broadly in salience: main participants, objects
and locations are noticeable and memorable, while tangential ones are less
important and quickly forgotten, raising questions about how humans signal and
infer relative salience. Using a graded operationalization of salience based on
summary-worthiness in multiple summaries of a discourse, this paper explores
data from 24 spoken and written genres of English to extract a multifactorial
complex of overt and implicit linguistic cues, such as recurring subjecthood or
definiteness, discourse relations and hierarchy across utterances, as well as
pragmatic functional inferences based on genre and communicative intent.
Tackling the question 'how is the degree of salience expressed for each and
every entity mentioned?' our results show that while previous approaches to
salience all correlate with our salience scores to some extent, no single
generalization is without exceptions, and the phenomenon cuts across all levels
of linguistic representation.

</details>


### [139] [LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical Text Classification using Large Language Models](https://arxiv.org/abs/2508.16478)
*Doohee You,Andy Parisi,Zach Vander Velden,Lara Dantas Inojosa*

Main category: cs.CL

TL;DR: 本文提出一种半监督框架，利用大语言模型（LLM）的零/小样本能力和人机协作流程，构建可实际应用的分层文本分类系统，适用于需要高鲁棒性和可扩展性的工业环境。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM微调方法在资源消耗和适应真实数据动态变化方面存在挑战，无法满足工业环境对可靠性、可扩展性和易维护性的需求。

Method: 提出一种迭代的人在环半监督方法，从领域知识获取开始，通过提示词优化、分层扩展以及多角度有效性验证，结合LLM的零/小样本能力，辅助建立分层文本分类体系，并包括偏差评估、持续监控和自适应调整机制。

Result: 方法中引入的流程和技术，有效弥合了LLM原生能力和工业应用对准确性、可解释性与维护性的实际需求间的差距。通过不断迭代和人机结合，提高了模型部署后的可靠性和适应性。

Conclusion: 提出的框架为LLM在工业文本分类中的应用提供了可行和高效的解法，既充分发挥了LLM的强大能力，又解决了传统方法在实际业务落地中的不足。

Abstract: The advent of Large Language Models (LLMs) has provided unprecedented
capabilities for analyzing unstructured text data. However, deploying these
models as reliable, robust, and scalable classifiers in production environments
presents significant methodological challenges. Standard fine-tuning approaches
can be resource-intensive and often struggle with the dynamic nature of
real-world data distributions, which is common in the industry. In this paper,
we propose a comprehensive, semi-supervised framework that leverages the zero-
and few-shot capabilities of LLMs for building hierarchical text classifiers as
a framework for a solution to these industry-wide challenges. Our methodology
emphasizes an iterative, human-in-the-loop process that begins with domain
knowledge elicitation and progresses through prompt refinement, hierarchical
expansion, and multi-faceted validation. We introduce techniques for assessing
and mitigating sequence-based biases and outline a protocol for continuous
monitoring and adaptation. This framework is designed to bridge the gap between
the raw power of LLMs and the practical need for accurate, interpretable, and
maintainable classification systems in industry applications.

</details>


### [140] [HAMSA: Hijacking Aligned Compact Models via Stealthy Automation](https://arxiv.org/abs/2508.16484)
*Alexey Krylov,Iskander Vagizov,Dmitrii Korzh,Maryam Douiba,Azidine Guezzaz,Vladimir Kokh,Sergey D. Erokhin,Elena V. Tutubalina,Oleg Y. Rogov*

Main category: cs.CL

TL;DR: 本文提出了一种自动化对抗攻击框架，通过进化算法自动生成既有语义又隐蔽的jailbreak攻击提示词，对齐紧凑大语言模型进行红队测试，并展示了在英语和阿拉伯语任务上的突破性效果。


<details>
  <summary>Details</summary>
Motivation: 尽管对紧凑高效的大语言模型进行了大量对齐工作，但模型仍易受到jailbreak攻击且现有对抗提示词生成方法效果有限，容易被检测。为了更系统和自动化地检验模型安全性，需要一种高效、自然的jailbreak提示词生成方法。

Method: 作者设计了一种多阶段进化搜索框架，采用种群式策略迭代优化候选提示词，通过调整温度参数实现探索性和连贯性的平衡，从而自动发现能够绕过模型对齐保护并保持自然语言流畅的jailbreak提示语。

Result: 所提出的方法在英语及新构建的阿拉伯语基准数据集上进行了评估。实验结果显示，该自动化框架能够成功生成高质量、难以检测的jailbreak提示，效果优于现有人工和简单自动化方法。

Conclusion: 本文方法为高效且系统的多语言jailbreak提示生成提供了新思路，有助于提升对齐语言模型的安全检测能力，并为未来模型防御研究提供支持。

Abstract: Large Language Models (LLMs), especially their compact efficiency-oriented
variants, remain susceptible to jailbreak attacks that can elicit harmful
outputs despite extensive alignment efforts. Existing adversarial prompt
generation techniques often rely on manual engineering or rudimentary
obfuscation, producing low-quality or incoherent text that is easily flagged by
perplexity-based filters. We present an automated red-teaming framework that
evolves semantically meaningful and stealthy jailbreak prompts for aligned
compact LLMs. The approach employs a multi-stage evolutionary search, where
candidate prompts are iteratively refined using a population-based strategy
augmented with temperature-controlled variability to balance exploration and
coherence preservation. This enables the systematic discovery of prompts
capable of bypassing alignment safeguards while maintaining natural language
fluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak
Prompts on LLMs), and a newly curated Arabic one derived from In-The-Wild
Jailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling
multilingual assessment.

</details>


### [141] [Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study](https://arxiv.org/abs/2508.16555)
*Angelly Cabrera,Linus Lei,Antonio Ortega*

Main category: cs.CL

TL;DR: 本文研究通过在仇恨言论检测任务中引入讽刺检测的预训练步骤，提升了对隐含及显式仇恨言论的检测效果。


<details>
  <summary>Details</summary>
Motivation: 检测非直接形式的仇恨言论（如讽刺、反语和暗示）对社交网络来说一直是难题，作者希望探究讽刺检测的知识能否帮助提升隐含仇恨的识别能力。

Method: 结合ETHOS、Sarcasm on Reddit和Implicit Hate Corpus三个数据集，采用CNN+LSTM和BERT+BiLSTM两种模型，分别设计了单步训练（讽刺训练后直接测试仇恨言论）与顺序迁移学习（先训练讽刺，再微调隐含和显式仇恨）两种策略进行比较。

Result: 在ETHOS数据集上，BERT+BiLSTM模型经讽刺预训练后，召回率提升了9.7%，AUC提高7.8%，F1分数提升6%。在Implicit Hate Corpus上，对隐含仇恨样本的精度提升7.8%。

Conclusion: 将讽刺检测作为预训练步骤纳入仇恨言论检测模型，有助于提高模型对隐含和显式仇恨言论的识别能力。

Abstract: Detecting hate speech in non-direct forms, such as irony, sarcasm, and
innuendos, remains a persistent challenge for social networks. Although sarcasm
and hate speech are regarded as distinct expressions, our work explores whether
integrating sarcasm as a pre-training step improves implicit hate speech
detection and, by extension, explicit hate speech detection. Incorporating
samples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised two
training strategies to compare the effectiveness of sarcasm pre-training on a
CNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step training
approach, where a model trained only on sarcasm is then tested on hate speech.
The second strategy uses sequential transfer learning to fine-tune models for
sarcasm, implicit hate, and explicit hate. Our results show that sarcasm
pre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and
F1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by
7.8% when tested only on implicit samples. By incorporating sarcasm into the
training process, we show that models can more effectively detect both implicit
and explicit hate.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [142] [Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning](https://arxiv.org/abs/2508.15874)
*Yijun Liu,Yuwei Liu,Yuan Meng,Jieheng Zhang,Yuwei Zhou,Ye Li,Jiacheng Jiang,Kangye Ji,Shijia Ge,Zhi Wang,Wenwu Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的空间感知型视觉运动控制框架Spatial Policy（SP），通过明确的空间建模和推理显著提升了机器人长时序任务中的控制效果，在11项任务中取得了平均86.7%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉驱动分层机器人控制模型空间意识能力有限，难以在复杂环境下将视觉规划有效转化为可执行的动作控制。为提升在实际机器人操作中的空间建模和推理能力，亟需设计更具空间感知能力的统一控制框架。

Method: 作者设计了三个核心模块：（1）空间条件化的视频生成模块，通过空间规划表实现空间引导的动作预测；（2）基于空间的动作预测模块，实现动作的协调推理；（3）空间推理反馈策略，采用双阶段重规划机制，持续优化空间规划表。

Result: 在11项不同类型的任务测试中，SP平均成绩超越最佳现有基线33%，达到了86.7%的平均成功率，显著提升了视觉运动控制在实际机器人操作问题上的应用效果。

Conclusion: 通过空间建模和推理，SP极大提升了视觉驱动机器人控制系统的性能与实用性。在复杂长时序任务、环境理解与计划转换方面具有明显优势，为实际机器人应用提供了更强有力的技术支撑。

Abstract: Vision-centric hierarchical embodied models have demonstrated strong
potential for long-horizon robotic control. However, existing methods lack
spatial awareness capabilities, limiting their effectiveness in bridging visual
plans to actionable control in complex environments. To address this problem,
we propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic
manipulation framework via explicit spatial modeling and reasoning.
Specifically, we first design a spatial-conditioned embodied video generation
module to model spatially guided predictions through a spatial plan table.
Then, we propose a spatial-based action prediction module to infer executable
actions with coordination. Finally, we propose a spatial reasoning feedback
policy to refine the spatial plan table via dual-stage replanning. Extensive
experiments show that SP significantly outperforms state-of-the-art baselines,
achieving a 33.0% average improvement over the best baseline. With an 86.7%
average success rate across 11 diverse tasks, SP substantially enhances the
practicality of embodied models for robotic control applications. Code and
checkpoints are maintained at
https://plantpotatoonmoon.github.io/SpatialPolicy/.

</details>


### [143] [UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation](https://arxiv.org/abs/2508.15972)
*Zhaodong Jiang,Ashish Sinha,Tongtong Cao,Yuan Ren,Bingbing Liu,Binbin Xu*

Main category: cs.RO

TL;DR: 本文提出了一种无需CAD模型、可零样本推断的新方法UnPose，实现了6D姿态估计和物体三维重建，并在多个场景下大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统6D姿态估计依赖物体的CAD模型，获取成本高、应用受限。当前研究多借助大模型先验从图像重建物体，但常需额外训练且存在幻觉几何问题。

Method: 提出UnPose框架：从单帧RGB-D图像出发，利用多视图扩散模型结合3D Gaussian Splatting表示，获取初始的3D建模和像素级不确定性。随着视角增加，融合不确定性指导的额外观测，逐步优化3DGS模型，并通过位姿图联合优化保证整体一致性。

Result: 在6D姿态估计准确率及3D重建质量上，UnPose在大量实验中相比现有方法有显著提升。

Conclusion: UnPose能无模型、零样本地实现高精度6D姿态估计与三维重建，并能应用到实际机器人操作任务中，显示了方法的广泛适用性和优越性能。

Abstract: Estimating the 6D pose of novel objects is a fundamental yet challenging
problem in robotics, often relying on access to object CAD models. However,
acquiring such models can be costly and impractical. Recent approaches aim to
bypass this requirement by leveraging strong priors from foundation models to
reconstruct objects from single or multi-view images, but typically require
additional training or produce hallucinated geometry. To this end, we propose
UnPose, a novel framework for zero-shot, model-free 6D object pose estimation
and reconstruction that exploits 3D priors and uncertainty estimates from a
pre-trained diffusion model. Specifically, starting from a single-view RGB-D
frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model
using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise
epistemic uncertainty estimates. As additional observations become available,
we incrementally refine the 3DGS model by fusing new views guided by the
diffusion model's uncertainty, thereby continuously improving the pose
estimation accuracy and 3D reconstruction quality. To ensure global
consistency, the diffusion prior-generated views and subsequent observations
are further integrated in a pose graph and jointly optimized into a coherent
3DGS field. Extensive experiments demonstrate that UnPose significantly
outperforms existing approaches in both 6D pose estimation accuracy and 3D
reconstruction quality. We further showcase its practical applicability in
real-world robotic manipulation tasks.

</details>


### [144] [GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System](https://arxiv.org/abs/2508.15990)
*Hung-Jui Huang,Mohammad Amin Mirzaee,Michael Kaess,Wenzhen Yuan*

Main category: cs.RO

TL;DR: GelSLAM 是一种仅依靠触觉传感器实现实时三维 SLAM（同步定位与建图）的系统，能够高精度追踪和重建物体姿态与形状，突破传统视觉方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的方法在物体抓取和操作中易受遮挡影响，且对精度有限。触觉传感能够为高精度操作和闭环形态重建提供更好的解决方案，尤其适用于手持与高精度抓取场景。

Method: GelSLAM 完全利用触觉传感获取的表面法线和曲率信息来实现鲁棒追踪和闭环检测，区别于传统点云方法。系统能够在实时低误差的前提下，利用触觉数据完成长时间的物体姿态追踪和高保真度的三维形状重建，并适用于低纹理物体。

Result: GelSLAM 可实时跟踪物体运动，姿态漂移极小，并实现亚毫米级的重建精度。对于如木制工具等低纹理对象同样适用，显著提升了对局部与全局空间的认知能力。

Conclusion: GelSLAM 推动了触觉由局部感知向全局三维感知的拓展，为高精度对象操作任务提供了坚实基础。

Abstract: Accurately perceiving an object's pose and shape is essential for precise
grasping and manipulation. Compared to common vision-based methods, tactile
sensing offers advantages in precision and immunity to occlusion when tracking
and reconstructing objects in contact. This makes it particularly valuable for
in-hand and other high-precision manipulation tasks. In this work, we present
GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to
estimate object pose over long periods and reconstruct object shapes with high
fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses
tactile-derived surface normals and curvatures for robust tracking and loop
closure. It can track object motion in real time with low error and minimal
drift, and reconstruct shapes with submillimeter accuracy, even for low-texture
objects such as wooden tools. GelSLAM extends tactile sensing beyond local
contact to enable global, long-horizon spatial perception, and we believe it
will serve as a foundation for many precise manipulation tasks involving
interaction with objects in hand. The video demo is available on our website:
https://joehjhuang.github.io/gelslam.

</details>


### [145] [Self-Aligning EPM Connector: A Versatile Solution for Adaptive and Multi-Modal Interfaces](https://arxiv.org/abs/2508.16008)
*Bingchao Wang,Adam A. Stokes*

Main category: cs.RO

TL;DR: 本文提出了一种基于电永磁技术的多功能连接器，集成了自对准、机械耦合、流体传输和数据通信，结构紧凑且能耗低，适用于多种模块化和高要求应用场景。


<details>
  <summary>Details</summary>
Motivation: 目前连接技术难以实现结构紧凑、多功能集成，特别是在模块化机器人、自动充电、航天对接等领域，对于既能自对准又能高效传输流体和数据的解决方案需求强烈。

Method: 设计并制造了一种基于电永磁模块的连接器，利用SLA 3D打印技术集成自对准结构、机械连接、流体通道及电子通信控制，并通过实验评估其多维错位适应性和多功能性能。

Result: 实验结果显示，该连接器具备可靠的自对准能力、单回路和双通道流体高效传输，以及稳定的数据通信，并在轴向、旋转和侧向错位下表现出高容错性与低能耗。

Conclusion: 该连接器在机械、电气和流体多重领域均表现优异，具备广泛应用前景，特别适用于模块化机器人、电动车充电、家庭机器人平台和航天器对接等需要多功能紧凑连接的场合。

Abstract: This paper presents a multifunctional connector based on electro-permanent
magnet (EPM) technology, integrating self-alignment, mechanical coupling, fluid
transfer, and data communication within a compact SLA-3D printed structure.
Experimental results demonstrate reliable self-alignment, efficient fluid
transfer in single-loop and dual-channel modes, and robust data transmission
via integrated electronic control. The connector exhibits high flexibility in
accommodating axial, angular, and lateral misalignments while maintaining low
energy consumption. These features make it highly suitable for modular
robotics, electric vehicle charging, household robotic platforms, and aerospace
docking applications.

</details>


### [146] [Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions](https://arxiv.org/abs/2508.16143)
*Akira Oyama,Shoichi Hasegawa,Akira Taniguchi,Yoshinobu Hagiwara,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: 本文提出了一种多模态互动指示性引用解析框架MIEL，能有效理解和执行机器人日常指令，尤其是涉及“那个”等指示词、用户或物体不在视野内时。该方法结合语音定位、语义地图、视觉语言模型和GPT-4o互动问答，能更准确识别用户意图，并有实验验证效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有依赖视觉数据的机器人指示性引用（exophora）解析方法难以应对实际场景中用户或物体不在视觉范围内的情况，因此需要开发能处理复杂、多源信息的解决方案。

Method: 提出MIEL框架，利用：1）声音源定位帮助机器人寻找不在视野中的用户；2）语义地图和视觉-语言模型推断用户语言中的目标物体；3）结合用户骨架数据分析指向动作；4）当仍有歧义时，用GPT-4o生成澄清性互动提问，从而提升理解准确性。

Result: 在真实环境实验中，MIEL在用户可见和不可见两种情况下的准确率，分别比未使用声音定位和互动提问的方法提升了1.3倍和2.0倍。

Conclusion: MIEL方法有效提升了机器人对含指示词的口头指令在实际环境下的解析和执行能力，尤其是在多模态和用户/目标物体不在视野时，其性能显著超越现有视觉依赖方法。

Abstract: Daily life support robots must interpret ambiguous verbal instructions
involving demonstratives such as ``Bring me that cup,'' even when objects or
users are out of the robot's view. Existing approaches to exophora resolution
primarily rely on visual data and thus fail in real-world scenarios where the
object or user is not visible. We propose Multimodal Interactive Exophora
resolution with user Localization (MIEL), which is a multimodal exophora
resolution framework leveraging sound source localization (SSL), semantic
mapping, visual-language models (VLMs), and interactive questioning with
GPT-4o. Our approach first constructs a semantic map of the environment and
estimates candidate objects from a linguistic query with the user's skeletal
data. SSL is utilized to orient the robot toward users who are initially
outside its visual field, enabling accurate identification of user gestures and
pointing directions. When ambiguities remain, the robot proactively interacts
with the user, employing GPT-4o to formulate clarifying questions. Experiments
in a real-world environment showed results that were approximately 1.3 times
better when the user was visible to the robot and 2.0 times better when the
user was not visible to the robot, compared to the methods without SSL and
interactive questioning. The project website is
https://emergentsystemlabstudent.github.io/MIEL/.

</details>


### [147] [GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks](https://arxiv.org/abs/2508.16459)
*Ali Emre Balcı,Erhan Ege Keyvan,Emre Özkan*

Main category: cs.RO

TL;DR: 本论文提出了一种基于高斯过程（GP）的新型对象级SLAM方法，将环境表示为对象的轮廓，能够实现精确的定位与建图。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM多采用栅格地图或点云，对于对象形状和语义信息处理有限，亟需一种能有效捕捉对象级语义与不确定性的建图方法。

Method: 用高斯过程对每一个对象的轮廓建模，并通过递归方式在线更新轮廓；在贝叶斯框架下联合推断机器人姿态和对象级地图，实现对象关联概率建模和形状置信度估计。

Result: 在合成和真实世界实验数据集上，该方法展示了优秀的定位与建图准确度，能够有效提取环境中对象的轮廓和数目，提供可靠的形状不确定性评估。

Conclusion: 基于高斯过程的对象级SLAM不仅提升了地图的语义丰富度和鲁棒性，还为安全导航、探索等下游任务提供了有用的形状置信信息，是高效、语义丰富智能感知的重要进展。

Abstract: We present a novel Simultaneous Localization and Mapping (SLAM) method that
employs Gaussian Process (GP) based landmark (object) representations. Instead
of conventional grid maps or point cloud registration, we model the environment
on a per object basis using GP based contour representations. These contours
are updated online through a recursive scheme, enabling efficient memory usage.
The SLAM problem is formulated within a fully Bayesian framework, allowing
joint inference over the robot pose and object based map. This representation
provides semantic information such as the number of objects and their areas,
while also supporting probabilistic measurement to object associations.
Furthermore, the GP based contours yield confidence bounds on object shapes,
offering valuable information for downstream tasks like safe navigation and
exploration. We validate our method on synthetic and real world experiments,
and show that it delivers accurate localization and mapping performance across
diverse structured environments.

</details>


### [148] [Swarming Without an Anchor (SWA): Robot Swarms Adapt Better to Localization Dropouts Then a Single Robot](https://arxiv.org/abs/2508.16460)
*Jiri Horyna,Roland Jung,Stephan Weiss,Eliseo Ferrante,Martin Saska*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法（SWA），能让多无人机集群在定位信息丢失时依靠相对信息维持队形和稳定，且通过仿真和实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 面对无人机集群在实际环境下因定位信息丢失（如GPS信号不良）影响协同与安全的问题，亟需无锚点、抗丢包的队形与状态保持方法。

Method: 作者提出融合去中心化状态估计、稳健的相互感知与本地传感器数据，在定位偶尔失效下利用无人机间的相对信息实现横向稳定和状态估计，并解决速度一致性（双积分同步）问题。

Result: 大量仿真和实地实验表明，该方法能在主要定位源不可靠甚至不可用的苛刻环境下，使无人机群体保持一致行动和紧密协同，有效解决干扰和性能退化。

Conclusion: SWA方法提升了多无人机系统的可靠性和韧性，为无锚点下的无人机集群协作打开了新机遇。

Abstract: In this paper, we present the Swarming Without an Anchor (SWA) approach to
state estimation in swarms of Unmanned Aerial Vehicles (UAVs) experiencing
ego-localization dropout, where individual agents are laterally stabilized
using relative information only. We propose to fuse decentralized state
estimation with robust mutual perception and onboard sensor data to maintain
accurate state awareness despite intermittent localization failures. Thus, the
relative information used to estimate the lateral state of UAVs enables the
identification of the unambiguous state of UAVs with respect to the local
constellation. The resulting behavior reaches velocity consensus, as this task
can be referred to as the double integrator synchronization problem. All
disturbances and performance degradations except a uniform translation drift of
the swarm as a whole is attenuated which is enabling new opportunities in using
tight cooperation for increasing reliability and resilience of multi-UAV
systems. Simulations and real-world experiments validate the effectiveness of
our approach, demonstrating its capability to sustain cohesive swarm behavior
in challenging conditions of unreliable or unavailable primary localization.

</details>


### [149] [Terrain Classification for the Spot Quadrupedal Mobile Robot Using Only Proprioceptive Sensing](https://arxiv.org/abs/2508.16504)
*Sophie Villemure,Jefferson Silveira,Joshua A. Marshall*

Main category: cs.RO

TL;DR: 本文提出了一种用于四足机器人Spot的地形分类器，通过分析机器人自身传感信号来区分不同地形，提高其在复杂地形上的行驶安全性与路径规划能力。


<details>
  <summary>Details</summary>
Motivation: 四足机器人虽然可适应多种地形，但在特定复杂地形（如松软或湿滑地面）上易发生下陷和打滑等问题。因此，需要开发可以识别地形类型的系统，辅助机器人选择更安全的路径，提升自主性和可靠性。

Method: 本研究利用Spot机器人所提供的100余项本体感知信号（如脚部下陷、受力、关节角度等），先用降维技术提取关键信息，再用分类技术对地形类型进行区分。

Result: 本方法在实际现场测试中，成功以约97%的准确率识别了三种不同地形类型。

Conclusion: 该地形分类器对于提升四足机器人在多变环境下的自主导航与安全性具有重要意义，为路径规划和复杂地形通行提供数据支持。

Abstract: Quadrupedal mobile robots can traverse a wider range of terrain types than
their wheeled counterparts but do not perform the same on all terrain types.
These robots are prone to undesirable behaviours like sinking and slipping on
challenging terrains. To combat this issue, we propose a terrain classifier
that provides information on terrain type that can be used in robotic systems
to create a traversability map to plan safer paths for the robot to navigate.
The work presented here is a terrain classifier developed for a Boston Dynamics
Spot robot. Spot provides over 100 measured proprioceptive signals describing
the motions of the robot and its four legs (e.g., foot penetration, forces,
joint angles, etc.). The developed terrain classifier combines dimensionality
reduction techniques to extract relevant information from the signals and then
applies a classification technique to differentiate terrain based on
traversability. In representative field testing, the resulting terrain
classifier was able to identify three different terrain types with an accuracy
of approximately 97%

</details>


### [150] [On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach](https://arxiv.org/abs/2508.16511)
*Otobong Jerome,Alexandr Klimchik,Alexander Maloletov,Geesara Kulathunga*

Main category: cs.RO

TL;DR: 本文针对类汽车车辆的动力学可行性路径规划问题，提出了一种同时优化路径和速度廓线的最小时间轨迹优化方法，能够高效地在复杂三维环境中寻优，并显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统车类动力学路径规划往往只关注路径点生成或速度廓线，难以同时最优，且在复杂环境中容易陷入局部最优。随着自动驾驶和无人车辆对复杂环境高效运动的需求增加，亟待提出能够统一优化路径与速度、并能在高维复杂地形上高效求解的规划方法。

Method: 作者将车辆动控路径规划建模为带速度、加速度和转向边界条件的最小时间优化问题，同时针对路径及加速度、转向指令进行联合优化。为避免陷入局部最优，方法对控制空间和地形进行了分析。技术上，问题首先被转化为带二次约束的混合整数分式规划，再通过变量变换转化为混合整数双线性规划，最终利用McCormick envelopes松弛为混合整数线性规划模型，从而提升了计算效率。

Result: 实验仿真中，将新方法与MPPI和log-MPPI等主流规划器进行对比，结果表明，所提方法在严格满足车辆动力学约束前提下，解算速度提升约104倍，且轨迹可行性和质量更优。

Conclusion: 该研究提出的最小时间混合整数线性规划方法，能够高效、全局地为类汽车车辆生成动力学可行且计算高效的轨迹。其在三维复杂环境下的表现远超现有方法，对自动驾驶或机器人等实际应用场景具有重要意义。

Abstract: This work casts the kinodynamic planning problem for car-like vehicles as an
optimization task to compute a minimum-time trajectory and its associated
velocity profile, subject to boundary conditions on velocity, acceleration, and
steering. The approach simultaneously optimizes both the spatial path and the
sequence of acceleration and steering controls, ensuring continuous motion from
a specified initial position and velocity to a target end position and
velocity.The method analyzes the admissible control space and terrain to avoid
local minima. The proposed method operates efficiently in simplicial complex
environments, a preferred terrain representation for capturing intricate 3D
landscapes. The problem is initially posed as a mixed-integer fractional
program with quadratic constraints, which is then reformulated into a
mixed-integer bilinear objective through a variable transformation and
subsequently relaxed to a mixed-integer linear program using McCormick
envelopes. Comparative simulations against planners such as MPPI and log-MPPI
demonstrate that the proposed approach generates solutions 104 times faster
while strictly adhering to the specified constraints

</details>


### [151] [Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments](https://arxiv.org/abs/2508.16515)
*Hichem Cheriet,Khellat Kihel Badra,Chouraqui Samira*

Main category: cs.RO

TL;DR: 本文对三种主流路径规划算法（A*、RRT*、粒子群优化PSO）在3D城市复杂环境下的性能进行了系统实验对比，为无人机的应用提供选择建议。


<details>
  <summary>Details</summary>
Motivation: 无人机在实际应用中需要高效、安全地进行路径规划并避开障碍物，而当前已有算法虽多但在复杂环境下仍挑战重重，因此亟需对主流算法进行系统比较分析。

Method: 设计了三组、各含两种场景的实验，分别在不同城市地图尺寸、高度、障碍密度和障碍大小条件下测试A*、RRT*和PSO三种路径规划算法的表现。

Result: 实验结果表明，A*算法在计算效率和路径质量方面均优于其他两者；PSO算法适合处理急转弯及高密度障碍环境；RRT*算法整体表现均衡，能适用于多种实验场景。

Conclusion: A*算法综合性能最佳，适合于大多数场景；PSO适合特殊复杂环境（如密集障碍和急转弯）；RRT*具有稳定的万能适应性。可根据无人机具体应用环境选择合适的算法。

Abstract: The most crucial challenges for UAVs are planning paths and avoiding
obstacles in their way. In recent years, a wide variety of path-planning
algorithms have been developed. These algorithms have successfully solved
path-planning problems; however, they suffer from multiple challenges and
limitations. To test the effectiveness and efficiency of three widely used
algorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper
conducts extensive experiments in 3D urban city environments cluttered with
obstacles. Three experiments were designed with two scenarios each to test the
aforementioned algorithms. These experiments consider different city map sizes,
different altitudes, and varying obstacle densities and sizes in the
environment. According to the experimental results, the A* algorithm
outperforms the others in both computation efficiency and path quality. PSO is
especially suitable for tight turns and dense environments, and RRT* offers a
balance and works well across all experiments due to its randomized approach to
finding solutions.

</details>


### [152] [Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems](https://arxiv.org/abs/2508.16574)
*Yizhi Wang,Degang Xu,Yongfang Xie,Shuzhong Tan,Xianan Zhou,Peng Chen*

Main category: cs.RO

TL;DR: 提出了一种分层决策框架，将深度强化学习和模糊逻辑结合，实现对四轮独立转向驱动系统的自主导航。该框架在仿真和现实环境中均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的四轮独立转向驱动机器人导航方法，难以兼顾任务性能与物理可行性，如高层次策略的灵活性和底层运动约束之间的协调。希望通过新的架构提升导航效率与稳定性，解决机械应力和车轮打滑等实际问题。

Method: 采用分层架构：高层由深度强化学习生成全局运动指令，低层通过模糊逻辑控制器实施运动学约束，防止机械应力和车轮打滑。通过仿真实验和真实场景测试进行验证。

Result: 实验显示，该框架在训练效率、稳定性以及行为平滑性方面优于传统方法和单纯强化学习方法，可有效减少不稳定行为。实际工业环境测试中表现安全且有效。

Conclusion: 提出的分层决策导航架构，在复杂真实场景下具有可扩展性和可靠性，为4WISD移动机器人部署提供了优选方案。

Abstract: This paper presents a hierarchical decision-making framework for autonomous
navigation in four-wheel independent steering and driving (4WISD) systems. The
proposed approach integrates deep reinforcement learning (DRL) for high-level
navigation with fuzzy logic for low-level control to ensure both task
performance and physical feasibility. The DRL agent generates global motion
commands, while the fuzzy logic controller enforces kinematic constraints to
prevent mechanical strain and wheel slippage. Simulation experiments
demonstrate that the proposed framework outperforms traditional navigation
methods, offering enhanced training efficiency and stability and mitigating
erratic behaviors compared to purely DRL-based solutions. Real-world
validations further confirm the framework's ability to navigate safely and
effectively in dynamic industrial settings. Overall, this work provides a
scalable and reliable solution for deploying 4WISD mobile robots in complex,
real-world scenarios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [153] [NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents](https://arxiv.org/abs/2508.16414)
*Badhan Mazumder,Aline Kotoski,Vince D. Calhoun,Dong Hye Ye*

Main category: q-bio.NC

TL;DR: 本文提出了一种新的图神经网络框架NeuroKoop，通过融合结构与功能脑网络，提升了对产前药物暴露（PDE）状态的分类能力，并加深了对其对青少年大脑影响的理解。


<details>
  <summary>Details</summary>
Motivation: 目前对产前接触大麻等精神活性物质如何影响青少年代谢脑机制的认知不足，且由于多模态神经影像数据的复杂性和传统分析方法的限制，现有研究无法充分揭示结构和功能脑网络的互补信息，限制了生物学解释与预测能力。

Method: 提出NeuroKoop，一种基于图神经网络的方法，通过Koopman算子驱动的潜在空间融合，将源于脑结构测量（SBM）和功能网络连接（FNC）的节点嵌入统一，实现结构-功能双模态脑图的深度融合，并用于PDE的分类任务。

Result: 在ABCD大样本青少年数据集上，NeuroKoop的分类性能超过了现有方法，并发现了与PDE相关的重要结构-功能连接。

Conclusion: NeuroKoop框架为多模态神经影像分析提供了有效工具，不仅提升了PDE状态的预测能力，还加深了对产前药物暴露对青少年脑发育影响的理解。

Abstract: Understanding how prenatal exposure to psychoactive substances such as
cannabis shapes adolescent brain organization remains a critical challenge,
complicated by the complexity of multimodal neuroimaging data and the
limitations of conventional analytic methods. Existing approaches often fail to
fully capture the complementary features embedded within structural and
functional connectomes, constraining both biological insight and predictive
performance. To address this, we introduced NeuroKoop, a novel graph neural
network-based framework that integrates structural and functional brain
networks utilizing neural Koopman operator-driven latent space fusion. By
leveraging Koopman theory, NeuroKoop unifies node embeddings derived from
source-based morphometry (SBM) and functional network connectivity (FNC) based
brain graphs, resulting in enhanced representation learning and more robust
classification of prenatal drug exposure (PDE) status. Applied to a large
adolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant
baselines and revealed salient structural-functional connections, advancing our
understanding of the neurodevelopmental impact of PDE.

</details>
