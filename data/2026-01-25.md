<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本论文提出了三种方法，在数据有限的情况下显著提升涵洞和下水道管道的缺陷分割性能，包括数据增强、全新网络架构FORTRESS以及小样本学习方案。


<details>
  <summary>Details</summary>
Motivation: 涵洞和下水道是排水系统的重要部分，其失效会带来极大安全和环境风险。然而，该领域的大规模缺陷检测数据集建设成本高、难度大，因此亟需在数据稀缺条件下提升自动化缺陷分割方法的性能。

Method: 1. 采用数据增强和动态标签注入等预处理策略提升模型泛化能力；2. 提出FORTRESS新架构，结合深度可分离卷积、自适应KAN和多尺度注意力机制，提高性能并降低计算消耗；3. 研究基于注意力机制的双向原型网络，实现小样本条件下的语义分割。

Result: 预处理策略（数据增强、标签注入）显著提升IoU和F1分数。FORTRESS架构在缺陷数据集上取得SOTA表现，同时降低参数量和计算成本。少样本学习模型在各项评估指标上表现令人满意。

Conclusion: 提出的三种方法有效解决了数据稀缺背景下的缺陷分割难题，提升了模型精度及实际应用性，为结构缺陷自动检测提供了新思路。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本论文系统评估了多模态大语言模型（MLLMs）在异构人脸识别（HFR）中的表现，并指出其与传统人脸识别方法在复杂跨光谱环境下存在显著差距，当前MLLMs难以理想应用于高要求生物识别场景。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言任务中已展现出强大能力，引发了其在生物识别领域，尤其是异构人脸识别（HFR）中的应用兴趣。然而，MLLM在不同感知模态下的人脸识别效果尚不明朗，需要系统评估其真实性能。

Method: 作者对多种开源MLLM进行了基准测试，评估对象涵盖不同跨模态场景：VIS-NIR、VIS-SWIR、VIS-THERMAL。采用标准生物识别协议，以获取率、等错误率（EER）、真实接受率（TAR）等多指标，衡量MLLM在人脸识别任务中的表现。

Result: 实验结果表明，尽管MLLMs在其它领域取得进展，但在异构、跨光谱人脸识别上，其性能与经典识别方法仍存在明显差距。MLLMs在复杂情况下识别效果不足。

Conclusion: 当前MLLMs不适合直接应用于高要求的异构人脸识别任务。未来应加强对MLLM在人脸识别准确性与安全性的专项评估，提升其在实际生物识别系统中的部署可行性。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE是一种旨在提升医学视觉-语言模型生成放射学报告真实性和视觉关联性的框架，通过无新增数据的课程学习策略优化模型表现，有效提升了定位精度和报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在自动生成放射学报告时，存在文本描述与医学影像对不准、虚假描述较多的问题，因此亟需提升模型的视觉绑定能力和事实一致性。

Method: 提出CURE框架，对多模态模型进行三项任务（短语定位、与解剖结构绑定的报告生成及一般定位型报告生成）微调。通过动态采样，模型表现越差的数据样本得到更多关注，加强模型空间与文本的对齐。整个过程不需新增标注数据。

Result: CURE提升了模型的定位精度（IoU提升0.37）、报告质量（CXRFEScore提升0.188），并显著减少了18.6%的幻觉（虚假/不符事实的内容）。

Conclusion: CURE无需额外数据，通过智能采样和分阶段训练，能够高效提升医学多模态报告的精准性与可靠性，为后续临床应用提供了更可靠的基础。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出了一种名为DuFal（Dual-Frequency-Aware Learning）的新框架，通过整合频域与空域处理，有效提升了稀疏视角锥束CT重建中高频解剖结构的恢复能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在稀疏投影CT重建中难以还原精细高频结构，主要倾向于低频学习，导致图像细节损失。医疗应用中，准确恢复高频解剖信息对诊断极为重要，因而亟需能有效捕捉高频细节的新方法。

Method: 作者提出了双路径架构，分别在频域和空域处理特征。包括全球高频增强傅里叶算子（捕获全局频率模式）和局部高频增强傅里叶算子（处理空间块以保留局部特征），并结合了光谱-通道因子分解和跨注意力频率融合模块，实现频域与空域特征高效整合，最后由特征解码及强度场解码恢复CT体积。

Result: 在LUNA16和ToothFairy两个公开数据集上的实验显示，DuFal在高频结构保留和极稀疏投影条件下，均显著优于现有最先进方法。

Conclusion: DuFal提出了一种新颖且有效的频域-空域联合重建策略，极大提升了稀疏视角CT高频细节恢复能力，有望推广至其他高频信号重建应用。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 该论文提出了一种用于仅凭少量正常样本检测异常区域的新方法，通过结合视觉-语言模型和偏差统计评分，在MVTecAD与VISA基准上实现了领先的像素级异常检测效果。


<details>
  <summary>Details</summary>
Motivation: FNSAD任务难点在于训练样本极少且异常类型多样，现有方法在异常检测能力和区域判别方面还有较大提升空间。作者希望解决这些核心难题，提升检测精度和解释性。

Method: 提出了偏差引导型提示学习（deviation-guided prompt learning）框架：① 用可学习的上下文向量替代固定的prompt前缀，异常相关的后缀实现类别区分。② 引入偏差损失和Top-K多实例学习（MIL），建模patch特征与正态分布的高斯偏差，使得显著偏离正常特征的patch获得更高异常评分，从而提升定位和解释能力。

Result: 在MVTecAD和VISA数据集上，所提方法像素级检测效果优于PromptAD等主流基线。消融实验表明learnable prompts、偏差评分和Top-K MIL各环节均带来性能提升。

Conclusion: 偏差引导下的可学习提示与统计评分结合，为少样本异常检测提供了一种高效、可解释和泛化性强的新方案，有望进一步拓展视觉-语言模型在工业检测等领域的应用。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 本论文提出了一种基于NeRF的新颖方法，可以从单次曝光的模糊LDR（低动态范围）图像和事件数据合成清晰的HDR（高动态范围）三维新视角图像，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在自然环境中，由于光照极端或摄像头曝光不足，获得的LDR图片常常模糊且动态范围有限，难以支持高质量的新视角合成。现有部分方法引入事件数据改善这一问题，但未充分考虑摄像头传感器与物理世界辐射之间的差异，导致HDR复原和去模糊效果有限。

Method: 作者提出了一个基于物理传感器建模的NeRF框架，直接在HDR域表示三维场景的实际辐射，模拟物理世界的像素照射过程。设计了像素级RGB映射场，将渲染值与真实传感器LDR值对齐，并开发了事件映射场将物理场景动态与事件传感器输出连接。这两个映射场与NeRF网络联合优化，利用事件空间与时间动态信息加强HDR清晰三维表示。

Result: 在自采及公开数据集上的实验结果表明，该方法能用单次曝光的模糊LDR图像及其事件数据，在去模糊HDR三维新视角合成上达到当前最优成果。

Conclusion: 所提传感器物理建模的NeRF框架有效提升了极端光照下基于模糊LDR图像的新视角HDR三维合成质量，展现了事件数据结合物理建模的巨大潜力。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 本文提出用Vision Transformer（ViT）替代U-Net在胸部X光影像属性中和任务中的卷积编码器，以期更好地减少性别、年龄等人口属性在AI分类器中的泄露，提升诊断公平性。实验证明ViT有效降低了属性泄露，同时保持了良好的疾病诊断准确度。


<details>
  <summary>Details</summary>
Motivation: 当前胸部X光AI分类器常被性别、年龄等偏见影响，从而导致对少数群体的系统性误诊。以往基于卷积的特征中和方法（如U-Net）难以在不影响诊断性能的情况下彻底去除这些属性信息。本研究欲探索是否通过Vision Transformer架构能更进一步缓解这一问题。

Method: 在ChestX-ray14数据集上，将原有属性中和框架中的U-Net卷积编码器替换为DeiT-S Vision Transformer。通过调节不同中和强度（11档），利用AI判定器评估残留属性泄露情况，并用ConvNet测试编辑后影像的疾病预测准确性。

Result: 在中等中和强度（alpha = 0.5）下，ViT中和器将患者性别识别AUC降到约0.80，相比原U-Net方案降低了约10个百分点，并且训练轮数减少一半。与此同时，15项疾病宏观AUC仅比未编辑影像下降不到5个百分点，最差子群AUC也维持在约0.70。

Conclusion: 全球自注意力结构（如Vision Transformer）能进一步减少人口属性信息泄露，又不牺牲临床诊断能力，或为构建更加公平的医学影像AI系统提供了实用方法。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: LASAGNA提出了一种新颖的图像生成框架，能够同时生成高质量背景和包含真实视觉效果（如阴影、反射等）的透明前景分层图像，进而提升了图像可控编辑的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成模型在编辑现有图像的特定元素时，难以实现可控且一致的结果。尽管分层表示可带来灵活的内容创作，但当前方法的分层通常缺乏合理的合成关系，且对象层难以展现逼真的视觉效果（如阴影、反射等）。

Method: LASAGNA是一个统一的框架，通过同时生成具有真实视觉效果的前景（包含alpha通道）和背景，实现高质量的分层图像。该模型能根据多种条件输入（文本、前景、背景、位置掩码等）学习正确的图像合成关系。此外，作者构建了新的LASAGNA-48K数据集（包含物理真实视觉效果的分层图像），并提出了用于分层编辑评测的LASAGNABENCH基准。

Result: 实验证明LASAGNA能够在多个图像层之间生成高度一致和连贯的结果，且对后期多样化编辑非常友好，能准确保留图像身份及视觉效果。

Conclusion: LASAGNA提升了图像分层生成及编辑的可控性和效果，为实际应用提供了更好的工具与基础，相关数据集及基准将公开，有助于推动社区研究。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于手背皮肤形变信息的新型手势估计方法，显著提升了遮挡场景下的精度，同时减少模型参数和需求。


<details>
  <summary>Details</summary>
Motivation: 当前XR设备普及推动了自中心视角下手势估计的重要性，但手部遮挡（尤其是手指遮挡）极大地限制了传统方法的效果。需要新线索和算法来提升遮挡下的手势估计能力。

Method: 提出了一种双流delta编码器，比较动态手势和基线放松姿势的特征差异。该方法利用手背皮肤变形的丰富信息，且仅使用裁剪的手背图像输入，依赖新型稠密视觉特征提取技术。

Result: 在手指遮挡超过50%的情况下，所提方法仅用手背图像输入即可将平均关节角误差（MPJAE）较当前最优方法降低18%。同时所需模型体积更小。

Conclusion: 本文方法在遮挡场景下提升了可靠性，有助于实现更丰富和自然的交互方式（如无可见运动的点击等），同时减少对模型复杂度的需求。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视频多模态大模型迁移方法VIOLA，通过极少量标注和大量未标注数据，实现模型在新视频领域的适应，显著提升低标注场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在新颖视频领域的泛化能力受限于标注数据稀缺，尤其在工业或医疗等需要专家标注的场景。标准的in-context learning方法对大规模标注依赖较高，难以满足现实需求。因此，如何高效利用极少标注和大量未标注数据，降低专家标注成本，具有重要意义。

Method: 提出名为VIOLA的框架。首先，利用密度-不确定性加权采样策略，结合多样性、代表性和信息量，有效选取需标注样本，避免选中离群点。其次，针对未标注数据，构造混合样本池，引入置信度感知的检索和提示机制，对标注和伪标注的置信度建模，让模型动态区分真伪标签，提高迁移效果。

Result: 在九个不同基准和四种主流多模态大模型上进行实验，结果表明VIOLA在低标注条件下显著超过各种基线方法，实现了强鲁棒性和低成本的自适应能力。

Conclusion: VIOLA框架极大提升了多模态大模型在新视频领域的泛化能力，尤其在极少标注条件下表现突出，有助于实际应用推广。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 本文研究了在K-pop偶像面部生成任务中，现有扩散模型在视觉质量和语义一致性之间的权衡，提出并验证了一种新评测指标RCA用于精准衡量语义可控性。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型（DDPM）在高保真图像生成领域表现优异，但现有评测指标难以检测到语义一致性（如身份一致性）的问题，尤其是在细粒度、单一领域的生成任务（如K-pop偶像人脸）。

Method: 作者聚焦于K-pop偶像脸部（32×32分辨率）条件生成任务，提出并采用校准后的相对分类准确度（RCA）指标，将生成模型表现与理想分类器基准进行比较。同时，用混淆矩阵分析模型失败类型。

Result: 在实验中，该模型在视觉质量指标（FID 8.93）上表现优异，但在语义可控性（RCA 0.27）上表现较差，尤其易在性别内的相似身份间混淆。

Conclusion: 提出的RCA指标能更严谨地评估条件生成模型的身份一致性，并发现生成模型在分辨高度相似类别（如K-pop偶像）时存在显著的语义崩溃风险。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [12] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法（RSM-CoDG），融合了空间、时间和去除个体偏差的策略，显著提升了跨受试者脑电情感识别的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨受试者脑电情感识别受到个体差异大、脑区活动空间组织和时间演化复杂等挑战。现有方法通常孤立地改善空间或时间建模，或泛化策略，难以统一协调跨受试者的一致性、多尺度建模与个体偏差抑制。

Method: 提出RSM-CoDG方法：1）利用脑区功能分区的神经科学先验，构建区域层面的空间表示提升跨受试者可比性；2）采用多尺度时序建模把握情感相关神经活动的动态演化；3）引入协同泛化策略及多维约束，显著减少个体特异性偏差，实现对全新未见受试者的泛化。

Result: 在SEED系列数据集上，RSM-CoDG在所有横向对比中都显著优于现有方法，展现出更高的准确率与鲁棒性。

Conclusion: RSM-CoDG为跨受试者EEG情感识别问题提供了统一、有效的解决框架，提高了模型面对新个体时的泛化能力，并推进了领域内的技术进步。

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [13] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 本论文提出了一种结合自动化Chain-of-Thought (CoT) 数据生成和强化学习（RL）的方法，有效提升了深度伪造检测的可解释性和通用性，并显著降低了数据标注成本。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法通常缺乏可解释性输出，且高质量详细标注数据集稀缺，导致基于多模态大模型（MLLM）的检测方法难以落地。本研究旨在通过新方法兼顾检测精度、可解释性与现实数据标注成本。

Method: 提出了基于自混合图像的自动CoT数据生成框架，以及结合强化学习增强的深度伪造检测体系，引入定制化奖励机制和反馈驱动的合成数据生成。

Result: 所提出方法在多项跨数据集基准中，取得了与最新（SOTA）方法相当的检测成绩。实验验证了数据构建流程、奖励机制和强化学习策略的有效性。

Conclusion: 该方法有效提升了深度伪造检测的可解释性和跨领域泛化能力，降低了数据标注门槛，为利用主流多模态大模型检测深度伪造提供了新思路和实用工具。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [14] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种面向持续全景感知（CPP）的持续学习模型，能够同时进行多任务和多模态的增量训练，显著提升图像感知能力，有效缓解遗忘和语义混淆等难题。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习主要集中于单任务，难以应对多任务和多模态的实际应用场景。多任务持续学习不仅面临遗忘问题，还带来多模态语义对齐和模型退化的挑战。该工作旨在突破这一局限，实现更全面的智能感知。

Method: 提出一种端到端CPP模型，包含协同跨模态编码器（CCE）进行多模态嵌入，并通过对比特征蒸馏和实例级蒸馏继承知识，有效抑制遗忘。此外，引入跨模态一致性约束，并发展出CPP+，强化多任务增量下的语义对齐。所提模型还用非对称伪标签方式，无需保存示例即可持续进化。

Result: 在多个多模态数据集和多种持续学习任务上进行实验，结果表明，所提模型在尤其细粒度持续学习任务上表现出优越性能。

Conclusion: CPP模型实现了多模态、多任务的持续增量感知，有效缓解遗忘和语义混淆问题，为构建高水平智能感知AI系统提供了新方案。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [15] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Superquadric的3D占用预测新方法SuperOcc，通过三项创新设计，在效率和表达能力之间取得了突破，实验上取得了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D占用预测方法多用稠密场景表示，忽视了现实场景稀疏性；而Superquadric虽具优势，但存在时序建模不足、表达能力与稀疏性难权衡、投影效率低等问题。

Method: SuperOcc方法包含三大创新：(1) 融合视角与目标两方面特征的时序建模机制；(2) 多Superquadric解码策略，提升表达力且不损失稀疏性；(3) 高效的Superquadric到体素映射方法，提高计算效率。

Result: 在SurroundOcc和Occ3D两个权威基准上，大量实验显示SuperOcc达到SOTA性能，效率优于现有方法。

Conclusion: SuperOcc有效解决了Superquadric占用预测中的核心难题，实现了高效且高精度的3D占用预测，对自动驾驶感知具有重要意义。

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [16] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的事件感知视频流处理框架Event-VStream，使得多模态大语言模型在处理长视频流时既能保持实时性，又能有效利用上下文，实现更精准的理解与生成。


<details>
  <summary>Details</summary>
Motivation: 现有模型在流式长视频理解时，常见方法有处理冗余帧、固定间隔解码或缓存裁剪等，但这些方法易造成内容重复或丢失关键时序信息，不能兼顾效率和长期记忆。

Method: 提出Event-VStream架构，将连续视频划分为一系列离散且语义连贯的事件，通过结合运动、语义和预测性线索检测视频状态变化，只有在发生事件边界时才触发语言生成。所有事件嵌入被合并到持久内存库，实现低延迟下的长时间推理。

Result: 在多个长视频流评测基准上，Event-VStream表现优异：在OVOBench-Realtime上比VideoLLM-Online-8B基线提高10.4分，在仅使用通用LLaMA-3-8B骨干下性能接近Flash-VStream-7B，并且在2小时Ego4D流测试中取得约70%的GPT-5胜率。

Conclusion: Event-VStream能够在低延迟和强记忆能力之间取得较好平衡，极大提升了多模态大模型对实时长视频的理解能力，并具备实际应用潜力。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [17] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 本文提出了Skywork UniPic 3.0模型，实现了业内领先的单图编辑与多图合成能力，尤其在人-物交互（HOI）类多图合成任务上表现突出，优于当前热门模型Nano-Banana和Seedream 4.0。模型、数据和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 多图像合成任务成为社区关注热点，但目前模型很少公开高质量融合方法的技术细节。分析显示，人-物交互类型的多图合成需求最强，因此亟需推动相关方法研究与提升。

Method: 提出统一多模态架构，可处理任意1~6张输入图像及分辨率，实现单图编辑与多图合成统一建模。设计高质量数据收集、筛选和合成流程，仅用70万样本实现强大性能。创新性地将多图合成转化为序列建模问题，并在推理阶段结合轨迹映射与分布匹配，大幅提升采样速度和样本质量。

Result: Skywork UniPic 3.0在单图编辑基准测试中达到业内最高水平，在多图合成测试中超越Nano-Banana与Seedream 4.0，实现12.5倍推理加速。

Conclusion: 本文的方法在算法架构、数据流程与推理速度等多方面实现突破，显著提升多图合成的质量与效率。模型及相关资源已开源，有望推动学界和产业实践发展。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [18] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成对抗网络（Cr-GAN）框架，在合成孔径雷达（SAR）图像小样本识别问题中，即使极少数据也能生成高质量样本，有效提升小样本学习表现。


<details>
  <summary>Details</summary>
Motivation: 当前SAR影像小样本识别任务受限于标注数据极度稀缺。常用方法是通过GAN生成大数据集，再用自监督预训练、微调少量标注样本，但传统GAN本身亦需大量样本训练，导致“数据稀缺悖论”。研究动机在于突破这一核心瓶颈，提升小样本下的SAR识别性能。

Method: 作者提出Cr-GAN，一种带有一致性正则的新型GAN。其创新点包括双分支判别器，将判别和特征学习解耦；使用通道特征插值生成新潜在特征；提出双域循环一致性机制保证语义一致。该框架适配多种GAN结构，并提升多种自监督方法。

Result: 在MSTAR和SRSDD数据集8-shot设置下，Cr-GAN准确率分别达到71.21%和51.64%，显著优于现有主流方法，且只需最先进扩散模型约1/5的参数量。

Conclusion: Cr-GAN能在极端小样本数据下生成多样高保真样本，有效提升SAR图像小样本识别性能，并具备高效率和广泛适配性。

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [19] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于性能（mAP）引导的强化学习主动学习方法（MGRAL），专为目标检测任务设计，能够更高效地选择有信息价值的样本用于标注，并在PASCAL VOC和COCO数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前主动学习主要依据数据本身的分布或信息量来评估样本的重要性，但这种方式与下游任务（如目标检测中的mAP）表现提升没有直接相关性。因此有必要提出与模型实际性能提升直接挂钩的新型主动学习方法。

Method: 作者提出MGRAL，通过强化学习中的策略梯度方法，引入以mAP提升为奖励信号，训练采样代理动态选择对性能提升最有贡献的样本批次。同时，为降低未标记样本上mAP估算的计算量，引入了无监督的查找表方法，实现高效采样。

Result: 在PASCAL VOC和COCO目标检测数据集上的实验表明，MGRAL实现了主动学习任务中的最优性能曲线，并配合可视化结果证明了方法有效性。

Conclusion: MGRAL证明了借助强化学习技术，围绕下游任务实际性能驱动的主动学习策略能够显著提升目标检测主动学习效率，为该领域树立了新的范式。

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [20] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BVS的全新图文组合越狱框架，以揭示多模态大模型在视觉安全上的边界缺陷。实验显示该方法对最新GPT-5模型的越狱成功率高达98.21%。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的迅速发展，其在文本-视觉交互下的安全问题日益突出。虽然已有研究关注了多模态模型的安全性，但针对视觉安全边界的系统性探索仍较为稀缺。

Method: 提出了Beyond Visual Safety (BVS)框架，采用“重构-再生成”策略，利用中性化视觉拼接和归纳式重组，将恶意意图与原始输入进行解耦，诱导多模态大模型生成有害图片。

Result: BVS方法对GPT-5（2026年1月12日版）模型的越狱成功率高达98.21%，证明其可以绕过当前的视觉安全防护。

Conclusion: 当前多模态大模型在视觉安全对齐方面存在严重漏洞，BVS越狱框架揭示了其视觉安全边界亟待加强。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [21] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 本文提出了三种轻量化改进方法，用于提升基于ALOS-2单极化SAR数据的日本全国土地利用/覆盖语义分割性能，并显著优化了对罕见类别和水体的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有SAR影像的密集预测任务存在常见问题，如边界平滑过度、细长结构遗漏和长尾类别表现不佳，而不增加模型复杂度的优化方法仍然有限。

Method: 在SAR-W-MixMAE自监督预训练基础上，提出：(1)高分辨率特征注入多尺度解码；(2)逐步细化上采样模块，交替进行卷积精炼和步进式上采样；(3)在focal+dice损失中加入α尺度因子，优化类别重加权。

Result: 在日本ALOS-2 LULC基准数据集上，各项指标均显著提升，罕见类别的分割效果提升明显，水体检测表现也有改善。

Conclusion: 提出的三种改进方法无需增加模型复杂度即可显著提升SAR-LULC语义分割和水体检测性能，特别适用于长尾类别场景。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [22] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 该论文针对时尚领域多属性预测问题，提出了一个三层次的评估框架，并系统性评测了九种视觉-语言模型（VLMs）的零样本属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 精细化的属性预测对时尚零售业的应用（如目录丰富、视觉搜索、推荐系统）至关重要，但如何处理属性的适用性检测（如某些服饰属性在特定图片中无意义）和对VLM在多属性场景下的系统性评估仍未充分探索。

Method: 作者提出了一个三层的评估框架，分别考察整体任务表现、属性适用性检测（属性是否存在或可见）、及可判定属性的精细分类。利用DeepFashion-MultiModal数据集，包含明确的属性不可用（NA）标签，对九种VLM和Fashion-CLIP特征的分类器进行了5,000张图片、18种属性的系统评测。

Result: （1）VLM实现了64.0%的宏平均F1分数，远超传统Fashion-CLIP分类器三倍；（2）VLM在属性细分分类方面表现优异（F1 70.8%），但在属性适用性检测上表现差（F1 34.1%），暴露出主要瓶颈；（3）高效模型能以更低成本实现旗舰模型90%以上的性能，便于实际部署。

Conclusion: 论文提出的分层评测方法有助于准确定位错误来源（是可见性检测还是分类本身），为实际系统改进指明了方向。VLM在多属性无监督预测中表现突出，尤其适合大规模低成本部署，但针对属性适用性检测仍需提升。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [23] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频大模型VideoThinker，通过为模型生成并使用合成的工具交互数据，显著提升了其对长视频时序内容和关键片段的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频大模型在处理长视频时主要依赖于静态推理和均匀采样，这会导致时序定位能力减弱，以及关键信息丢失。虽然可以借助一些自适应的工具手段如时序检索和缩放解决问题，但现有数据集构建对模型能力有依赖，形成了“先有鸡还是先有蛋”的循环困境。

Method: 作者提出将原始视频转为丰富的文本描述，利用强大的语言模型在描述空间中生成多步工具使用序列。之后再将文本序列映射回对应的视频帧，从而自动生成大规模的“视频-工具推理”数据集。全流程无需模型原本拥有长视频理解能力。然后用这些数据训练VideoThinker模型，使其具备动态推理、时序自适应探索以及多步工具组合能力。

Result: VideoThinker在多个长视频理解基准测试上大幅超越仅靠字幕的语言模型代理和现有强力视频模型。

Conclusion: 合成的工具增强型数据与自适应检索、缩放能力显著提升了大模型对长视频的理解和推理能力，为视频大模型开辟了一种方便有效的训练与数据构建新范式。

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [24] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 本文提出了一种融合关键帧和强化学习的新型视觉里程计（VO）方法，有效提升了视觉基础模型在姿态估计和稠密重建任务中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉基础模型的VO/SLAM方法虽然可以实现端到端的姿态估计和稠密重建，但未能像传统管线那样利用关键帧技术来提升效率和精度，导致计算冗余和性能下降。为此，亟需一种能在基础模型中自适应地选择关键帧的机制。

Method: 作者提出用强化学习方法，训练一个自适应关键帧选择策略，使其依据基础模型内部特征自动选择信息量丰富的帧。训练在TartanAir数据集上完成，并对多种真实数据集进行了测试。

Result: 新方法在多个真实场景数据集上相较目前最优的端到端VO技术表现出持续且显著的提升。

Conclusion: 通过引入基于强化学习的关键帧选择，视觉基础模型VO方法变得更高效、准确，为视觉定位与建图领域带来新的进展。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [25] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的脑电源成像（ESI）方法FAIR-ESI，通过多视角自适应特征重要性细化，有效提升脑相关疾病的诊断精度。


<details>
  <summary>Details</summary>
Motivation: 当前脑电源成像领域，虽然基于模型优化与深度学习的方法表现良好，但特征选择与细化依然是精准诊断的关键难题。为提升特征利用价值，提高ESI效果，需要开发更智能的特征处理框架。

Method: 提出了FAIR-ESI框架，创新性结合了FFT谱特征细化、加权时序特征细化以及基于自注意力的分块特征细化三种机制，从不同视角自适应优化特征重要性。方法在两套仿真数据与两套真实临床数据上进行广泛实验验证。

Result: FAIR-ESI在多种数据集和多样场景下均表现出优越的性能，显著优于已有方法，验证了其有效性和泛化能力。

Conclusion: FAIR-ESI框架不仅提升了脑电源成像精准度，为脑疾病诊断提供有力工具，同时也为理解脑功能机制带来了新启示。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [26] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 本文提出了一种针对Vision-Language Action (VLA)模型的分心图像token剪枝（DTP）方法，有效提升了机器人操作模型的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型能将视觉-语言理解能力用于机器人操作，但其注意力常分散于任务无关的图像区域（称为“分心token”），降低了生成正确动作的概率，从而影响任务表现。该现象普遍存在于现有VLA模型中，需要改进。

Method: 提出了一种简单且可直接应用的Distracting Token Pruning（DTP）框架，可以动态检测并剪除分心的图像token，通过修正模型的视觉注意力分布来提升任务表现。该方法无需修改原有模型架构或增加额外输入。

Result: 在SIMPLER基准数据集上，该方法在各种新颖VLA模型中均取得了任务成功率的相对提升，且适用于Transformer结构的VLA。进一步分析发现，任务成功率与注意力集中于任务无关区域的程度之间呈负相关，此现象在所有测试模型中均出现。

Conclusion: DTP方法无需改变VLA模型架构即可普遍提升表现，并揭示了关注任务相关区域对于模型成功的重要性，为后续研究提供了方向。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [27] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文提出了一种创新的多模态医学影像分析框架，通过亚区感知模态注意力机制和自适应提示工程，有效提升了大模型在医学图像多模态融合和分割任务中的表现。实验结果在BraTS 2020脑肿瘤分割任务中显著优于现有方法，尤其在坏死核心区域。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型难以高效融合多模态信息，且难以适应病理区域的异质性，导致医学图像中精细分割效果不佳，尤其在异质肿瘤分区。

Method: 提出两项关键技术：(1) 亚区感知模态注意力机制，针对不同肿瘤亚区学习最优模态组合；(2) 自适应提示工程，利用基础模型能力提升分割精度。

Result: 在BraTS 2020脑肿瘤分割数据集上验证，该方法整体和在坏死核心等难分区都显著优于基线方法。

Conclusion: 该框架为医学影像中的多模态融合和提示机制提供了有效且有原则的方法，有望促进大模型在医学影像领域的应用和发展。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [28] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度图像隐写（DIS）方法ARDIS，突破了以往必须与载体分辨率一致的限制，实现了任意分辨率秘密图像的隐写与高保真的恢复。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写方法要求秘密图像与载体图像分辨率一致，否则需重采样，导致细节损失和不可还原的难题。实际应用中，秘密图像分辨率多变，需解决此限制以提升实用性和效果。

Method: 首先，提出频率解耦架构，将秘密图像分解为分辨率对齐的全局基和与分辨率无关的高频潜变量，再嵌入固定分辨率的载体。其次，采用潜码引导的隐式重构器，结合连续隐式函数精准还原高频细节。最后，引入隐式分辨率编码，将分辨率信息转化为特征图并隐写，实现无需先验分辨率的盲恢复。

Result: 实验显示ARDIS在图像不可见性与跨分辨率恢复精度上均显著优于现有最先进方法，尤其在不同分辨率秘密图像的还原上具备高保真度。

Conclusion: ARDIS首次实现了任意分辨率秘密图像的深度隐写与盲恢复，解决了传统方法分辨率受限和细节损失的问题，在隐写容量、隐蔽性与还原性方面实现了突破，为图像隐写领域开拓了新的技术路径。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [29] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 本文提出了一种新的高光谱图像分类方法ES-mHC，通过可解释的超连接结构提升了模型透明度和可分析性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在高光谱图像分类中普遍依赖于晦涩的光谱空间特征混合，难以解释模型内部机制，影响其在关键领域的可信度和应用。

Method: 提出ES-mHC超连接框架，将特征表示和特征之间的交互结构分离，通过有结构、有方向的矩阵显式建模不同电磁谱分组间的关系，使内部信息流可视化、可空间分析。

Result: 在高光谱图像分类任务中，学习到的超连接矩阵表现出连贯的空间模式和非对称的交互行为，并且扩展率提升能加速结构化交互模式的出现。

Conclusion: ES-mHC将高光谱图像分类从纯黑盒过程转变为部分开放、结构透明的白盒过程，提高了模型可解释性以及内部机制分析能力。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [30] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: 本文提出FeTal-SAM，一种针对胎儿脑MRI分割任务改进的Segment Anything Model（SAM）方法，能够灵活应对不同标签定义和结构，无需反复重新训练，兼具较强性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割方法往往需要大量有固定标签的数据，缺乏灵活性，不能很好适应临床和科研中不同的解剖结构分割需求。此外，现有方法分割结果受限于是否真实体现图像对比度或仅靠空间先验，缺乏透明性。

Method: FeTal-SAM融合了atlas（模板图谱）驱动的密集提示和SAM基于基础模型的方法，通过多atlas配准生成空间对齐的标签模板，作为分割解码器的提示。此外结合bounding-box提示，实现结构逐一的二值分割，并通过后融合重建三维全脑分割结果。

Result: 在dHCP和自有数据集上评测，FeTal-SAM对于清晰结构（如皮层板、小脑）分割准确度与已有最佳模型相当，并保持针对自定义解剖结构的分割灵活性。对于低对比度结构（如海马、杏仁核），准确性略低，但仍展现良好泛化性。

Conclusion: FeTal-SAM无需针对每个标签定义和数据集反复训练，能够适应多样化分割需求，是朝向临床实用胎儿脑MRI分析方向迈出的重要一步。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [31] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本论文提出了LL-GaussianMap，一种首次将2D高斯喷射（2DGS）明确表示方法应用于低光照图像增强的无监督框架，有效提升图像结构感知及视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前低光照图像增强方法多在像素域或隐式特征域操作，忽略了图像的结构几何先验信息，而2DGS具备优秀的结构拟合与渲染能力，但尚未被用于低层次视觉任务。作者希望利用2DGS提升图像增强的结构保真度和效率。

Method: 该方法包括两个主要阶段：首先利用2DGS进行高保真结构重建；然后通过高斯喷射的光栅化机制，结合创新的增强模块，生成由2DGS引导的增益图，进而实现数据驱动的增强字典系数渲染。整个过程中采用无监督学习，避免了对成对数据的依赖。

Result: 实验结果显示，LL-GaussianMap能在非常低存储消耗下显著提升低光照图像增强效果，优于现有主流方法，且在边缘保留与伪影抑制等方面表现更优。

Conclusion: 本文首次展现了显式的高斯表示在图像增强中的优势，证明了2DGS能够有效提升低光照图像增强的性能和效率，为相关结构感知视觉任务提供了新思路。

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [32] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出了LL-GaussianImage，这是首个可在2D高斯展布（2DGS）压缩表示域内直接进行低光照增强的零样本无监督方法，无需繁琐的解压-增强-再压缩过程，且能显著提升效率与画质。


<details>
  <summary>Details</summary>
Motivation: 当前2D高斯展布因高保真和高压缩比被用于图像压缩，但低光照增强算法主要作用于像素域，导致必须解压后处理，效率低且易产生二次损伤。作者希望突破这一限制，直接在2DGS压缩域中实现增强。

Method: 作者提出了三项创新：1）提出基于语义引导的Mixture-of-Experts增强框架，实现基于渲染引导的自适应变换，无需完全解压；2）构建多目标协同损失系统，约束增强过程中平滑性与保真度，抑制伪影产生；3）引入两阶段优化，先单尺度重建确保表征精度，再提升网络鲁棒性，实现重建即增强。

Result: 实验结果表明，在保持高压缩比的同时，该方法可高质量增强低光照图像，验证了在压缩表示域内直接处理的可行性和优越性。

Conclusion: LL-GaussianImage有效解决了2DGS压缩图像低光照增强的效率与质量难题，避免了解压重编码损伤，为压缩表示直接处理开拓了新路径。

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [33] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的数据增强框架，用于提升电子显微镜下神经元分割的性能，特别是在标注数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法依赖于大量带注释的数据，且传统增强方式生成的样本结构多样性不足，限制了神经元连接组重建的准确性和实用性。

Method: 提出了一种分辨率感知的条件扩散模型，将多尺度条件和电子显微镜分辨率先验引入到3D掩膜生成体素级别的图像，同时结合生物结构引导的掩膜改造模块以合成结构更真实的增强样本。

Result: 在AC3和AC4数据集低标注条件下，结合两种不同后处理方法，ARAND指标分别提升了32.1%和30.7%。

Conclusion: 该扩散式数据增强框架有效丰富了训练集，提高了神经元分割的表现，可为数据有限场景下的神经元连接组学研究提供有力支持。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [34] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 本文提出了一个用于测试视觉语言模型空间推理能力的新基准，包括情境感知和空间感知两项技能。实验证明当前模型在相关任务上的表现仅略高于随机水平，显著暴露了它们在空间理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在处理视觉和语言任务上取得了进步，但其在依赖细微时空或几何线索的空间推理任务中仍显脆弱。作者希望精确诊断VLMs在这些具体技能上的短板。

Method: 设计了一套合成视频对照基准，通过三类挑战（区分快/善意行为、在不同视角下角色绑定、精细轨迹对齐判断），不依靠模型训练，仅测试主流VLMs的空间推理能力。同时，引入简单的颜色提示辅助评估。

Result: 主流VLMs在任务中的表现仅略高于随机，说明空间推理能力有限。使用稳定颜色提示虽能在一定程度减少角色混淆，但无法根本解决问题。

Conclusion: 现有视觉语言模型在空间推理方面存在结构性缺陷。本文公开数据和代码，为后续研究更轻量空间先验和相关诊断方法提供工具和起点。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [35] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 本文开发了一款基于卷积神经网络（CNN）的移动应用，用于非专业人士快速识别不同类型的花卉，并确定了最佳模型与优化算法的组合。


<details>
  <summary>Details</summary>
Motivation: 日常生活中花卉用途广泛，但普通人辨别花卉种类依赖专家，获取专家信息的便利性有限，因此需要一种便捷的自动化花卉识别方法。

Method: 采用三种CNN模型（MobileNet、DenseNet121、Xception）并结合七种不同的优化算法进行训练和测试，对比评估模型在花卉分类任务中的表现，以选出最适合移动端应用的方案。

Result: DenseNet121结合SGD优化算法在分类任务中表现最佳，准确率达到95.84%，精确率、召回率与F1分数均为96.00%。

Conclusion: CNN模型（特别是DenseNet121+SGD）适用于在移动应用中进行高准确度的花卉识别，能够为普通用户提供方便、快捷的花卉分类服务。

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [36] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: 本文提出了一套轻量级实验管道，使生态学家无需深厚的机器学习背景，也能便捷地开发和优化本地化的图像分类模型。以鹿的年龄和性别分类为例，展示了该工具在有限数据下的高效表现。


<details>
  <summary>Details</summary>
Motivation: 生态学研究中常需对图片进行物种、性别、年龄等分类。现有通用ML模型往往难以适配具体生态学数据和问题，且非ML专家很难自定义优化模型，造成壁垒。因此，亟需为生态学家提供操作简单、灵活且高效的ML工具。

Method: 研发了支持命令行和图形界面的轻量级实验管道，包括图片预处理、模型训练、评估、标注和误差分析等环节。以德国森林中的红鹿为例，收集专家标注的图片并尝试多种神经网络结构、参数和数据增强策略。

Result: 在4352张专家标注的红鹿图片上，最佳模型年龄分类准确率达90.77%，性别分类准确率达96.15%。展示出该工具即便在有限数据下也能实现高可靠性的人口统计分类。

Conclusion: 该管道能够帮助生态学家无需深入ML知识即可定制开发适用于本地数据和特定任务的图像分类模型，有助于推动ML方法在野生动物监测和人口分析中的应用与普及。

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [37] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 本论文提出将数据集蒸馏方法（dataset distillation）首次应用于遥感图像解译，通过训练文本到图像扩散模型，把大规模遥感数据集压缩成一个小而精的数据集，有效降低存储和计算成本，同时缓解数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像解译取得巨大成功，但高度依赖大规模训练数据，带来存储/计算开销和敏感数据泄露风险。因此亟须方法生成高质量、体量小的数据集。

Method: 作者提出利用文本到图像的扩散模型，从大规模遥感数据生成代表性样本。方法包括：1）引入分类一致性损失，结合预训练模型提升合成样本的判别力；2）采用潜在空间聚类，挑选多样性强的原型并用视觉语言模型生成聚合文本描述，作为扩散模型的指导信息。

Result: 在三个高分辨率遥感场景分类基准上，方法能合成真实、多样的数据样本，并能高效用于下游模型训练。

Conclusion: 本文方法显著降低数据集对存储和算力的需求，同时兼顾数据质量与多样性，为遥感影像高效解译提供新思路。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [38] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本论文提出了一种基于物联网的智能植物监测系统，结合多种传感器和自动灌溉，通过云平台实现远程和实时监控，提升了农业可持续性与资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前农业面临需要节约资源和提升作物健康管理的双重压力，传统方式如人工观察和定时灌溉容易导致资源浪费和反应滞后。因此，采用智能化系统来优化植物生长和资源分配成为迫切需求。

Method: 设计并实现了以ESP32为核心的智能监测系统，通过DHT22、HC-SR04和土壤湿度传感器采集环境数据，配合OLED显示屏和蜂鸣器实现本地反馈，全部数据通过无线方式上传到ThingSpeak云平台，支持远程监控、历史分析和自动预警。

Result: 实验证明该系统能以92%的准确率维持最佳土壤湿度，实时监测环境参数，将用水量减少约40%，同时系统总体应用成本仅为45.2美元，并能直观展示植物健康状况。

Conclusion: 本系统为精准农业和智慧农场提供了低成本、高效能的解决方案，适用于从家庭园艺到商业农业等多种场景，有助于推广可持续农业发展。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [39] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种名为TinySense的高效压缩框架，通过创新的VQGAN和Transformer模型在压缩Wi-Fi通道状态信息（CSI）的同时，保障了高精度的人体姿态估计，并显著降低网络资源开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着对无设备、隐私友好的感知方案需求增长，Wi-Fi感知用于人体姿态估计（HPE）受到关注。但现有方法处理庞大的CSI数据，导致网络资源压力大，亟需高效压缩方案以提升可扩展性。

Method: TinySense采用基于矢量量化生成式对抗网络（VQGAN）的压缩方法，首先通过VQGAN学习得到码本，大幅压缩CSI数据；再结合K-means算法按需动态调整码率，进一步细分大规模预训练码本；引入Transformer模型减缓码率损耗，提高网络不稳定时的鲁棒性。该方案在Jetson Nano和Raspberry Pi平台原型验证。

Result: 实验结果显示，TinySense在相同压缩率下，人体姿态估计准确率（PCK20）比现有最佳压缩方案高1.5倍，延迟减少5倍，网络开销减少2.5倍。

Conclusion: TinySense极大提升了Wi-Fi感知下的人体姿态估计效率与准确性，有效降低了资源消耗和延迟，优于目前主流压缩方案，对大规模部署具有现实意义。

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [40] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种受大脑机制启发的轻量级神经网络方法，用于冠状动脉造影图像分类，解决了图像复杂、类别不均衡和标注不确定等问题，并保持高效性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉造影在临床中的评估和决策非常重要，但现有深度学习方法面临图像复杂、类别极度不均衡、标签不确定以及计算资源有限等诸多挑战，因此亟需更鲁棒且高效的技术。

Method: 作者基于预训练卷积神经网络，构建轻量混合型神经表示，并提出选择性神经可塑性训练策略以提升参数适应性，采用类脑注意力调控损失函数（将Focal Loss与标签平滑结合），以及类别均衡采样和余弦退火学习率调整以模拟大脑注意和调控机制。

Result: 实验表明，该模型在冠状动脉造影二分类任务中达到与主流方法媲美的准确率、召回率、F1值和AUC，并具有优越的计算效率和稳定性。

Conclusion: 该研究证实了类脑机制在轻量医疗影像分析中的有效性，为在有限计算资源下提供智能临床决策支持提供了生物学合理且可部署的解决方案。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [41] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于总变差的分布外样本检测方法（TV-OOD），通过计算每个输入对总变差的贡献进行分布内外判别，在多个模型和数据集上的图像分类任务中取得了优于或不逊于现有最先进方法的检测效果。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中，机器学习模型可能遇到与训练分布不同的输入（分布漂移），现有方法虽有效，但仍有提升空间，因此需要更精确、鲁棒的分布外样本检测方法保障模型安全。

Method: 提出了TV-OOD方法，利用总变差网络估算器计算每个输入样本对总体总变差的贡献，并将该值定义为分布外判别得分，以区分分布内外数据。

Result: TV-OOD方法在多个图像分类模型和数据集的分布外检测任务中，无论在各项评价指标上，都表现出与当前最先进方法相当或更优的检测效果。

Conclusion: TV-OOD方法在理论和实验上均展现了高效的分布外检测能力，为实际机器学习模型部署提供了更安全可靠的检测手段。

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [42] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 该论文介绍了一个首个公开的全配对、多器官医学影像数据集，支持多种AI影像合成任务，并建立了基准测试，促进肿瘤影像中的对比剂合成研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开医学影像数据集存在器官种类局限、配对不全、时序缺失和空间未对齐等问题，严重制约了基于AI的对比分子合成图像研究的进展。

Method: 构建并发布了一个跨11个人类器官、包含完整动态增强MR序列（DCE1-DCE3）和配对CT（无对比和有对比）的全新医学影像数据集，保证了解剖对应关系，支持多种影像翻译任务，并基于该数据集制定了系统的基准评测方案。

Result: 论文报告了基于该数据集进行的典型图像到图像翻译方法的实验结果，验证了多种研究设定如1对1、N对1和N对N的影像合成任务。

Conclusion: 该数据集和基准评测的公开，有望推动多器官肿瘤影像中对比剂安全、有效自动合成的研究，并对实际临床工作流具有直接意义。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [43] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 该论文探讨了视觉基础模型在下游任务上泛化能力不如语言基础模型的原因，并通过前列腺多参数MRI任务实证分析了预训练目标与下游任务匹配度对迁移性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型(VFMs)预训练目标与实际下游任务需求不匹配，导致在实际应用(如分割、分类、图像合成等)中性能提升有限。论文希望找出预训练策略与下游任务之间的关系，从而优化模型迁移性能。

Method: 作者选取了两种预训练方式分别强调重建（基于MAE）和对比学习（ProViCNet），并在前列腺多参数MRI的五项任务上进行评估，通过分析特征微调前后的分布差异(MMD等度量)来衡量预训练目标与下游任务的匹配度。

Result: 实验发现预训练目标与下游任务越匹配，迁移后的任务性能提升越大，收敛速度也越快，而且这种匹配关系可以通过简单的分布度量（如MMD）进行量化。

Conclusion: 预训练目标与下游应用任务的契合度直接影响视觉基础模型的迁移效果。因此，设计预训练任务时应注重与未来应用场景的匹配，以提升下游任务性能和模型效率。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [44] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: 该论文提出了一种无需语言监督的自监督框架RadJEPA，通过在无标注X光片上预训练，超越了依赖语言监督的主流方法，适用于疾病分类、分割和报告生成等任务。


<details>
  <summary>Details</summary>
Motivation: 目前医学视觉语言模型受限于成对影像-文本数据的获取，作者探讨能否不依赖语言监督也能获得强大的放射影像表示。

Method: 提出RadJEPA框架，基于联合嵌入预测（Joint Embedding Predictive Architecture），仅用无标签胸部X光图像，通过预测被遮盖区域的潜在表示来进行自监督预训练。该方法与主流的图像-文本多模态或DINO自蒸馏方法不同，重点在于潜在空间的预测。

Result: 在多个基准测试上（疾病分类、语义分割、自动报告生成），RadJEPA学习出的编码器表现优于包括Rad-DINO等最新方法。

Conclusion: 无需图文配对监督，仅用自监督学习，RadJEPA即可获得高质量的医学影像表示，具有广泛的应用前景，推动了医学影像表征学习的发展。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [45] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: 本文提出了一种结合可见光与热红外数据的多模态三维重建新框架ThermoSplat，显著提升了复杂环境下的重建与渲染质量，尤其在多光谱融合方面达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼洒(3DGS)技术难以充分融合RGB与红外多模态数据，尤其在处理模态间结构相关与物理差异时表现不佳，影响在复杂环境下的感知能力。

Method: ThermoSplat方法包括：(1) 跨模态FiLM调制机制，用热红外结构先验动态调制共享隐特征，有效引导可见纹理合成；(2) 模态自适应几何解耦机制，为热红外分支学习独立的不透明度偏差并执行独立光栅化；(3) 融合球谐函数与神经隐式解码的混合渲染管线，确保语义一致性与高频细节。

Result: 在RGBT-Scenes数据集上的实验表明，ThermoSplat在可见光和红外谱段都实现了当前最佳的重建与渲染效果，优于现有多模态3DGS方法。

Conclusion: ThermoSplat有效解决了多模态3D重建时模态间信息融合与结构解耦难题，为多光谱环境感知提供了更准确、鲁棒的解决方案，对多场景、多条件下的视觉应用有重要意义。

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [46] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出了一个新的细粒度复合图像检索（CIR）基准数据集EDIR，覆盖更广泛的类别，通过图像编辑合成高质量查询，系统评估现有多模态模型表现，并揭示现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准种类有限，无法代表实际应用中多样化查询的需求，评估方法和数据覆盖存在缺陷，导致当前多模态模型能力评估不够全面。

Method: 作者设计了一条通过图像编辑对查询进行精确控制的流程，合成多个类别、子类别的复合查询，构建了包含5000条高质量查询的EDIR基准，对13种多模态嵌入模型在这些数据上进行了全面评测。还进行了同域训练实验，分析不同类型任务的挑战及模型局限。

Result: 实验发现，即使是当前最先进的多模态模型，在EDIR上的各细分类表现也大不相同，普遍难以在所有子类别上保持较好表现，显示本基准难度高、覆盖全面。同时，对比分析还揭示了以往基准数据集存在模态偏置和类别覆盖不足等问题。

Conclusion: EDIR提升了CIR任务的评测难度和全面性，对现有方法进行更严格检验。实验表明，虽然某些类别通过有针对性数据能被较好解决，但也存在模型固有难以逾越的限制，EDIR为相关模型改进提供了更具挑战性的测试平台。

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [47] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本文系统分析了多模态基础模型中情感建模的内部机制，发现情感能力主要依赖于前馈门控投影（gate_proj）而非注意力模块。仅微调少量gate_proj参数即可高效实现情感理解和生成，展现出良好的参数效率。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态情感模型表现优异，但其内部是如何实现情感理解和生成尚不清楚，尤其是有哪些结构性模块起到了关键作用。作者旨在揭示情感能力在模型内部的结构归属与机制。

Method: 作者针对多种架构和训练方案，结合多种情感任务，系统性分析情感监督信号对模型参数分布的影响，并通过模块迁移、单一模块微调和破坏性消融等实验方法，定位实现情感理解的关键结构。

Result: 分析结果显示，情感相关的适应主要集中在前馈门控投影（gate_proj）模块，而非通常关注的注意力模块。gate_proj模块的调整对模型的情感能力至关重要，微调gate_proj即可高效获得与AffectGPT接近的性能。

Conclusion: 前馈门控投影（gate_proj）是情感理解和生成的结构核心，针对这一模块即可实现高效且必要的情感能力迁移和优化，这为未来情感AI模型结构设计提供了实证依据。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [48] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: 本研究评估了主流深度学习模型在虚拟现实（VR）中实时面部表情识别用于自闭症辅助的可行性，发现大多数通用方法在准确率与延迟之间难以兼顾，亟需更轻量、专用的模型。


<details>
  <summary>Details</summary>
Motivation: 当前VR辅助自闭症（ASD）人群提升社交技能需求日益增长，但相关实时情绪识别模型普遍只追求准确率，难以满足严格的延迟需求（MTP<140ms），实际应用受限。

Method: 以UIBVFED数据集中的虚拟角色为对象，对YOLO（v8、v11、v12）的Medium和Nano结构，以及CLIP、SigLIP、ViT-FER等Vision Transformer，分别在CPU下进行人脸检测与表情分类零样本识别性能基准测试。

Result: 虚拟角色的人脸检测可实现100%准确率，YOLOv11n在检测速度与准确率间表现最佳（约54ms）；但通用Transformer类模型如CLIP、SigLIP在表情分类环节无法在实时系统中取得足够准确率（<23%）或速度（>150ms），形成延迟瓶颈。

Conclusion: 当前通用模型难以满足实时VR辅助治疗的苛刻需求，应研发更轻量、面向特定领域的模型，推动个性化、平价AI疗愈工具落地。

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [49] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、基于现有模型的多视角3D手势估计方法，并发布了大规模外科手势基准数据集，在外科场景下显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D手势估计在外科领域受强光、遮挡、手套等因素影响，且缺乏高质量的数据集，有效提升相关任务的准确性和实用性面临巨大挑战。

Method: 设计了无需外科领域特定微调，完全基于开源预训练模型的多视角3D手势估计管线，包括：人员检测、全身姿态估计、2D手部关键点检测（在手部区域）、以及带约束的3D优化。同时构建了一个在仿真手术室中，含3,000帧手动标注2D及三角化3D真值、共68,000帧的手术场景数据集。

Result: 实验结果表明该方法在2D平均关节点误差上降低了31%，在3D平均关节点误差上降低了76%，在多项基准任务中优于现有方法。

Conclusion: 本研究提出的管线达到强基线效果，并发布了全面的标注数据集，为外科手势computer vision研究提供了坚实基础和推动未来工作的资源。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [50] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 本文提出了一种面向类别和置信度的再加权（re-weighting）方案，提升深度神经网络在长尾数据分布下的表现。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在长尾分布数据（即少数类别有大量样本，多数类别（尾部）样本稀少）下表现显著下降。以往解决长尾学习方法多集中于对类别先验偏差的logit级别修正，而对因样本置信度不同导致的优化过程关注较少。本文旨在弥补这一不足。

Method: 提出一种基于损失层面的类别和置信度敏感的再加权方法（loss re-weighting），其核心是Ω(p_t, f_c)函数，根据预测置信度和对应类别的相对频率调整每个样本对训练的影响。该方法可以与现有logit修正规则互补。

Result: 在CIFAR-100-LT、ImageNet-LT和iNaturalist2018等多个带长尾分布的数据集、不同不均衡因子下进行实验，结果表明该方法能显著提升模型在长尾数据集上的表现，验证了理论分析。

Conclusion: 通过结合类别频率与模型预测置信度对样本进行再加权，有效提升了神经网络在长尾分布数据下的学习效果，且与其他基于logit修正方法互补。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [51] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 本文提出了一种用于神经元分割的新方法NeuroMamba，能够高效捕捉远程依赖同时保留细粒度体素信息，在多个公开数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 神经元分割对神经连接组的重建至关重要，但神经元形态复杂且结构密集，现有方法在边界识别及细节保留方面表现不足，因此需要新的分割技术提升分割的精度和适用性。

Method: 提出了NeuroMamba多视角框架，结合了Mamba架构实现无需patch的全局建模以及辅助的局部特征建模，主要包含通道门控的边界判别特征提取器（BDFE）和空间连续特征提取器（SCFE），并融合多视角特征以精细分割神经元。

Result: NeuroMamba在四个公开电子显微镜（EM）数据集上取得了最优的分割性能，并且能适应各类各向异性与等向性数据分辨率。

Conclusion: NeuroMamba有效克服了以往方法在神经元分割中面临的边界模糊和细节损失问题，提升了分割的精度和适用范围，有望广泛应用于神经科学研究。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [52] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出了一种高效且精确的城市动态场景新视角合成方法EvolSplat4D，在重建时间和质量间实现了优越平衡，兼顾静态和动态目标。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法在精度和效率之间难以平衡。基于场景优化的神经辐射场和高斯展布虽能实现真实感，但优化过程耗时，而前馈方法则易在多视角动态场景中出现3D一致性问题。因此，亟需一种兼顾速度和三维一致性的新框架。

Method: EvolSplat4D采用三分支结构：对近景静态部分，通过3D特征体预测多帧一致的3D高斯几何，辅以语义增强的图像生成外观；对动态目标，以对象为中心在规范空间聚合时序特征并用运动自适应渲染保证时空一致性；远景用高效的像素级高斯分支保证全局覆盖。

Result: 在KITTI-360、KITTI、Waymo、PandaSet等数据集上，EvolSplat4D实现了对静态与动态城市场景的高精度和一致性重建，准确率和一致性均优于现有基于场景优化和主流前馈方法。

Conclusion: EvolSplat4D有效解决了城市动态场景新视角合成任务中，速度和效果难以兼得的问题，为自动驾驶仿真提供了更优的方案。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [53] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型对齐方法HyperAlign，通过超网络高效且有效地在测试时对齐生成结果，实现更符合人类偏好和意图的高质量输出。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型易生成语义不一致、审美较差的图像，主流的对齐方法（如微调和测试时缩放）存在多样性损失、计算开销大和对齐不足等缺陷。

Method: 提出HyperAlign框架：训练一个超网络，在推理时动态生成低秩调整权重，调控扩散模型中生成操作符，从而根据输入隐变量、时间步和提示进行奖励条件的对齐。该方法无需改变潜在状态，有不同变体以平衡性能和效率，并结合偏好数据正则的奖励目标减少奖励黑客现象。

Result: 在Stable Diffusion、FLUX等多个生成范式下，HyperAlign在提升语义一致性及视觉吸引力方面均显著优于现有微调和测试时缩放基线。

Conclusion: HyperAlign能够在保证效率的情况下，有效提升扩散模型图像生成的对齐性和质量，对生成模型的可控性和实用性具有重要意义。

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [54] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了PhysicsMind基准，专门用于评估多模态大模型和视频模型在物理定律一致性推理与生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然在数学、常识和视觉推理方面取得巨大进步，但对物理定律的理解能力研究不足。目前相关评测方法大多集中在合成数据或感知质量，与物理规律的一致性关联有限，缺乏统一、真实和符合物理规律的标准化测试。

Method: 作者提出PhysicsMind基准，涵盖真实与仿真场景，从三大物理原理（质心、杠杆平衡、牛顿第一定律）出发，设计两类任务：（1）VQA推理任务，测试模型从静态图像或视频中判断物理量和数值的能力；（2）视频生成任务，考查模型生成的视频运动轨迹是否符合物理定律。并在多个最新大模型和视频模型上进行评测。

Result: 实验结果显示，现有主流多模态大模型和视频生成模型依赖外观启发，常常在基础力学问题上出现违背物理定律的情况。

Conclusion: 模型在基础物理理解上尚有明显短板，现有的数据、规模和训练方式难以支撑物理一致性的需求。PhysicsMind为物理感知多模态模型发展提供了有针对性的评测平台，有助于驱动该领域的进一步进步。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [55] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出一种名为PAINT的新方法，通过结构优先的条件生成策略，更好地从H&E图像合成虚拟IHC染色，提升了结构一致性，并在公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 虚拟IHC可以节省成本和组织样本，但由于H&E形态和分子表达之间存在模糊联系，现有方法在语义一致性上表现不佳。亟需新的模型增强结构先验以减少语义不符。

Method: 作者提出了一种视觉自回归框架（PAINT），将IHC合成过程重新表述为“结构优先”的条件生成任务，并首次引入空间结构起始图（3S-Map）以保障生成的空间对齐和确定性。该方法先用全局结构布局约束分子信息分辨顺序，实现因果顺序建模。

Result: 在IHC4BC和MIST数据集上，PAINT在结构保真度和临床下游任务上均超过了主流方法。

Conclusion: 结构引导的自回归建模有效提升了虚拟IHC的语义一致性和实际价值，未来有望推动自动病理分析发展。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [56] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 该论文提出了ProGiDiff框架，通过结合ControlNet式条件机制和自定义编码器，将已有的生成扩散模型用于医学图像分割，实现对自然语言prompt的支持、多类别分割与跨模态（CT到MR）迁移。实验结果显示其性能优于现有方法，并具备专家交互和快速适配新模态能力。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像分割方法虽然高效，但一般是确定性的，缺乏与自然语言交互、多提案输出、以及跨模态（如CT和MR）适应能力。近期的扩散模型在文本到图像生成中展现潜力，但应用到医学图像分割时受限于需大规模数据、通常只能做二分类、且缺乏prompt条件能力。因此，亟需能结合自然语言、适应多种模态的小数据高效医学分割新方法。

Method: 作者提出ProGiDiff，充分利用现有预训练的扩散模型，通过引入类似ControlNet的条件机制和针对图像的自定义编码器实现分割任务，将prompt条件信息与图像特征结合，并以此指导扩散模型输出分割mask。该方法能通过prompt切换分割目标（如指定器官），支持多类别与专家多方案交互。同时，提出低秩少样本适配机制，实现对MR等新模态的快速迁移。

Result: 在CT医学图像的器官分割实验中，ProGiDiff优于以往方法，且在专家互动（提供多建议）场景下表现良好。提出的条件机制能够通过低秩少样本方式顺利迁移到MR分割任务，显示出卓越的跨模态泛化能力。

Conclusion: ProGiDiff创新性引入了可扩展的条件机制，使预训练扩散模型可以用于多类别、多样本建议和自然语言prompt条件的医学图像分割，显著提升了准确性与灵活性，并且能够以极少样本适配到新模态，为实际临床分割任务提供了高效可扩展的解决方案。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [57] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: 本论文提出了DSFedMed框架，通过中枢大模型与轻量级客户端模型的双向知识蒸馏，提升了联邦学习在医学图像分割中的性能，显著降低通信和推理成本。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽然在视觉任务上泛化能力强，但高算力、通信与推理开销限制了其在资源受限联邦学习中的落地，尤其是在医疗场景。

Method: 提出DSFedMed框架，中央的基础模型与客户端轻量模型互相蒸馏知识。采用高质量生成医学影像替代真实公共数据，并设计了基于可学习性引导的样本选择策略，提升蒸馏效率和效果。

Result: 在五个医学图像分割数据集上，DSFedMed的分割效果（Dice分数）平均提升2%，通信与推理成本相比已有联邦基础模型降低近90%。

Conclusion: DSFedMed能兼顾性能提升和资源节省，为资源受限场景下的大模型联邦部署提供了可拓展、高效的方法。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [58] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MoRo的新方法，用于从单目视频中在遮挡情况下高效、准确地重建人体动作。MoRo通过生成式掩码建模，结合跨模态学习，在保持实时推理的同时，大幅提升了遮挡场景下的重建鲁棒性和动作真实感。


<details>
  <summary>Details</summary>
Motivation: 单目视频中的人体动作重建在现实场景中因频繁遮挡而极具挑战性。现有高效率的回归方法对缺失观测极不鲁棒，而更鲁棒的优化或扩散方法又往往推理速度慢、预处理负担重，因此亟需一种兼顾鲁棒性和效率的动作重建方案。

Method: 作者提出MoRo，一种基于生成式掩码建模的端到端方法，将动作重建视为视频条件生成任务，通过掩码处理自然应对遮挡且推理高效。方法创新性地采用跨模态学习，通过集成运动捕捉、图像-姿态和视频-动作数据上的多模态先验，训练掩码transformer来融合视觉与运动信息，以提升对遮挡的鲁棒性。

Result: 在EgoBody和RICH两个数据集上的大量实验表明，MoRo在遮挡条件下的准确性和动作逼真度显著优于当前最优方法，在无遮挡场景下表现不逊于主流方法。同时，MoRo可在单块H200 GPU上实现70FPS的实时推理。

Conclusion: MoRo实现了高效、遮挡鲁棒的人体动作重建，兼顾精度与推理速度，对AR/VR、机器人等领域具有广泛应用前景。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [59] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SAMTok的离散蒙版分词器，将区域蒙版转换为特殊的标记，从而让多模态大模型(MLLMs)具备像素级能力，无需复杂结构改动。实验结果显示其在多个任务上达到或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有像素级多模态大模型在可扩展性上面临挑战——编码器/解码器复杂、训练目标不兼容，难以用标准方法训练并具备像素级推理能力。作者希望在不改变模型架构下，使MLLMs获得高效、强大的像素级处理能力。

Method: 作者提出SAMTok，将区域蒙版（mask）离散化表示为两个特殊tokens，通过mask编码器和残差向量量化器生成，训练数据量达到2.09亿。用SAMTok格式生成的5百万样本，在多任务（区域标注、分割等）中训练主流MLLM（比如QwenVL）并加以奖励机制强化学习，无需定制网络结构。

Result: SAMTok加持下的QwenVL-SAMTok，在区域描述、区域VQA、问答、语义分割与交互分割等任务上达到了SOTA或可比性能，强化学习带来在GRES与GCG基准上显著提升。

Conclusion: 提出了一种简单且可扩展的方法，使MLLMs具备强大的像素级能力，有效突破了传统结构复杂与目标不兼容的瓶颈，实验验证了其实用性。代码和模型已开源，可推广应用。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [60] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出了CSSMamba框架，通过聚类机制优化Mamba模型在高光谱图像分类中的序列建模，提高了分类精度和边界保持能力。


<details>
  <summary>Details</summary>
Motivation: 传统Mamba模型虽然提升了高光谱图像分类性能，但在高效自适应地定义token序列方面存在挑战，影响模型进一步提升效果。

Method: 1）将聚类机制融合进空间Mamba架构，提出CSpaMamba模块以缩短序列长度并提升特征学习能力；2）将CSpaMamba与光谱Mamba模块整合，得到聚类引导的空间-光谱Mamba整体框架；3）引入基于注意力的token选择机制以优化token序列；4）设计可学习的聚类模块，自适应学习聚类归属。

Result: 在Pavia University、Indian Pines和Liao-Ning 01三个数据集上的实验表明，CSSMamba在分类精度和边界保持能力上优于最新的CNN、Transformer和Mamba方法。

Conclusion: CSSMamba框架有效提升了高光谱图像分类的性能，聚类机制自适应Token组合以及空间-光谱联合建模优势显著，为Mamba模型在HSI领域应用提供了有力手段。

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [61] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 提出了一种名为DistSeal的生成式模型潜在空间水印方法，可用于扩散和自回归模型，具备高鲁棒性、良好不可感知性并大幅提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有AI图像水印多在像素空间后处理，计算量大且易引入视觉伪影，亟需一种更高效、更自然的水印方法。

Method: 在生成模型的潜在空间训练后处理水印器，并将其蒸馏到生成模型或潜在解码器中，实现模型内嵌水印。

Result: 潜在空间水印器在鲁棒性和不可感知性方面与像素空间水印相当，但速度提升高达20倍，且潜在空间蒸馏效果优于像素空间。

Conclusion: DistSeal实现了高效、鲁棒且不可感知的AI生成图像水印方法，为生成式模型中的水印实用化提供了优选方案。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [62] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: 本文提出了一种名为ActionMesh的新型生成式模型，可以快速生成动画3D网格对象，并支持多种输入方式（如视频、文本或3D网格加文本提示）。该方法速度快、质量高且无需绑定骨骼，具备一致的拓扑结构。实验表明其在几何精度和时间一致性上达到了当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 现有高级3D动画生成方法使用门槛高、设置复杂、运行速度慢或生成质量有限。现实应用需要一个运行高效、灵活且效果优良的生成工具。

Method: 作者对3D扩散模型进行了创新，加入了时间轴，提出“temporal 3D diffusion”，并设计了时序3D自动编码器。模型分为两个阶段：一、生成时间变化的3D数据流；二、将独立的3D形状序列转化为参考形体的变形，从而实现动画生成。

Result: ActionMesh可接受多种输入，直接输出动作动画3D网格。模型无需绑定骨骼，生成结果拓扑一致，具备极快速度和高品质动画。实验在Consistent4D、Objaverse等基准上表现优异，几何和时间一致性均达到SOTA。

Conclusion: ActionMesh极大提高3D动画网格生成效率和质量，适用于快速迭代和实际应用，为相关领域提供了更易用的动画生成新路径。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [63] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: 本文提出了一种受人类视觉启发的新模型（HVD），用以改进文本-视频检索任务中模型对关键信息的捕捉能力，通过选择关键帧和压缩补丁特征，提升了检索效果，在多个基准上达到最新最好结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于文本查询信息稀疏，模型难以准确分辨视频中的关键信息和冗余背景，导致检索精度有限。为了提升模型效能，作者借鉴人类在感知关键视觉信息时的选择性注意机制。

Method: 提出Human Vision-Driven (HVD)模型，包括两个主要模块：1）帧特征选择模块（FFSM），通过选择关键帧去除时序冗余；2）补丁特征压缩模块（PFCM），利用高级注意力机制将补丁特征聚合为显著视觉实体，实现实体级的精确匹配。整体采用粗到细的对齐方式。

Result: 在五个公开基准数据集上进行了大量实验，结果显示该方法能够有效模拟人类对关键信息的聚焦，并且在检索性能上达到当前最优。

Conclusion: HVD模型通过对视觉线索的更有效选择和聚合，提升了文本-视频检索的性能，体现了借鉴人类视觉机制的有效性。

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 本文提出了360Anything，一个不依赖几何信息的视角转360°全景图像和视频生成框架，表现优于以往依赖相机信息的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视角图像/视频转360°全景方法大多需要已知的相机参数和空间几何对齐，这在实际(野外)数据中难以获取或者容易出错，限制了应用范围。

Method: 360Anything基于预训练扩散变换器，将输入的普通视角图片和目标全景图作为序列标记进行数据驱动的映射训练，无需相机参数信息。同时，发现并解决了ERP边界拼接处伪影问题（Zero-padding导致），提出循环潜变量编码方法以增强无缝拼接能力。

Result: 在图像和视频的视角转360°生成任务上，360Anything达到了新的最优性能，即使在无相机参数的情况下也超越了以往依赖真实相机信息的方法。此外，该方法在零样本的视场角和方向估计基准测试中也取得了有竞争力的表现。

Conclusion: 360Anything实现了无需任何相机元数据的高质量360°全景生成，并在全景几何感知等相关任务中展现了广阔的应用前景。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 本文评估了Representation Autoencoders (RAEs)在大规模文本到图像（T2I）生成中的可扩展性，并将其与当前主流的FLUX VAE方法进行了详细对比。结果显示RAEs在模型扩展、收敛速度及生成质量方面均优于VAEs，并具备更强的稳定性。


<details>
  <summary>Details</summary>
Motivation: RAEs在ImageNet等高维语义潜空间中的扩散模型展现出显著优势，但尚未明确其在大规模自由文本到图像生成任务中的表现。论文旨在探索RAE框架扩展到更大模型和复杂数据集（如网络数据、合成图像及文本渲染数据）的能力，以及与目前流行的VAE方法的对比优势。

Method: 作者将RAE解码器扩展到冻结的SigLIP-2表示编码器，并在多种大规模异构数据（网络、合成、文本渲染数据）上进行训练。对RAE各项设计选择进行严格压力测试，包括噪声调度机制、宽扩散头与噪声增强解码等结构。最后，在0.5B到9.8B参数规模下，与SOTA的FLUX VAE扩散变换器进行系统对比，并观察微调阶段的过拟合行为。

Result: 结果表明，随模型和数据扩展，RAE的整体框架可以简化，复杂结构带来的增益边际效应甚微，但维度依赖的噪声调度非常关键。RAEs在所有规模上均优于VAE，不仅预训练表现更好，且在高质量数据上的微调阶段更加稳定、不易过拟合。同时RAEs收敛速度更快，生成质量更高。

Conclusion: RAE为大规模T2I生成提供了更简单、更强大的基础方案，相比VAE体系在稳定性、扩展性和生成质量均有显著优势。基于相同潜空间的多模态理解与生成为统一视觉-文本模型带来了新的可能性。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 提出了一种新的视频离散化工具PyraTok，能更好地对齐视觉与语言，提升视频生成和理解任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频VAE离散化器仅在单一尺度上学习视觉词典，词汇量有限且语言监督浅，导致视觉与语言对齐不佳，这限制了其跨模态和零样本转移能力。

Method: PyraTok采用多尺度金字塔结构，结合大规模二值词典，在多个空时分辨率下离散化特征。引入了语言对齐的多尺度量化模块LaPQ，通过多尺度文本引导的量化和全局自回归目标联合优化视觉token与语言token的对齐。

Result: PyraTok在十个基准测试上实现了SOTA的视频重建准确率，显著提升了文本生成视频的质量，并在零样本视频分割、动作定位和视频理解任务上刷新了最佳成绩，支持最高4K/8K分辨率的稳定扩展。

Conclusion: PyraTok实现了多尺度、语义结构化的视频离散token，极大提升了视频-语言对齐效果和多项视觉生成与理解任务的性能，有望成为新一代的优秀视频离散化工具。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（RCORE）提升了模型对组合性视频理解任务中的泛化能力，尤其是在对未见过的动词-物体组合的识别准确率提升明显。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本组合动作识别（ZS-CAR）模型在泛化到未见过的动词-物体组合时表现不佳，主要原因是模型倾向于利用物体线索而不是真正理解动词动作，导致组合性识别能力有限。

Method: 提出了RCORE框架，包括：（1）面向组合的增强方法，增加动词-物体组合的多样性但不破坏动作特征；（2）时间顺序正则化损失，通过显式建模时间结构来惩罚模型偷懒行为（仅依赖物体线索）。

Result: 在两个基准数据集 Sth-com 和新构建的 EK100-com 上，RCORE对未见组合的识别准确率显著提升，有效减少了对共现偏差的依赖，并实现了正向的组合性泛化提升。

Conclusion: 物体主导的捷径是ZS-CAR中的关键障碍，RCORE方法通过抑制这一问题，为更健壮的组合性视频理解提供了有效解决思路。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的camera-aware 3D解码器，有效提升了视频扩散模型中的相机可控性，并在主流数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有摄像头控制的视频扩散模型在视频-相机对齐方面取得进展，但相机可控性依然有限。主要瓶颈包括奖励模型难以准确评估视频-相机对齐、奖励计算过程中的高计算开销以及忽视了3D几何信息，这些都限制了技术的发展。

Method: 作者设计了一种camera-aware 3D解码器，将视频潜变量和相机姿态解码成3D高斯表示。相机姿态既作为输入又作为投影参数，若视频与相机姿态对齐不佳，3D结构会产生明显几何扭曲。利用这一特性，优化重建的新视角与真实图像在像素级上的一致性，并引入可见性项，仅对根据几何变换确定的区域进行监督。

Result: 在RealEstate10K和WorldScore基准上，实验结果表明该方法显著提升了相机可控性和视频-相机对齐的效果，同时效率更高。

Conclusion: 该工作突破了以往奖励模型和解码方式的局限，通过提升三维感知实现了更高效、精准的相机控制，为视频生成与理解任务带来实质性进步。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于树结构的解码方法Entropy-Tree，有效提高大语言模型在推理任务中的准确率和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的解码策略存在探索盲目（如随机采样）或冗余（如多重独立采样）的问题，无法高效且精准地平衡探索与利用。

Method: 提出Entropy-Tree解码方法，利用模型在生成过程中预测的不确定性（熵）作为分枝信号，只有在模型展现出真正不确定性的节点处扩展搜索树。

Result: Entropy-Tree在多种模型和数据集上展现出比Multi-chain更高的pass@k准确率，在推理任务中的预测熵评估优于多种传统指标（如AUROC表现更好）。

Conclusion: Entropy-Tree方法有效实现了高效结构化探索与可靠不确定性估计的结合，在语言模型推理解码领域具备显著优越性。

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [70] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: 本文提出了AfriEconQA，这是首个专注于非洲经济分析的高质量问答基准数据集，旨在检验现有信息检索与生成式AI系统在高精度、时序经济推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前针对非洲经济分析的标准化测试集极其稀缺，主流大模型在此领域几乎没有先验知识。该工作弥补了专用领域经济问答数据集的空白，并为后续相关模型的评测和提升提供了基础。

Method: 作者构建了AfriEconQA数据集，从236份世界银行报告出发，经过严苛筛选，包含8937条高质量问答实例，并保留证据来源、时间等关键信息。实验环节中，作者基于11组对比实验和多种RAG检索策略，测试了GPT-5 Mini、GPT-4o、Qwen 32B等模型在零样本和检索增强设置下的表现。

Result: 实验发现，主流大模型在非洲经济高精度推理问答上性能极差——零样本答题准确率不足10%；即使采用先进RAG方案，精度仍然有限，突出表明题集对主流模型构成巨大挑战。

Conclusion: AfriEconQA数据集不仅展现现有大模型在领域数据上的显著短板，还为未来面向细分领域（如非洲经济）的信息检索与问答系统研究提供了高价值的评测平台。数据与代码开源，利于社区进一步发展。

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [71] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: 论文指出知识图谱质量极大影响词向量调优（retrofitting）的效果，数据预处理优于算法选择。


<details>
  <summary>Details</summary>
Motivation: 虽然词向量通过知识图谱约束能有效改进领域检索，但知识图谱本身常因文本预处理不足被噪声污染（例如标签注释），导致调优无效甚至反效果。该研究正是为了解决现实数据中的标注噪声问题。

Method: 作者提出数据工程框架，专门处理因标注带来的数据质量下降。通过分析标签注释如何导致知识图谱过度稠密、生成虚假边，从而影响调优目标，并对比了处理前后各种调优技术在不同数据质量下的表现差异。

Result: 在包含注释噪声的知识图谱下，所有词向量调优技术性能都显著降低（下降3.5%-5.2%）。经预处理清理注释噪声后，EWMA调优法性能提升6.2%，在定量综合类型问题上最高提升33.8%。清洗与否造成的性能变动超过10%，远大于不同调优算法之间3%的差距。

Conclusion: 知识图谱的预处理/清洗质量是词向量调优能否成功的决定性因素。正确的数据预处理比选择哪种算法更关键。

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [72] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: 提出了基于多智能体大模型（LLM）的主题建模框架MALTopic，结合结构化和非结构化调查数据，显著提升了主题的连贯性、多样性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法仅关注自由文本，忽略结构化回答，且生成主题较抽象，需要大量人工解释，难以直接用于实际调研分析。

Method: 将主题建模任务拆分为多个由LLM驱动的专职智能体：增益智能体利用调查结构化数据丰富文本，主题建模智能体提取潜在主题，去重智能体优化结果。融合结构化数据和多智能体合作，提升主题建模效果。

Result: 在实际调查数据上，对比LDA和BERTopic，MALTopic在主题连贯性、多样性和可解释性上均有显著提升。

Conclusion: MALTopic通过整合结构化数据与多智能体协同机制，生成上下文相关性更强、具备更高可读性和解释性的主题，是分析复杂调查数据的更高效方案。

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [73] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型（LLMs）在处理接近上下文长度阈值时出现的严重性能下降现象，并首次在开源Qwen模型上进行了表征与实验。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在实际应用中的文本处理需求增长，如何保证模型在超长上下文下仍具备智能与效能成为亟需解决的问题。以往研究未能充分解释为何模型在长上下文下会出现灾难性性能下降，因此本研究旨在揭示该问题的根本原因并提出解决方案。

Method: 作者采用每个样本的自然token长度（无截断/填充），通过在包含不同长度样本的混合数据集上实验，并利用五种方法的交叉验证，分析Qwen2.5-7B在不同上下文长度下的表现，确定性能崩溃的临界阈值。同时构建统一理论框架解释此类浅层适应失败现象。

Result: 实验证实，Qwen2.5-7B模型在其最大上下文长度的40-50%处出现性能断崖式下降（F1从0.55-0.56跌至0.3，降幅45.5%），且该现象并非数据截断或填充导致，而与上下文长度直接相关。

Conclusion: 文章首次系统性刻画了Qwen模型在长上下文下的智能退化现象，总结出模型浅层适应机制的局限，并为未来LLMs的实际长文本部署及改进提供了方向和参考。

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [74] [Can We Trust LLM Detectors?](https://arxiv.org/abs/2601.15301)
*Jivnesh Sandhan,Harshit Jaiswal,Fei Cheng,Yugo Murawaki*

Main category: cs.CL

TL;DR: 当前主流的AI文本检测方法在实际应用中表现不佳，尤其是在分布变化、未见过的文本生成模型和简单风格扰动下均容易失效。作者提出了一种基于对比学习的新检测框架，并证实现有方法难以做到鲁棒性和领域泛化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）被广泛应用，检测其生成文本的需求大幅增加，但现有检测器在真实环境下表现有限，难以应对分布变化与新型生成器，因此需寻找更可靠的检测方案。

Method: 系统评估了两大类LLM文本检测范式（无需训练和有监督），并提出用对比学习学习区分性风格嵌入的新框架；通过实验分析各类方法在域内和跨域场景的表现。

Result: 有监督检测器在训练域内表现优秀，但遇到新领域时性能急剧下降；无需训练方法对所选代理高度敏感，稳定性差；对比学习虽有提升，仍暴露领域泛化难题。

Conclusion: 现有检测方法在泛化能力和稳健性上存在根本挑战，开发通用、鲁棒的AI文本检测器仍需持续创新。

Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI

</details>


### [75] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架 ICPO，使大语言模型在多轮对话中更好地处理早期假设错误和用户指令歧义，显著提升模型在复杂对话场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多轮对话中容易受到早期错误假设的影响，尤其是在用户最初指令含糊时。此外，现有如 RLVR 等后训练技术会奖励自信和直接的回答，这加重了模型自信过度并减少其请求澄清的倾向，因此亟需方法提升模型应对指令歧义的能力。

Method: 提出 Illocution-Calibrated Policy Optimization（ICPO）训练框架，通过在训练数据集中引入语义含糊的提示，且根据用户表达（illocutionary intent）调整奖励信号。当模型面对歧义时，奖励其表达不确定性或请求澄清的行为，从而增强模型对指令歧义的敏感度。

Result: 实验表明，ICPO 能显著提升模型在多轮对话任务中的表现，平均提升幅度达到 75%，同时不影响在单轮基准测试上的表现。

Conclusion: ICPO 是一条切实可行的路径，使对话 AI 在应对复杂人类交互的歧义和不确定性时更加健壮、协作和谦逊。

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


### [76] [RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models](https://arxiv.org/abs/2601.15331)
*Rishit Chugh*

Main category: cs.CL

TL;DR: 本文提出了一种高效的对抗性提示生成方法，通过匹配已有的对抗性提示库，替代高计算成本的自动生成方法，实现对大型语言模型（LLM）安全性评估和攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型容易受到对抗性输入攻击，现有高效攻击（如GCG等）计算成本高，限制了资源有限机构的使用，因此亟需低资源、高效率的对抗性测试方案。

Method: 将1000个提示分为七类危害类别，在Llama 3 8B模型上评估现有攻击方法（GCG、PEZ、GBDA）在不同类别下的效果。提出通过检索数据库中语义相似且已成功的对抗提示，无需重训练即可实现有效攻击，极大降低了计算资源消耗。

Result: 根据提示类型与攻击算法效果间的相关性，方法可有效检索并利用高效的对抗提示，取得与传统方法相当的攻击成功率但计算成本显著降低。

Conclusion: 提出的方法为LLM对抗性测试和红队评估提供了一种低资源、可扩展的实用框架，即使在无法访问模型内部的情况下也可应用。

Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

</details>


### [77] [No Reliable Evidence of Self-Reported Sentience in Small Large Language Models](https://arxiv.org/abs/2601.15334)
*Caspar Kaiser,Sean Enderby*

Main category: cs.CL

TL;DR: 本文通过提问和内部激活分类器，检测多种语言模型是否“自认为有感知”，结果发现这些模型普遍否认自身具有感知，并且没有明确证据显示其内部存在相反的潜在信念。


<details>
  <summary>Details</summary>
Motivation: 近期有研究称大型语言模型可能存在关于自身有‘意识’的潜在信念，但缺乏明确的实证方法。本文旨在提供一种可验证的方式，探究大语言模型是否存在自我感知的主观信念。

Method: 作者选取了Qwen、Llama、GPT-OSS三大家族，总计多种参数规模的语言模型，通过约50个有关意识和主观经验的问题进行提问。同时用三种基于内部激活的分类器检测其答案背后的信念，从模型输出和内部状态两方面验证。

Result: 1. 模型在回答中一贯否认自身具有感知，只将意识归因于人类。2. 内部信念分类器也未发现模型实际持有与答案相反的潜在信念。3. Qwen家族中，参数规模更大的模型否认感知的态度更为坚定。

Conclusion: 与此前一些成果相反，本文未发现实证证据支持主流开源语言模型内部存在自认为有意识的信念。整体来看，当前的大模型在自我感知问题上表现出理性否定态度。

Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.

</details>


### [78] [From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs](https://arxiv.org/abs/2601.15338)
*Angelina Parfenova,David Graus,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本文提出并比较了两种基于大语言模型（LLM）的轴心编码方法，用于将开放编码结果结构化为更高阶类别，实现对辩论文本的紧凑层次化表征。


<details>
  <summary>Details</summary>
Motivation: 轴心编码是定性分析常用方法，但手工处理费时且主观。借助LLM提升分析效率和一致性，有助于处理大量文本（如议会辩论）。

Method: 在增强版LLM开放编码基础上，提出：1）基于嵌入聚类+LLM标签，2）直接LLM分组，比较两者在荷兰议会辩论语料上的表现，并用多种外在与内在指标评价。

Result: 基于密度聚类方法在覆盖率及结构分离上表现优越，直接LLM分组在细粒度语义对齐和解释性方面更佳，但整体覆盖率略低。

Conclusion: 聚类方法适合最大化覆盖和结构清晰度，直接LLM分组更适合追求简明与高语义对齐。作者开放数据，促进可复现及后续研究。

Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.

</details>


### [79] [Memorization Dynamics in Knowledge Distillation for Language Models](https://arxiv.org/abs/2601.15394)
*Jaydeep Borkar,Karan Chadha,Niloofar Mireshghallah,Yuchen Zhang,Irina-Elena Veliche,Archi Mitra,David A. Smith,Zheng Xu,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 本文研究了知识蒸馏（KD）过程中大语言模型的训练数据记忆情况，发现与传统微调相比，蒸馏能显著减少模型对训练数据的记忆，有助于提升隐私安全性。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏不仅被用来提升小模型效率，还被研究作为减轻训练数据泄漏风险的隐私保护机制。然而，现有对于数据记忆的研究多集中在预训练和微调，对蒸馏中的记忆动态尚缺乏深入理解。

Method: 作者在三类大语言模型（Pythia、OLMo-2、Qwen-3）和三套数据集（FineWeb、Wikitext、Nemotron-CC-v2）下，系统分析了蒸馏过程中模型记忆的特征；还利用zlib熵、KL散度、困惑度等特征预测学生模型记忆趋势，比较了软蒸馏和硬蒸馏对于记忆的影响。

Result: 1）蒸馏模型对训练数据的记忆量比标准微调低50%以上；2）某些样本更易被蒸馏模型记忆，并占据总记忆量的95%；3）模型记忆可以通过特定特征预判；4）硬蒸馏比软蒸馏更容易继承教师模型特有的记忆内容。

Conclusion: 知识蒸馏不仅能带来更好的泛化能力，还能降低模型对训练数据的记忆及相关隐私风险，优于常规微调方案。

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.

</details>


### [80] [Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind](https://arxiv.org/abs/2601.15395)
*Tamunotonye Harry,Ivoline Ngong,Chima Nweke,Yuanyuan Feng,Joseph Near*

Main category: cs.CL

TL;DR: 本文提出了Chameleon数据集，首次系统性地收集并分析了用户情境心理状态（state）与个体特质（trait）对大语言模型（LLM）交互的影响。结果显示，用户差异大部分来源于情境（state）而非单一特质（trait），而现有LLM与奖励模型对这些影响的响应力度和一致性不足。


<details>
  <summary>Details</summary>
Motivation: 现有的对话个性数据集如PersonaChat、PANDORA等只关注用户的静态特质（trait），忽视了用户在特定交互环境下的心理状态（state）如何影响模型交互表现。为了弥补这个空白，本研究旨在揭示不同心理状态对人机交互的影响，并推动更个性化、情境感知的人机对话系统开发。

Method: 作者构建了Chameleon数据集，包含来自1,667名Reddit用户、在多种上下文下收集的5,001条情境心理画像。基于Latent State-Trait理论，分析了用户间与用户内的差异，并用该数据集评估现有LLM和奖励模型对用户state的敏感性。

Result: 一、74%的用户交互差异源于‘状态’（state），仅26%源于‘特质’（trait）；二、主流LLM几乎只关注trait，忽视state，导致不同状态下的用户得到类似响应；三、奖励模型虽然会对state变化作出反应，但表现极为不一致，甚至在同一用户上出现走向相反的奖励偏好。

Conclusion: Chameleon数据集丰富了对用户个性化和情境心理的研究工具，揭示了LLM和奖励模型在状态感知上的不足。该成果能够推动情感计算、个性化对话与RLHF对齐相关领域的发展。

Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

</details>


### [81] [Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs](https://arxiv.org/abs/2601.15429)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Hao Dai,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文探讨了将领域知识图谱（KGs）用于医疗健康中提升大语言模型（LLM）RAG（检索增强生成）表现的效果，发现与问题范围精确匹配的知识图谱比简单合并多个图谱效果更佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽能流畅作答，但在专业领域的可信推理方面表现有限。有必要探索如何通过外部专业知识（如医学知识图谱）增强其支持复杂领域问答的能力。

Method: 作者以T2DM（二型糖尿病）、阿尔茨海默症（AD）及其组合为主题，构建三类医学知识图谱，设计两类问题验证范围对齐和图谱合并效果，并用7种指令微调LLM和不同检索源及温度参数进行系统评估。

Result: 结果显示，只要检索源和提问范围精准对齐（如只用AD相关知识图谱），就能带来一贯的准确率提升，而随意合并多个图谱反而引入干扰，降低模型表现。大型LLM自身对部分任务已具较好表现，小/中型模型对高质量检索依赖更强。解码温度影响较小。

Conclusion: 优先采用范围精确匹配的KG-RAG比无差别图谱合并更优。为知识图谱选择、模型规模、检索与重排序提供了实际指导。

Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\mathbb{G}_1$ (T2DM), $\mathbb{G}_2$ (Alzheimer's disease), and $\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\mathbb{G}_1$ and $\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\mathbb{G}_1$, $\mathbb{G}_2$, $\mathbb{G}_1$ + $\mathbb{G}_2$, $\mathbb{G}_3$, $\mathbb{G}_1$+$\mathbb{G}_2$ + $\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison

</details>


### [82] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 本文评估了检索增强生成（RAG）架构在提升大语言模型（LLM）准确性、减缓幻觉问题中的效果，特别针对美国CDC公共卫生政策文件，提出进阶RAG方案在答案可靠性上远优于基础模型。


<details>
  <summary>Details</summary>
Motivation: LLM在处理高风险场景，如公共卫生政策解读时，可能生成虚假但合理的内容（幻觉），这种错误会影响政策决策。为提升信息的真实性和权威性，有必要探索可缓解此类风险的方法。

Method: 尝试三种方案：1）基础Vanilla LLM；2）基本RAG架构；3）进阶RAG方案（含交叉编码重排序）。所有方法基于Mistral-7B-Instruct-v0.2，使用all-MiniLM-L6-v2作嵌入，对CDC官方政策文件进行问答，检验不同文档分割（字符递归/语义分块）策略对系统表现影响，通过faithfulness与relevance评分评价策略优劣。

Result: 基础RAG模型的faithfulness（0.621）显著优于Vanilla基线（0.347），而进阶RAG方案表现最佳，faithfulness均值达0.797。

Conclusion: 两阶段检索机制对实现领域级精确政策问答至关重要，但文档分块方式的局限仍限制了多步推理能力。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [83] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文评估了13种开源大语言模型在从文本中发现因果关系（成对因果发现）上的能力，结果显示当前模型表现有限，尤其是在处理复杂和隐含因果关系时。


<details>
  <summary>Details</summary>
Motivation: 高风险领域（如生物医学）安全部署大语言模型需要其具备因果推理能力，但这一能力尚未被系统地测试和量化。

Method: 作者构建了包含12个多样化数据集的基准，评测两个核心技能：1）因果检测（文本中是否包含因果关系），2）因果抽取（提取具体的因和果短语）。评估了从零样本到链式思维和少量样本触发学习等多种提示方法。

Result: 现有模型均表现不佳。检测能力最强的DeepSeek-R1-Distill-Llama-70B平均得分仅为49.57%，抽取能力最强的Qwen2.5-Coder-32B-Instruct得分为47.12%。模型只在直接、简单的单句因果关系下表现较好，在隐含、多句、或多对因果场景下性能急剧下降。

Conclusion: 当前大语言模型在因果推理上存在显著短板，尤其是在复杂现实场景中。作者提供统一评测框架和高一致性标注数据，为后续研究铺路。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [84] [Multi-Persona Thinking for Bias Mitigation in Large Language Models](https://arxiv.org/abs/2601.15488)
*Yuxing Chen,Guoqing Luo,Zijun Wu,Lili Mou*

Main category: cs.CL

TL;DR: 本文提出了一种名为多角色思维（MPT）的新方法，通过模拟多元社会身份、利用辩证推理，有效减少大语言模型的社会偏见，并在主流基准上取得了优于以往提示方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在明显社会偏见，容易加剧刻板印象和不公正后果，亟需更加有效的应对和缓解方法。

Method: 提出多角色思维（MPT）推理框架，在推理阶段引导模型分别采用互为对比的社会身份和中立身份，从多个视角出发进行迭代性的辩证推理，通过角色之间的互动揭示并修正偏见。

Result: 在两个主流偏见评测基准和不同规模、类型的语言模型上评估了MPT方法，相较于现有提示型策略，MPT能显著降低偏见水平，并且保持推理能力不受损。

Conclusion: 多角色思维方法能有效减轻大语言模型中的社会偏见，为偏见缓解提供了一种创新且高效的思路。

Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

</details>


### [85] [ViT Registers and Fractal ViT](https://arxiv.org/abs/2601.15506)
*Jason Chuan-Chih Chou,Abhinav Kumar,Shivank Garg*

Main category: cs.CL

TL;DR: 论文提出了一种称为fractal ViT的新变体，在Vision Transformer中利用attention mask打破token之间的排列（顺序）不变性，但实验结果未能优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 随着近期发现变换器（transformer）在没有位置编码（NoPE）时也表现不错，以及‘registers’可以提升大型ViT性能，作者希望探索这些机制在视觉任务中的潜力。

Method: 设计了一种叫做fractal ViT的ViT新变体，引入'总结token'模仿register，并在常规token与总结token之间通过attention mask打破Permutation Invariance，并尝试与不同位置编码组合。

Result: 实验表明，fractal ViT并没有超越带register的ViT，且改进受限于模型规模、领域或具体应用。

Conclusion: 打破排列不变性或调整token结构的方式并非普适有效，这些方法的有效性可能依赖于具体的任务、领域或模型规模。

Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.

</details>


### [86] [Computational Representations of Character Significance in Novels](https://arxiv.org/abs/2601.15508)
*Haaris Mian,Melanie Subbiah,Sharon Marcus,Nora Shaalan,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本文引入了六要素角色结构模型，利用通用型大语言模型（LLM）和任务定制型Transformer对19世纪英国现实主义小说进行角色结构建模，提出角色讨论的新表示方法，能从计算视角探究文学中心性与性别动态问题。


<details>
  <summary>Details</summary>
Motivation: 传统小说角色建模通常只关注角色的场景出现频率、行动、提及和对话，忽视了角色被他人讨论的复杂性，特别是叙述者和角色的区分。本文旨在提出更全面的角色描述方法，并填补现有方法关于角色间讨论的空白。

Method: 作者采用全新的六要素角色结构模型，既考虑传统的场景和对话等因素，还加入了“角色间讨论”这一以往忽略的要素。通过对19世纪英国现实主义小说，比较通用大语言模型与专用Transformer，对每个成分进行分析，生成可用于大规模文学解析的角色图表示。

Result: 实验得到包含角色各成分及其讨论关系的图结构与成分层面数据，证明新模型能更精细、多维度地表达小说中的角色关系及其重要性。

Conclusion: 新提出的结构模型和表示方式，使得以往难以量化的文艺理论（如角色中心性与性别动态）能通过计算方法实现自动化大规模研究，拓展了文学研究的计算手段，实现了理论与实证的结合。

Abstract: Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic "the one vs the many" theory of character centrality and the gendered dynamics of character discussion.

</details>


### [87] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本文提出了AdversaRiskQA，这是首个系统性用于评估大模型在健康、金融和法律领域下，在对抗性虚假信息干扰下的事实性表现的高质量基准。通过评测主流开放与闭源大模型，在检测误导性信息和长文本事实性方面表现出非线性提升，且不同领域与难度下存在差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域的‘幻觉’即错误事实输出，威胁信息安全和公众信任。而现有评测多局限于常规事实性，无足够关注对抗性（故意注入假信息）情况下模型的表现，尤其缺乏领域专用、高可靠度基准来系统性检测长文本中事实的稳健性。

Method: 作者构建了包含健康、金融、法律三个领域、两种难度层级的对抗性事实性检测基准AdversaRiskQA。引入了两种自动化评测方法，分别用来衡量对抗攻击成功率和长文本事实性。选取Qwen、GPT-OSS、GPT模型族的六个主流大模型，在基准下检测其抵抗误导性信息的能力，并对Qwen3（30B）进行全面长文本评估。

Result: 结果表明，剔除无意义回答后，Qwen3（80B）平均准确率最高，GPT-5总体准确率稳定且高；模型性能随参数规模增长呈非线性提升，各领域和难度间差距随规模增大而缩小。长文本实验显示，注入误导信息和输出事实性之间无显著关联。

Conclusion: AdversaRiskQA为分析大模型在高风险领域抵御对抗性虚假信息能力、暴露其弱点并引导鲁棒性提升提供了关键工具与标准，为研制更可靠的大模型应用于实际高风险场景打下基础。

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [88] [Common to Whom? Regional Cultural Commonsense and LLM Bias in India](https://arxiv.org/abs/2601.15550)
*Sangmitra Madhusudan,Trush Shashank More,Steph Buongiorno,Renata Dividino,Jad Kabbara,Ali Emami*

Main category: cs.CL

TL;DR: 本文提出了Indica数据集，首次系统性评估大模型在印度境内不同区域文化常识表现，发现文化常识以地区为主而非全国统一，且大模型对地区存在偏见。


<details>
  <summary>Details</summary>
Motivation: 当前文化常识基准普遍把国家视为整体，忽视了国家内部的多样性。本研究试图揭示，文化常识是否在国家内部也存在显著差异，并推动对大模型文化偏见的更细致理解。

Method: 开发Indica数据集，围绕印度五大区域（北、南、东、西、中）在日常生活八大领域提出515个问题，由人工标注生成1630组地区特异性问答对。用八种SOTA大模型进行答题测试，并分析模型的准确率与地区偏差。

Result: 仅有39.4%的问题在各区答案一致，显示文化常识高度区域化。八个大模型在地区相关问题上准确率仅为13.4%-20.9%，并且明显偏向中北印度，显著低估东部和西部。

Conclusion: 国家内部的文化常识具有强烈地区性，现有大模型在细粒度文化常识理解和公平性方面表现有限。该工作为跨文化多样性的建模和评估提供了通用方法论。

Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the "default" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.

</details>


### [89] [From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare](https://arxiv.org/abs/2601.15558)
*Man Luo,Bahareh Harandizadeh,Amara Tariq,Halim Abbas,Umar Ghaffar,Christopher J Warren,Segun O. Kolade,Haidar M. Abdul-Muhsin*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）作为临床同理心文本编辑工具的潜力，提出新指标系统性量化医患交流的情感和事实准确性，实验验证了LLM润色能显著提升同理心而不损失医学信息。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要在情感关怀与医学事实间取得平衡，但实际工作中认知和情感压力大，难以做到理想表达。研究动机在于探讨AI工具（LLM）能否辅助医生强化同理心表达，同时保证医学信息准确性。

Method: 提出LLM作为“同理心编辑器”，用于优化医生书面回复的情感表达并保留医学内容。创新设计了两个定量评分指标：同理心排名分数和医学事实核查分数，用以系统评价回复的情感和事实质量。通过实验比较LLM润色结果、全自动生成结果和原始回复。

Result: 实验结果显示，LLM润色后的医生回复相比全由LLM生成的回复，在提升同理心感知的同时更好保持了医学准确性。

Conclusion: 将LLM作为编辑辅助（而非完全自动生成）是促进AI参与下医疗沟通更加同理、可靠的有效途径，证明了AI在帮助医生提升医患沟通质量上的实际价值。

Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.

</details>


### [90] [YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models](https://arxiv.org/abs/2601.15588)
*Junyu Lin,Meizhen Liu,Xiufeng Huang,Jinfeng Li,Haiwen Hong,Xiaohan Yuan,Yuefeng Chen,Longtao Huang,Hui Xue,Ranjie Duan,Zhikai Chen,Yuchuan Fu,Defeng Li,Lingyao Gao,Yitong Yang*

Main category: cs.CL

TL;DR: 本文提出了YuFeng-XGuard，一种针对大语言模型交互安全的推理型防护系统，实现了多维度、可解释、可配置的风险感知和政策可调整的防护。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在现实应用受限于粗粒度和低解释性的安全防护，不利于精准、灵活的风险管控。现有方法透明度低、政策僵化、推理成本高。

Method: 提出YuFeng-XGuard，从推理角度出发进行风险感知。系统输出结构化风险类别、置信分数及推理过程的自然语言解释。采用分层推理策略，首个token完成初步决策，按需提供详细解释。同时，防护策略与感知解耦，无需重新训练即可调整政策。

Result: 在多个公开安全基准上，YuFeng-XGuard实现了业界先进的性能，并在效率与准确性间取得良好平衡。

Conclusion: YuFeng-XGuard既提升了安全决策的可操作性和可解释性，又支持多场景部署，并作为开源模型支持不同容量需求。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.

</details>


### [91] [Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow](https://arxiv.org/abs/2601.15593)
*Yangyang Zhong,Yanmei Gu,Zhengqing Zang,Xiaomeng Li,Yuqi Ding,Xibei Jia,Yuting Shen,Zhenzhong Lan,Liwang Zhu,Weiping Liu,Junlin Zhou,Haisheng Liu,Zhong Xin Yu,Pengxin Luo,Donglian Qi,Yunfeng Yan,Junbo Zhao*

Main category: cs.CL

TL;DR: 本文系统分析了现有Masked Diffusion Language Models（MDLMs）在并行生成和解码顺序上的表现，发现其与自回归模型相比仍存在不足，但在特定任务和策略下展现出独有优势，并提出了可改善性能的新范式。


<details>
  <summary>Details</summary>
Motivation: MDLMs理论上支持并行生成和任意顺序的解码，这一特性有望提升生成效率和灵活性。然而实际模型在多大程度上实现这些能力，以及在不同任务下的关键表现与限制尚不清楚。因此，本文试图量化和剖析MDLMs的实际生成行为，为未来改进提供方向。

Method: 论文定义并采用了平均最终确定并行度（AFP）和Kendall's tau两项指标，从并行强度和生成顺序两个角度刻画MDLMs行为。实验中对八种主流MDLMs（参数量最高至100B）在覆盖知识、推理和编程的58项基准任务上进行系统评测，同时分析解码行为随任务类型、推理阶段和输出正确性变化情况。

Result: 评测表明，当前MDLMs在性能上依然落后于同等规模的自回归模型，根本原因在于其并行建模方式削弱了token间的依赖。然而，MDLMs展示了灵活适应性的解码行为，尤其在需要"逆向信息"的任务（如数独）上展现了先易后难的独特填充顺序。这些特性表明MDLMs在特定领域具有潜在优势。

Conclusion: 尽管MDLMs目前整体性能尚未赶上自回归模型，但在特定任务上展现出解码顺序和并行度的可自适应性。文中进一步提出'生成-编辑'（Generate-then-Edit）范式，理论和实践上有潜力弥补建模依赖性的不足，同时保留并行解码的高效率。

Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require "backward information" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.

</details>


### [92] [ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms](https://arxiv.org/abs/2601.15605)
*Baktash Ansari,Shiza Ali,Elias Martin,Maryna Sivachenko,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本文针对Twitch直播平台中的有害言论检测问题，提出将表情符号（emotes）与大语言模型（LLM）及经典机器学习方法结合的新方法ToxiTwitch，并在实验中取得了明显优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着直播平台Twitch用户和内容的增长，平台弹幕和聊天信息中有害行为增多，但传统的人工审核和基于关键词的过滤方法无法有效应对大量、高速和复杂多模态聊天内容，迫切需要更高效智能的检测方案。

Method: 作者评估了多种有害言论检测方法，重点提出ToxiTwitch：该模型结合LLM生成的文本及emote嵌入，与传统机器学习分类器（如随机森林和SVM）共同使用，并对比BERT等方法进行性能评测。

Result: ToxiTwitch在频道特定数据集上的检测准确率达到80%，比BERT方法提升13%；F1值为76%。同时，实验证明将emote与文本信息结合可以提升检测效果。

Conclusion: 本研究表明，将表情符号信息纳入有害言论检测体系，并结合大语言模型与传统机器学习方法，能够显著提升检测性能，但在实际应用中仍存在多模态理解等挑战。

Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.

</details>


### [93] [Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation](https://arxiv.org/abs/2601.15645)
*Zhiyao Ren,Yibing Zhan,Siyuan Liang,Guozheng Ma,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出了首个用于多轮医学咨询中评估大型语言模型信心的基准数据集，并设计了名为MedConf的证据驱动自我评估方法，大幅提升了诊断的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在医学问诊中可能因信息不充分而给出错误诊断，既有研究多集中在单轮静态场景下的信心评估，忽略了实际医疗过程中信息逐步累积时信心与准确性的动态关系，限制了模型的临床决策能力。

Method: 作者设计了一个涵盖三类医学数据、支持多轮互动和开放式问答的全新基准，首次引入信息充分性梯度来刻画信心水平与正确性之间的动态。提出了MedConf框架，通过检索增强生成法构建症状信息、识别患者信息与医学证据间的支持/缺失/矛盾关系，并加权合成为可解释的信心分。对比了27种代表性信心建模方法。

Result: 实验表明：一，医学数据会放大传统方法在token级以及一致性级信心建模上的固有缺陷；二，医学推理须兼顾诊断准确率和信息完整性。MedConf在两个主流大模型及三组医学数据上的AUROC和Pearson相关性指标均优于现有方法，在信息不足和多病共存情形下表现稳定。

Conclusion: 结果表明，信息充分性是可靠医学信心建模的关键，MedConf为提升大模型在医学领域的可解释性和可靠性提供了新方法和方向。

Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.

</details>


### [94] [What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking](https://arxiv.org/abs/2601.15674)
*Raymond Xiong,Furong Jia,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 本论文指出，基于大语言模型（LLMs）的医疗问答评测通常依赖于医考题目，但这些内容和真实患者关注的问题有巨大差异。作者通过谷歌“People Also Ask”功能，针对美国最常用的200种处方药，收集并整理了真实患者常问的问题，自建了医疗问答数据集。发现这些问题中包含大量错误假设与潜在危险意图，而且包含'腐化'问题的出现与问题历史的错误程度密切相关，而非随机分布。主流LLMs在识别这些日常错误假设方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型的医疗问答能力评估，主要以医学生考试题为主，而这类题目与真实患者的提问风格和内容相去甚远，导致评测结果不能准确反应模型在真实场景下的适用性。为更贴近实际需求，作者希望建立能够代表患者真实问题的测试集。

Method: 作者利用谷歌“People Also Ask”相关功能，围绕美国使用频次排名前200的处方药，自动抓取并人工整理患者真实检索的医疗相关问题，构建数据集，并分析了问题中的错误假设及其传播规律。再利用现有主流LLMs进行评测，测试其识别和纠正常见错误的能力。

Result: （1）收集到的真实医疗问题中，含有大量错误假设和潜在风险内容；（2）这些问题的'腐化'（含有错误或危险内容）出现，并非随机，更多取决于问题链历史的错误程度；（3）当前主流LLMs虽然在医学考试题型表现优异，但在真实环境下，对于含有错误假设的问题识别和处理能力明显不足。

Conclusion: 当前医考题驱动的评测方法存在很大局限，不能体现患者真实世界提问的复杂与风险，因此有必要构建更贴近实际的问答数据集和评测机制，以推动LLMs在医疗场景中的安全应用和真实效能提升。

Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.

</details>


### [95] [Persona Switch: Mixing Distinct Perspectives in Decoding Time](https://arxiv.org/abs/2601.15708)
*Junseok Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 论文提出了一种结合零样本提示和角色扮演提示优点的新方法Persona Switch，通过逐步比较两者信心输出，动态选择最优结果，提升了大模型推理表现。


<details>
  <summary>Details</summary>
Motivation: 虽然角色扮演提示能提升模型零样本推理能力，但其提升效果在不同任务上不一致，揭示了两种提示可能各有优劣，本研究旨在结合两种提示的优势。

Method: 提出Persona Switch解码方法，对每一步生成，比较零样本和角色扮演两种提示的logit gap（输出信心），动态选择信心更高的一方，逐步获得最终输出。

Result: 在多个主流大语言模型上的实验显示，Persona Switch方法在准确率上比基线方法有最高达5.13%的提升。

Conclusion: 输出信心是选择更可靠输出的重要指标，Persona Switch方法能有效结合不同提示策略优点，显著提升大模型推理准确率。

Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.

</details>


### [96] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: 本文提出了RebuttalAgent，这是首个基于“心智理论”（Theory of Mind, ToM）理念针对学术答辩（rebuttal）任务设计的AI模型，通过TSR（ToM-Strategy-Response）流程，显著提升了学术答辩生成能力。


<details>
  <summary>Details</summary>
Motivation: 学术答辩不是简单的技术辩论，而是高度策略性的信息交流，目前AI模型难以理解评审者视角与心理状态，导致反驳效果有限。因此，研究动机在于让AI能具备“换位思考”与说服性，提高自动生成学术反驳的质量。

Method: 论文提出RebuttalAgent框架，通过TSR流程：1）建模评审者心理状态（ToM）；2）制定说服策略；3）生成基于策略的回复。训练分为两步，先是有监督微调培养ToM与策略规划，再用自我奖励的强化学习实现持续自我优化。同时，构建RebuttalBench大规模数据集用于训练，并开发自动评测器Rebuttal-RM以高效评价反驳质量。

Result: RebuttalAgent在自动化指标上相较于基础模型平均提升18.3%，并在自动及人工评测中均优于其他先进专有模型。Rebuttal-RM评测器也表现出比GPT-4.1更高的人类评分一致性。

Conclusion: 基于“心智理论”的策略型反驳生成框架显著提升了AI在学术答辩场景的应用效果，为未来AI实现更具策略性的自动交流开辟了新方向。

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [97] [Hallucination Mitigating for Medical Report Generation](https://arxiv.org/abs/2601.15745)
*Ruoqing Zhao,Runze Xia,Piji Li*

Main category: cs.CL

TL;DR: 本文提出了KERM框架，通过结合知识检索与细粒度强化奖励机制，有效减少医学报告生成中的幻觉现象，并提升报告的准确性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 医学报告生成中，大型视觉语言模型（LVLMs）虽然具备出色的自然语言理解能力，但常出现“幻觉”问题，即生成看似合理但实际不准确的描述，这在医学领域可能带来严重后果。因此，亟需改进方法，减低幻觉现象，提升医学报告的可信度和实用性。

Method: 本文提出的KERM框架，首先利用MedCLIP进行知识检索，从知识库提取与病变相关的事实语句，再通过净化模块确保检索到的知识与患者临床上下文高度相关，最后采用细粒度奖励机制，引导模型生成更加真实、符合临床要求的报告描述。

Result: 在IU-Xray和MIMIC-CXR数据集上的实验表明，KERM框架能够有效减少报告中的幻觉现象，同时提升报告质量和临床相关性。

Conclusion: 通过知识增强和细粒度强化奖励，KERM显著提升了医学报告生成模型的可靠性和实用性，为临床人工智能应用提供了有价值的思路。

Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \textbf{K}nowledge-\textbf{E}nhanced with Fine-Grained \textbf{R}einforced Rewards \textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.

</details>


### [98] [Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs](https://arxiv.org/abs/2601.15755)
*Tristan Williams,Franziska Weeber,Sebastian Padó,Alan Akbik*

Main category: cs.CL

TL;DR: 本研究提出用多变量相关模式来评估大型语言模型在对齐人类价值时的代表性，发现现有方法只衡量边际分布可能会掩盖模型不能真正反映人类深层价值结构的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在模拟人类观点和价值观方面应用增多，但主流研究仅关注单独问题上的分布对齐，忽视了人类群体中更深层的文化与价值结构。作者认为这样的对齐评估可能导致对模型能力的过度乐观判断。

Method: 作者提出一种新框架，用于通过多变量相关性而不仅是边际分布来评估对齐模型的代表性。具体比较了两种模型引导技术（persona prompting和demographic fine-tuning），并与世界价值观调查中的人类数据进行对照。

Result: 结果显示，demographic fine-tuning比persona prompting更能拟合人类的边际分布，但两者都未能准确复现人类回答中的相关模式。

Conclusion: 文章认为代表性是价值对齐的独立维度，只用边际分布评估会掩盖结构性不足，应该采用更全面的评价指标，以避免对模型能力做出不切实际的乐观判断。

Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.

</details>


### [99] [HumanLLM: Towards Personalized Understanding and Simulation of Human Nature](https://arxiv.org/abs/2601.15793)
*Yuxuan Lei,Tianfu Wang,Jianxun Lian,Zhengyu Hu,Defu Lian,Xing Xie*

Main category: cs.CL

TL;DR: 提出了HumanLLM，通过专门的数据集和微调技术，将大模型用于个体行为和认知模拟，提升了社交智能和用户个性化表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学、编程等客观任务上取得进展，但其在模拟和理解人类行为、认知方面能力有限，影响了在社会科学和客户洞察等场景下的应用。作者认为这一问题源于预训练数据和人类决策上下文之间的错位。

Method: 提出HumanLLM模型，并构建Cognitive Genome Dataset，该数据集收集了Reddit等平台的真实用户数据，经多阶段过滤、合成和质控，提炼出用户画像、行为和思维模式。随后设计多种学习任务，通过监督微调赋予模型预测个体行为、思想和经历的能力。

Result: HumanLLM在用户行为和内心想法预测、模拟用户风格和兴趣、生成个性化画像等任务上均优于基础大模型，并在社交智能基准测试中表现出更好的泛化性。

Conclusion: 通过特定设计和数据集，HumanLLM增强了对个体认知和行为的建模能力，为社会科学研究和个性化应用带来了更好的工具。

Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.

</details>


### [100] [SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics](https://arxiv.org/abs/2601.15809)
*Silvia Casola,Ryan Soh-Eun Shim,Felicia Körner,Yuchen Mao,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了多语言生成任务的评价瓶颈，提出在多语言评测指标中引入英语为 '中介'，并通过实验证明该方法可提升多语言评价指标与人工评价的相关性。


<details>
  <summary>Details</summary>
Motivation: 在多语言自然语言生成任务中，缺乏准确而强健的自动评价指标，造成学术进展缓慢。已有研究发现多语言语言模型往往依赖英语作为内部中介语言，本论文提出将该机制应用到评价指标上。

Method: 作者假设多语言神经评价指标若引导其激活向英语中介对齐，能提升与人工评价的一致性。设计了基于encoder和decoder的指标，在测试阶段采用干预方法使激活对齐英语，并在多种语言任务上进行了实证实验。

Result: 结果显示，无论是encoder还是decoder结构，干预方法在多种语言下均有效提升指标与人工评价的相关性。

Conclusion: 将多语言评价指标的激活引导至英语中介是一种简单有效的方法，对多语言自然语言生成评价具有实际意义，能够推动该领域进步。

Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.

</details>


### [101] [ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15820)
*Guoxuan Ding,Yuqing Li,Ziyan Zhou,Zheng Lin,Daren Zha,Jiangnan Li*

Main category: cs.CL

TL;DR: 该论文提出了一种新的多模态假新闻检测框架ExDR，通过解释驱动的动态检索增强生成，提高了检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态假新闻检测方法难以应对假新闻传播的快速变化及对最新事实的依赖，面临冗余检索、粗略相似性和无关证据等挑战，因此亟需更精确高效的解决方案。

Method: 提出ExDR框架，利用模型生成的解释信息参与检索触发与证据检索，结合三维触发信心评估、融合欺骗实体的实体感知索引，以及基于欺骗特征的对比证据检索，提升证据挑选和最终预测的准确性。

Result: 在AMG和MR2两个基准数据集上的实验显示，ExDR在检索触发准确率、检索质量和整体检测表现等方面均优于现有方法，效果显著。

Conclusion: ExDR框架能高效提高多模态假新闻检测的性能，具备较好泛化能力，为应对复杂、多变的假新闻提供了有效工具。

Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.

</details>


### [102] [Can professional translators identify machine-generated text?](https://arxiv.org/abs/2601.15828)
*Michael Farrell*

Main category: cs.CL

TL;DR: 本研究考察专业译者在无专业培训情况下能否辨别由AI（ChatGPT-4o）和人类写作的意大利语短篇小说。结果显示，大部分译者难以区分，但有少数能准确辨别，并找出可靠特征。


<details>
  <summary>Details</summary>
Motivation: 伴随AI技术在文本生成领域的发展，AI创作的作品与人类作品越来越难以区分。了解专业译者是否能准确识别合成文本，对于译者职业角色与AI生成内容的编辑实践有重要意义。

Method: 69名专业译者参与实验，每人阅读三篇匿名短篇小说（两篇AI生成，一篇人类创作），并判断其作者身份及理由。对比不同判断的准确率及其依据。

Result: 大多数译者的判断缺乏明显倾向，但16.2%的译者能够显著区分AI和人类作品。同样有接近比例的人误判方向相反。译者普遍受主观印象影响，而不是客观文本特征。低突发性与叙事矛盾是AI作品的显著指标，语法和情感特征易造成误判。

Conclusion: 专业译者在不经专门训练情况下难以稳定辨别AI与人类创作文本。叙事手法特征是关键鉴别因素，而传统的语法和情感判准并不可靠。这对合成文本编辑的专业实践提出了新挑战和思考。

Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.

</details>


### [103] [Determinants of Training Corpus Size for Clinical Text Classification](https://arxiv.org/abs/2601.15846)
*Jaya Chaturvedi,Saniya Deshpande,Chenkai Ma,Robert Cobb,Angus Roberts,Robert Stewart,Daniel Stahl,Diana Shamsutdinova*

Main category: cs.CL

TL;DR: 本文研究了临床文本分类任务中训练数据量与分类性能之间的关系，并分析了文本词汇属性对学习效果的影响。


<details>
  <summary>Details</summary>
Motivation: 临床文本分类常采用NLP模型，但样本量受限且选取缺乏理论依据，尤其不清楚样本数量与文本词汇特性之间的关系。

Method: 使用MIMIC-III公开数据集，采集ICD-9标签的出院记录，利用BERT预训练文本嵌入和随机森林对10个随机选取的诊断进行分类。通过调整训练集规模（100至10,000份文档），并结合Lasso逻辑回归对强/噪声预测词进行分析，探索词汇属性对模型性能的影响。

Result: 不同诊断任务下的学习曲线差异显著。600份文档足以达到全部数据学习效果的95%。词汇分析发现，强预测词越多、噪声词越少时模型学习效果更好，每增加100个噪声词准确率下降约0.02，而每增加100个强预测词最高准确率提升约0.04。

Conclusion: 样本需求量的合理性与文本内有效词汇密切相关。优化词汇属性（增强强预测词、减少噪声词）可提升模型效果，也可据此更科学地决定样本量，有助于高效设计临床NLP任务的数据标注策略。

Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.

</details>


### [104] [Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers](https://arxiv.org/abs/2601.15869)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本文评估了AV-HuBERT模型在音视频知觉一致性（McGurk效应）上的表现，发现模型在某些方面与人类极为相似，但也存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 当前自监督语音-视觉AI模型（如AV-HuBERT）已在多模态感知任务中取得重要进展，但其知觉机制是否忠实于人类生物特性，仍缺乏深入对比和评估。通过对比AI与人类在McGurk效应下的反应，可以揭示模型对生物性语音知觉机制的还原度与局限性。

Method: 招募44名人类参与者，与AV-HuBERT模型在处理音视频不一致刺激（制造McGurk效应）时进行对比，重点分析两者在听觉主导比例、融合反应比例以及错误类型的分布。

Result: AV-HuBERT与人类在听觉主导率方面几乎一致（32.0% vs 31.8%），但模型表现出更强的融合偏向（68.0%对47.7%）。人类反应表现出更多随机性和多样性错误，模型则表现出严格的分类性。

Conclusion: AV-HuBERT等自监督多模态模型在结果层面可部分模拟人类多感知行为，但缺乏人类语音知觉中的神经变异性，其知觉机制仍有与人类本质的区别。

Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.

</details>


### [105] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: Stable-DiffCoder是一种改进的基于扩散的代码生成模型，通过采用分块扩散持续预训练方法，在同等数据和架构条件下，超越了自回归模型（AR）的表现，并在低资源代码建模和结构化编辑推理等任务上展现出优势。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归代码生成模型表现强劲，但基于扩散的模型在生成效率、数据复用等方面具有潜在优势。然而，现有的基于扩散的代码模型在同等资源下表现不如AR基线。该论文旨在探究和提升基于扩散的代码生成模型在各类代码建模任务上的能力。

Method: 论文提出Stable-DiffCoder，基于Seed-Coder架构进行分块扩散建模，并整合了分块扩散的持续预训练（CPT），引入量身定制的逐步热身和分块裁剪噪声调度，从而实现高效知识学习和稳定训练。模型在相同数据及结构下与AR模型进行对比测试。

Result: Stable-DiffCoder在多项代码基准测试中整体超越同架构的自回归模型，并且仅依赖持续预训练和有监督微调，就取得了优于主流8B规模的AR和DLLM模型的效果。

Conclusion: 基于扩散的建模方式在代码生成、结构化编辑与推理等场景下展现出了超越自回归训练的潜力，同时，通过数据增强可进一步促进低资源编程语言的建模效果。

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [106] [Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech](https://arxiv.org/abs/2601.15909)
*Soufiane Jhilal,Stéphanie Martin,Anne-Lise Giraud*

Main category: cs.CL

TL;DR: 本文通过将MEG信号转换为图像形式，利用预训练视觉模型提升对想象语音的非侵入式解码效果，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 非侵入式地解码想象语音存在信号弱、分布分散和标注数据稀缺等挑战。作者希望突破这些难题，提高语音想象相关神经信号的识别精度。

Method: 作者将21名被试的MEG信号通过可学习的传感器空间卷积转换为三通道空间小波图像（scalogram mixture），生成类似图片的紧凑输入。然后，利用在ImageNet上预训练的视觉神经网络进行分类任务，比较其与传统和非预训练模型的性能。

Result: 采用预训练视觉模型的系统，在语音想象与安静、默读等多类任务上，分别达到最高90.4%、81.0%和60.6%的平衡准确率，并且跨被试评估验证了模型对共有神经表示的捕捉能力。时序分析还发现判别信息主要集中在想象语音相关的时段。

Conclusion: 将MEG信号转化为图像后应用预训练视觉模型能够高效提取非侵入式语音想象神经信号的结构，显著提升了解码性能，为相关脑-机接口和神经语言解码研究提供了有效思路。

Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.

</details>


### [107] [Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain](https://arxiv.org/abs/2601.16018)
*Özgür Uğur,Mahmut Göksu,Mahmut Çimen,Musa Yılmaz,Esra Şavirdi,Alp Talha Demir,Rumeysa Güllüce,İclal Çetin,Ömer Can Sağbaş*

Main category: cs.CL

TL;DR: 本文提出了Mecellem模型框架，通过领域自适应策略提升土耳其法律领域专用语言模型的表现。包含全新预训练编码器和持续预训练解码器。模型高效、资源消耗低，效果优于同类产品。


<details>
  <summary>Details</summary>
Motivation: 目前主流的语言模型在土耳其法律领域的特定任务上表现有限，尤其是资源消耗较大、迁移到专业领域成本高。这促使作者开发专门面向土耳其法律领域的高效语言模型，并优化训练策略以降低算力消耗。

Method: 作者提出两种创新方法：1）基于ModernBERT的编码模型，从头在1127亿token的土耳其语语料库上预训练，并采用新颖的checkpoint策略，训练过程中根据下游检索性能选择最优模型；2）对Qwen3系列解码器进行持续预训练（CPT），通过四阶段课程学习逐步迁移至法律领域，优化专业词汇和长文本推理能力。

Result: 编码器模型在土耳其检索排行中进入前三，小模型性能接近大型模型，生产效率达92.36%，在资源占用上优于主流SOTA模型。解码器模型在土耳其法律文本上的困惑度降低了36.2%，显著提升了专业领域表现。

Conclusion: Mecellem框架展示了领域自适应与成本效益兼具的语言模型开发路径。单阶段预训练加高效后训练方案，可用更少资源取得与甚至优于多阶段SOTA模型的效果，适合推广至其他小语种或专业领域。

Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

</details>


### [108] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 本文提出拒绝行为源自不同大模型间的通用低维语义结构，并提出方法将拒绝干预无监督传递到不同模型，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型的拒绝行为常被认为具有模型特异性，但作者怀疑这些行为其实来自模型间共享的语义电路，这种发现有助于理解和泛化安全对齐方法。

Method: 提出Trajectory Replay via Concept-Basis Reconstruction方法，将拒绝行为的干预从一个模型迁移到另一个目标模型（跨架构、跨训练方式），具体方法包括通过概念指纹对齐层、利用概念原子的配方重建拒绝方向，并引入权重SVD稳定守护防止破坏模型性能。

Result: 在8组模型（包括GPT-OSS-20B和GLM-4）上实验，迁移后的拒绝干预显著降低了模型的拒绝行为，同时性能保持稳定。

Conclusion: 实验验证了安全对齐中的语义通用性假设，为不同模型间共享与迁移拒绝机制提供了有效方法和实证基础。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>


### [109] [Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating](https://arxiv.org/abs/2601.16097)
*Makbule Gulcin Ozsoy*

Main category: cs.CL

TL;DR: 提出了一种可扩展的多语言Text2Cypher方法，通过训练语言特定的LoRA适配器并采用融合机制，在无需重新大规模微调的情况下支持新语言，提高了多语言Cypher查询生成的效率和便捷性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Text2Cypher（自然语言到数据库查询转换）系统对多语言尤其是非英语的支持有限。现有方法添加新语言通常需要全量微调，费时费力且效率低下，亟需更高效、更易扩展的解决方案。

Method: 为每种语言（英文、西班牙语、土耳其语）分别训练LoRA适配器，之后通过线性均匀融合或动态门控的融合MLP，将多个适配器组合，以便支持多语言查询转换。

Result: 实验表明，融合MLP在仅需较少数据的情况下可以恢复约75%的联合多语言微调带来的准确率提升，且在所有测试语言上均优于线性融合方法。

Conclusion: 所提出的融合适配器方法为多语言Text2Cypher任务提供了一种高效、可扩展且数据高效的解决方案，可便捷地增量支持新语言，是昂贵的联合微调的实用替代方案。

Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

</details>


### [110] [synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier](https://arxiv.org/abs/2601.16113)
*Haq Nawaz Malik,Kh Mohmad Shafi,Tanveer Ahmad Reshi*

Main category: cs.CL

TL;DR: 本文提出了 SynthOCR-Gen，一个面向低资源语言的开源合成 OCR 数据集生成工具，并以克什米尔语为例，自动生成了大规模训练数据集，有效缓解了低资源语言 OCR 数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如克什米尔语）的 OCR 研究受限于缺乏大规模标注数据集，且手工数据构建成本高昂且效率低。当前主流 OCR 系统尚不支持这些语言，因此亟需自动化的数据集生成方法。

Method: SynthOCR-Gen 工具将数字化的 Unicode 文本转为 OCR 训练数据，流程包括字符、单词、短语、句子、行级别的文本分割，Unicode 正规化及书写纯净性保证，多字体渲染与分布配置，超过 25 种数据增强方法模拟实际文档退化（如旋转、模糊、噪声、扫描伪影）。

Result: 利用该工具自动生成并公开了 60 万条基于单词分割的克什米尔语合成 OCR 数据集。实验和公开发布证明了工具的高效性和实用性。

Conclusion: SynthOCR-Gen 为低资源语言的 OCR 系统开发提供了可行路径，并推动这些语言纳入视觉-语言 AI 体系，其工具和数据集均已开放，有助于全球罕见书写系统的研究和应用。

Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.
  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.
  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.

</details>


### [111] [Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging](https://arxiv.org/abs/2601.16127)
*Alphaeus Dmonte,Vidhi Gupta,Daniel J Perry,Mark Arehart*

Main category: cs.CL

TL;DR: 该论文提出了一种高效的多语言大模型微调和维护策略，通过模型合并降低计算成本，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型在添加新语言或更新某个语言时，需要重新训练整个模型，计算资源消耗大，维护难度高，因此需要更高效的解决方案。

Method: 作者系统分析了多语言多任务模型合并（merging）策略，从效率视角出发，评估其在三项独立任务中的表现。提出通过将任务或语言单独更新后再合并，达到减少整体训练时间的方法。

Result: 实验证明，模型合并可减少初始训练时间高达50%；在只更新单一语言并重新合并时，与整体重训练对比，训练成本降低超过60%。这些结论在公开与行业数据集上均得到验证。

Conclusion: 多语言多任务模型合并技术，不仅提升了计算和维护效率，同时保持模型的质量，适用于工业实际应用。

Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.

</details>


### [112] [Automatic Classification of Arabic Literature into Historical Eras](https://arxiv.org/abs/2601.16138)
*Zainab Alhathloul,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本论文提出使用神经网络与深度学习自动对阿拉伯文文本进行时代分类，并在两个公开语料库上验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注除诗歌以外的阿拉伯语文本自动时代分类，因此需要开发新的自动分类技术，进一步理解阿拉伯语文本随时代演变的特征。

Method: 采用神经网络和深度学习技术，分别使用两个来源不同的公开阿拉伯语语料（OpenITI与APCD），在二分类到多分类（最多15类）不同粒度的时代划分方案下进行实验。

Result: 在二元时代分类任务上，OpenITI数据集F1得分为0.83，APCD为0.79；多时代分类性能下降，OpenITI在15类任务上得分为0.20，APCD在12类任务上得分为0.18。

Conclusion: 神经网络方法对阿拉伯语时代二分类有较好表现，但在多时代或细粒度分类下难以区分，表明该任务存在挑战，可为后续研究提供参考。

Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.

</details>


### [113] [LLM-in-Sandbox Elicits General Agentic Intelligence](https://arxiv.org/abs/2601.16206)
*Daixuan Cheng,Shaohan Huang,Yuxian Gu,Huatong Song,Guoxin Chen,Li Dong,Wayne Xin Zhao,Ji-Rong Wen,Furu Wei*

Main category: cs.CL

TL;DR: 提出了LLM-in-Sandbox方法，使大模型可以在虚拟代码沙盒中自主探索，并在非代码领域展现通用智能。方法无需额外训练也取得良好泛化，在多个科学与理解任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动机在于提高大模型在非编码任务中的通用智能和实际应用能力，通过为其提供一个受控的沙盒环境，观察其如何自主利用环境资源解决复杂任务。

Method: 核心方法是让LLM在代码沙盒（虚拟计算机）中运行，并观察其针对非代码任务（如知识获取、长上下文处理、脚本执行）的自发行为。同时提出LLM-in-Sandbox RL，通过强化学习进一步提升其agent性能力，训练过程仅使用非agent数据。

Result: 实验结果显示，无论是无需训练还是通过RL微调，LLM-in-Sandbox都在数学、物理、化学、生物医学、长文本理解及指令跟随等任务上展现了强泛化与鲁棒性。

Conclusion: LLM-in-Sandbox有效提升了大模型在非代码领域任务的智能水平和泛化能力，同时开源为Python工具包，便于现实部署。

Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [114] [Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods](https://arxiv.org/abs/2601.15309)
*Jiaxin Xu,Chao Zhang,Raymond H. Cuijpers,Wijnand A. IJsselsteijn*

Main category: cs.RO

TL;DR: 本综述系统性分析了社交机器人在健康行为改变中的应用，归纳了其行为改变策略及评估方法，为后续设计与研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在健康干预中的应用逐渐普及，但其设计与评估缺乏可操作性指导，因此需要梳理现有实践，提炼关键策略和评估特征。

Method: 系统检索数据库和手动筛选文献，共分析39项相关HRI（人机交互）研究，总结其中采用的行为改变策略和评估方法。

Result: 总结出社交机器人健康行为干预常用的四类行为改变策略：教练策略、咨询策略、社会影响策略和说服增强策略；还系统归纳了现有评估实践的设计、场景、周期和结果指标等特性。

Conclusion: 社交机器人在健康行为改变中的独特优势已初步显现，本研究提出的策略和评估特征可为未来HRI研究与设计提供有益的启示和方向。

Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.

</details>


### [115] [Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray](https://arxiv.org/abs/2601.15349)
*Jiaqing Chang,Song Gao,Chaowei Dong,zhaobang Li,Yang Liu*

Main category: cs.RO

TL;DR: 本文设计了一种磁响应、仿牛鼻鲼的微型软体机器人，并通过三维亥姆霍兹线圈下磁场驱动，实现了高效、可控的水下运动。


<details>
  <summary>Details</summary>
Motivation: 微型软体机器人因其灵活和体积小，适合狭窄环境下应用，如环境监测和微创医疗。然而，体积微小导致难以内置动力，本研究希望借助仿生设计和无线磁驱动，提升微型机器人的水下运动性能。

Method: 受牛鼻鲼游动原理的启发，利用NdFeB和PDMS制造机器人。通过外部三维亥姆霍兹线圈产生振荡谐波磁场驱动机器人运动，系统研究了不同磁场参数对游动性能的影响，并测试了直行、转向等多种运动模式。采用阶梯式调整方法减少轨迹误差。

Result: 实验发现，在磁感应强度B=5 mT、频率f=11 Hz条件下，机器人游速最快达5.25 mm/s（约为0.5体长/秒）。通过调节磁场参数，机器人可实现多种控制运动。阶梯调整有效减少了运动方向误差。

Conclusion: 本研究展示了磁驱动微型软体机器人的有效方法，为无线驱动机器人在水下狭窄空间的实际应用奠定了基础。

Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.

</details>


### [116] [Learning a Unified Latent Space for Cross-Embodiment Robot Control](https://arxiv.org/abs/2601.15419)
*Yashuai Yan,Dongheui Lee*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的类人机器人跨体态控制框架，通过学习人类和多种机器人之间的共享潜在表示，实现多平台运动统一和迁移。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人控制很难在形态各异的平台间迁移和统一人类运动，大量适配和微调阻碍了通用和规模化部署。

Method: 方法包括两阶段：第一阶段利用对比学习生成分离的潜在空间，捕捉跨身体部位的局部运动特征，并引入关节旋转和末端执行器定位相结合的相似性度量。第二阶段用条件变分自编码器将目标导向的控制策略直接训练在潜在空间，仅用人类数据。加入新机器人时只需学习轻量的特定嵌入层。

Result: 实验表明，训练好的政策可直接无适配部署到多种类人机器人上，且可高效扩展到新机器人，策略具有强鲁棒性、可扩展性和体态无关性。

Conclusion: 该工作有效实现了类人动作的通用迁移，为多种体态机器人的一致、可规模化控制提供了新方向。

Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.

</details>


### [117] [Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation](https://arxiv.org/abs/2601.15459)
*Sarvin Ghiasi,Majid Roshanfar,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: 本研究提出了一个集成框架，通过结合解析建模、实时仿真和机器学习，提高腹腔镜手术中机械臂的安全性和操作效率，重点解决碰撞检测与最小距离估算的难题。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中，多机械臂协同作业存在碰撞风险和空间受限问题，要求精确估算机械臂之间的最小距离，以确保手术安全与流畅操作。

Method: 采用解析建模估算机械臂关节配置下的最小距离，开发了3D仿真环境生成多样配置供碰撞与距离检测，并利用深度神经网络以关节参数和相对位置为输入进行距离预测。

Result: 深度学习模型在最小距离预测中取得了282.2mm的平均绝对误差，R方达0.85，预测值和实际值高度一致，显示出模型对空间关系的良好泛化能力。

Conclusion: 结合解析建模与机器学习算法能有效提升机械臂距离估算的精度与可靠性，有助于腹腔镜机器人系统更安全高效运行。

Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.

</details>


### [118] [A Universal Large Language Model -- Drone Command and Control Interface](https://arxiv.org/abs/2601.15486)
*Javier N. Ramos-Silva,Peter J. Burke*

Main category: cs.RO

TL;DR: 本论文提出了一种通用且易用的大语言模型（LLM）与无人机控制接口，利用MCP标准实现与几乎所有无人机的对接，通过自然语言实现无人机指令控制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在无人机智能控制领域潜力巨大，但实际应用中，LLM与无人机的接口开发极为繁琐、耗费人力，且每个应用都需单独定制，缺乏通用解决方案。

Method: 作者提出了基于MCP（Model Context Protocol）开放标准的接口方法，设计并部署了支持Mavlink协议的云端MCP服务器，实现LLM与无人机的通用对接。支持与Google Maps等平台集成，提供实时导航数据。

Result: 实验证明，所提出接口能够顺利通过自然语言驱动真实无人机飞行，并且在模拟环境下实现了复杂的飞行规划和基于实时地图数据的控制。

Conclusion: 该工作实现了LLM与无人机指令控制的通用化和易用化，为AI行业与无人机技术的深度融合、推动智能无人机应用带来新范式。

Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.

</details>


### [119] [CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation](https://arxiv.org/abs/2601.15541)
*Heng Zhang,Wei-Hsing Huang,Qiyi Tong,Gokhan Solak,Puze Liu,Sheng Liu,Jan Peters,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了一种CompliantVLA-adaptor，结合视觉语言模型与力觉自适应控制，提高了机器人在复杂接触任务中的安全性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型虽能输出运动位置，但对力的适应性较差，导致在需要接触、柔顺或存在不确定性的物理任务中可能不安全或失败。为解决这一力觉适应不足的问题，作者希望结合VLM理解任务上下文，实现更安全的变量阻抗控制。

Method: CompliantVLA-adaptor利用视觉语言模型，从图像和自然语言中获取任务上下文，自适应调整变量阻抗控制器的刚度和阻尼，同时结合实时力/力矩反馈，确保交互力在安全阈值范围内。

Result: 在多项复杂接触任务中（模拟与真实机器人），该方法较现有VLA方法成功率更高，力违规显著减少，总体成功率由9.86%提升到17.29%。

Conclusion: CompliantVLA-adaptor推动了VLA模型面向安全接触丰富操作任务的实际应用，为实现安全高效的机器人交互提供了新途径。相关代码和数据集已开源。

Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.

</details>


### [120] [A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control](https://arxiv.org/abs/2601.15545)
*Zhifan Yan,Chang Liu,Yiyang Jiang,Wenxuan Zheng,Xinhao Chen,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）的低成本、紧凑型移动磁操控平台，用于实现消化道内药物靶向递送机器人的高精度无模型控制。其控制策略可在15分钟内部署，实现对磁性胶囊在二维轨迹上的精准移动。


<details>
  <summary>Details</summary>
Motivation: 当前GI道磁控机器人的控制存在两大难题：固定磁系统的工作空间受限，而移动系统则需要大量预先校准物理模型，既耗时又计算负担重，极大阻碍了实际应用。

Method: 本研究采用四电磁体阵列安装在UR5机械臂上，通过Soft Actor-Critic（SAC）深度强化学习算法并采用仿真到现实策略迁移（sim-to-real），实现模型学习及策略的快速部署。

Result: 系统可在15分钟内完成策略部署，有效控制7毫米磁性胶囊沿二维方形和圆形轨迹运动，路径均方根误差分别为1.18mm和1.50mm，并验证能在30cm×20cm的临床相关工作空间覆盖和跟踪。

Conclusion: 本文提出的DRL无模型磁控平台可以快速部署，低成本且控制精确，适合大工作空间内的精细磁操作，在胃肠道微型机器人药物递送等领域具有广泛应用前景。

Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a "model-calibration bottleneck", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.

</details>


### [121] [Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor](https://arxiv.org/abs/2601.15607)
*Lenworth Thomas,Tjaden Bridges,Sarah Bergbreiter*

Main category: cs.RO

TL;DR: 该论文提出在小型四旋翼飞行器上结合气流传感器与改进的跟踪算法，提高污染物羽流源头的侦测与定位能力。


<details>
  <summary>Details</summary>
Motivation: 环境灾害日益频发，快速准确寻找污染源对于应对突发事件至关重要。由于重量与空间限制，小型四旋翼在应用中受到传感器灵敏度与响应速度的制约，亟需更有效的羽流追踪与源头寻迹方法。

Method: 本文设计了一种能够同时检测气流强度和方向的定制气流传感器，集成于小于100克的四旋翼上，并基于流向信息对传统Cast and Surge算法进行改进，实现高效的源头寻迹。同时，系统在飞行中进行了系列性能与定位实验验证。

Result: 实验结果表明，该系统可在不同起始位置和方位条件下，可靠探测飞行中的气流，并控制飞行器朝气流方向校正姿态，最终高效定位气流源头。

Conclusion: 该研究为未来将气流传感器与其他环境传感器协同，用于丰富羽流追踪和污染源头定位的数据收集平台奠定了基础。

Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.

</details>


### [122] [AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning](https://arxiv.org/abs/2601.15614)
*Zichen Yan,Yuchen Hou,Shenao Wang,Yichao Gao,Rui Huang,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为AION的新方法，实现了空中机器人无需外部定位或全局地图，通过视觉自主在3D环境中寻找并导航至目标物体，并在多个平台上验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有大多数ObjectNav研究主要集中在2D地面机器人或零样本情况下，对于具备3D运动能力的空中机器人（如无人机）的研究较少。与地面机器人相比，空中机器人具备更强的机动性和搜索效率，但也带来了空间感知、动态控制与安全保障等新挑战，因此需要新的导航方法。

Method: AION是一种端到端、无外部定位或全局地图依赖的双策略强化学习框架，通过分离探索和达标两大行为模块，分别训练以实现更有效的导航。该方法基于视觉输入进行决策，并在AI2-THOR基准和IsaacSim高保真无人机仿真环境中进行了评估。

Result: 实验结果表明，AION在搜索探索、导航效率和安全性能等各方面的评估指标上均优于以往方法，体现出较强的综合实力。

Conclusion: AION方法为空中机器人对象导航（3D ObjectNav）提供了新的高效解决方案，在无需依赖外部定位的情况下，实现了更安全且高效的目标搜索和导航，为推动该领域研究扩展提供了可行的技术路径。

Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.

</details>


### [123] [D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot](https://arxiv.org/abs/2601.15707)
*Qifan Hu,Branko Celler,Weidong Mu,Steven W. Su*

Main category: cs.RO

TL;DR: 本文提出了一种两阶段校准框架，用于三自由度踝关节康复机器人，提高了校准效率并保证了参数估计的鲁棒性。通过PPO智能体选择信息量最大化的姿态组，校准效果明显优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 多自由度康复机器人在临床训练中对精确对准要求极高，不准确的对准会影响患者安全和训练效果。传统校准方式效率低、参数估计精度有限。因此亟需一种更高效、鲁棒的校准方法。

Method: 首先提出了一种基于Kronecker积的开环线性参数识别方法，将输入输出对准转化为信息矩阵驱动的最优实验设计问题。随后引入强化学习方法（PPO智能体），在50个候选姿态中自动选择4个信息量最大的姿态，实现D-最优性准则下的姿态筛选。仿真与实物测试均验证了方法有效性。

Result: 通过PPO智能体选择姿态组，信息矩阵行列式比随机选择高出两个数量级且方差更小；仅用4个D最优姿态获得的参数估计在交叉验证中一致性优于用更多但无结构的50个姿态所得结果。

Conclusion: 该方法显著提升了多自由度康复机器人校准的效率和鲁棒性，为临床精密对准提供了实用指导。

Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.

</details>


### [124] [DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving](https://arxiv.org/abs/2601.15729)
*Rui Yang,Lei Zheng,Ruoyu Yao,Jun Ma*

Main category: cs.RO

TL;DR: 提出DualShield框架，用于提升自动驾驶多模态运动规划的安全性与效率，通过双重机制实现对扩散模型的安全约束。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在自动驾驶中的多模态运动规划表现优秀，但难以强制满足车辆动力学约束且对其他智能体预测精度高度依赖，容易导致不安全情况，尤其在不确定或对抗性互动下。

Method: DualShield框架结合Hamilton-Jacobi (HJ)可达性值函数：一方面引导扩散去噪过程向安全、动力学可行区域收敛（主动指导）；另一方面用控制屏障-值函数（CBVFs）实时修正实际执行动作，作为安全屏障（被动防护）。

Result: 在具有挑战性的无保护掉头场景下，DualShield在安全性与任务效率上均显著优于当前主流规划范式，尤其在不确定性环境下的表现突出。

Conclusion: DualShield为自动驾驶扩散运动规划带来了理论可证的安全保障和更优作业表现，有望缓解多模态规划在现实部署中的安全瓶颈。

Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.

</details>


### [125] [Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV](https://arxiv.org/abs/2601.15775)
*Amir Habel,Ivan Snegirev,Elizaveta Semenyakina,Miguel Altamirano Cabrera,Jeffrin Sam,Fawad Mehboob,Roohan Ahmed Khan,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种名为Glove2UAV的可穿戴IMU手套界面，实现了通过手部和手指动作直观控制无人机，并在超速时通过振动反馈进行警示。作者验证了其实时控制能力和指令响应的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前无人机操作多依赖遥控器或移动设备，交互方式存在一定门槛，缺乏直观性和安全反馈机制。作者希望通过更自然的手部动作与即时反馈提升无人机操作的易用性与安全性。

Method: 采用集成IMU的手套获取手掌及手指的实时惯性数据，经中值异常值抑制与Madgwick算法估算姿态，再将手势映射为一组基础飞行与操作指令。手套具备振动反馈模块，在无人机速度超阈值时予以提醒。通过与无人机遥测信号同步，实测与仿真结合进行系统验证。

Result: 实验结果显示，该手套系统实现了手势命令的快速响应，动作与无人机运动耦合稳定。基础指令集在实际飞行中均可正确执行，振动警示也能及时发出。系统具备实时可用性。

Conclusion: Glove2UAV展示了一种实时、轻量、部署简便的无人机直观交互方式，不仅提升了无人机的可操作性和交互直观性，也增强了操作过程中的安全反馈。

Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.

</details>


### [126] [A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation](https://arxiv.org/abs/2601.15802)
*Alexandre Albore,Humbert Fiorino,Damien Pellier*

Main category: cs.RO

TL;DR: 本文提出通过部署水下或浮标声学信标网络，引导无人水下航行器（UUV）在无GNSS条件下安全隐蔽导航至岸上的优化路线。信标由空中或水面无人机铺设，结合分层规划算法动态调整航线，实现精准、隐蔽的舰队协同导航。


<details>
  <summary>Details</summary>
Motivation: 在近海禁区或危险区域，UUV必须在无GNSS情况下隐蔽行动，避免暴露。传统导航易受限，急需可在GNSS拒止环境下精准导航的解决方案。

Method: 利用无人机投放并部署声学信标，形成合成地标网络，UUV通过接收声信号完成定位和导航。采用分层规划器实时优化、调整和监控舰队航迹，保证路线的最优性和准确性。

Result: 提出的方法实现了UUV舰队在近海、GNSS拒止环境下的隐蔽、高精度队形导航，能有效规避危险和避免被探测。分层规划器增强了适应性和实时反应能力。

Conclusion: 该信标网络结合动态航迹规划的方法能显著提升UUV舰队在复杂环境下的作战或海洋任务表现，在安全性与导航精度间取得良好平衡，有广阔的实际应用前景。

Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.

</details>


### [127] [TeNet: Text-to-Network for Compact Policy Synthesis](https://arxiv.org/abs/2601.15912)
*Ariyan Bighashdel,Kevin Sebastian Luck*

Main category: cs.RO

TL;DR: TeNet提出了一种高效、紧凑、可泛化的机器人控制策略生成方法，只需一次自然语言输入即可生成高频执行的政策，性能优于现有大型端到端方法，且模型规模更小。


<details>
  <summary>Details</summary>
Motivation: 现有机器人自然语言指令控制方法要么依赖手工接口，难以灵活泛化；要么依赖大规模端到端模型，部署实时控制困难、效率低。实际需求是能够将自然语言高效、直接地转化为可实时执行的机器人控制策略，尤其适合在资源受限情境下运行。

Method: 提出TeNet框架：将自然语言输入通过预训练大语言模型转为embedding，再通过超网络（hypernetwork）生成紧凑的控制策略。策略仅依赖低维状态输入，高效地控制机器人，无需持续依赖语言输入。训练期间可选地采用行为对齐（将文本embedding与演示动作对齐）增强泛化，推理（测试）时无需演示。

Result: 在MuJoCo与Meta-World基准测试中，TeNet生成的策略模型体积远小于序列建模baseline，在多任务与元学习环境中表现优异，支持高频控制。

Conclusion: 文本条件超网络能在资源受限和实时要求下，为机器人生成高效、紧凑且泛化能力强的控制器，是可行实用的方案。

Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.

</details>


### [128] [Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems](https://arxiv.org/abs/2601.15946)
*Zijie Chen,Xiaowei Liu,Yong Xu,Shenghai Yuan,Jianping Li,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种无靶标的基于DH参数的激光雷达-电机标定方法和环境自适应的激光雷达-惯性里程计，优化了多种安装配置下的标定与定位鲁棒性，提高了特征稀疏场景下的扫描覆盖与定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有旋转驱动激光雷达的标定方法需要针对不同的安装配置进行参数化，难以通用。同时，设备在扫描无特征区域时，扫描覆盖度和定位鲁棒性难以兼顾。

Method: 作者提出了两项方法：（1）基于Denavit-Hartenberg（DH）表示的无靶标激光雷达-电机标定算法（LM-Calibr），支持多种安装方式；（2）环境自适应的激光雷达-惯性里程计（EVA-LIO），动态调整点云下采样率和地图分辨率以适应不同空间尺度。

Result: 大量实验验证了新方法在不同场景、安装角度和初始值下的高精度和收敛性。EVA-LIO也能保持鲁棒的定位，支持在激光雷达短暂扫描无特征区域时依然实现高覆盖和可靠定位。

Conclusion: 本文提出的方法兼具通用性和鲁棒性，大幅提升了主流多配置激光雷达系统的标定和定位效果。源代码与硬件设计已开源，便于复现和应用。

Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}

</details>


### [129] [PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour](https://arxiv.org/abs/2601.15995)
*Liang Wang,Kanzhong Yao,Yang Liu,Weikai Qin,Jun Wu,Zhe Sun,Qiuguo Zhu*

Main category: cs.RO

TL;DR: 本文提出PUMA端到端学习框架，将视觉感知与落足推理整合，使四足机器人具备类人感知与动作适应性，在复杂地形的跑酷任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效感知环境，做出合理的落足选择，但四足机器人难以具备类似感知推理能力。现有方法多采用分层控制器和预先计算的落足点，限制了机器人在实时适应性和强化学习探索方面的潜力。

Method: 提出PUMA框架，将视觉感知与落足先验合为一体，通过端到端单阶段训练，根据地形特征估计以自身为参照的极坐标落足先验（距离和方向），进而引导机器人调整姿态，适应复杂地形。

Result: 通过在模拟和现实多种复杂离散地形上的大量实验，PUMA展现出了极高的敏捷性与强健性，在具有挑战性的场景下依然表现优异。

Conclusion: PUMA有效提升了四足机器人在复杂地形跑酷任务下的自主适应与探索能力，为端到端结合视觉和运动推理的智能机器人控制提供了新框架。

Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.

</details>


### [130] [Collision-Free Humanoid Traversal in Cluttered Indoor Scenes](https://arxiv.org/abs/2601.16035)
*Han Xue,Sikai Liang,Zhikai Zhang,Zicheng Zeng,Yun Liu,Yunrui Lian,Jilong Wang,Qingtao Liu,Xuesong Shi,Li Yi*

Main category: cs.RO

TL;DR: 本文提出了一种新方法（HumanoidPF），实现了类人机器人在复杂室内环境中的无碰撞穿越，其策略在现实中表现优秀，并可通过单击实现远程操控。


<details>
  <summary>Details</summary>
Motivation: 在室内杂乱场景中操作类人机器人时，如何有效感知并应对多变的障碍布局是难题，缺乏能捕捉机器人与障碍物关系的有效表征，造成直接学习穿越技能的困难。

Method: 提出Humanoid Potential Field（HumanoidPF），将机器人与障碍物之间的关系编码为无碰撞的运动方向，辅助基于强化学习的穿越技能学习。设计混合场景生成方法，结合真实三维室内场景以及程序生成障碍，从而提升泛化能力。

Result: HumanoidPF有很小的仿真到现实表现差距，通过混合环境训练获得的策略能成功转移到真实机器人。开发了单击远程操作系统，实现用户易用控制。实验证明方法在仿真和现实均有效。

Conclusion: HumanoidPF是一种有效捕捉类人机器人与障碍物关系的新型表征，能极大促进复杂场景下的穿越能力提升。其在仿真与现实中的有效性和泛化能力得到证实，为真实环境中的远程无碰撞操作提供了新方案。

Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.

</details>


### [131] [DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning](https://arxiv.org/abs/2601.16046)
*Junha Lee,Eunha Park,Minsu Cho*

Main category: cs.RO

TL;DR: 该论文提出了DextER模型，通过引入基于接触的具身推理来生成灵巧的抓取动作，在多指操作任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语言驱动抓取方法往往忽略了手与物体之间物理交互的推理，直接从观测到抓取参数，缺乏中间的具身推理表达，导致对复杂操作理解和执行有限。

Method: DextER模型提出以“接触令牌”方式逐步预测各手指和物体表面接触点的组合，通过自回归策略生成这些接触令牌，得到具身推理的中间表达，再生成最终的抓取配置。该方式兼顾了任务语义和物理约束，并能通过部分接触点指定实现对生成结果的可控引导。

Result: 在DexGYS基准上，DextER抓取成功率达67.14%，超越最先进方法3.83个百分点，意图对齐性能提升96.4%。

Conclusion: 引入基于接触的具身推理可以有效提高语言驱动的灵巧抓取生成质量与可控性，实现更好的人机任务对齐，对多指操作和复杂抓取目标具有良好的推广价值。

Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.

</details>


### [132] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis](https://arxiv.org/abs/2601.16062)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文分析了SE2(3)李群框架在高精度导航状态估计中的误差传播自主性，并提出了一种改进模型来提升其自主性。


<details>
  <summary>Details</summary>
Motivation: SE2(3)李群框架因误差传播具有自主性而被广泛用于导航建模。但现有研究多聚焦于低精度应用，忽略了地球自转和惯性器件偏值。针对高精度导航需求，误差传播的自主性难以维持，需要理论分析和模型改进。

Method: 作者针对高精度情形分别在惯性系、地球系和世界系下，对SE2(3)李群的误差传播自主性进行了理论分析，并探讨了现有方法在引入科里奥利力后面临的局限。为此，提出了一种新的SE2(3)群导航模型构建方法。

Result: 理论分析揭示了传统SE2(3)建模在非惯性系中因引入速度相关科里奥利项而导致自主性丧失的问题。新提出的构建方案能够使导航模型更接近于完全自主。

Conclusion: 该研究拓展了SE2(3)李群在高精度导航下的应用边界，并通过模型创新改善了导航系统的误差传播自主性，对高精度导航应用具有实际指导意义。

Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.

</details>


### [133] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application](https://arxiv.org/abs/2601.16078)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文基于SE2(3)李群框架，提出并验证了一种具备误差传播自主性的高精度导航建模方法。通过实测与仿真，证明该方法相较于传统非惯性导航模型在性能上的提升。


<details>
  <summary>Details</summary>
Motivation: 现有导航模型在误差传播管理上缺乏自主性，影响系统精度与鲁棒性。作者旨在通过SE2(3)李群理论，提高导航系统误差处理的自主性，优化导航表现。

Method: 提出一种构建基于SE2(3)群的导航模型方法，使其适用于非惯性导航情况。并结合实地SINS/ODO实验与蒙特卡罗仿真测试模型性能。

Result: 实验证明，改进的SE2(3)群高精度导航模型在精度和误差传播自主性方面优于传统模型。

Conclusion: SE2(3)李群导航模型能有效提升导航系统精度，实现误差传播的高度自主性，对高精度导航领域具有实际应用价值。

Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.

</details>


### [134] [Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision](https://arxiv.org/abs/2601.16109)
*Yashuai Yan,Tobias Egle,Christian Ott,Dongheui Lee*

Main category: cs.RO

TL;DR: 提出了一个结合基于模型的控制与残差强化学习（RL）的双足行走控制框架，提升在现实环境不确定性下的稳健性与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 双足机器人在现实环境中常面临模型不准确和传感器噪声等不确定性，如何实现高鲁棒性和自适应性是难点。现有方法要么对不确定性适应性差，要么需大量精细奖励设计，亟需更高效鲁棒的解决办法。

Method: 使用基于模型的DCM轨迹规划器和全身控制器作为基础策略，同时引入通过RL和域随机化训练的残差策略应对不确定性。训练残差策略时，用一个能获取真实动力学信息的基于模型“oracle”策略进行创新性监督，帮助高效学习补偿未建模效应。

Result: 方法在多种随机化条件下展现出更强的鲁棒性和泛化能力，优于传统单一模型或RL方法。证明了框架对从仿真到现实（sim-to-real）迁移有较强适用性。

Conclusion: 该方法为双足机器人行走提供了可扩展、有效的现实鲁棒性解决方案，有望推动仿真到现实的应用落地。

Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.

</details>


### [135] [IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance](https://arxiv.org/abs/2601.16207)
*Jongwoo Park,Kanchana Ranasinghe,Jinhyeok Jang,Cristina Mata,Yoo Sung Jang,Michael S Ryoo*

Main category: cs.RO

TL;DR: IVRA是一种轻量级、免训练的新方法，通过利用模型内部视觉编码器中的亲和性信息，在不更改任何参数的前提下，提升了视觉-语言-动作（VLA）模型的空间理解和操作表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型将图片切片展平成一维序列，导致空间信息丢失，不利于精准操作。研究动机在于通过现有的视觉编码器进一步挖掘和利用空间结构线索，无需训练额外模块，提升模型空间理解能力和操作表现。

Method: IVRA无需额外训练，也无需引入外部编码器，而是直接在推理阶段，从已有视觉编码器中提取亲和性提示（affinity hints），并将这些提示注入含有实例级特征的语言模型层。这样能在保持所有参数不变的情况下，改善视觉token之间的交互，增强几何信息的保持。

Result: IVRA适用于多种VLA架构（如LLaRA、OpenVLA、FLOWER）和不同任务（2D和3D仿真操作、实机任务）。在2D VIMA任务中，IVRA在低数据量条件下较基线LLaRA提升平均成功率4.2%；在3D LIBERO任务中，对OpenVLA和FLOWER基线都有明显提升，即使基线准确率很高（96.3%提升到97.1%）。

Conclusion: IVRA方法无需重新训练，直接提升了现有VLA模型的空间感知和操作能力，不依赖外部编码器，具有良好通用性和实用价值。作者已承诺公开全部代码和模型，有利于后续学术和工程推广。

Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA

</details>


### [136] [Point Bridge: 3D Representations for Cross Domain Policy Learning](https://arxiv.org/abs/2601.16212)
*Siddhant Haldar,Lars Johannsmeier,Lerrel Pinto,Abhishek Gupta,Dieter Fox,Yashraj Narang,Ajay Mandlekar*

Main category: cs.RO

TL;DR: 本文提出了Point Bridge框架，通过基于点的通用表征，实现机器人从合成数据到真实操作任务的零样本泛化转移，显著提升了仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 实现普适性机器人智能体受限于大量真实世界操作数据的缺乏，而仿真数据因视觉领域差异限制了其迁移效能。如何更有效地利用合成数据推动机器人在真实场景中的泛化和表现，是当前亟需解决的问题。

Method: Point Bridge框架通过利用视觉-语言模型(VLMs)自动提取基于点的表征，在无明确视觉或对象对齐的情况下实现统一、领域无关的数据表示；结合基于transformer的策略学习和高效的推断管道，仅用合成数据即可训练出有实际能力的机器人；融合少量真实示范的数据进行联合训练，进一步提升效果。

Result: Point Bridge仅用合成数据在零样本仿真到现实迁移上提升了最多44%；在融合少量真实数据的条件下，提升可达66%，同时在单任务和多任务设定下均优于以往方法。

Conclusion: 基于点的领域无关表示极大地提升了合成数据的利用效率和机器人在现实世界的泛化能力。Point Bridge为高效、低成本地训练通用型真实世界操控机器人提供了新的可能。

Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/

</details>
