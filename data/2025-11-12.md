<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 117]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.RO](#cs.RO) [Total: 33]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs](https://arxiv.org/abs/2511.07429)
*Hari Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的可解释视频异常检测（TbVAD）框架，通过将视频内容转化为结构化的文本，实现对异常的检测与解释。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督视频异常检测方法主要依赖于视觉特征，导致结果不够可解释或无法利用知识推理。作者希望通过引入语言表达增强对异常决策过程的解释性与透明度。

Method: TbVAD包含三大步骤：（1）使用视觉-语言模型将视频内容转化为细粒度的文本描述（captions）；（2）按照动作、对象、场景、环境四个语义槽，对文本描述进行结构化；（3）依据语义槽生成解释，揭示哪些特征对异常判断有关键作用。

Result: 在UCF-Crime和XD-Violence两个公开基准数据集上进行实验，结果表明TbVAD方法不仅在异常检测上表现良好，同时能够提供可解释的推理过程。

Conclusion: 将视频语义转化为结构化文本并应用语言推理，可以有效提升视频异常检测方法的可解释性和实用性，适用于实际监控场景。

Abstract: We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.

</details>


### [2] [Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM](https://arxiv.org/abs/2511.07438)
*Joe Kileel,Oscar Mickelin,Amit Singer,Sheng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的数据融合方法MoDM，基于双重二阶矩信息，有效提升了冷冻电镜三维重构的质量。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜重建分子三维结构时，投影噪声大且粒子的取向分布未知，使得准确重构具有挑战性。当前方法在使用不同取向分布采集的数据上融合能力有限，影响重建质量。

Method: 作者提出了MoDM方法：利用在两种取向分布（分别为已知均匀和未知非均匀）下获得的投影影像的二阶矩，通过证明这些矩能够唯一确定分子结构（旋转/反射不计），并设计了基于凸松弛的算法，仅依赖二阶统计量完成三维重构。

Result: 实验结果表明，MoDM方法能够利用来自不同实验条件的数据集，实现更高质量的分子结构重构，相较于单一数据来源或传统方法有明确优势。

Conclusion: 该研究证明了融合和建模多样化数据集的重要性，为提升计算成像任务中的重建质量提供了新思路，对冷冻电镜和相关领域有较大应用价值。

Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.

</details>


### [3] [Modulo Video Recovery via Selective Spatiotemporal Vision Transformer](https://arxiv.org/abs/2511.07479)
*Tianyu Geng,Feng Ji,Wee Peng Tay*

Main category: cs.CV

TL;DR: 本文提出了首个用于模视频重建的深度学习框架SSViT，突破了传统HDR方法在模相机场景下的局限，实现了高质量、最先进的重建表现。


<details>
  <summary>Details</summary>
Motivation: 模相机通过对入射照度折叠入有限区间，解决了传统图像传感器在高动态场景下的饱和问题。但模恢复与传统HDR方法不同，前者需还原被折叠的真实值且尚缺乏高效深度学习方案。因此，亟需探索新型神经网络用于模视频重建以提升还原质量和效率。

Method: 作者提出了Selective Spatiotemporal Vision Transformer（SSViT），结合Token选择机制聚焦重要区域，并利用Transformer强大的空间与时序关系建模能力，有效处理模相机数据的“折叠”特性，实现高效视频重建。

Result: 实验结果表明，SSViT能够从8位折叠视频中重建出高质量的结果，并实现了目前最好的模视频恢复效果，超越了现有传统方法及其它深度网络。

Conclusion: SSViT框架验证了Transformer类神经网络在模视频重建领域的巨大潜力，为该方向的深度学习应用奠定了基础；未来可进一步优化用于各类成像场景。

Abstract: Conventional image sensors have limited dynamic range, causing saturation in high-dynamic-range (HDR) scenes. Modulo cameras address this by folding incident irradiance into a bounded range, yet require specialized unwrapping algorithms to reconstruct the underlying signal. Unlike HDR recovery, which extends dynamic range from conventional sampling, modulo recovery restores actual values from folded samples. Despite being introduced over a decade ago, progress in modulo image recovery has been slow, especially in the use of modern deep learning techniques. In this work, we demonstrate that standard HDR methods are unsuitable for modulo recovery. Transformers, however, can capture global dependencies and spatial-temporal relationships crucial for resolving folded video frames. Still, adapting existing Transformer architectures for modulo recovery demands novel techniques. To this end, we present Selective Spatiotemporal Vision Transformer (SSViT), the first deep learning framework for modulo video reconstruction. SSViT employs a token selection strategy to improve efficiency and concentrate on the most critical regions. Experiments confirm that SSViT produces high-quality reconstructions from 8-bit folded videos and achieves state-of-the-art performance in modulo video recovery.

</details>


### [4] [Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models](https://arxiv.org/abs/2511.07496)
*Barath Chandran. C,Srinivas Anumasa,Dianbo Liu*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型推理阶段，通过调整score函数的Laplacian值来减少“幻觉”样本（虚假、不协调样本）生成的方法，显著提升了样本质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优秀，但容易生成幻觉样本（如非现实、不连贯的图像），影响模型可靠性。已有研究认为原因是模式内插和score平滑化，但缺乏有效的实用防护方法。

Method: 该方法在推理采样阶段，对score函数进行事后调整，通过引入score的Laplacian（尖锐度）来抑制模式内插导致的幻觉。为高维数据高效逼近score的Laplacian，引入了一种基于有限差分和Hutchinson trace估算器的算法。

Result: 在1D、2D toy分布及高维图像数据集实验证明，所提校正策略明显减少了幻觉样本的比例，生成样本更加真实且连贯。

Conclusion: score的Laplacian可有效表征并抑制扩散模型中的幻觉现象，所提方法高效易用，对扩散模型的实际部署具有重要价值。

Abstract: Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.

</details>


### [5] [Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance](https://arxiv.org/abs/2511.07499)
*Kwanyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型指导方法ASAG，利用最优传输理论，通过Sinkhorn算法在自注意力中注入对抗性扰动，提高了文本到图像生成的质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有CFG等扩散模型指导方法通过人为加噪扰动无条件分布提升生成质量，但缺乏理论基础且依赖手工设计，难以系统提升生成效果。

Method: 利用最优传输理论，将扩散模型的注意力机制中的传输代价通过Sinkhorn算法进行刻意扰动，具体是在自注意力层中注入对抗性代价以削弱不良的attention对齐，从而优化生成。

Result: ASAG方法在文本到图像生成中取得了持续提升，并在IP-Adapter、ControlNet等下游任务中增强了可控性和保真度。该方法轻量、可即插即用，无需重新训练模型。

Conclusion: ASAG为扩散模型提供了一种有理论依据、实用性强的新型指导机制，显著提升生成质量，扩展了扩散模型的实际应用能力。

Abstract: Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.

</details>


### [6] [LiveNeRF: Efficient Face Replacement Through Neural Radiance Fields Integration](https://arxiv.org/abs/2511.07552)
*Tung Vu,Hai Nguyen,Cong Tran*

Main category: cs.CV

TL;DR: 本论文提出了LiveNeRF，一种支持实时、高质量人脸替换的新框架，具备33 FPS的实时性能，适用于直播、视频会议等场景。


<details>
  <summary>Details</summary>
Motivation: 当前人脸替换技术在可用性、实时性和视觉质量上的局限，影响了其在娱乐、教育、通信等领域的实际应用。

Method: 提出LiveNeRF框架，实现了比现有方法更高的实时性能（33 FPS）和更优的视觉质量，从而支持实际场景下的部署。

Result: LiveNeRF框架在保证优异可视质量的前提下，达到了33 FPS的实时性能，可用于直播、视频会议等场景。

Conclusion: LiveNeRF提升了人脸替换技术的实用性和视觉表现，为内容创作者、教育者和有语音障碍的人群带来便利。同时，作者强调了需采取用户同意验证和检测系统集成以避免技术滥用。

Abstract: Face replacement technology enables significant advancements in entertainment, education, and communication applications, including dubbing, virtual avatars, and cross-cultural content adaptation. Our LiveNeRF framework addresses critical limitations of existing methods by achieving real-time performance (33 FPS) with superior visual quality, enabling practical deployment in live streaming, video conferencing, and interactive media. The technology particularly benefits content creators, educators, and individuals with speech impairments through accessible avatar communication. While acknowledging potential misuse in unauthorized deepfake creation, we advocate for responsible deployment with user consent verification and integration with detection systems to ensure positive societal impact while minimizing risks.

</details>


### [7] [TrackStudio: An Integrated Toolkit for Markerless Tracking](https://arxiv.org/abs/2511.07624)
*Hristo Dimitrov,Giulia Dominijanni,Viktorija Pavalkyte,Tamar R. Makin*

Main category: cs.CV

TL;DR: TrackStudio集成了现有的开源工具，提供无需编程的2D/3D无标记运动跟踪GUI工具，实现一站式的自动化跟踪与分析，适合非专业用户。


<details>
  <summary>Details</summary>
Motivation: 虽然无标记运动跟踪技术进步显著，但现有工具多需要专业的技术背景，难以推广给非专业研究人员，缺乏一个简单易用且高度集成的解决方案。

Method: 开发了TrackStudio，将主流开源工具模块化整合进一个带图形界面的流水线，实现自动的2D与3D运动跟踪、标定、预处理、特征提取及可视化，并提供详细的用户指南。

Result: 在三种不同的实验环境、不同摄像机配置、76名参与者中测试，平均帧间相关性超过0.98，三角测量误差小于13.6mm（手部跟踪），展现出在各种复杂条件下高度稳定和准确的结果。方法亦可扩展到全身和面部跟踪。

Conclusion: TrackStudio为需要可靠无标记跟踪但缺乏专业知识的研究人员和使用者提供了一个低门槛、高性能、可扩展的解决方案，具有广泛的应用前景。

Abstract: Markerless motion tracking has advanced rapidly in the past 10 years and currently offers powerful opportunities for behavioural, clinical, and biomechanical research. While several specialised toolkits provide high performance for specific tasks, using existing tools still requires substantial technical expertise. There remains a gap in accessible, integrated solutions that deliver sufficient tracking for non-experts across diverse settings.
  TrackStudio was developed to address this gap by combining established open-source tools into a single, modular, GUI-based pipeline that works out of the box. It provides automatic 2D and 3D tracking, calibration, preprocessing, feature extraction, and visualisation without requiring any programming skills. We supply a user guide with practical advice for video acquisition, synchronisation, and setup, alongside documentation of common pitfalls and how to avoid them.
  To validate the toolkit, we tested its performance across three environments using either low-cost webcams or high-resolution cameras, including challenging conditions for body position, lightning, and space and obstructions. Across 76 participants, average inter-frame correlations exceeded 0.98 and average triangulation errors remained low (<13.6mm for hand tracking), demonstrating stable and consistent tracking. We further show that the same pipeline can be extended beyond hand tracking to other body and face regions. TrackStudio provides a practical, accessible route into markerless tracking for researchers or laypeople who need reliable performance without specialist expertise.

</details>


### [8] [Predicting Coronary Artery Calcium Severity based on Non-Contrast Cardiac CT images using Deep Learning](https://arxiv.org/abs/2511.07695)
*Lachlan Nguyen,Aidan Cousins,Arcot Sowmya,Hugh Dixson,Sonit Singh*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的CNN模型，可自动将心脏CT影像的冠状动脉钙化分数分为六个临床类别，表现出与当前半自动分法高度一致的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前冠状动脉钙化评分需由医生和技师采用半自动方法完成，耗时又依赖人工，亟需自动化、高效且准确的工具来提升工作效率并减轻医疗资源负担。

Method: 研究回顾性收集了68例患者的心脏CT影像及其相应的半自动CAC评分，将数据分为训练、验证和测试集。建立并训练了一个卷积神经网络（CNN）模型，以半自动CAC为参考标签，将影像分为六类。评估模型在六类任务中的表现，并与人工评分结果进行比较。

Result: 模型在六类CAC评分任务上表现优异，Cohen's kappa系数高达0.962，总体准确率达96.5%，具有良好泛化性。模型共误判32例，多为高估CAC等级。实验结果表明该CNN模型与半自动法高度一致，且对测试集有良好适应性。

Conclusion: 该研究验证了深度学习CNN模型可实现冠状动脉钙化六分类，准确性高，可为心血管风险分层提供高效的自动化工具，有望减少人工工作量，提升诊断效率。

Abstract: Cardiovascular disease causes high rates of mortality worldwide. Coronary artery calcium (CAC) scoring is a powerful tool to stratify the risk of atherosclerotic cardiovascular disease. Current scoring practices require time-intensive semiautomatic analysis of cardiac computed tomography by radiologists and trained radiographers. The purpose of this study is to develop a deep learning convolutional neural networks (CNN) model to classify the calcium score in cardiac, non-contrast computed tomography images into one of six clinical categories. A total of 68 patient scans were retrospectively obtained together with their respective reported semiautomatic calcium score using an ECG-gated GE Discovery 570 Cardiac SPECT/CT camera. The dataset was divided into training, validation and test sets. Using the semiautomatic CAC score as the reference label, the model demonstrated high performance on a six-class CAC scoring categorisation task. Of the scans analysed, the model misclassified 32 cases, tending towards overestimating the CAC in 26 out of 32 misclassifications. Overall, the model showed high agreement (Cohen's kappa of 0.962), an overall accuracy of 96.5% and high generalisability. The results suggest that the model outputs were accurate and consistent with current semiautomatic practice, with good generalisability to test data. The model demonstrates the viability of a CNN model to stratify the calcium score into an expanded set of six clinical categories.

</details>


### [9] [FlowFeat: Pixel-Dense Embedding of Motion Profiles](https://arxiv.org/abs/2511.07696)
*Nikita Araslanov,Anna Sonnweber,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlowFeat的高分辨率、多任务图像特征表征方法，显著提升了多项密集视觉任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有主流神经网络（如transformers）输出的特征分辨率较低，难以满足像视频对象分割、深度估计等需要高分辨率特征的密集视觉任务需求。作者希望解决视觉任务中通用高分辨率特征表征不足的问题。

Method: 提出了一种创新的蒸馏技术，将从光流网络和多样化视频数据中获得的显著运动分布（motion profiles）嵌入特征表征中。通过自监督训练框架，统计性地逼近图像中的表观运动，从而获得细致、丰富的空间和语义信息。该方法能对不同主流网络进行增强，并适配不同上采样策略。

Result: 实验证明，FlowFeat能有效增强五种主流编码器在视频对象分割、单目深度估计和语义分割这三项密集任务中的表现，且训练成本较低，对光流估计误差有较强鲁棒性，即便采用无监督光流网络也效果稳定。

Conclusion: FlowFeat为密集视觉任务提供了高效、可靠且多功能的高分辨率图像特征表示，为通用视觉表征进一步发展提供了新的方向。

Abstract: Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present FlowFeat, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or motion profiles. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.

</details>


### [10] [Cross Modal Fine-grained Alignment via Granularity-aware and Region-uncertain Modeling](https://arxiv.org/abs/2511.07710)
*Jiale Liu,Haoming Zhou,Yishu Zhu,Bingzhi Chen,Yuncheng Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，提升图像和文本之间细粒度对齐的准确性和鲁棒性，并在多个数据集上达到了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度的图像-文本对齐对于多模态学习任务（如视觉问答、图像描述等）十分关键，现有方法在注意力机制和跨模态关系建模方面存在不足，导致对复杂场景泛化能力差及细粒度不确定性建模不足。

Method: 提出一种同时考虑显著性和粒度的建模方式，并在区域级别引入不确定性建模。具体方法利用模态特有的偏置识别显著特征，避免依赖脆弱的跨模态注意力，且将区域特征表示为高斯混合分布以捕捉细粒度不确定性。

Result: 在Flickr30K和MS-COCO等数据集测试，所提方法在不同主干网络下均取得了最先进的性能，显著提高了细粒度图文对齐的鲁棒性和可解释性。

Conclusion: 该方法有效弥补了现有方法对细粒度对齐建模上的两大短板，推进了多模态领域的发展，在理论和实验层面均表现突出。

Abstract: Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.

</details>


### [11] [UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis](https://arxiv.org/abs/2511.07743)
*Yuezhe Yang,Wenjie Cai,Dexin Yang,Yufang Dong,Xingbo Dong,Zhe Jin*

Main category: cs.CV

TL;DR: 提出UltraGS，高斯泼洒框架，专为超声成像优化，在多个数据集上取得了当前最优性能并支持实时合成。


<details>
  <summary>Details</summary>
Motivation: 超声成像具有无创性但视野有限，传统合成新视角（novel view synthesis）方法效果不佳，因此需要专门针对超声影像优化的新方法。

Method: (1) 提出深度感知高斯泼洒策略，每个高斯分配一个可学习的视场，实现精确的深度预测与结构表达；(2) 设计SH-DARS渲染函数，结合低阶球谐函数与超声特有波物理（如深度衰减、反射、散射）建模组织亮度；(3) 发布了多样体检协议下采集的临床超声数据集作为基准。

Result: 在三个数据集上，UltraGS在PSNR（最高29.55）、SSIM（最高0.89）、MSE（最低0.002）等指标上达到SOTA，并能以64.69帧每秒实时生成。

Conclusion: UltraGS为超声视角合成提供了强大的新方法，兼顾结构、物理与临床可用性，且代码与数据集已开放。

Abstract: Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.

</details>


### [12] [VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics](https://arxiv.org/abs/2511.07744)
*Daniel Cher,Brian Wei,Srikumar Sastry,Nathan Jacobs*

Main category: cs.CV

TL;DR: 本文提出了VectorSynth，一个基于扩散模型的框架，可根据带有语义属性的多边形地理标注生成高精度卫星图像，并实现细粒度的空间编辑和语言引导下的图像生成。研究表明该方法在语义保真度和结构真实性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前卫星图像生成多依赖文本或布局等单一条件，难以同时准确对应地理空间和语义多样性要求。作者希望借助更细致的地理多边形标注和丰富的语义信息，实现高度契合实际需求的卫星影像合成与编辑。

Method: 提出了VectorSynth框架，结合扩散模型和视觉-语言对齐模块，将多边形几何及其语义属性转化为像素级嵌入，指导条件图像生成。模型支持地理信息与自然语言混合输入，可生成符合空间与语义约束的卫星图像，并允许交互式空间编辑。使用包含多样城市场景及像素级多边形标注的大规模数据集进行训练与评估。

Result: VectorSynth在基于语义和结构真实性的指标上显著提升，能细致地将地理多边形及语义信息转换为空间上精确且内容合理的卫星图像。与以往方法相比，在细粒度空间对齐及语义还原能力上均表现更优。

Conclusion: VectorSynth实现了多模式条件下的高精度卫星影像生成和编辑，极大拓展了地理智能和空间可视化的应用潜力。其代码及数据集公开，为相关领域研究和实际应用提供了重要资源。

Abstract: We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.

</details>


### [13] [Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs](https://arxiv.org/abs/2511.07748)
*Yuezhe Yang,Yiyue Guo,Wenjie Cai,Qingqing Ruan,Siying Wang,Xingbo Dong,Zhe Jin,Yong Dai*

Main category: cs.CV

TL;DR: 本论文提出了Auto-US，一个将超声视频数据与临床诊断文本结合的智能诊断系统，显著提升了超声影像分析的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助超声影像分析方法存在数据集多样性不足、诊断性能有限及临床实用性不足的问题。

Method: 作者建立了包含495个跨三类器官、五大类的超声视频的CUV数据集，设计了CTU-Net以实现超声视频的高效分类，并结合大语言模型实现诊断建议自动生成。

Result: CTU-Net在视频分类上取得了86.73%的准确率，Auto-US生成的诊断建议被临床专家评估为每例得分超过3（满分5分），体现了其较高的临床相关性。

Conclusion: Auto-US展现了优秀的诊断性能和潜在的临床应用价值，可推动超声视频智能诊断的实际落地。

Abstract: AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: https://github.com/Bean-Young/Auto-US.

</details>


### [14] [Class Incremental Medical Image Segmentation via Prototype-Guided Calibration and Dual-Aligned Distillation](https://arxiv.org/abs/2511.07749)
*Shengqian Zhu,Chengrong Yu,Qiang Wang,Ying Song,Guangjun Li,Jiafei Wu,Xiaogang Xu,Zhang Yi,Junjie Hu*

Main category: cs.CV

TL;DR: 提出了两种新的蒸馏方法（PGCD和DAPD）用于持续医学图像分割，解决旧知识遗忘问题，方法在多器官分割数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 连续医学图像分割任务中，需在学习新类别时保护已学类别的分割能力，现有方法对空间区域/特征处理不够细致或对局部原型利用不足，导致知识退化。

Method: 提出了PGCD和DAPD两种方法：PGCD通过原型与特征的相似度分配各区域蒸馏强度，强化可靠旧知识，抑制干扰信息；DAPD则使旧类别局部原型在现模型中既与全局原型也与局部原型对齐，提升旧类表现。

Result: 在两个主流多器官分割基准上，所提方法明显优于现有顶尖方法，展现出更强的健壮性和泛化性。

Conclusion: 提出的方法解决了CIMIS中旧知识保护不足的问题，并在实际评估中获得了更好的分割表现。

Abstract: Class incremental medical image segmentation (CIMIS) aims to preserve knowledge of previously learned classes while learning new ones without relying on old-class labels. However, existing methods 1) either adopt one-size-fits-all strategies that treat all spatial regions and feature channels equally, which may hinder the preservation of accurate old knowledge, 2) or focus solely on aligning local prototypes with global ones for old classes while overlooking their local representations in new data, leading to knowledge degradation. To mitigate the above issues, we propose Prototype-Guided Calibration Distillation (PGCD) and Dual-Aligned Prototype Distillation (DAPD) for CIMIS in this paper. Specifically, PGCD exploits prototype-to-feature similarity to calibrate class-specific distillation intensity in different spatial regions, effectively reinforcing reliable old knowledge and suppressing misleading information from old classes. Complementarily, DAPD aligns the local prototypes of old classes extracted from the current model with both global prototypes and local prototypes, further enhancing segmentation performance on old categories. Comprehensive evaluations on two widely used multi-organ segmentation benchmarks demonstrate that our method outperforms state-of-the-art methods, highlighting its robustness and generalization capabilities.

</details>


### [15] [Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks](https://arxiv.org/abs/2511.07755)
*Aja Khanal,Ahmed Faid,Apurva Narayan*

Main category: cs.CV

TL;DR: 提出一种新型视觉Transformer架构Filtered-ViT，结合空间自适应的SMART-VMF滤波，对多个局部干扰有强鲁棒性，在ImageNet与医学影像上效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有深度视觉系统容易被多个对抗补丁攻击，而主流防御方法大多假设单一补丁，这无法应对现实中多点局部扰动，迫切需要突破。

Method: 构建Filtered-ViT架构，将SMART Vector Median Filtering机制集成进Vision Transformer，实现空间自适应、多尺度的鲁棒局部噪声抑制，同时保持语义细节不丢失。

Result: 在ImageNet上应对LaVAN多补丁攻击时，Filtered-ViT在四处1%补丁下仍有79.8%的原始准确率与46.3%鲁棒准确率，超越现有防御。同时在医学影像（如X光片）中也能减缓遮挡与扫描噪声影响，而诊断信息不受损。

Conclusion: Filtered-ViT是首个同时对抗对抗与自然补丁擾动的transformer，展示了往高可靠视觉系统方向发展的可行路径。

Abstract: Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.

</details>


### [16] [Beyond Randomness: Understand the Order of the Noise in Diffusion](https://arxiv.org/abs/2511.07756)
*Song Yan,Min Li,Bi Xinliang,Jian Yang,Yusen Zhang,Guanye Xiong,Yunwei Lan,Tao Zhang,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种新的“语义抹除-注入”方法来调控文本驱动生成扩散模型中的初始噪声，显著提升了生成内容的可控性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有T2C扩散模型主要将生成语义归因于文本嵌入和注意力机制，而初始噪声通常被视为促进多样性的纯随机元素。然而，噪声对语义的实际影响缺乏深入探究。该文试图揭示噪声中的结构化语义信息，并提出通过调控噪声来优化生成过程的新方法。

Method: 作者首先系统性分析了随机噪声对扩散生成模型的影响，发现噪声中蕴含丰富语义且可通过基于信息论的方法简单有效地抹除不需要的语义。然后，借助扩散模型生成过程与语义注入的等价性，提出一种通用、免训练的两步“语义抹除-注入”流程，对初始噪声进行修正和语义注入。

Result: 实验证明，该方法在基于DiT与UNet架构的多种T2C模型中都取得了持续有效的优化效果，能够为扩散模型提供更一致、更可控的生成结果。

Conclusion: 本文发现并充分利用了T2C扩散模型初始噪声中的语义结构，提出的语义“抹除-注入”方法具有通用性和免额外训练的优点，为内容生成一致性与可控性提供了新的优化视角和有效工具。

Abstract: In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step "Semantic Erasure-Injection" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.

</details>


### [17] [Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval](https://arxiv.org/abs/2511.07780)
*Likang Peng,Chao Su,Wenyuan Wu,Yuan Sun,Dezhong Peng,Xi Peng,Xu Wang*

Main category: cs.CV

TL;DR: 提出了一种新方法SCBCH，通过跨模态语义一致性和软对比学习应对多标签数据中的标注噪声问题，实现更鲁棒的跨模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的跨模态哈希方法对高质量标注数据依赖性强，实际多标签数据中标注噪声普遍且严重影响检索效果，并且现有方法忽略了标签间部分语义重叠，导致泛化能力和鲁棒性不足。

Method: 提出语义一致双向对比哈希（SCBCH）框架，包括：1）跨模态语义一致分类（CSCC）模块，通过跨模态语义一致性估算样本可靠性，降低噪声标签影响；2）双向软对比哈希（BSCH）模块，结合多标签语义重叠动态生成软对比样本对，实现跨模态间自适应对比学习。

Result: 在四个主流跨模态检索基准上进行了实验，结果显示该方法在有噪声多标签情形下显著优于当前各种方法。

Conclusion: SCBCH能够有效提升跨模态检索在多标签、噪声标签场景下的鲁棒性和性能。

Abstract: Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.

</details>


### [18] [Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2511.07798)
*Runmin Cong,Anpeng Wang,Bin Wan,Cong Zhang,Xiaofei Zhou,Wei Zhang*

Main category: cs.CV

TL;DR: 提出了一种新方法DCDNet，通过有效解耦语义类别与领域特征，提高跨领域小样本分割的泛化能力和快速适应能力，实验效果显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域小样本分割任务中，现有方法的特征编码往往混杂了领域信息和类别信息，导致对新域和新类别的泛化和适应性不足。如何实现特征的更好解耦与融合，是进一步提升性能的关键难题。

Method: 方法上一方面，提出了“对抗-对比特征解耦（ACFD）”模块，通过对比学习和对抗学习，将编码器输出区分为类别相关的私有特征和领域相关的共享特征；另一方面，为防止特征解耦带来的结构信息损失，提出了“矩阵引导动态融合（MGDF）”模块，借助空间引导自适应融合不同类型特征；此外，在微调阶段引入“跨自适应调制（CAM）”模块，进一步提升融合效果和模型泛化能力。

Result: 在四个具有挑战性的数据集上，所提方法均大幅超越（set new SOTA）现有主流CD-FSS方法，表现出更强的跨域泛化与小样本适应能力。

Conclusion: DCDNet不仅有效解决了特征混杂带来的跨域与新类别适应问题，还通过多模块协同，提升了分割模型的泛化、适应和稳健性能，可为实际多域推理和小样本学习任务提供更优解决思路。

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to tackle the dual challenge of recognizing novel classes and adapting to unseen domains with limited annotations. However, encoder features often entangle domain-relevant and category-relevant information, limiting both generalization and rapid adaptation to new domains. To address this issue, we propose a Divide-and-Conquer Decoupled Network (DCDNet). In the training stage, to tackle feature entanglement that impedes cross-domain generalization and rapid adaptation, we propose the Adversarial-Contrastive Feature Decomposition (ACFD) module. It decouples backbone features into category-relevant private and domain-relevant shared representations via contrastive learning and adversarial learning. Then, to mitigate the potential degradation caused by the disentanglement, the Matrix-Guided Dynamic Fusion (MGDF) module adaptively integrates base, shared, and private features under spatial guidance, maintaining structural coherence. In addition, in the fine-tuning stage, to enhanced model generalization, the Cross-Adaptive Modulation (CAM) module is placed before the MGDF, where shared features guide private features via modulation ensuring effective integration of domain-relevant information. Extensive experiments on four challenging datasets show that DCDNet outperforms existing CD-FSS methods, setting a new state-of-the-art for cross-domain generalization and few-shot adaptation.

</details>


### [19] [Learning Sparse Label Couplings for Multilabel Chest X-Ray Diagnosis](https://arxiv.org/abs/2511.07801)
*Utkarsh Prakash Srivastava,Kaushik Gupta,Kaushik Nath*

Main category: cs.CV

TL;DR: 本文提出了一种基于SE-ResNeXt101的新多标签胸部X光片分类管线，并引入“Label-Graph Refinement”模块，在多个标签不均衡、共现关系复杂的现实场景下，显著提升了分类性能和实用性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光多标签分类任务面临类别极度不平衡以及疾病标签间复杂共现关系，传统方法难以兼顾分类性能和计算效率，因此有必要提出更有效且实用的解决方案。

Method: 以SE-ResNeXt101为骨干网络，采用MIS方法进行多标签分层交叉验证，损失函数选用Asymmetric Loss并辅以多种优化技巧（如混合精度、梯度裁剪、学习率衰减等）。创新性地提出Label-Graph Refinement模块，通过可训练稀疏互标签耦合矩阵，以单步消息传递方式细化分类结果。推理时采用TTA及深度集成方法提升稳健性。

Result: SE-ResNeXt101基线模型已取得92.64%的macro AUC，加上Label-Graph Refinement模块后，各折验证集的macro AUC进一步提升，计算资源需求基本不变。

Conclusion: 该方法可复现，硬件友好，无需额外标注信息，有效提高了多标签胸片分类器的性能，为临床应用提供了更优选择。

Abstract: We study multilabel classification of chest X-rays and present a simple, strong pipeline built on SE-ResNeXt101 $(32 \times 4d)$. The backbone is finetuned for 14 thoracic findings with a sigmoid head, trained using Multilabel Iterative Stratification (MIS) for robust cross-validation splits that preserve label co-occurrence. To address extreme class imbalance and asymmetric error costs, we optimize with Asymmetric Loss, employ mixed-precision (AMP), cosine learning-rate decay with warm-up, gradient clipping, and an exponential moving average (EMA) of weights. We propose a lightweight Label-Graph Refinement module placed after the classifier: given per-label probabilities, it learns a sparse, trainable inter-label coupling matrix that refines logits via a single message-passing step while adding only an L1-regularized parameter head. At inference, we apply horizontal flip test-time augmentation (TTA) and average predictions across MIS folds (a compact deep ensemble). Evaluation uses macro AUC averaging classwise ROC-AUC and skipping single-class labels in a fold to reflect balanced performance across conditions. On our dataset, a strong SE-ResNeXt101 baseline attains competitive macro AUC (e.g., 92.64% in our runs). Adding the Label-Graph Refinement consistently improves validation macro AUC across folds with negligible compute. The resulting method is reproducible, hardware-friendly, and requires no extra annotations, offering a practical route to stronger multilabel CXR classifiers.

</details>


### [20] [PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier](https://arxiv.org/abs/2511.07806)
*Shaomeng Wang,He Wang,Xiaolu Wei,Longquan Dai,Jinhui Tang*

Main category: cs.CV

TL;DR: 本论文提出PC-Diffusion框架，利用轻量的偏好分类器对扩散模型进行人类偏好对齐，实现了与DPO方法相当的效果但大幅降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有DPO等扩散模型偏好对齐方法需全模型微调，计算开销大且对参考模型敏感，表现不稳定。急需更加高效、稳定的偏好对齐新方案。

Method: 作者提出PC-Diffusion框架，通过训练偏好分类器来建模样本间相对偏好，将偏好学习与生成模型解耦，避免了全模型微调和依赖参考模型。同时，理论上证明了该方法的有效性，包括偏好引导分布跨时间步一致、无须参考模型等性质。

Result: 实验证明，PC-Diffusion能够大幅降低训练成本，在偏好一致性方面达到与DPO相当的效果，实现了高效、稳定的偏好引导生成。

Conclusion: PC-Diffusion作为一种高效、稳定的人类偏好对齐方法，有望替代现有高开销、敏感的偏好对齐技术，推动扩散模型在满足人类偏好方面的应用。

Abstract: Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.

</details>


### [21] [DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model](https://arxiv.org/abs/2511.07808)
*Zhongle Ren,Hui Ding,Kai Wang,Biao Hou,Xingyu Luo,Weibin Li,Licheng Jiao*

Main category: cs.CV

TL;DR: 本文提出了一种适用于SAR（合成孔径雷达）地物分类的通用基础模型，并设计了新的对比学习框架DI3CL，有效提升了模型的泛化能力和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有SAR地物分类方法主要依赖有监督学习，需要大量标注数据，这限制了方法的可扩展性和适应性，因此需要一种高效、通用且可适应不同场景的新方法。

Method: 作者提出了DI3CL对比学习预训练框架，包括动态实例（DI）模块和轮廓一致性（CC）模块。DI模块通过不同视角的局部一致性增强全局上下文感知，CC模块利用浅层特征帮助模型关注地物轮廓，提高结构判别力。同时，构建了包含46万张SAR图像的大规模SARSense数据集用于预训练。

Result: 在SAR地物分类、水体检测和道路提取等多项任务上进行了大量实验，结果显示DI3CL方法在这些任务中均优于现有方法。

Conclusion: DI3CL作为SAR地物分类的基础模型，具备良好的泛化能力与鲁棒性，可为下游多种SAR影像分析任务提供有力支持。代码和预训练权重已公开。

Abstract: Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: https://github.com/SARpre-train/DI3CL.

</details>


### [22] [Revisiting MLLM Based Image Quality Assessment: Errors and Remedy](https://arxiv.org/abs/2511.07812)
*Zhenchen Tang,Songlin Yang,Bo Peng,Zichuan Wang,Jing Dong*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Q-Scorer的框架，通过引入轻量级回归模块和IQA专用分数token，有效解决了多模态大语言模型在图像质量评估任务中将离散输出转换为连续分数时的误差问题，达到了最新最优的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图像质量评估中受到其输出为离散token，但评估任务需连续分数的天然不匹配影响，导致转换误差，并且语义token的引入（如“good”）产生混淆，极大制约了模型性能。

Method: 作者首先理论分析了现有方法的误差来源，并提出了Q-Scorer框架，在MLLM输出后增设轻量级回归模块，并引入专门用于评分的token，实现更精准的分数输出。

Result: Q-Scorer在多个主流IQA基准上达到了SOTA（最新最优）性能，对混合数据集有良好泛化性，与其他方法结合时还能进一步提升效果。

Conclusion: Q-Scorer有效缓解了离散与连续之间的转换误差问题，提升了多模态大语言模型在图像质量评估任务中的适用性和表现。

Abstract: The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.

</details>


### [23] [Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views](https://arxiv.org/abs/2511.07813)
*Haida Feng,Hao Wei,Zewen Xu,Haolin Wang,Chade Li,Yihong Wu*

Main category: cs.CV

TL;DR: 本文提出了Sparse3DPR，一种无需训练、基于大语言模型的3D场景理解新方法，使用稀疏视角RGB输入实现高效准确的场景推理。


<details>
  <summary>Details</summary>
Motivation: 当前很多3D场景理解方法依赖于大型训练，但训练自由（无需再训练的）方法在灵活性和泛化性上有明显优势。然而现有训练自由方法在实际应用中常出现效率和准确性不足的问题。

Method: 作者提出Sparse3DPR框架：构建层次化的平面增强场景图，以主导平面结构作为空间锚点，对场景中的元素做有条理的推理，并支持开放词汇表。该方法设计了自适应子图提取机制，可根据任务动态过滤无关信息，减少噪声，提高推理准确率和效率。同时仅需提供稀疏视角的RGB图片作为输入。

Result: 实验表明，Sparse3DPR在Space3D-Bench上相对于ConceptGraphs实现了28.7%的EM@1提升和78.2%的推理加速，在ScanQA数据集上可达训练型方法水准。实际落地实验也证实了方法的鲁棒性和泛化能力。

Conclusion: Sparse3DPR结合了训练自由方法的灵活性和高效场景推理，显著提升了3D场景理解的效率与准确率，并具备较强的泛化能力和实际应用潜力。

Abstract: Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.

</details>


### [24] [Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging](https://arxiv.org/abs/2511.07816)
*Jarett Dewbury,Chi-en Amy Tai,Alexander Wong*

Main category: cs.CV

TL;DR: 该论文提出利用合成相关扩散成像（CDI$^s$）提升前列腺癌病灶分割的深度学习模型效果，并对多种主流分割架构进行了系统评估。结果显示，CDI$^s$结合标准扩散序列可显著提升分割性能，具备临床即插即用的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习分割前列腺癌病灶的效果有限，主要受限于现有成像协议的数据表达能力。提高分割性能对临床诊断和治疗规划具有重要意义，因此探索新型成像数据的融合方法成为该研究的动机。

Method: 作者引入了一种合成相关扩散成像（CDI$^s$），并将其与现有的扩散加权成像（DWI）及表观扩散系数（ADC）序列结合。通过200例患者、6种主流分割网络的大规模实验，对各类数据融合方案进行了对比分析。

Result: CDI$^s$的引入在94%的测试配置中提升或保持分割性能，部分架构分割准确率（Dice 系数）相对提升幅度高达72.5%。CDI$^s$与DWI联合应用可在一半以上模型中带来显著提升，且完全无性能下降。

Conclusion: CDI$^s$不需额外扫描时间，无需改模型结构，作为一种即插即用的影像增强方式，能广泛提升前列腺癌分割性能，有望迅速应用于临床流程。

Abstract: Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.

</details>


### [25] [Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy](https://arxiv.org/abs/2511.07819)
*Gong Jingyu,Tong Kunkun,Chen Zhuoran,Yuan Chuanhan,Chen Mingang,Zhang Zhizhong,Tan Xin,Xie Yuan*

Main category: cs.CV

TL;DR: 该论文提出了SSOMotion框架，通过统一的场景语义占据（SSO）来提升3D场景中的人类动作合成。利用CLIP和降维获取语义特征，实验表明方法性能优异。


<details>
  <summary>Details</summary>
Motivation: 当前3D人类动作合成方法主要关注场景结构，缺乏对场景语义的理解，导致合成动作缺乏现实感和适应性。作者希望引入更丰富的场景语义特征以提升合成结果的质量。

Method: 提出SSOMotion框架，采用双向三平面分解以精简场景语义占据（SSO）表示，通过CLIP编码加线性降维获得统一的场景语义空间。结合指令提取动作方向，通过逐帧场景查询实现运动控制。

Result: 在ShapeNet、PROX及Replica三类复杂场景上进行了大量实验和消融分析。结果显示，SSOMotion在动作生成质量和泛化能力方面表现优异，优于现有方法。

Conclusion: 引入统一的场景语义占据表征极大提升了3D人类动作合成的表现力和泛化性，并显著减少计算冗余。相关代码将公开，便于学界和业界进一步研究。

Abstract: Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.

</details>


### [26] [CloudMamba: Grouped Selective State Spaces for Point Cloud Analysis](https://arxiv.org/abs/2511.07823)
*Kanglin Qu,Pan Gao,Qun Dai,Zhanzhi Ye,Rui Ye,Yuanhao Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云处理网络CloudMamba，通过改进序列化方法和状态空间模型，实现更高效和高精度的点云分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的点云处理方法存在点云序列化不完善、高层几何感知不足及S6模型过拟合等问题。

Method: 提出CloudMamba网络，采用轴向序列展开与序列合并实现更合理的点云序列化和特征融合；设计ChainedMamba结构以提升几何捕捉能力；提出GS6模型，通过参数共享缓解S6过拟合。

Result: 在多个点云任务实验上，CloudMamba以较低的计算复杂度显著优于现有方法，取得了新的最先进水平。

Conclusion: CloudMamba有效解决了点云分析中的关键问题，为基于SSM的点云理解提供了高效且表现优越的新框架。

Abstract: Due to the long-range modeling ability and linear complexity property, Mamba has attracted considerable attention in point cloud analysis. Despite some interesting progress, related work still suffers from imperfect point cloud serialization, insufficient high-level geometric perception, and overfitting of the selective state space model (S6) at the core of Mamba. To this end, we resort to an SSM-based point cloud network termed CloudMamba to address the above challenges. Specifically, we propose sequence expanding and sequence merging, where the former serializes points along each axis separately and the latter serves to fuse the corresponding higher-order features causally inferred from different sequences, enabling unordered point sets to adapt more stably to the causal nature of Mamba without parameters. Meanwhile, we design chainedMamba that chains the forward and backward processes in the parallel bidirectional Mamba, capturing high-level geometric information during scanning. In addition, we propose a grouped selective state space model (GS6) via parameter sharing on S6, alleviating the overfitting problem caused by the computational mode in S6. Experiments on various point cloud tasks validate CloudMamba's ability to achieve state-of-the-art results with significantly less complexity.

</details>


### [27] [MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection](https://arxiv.org/abs/2511.07862)
*Sunghun Yang,Minhyeok Lee,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TL;DR: MonoCLUE通过结合视觉特征聚类和场景记忆显著提升了单目3D物体检测的鲁棒性，尤其在遮挡和可见性差的情况下效果突出，在KITTI基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测因深度不准确和视野受限问题，导致在遮挡或截断场景下的检测准确率较低。现有方法虽然引入了深度信息，但忽略了对关键视觉线索的利用。作者旨在解决视觉线索不足带来的检测不稳定问题。

Method: 提出MonoCLUE方法，利用K-means对视觉特征进行聚类，提取不同物体部位的特征，实现更好的局部感知，并跨区域传播这些特征提升外观相似物体的检测。还通过聚合不同图像中的聚类特征，构建泛化的场景记忆，提高特征一致性。最终将局部聚类特征和场景记忆引入目标查询，引导注意力聚焦于有效区域。

Result: 利用统一的局部聚类和场景记忆策略，MonoCLUE在KITTI基准数据集上达到了最新最优表现，尤其在物体遮挡和部分可见情况下。

Conclusion: MonoCLUE证明了结合视觉特征聚类与场景级记忆能够提升单目3D检测在复杂环境下的鲁棒性和准确率，对实际自动驾驶任务具有重要意义。

Abstract: Monocular 3D object detection offers a cost-effective solution for autonomous driving but suffers from ill-posed depth and limited field of view. These constraints cause a lack of geometric cues and reduced accuracy in occluded or truncated scenes. While recent approaches incorporate additional depth information to address geometric ambiguity, they overlook the visual cues crucial for robust recognition. We propose MonoCLUE, which enhances monocular 3D detection by leveraging both local clustering and generalized scene memory of visual features. First, we perform K-means clustering on visual features to capture distinct object-level appearance parts (e.g., bonnet, car roof), improving detection of partially visible objects. The clustered features are propagated across regions to capture objects with similar appearances. Second, we construct a generalized scene memory by aggregating clustered features across images, providing consistent representations that generalize across scenes. This improves object-level feature consistency, enabling stable detection across varying environments. Lastly, we integrate both local cluster features and generalized scene memory into object queries, guiding attention toward informative regions. Exploiting a unified local clustering and generalized scene memory strategy, MonoCLUE enables robust monocular 3D detection under occlusion and limited visibility, achieving state-of-the-art performance on the KITTI benchmark.

</details>


### [28] [Visual Bridge: Universal Visual Perception Representations Generating](https://arxiv.org/abs/2511.07877)
*Yilin Gao,Shuguang Dou,Junzhou Li,Zhiheng Yu,Yin Li,Dongsheng Jiang,Shugong Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的视觉感知框架，能够在多个任务之间实现高效的表示迁移，并在多项视觉任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型多为“单任务单模型”范式，在多任务场景下泛化性和可扩展性不足。而大语言模型的跨领域泛化能力为视觉领域的通用解法提供了启发。作者希望打破单一任务限制，实现视觉模型的多任务泛化能力。

Method: 作者提出以flow matching为基础的通用视觉感知框架，将从图像patch token到任务特定表示的过程建模为统一的流匹配问题，并引入自监督基础模型作为锚点，同时设计多尺度、循环的任务嵌入机制，从而学习一个能链接不同任务的通用速度场，实现高效的跨任务表现力传递。

Result: 该方法在分类、检测、分割、深度估计、图文检索等任务上，通过零样本和微调两种设定，均取得了对现有通用模型和部分专用模型的超越表现。消融实验进一步证明了模型的鲁棒性、可扩展性和泛化性。

Conclusion: 该工作为未来的通用视觉建模奠定了基础，是向大一统视觉感知迈出的重要一步。

Abstract: Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.

</details>


### [29] [Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level](https://arxiv.org/abs/2511.07889)
*Sicong Zang,Shuhui Gao,Zhijun Fang*

Main category: cs.CV

TL;DR: 本文提出了一种层次化自回归的素描生成方法，可以在素描生成过程中灵活且逐步地进行控制与干预。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能在素描生成开始前统一设定所有笔画的特征条件，生成过程中无法进一步操控，这限制了生成的灵活性与定制化操作能力。

Method: 作者提出了一种三阶段、层次化的自回归生成流程：首先预测下一个要画的笔画的嵌入表示，然后决定该笔画在画布上的锚定位置，最后将该嵌入翻译为一系列绘图动作。整个流程自回归进行，每一步都会参考已生成的笔画及其位置。

Result: 这种方法允许在生成素描的任意时刻，灵活调整和编辑单个笔画的嵌入，从而实现更自由的草图操控和更具交互性的生成。

Conclusion: 层次化自回归架构使得素描生成过程可控性大幅提高，实现了对单笔画、多层级的灵活编辑，显著优于以往只可预先整体设定的方案。

Abstract: Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.

</details>


### [30] [Theoretical Analysis of Power-law Transformation on Images for Text Polarity Detection](https://arxiv.org/abs/2511.07916)
*Narendra Singh Yadav,Pavan Kumar Perepu*

Main category: cs.CV

TL;DR: 本论文针对文本极性检测及二值化进行了理论分析，并对已有基于幂律变换的方法进行了理论解释和验证。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉任务如车牌识别、验证码识别和字符识别等都需要先将图像二值化，而这一过程依赖于对文本极性的判断。已有研究发现幂律变换对两类极性（暗文本/亮背景与亮文本/暗背景）的类间方差有不同影响，但仅有实证解释，缺乏理论分析。

Method: 文章首先回顾了幂律变换下，图像直方图中类别间方差随文本极性的变化情况，随后对这种现象进行了理论推导，揭示其背后的统计与数学原理。

Result: 作者提出并证明了幂律变换后，暗文本/亮背景情况下类间方差增加，亮文本/暗背景情况下类间方差减少的理论基础，并给出了分析过程。

Conclusion: 本文为基于极性的二值化方法提供了理论支撑，有助于更好地理解幂律变换在文本识别等应用中的效果，并为后续相关算法设计提供了理论依据。

Abstract: Several computer vision applications like vehicle license plate recognition, captcha recognition, printed or handwriting character recognition from images etc., text polarity detection and binarization are the important preprocessing tasks. To analyze any image, it has to be converted to a simple binary image. This binarization process requires the knowledge of polarity of text in the images. Text polarity is defined as the contrast of text with respect to background. That means, text is darker than the background (dark text on bright background) or vice-versa. The binarization process uses this polarity information to convert the original colour or gray scale image into a binary image. In the literature, there is an intuitive approach based on power-law transformation on the original images. In this approach, the authors have illustrated an interesting phenomenon from the histogram statistics of the transformed images. Considering text and background as two classes, they have observed that maximum between-class variance between two classes is increasing (decreasing) for dark (bright) text on bright (dark) background. The corresponding empirical results have been presented. In this paper, we present a theoretical analysis of the above phenomenon.

</details>


### [31] [An Image-Based Path Planning Algorithm Using a UAV Equipped with Stereo Vision](https://arxiv.org/abs/2511.07928)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: 提出一种基于计算机视觉的图像路径规划算法，并与A*和PRM对比，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 二维图像难以区分地形的坑洼和高低，传统算法不够准确，需要更精确的路径规划方法。

Method: 使用无人机拍摄地形，生成视差图；通过边缘、线、角点检测和立体深度重建，自动检测起止点，确定路径候选点，并与A*和PRM算法在仿真和实验环境下对比。

Result: 所提算法在虚拟和物理环境中的实验结果显示，性能优于对比算法，具有良好的应用前景。

Conclusion: 基于视觉的路径规划算法能更准确地反映地形信息，提升路径安全性和规划质量，实验验证了其实用性。

Abstract: This paper presents a novel image-based path planning algorithm that was developed using computer vision techniques, as well as its comparative analysis with well-known deterministic and probabilistic algorithms, namely A* and Probabilistic Road Map algorithm (PRM). The terrain depth has a significant impact on the calculated path safety. The craters and hills on the surface cannot be distinguished in a two-dimensional image. The proposed method uses a disparity map of the terrain that is generated by using a UAV. Several computer vision techniques, including edge, line and corner detection methods, as well as the stereo depth reconstruction technique, are applied to the captured images and the found disparity map is used to define candidate way-points of the trajectory. The initial and desired points are detected automatically using ArUco marker pose estimation and circle detection techniques. After presenting the mathematical model and vision techniques, the developed algorithm is compared with well-known algorithms on different virtual scenes created in the V-REP simulation program and a physical setup created in a laboratory environment. Results are promising and demonstrate effectiveness of the proposed algorithm.

</details>


### [32] [Exploring the Underwater World Segmentation without Extra Training](https://arxiv.org/abs/2511.07923)
*Bingyu Li,Tao Huo,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了首个大规模细粒度水下生物分割数据集AquaOV255和对应的开放词汇分割基准测试UOVSBench，并提出无训练水域领域的开放词汇分割框架Earth2Ocean，显著推动水下场景目标分割研究。


<details>
  <summary>Details</summary>
Motivation: 现有目标分割数据集和模型主要集中于陆地场景，而水下生物的精确分割对生物多样性监测和生态评估至关重要，水下领域却缺乏相关大规模数据集和开放词汇分割方法。

Method: 1. 构建AquaOV255水下分割数据集，涵盖255类生物、2万多张图片。2. 整合六个水下数据集建立UOVSBench开放词汇分割评测基准。3. 提出Earth2Ocean框架，通过几何引导的视觉掩膜生成器（GMG）和类别-视觉语义对齐模块（CSA），实现依靠VLMs无训练迁移，从陆地模型迁移到水下。

Result: 在UOVSBench基准上，Earth2Ocean无需额外水下训练即取得了较高的开放词汇分割精度，并且推理效率优秀，显著优于现有方法。

Conclusion: AquaOV255与UOVSBench填补了水下分割领域公开数据和评测体系空白，Earth2Ocean为水下开放词汇分割提供了高效、无训练的新范式，对生物监测等实际应用具有重大意义。

Abstract: Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.

</details>


### [33] [HD$^2$-SSC: High-Dimension High-Density Semantic Scene Completion for Autonomous Driving](https://arxiv.org/abs/2511.07925)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 论文提出一种新的基于摄像头的3D语义场景补全框架HD$^2$-SSC，有效弥补2D图像与3D体素之间的表达和标注密度差距，在主流数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义场景补全方法在二维（2D）输入特征和三维（3D）输出预测间存在维度差距，同时用于训练的稀疏标注与现实中密集场景之间存在密度差距，导致模型难以精确还原真实世界的三维视角。

Method: 作者提出HD$^2$-SSC框架，其中包括两个核心模块：1)高维度语义解耦模块（高维解耦像素语义，剥离遮挡因素，丰富细粒度区域语义信息，以补足特征维度差）；2)高密度占据精修模块（通过“检测-精修”结构，利用几何和语义上下文补全部分缺失体素并修正预测错误，增强3D语义密度）。

Result: 在SemanticKITTI和SSCBench-KITTI-360等主流数据集上进行了大量实验，验证了所提HD$^2$-SSC方法在3D场景补全精度和密度上的提升效果。

Conclusion: HD$^2$-SSC框架能够有效缓解输入输出维度差及标注密度差问题，提升基于视觉的自动驾驶3D场景理解能力。

Abstract: Camera-based 3D semantic scene completion (SSC) plays a crucial role in autonomous driving, enabling voxelized 3D scene understanding for effective scene perception and decision-making. Existing SSC methods have shown efficacy in improving 3D scene representations, but suffer from the inherent input-output dimension gap and annotation-reality density gap, where the 2D planner view from input images with sparse annotated labels leads to inferior prediction of real-world dense occupancy with a 3D stereoscopic view. In light of this, we propose the corresponding High-Dimension High-Density Semantic Scene Completion (HD$^2$-SSC) framework with expanded pixel semantics and refined voxel occupancies. To bridge the dimension gap, a High-dimension Semantic Decoupling module is designed to expand 2D image features along a pseudo third dimension, decoupling coarse pixel semantics from occlusions, and then identify focal regions with fine semantics to enrich image features. To mitigate the density gap, a High-density Occupancy Refinement module is devised with a "detect-and-refine" architecture to leverage contextual geometric and semantic structures for enhanced semantic density with the completion of missing voxels and correction of erroneous ones. Extensive experiments and analyses on the SemanticKITTI and SSCBench-KITTI-360 datasets validate the effectiveness of our HD$^2$-SSC framework.

</details>


### [34] [Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification](https://arxiv.org/abs/2511.07929)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP的联邦学习（FedMedCLIP）方法，有效提升了医疗图像分类的准确性与效率，同时降低了数据传输和计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在医疗影像领域表现优异，但需集中数据训练，存在隐私问题。联邦学习可缓解数据共享难题，但数据异构和资源消耗阻碍了实际应用，特别是在视觉语言模型的场景下。

Method: 作者提出了FedMedCLIP方法，主要包括：1）引入掩码特征自适应模块（FAM），作为通信模块，减少传输负载；2）通过冻结CLIP编码器降低计算开销；3）提出掩码多层感知机（MLP）作为本地私有分类器，适应不同客户端任务；4）设计自适应KL散度蒸馏正则方法，实现FAM与MLP之间的互学习；5）采用模型压缩方式传输FAM参数，并利用集成预测进行最终分类。

Result: 在四个公开的医疗数据集上，FedMedCLIP取得了优异表现。例如，在ISIC2019数据集上，比次优基线高出8%的性能，资源消耗显著降低（如比FedAVG快120倍）。

Conclusion: FedMedCLIP方法在保证数据隐私和资源高效利用的前提下，显著提升了医疗图像分类的准确性和可扩展性，为医疗领域联邦学习落地提供了新思路。

Abstract: Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).

</details>


### [35] [Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers](https://arxiv.org/abs/2511.07934)
*Sida Huang,Siqi Huang,Ping Luo,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种提升文生图扩散模型空间可控性的布局生成方法，包括LaySyn数据集和Laytrol网络，能够更好地保持预训练知识，实现高质量、风格一致的布局到图像生成。


<details>
  <summary>Details</summary>
Motivation: 融合布局条件的文生图任务中，现有方法常导致生成结果质量下降和风格失真，主要源于适配器引入造成的预训练知识损失。这亟需一种既保留原模型知识，又能准确空间控制的新方法。

Method: 1. 构建LaySyn数据集，用基础生成模型自身合成的图像，缓解预训练与下游布局任务间的分布偏移。2. 提出Laytrol网络，从MM-DiT继承全部参数以保持其预训练知识。3. 采用专门初始化方案：布局编码器先作为文本编码器初始化以保证输出在数据域内，网络输出初始化为零。4. 加入Object-level Rotary Position Embedding为布局标记提供位置信息。

Result: 通过定性和定量实验，方法在视觉质量、空间一致性、风格保持等方面均优于现有布局到图像生成方案。

Conclusion: 提出的方法能有效提升布局到图像生成中空间控制的质量和风格一致性，并保持基础模型的知识和能力。

Abstract: With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.

</details>


### [36] [DiffRegCD: Integrated Registration and Change Detection with Diffusion Features](https://arxiv.org/abs/2511.07935)
*Seyedehnanita Madani,Rama Chellappa,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffRegCD的新型方法，将图像配准和变化检测整合在同一模型内，有效应对遥感图像中的大视角、时间跨度及未对齐问题，在多项遥感和地面场景数据集上优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法大多要求输入图像严格配准，但实际应用中常见的视角差异、大时间跨度及影像未对齐会导致传统两阶段配准-检测方法及现有联合框架性能下降。因此，需要一种能够自适应大范围不对齐情况下进行鲁棒变化检测的新方法。

Method: DiffRegCD将致密配准与变化检测统一建模，用高斯平滑的分类任务重塑对应关系估计，达到了像素级精度和稳定训练。方法利用预训练去噪扩散模型的多尺度特征，增强对光照和视角变化的鲁棒性，并用可控仿射扰动制造带标签训练样本，无需伪标签。数据集覆盖了多种空中和地面场景。

Result: 大量实验证明，DiffRegCD在LEVIR-CD、DSIFN-CD、WHU-CD、SYSU-CD和VL-CMU-CD等公开数据集上均明显优于最新主流方法，即便在大幅度时间和几何变化下仍表现出高度鲁棒性。

Conclusion: DiffRegCD证明了基于扩散特征和分类对应的新范式在统一变化检测上的高效、可靠性，为应对现实遥感影像中的大幅不对齐难题提供了有力解决方案。

Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.

</details>


### [37] [Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?](https://arxiv.org/abs/2511.07940)
*Rui-Qing Sun,Ang Li,Zhijing Wu,Tian Lan,Qianyu Lu,Xingshan Yao,Chen Xu,Xian-Ling Mao*

Main category: cs.CV

TL;DR: 本论文提出了ISExplore方法，通过自动选取信息量丰富的5秒参考视频片段，大幅提升了基于NeRF和3DGS的说话人脸生成任务的数据处理和训练效率，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 当前说话人脸生成对参考视频的长度要求高，需耗时处理和拟合数分钟视频，导致实际应用受限。探索是否仅需短信息丰富的视频片段即可取得同等甚至更佳效果。

Method: 提出ISExplore方法，基于音频特征多样性、唇动幅度和摄像机视角数三个数据质量维度，自动选择最具信息量的5秒参考视频片段，用于NeRF或3DGS模型训练。

Result: 通过实验验证，ISExplore使数据处理及训练速度提升5倍以上，同时输出的视频质量与使用全部参考视频时持平甚至更优。

Conclusion: 信息量丰富的视频片段比视频长度更关键。针对说话人脸生成，精选短片段可有效提升效率，为相关领域应用广泛铺路。

Abstract: Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.

</details>


### [38] [Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2511.07941)
*Zhenfeng Zhuang,Fangyu Zhou,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态原型驱动的多实例学习方法，通过引入双向信息交互和高维语义对齐，提高了大模型在病理任务中的泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在病理影像分析中受限于巨量数据带来的计算压力与标签粒度低的问题（只有包级标签，缺乏实例级标注），同时描述的专业性不足，难以有效提升疾病识别的准确性与解释性。

Method: 方法上，作者提出构建任务相关的病理实体原型作为视觉—文本双模态信息桥梁：利用冻结的大语言模型生成文本描述作为文本原型，同时视觉分支生成实例原型并减少冗余信息，最终采用立体最优传输（SOT）算法实现高维空间的跨模态对齐。整个流程促进了双向语义互动并优化了信息压缩。

Result: 在三个不同癌症数据集上，方法进行了小样本分类和可解释性实验。实验结果显示，该方法优于现有方法，表现出更强的泛化能力和可解释性。

Conclusion: 多模态原型驱动的多实例学习，通过促进视觉与文本信息的高效互动与高维对齐，有效增强了大模型在病理任务中的效果，对于计算病理学的发展具有重要意义。

Abstract: While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.

</details>


### [39] [ReIDMamba: Learning Discriminative Features with Visual State Space Model for Person Re-Identification](https://arxiv.org/abs/2511.07948)
*Hongyang Gu,Qisong Yang,Lei Pu,Siming Han,Yao Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Mamba架构的行人重识别新框架ReIDMamba，通过引入多类别token和新模块，有效提升了特征判别性和泛化能力，并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer方法在ReID任务中弥补了一些CNN的局限，但计算和显存开销随输入序列长度急剧增加，影响可扩展性。作者希望用更高效的架构改进这一点。

Method: 提出ReIDMamba（纯Mamba架构），核心创新包括：1）设计基于多类别token的强基线，提取细粒度判别特征；2）MGFE模块，通过多分支结构和类别token融合提升多粒度特征表达；3）RATR正则项，减少多分支特征冗余，强化类内与类间多样性，从而提高鲁棒性。

Result: ReIDMamba参数量仅为TransReID的1/3，显著降低显存需求和推理延迟，在五个主流行人重识别数据集上性能优于现有方法。

Conclusion: ReIDMamba是首个采用纯Mamba驱动的ReID框架，在性能、效率和资源消耗方面实现了全面突破，对后续相关研究具有较高的参考价值。

Abstract: Extracting robust discriminative features is a critical challenge in person re-identification (ReID). While Transformer-based methods have successfully addressed some limitations of convolutional neural networks (CNNs), such as their local processing nature and information loss resulting from convolution and downsampling operations, they still face the scalability issue due to the quadratic increase in memory and computational requirements with the length of the input sequence. To overcome this, we propose a pure Mamba-based person ReID framework named ReIDMamba. Specifically, we have designed a Mamba-based strong baseline that effectively leverages fine-grained, discriminative global features by introducing multiple class tokens. To further enhance robust features learning within Mamba, we have carefully designed two novel techniques. First, the multi-granularity feature extractor (MGFE) module, designed with a multi-branch architecture and class token fusion, effectively forms multi-granularity features, enhancing both discrimination ability and fine-grained coverage. Second, the ranking-aware triplet regularization (RATR) is introduced to reduce redundancy in features from multiple branches, enhancing the diversity of multi-granularity features by incorporating both intra-class and inter-class diversity constraints, thus ensuring the robustness of person features. To our knowledge, this is the pioneering work that integrates a purely Mamba-driven approach into ReID research. Our proposed ReIDMamba model boasts only one-third the parameters of TransReID, along with lower GPU memory usage and faster inference throughput. Experimental results demonstrate ReIDMamba's superior and promising performance, achieving state-of-the-art performance on five person ReID benchmarks. Code is available at https://github.com/GuHY777/ReIDMamba.

</details>


### [40] [Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks](https://arxiv.org/abs/2511.07958)
*Xiaoye Liang,Lai Jiang,Minglang Qiao,Yichen Guo,Yue Zhang,Xin Deng,Shengxi Li,Yufan Liu,Mai Xu*

Main category: cs.CV

TL;DR: 本文提出了序列图片质量评估任务（BuIQA），包括第一个BuIQA基准数据集和一种统一的框架，能有效选择高质量爆发图片，提升后续任务效果。


<details>
  <summary>Details</summary>
Motivation: 爆发图片（Burst Images）冗余度高，导致存储、传输负担重，并降低图像后处理任务的效率。缺乏能根据具体任务需求自动筛选高质量帧的方法。

Method: 1）建立了包含7,346组序列、45,827张图片、191,572条多场景质量标注的新型数据集。2）提出统一BuIQA框架，包括：a. 基于任务驱动的提示生成网络，通过异构知识蒸馏获取下游任务先验信息；b. 基于任务提示的图片质量评估网络，实现与任务相关的高效图片筛选。

Result: 实验覆盖10种下游场景，方法在BuIQA任务上显著优于当前最佳方案。同时，在去噪和超分辨任务中，基于筛选结果能提升0.33dB的PSNR。

Conclusion: 所提方法能高效在不同应用场景下评估和筛选高质量爆发图片，提升后续任务效果，推动爆发图像智能处理发展。

Abstract: In recent years, the development of burst imaging technology has improved the capture and processing capabilities of visual data, enabling a wide range of applications. However, the redundancy in burst images leads to the increased storage and transmission demands, as well as reduced efficiency of downstream tasks. To address this, we propose a new task of Burst Image Quality Assessment (BuIQA), to evaluate the task-driven quality of each frame within a burst sequence, providing reasonable cues for burst image selection. Specifically, we establish the first benchmark dataset for BuIQA, consisting of $7,346$ burst sequences with $45,827$ images and $191,572$ annotated quality scores for multiple downstream scenarios. Inspired by the data analysis, a unified BuIQA framework is proposed to achieve an efficient adaption for BuIQA under diverse downstream scenarios. Specifically, a task-driven prompt generation network is developed with heterogeneous knowledge distillation, to learn the priors of the downstream task. Then, the task-aware quality assessment network is introduced to assess the burst image quality based on the task prompt. Extensive experiments across 10 downstream scenarios demonstrate the impressive BuIQA performance of the proposed approach, outperforming the state-of-the-art. Furthermore, it can achieve $0.33$ dB PSNR improvement in the downstream tasks of denoising and super-resolution, by applying our approach to select the high-quality burst frames.

</details>


### [41] [Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection](https://arxiv.org/abs/2511.07966)
*Shenao Zhao,Pengpeng Liang,Zhoufan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMAssist的多模态辅助方法，用于提升基于LiDAR的3D目标检测在无监督领域自适应场景下的表现。通过引入图像与文本特征在源域与目标域之间对齐3D特征，并融合多模态信息，实现了对主流方法的性能超越。


<details>
  <summary>Details</summary>
Motivation: 尽管现实中常常同时采集点云与图像数据，但现有3D无监督领域自适应方法很少利用图像信息来提升检测效果。因此，作者希望通过融合多模态信息提升3D目标检测的跨域鲁棒性与精度。

Method: 该方法首先将3D框投影到图像中获得2D框，然后分别提取图像特征（通过预训练视觉骨干网络）与文本特征（通过LVLM+文本编码器）作为中介，将3D特征与其对齐并加权融合，最后进行检测预测。此外，通过2D目标检测器辅助生成伪标签，进一步提升伪标签质量。

Result: 在三个主流3D目标检测跨域自适应数据集及任务上，MMAssist均取得了优于现有主流方法的检测性能。

Conclusion: 融合多模态（图像+文本）信息，能显著提升点云3D目标检测模型在无监督跨域场景下的性能，是一种有效且具备推广性的改进方案。

Abstract: Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.

</details>


### [42] [Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection](https://arxiv.org/abs/2511.07976)
*Seyedehanita Madani,Vishal M. Patel*

Main category: cs.CV

TL;DR: 该论文提出了一种模块化的遥感变化检测流程，通过扩散语义变形、中密度配准和残差流细化，显著提升了时空鲁棒性，无需修改现有检测网络即可提升配准与检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的变化检测方法对于影像空间对齐非常敏感，实际应用中由于长时间间隔造成影像错位，导致检测效果下降。现有联合配准-检测方法泛化性弱，需要重训练，限制了其实际应用。

Method: 作者提出一个模块化流程：利用扩散模块合成中间变形帧，逐步桥接双时相影像的外观差异；RoMa模块估算连续帧对应关系；最后通过轻量化U-Net对组合流进行细化，实现高保真的配准。

Result: 在LEVER-CD、WHU-CD和DSIFN-CD等数据集上，方法在多种主干网络下均显著提升了配准精度及变化检测能力，优于现有方法。

Conclusion: 该流程无需改动检测网络本身，通过前置配准增强注册与检测鲁棒性，具有良好的通用性和有效性，有助于遥感变化检测在实际复杂环境中的推广应用。

Abstract: Remote sensing change detection is often challenged by spatial misalignment between bi-temporal images, especially when acquisitions are separated by long seasonal or multi-year gaps. While modern convolutional and transformer-based models perform well on aligned data, their reliance on precise co-registration limits their robustness in real-world conditions. Existing joint registration-detection frameworks typically require retraining and transfer poorly across domains. We introduce a modular pipeline that improves spatial and temporal robustness without altering existing change detection networks. The framework integrates diffusion-based semantic morphing, dense registration, and residual flow refinement. A diffusion module synthesizes intermediate morphing frames that bridge large appearance gaps, enabling RoMa to estimate stepwise correspondences between consecutive frames. The composed flow is then refined through a lightweight U-Net to produce a high-fidelity warp that co-registers the original image pair. Extensive experiments on LEVIR-CD, WHU-CD, and DSIFN-CD show consistent gains in both registration accuracy and downstream change detection across multiple backbones, demonstrating the generality and effectiveness of the proposed approach.

</details>


### [43] [DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion](https://arxiv.org/abs/2511.07978)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出了一种称为DANCE的新型点云补全框架，能够适应不同密度和类别信息，仅补全缺失区域且保留原有结构。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法通常假设输入/输出密度固定或依赖图像表示，限制了在稀疏、密度变化或弱监督等实际场景下的应用。因此，需要开发对密度和类别更具鲁棒性的方法。

Method: 提出DANCE框架，通过多视角射线采样生成候选点，利用transformer解码器优化点的位置并预测不透明度分数，决定每个点是否属于最终表面。还添加了一个轻量级分类头，实现端到端的几何与类别联合引导，无需外部图像监督。

Result: 在PCN和MVP基准上，DANCE在准确性和结构一致性方面超越现有方法，且对输入密度和噪声具备较高鲁棒性。

Conclusion: DANCE框架有效实现了密度不可知、类别感知的点云补全，提升了方法的通用性和实际应用能力。

Abstract: Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.

</details>


### [44] [ChexFract: From General to Specialized - Enhancing Fracture Description Generation](https://arxiv.org/abs/2511.07983)
*Nikolay Nechaev,Evgeniia Przhezdzetskaia,Dmitry Umerenkov,Dmitry V. Dylov*

Main category: cs.CV

TL;DR: 本文针对放射科报告自动生成中对罕见但临床重要病变（如骨折）描述不足的问题，提出了专门的骨折检测及描述模型，并显著提升了报告准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然当前视觉-语言模型在放射科报告自动生成领域表现优异，但对于骨折等罕见、重要病变的描述仍然不充分。准确报告这些病变对于临床诊断具有重要意义，因此需要针对这类任务开发更专业的模型。

Method: 作者以MAIRA-2和CheXagent的编码器为基础，训练专门针对骨折的视觉-语言模型，同时从骨折类型、部位和发生时间等维度对模型输出进行分析。

Result: 专用骨折模型在骨折描述的准确性上相较于通用模型有明显提升。同时，分析也揭示了目前模型结构在不同骨折情境下的优势和不足。

Conclusion: 提出的骨折专用报告生成模型显著提高了罕见病变的自动报告效果。最优模型已公开发布，有助于推动罕见疾病自动诊断和报告方面的研究进展。

Abstract: Generating accurate and clinically meaningful radiology reports from chest X-ray images remains a significant challenge in medical AI. While recent vision-language models achieve strong results in general radiology report generation, they often fail to adequately describe rare but clinically important pathologies like fractures. This work addresses this gap by developing specialized models for fracture pathology detection and description. We train fracture-specific vision-language models with encoders from MAIRA-2 and CheXagent, demonstrating significant improvements over general-purpose models in generating accurate fracture descriptions. Analysis of model outputs by fracture type, location, and age reveals distinct strengths and limitations of current vision-language model architectures. We publicly release our best-performing fracture-reporting model, facilitating future research in accurate reporting of rare pathologies.

</details>


### [45] [CSF-Net: Context-Semantic Fusion Network for Large Mask Inpainting](https://arxiv.org/abs/2511.07987)
*Chae-Yeon Heo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义引导的大面积图像修补框架，通过融合上下文和结构化语义信息，有效提升缺失区域修补的结构准确性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 大面积图像修补任务中，原始图像缺失内容多、上下文信息不足，现有方法常导致修补区域的结构错乱或语义不一致，亟需融合更强的结构和语义先验以提升修补表现。

Method: 首先利用预训练的非现显（Amodal Completion, AC）模型为缺失区域生成结构感知的候选区域，为修补区域提供语义先验。然后提出基于transformer的上下文-语义融合网络（CSF-Net），将生成的语义候选与上下文特征融合，生成语义引导图像以提升修补效果。该网络无需改变现有修补模型的结构即可集成。

Result: 在Places365和COCOA数据集上，大量实验表明CSF-Net能够有效减少目标幻觉，提高修补图像的结构真实性和语义一致性，在多种遮挡场景下均显著提升性能。

Conclusion: CSF-Net作为可无缝集成的语义引导模块，显著提升了大面积修补的质量和鲁棒性。其方法为图像修补领域提供了新的结构和语义引导思路。

Abstract: In this paper, we propose a semantic-guided framework to address the challenging problem of large-mask image inpainting, where essential visual content is missing and contextual cues are limited. To compensate for the limited context, we leverage a pretrained Amodal Completion (AC) model to generate structure-aware candidates that serve as semantic priors for the missing regions. We introduce Context-Semantic Fusion Network (CSF-Net), a transformer-based fusion framework that fuses these candidates with contextual features to produce a semantic guidance image for image inpainting. This guidance improves inpainting quality by promoting structural accuracy and semantic consistency. CSF-Net can be seamlessly integrated into existing inpainting models without architectural changes and consistently enhances performance across diverse masking conditions. Extensive experiments on the Places365 and COCOA datasets demonstrate that CSF-Net effectively reduces object hallucination while enhancing visual realism and semantic alignment. The code for CSF-Net is available at https://github.com/chaeyeonheo/CSF-Net.

</details>


### [46] [Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture](https://arxiv.org/abs/2511.07990)
*Charalampos S. Kouzinopoulos,Yuri Manna*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8n模型、适用于低功耗STM32微控制器的田间杂草实时检测系统，通过模型压缩与优化，实现了在硬件受限环境下的高效能、低能耗杂草检测。


<details>
  <summary>Details</summary>
Motivation: 传统杂草管理主要依赖化学除草剂，导致环境污染以及抗药性杂草的出现，而现有的精确农业视觉检测系统功耗高、难以广泛部署。因此需要一种低能耗、可扩展的杂草自动检测方案。

Method: 采用YOLOv8n目标检测模型，并结合结构化剪枝、整数量化、图像分辨率缩放等多种模型压缩技术，将优化后的模型部署到STM32U575ZI低功耗微控制器上。利用74类植物的CropAndWeed数据集进行模型训练和评估。

Result: 优化后模型在保证检测精度的前提下，能够在能效仅为每次推理51.8mJ的情况下，实现田间杂草的实时检测。

Conclusion: 本系统实现了在功耗受限的农业环境中，用极低能耗进行实时杂草检测，为大规模智能农业装备的推广提供了可行性与应用前景。

Abstract: Weeds significantly reduce crop yields worldwide and pose major challenges to sustainable agriculture. Traditional weed management methods, primarily relying on chemical herbicides, risk environmental contamination and lead to the emergence of herbicide-resistant species. Precision weeding, leveraging computer vision and machine learning methods, offers a promising eco-friendly alternative but is often limited by reliance on high-power computational platforms. This work presents an optimized, low-power edge AI system for weeds detection based on the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. Several compression techniques are applied to the detection model, including structured pruning, integer quantization and input image resolution scaling in order to meet strict hardware constraints. The model is trained and evaluated on the CropAndWeed dataset with 74 plant species, achieving a balanced trade-off between detection accuracy and efficiency. Our system supports real-time, in-situ weeds detection with a minimal energy consumption of 51.8mJ per inference, enabling scalable deployment in power-constrained agricultural environments.

</details>


### [47] [Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning](https://arxiv.org/abs/2511.08003)
*Jialong Qin,Xin Zou,Di Lu,Yibo Yan,Xuming Hu*

Main category: cs.CV

TL;DR: SharpV提出了一种自适应裁剪视觉tokens及KV缓存的方法，提升了VideoLLMs效率并降低资源消耗，同时能保持甚至超越原模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VideoLLMs在处理大量冗余视觉tokens时，计算复杂度呈二次增长，且KV缓存扩展受限，导致效率低下。需要一种高效且智能的裁剪机制，以降低计算和存储负担。

Method: SharpV引入自适应裁剪机制，根据时空信息动态调整视觉tokens和KV缓存的裁剪比例。在KV缓存裁剪阶段，通过对退化视觉特征与原始特征的相似度进行自校准，经分层信息瓶颈实现有效裁剪。框架无需访问注意力分数，兼容硬件加速如Flash Attention。

Result: 在多个公开基准测试中，SharpV在准确率与效率上均超越现有方法，有时甚至优于未裁剪的稠密模型。

Conclusion: SharpV为VideoLLMs提供了一种高效自适应裁剪新范式，不仅大幅提升计算和存储效率，还能实现与甚至超越原有性能，具有良好的硬件兼容性与推广前景。

Abstract: Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.

</details>


### [48] [EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision](https://arxiv.org/abs/2511.08007)
*Yifei Cao,Yu Liu,Guolong Wang,Zhu Liu,Kai Wang,Xianjie Zhang,Jizhe Yu,Xun Tu*

Main category: cs.CV

TL;DR: EAGLE提出了一种新颖的2D-3D视觉查询定位框架，通过模拟鸟类记忆巩固，将外观和几何记忆结合，显著提升了自体视觉下的查询定位精度，并在Ego4D-VQ基准上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 自体视觉下的视觉查询定位因摄像头运动、视角变化和外观改变难以解决，然而该任务对于具身AI和VR/AR非常关键，因此需要更鲁棒的方法。

Method: 受鸟类记忆巩固机制启发，EAGLE框架提出了外观感知元学习记忆（AMM）和几何感知定位记忆（GLM），通过分割与跟踪协同工作，将高置信度检索样本存储于结构化记忆库，有效建模目标外观的长期与短期变化。同时通过基于视觉几何的Transformer（VGGT）实现2D查询结果与3D空间的快速结合。

Result: 方法在Ego4D-VQ基准数据集上取得了最优性能，精度明显提升。

Conclusion: EAGLE通过记忆巩固与2D-3D任务统一，为自体视觉的目标定位提供了新的解决思路，在具身AI和VR/AR领域具有广阔应用前景。

Abstract: Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.

</details>


### [49] [Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving](https://arxiv.org/abs/2511.08015)
*Jian Wang,Lijun He,Yixing Yong,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 本文提出了一种生成自然路面风格对抗海报的新方法AdvRoad，可在自动驾驶场景下成功欺骗基于视觉的3D物体检测模型，且对抗扰动自然、不易被人类察觉。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB摄像头的3D目标检测虽然成本低且精度高，但对抗扰动的安全隐患严重。以往的物理对抗攻击方法（如道路上的对抗海报）容易被人类察觉，且攻击方式单一、容易防御。因此，需要更隐蔽且有效的物理对抗攻击方法。

Method: 提出AdvRoad框架，采用两个阶段：1）路面风格对抗样本生成，使对抗海报具有自然路面外观；2）场景自适应调整，保证对抗样本对不同场景和检测器依然有效。通过这种方式，生成的对抗海报能够欺骗检测器，但对人类观察者自然不突兀。

Result: 实验中，AdvRoad在不同检测器、不同场景与不同位置均表现出较好的通用攻击效果。物理世界实验进一步证明了其攻击的现实威胁性。

Conclusion: 本文方法不仅提升了对抗攻击的隐蔽性和通用性，还揭示了视觉3D检测系统在真实环境下面临的安全风险。

Abstract: Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.

</details>


### [50] [High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection](https://arxiv.org/abs/2511.08018)
*Zhiyuan Chen,Yuelin Guo,Zitong Huang,Haoyu He,Renhao Lu,Weizhe Zhang*

Main category: cs.CV

TL;DR: 本文提出Cascade HQP-DETR，大力提升虚拟监督目标检测（ISOD）模型在合成到真实场景的泛化能力，通过高质量数据、引导式查询及级联去噪，最终在PASCAL VOC 2007上刷新性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测依赖大量昂贵的标注数据，而ISOD虽然通过合成数据训练，但存在数据质量低、模型泛化差与去噪不足等问题，限制了实际应用价值。

Method: 1）利用LLaMA-3、Flux和Grounding DINO，创建高质量虚拟数据集FluxVOC与FluxCOCO，实现强监督合成数据；2）提出高质量proposal引导的查询初始化，用SAM结果和RoI特征初始化object queries，提高模型收敛速度和特征泛化能力；3）设计级联去噪算法，解码器各层采用递增IoU阈值动态调整训练权重，提升对真实物体边界的学习能力，减少对伪标签噪声的过度拟合。

Result: 在仅用FluxVOC训练12轮的情况下，Cascade HQP-DETR在PASCAL VOC 2007数据集上获得了61.04% mAP@0.5（SOTA），超过现有强基线，且展现出良好的真实数据适应能力。

Conclusion: Cascade HQP-DETR系统性解决了ISOD中的主要挑战，将ISOD模型带入强监督阶段，提高了对真实世界目标的检测泛化，并为低成本目标检测提供了新范式。

Abstract: Object detection models demand large-scale annotated datasets, which are costly and labor-intensive to create. This motivated Imaginary Supervised Object Detection (ISOD), where models train on synthetic images and test on real images. However, existing methods face three limitations: (1) synthetic datasets suffer from simplistic prompts, poor image quality, and weak supervision; (2) DETR-based detectors, due to their random query initialization, struggle with slow convergence and overfitting to synthetic patterns, hindering real-world generalization; (3) uniform denoising pressure promotes model overfitting to pseudo-label noise. We propose Cascade HQP-DETR to address these limitations. First, we introduce a high-quality data pipeline using LLaMA-3, Flux, and Grounding DINO to generate the FluxVOC and FluxCOCO datasets, advancing ISOD from weak to full supervision. Second, our High-Quality Proposal guided query encoding initializes object queries with image-specific priors from SAM-generated proposals and RoI-pooled features, accelerating convergence while steering the model to learn transferable features instead of overfitting to synthetic patterns. Third, our cascade denoising algorithm dynamically adjusts training weights through progressively increasing IoU thresholds across decoder layers, guiding the model to learn robust boundaries from reliable visual cues rather than overfitting to noisy labels. Trained for just 12 epochs solely on FluxVOC, Cascade HQP-DETR achieves a SOTA 61.04\% mAP@0.5 on PASCAL VOC 2007, outperforming strong baselines, with its competitive real-data performance confirming the architecture's universal applicability.

</details>


### [51] [Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031)
*Chende Zheng,Ruiqi Suo,Zhoulin Ji,Jingyi Deng,Fangbin Yi,Chenhao Lin,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种结合音频与视频信息的多模态深度伪造检测与定位框架，能更精确地检测并定位高水平深度伪造内容。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态检测方法无法利用音视频跨模态相关性，且对复杂伪造的定位精度有限，这限制了其在真实环境下的实用性。

Method: 方法基于FPN-Transformer结构，结合预训练的WavLM（音频）和CLIP（视频）模型提取多层次时序特征，通过R-TLM模块和局部注意力机制构建多尺度特征金字塔，实现跨模态联合时序分析。预测头分支同时输出伪造概率和操作片段的精确边界回归，实现帧级定位。

Result: 在IJCAI'25 DDL-AV基准测试集上获得0.7535最终得分，验证了该方法在复杂环境下的跨模态深度伪造检测与定位有效性。

Conclusion: 该方法为深度伪造检测和定位问题提供了新思路，特别是在多模态、细粒度定位场景下具有良好泛化能力和实际应用价值。

Abstract: The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL

</details>


### [52] [Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric](https://arxiv.org/abs/2511.08032)
*Zhaolin Wan,Yining Diao,Jingqi Xu,Hao Wang,Zhiyang Li,Xiaopeng Fan,Wangmeng Zuo,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了首个3D Gaussian Splatting (3DGS)主观质量评价数据集（3DGS-QA），并基于此开发了无需参考图像或真实值的3DGS品质预测模型，对3DGS失真质量评估进行了系统化研究。


<details>
  <summary>Details</summary>
Motivation: 现有关于3DGS的研究聚焦于算法和视觉保真度，但不同重建条件下的感知质量影响尚未系统研究，实际中多种失真因素会严重影响视觉质量。因此，亟需一套针对3DGS内容感知质量的系统性评价方法与数据集。

Method: 作者首先构建了包含15类物体、225个失真重建样本的主观质量评价数据集3DGS-QA，用于分析常见3DGS失真因素；其次，提出了一种无参考的质量预测模型，直接基于3D高斯基本体提取空间和色彩线索实现结构感知的感知质量评分。

Result: 实验表明，该无参考感知质量预测模型在3DGS内容评价上优于现有传统及学习型方法，具有更好的稳健性和准确性。作者也基于新数据集对现有质量评价方法进行了系统基准测试。

Conclusion: 所提出的数据集和模型为3DGS内容感知质量评估提供了全新基准和工具，有助于推动该领域后续研究与应用开发。数据集和代码已公开，方便学术社区使用。

Abstract: With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.

</details>


### [53] [WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation](https://arxiv.org/abs/2511.08036)
*Gongshu Wang,Zhirui Wang,Kan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法WEDepth，用于改进单目深度估计，无需修改大型视觉基础模型结构和权重，通过分层激发和利用其内在先验，在多个公开数据集上取得了新SOTA表现，还具备良好的零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计由于从单幅图像重建3D场景本质上是病态问题，长期以来效果有限。虽然大型视觉基础模型预训练能力强，相关工作也用微调方式提升了MDE表现，但这些方法通常需要修改模型结构或权重。作者希望探索无需改变VFM结构和参数，也能高效利用其表征能力，进一步提升MDE，并增加泛化和迁移性。

Method: 作者提出WEDepth方法，将视觉基础模型作为多层特征增强器，系统地在不同表征层引入先验知识，而不需要对模型结构或已预训练权重做任何修改，从而充分挖掘和利用其内在的世界知识。

Result: 在NYU-Depth v2和KITTI数据集上，WEDepth方法取得了新的SOTA表现，比当前扩散方法和基于相对深度预训练的方法更具竞争力，同时实验也证明该方法在多种场景下具备很强的零样本迁移能力。

Conclusion: WEDepth实现了无需修改基础模型结构和权重的单目深度估计新突破，兼具高性能与强泛化能力，对实际部署具有重要意义。

Abstract: Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.

</details>


### [54] [ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation](https://arxiv.org/abs/2511.08046)
*Aya Elgebaly,Nikolaos Delopoulos,Juliane Hörner-Rieber,Carolin Rippke,Sebastian Klüter,Luca Boldrini,Lorenzo Placidi,Riccardo Dal Bello,Nicolaus Andratschke,Michael Baumgartl,Claus Belka,Christopher Kurz,Guillaume Landry,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 本文提出了一种基于自然语言提示实现个性化医学图像分割的方法ProSona，在肺结节和前列腺MRI数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中存在专家标注风格差异大（如肺结节标注）。现有方法通常将多标注合并或为每位专家单独建模，难以实现灵活的个性化和解释性结果。

Method: 提出ProSona两阶段框架：（1）用概率U-Net学习多样化专家分割假设；（2）引入自然语言引导的投影机制，将文本提示映射到专家风格的连续潜在空间，实现可控、可解释的个性化分割。采用多级对比目标对齐文本和视觉特征。

Result: 在LIDC-IDRI肺结节和多中心前列腺MRI数据集上，ProSona方法的广义能量距离降低17%，平均Dice系数较DPersona提升超1个百分点。

Conclusion: 自然语言提示能够实现灵活、精准且可解释的个性化医学图像分割，ProSona在实际数据集上效果显著优于现有方法，为临床应用带来更高可控性和解释性。

Abstract: Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .

</details>


### [55] [Generalized-Scale Object Counting with Gradual Query Aggregation](https://arxiv.org/abs/2511.08048)
*Jer Pelhan,Alan Lukezic,Matej Kristan*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法（GECO2），通过引入多尺度密集查询机制，有效提升了少样本计数与检测的精度和速度，尤其适用于对象尺度多样和小目标密集场景。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本检测与计数方法通常依赖多分辨率特征合并与输入图像上采样、切块等手段以适应目标尺度变化和高密度小目标场景。但这些方法带来高计算和内存消耗，且在多尺度和高密度小目标场景下表现不佳。因此，亟需一种有效解决对象尺度变化问题的端到端方案。

Method: 论文提出GECO2方法，引入了新的密集查询表示（dense query representation），该表示逐步聚合不同尺度下的示例特征，实现高分辨率的密集查询，从而提升大对象和小对象的检测能力。整体方法为端到端的少样本计数和检测系统，无需图像切块和上采样等辅助手段。

Result: GECO2在少样本计数和检测精度上超过当前最先进方法10%，同时速度提高3倍且所需GPU显存更低。

Conclusion: GECO2显著提升了少样本目标计数与检测在复杂场景（多尺度、密集小目标）下的性能，并在精度、速度与资源占用方面均优于现有主流方法，展现了广阔的应用前景。

Abstract: Few-shot detection-based counters estimate the number of instances in the image specified only by a few test-time exemplars. A common approach to localize objects across multiple sizes is to merge backbone features of different resolutions. Furthermore, to enable small object detection in densely populated regions, the input image is commonly upsampled and tiling is applied to cope with the increased computational and memory requirements. Because of these ad-hoc solutions, existing counters struggle with images containing diverse-sized objects and densely populated regions of small objects. We propose GECO2, an end-to-end few-shot counting and detection method that explicitly addresses the object scale issues. A new dense query representation gradually aggregates exemplar-specific feature information across scales that leads to high-resolution dense queries that enable detection of large as well as small objects. GECO2 surpasses state-of-the-art few-shot counters in counting as well as detection accuracy by 10% while running 3x times faster at smaller GPU memory footprint.

</details>


### [56] [Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching](https://arxiv.org/abs/2511.08061)
*Aditi Singhania,Arushi Jain,Krutik Malani,Riddhi Dhawan,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 本论文提出了一种结合LoRA微调和潜在拼接策略的扩散模型，实现对特定主体形象在多样化情境下的高一致性生成，并利用数据蒸馏和多阶段筛选方法提高训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前主导式图像生成面临一个核心矛盾：如何兼顾主体身份高度一致性和丰富的生成多样性。多数方法难以两者兼得。作者因此希望提出新模型和训练框架来缓解这种权衡，并实现更优性能。

Method: 作者提出采用LoRA微调的扩散模型，在潜在空间拼接参考和目标图像，结合masked Conditional Flow Matching（CFM）目标以实现身份特征保留。同时，为扩大训练规模，引入两阶段蒸馏数据筛选流程：第一阶段通过数据修复和视觉语言模型过滤得到高质量种子数据集，第二阶段利用该数据集做高效微调。此外，作者还设计了名为CHARIS的细粒度评价框架，从五个维度对生成结果进行评估。

Result: 该方法无需对网络结构做出修改，即可在身份一致性和生成多样性之间取得良好平衡，并能够高效扩展到大规模主体和情境。CHARIS评测证明其在身份保持、提示符合度、颜色一致性、视觉质量和变化多样性五个指标上均表现优异。

Conclusion: 论文提出的方法有效解决了主导式图像生成中身份一致性与生成多样性的矛盾，提供了高效且可扩展的训练和评测流程，为后续相关研究和实际应用提供了技术基础。

Abstract: Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.

</details>


### [57] [I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks](https://arxiv.org/abs/2511.08065)
*Ruichen Ma,Liwei Meng,Guanchao Qiao,Ning Ning,Yang Liu,Shaogang Hu*

Main category: cs.CV

TL;DR: 提出I2E算法，能将静态图像高效转换为高质量事件流数据，助力脉冲神经网络（SNNs）训练，性能领先，数据和代码开源。


<details>
  <summary>Details</summary>
Motivation: SNNs计算能效高，但缺少事件流数据严重限制了其发展，因此亟需方法将普通图像转换为可用于SNN的数据。

Method: I2E框架通过模拟微型快速眼动，并引入高度并行卷积，将静态图像转为事件流数据，实现了比此前方法快300倍的数据生成与扩增。

Result: 在大规模基准测试中，I2E生成数据上训练的SNN在ImageNet数据集上达到60.5%精度。通过先用I2E合成数据预训练，再在真实CIFAR10-DVS数据上微调，精度达92.5%，创下新高。

Conclusion: I2E方法不仅高效生成高保真事件流数据，还证明合成数据能很好迁移到真实任务，为神经拟态工程中的数据难题和高性能系统开发提供了基础工具和新范式。

Abstract: Spiking neural networks (SNNs) promise highly energy-efficient computing, but their adoption is hindered by a critical scarcity of event-stream data. This work introduces I2E, an algorithmic framework that resolves this bottleneck by converting static images into high-fidelity event streams. By simulating microsaccadic eye movements with a highly parallelized convolution, I2E achieves a conversion speed over 300x faster than prior methods, uniquely enabling on-the-fly data augmentation for SNN training. The framework's effectiveness is demonstrated on large-scale benchmarks. An SNN trained on the generated I2E-ImageNet dataset achieves a state-of-the-art accuracy of 60.50%. Critically, this work establishes a powerful sim-to-real paradigm where pre-training on synthetic I2E data and fine-tuning on the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of 92.5%. This result validates that synthetic event data can serve as a high-fidelity proxy for real sensor data, bridging a long-standing gap in neuromorphic engineering. By providing a scalable solution to the data problem, I2E offers a foundational toolkit for developing high-performance neuromorphic systems. The open-source algorithm and all generated datasets are provided to accelerate research in the field.

</details>


### [58] [Radar-APLANC: Unsupervised Radar-based Heartbeat Sensing via Augmented Pseudo-Label and Noise Contrast](https://arxiv.org/abs/2511.08071)
*Ying Wang,Zhaodong Sun,Xu Cheng,Zuxian He,Xiaobai Li*

Main category: cs.CV

TL;DR: 该文提出了一种无监督的FMCW雷达心跳检测方法Radar-APLANC，利用噪声对比和伪标签增强，在无需昂贵标签的条件下实现了鲁棒的非接触式心跳感知，性能媲美有监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于雷达的心跳检测方法容易受噪声影响导致性能下降；而学习型方法虽然鲁棒性更强，但受限于对昂贵标注信号的依赖。研究动机是开发既鲁棒又不依赖高成本标签的无监督方案。

Method: 提出了Radar-APLANC无监督框架，利用雷达数据中心跳区间（正样本）与噪声区间（负样本）进行噪声对比训练，同时使用传统方法生成的伪标签，无需真实生理信号作为标签。创新性还在于伪标签增强模块，能自适应选择高质量的噪声感知标注。

Result: 在Equipleth公开数据集和自采雷达数据集上进行了大量实验。结果显示，该无监督方法与最先进的有监督方法在性能上相当，具备较好的鲁棒性和实用价值。

Conclusion: Radar-APLANC证明了无监督雷达心跳检测的可行性和有效性，为成本敏感的实际应用提供了新思路。代码和数据集已公开，便于后续研究和应用。

Abstract: Frequency Modulated Continuous Wave (FMCW) radars can measure subtle chest wall oscillations to enable non-contact heartbeat sensing. However, traditional radar-based heartbeat sensing methods face performance degradation due to noise. Learning-based radar methods achieve better noise robustness but require costly labeled signals for supervised training. To overcome these limitations, we propose the first unsupervised framework for radar-based heartbeat sensing via Augmented Pseudo-Label and Noise Contrast (Radar-APLANC). We propose to use both the heartbeat range and noise range within the radar range matrix to construct the positive and negative samples, respectively, for improved noise robustness. Our Noise-Contrastive Triplet (NCT) loss only utilizes positive samples, negative samples, and pseudo-label signals generated by the traditional radar method, thereby avoiding dependence on expensive ground-truth physiological signals. We further design a pseudo-label augmentation approach featuring adaptive noise-aware label selection to improve pseudo-label signal quality. Extensive experiments on the Equipleth dataset and our collected radar dataset demonstrate that our unsupervised method achieves performance comparable to state-of-the-art supervised methods. Our code, dataset, and supplementary materials can be accessed from https://github.com/RadarHRSensing/Radar-APLANC.

</details>


### [59] [CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion](https://arxiv.org/abs/2511.08075)
*Cameron Braunstein,Mariya Toneva,Eddy Ilg*

Main category: cs.CV

TL;DR: 该论文探讨了Stable Diffusion等潜变量扩散模型在文本生成图像任务中对语义信息的理解能力及其来源。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像的生成任务中取得了优异表现，但它们是否真的理解并表示了人类可理解的语义信息尚不明确。

Method: 作者在Stable Diffusion中使用简单的回归层对生成图像内部表示进行分析，预测物体的语义属性，并将这些预测结果与人工标注数据进行比较。还研究了扩散反向过程在属性分辨率方面的表现。

Result: 实验发现，模型的语义表达能力主要来自CLIP文本编码部分，而不是扩散反向过程。不同属性组的解码准确度存在较大差异，属性在逆扩散过程中愈发难以区分，表明对象属性的强语义表示主要在CLIP中。

Conclusion: Stable Diffusion中的CLIP模型决定了人类可理解的语义表示，扩散过程更像是图像的视觉解码器。

Abstract: Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.

</details>


### [60] [Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis](https://arxiv.org/abs/2511.08087)
*Aditi Singhania,Krutik Malani,Riddhi Dhawan,Arushi Jain,Garv Tandon,Nippun Sharma,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层评估框架，用于细粒度地衡量生成模型在身份保持方面的能力，比以往方法更精准且更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型身份保持的评估方法依赖于全局嵌入或粗略的视觉语言模型（VLM）提示，难以捕捉细节变化，诊断意义有限。因此，亟需一种能更细致、解释性更强的身份一致性评估方法。

Method: 提出“Beyond the Pixels”框架，将身份评估分解为分层的特征变换，具体通过：（1）建立(type, style)->attribute->feature的决策树结构，对主体进行分层分解；（2）通过具体变换而非抽象分数引导VLM进行分析，以获得具象的视觉证据。

Result: 在四个最先进生成模型上进行实验，验证该框架与人类判断高度一致，能更有效地衡量身份一致性。同时，构建了一个包含1078组多样主体类型（包括边缘类别如拟人和动画角色）、每组包含6-7个变换轴的新评测基准，用于全面压力测试生成模型。

Conclusion: 新框架有效提升了生成模型身份保持测评的细致度和诊断能力，显著优于现有方法，并提供了新的高质量评测数据集。

Abstract: Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.

</details>


### [61] [StableMorph: High-Quality Face Morph Generation with Stable Diffusion](https://arxiv.org/abs/2511.08090)
*Wassim Kabbani,Kiran Raja,Raghavendra Ramachandra,Christoph Busch*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的人脸融合攻击生成方法StableMorph，能生成高质量、真实且无伪影的融合人脸图像，用于更有效地测试和改进生物特征识别系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸融合生成方法往往会产生模糊、有伪影或者结构不良的图像，这些缺陷使得这些合成攻击在现实中较易被检测，难以反映最具威胁的实际攻击需求。因此，需要一种能生成高质量、真实且难以检测的人脸融合图像的方法。

Method: 作者提出了StableMorph，这是一种基于现代扩散式图像合成技术的方法。StableMorph能够生成全头部、细节清晰、无伪影的人脸融合图像，并允许对视觉属性进行高度可控的调节。作者对其生成的图像从视觉质量和攻击有效性两个角度进行了全面评估。

Result: 实验结果表明，StableMorph生成的图像在视觉质量上能够媲美甚至超过真实人脸图像，并显著提升了欺骗现有面部识别系统的能力，对当前的融合攻击检测系统（MAD）提出了更高挑战，设置了新的评测标准。

Conclusion: StableMorph为评估生物识别安全性提供了更“真实”、更具威胁性的攻击样本，有助于推动更强健的检测系统研发，为学术和实际应用领域树立了新的图像质量基准。

Abstract: Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.

</details>


### [62] [Introducing Nylon Face Mask Attacks: A Dataset for Evaluating Generalised Face Presentation Attack Detection](https://arxiv.org/abs/2511.08114)
*Manasa,Sushrut Patwardhan,Narayan Vetrekar,Pavan Kumar,R. S. Gad,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 本论文提出了一种新的人脸攻击数据集，专注于新颖且仿真度极高的“尼龙面具”攻击方式，并以此评估现有防伪技术的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别在日常生活中广泛应用，其安全性受到越来越多的关注，尤其是针对日趋逼真的仿冒攻击手段。传统的人脸防伪系统面对新型高仿真的3D假体（如尼龙面具）攻击面临巨大挑战，因此亟需针对现实威胁的全新数据集和防护机制。

Method: 作者设计了一套基于iPhone 11 Pro采集的真实场景数据集，包含100名受试者的3,760份真人样本和51,281份尼龙面具攻击样本，覆盖人类和假人四种不同的展示场景。利用该数据集，对五种主流的人脸防伪（PAD）方法进行了系统性基准测试，以评估其在未知攻击条件下的表现。

Result: 实验结果显示，不同PAD方法在尼龙面具攻击下性能表现出显著差异，部分方法无法有效辨别真实与攻击，实现泛化能力十分有限。

Conclusion: 尼龙面具等高级攻击手段对现有防伪系统构成严峻挑战，未来应专注于提升防伪模型对新兴仿冒威胁的泛化与鲁棒性，以保障人脸识别系统的安全可靠应用。

Abstract: Face recognition systems are increasingly deployed across a wide range of applications, including smartphone authentication, access control, and border security. However, these systems remain vulnerable to presentation attacks (PAs), which can significantly compromise their reliability. In this work, we introduce a new dataset focused on a novel and realistic presentation attack instrument called Nylon Face Masks (NFMs), designed to simulate advanced 3D spoofing scenarios. NFMs are particularly concerning due to their elastic structure and photorealistic appearance, which enable them to closely mimic the victim's facial geometry when worn by an attacker. To reflect real-world smartphone-based usage conditions, we collected the dataset using an iPhone 11 Pro, capturing 3,760 bona fide samples from 100 subjects and 51,281 NFM attack samples across four distinct presentation scenarios involving both humans and mannequins. We benchmark the dataset using five state-of-the-art PAD methods to evaluate their robustness under unseen attack conditions. The results demonstrate significant performance variability across methods, highlighting the challenges posed by NFMs and underscoring the importance of developing PAD techniques that generalise effectively to emerging spoofing threats.

</details>


### [63] [LatentPrintFormer: A Hybrid CNN-Transformer with Spatial Attention for Latent Fingerprint identification](https://arxiv.org/abs/2511.08119)
*Arnab Maity,Manasa,Pavan Kumar C,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 本文提出了一种新的潜在指纹识别方法LatentPrintFormer，结合CNN和Transformer提取特征，并采用空间注意力机制以提升识别准确性。实验表明该方法优于现有主流技术。


<details>
  <summary>Details</summary>
Motivation: 潜在指纹因图像质量低、噪声大和指纹不完整等因素，识别难度大。现有方法效果有限，需要更强的特征提取与降噪能力。

Method: 提出LatentPrintFormer模型，将EfficientNet-B0（CNN）和Swin Tiny（Transformer）结合，挖掘局部和全局特征；引入空间注意力模块强化高质量脊线区域并抑制噪声；特征融合成512维向量，通过余弦相似度进行匹配。

Result: 在两个公开数据集上，LatentPrintFormer在Rank-10识别准确率上，均超越当前三种主流潜在指纹识别技术。

Conclusion: LatentPrintFormer通过融合多元特征及空间注意力机制，有效提升了潜在指纹的识别性能，在同类任务中表现出色。

Abstract: Latent fingerprint identification remains a challenging task due to low image quality, background noise, and partial impressions. In this work, we propose a novel identification approach called LatentPrintFormer. The proposed model integrates a CNN backbone (EfficientNet-B0) and a Transformer backbone (Swin Tiny) to extract both local and global features from latent fingerprints. A spatial attention module is employed to emphasize high-quality ridge regions while suppressing background noise. The extracted features are fused and projected into a unified 512-dimensional embedding, and matching is performed using cosine similarity in a closed-set identification setting. Extensive experiments on two publicly available datasets demonstrate that LatentPrintFormer consistently outperforms three state-of-the-art latent fingerprint recognition techniques, achieving higher identification rates across Rank-10.

</details>


### [64] [Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2](https://arxiv.org/abs/2511.08130)
*Mehmet Batuhan Duman,Alejandro Carnero,Cristian Martín,Daniel Garrido,Manuel Díaz*

Main category: cs.CV

TL;DR: 本文提出了一种结合联邦学习（FL）和先进图像分割模型（SAM2）的框架，实现了在保障隐私的前提下多厂协同自动监测污水处理厂泡沫的解决方案。


<details>
  <summary>Details</summary>
Motivation: 污水处理厂泡沫问题会降低处理效率并增加成本，需实时自动监测；但受限于标注数据稀缺、异质性及数据隐私保护，常规机器学习模型难以落地应用。

Method: 提出使用联邦学习框架，结合大规模预训练的Segment Anything Model 2（SAM2），在各地污水处理厂本地对模型进行微调，中央服务器仅聚合权重不接触原始数据，实现多节点协同训练。实验数据包括真实厂区图像、合成泡沫数据及公开数据集。

Result: 该框架在有限本地数据情况下，加速了模型收敛，提高了泡沫分割性能，在通用性和隐私保护方面展现出明显优势。

Conclusion: 结合强大基础模型与联邦学习的框架，为分布式且隐私敏感的工业场景（如污水厂泡沫追踪）提供了实用、可扩展的自动化解决方案，对其它类似难题具有参考价值。

Abstract: Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.

</details>


### [65] [OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition](https://arxiv.org/abs/2511.08133)
*Lixu Sun,Nurmemet Yolwas,Wushour Silamu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的神经认知启发式的三阶段场景文本识别网络OTSNet，大幅提升了复杂场景下的文本识别准确率，刷新了多个数据集的性能纪录。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本识别方法在视觉与语言优化上解耦，导致跨模态错配放大误差、视觉编码器对干扰背景关注过多、解码器解析变形文本时空间错位，严重影响不规则文本的识别效率。因此作者受人类视觉认知过程启发，试图打造一种能统一感知、理解与拼写流程的模型。

Method: 作者提出OTSNet框架，包括三个核心部分：（1）双重注意力Macaron编码器（DAME）强化视觉特征，抑制无关区域；（2）位置感知模块与语义量化器联合融合空间与语义信息；（3）多模态协作验证器（MMCV）结合视觉、语义和字符特征进行自校正。三者构成了观察-思考-拼写的端到端流水线，提升模型整体协同性与抗干扰能力。

Result: OTSNet在Union14M-L数据集上取得了83.5%的平均准确率，在遮挡严重的OST数据集上取得了79.1%的准确率，在14个评测场景中有9项刷新了当前最佳记录。结果表明模型在复杂、不规则场景下大幅提升了文本识别性能。

Conclusion: OTSNet通过模拟人类认知分阶段的处理机制，显著缓解了场景文本识别中跨模态错配与空间错位问题，实现了不同特征的协同优化，推动了该领域的最新性能极限。

Abstract: Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.

</details>


### [66] [PEOD: A Pixel-Aligned Event-RGB Benchmark for Object Detection under Challenging Conditions](https://arxiv.org/abs/2511.08140)
*Luoping Cui,Hanqing Liu,Mingjie Liu,Endian Lin,Donghong Jiang,Yuhao Wang,Chuang Zhu*

Main category: cs.CV

TL;DR: 现有的Event-RGB数据集在极端条件覆盖度和分辨率上存在限制，影响了复杂场景下的检测器评估。该文提出了首个大规模、高分辨率（1280x720）、像素对齐的PEOD数据集，专为复杂环境下的目标检测设计，并对多种方法进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前事件相机在复杂场景下目标检测中作用日益突出，但现有Event-RGB数据集普遍分辨率较低且极端场景采集有限，这给全面评测和发展新模型带来障碍。

Method: PEOD数据集收集了130多个时空对齐的视频序列，涵盖低光、过曝和高速运动环境，共标注34万个人工边界框。测试了14种主流检测方法，输入模式包括事件流、RGB、二者融合，并在全数据集、普通子集及照明挑战子集上综合对比。

Result: 融合模型在全测试集和普通子集表现最好。在照明受挑战子集，最好的事件流模型超越所有融合模型，而融合模型依然好于RGB-only模型，显示出单独事件流在极端照明下的优势和融合方法的局限。

Conclusion: PEOD提供了高质量、高分辨率、现实复杂环境下的多模态检测基准，为事件-RGB融合以及后续多模态感知研究提供了有力工具。

Abstract: Robust object detection for challenging scenarios increasingly relies on event cameras, yet existing Event-RGB datasets remain constrained by sparse coverage of extreme conditions and low spatial resolution (<= 640 x 480), which prevents comprehensive evaluation of detectors under challenging scenarios. To address these limitations, we propose PEOD, the first large-scale, pixel-aligned and high-resolution (1280 x 720) Event-RGB dataset for object detection under challenge conditions. PEOD contains 130+ spatiotemporal-aligned sequences and 340k manual bounding boxes, with 57% of data captured under low-light, overexposure, and high-speed motion. Furthermore, we benchmark 14 methods across three input configurations (Event-based, RGB-based, and Event-RGB fusion) on PEOD. On the full test set and normal subset, fusion-based models achieve the excellent performance. However, in illumination challenge subset, the top event-based model outperforms all fusion models, while fusion models still outperform their RGB-based counterparts, indicating limits of existing fusion methods when the frame modality is severely degraded. PEOD establishes a realistic, high-quality benchmark for multimodal perception and facilitates future research.

</details>


### [67] [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](https://arxiv.org/abs/2511.08152)
*Jun Sun,Xinxin Zhang,Simin Hong,Jian Zhu,Xiang Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的多模态异构领域自适应方法Boomda，通过独立建模每种模态并利用信息瓶颈和相关对齐，实现对不同模态领域差异的平衡自适应，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在许多领域取得成功，但人工标注昂贵，标注数据稀缺问题突出。现有的无监督领域自适应在单一模态下研究较多，在多模态、特别是异构多模态领域差异处理方面研究不足。

Method: 作者采用信息瓶颈方法为每种模态单独学习表征，然后通过相关对齐（correlation alignment）在表征空间对源域和目标域进行匹配。将多模态对齐问题建模为多目标优化任务，寻求帕累托最优解，通过模型特性将优化转化成二次规划问题，并进一步近似得到了闭式解，实现高效的自适应算法Boomda。

Result: 实验结果表明，作者提出的Boomda方法在多个数据集上优于现有主流多模态自适应方法，在效果和效率上均有明显提升。

Conclusion: Boomda成功解决了多模态领域自适应中的模态对齐与平衡难题，提供了理论和实践上的有效方法，为多模态无监督领域自适应技术发展提供了新思路。

Abstract: Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.

</details>


### [68] [Non-Aligned Reference Image Quality Assessment for Novel View Synthesis](https://arxiv.org/abs/2511.08155)
*Abhijay Ghildyal,Rajesh Sureddi,Nabajeet Barman,Saman Zadtootaghaj,Alan Bovik*

Main category: cs.CV

TL;DR: 论文针对新颖视图合成(NVS)图像无像素级对齐参考时的感知质量评价难题，提出了NAR-IQA方法，无需完美对齐，实现强泛化与高准确性。


<details>
  <summary>Details</summary>
Motivation: NVS图像难以获得像素级对齐的参考图，现有FR-IQA方法在对齐误差下效果差，NR-IQA方法泛化性弱，亟需适用于非对齐参考的新型质量评价框架。

Method: 提出NAR-IQA框架，假设参考视图与待评图像有部分场景重叠但无像素级对齐。构建含合成失真大规模数据集，基于对比学习、LoRA增强的DINOv2嵌入，并结合现有IQA方法监督，仅以合成失真训练，避免对特定NVS样本过拟合。

Result: 所提方法在存在对齐与非对齐参考的条件下，超越了现有最佳FR-IQA、NR-IQA和NAR-IQA方法，在泛化能力和评价相关性上表现突出。

Conclusion: NAR-IQA能在参考视图未对齐情况下稳健预测视图合成图像质量，模型与主观评分高度相关，支持实际NVS应用。

Abstract: Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/

</details>


### [69] [LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping](https://arxiv.org/abs/2511.08156)
*Chenying Liu,Wei Huang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的土地利用与覆盖（LULC）基础模型框架LandSegmenter，通过引入多源弱标注数据集和模块化结构，实现对不同输入模态和类别体系的泛化，特别提升了零样本任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LULC模型通常局限于特定模态和固定类别，难以在多样任务和领域间泛化。同时，大规模有标签数据的获取成本高昂，限制了遥感领域基础模型的应用。为解决模型可扩展性和数据依赖问题，本文希望借助基础模型理念和弱监督数据提升遥感LULC任务的适应性。

Method: 提出LandSegmenter框架：1）在输入端，构建基于全球弱标注产品的大规模、多模态、多源LULC数据集LAS，以弱标签替代高成本手工标注；2）在模型结构上，集成遥感定制适配器进行跨模态特征提取，配置文本编码器增强语义感知；3）输出端，设计类置信度引导融合策略，减轻语义遗漏，提升零样本性能。

Result: 在六个高质量LULC数据集上的迁移学习和零样本实验表明，LandSegmenter在多种模态和类别体系下均取得有竞争力甚至优越的效果，尤其在无需微调时表现突出。

Conclusion: LandSegmenter框架及其利用弱监督的大规模数据的方法，能有效提升LULC基础模型的通用性与迁移能力，为遥感领域构建任务专属基础模型提供了新思路。

Abstract: Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.

</details>


### [70] [Multi-Granularity Mutual Refinement Network for Zero-Shot Learning](https://arxiv.org/abs/2511.08163)
*Ning Wang,Long Yu,Cong Hua,Guangming Zhu,Lin Mei,Syed Afaq Ali Shah,Mohammed Bennamoun,Liang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种多粒度互相精炼网络（Mg-MRN），通过学习分离的多粒度特征和跨粒度特征交互，加强局部区域间的内在联系，从而提升零样本学习（ZSL）的性能。


<details>
  <summary>Details</summary>
Motivation: 目前零样本学习方法主要关注全局或局部视觉特征与语义之间的相关性，但往往忽视了局部区域特征之间的内在交互，这限制了特征的判别性和可迁移性。作者希望通过挖掘局部特征之间的关系，提升模型识别未见类别的能力。

Method: 作者提出了Mg-MRN网络，包括多粒度特征提取模块和跨粒度特征融合模块。前者通过分离的区域特征挖掘，学习判别性强的区域特征；后者通过整合不同粒度间的区域特征，加强各粒度层级间的内在交互，提升特征表达能力。

Result: 在三个流行的ZSL基准数据集上进行大量实验，结果显示Mg-MRN在识别性能上优于当前主流方法，具有较强的竞争力。

Conclusion: 通过多粒度特征学习和融合，能够更好地捕捉区域特征间的关系，显著提升零样本学习的泛化能力和判别能力。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes with zero samples by transferring semantic knowledge from seen classes. Current approaches typically correlate global visual features with semantic information (i.e., attributes) or align local visual region features with corresponding attributes to enhance visual-semantic interactions. Although effective, these methods often overlook the intrinsic interactions between local region features, which can further improve the acquisition of transferable and explicit visual features. In this paper, we propose a network named Multi-Granularity Mutual Refinement Network (Mg-MRN), which refine discriminative and transferable visual features by learning decoupled multi-granularity features and cross-granularity feature interactions. Specifically, we design a multi-granularity feature extraction module to learn region-level discriminative features through decoupled region feature mining. Then, a cross-granularity feature fusion module strengthens the inherent interactions between region features of varying granularities. This module enhances the discriminability of representations at each granularity level by integrating region representations from adjacent hierarchies, further improving ZSL recognition performance. Extensive experiments on three popular ZSL benchmark datasets demonstrate the superiority and competitiveness of our proposed Mg-MRN method. Our code is available at https://github.com/NingWang2049/Mg-MRN.

</details>


### [71] [KPLM-STA: Physically-Accurate Shadow Synthesis for Human Relighting via Keypoint-Based Light Modeling](https://arxiv.org/abs/2511.08169)
*Xinhui Yin,Qifei Li,Yilin Guo,Hongxia Xie,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合关键点线性模型（KPLM）和阴影三角算法（STA）的方法，实现了在人像合成中更真实和精准的阴影生成，优于现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的图像合成方法虽然比GAN有进步，但在人像阴影的真实感和几何精度上仍不理想，尤其是在复杂姿态的人像合成中。因此，需要新的方法提升阴影合成的真实性和几何准确性。

Method: 提出了基于九个人体关键点和一个包围块的KPLM模型，实现关节点动态物理合理的阴影投射；同时引入STA，通过显式计算阴影角度、长度和空间位置，提高了阴影的几何精度。

Result: 实验结果显示，该方法在人像阴影真实感和精度评估基准上达到最新水平，尤其在复杂姿态和多方向重光等场景下表现突出。

Conclusion: 提出的方法有效提升了人像图像合成中阴影的外观真实性和几何准确性，对多方向光照也具备较强的泛化能力，有望推动图像合成技术进一步发展。

Abstract: Image composition aims to seamlessly integrate a foreground object into a background, where generating realistic and geometrically accurate shadows remains a persistent challenge. While recent diffusion-based methods have outperformed GAN-based approaches, existing techniques, such as the diffusion-based relighting framework IC-Light, still fall short in producing shadows with both high appearance realism and geometric precision, especially in composite images. To address these limitations, we propose a novel shadow generation framework based on a Keypoints Linear Model (KPLM) and a Shadow Triangle Algorithm (STA). KPLM models articulated human bodies using nine keypoints and one bounding block, enabling physically plausible shadow projection and dynamic shading across joints, thereby enhancing visual realism. STA further improves geometric accuracy by computing shadow angles, lengths, and spatial positions through explicit geometric formulations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on shadow realism benchmarks, particularly under complex human poses, and generalizes effectively to multi-directional relighting scenarios such as those supported by IC-Light.

</details>


### [72] [Distributed Zero-Shot Learning for Visual Recognition](https://arxiv.org/abs/2511.08170)
*Zhi Chen,Yadan Luo,Zi Huang,Jingjing Li,Sen Wang,Xin Yu*

Main category: cs.CV

TL;DR: 本文提出了一种用于分布式零样本学习的新框架DistZSL，通过协同各节点数据，有效提升对未见类别的识别能力，并在多个实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境下，数据异构性（如不同节点拥有的数据分布不同）会破坏零样本学习的泛化效果，因此急需新方法解决节点间数据差异对模型性能的影响。

Method: DistZSL框架包含两个主要创新点：1）跨节点属性正则化器，使不同节点的属性特征距离相似，从而稳定整体属性空间；2）全局属性-视觉一致性机制，使不同节点间属性与视觉特征的映射保持一致，防止各节点产生有偏的V2A映射。

Result: 通过大量实验验证，DistZSL能够在分布式数据设置下表现优于现有零样本学习方法，兼顾了泛化能力与准确率。

Conclusion: DistZSL框架有效缓解了分布式环境下的数据异构性对零样本学习的负面影响，为分布式零样本识别提供了更优的解决方案。

Abstract: In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.

</details>


### [73] [VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion](https://arxiv.org/abs/2511.08173)
*Samet Hicsonmez,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督多类视觉异常检测方法，结合了潜在扩散模型与视觉-语言模型，实现了对真实多类图像异常的高效检测与定位，性能超越现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 多类真实世界图像中的视觉异常检测任务极具挑战性。现有基于扩散的检测方法泛化能力有限，且大多需为每一类单独训练模型，限制了其可扩展性。

Method: 方法将预训练视觉-语言模型（VLM）和潜在扩散模型（LDM）相结合。首先，使用VLM通过简单提示生成详细的图像描述，这些描述作为条件输入辅助LDM学习。整个过程中无需手动标注或额外训练，大幅提升了多类别无监督异常检测的泛化能力。

Result: 在Real-IAD和COCO-AD数据集上，本方法的像素级区域重叠（PRO）指标分别提高了25点和8点，显著优于最新的扩散类方法。

Conclusion: 结合视觉-语言模型作为条件输入，能够提升扩散模型在多类视觉异常检测的能力和泛化性，且无需为每一类别单独训练，具有更高的实用价值和推广潜力。

Abstract: Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

</details>


### [74] [WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting](https://arxiv.org/abs/2511.08178)
*Kaitao Huang,Yan Yan,Jing-Hao Xue,Hanzi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D GAN反演方法WarpGAN，通过结合图像修复与3D GAN反演，更好地合成单视图下不可见区域的内容，实现高质量的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 传统3D GAN反演方法在重建可见区域表现较好，但对于被遮挡区域，仅依赖GAN的生成先验，导致质量较差，尤其由于低比特率隐空间编码导致信息丢失。因此有必要提升遮挡区域的生成质量，实现更真实且一致的新视角合成。

Method: WarpGAN方法包括三步：(1) 首先用3D GAN反演编码器将单视图图片投影到3D GAN的隐空间。(2) 利用3D GAN生成的深度图进行新视角扭曲(warping)。(3) 提出SVINet网络，结合对称性先验和同一潜码下多视角图像的对应关系，对扭曲后图像的被遮挡区域进行修复。

Result: 实验（定量和定性）表明WarpGAN方法在新视角生成任务中，生成质量和一致性均优于多种最新方法，尤其在被遮挡区域的合成上表现突出。

Conclusion: 通过将图像修复策略与3D GAN反演结合，WarpGAN显著提升了单视图新视角合成的真实性和一致性，为3D重建、虚拟现实等领域提供了更高质量的图像合成解决方案。

Abstract: 3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.

</details>


### [75] [Pixel-level Quality Assessment for Oriented Object Detection](https://arxiv.org/abs/2511.08186)
*Yunhui Zhu,Buliao Huang*

Main category: cs.CV

TL;DR: 提出了一种新的像素级质量评估（PQA）方法，用于提升面向目标检测器的定位质量评估，优于传统的IoU预测。


<details>
  <summary>Details</summary>
Motivation: 现有定向目标检测器多依赖预测的IoU作为定位质量指标，但由于结构耦合问题，IoU预测往往对定位不准的框存在过高估计，影响检测整体性能。

Method: 文章提出了像素级质量评估(PQA)框架，用每个像素在预测框和真实框之间的空间一致性来取代传统的盒级IoU预测，并设计了整合像素级空间一致性的整合度量，将其归纳为整体质量分数。这样有效规避了由于直接比较预测框与估算的真实框所导致的相似性偏差。

Result: 在HRSC2016和DOTA数据集上大量实验证明，PQA框架可以无缝集成进不同的定向目标检测器，并稳定提升检测精度，例如在Rotated RetinaNet上AP$_{50:95}$提升5.96%，在STD上提升2.32%。

Conclusion: PQA能更准确评估定位质量，克服了IoU预测的结构性缺陷，能明显提升定向目标检测器的性能。

Abstract: Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).

</details>


### [76] [UI2Code$^\text{N}$: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation](https://arxiv.org/abs/2511.08195)
*Zhen Yang,Wenyi Hong,Mingde Xu,Xinyue Fan,Weihan Wang,Jiele Cheng,Xiaotao Gu,Jie Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新的交互式UI代码生成范式，并开发了UI2Code$^N$多模态视觉语言模型，能实现UI自动编码、编辑与润色，实验结果达到了开源模型新SOTA，并媲美闭源大模型。


<details>
  <summary>Details</summary>
Motivation: 当前UI编程复杂且依赖人工，自动化UI编码需求迫切。但现有视觉语言模型在多模态编码能力和多轮交互性方面存在显著不足，难以支持真实的开发流程。

Method: 提出了交互式UI-to-code范式，并实现了UI2Code$^N$模型。该模型通过分阶段的预训练、微调和强化学习提升多模态编码能力，实现了UI转代码、UI编辑和润色三大功能，并支持多轮反馈的系统性交互自适应生成。

Result: 在UI-to-code和UI润色基准测试上，UI2Code$^N$模型刷新了开源模型的最佳水平（SOTA），并在性能上接近或可比闭源强大模型如Claude-4-Sonnet与GPT-5。

Conclusion: 交互式UI代码生成范式及UI2Code$^N$模型极大提升了多模态自动UI编程能力，为真实软件开发提供了更有效实用的新工具，显著推动了该领域发展。

Abstract: User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.

</details>


### [77] [UCDSC: Open Set UnCertainty aware Deep Simplex Classifier for Medical Image Datasets](https://arxiv.org/abs/2511.08196)
*Arnav Aditya,Nitin Kumar,Saurabh Shigwan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的损失函数，通过利用辅助数据集，有效提升医学影像诊断中未知类别（open-set）识别的能力，实验在多个医学数据集显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学影像诊断领域，由于数据有限且标注困难，模型在实际应用中往往会遇到未见过的类别（如罕见或新发疾病），传统方法难以准确识别这些未知类别，影响临床应用的可靠性，因此急需提升open-set识别能力。

Method: 作者基于深度神经网络后期特征聚类表现，提出了一种新型损失函数，利用辅助数据集来惩罚特征空间中的“开放区域”，从而更好地拒绝未知类别的样本。

Result: 新方法在BloodMNIST、OCTMNIST、DermaMNIST、TissueMNIST四个MedMNIST数据集和公开皮肤数据集上的open-set识别性能显著超过了当前主流方法。

Conclusion: 通过设计针对open-set情景的损失函数，结合辅助数据集，本方法有效提升了医学影像open-set识别能力，为实际临床部署提供了更可靠的技术。

Abstract: Driven by advancements in deep learning, computer-aided diagnoses have made remarkable progress. However, outside controlled laboratory settings, algorithms may encounter several challenges. In the medical domain, these difficulties often stem from limited data availability due to ethical and legal restrictions, as well as the high cost and time required for expert annotations-especially in the face of emerging or rare diseases. In this context, open-set recognition plays a vital role by identifying whether a sample belongs to one of the known classes seen during training or should be rejected as an unknown. Recent studies have shown that features learned in the later stages of deep neural networks are observed to cluster around their class means, which themselves are arranged as individual vertices of a regular simplex [32]. The proposed method introduces a loss function designed to reject samples of unknown classes effectively by penalizing open space regions using auxiliary datasets. This approach achieves significant performance gain across four MedMNIST datasets-BloodMNIST, OCTMNIST, DermaMNIST, TissueMNIST and a publicly available skin dataset [29] outperforming state-of-the-art techniques.

</details>


### [78] [Twist and Compute: The Cost of Pose in 3D Generative Diffusion](https://arxiv.org/abs/2511.08203)
*Kyle Fogarty,Jack Foster,Boqiao Zhang,Jing Yang,Cengiz Öztireli*

Main category: cs.CV

TL;DR: 本文指出，当前的大规模图像到3D生成模型（如Hunyuan3D 2.0）存在明显的标准视角偏置，在处理旋转输入时性能减少。提出简单的CNN模块可校正输入方向，显著恢复性能，无需修改主生成模块。


<details>
  <summary>Details</summary>
Motivation: 尽管现有大模型在3D生成领域展示出强大能力，但其内在归纳偏差尚不透明。作者希望揭示这些模型的实际弱点并探讨是否单靠模型规模即可克服结构性问题。

Method: 通过对输入图像做2D旋转实验，系统分析模型在不同视角下的泛化能力。此外，设计轻量CNN模块用于判断并校正输入方向，再评估性能变化。

Result: 实验表明，主流3D生成模型（Hunyuan3D 2.0）在旋转输入下生成效果明显下降。引入方向校正的CNN后，模型性能得到恢复。

Conclusion: 当前大规模模型对于视角存在内在偏置，仅靠扩大规模难以消除。模块化、具备对称性意识的设计应该成为进一步研究方向。

Abstract: Despite their impressive results, large-scale image-to-3D generative models remain opaque in their inductive biases. We identify a significant limitation in image-conditioned 3D generative models: a strong canonical view bias. Through controlled experiments using simple 2D rotations, we show that the state-of-the-art Hunyuan3D 2.0 model can struggle to generalize across viewpoints, with performance degrading under rotated inputs. We show that this failure can be mitigated by a lightweight CNN that detects and corrects input orientation, restoring model performance without modifying the generative backbone. Our findings raise an important open question: Is scale enough, or should we pursue modular, symmetry-aware designs?

</details>


### [79] [Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone](https://arxiv.org/abs/2511.08215)
*Rizal Khoirul Anam*

Main category: cs.CV

TL;DR: 本文比较并评估了一种解耦的多模态食物识别流水线，结合高效视觉骨干网络与强大生成式大语言模型，实现自动营养分析和菜谱指导，尤其针对中餐数据集，系统效能以视觉前端准确率为瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着数字化食物应用增多，迫切需要自动化高效的营养分析和烹饪辅助方法。现有数据和模型多带有文化偏见，且多模态系统的误差传递机制不明，难以提升整体效能。本文提出评测框架，旨在分析和优化视觉与语言模型组合效果。

Method: 提出一种多模态流水线，视觉模块对比EfficientNet-B4、VGG-16、ResNet-50、YOLOv8，生成模块对比Gemini和Gemma大语言模型，通过自建中餐数据集（CCFD）系统性测试；同时首次形式化并分析了语义误差传播（SEP），研究分类错误如何影响生成内容的正确性。

Result: EfficientNet-B4在准确率和计算效率间表现最佳（Top-1准确率89.0%），Gemini生成的营养和菜谱信息准确度最高（9.2/10）；系统效能由视觉识别准确率主导，语义相似类别间误判率高，是主要失败来源。

Conclusion: 高质量多模态食物分析系统需以高精度视觉前端为基础，否则生成模块难以补足感知层面的错误；提升细粒度分类和减少语义相似误差，是系统有效性的关键。

Abstract: The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.

</details>


### [80] [2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time](https://arxiv.org/abs/2511.08224)
*Ignasi Mas,Ivan Huerta,Ramon Morros,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 提出了一种无需高分辨率RGB引导、可实时实现单视图3D超分辨率的2Dto3D-SR框架。通过将3D数据编码为结构化2D表示，直接利用现有2D超分模型，实验显示精度高且高效。


<details>
  <summary>Details</summary>
Motivation: 传统3D超分方法常依赖于高分辨率RGB图像引导，或采用复杂的3D点云处理，难以在缺乏RGB数据或要求实时性时应用。本工作旨在简化流程，提高适用性。

Method: 将单视角下的3D数据编码为2D结构化图像（利用PNCC方案），从而可应用成熟的2D图像超分辨率架构。分别采用Swin Transformer（高精度）与Vision Mamba（高效率）实现，对比验证性能。

Result: 基于Swin Transformer的实现达到了标准基准集上的最新最优精度，Vision Mamba实现则实现了实时推理速度且保持了有竞争力的效果。

Conclusion: 所提2Dto3D-SR方法结构简单、应用灵活，尤其适合实际场景中高分辨率RGB数据不可用时的3D超分任务，兼顾高精度与高效率，实现了几何引导的有效方案。

Abstract: We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.

</details>


### [81] [Accurate and Efficient Surface Reconstruction from Point Clouds via Geometry-Aware Local Adaptation](https://arxiv.org/abs/2511.08233)
*Eito Ogawa,Taiga Hayami,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 本文提出了一种基于输入点云曲率自适应调整局部区域间距和大小的表面重建方法，从而提升了重建的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有点云表面重建方法更多关注于整体点云或均匀分配的局部区域，通常设定固定局部区域大小，导致在几何复杂区域适应性较差。为提升在曲率变化较大场景下的适应性和泛化能力，有必要根据点云本身的几何特征调整区域划分策略。

Method: 本文方法根据输入点云的曲率信息，自适应调整每个局部区域的间距和大小。在高曲率区域采用更精细、更密集的小局部区域布局，而在低曲率区域则使用较大和稀疏的局部区域，从而提升重建的细节和整体效率。

Result: 实验表明，与以往采用固定局部区域的重建方法相比，本文方法在重建精度和效率上均取得了更优的表现，特别是在几何复杂变化区域优势更为明显。

Conclusion: 通过基于曲率自适应调整局部区域布局，本文方法有效增强了点云表面重建模型的精度与计算效率，在基础设施检测等实际应用中具有更好的适用性。

Abstract: Point cloud surface reconstruction has improved in accuracy with advances in deep learning, enabling applications such as infrastructure inspection. Recent approaches that reconstruct from small local regions rather than entire point clouds have attracted attention for their strong generalization capability. However, prior work typically places local regions uniformly and keeps their size fixed, limiting adaptability to variations in geometric complexity. In this study, we propose a method that improves reconstruction accuracy and efficiency by adaptively modulating the spacing and size of local regions based on the curvature of the input point cloud.

</details>


### [82] [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](https://arxiv.org/abs/2511.08238)
*Xiangyang Wu,Liu Liu,Baosheng Yu,Jiayan Qiu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出一种改进视觉-语言微调的方法，通过更好地利用图像中的语义关系，提高多模态基础模型在视觉与语言表达对齐与融合上的能力，并在多个任务上取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言微调方法在多模态对齐时，往往忽视了文本上下文强调的图像内部语义关系，导致模型性能不佳。为充分发挥视觉与语言的协同理解能力，有必要设计能同时利用语义和关系信息的对齐方法。

Method: 方法包括：1）利用不同视觉编码器提取多层次的图像语义特征，丰富视觉关系信息；2）学习映射视觉特征，将相关语义进行分组，从而更好地捕捉关系；3）采用可继承的跨注意力机制融合视觉与文本特征，全局移除冗余的低相关视觉-语言特征对，提高对齐效率。

Result: 在8种基础模型和图像问答、图像描述两个下游任务中，提出方法的性能均优于现有所有方法。

Conclusion: 通过结合语义和关系信息进行视觉—语言对齐和融合，本文方法显著提升多模态模型的表现，为多模态基础模型的训练和应用提供了新思路。

Abstract: Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.

</details>


### [83] [Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning](https://arxiv.org/abs/2511.08240)
*Chenyu Hu,Xiaotong Li,Hao Zhu,Biao Hou*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云深度学习方法——DiPVNet，能够更有效地处理旋转变化，提升点云分类和分割任务的表现。


<details>
  <summary>Details</summary>
Motivation: 点云在3D视觉任务中应用广泛，但在旋转扰动下表现不佳，其本质是点云方向特性易受旋转影响。以往方法难以充分捕捉和利用点云多尺度方向信息，导致特征表达能力受限。

Method: 核心包括：1）原子点积算子，能同时编码方向选择性和旋转不变性；2）局部提出可学习的局部点积（L2DP）算子，自适应捕捉邻域内非均匀结构；3）全局上，基于广义谐波分析，证明点云与球面采样向量的点积等价于方向感知的球面傅立叶变换（DASFT），并构建全局方向响应谱。作者还严格证明了这两个算子的旋转不变性。

Result: 在噪声和大角度旋转等复杂情况下进行了大量实验，DiPVNet在点云分类和分割任务中达到了最新的最优性能。

Conclusion: DiPVNet能够自适应感知方向，并具备良好的旋转对称建模能力，显著提升了点云特征表达和下游任务表现，可为3D点云处理提供新的有效工具。

Abstract: Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at https://github.com/wxszreal0/DiPVNet.

</details>


### [84] [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](https://arxiv.org/abs/2511.08248)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种高效的训练自由型开放词汇语义分割方法NERVE，结合全局与局部信息，通过从稳定扩散模型的自注意力层提取邻域结构并引入熵引导的随机游走进行关联细化，取得了最先进的零样本分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇语义分割方法在无需训练的情况下有计算复杂、空间感知局限等问题，包括昂贵的后处理与对自注意力融合方式的局限。作者希望设计一种无需训练的高效方法，同时能有效整合全局和局部信息以提升分割表现。

Method: 提出NERVE方法：1）从稳定扩散模型的自注意力层中提取邻域结构实现全局和细粒度信息的结合；2）采用随机游走代替固定尺度的高斯核来细化像素关联，更灵活地捕捉空间结构；3）利用熵引导选择最有效的自注意力图，而非简单平均多个头或层；4）无需传统后处理（如CRF或PAMR）。

Result: 在7个主流语义分割基准数据集上进行了实验，所提出方法在零样本分割任务中整体上达到最先进性能。

Conclusion: NERVE为训练自由的开放词汇语义分割提供了新的高效方法，能有效融合全局与局部信息，且不需额外复杂后处理，实验展示了其极具竞争力的性能。

Abstract: Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.

</details>


### [85] [LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning](https://arxiv.org/abs/2511.08251)
*Fengyi Fu,Mengqi Huang,Lei Zhang,Zhendong Mao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多层解耦编辑框架 LayerEdit，实现了复杂多物体图像按照文本描述精确编辑，显著提升了多物体编辑的可控性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的多物体图像编辑方法主要采用定位-编辑范式，但忽略了多物体交互区域的注意力纠缠，导致编辑时经常出现物体间修改互相干扰或单独物体编辑受限。为解决这一问题，亟需冲突感知且能解耦编辑的方法。

Method: 提出LayerEdit框架，包括：(1) 冲突感知的层分解模块，用注意力感知IoU和时间相关的区域移除提升分层时对冲突区域的感知和抑制；(2) 物体分层编辑模块，实现每层语义独立编辑及跨层几何映射；(3) 透明度引导的层融合模块，通过透明度引导学习实现结构一致的层融合。该方法为无训练方法。

Result: 大量实验证明，LayerEdit在复杂多物体场景下，能够实现比现有方法更高的物体内部可控性和物体间统一性。

Conclusion: LayerEdit首次通过精确的物体分层和一致的融合，实现了无冲突的多物体文本驱动图像编辑，为未来多物体编辑领域提供了一种新的高效框架。

Abstract: Text-driven multi-object image editing which aims to precisely modify multiple objects within an image based on text descriptions, has recently attracted considerable interest. Existing works primarily follow the localize-editing paradigm, focusing on independent object localization and editing while neglecting critical inter-object interactions. However, this work points out that the neglected attention entanglements in inter-object conflict regions, inherently hinder disentangled multi-object editing, leading to either inter-object editing leakage or intra-object editing constraints. We thereby propose a novel multi-layer disentangled editing framework LayerEdit, a training-free method which, for the first time, through precise object-layered decomposition and coherent fusion, enables conflict-free object-layered editing. Specifically, LayerEdit introduces a novel "decompose-editingfusion" framework, consisting of: (1) Conflict-aware Layer Decomposition module, which utilizes an attention-aware IoU scheme and time-dependent region removing, to enhance conflict awareness and suppression for layer decomposition. (2) Object-layered Editing module, to establish coordinated intra-layer text guidance and cross-layer geometric mapping, achieving disentangled semantic and structural modifications. (3) Transparency-guided Layer Fusion module, to facilitate structure-coherent inter-object layer fusion through precise transparency guidance learning. Extensive experiments verify the superiority of LayerEdit over existing methods, showing unprecedented intra-object controllability and inter-object coherence in complex multi-object scenarios. Codes are available at: https://github.com/fufy1024/LayerEdit.

</details>


### [86] [Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation](https://arxiv.org/abs/2511.08258)
*Jae Joong Lee,Bedrich Benes*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的方法Top2Ground，可以从航空图像直接生成高质量的地面视角图像，无需中间深度图或3D体素。


<details>
  <summary>Details</summary>
Motivation: 现有方法在根据航拍图像生成地面图像时经常需要中间表示（如深度图、3D体素），但这些中间步骤增加了不确定性和误差。极大的视角差异、视野遮挡等问题也导致生成任务非常困难，因此亟需新方法提升真实感与鲁棒性。

Method: Top2Ground以航空RGB图和估算的高度图为输入，对空间特征进行VAE编码，同时采集CLIP语义嵌入。在扩散去噪过程中，将这两类特征结合，用于条件生成。这样既保证生成图片与场景3D结构保持几何一致，又实现语义内容匹配。

Result: 在CVUSA、CVACT和Auto Arborist三大数据集上，Top2Ground平均提升了7.3%的SSIM分数，并能处理宽/窄视野等多种任务，体现出强大的泛化能力。

Conclusion: Top2Ground无需中间表示便能实现从航拍图到地面图的高质量、几何与语义一致生成，对复杂场景具有良好适应性，推动了跨视角图像合成领域的发展。

Abstract: Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.

</details>


### [87] [ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation](https://arxiv.org/abs/2511.08263)
*Yue Min,Shaobo Wang,Jiaze Li,Tianle Niu,Junxin Fan,Yongliang Miao,Lijin Yang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了ImageBindDC，一种新颖的数据浓缩框架，能在多模态场景下有效压缩数据并保持性能，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据浓缩方法在单模态数据（如仅图片或仅文本）上有效，但在多模态（如图像与文本、音频等组合）时，难以保留不同模态复杂的相关性。为了解决多模态数据压缩中信息丢失和表现退化的问题，该工作提出新方法。

Method: 创新性地基于ImageBind统一特征空间，利用Fourier域的Characteristic Function（CF）损失，实现无穷矩精确对齐。其优化目标包含三个层次：（i）单模态对齐；（ii）跨模态对齐（混合配对的分布对齐）；（iii）联合模态对齐（多元联合分布对齐），从而充分保留多模态数据结构。

Result: 在NYU-v2数据集上，仅用每类5个浓缩样本，模型表现与用全部数据训练时持平，且比最佳方法提升8.2%，压缩用时减少超过4倍，显示出方法的高效高性能。

Conclusion: ImageBindDC显著提升了多模态数据压缩的方法效果和效率，是多模态小样本学习和大模型快速训练的新突破。

Abstract: Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\% absolute improvement over the previous best method and more than 4$\times$ less condensation time.

</details>


### [88] [Re-coding for Uncertainties: Edge-awareness Semantic Concordance for Resilient Event-RGB Segmentation](https://arxiv.org/abs/2511.08269)
*Nan Bao,Yifan Zhao,Lin Zhu,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种融合事件-视觉多模态边缘信息的分割框架，有效提升了在极端条件下的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 受限于光照不足、相机剧烈运动等极端条件，普通RGB语义分割方法会因信息丢失导致结果严重下降。虽有研究引入事件传感器作为补充，但事件与RGB本质异构，导致多模态融合困难。

Method: 作者提出了Edge-awareness Semantic Concordance框架，通过显式引入边缘信息来弥合事件与RGB之间的差异。框架包括边缘感知潜在再编码（将异构模态对齐到统一的语义空间）和再编码整合与不确定性优化（利用边缘和不确定性指标提升融合效果）。

Result: 在两个合成与一个真实世界事件-RGB分割数据集上，本文方法在极端条件下优于现有方法，在DERS-XS数据集上mIoU提升2.55%，空间遮挡下表现尤为突出。

Conclusion: 融合基于边缘的潜在编码能显著缓解RGB与事件模态异构问题，使分割模型在极端环境中更具鲁棒性。

Abstract: Semantic segmentation has achieved great success in ideal conditions. However, when facing extreme conditions (e.g., insufficient light, fierce camera motion), most existing methods suffer from significant information loss of RGB, severely damaging segmentation results. Several researches exploit the high-speed and high-dynamic event modality as a complement, but event and RGB are naturally heterogeneous, which leads to feature-level mismatch and inferior optimization of existing multi-modality methods. Different from these researches, we delve into the edge secret of both modalities for resilient fusion and propose a novel Edge-awareness Semantic Concordance framework to unify the multi-modality heterogeneous features with latent edge cues. In this framework, we first propose Edge-awareness Latent Re-coding, which obtains uncertainty indicators while realigning event-RGB features into unified semantic space guided by re-coded distribution, and transfers event-RGB distributions into re-coded features by utilizing a pre-established edge dictionary as clues. We then propose Re-coded Consolidation and Uncertainty Optimization, which utilize re-coded edge features and uncertainty indicators to solve the heterogeneous event-RGB fusion issues under extreme conditions. We establish two synthetic and one real-world event-RGB semantic segmentation datasets for extreme scenario comparisons. Experimental results show that our method outperforms the state-of-the-art by a 2.55% mIoU on our proposed DERS-XS, and possesses superior resilience under spatial occlusion. Our code and datasets are publicly available at https://github.com/iCVTEAM/ESC.

</details>


### [89] [SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces](https://arxiv.org/abs/2511.08271)
*Sweta Banerjee,Timo Gosch,Sara Hester,Viktoria Weiss,Thomas Conrad,Taryn A. Donovan,Nils Porsche,Jonas Ammeling,Christoph Stroblberger,Robert Klopfleisch,Christopher Kaltenecker,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: 本研究介绍了一款名为SWAN的开源网页应用程序，通过滑动手势实现病理图像标注，在易用性和效率上优于传统文件夹分拣法，并在保持高标注一致性的同时提升了标注速度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 大规模病理图像数据集的标注是开发深度学习模型的重要环节，但现有的基于文件夹的标注流程缓慢且难以扩展。因此，提升标注效率和用户体验成为亟需解决的问题。

Method: 研究开发了SWAN（SWipeable ANnotations）这一支持桌面及移动平台的网页应用，通过滑动手势对图像补丁进行分类，并在实时收集元数据的同时，允许灵活映射滑动动作到不同类别。通过让四名病理学家标注文献性图像补丁并与传统法对比，评估其性能。

Result: SWAN在标注速度上表现优异，参与者之间配对一致性达到86.52%-93.68%（Cohen’s Kappa=0.61-0.80），与传统文件夹方法的86.98%-91.32%（Cohen’s Kappa=0.63-0.75）相当，体现了高一致性。用户高度认可其可用性和移动端支持。

Conclusion: SWAN能在保证标注质量的前提下加速图像标注过程，是一种可扩展、用户友好的新型标注方案，有望取代传统流程。

Abstract: The annotation of large scale histopathology image datasets remains a major bottleneck in developing robust deep learning models for clinically relevant tasks, such as mitotic figure classification. Folder-based annotation workflows are usually slow, fatiguing, and difficult to scale. To address these challenges, we introduce SWipeable ANnotations (SWAN), an open-source, MIT-licensed web application that enables intuitive image patch classification using a swiping gesture. SWAN supports both desktop and mobile platforms, offers real-time metadata capture, and allows flexible mapping of swipe gestures to class labels. In a pilot study with four pathologists annotating 600 mitotic figure image patches, we compared SWAN against a traditional folder-sorting workflow. SWAN enabled rapid annotations with pairwise percent agreement ranging from 86.52% to 93.68% (Cohen's Kappa = 0.61-0.80), while for the folder-based method, the pairwise percent agreement ranged from 86.98% to 91.32% (Cohen's Kappa = 0.63-0.75) for the task of classifying atypical versus normal mitotic figures, demonstrating high consistency between annotators and comparable performance. Participants rated the tool as highly usable and appreciated the ability to annotate on mobile devices. These results suggest that SWAN can accelerate image annotation while maintaining annotation quality, offering a scalable and user-friendly alternative to conventional workflows.

</details>


### [90] [MAUGIF: Mechanism-Aware Unsupervised General Image Fusion via Dual Cross-Image Autoencoders](https://arxiv.org/abs/2511.08272)
*Kunjing Yang,Zhiwei Wang,Minru Bai*

Main category: cs.CV

TL;DR: 本论文提出了一种机制感知的无监督通用图像融合方法（MAUGIF），利用双跨图像自编码器，实现多源图像结构与互补信息的有效整合，并在多种融合任务中验证了该方法的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法要么过于针对特定任务，缺乏通用性，要么是统一方案而忽略不同任务间融合机制的差异。因此需要一种能够根据不同融合任务机制调整的通用图像融合方法。

Method: 作者提出MAUGIF方法，并将不同融合任务按本质机理分为加性和乘性两类。设计了双编码器，将源图像映射到共享潜空间，提取公共内容并分离模态特有信息；在解码阶段，双解码器以特征注入器角色，将模态特有信息有选择地重新注入共享内容，构建融合图像。融合解码器结构可按融合机制定制。

Result: 在多个不同类型的图像融合任务（涉及不同机制）上进行了大量实验证明MAUGIF方法具备良好的有效性和泛化能力。

Conclusion: MAUGIF既可根据任务机理调整融合方式，又具备无监督和通用特性，提高了融合性能和可解释性。

Abstract: Image fusion aims to integrate structural and complementary information from multi-source images. However, existing fusion methods are often either highly task-specific, or general frameworks that apply uniform strategies across diverse tasks, ignoring their distinct fusion mechanisms. To address this issue, we propose a mechanism-aware unsupervised general image fusion (MAUGIF) method based on dual cross-image autoencoders. Initially, we introduce a classification of additive and multiplicative fusion according to the inherent mechanisms of different fusion tasks. Then, dual encoders map source images into a shared latent space, capturing common content while isolating modality-specific details. During the decoding phase, dual decoders act as feature injectors, selectively reintegrating the unique characteristics of each modality into the shared content for reconstruction. The modality-specific features are injected into the source image in the fusion process, generating the fused image that integrates information from both modalities. The architecture of decoders varies according to their fusion mechanisms, enhancing both performance and interpretability. Extensive experiments are conducted on diverse fusion tasks to validate the effectiveness and generalization ability of our method. The code is available at https://anonymous.4open.science/r/MAUGIF.

</details>


### [91] [SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer](https://arxiv.org/abs/2511.08291)
*Kaiyi Xu,Junchao Gong,Zhiwang Zhou,Zhangrui Li,Yuandong Pu,Yihao Liu,Ben Fei,Fenghua Ling,Wenlong Zhang,Lei Bei*

Main category: cs.CV

TL;DR: 该论文提出了SynWeather数据集和SynWeatherDiff模型，实现了多区域、多变量统一气象观测数据合成，并有效改善了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有气象数据处理方法通常只对单一变量、单一区域进行建模，且多为确定性方法，导致不能综合各变量和区域间信息、忽略变量间互补性，还容易结果过于平滑。

Method: 作者构建了SynWeather数据集，涵盖美国大陆、欧洲、东亚和热带气旋四大典型区域，并包含多个关键气象变量（如雷达反射率、小时降水、可见光和微波亮温）。同时提出SynWeatherDiff，一种基于Diffusion Transformer框架的通用概率型合成模型，以缓解过度平滑问题。

Result: 在SynWeather数据集上的实验表明，作者提出的网络在多个评测任务中相较于已有的特定任务模型和通用模型表现更优。

Conclusion: SynWeather数据集和SynWeatherDiff模型为实现更综合、高效的气象观测数据合成提供了新途径，能更好地利用多区域多变量间的信息并生成更真实的观测结果。

Abstract: With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.

</details>


### [92] [SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering](https://arxiv.org/abs/2511.08294)
*Laura Bragagnolo,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: SkelSplat提出了一种基于可微高斯渲染的人体关节点3D高精度姿态估计框架，无需3D真实标签即可多视角融合，具有强泛化能力和遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D人体姿态估计方法严重依赖大规模带注释数据，导致泛化性能差且对未知场景不鲁棒，尤其在缺乏3D真实标签或出现遮挡时效果很差。

Method: SkelSplat把人体骨架表示为各关节处的3D高斯分布，通过可微渲染优化每个关节点，对多视角图像特征无缝融合，无需3D真实标签监督。为避免高斯渲染本身设计用于稠密场景，作者提出独特的一位编码策略，实现对关节点的独立优化。

Result: 在Human3.6M和CMU数据集上，SkelSplat在不借助3D真实标签情况下表现优异，跨数据集误差较传统学习方法最多降低47.8%。在人为遮挡的Human3.6M-Occ和Occlusion-Person数据集上也体现出很强的鲁棒性，无需针对性微调。

Conclusion: SkelSplat能够实现无需3D真实标签的高精度、鲁棒的多视角3D人体姿态估计，推广性强，适用性广，是现有方法的有效替代和提升。

Abstract: Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.

</details>


### [93] [NeuSpring: Neural Spring Fields for Reconstruction and Simulation of Deformable Objects from Videos](https://arxiv.org/abs/2511.08310)
*Qingshan Xu,Jiao Liu,Shangshu Yu,Yuxuan Wang,Yuan Zhou,Junbao Zhou,Jiequan Cui,Yew-Soon Ong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了NeuSpring方法，通过神经弹簧场技术实现对可变形物体在交互下的物理数字孪生，提升了对物体当前状态建模和未来预测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物体可变形行为的物理学习方面，虽然能建模当前状态，但在未来状态预测上表现不足，主要因为未能有效利用可变形物体的固有物理属性，导致模型泛化能力和准确性受限。

Method: 提出NeuSpring方法，基于弹簧-质量模型，采用分段拓扑方案高效刻画多区域弹簧连接及材料异质性，并以神经网络建模弹簧物理属性，利用弹簧的空间相关性实现物理特性学习，从视频中实现可变形物体的重建与仿真。

Result: 在真实数据集上，NeuSpring在当前状态建模与未来预测方面均优于现有方法，Chamfer距离分别提升了20%和25%。

Conclusion: NeuSpring能够更准确地重建和预测可变形物体的物理行为，为数字孪生和仿真提供了新的解决方案。

Abstract: In this paper, we aim to create physical digital twins of deformable objects under interaction. Existing methods focus more on the physical learning of current state modeling, but generalize worse to future prediction. This is because existing methods ignore the intrinsic physical properties of deformable objects, resulting in the limited physical learning in the current state modeling. To address this, we present NeuSpring, a neural spring field for the reconstruction and simulation of deformable objects from videos. Built upon spring-mass models for realistic physical simulation, our method consists of two major innovations: 1) a piecewise topology solution that efficiently models multi-region spring connection topologies using zero-order optimization, which considers the material heterogeneity of real-world objects. 2) a neural spring field that represents spring physical properties across different frames using a canonical coordinate-based neural network, which effectively leverages the spatial associativity of springs for physical learning. Experiments on real-world datasets demonstrate that our NeuSping achieves superior reconstruction and simulation performance for current state modeling and future prediction, with Chamfer distance improved by 20% and 25%, respectively.

</details>


### [94] [Mitigating Negative Flips via Margin Preserving Training](https://arxiv.org/abs/2511.08322)
*Simone Ricci,Niccolò Biondi,Federico Pernici,Alberto Del Bimbo*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过调整logit边界与双源蒸馏损失，减少图像分类模型版本变更时的负翻转现象，并在保证整体准确率的同时提升模型一致性。


<details>
  <summary>Details</summary>
Motivation: 随着训练类别的增加，AI模型在新旧类别上的判别边界会变小，容易导致新版本模型对旧数据的性能下降（即负翻转）。这严重影响模型部署的一致性与可靠性，因此亟需方法来降低这种负面影响。

Method: 方法分为两部分：1）在logit上加入显式边界校准项，鼓励旧类别与新类别间的相对边界更大，保护原模型的判别能力；2）采用双源focal蒸馏损失，引入历史模型和新独立训练模型，同时学习合适的决策边界，兼顾原有和新数据的表现。

Result: 在图像分类基准数据集上的大量实验结果显示，该方法能有效降低负翻转率，同时维持较高的整体准确率，优于传统的训练方式。

Conclusion: 所提方法在持续学习和类别扩充场景下，能有效缓解原有类别性能退化问题，实现新旧知识的平衡融合，推动AI系统更稳定地迭代升级。

Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.

</details>


### [95] [The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment](https://arxiv.org/abs/2511.08328)
*Solveig Thrun,Stine Hansen,Zijun Sun,Nele Blum,Suaiba A. Salahuddin,Xin Wang,Kristoffer Wickstrøm,Elisabeth Wetzer,Robert Jenssen,Maik Stille,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 该论文探讨了乳腺癌定期筛查中的纵向深度学习风险建模，重点比较了不同的空间对齐策略对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习逐渐用于个性化乳腺癌筛查，准确对齐不同时间点的乳腺X光片成为关键难题；解决此问题有助于更好地利用历史影像信息，提升高危人群筛查的精准度。

Method: 作者系统比较了多种对齐方法，包括基于图像的配准，对特征空间的显式/隐式对齐，以及引入正则化的方式，在两个大型乳腺X光片数据集上，通过预测准确率、查全率、查准率及形变场质量等指标评估其效果。

Result: 结果显示：基于图像的配准在所有评估指标上都优于特征级和隐式对齐方法，产生了更准确且时序一致的结果，同时生成了流畅且解剖学上合理的变形场。虽然正则有助于提升变形质量，但却削弱了基于特征对齐的风险预测能力，而在特征空间中引入图像级的变形场取得了最佳的风险预测性能。

Conclusion: 空间对齐方式对纵向风险建模有重要影响，图像级配准尤其关键，能够明显提升个性化乳腺癌筛查的预测准确率和模型鲁棒性。该方法有望推动高危人群更早干预和精准筛查。

Abstract: Regular mammography screening is crucial for early breast cancer detection. By leveraging deep learning-based risk models, screening intervals can be personalized, especially for high-risk individuals. While recent methods increasingly incorporate longitudinal information from prior mammograms, accurate spatial alignment across time points remains a key challenge. Misalignment can obscure meaningful tissue changes and degrade model performance. In this study, we provide insights into various alignment strategies, image-based registration, feature-level (representation space) alignment with and without regularization, and implicit alignment methods, for their effectiveness in longitudinal deep learning-based risk modeling. Using two large-scale mammography datasets, we assess each method across key metrics, including predictive accuracy, precision, recall, and deformation field quality.
  Our results show that image-based registration consistently outperforms the more recently favored feature-based and implicit approaches across all metrics, enabling more accurate, temporally consistent predictions and generating smooth, anatomically plausible deformation fields. Although regularizing the deformation field improves deformation quality, it reduces the risk prediction performance of feature-level alignment. Applying image-based deformation fields within the feature space yields the best risk prediction performance.
  These findings underscore the importance of image-based deformation fields for spatial alignment in longitudinal risk modeling, offering improved prediction accuracy and robustness. This approach has strong potential to enhance personalized screening and enable earlier interventions for high-risk individuals. The code is available at https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git, allowing full reproducibility of the results.

</details>


### [96] [Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter](https://arxiv.org/abs/2511.08334)
*Zhiyang Chen,Chen Zhang,Hao Fang,Runmin Cong*

Main category: cs.CV

TL;DR: 本文提出了一种面向水下实例分割（UIS）的新方法DiveSeg，通过引入AquaStyle Aligner和ObjectPrior Prompter，在UIIS和USIS10K两个数据集上实现了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割是海洋资源探索和生态保护的重要技术。当前的视觉基础模型虽然泛化能力强，但面临水下环境特有的颜色偏移和目标识别等挑战。因此，亟需开发适合水下场景的分割方法。

Method: 作者以DINO为特征学习器，提出DiveSeg框架。该框架包含两个创新模块：（1）AquaStyle Aligner，将水下图像的颜色风格嵌入DINO的微调过程中，提高对水下域的适应；（2）ObjectPrior Prompter，利用基于二值分割的提示，融入目标级先验，增强实例级推理能力。

Result: 在UIIS和USIS10K两个主流水下实例分割数据集上，DiveSeg均达到最新的最佳性能（state-of-the-art）。

Conclusion: DiveSeg通过针对水下环境优化的风格对齐和目标先验，为水下实例分割任务带来了显著性能提升，为水下视觉理解提供了新思路。

Abstract: Underwater instance segmentation (UIS), integrating pixel-level understanding and instance-level discrimination, is a pivotal technology in marine resource exploration and ecological protection. In recent years, large-scale pretrained visual foundation models, exemplified by DINO, have advanced rapidly and demonstrated remarkable performance on complex downstream tasks. In this paper, we demonstrate that DINO can serve as an effective feature learner for UIS, and we introduce DiveSeg, a novel framework built upon two insightful components: (1) The AquaStyle Aligner, designed to embed underwater color style features into the DINO fine-tuning process, facilitating better adaptation to the underwater domain. (2) The ObjectPrior Prompter, which incorporates binary segmentation-based prompts to deliver object-level priors, provides essential guidance for instance segmentation task that requires both object- and instance-level reasoning. We conduct thorough experiments on the popular UIIS and USIS10K datasets, and the results show that DiveSeg achieves the state-of-the-art performance. Code: https://github.com/ettof/Diveseg.

</details>


### [97] [Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning](https://arxiv.org/abs/2511.08344)
*Chen Liu,Can Han,Weishi Xu,Yaqi Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 提出了一种新的基于扩散模型的数据增强方法（SASG-DA），有效提升了sEMG手势识别的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: sEMG手势识别在人机交互和康复、假肢等领域应用广泛，但因训练数据稀缺，深度学习模型容易过拟合且泛化性差。传统数据增强方法在多样性与有效性等方面存在不足。

Method: 1. 提出扩散模型数据增强框架SASG-DA。2. 通过语义表示引导（SRG）确保生成样本的真实性和任务相关性。3. 采用高斯建模的语义采样（GMSS），随机采样以兼顾真实性和多样性。4. 结合稀疏感知策略，针对未被充分代表的数据区域生成样本，提升多样性和分布覆盖度。

Result: 在多个标准sEMG数据集（Ninapro DB2, DB4, DB7）上实验，SASG-DA在准确率、泛化能力等方面显著优于现有数据增强方法。

Conclusion: SASG-DA有效缓解了数据稀缺导致的过拟合问题，并提升了sEMG基于深度学习的手势识别性能和泛化能力，兼顾了生成数据的真实性与多样性。

Abstract: Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Modeling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.

</details>


### [98] [VideoChain: A Transformer-Based Framework for Multi-hop Video Question Generation](https://arxiv.org/abs/2511.08348)
*Arpan Phukan,Anupam Pandey,Deepjyoti Bodo,Asif Ekbal*

Main category: cs.CV

TL;DR: 该论文提出了VideoChain，一个能够生成多跳视频问题（MVQG）的框架，旨在推动视频内容下的复杂推理问题生成。通过融合文本与视频特征，并基于大规模生成的新数据集，VideoChain在多项评估指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问题生成主要集中在文本领域，而视频问题生成又大多局限于单段、零跳问题。缺少能够综合多段视频、要求复杂推理能力的问题生成方法。因此，作者希望推动多段、跨时长视频内容的推理式问题生成。

Method: 提出了VideoChain框架，基于改进版BART结构，并加入视频嵌入特征，从而同时捕捉文本与视觉依赖；通过自动化方式将TVQA+数据集中的零跳问题配对合并，构建了大规模MVQ-60多跳视频问答数据集。

Result: VideoChain在ROUGE-L、ROUGE-1、BLEU-1、BERTScore-F1和语义相似度等文本生成指标上表现领先，说明方法有效提升了生成问题的连贯性、语境相关性和推理深度。

Conclusion: VideoChain有效实现了多段视频内容下复杂推理问题的生成，推动了多跳视频问答领域的发展，并为后续视频理解研究带来了新方向。

Abstract: Multi-hop Question Generation (QG) effectively evaluates reasoning but remains confined to text; Video Question Generation (VideoQG) is limited to zero-hop questions over single segments. To address this, we introduce VideoChain, a novel Multi-hop Video Question Generation (MVQG) framework designed to generate questions that require reasoning across multiple, temporally separated video segments. VideoChain features a modular architecture built on a modified BART backbone enhanced with video embeddings, capturing textual and visual dependencies. Using the TVQA+ dataset, we automatically construct the large-scale MVQ-60 dataset by merging zero-hop QA pairs, ensuring scalability and diversity. Evaluations show VideoChain's strong performance across standard generation metrics: ROUGE-L (0.6454), ROUGE-1 (0.6854), BLEU-1 (0.6711), BERTScore-F1 (0.7967), and semantic similarity (0.8110). These results highlight the model's ability to generate coherent, contextually grounded, and reasoning-intensive questions.

</details>


### [99] [Extreme Model Compression with Structured Sparsity at Low Precision](https://arxiv.org/abs/2511.08360)
*Dan Liu,Nikita Dvornik,Xue Liu*

Main category: cs.CV

TL;DR: 本文提出了SLOPE框架，有效结合了权重量化与结构化稀疏性，大幅减少模型体积且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络通常参数量大、计算成本高，难以部署在资源受限的设备上。权重量化与结构化稀疏性分别能减小模型，但单独应用；若直接叠加，两者会严重损害模型性能。

Method: 提出了SLOPE框架，在训练阶段加入一种正则化方法，鼓励稀疏且量化的权重与全精度权重在角度上对齐，而非直接数值匹配，有效缓解精度损失。

Result: 在ResNet-18上，SLOPE实现了约20倍的模型压缩，准确率保持在原模型的约99%。在分类、检测、分割多任务上，SLOPE在ResNet-18、ViT-Small及Mask R-CNN等模型上，性能均优于现有最先进方法。

Conclusion: SLOPE能够科学、高效地融合量化和稀疏技术，显著压缩DNN模型且精度损失极低，适用于多种任务和主流网络结构。

Abstract: Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.

</details>


### [100] [Retrospective motion correction in MRI using disentangled embeddings](https://arxiv.org/abs/2511.08365)
*Qi Wang,Veronika Ecker,Marcel Früh,Sergios Gatidis,Thomas Küstner*

Main category: cs.CV

TL;DR: 本文提出了一种分层矢量量化变分自编码器（VQ-VAE），用于通用磁共振成像（MRI）运动伪影校正，通过解耦并捕捉运动伪影的潜在特征，实现对新型和不同类型运动伪影的有效修正。


<details>
  <summary>Details</summary>
Motivation: 现有MRI运动伪影校正方法难以泛化到不同运动类型和身体部位，尤其是基于机器学习的方法通常受限于特定的数据集和应用场景，缺乏普适性。作者假设运动伪影存在可被提取和利用的共同内在特征。

Method: 提出了一种分层矢量量化变分自编码器，对运动伪影到清晰图像之间的特征进行解耦表示。多分辨率码本用于捕捉有限的运动特征模式，实现由粗到细的校正。同时训练自回归模型，学习无运动伪影图像的先验分布并指导校正过程。该方法无需特定伪影的训练即可推广至新的运动模式。

Result: 在模拟的全身运动伪影数据上实验，方法能够在不同运动严重程度下实现稳健的伪影校正，表明模型有效地解耦并捕捉了物理运动特征。

Conclusion: 提出的方法提高了MRI运动伪影校正的泛化能力，对于不同解剖区域和多样化运动形式均有潜在应用价值。

Abstract: Physiological motion can affect the diagnostic quality of magnetic resonance imaging (MRI). While various retrospective motion correction methods exist, many struggle to generalize across different motion types and body regions. In particular, machine learning (ML)-based corrections are often tailored to specific applications and datasets. We hypothesize that motion artifacts, though diverse, share underlying patterns that can be disentangled and exploited. To address this, we propose a hierarchical vector-quantized (VQ) variational auto-encoder that learns a disentangled embedding of motion-to-clean image features. A codebook is deployed to capture finite collection of motion patterns at multiple resolutions, enabling coarse-to-fine correction. An auto-regressive model is trained to learn the prior distribution of motion-free images and is used at inference to guide the correction process. Unlike conventional approaches, our method does not require artifact-specific training and can generalize to unseen motion patterns. We demonstrate the approach on simulated whole-body motion artifacts and observe robust correction across varying motion severity. Our results suggest that the model effectively disentangled physical motion of the simulated motion-effective scans, therefore, improving the generalizability of the ML-based MRI motion correction. Our work of disentangling the motion features shed a light on its potential application across anatomical regions and motion types.

</details>


### [101] [A Circular Argument : Does RoPE need to be Equivariant for Vision?](https://arxiv.org/abs/2511.08368)
*Chase van de Geijn,Timo Lüddecke,Polina Turishcheva,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 本文围绕RoPE在高维数据中的推广及其等变性作用展开，提出Spherical RoPE，实验证明其性能优异，挑战了相对位置编码必要性的传统观点。


<details>
  <summary>Details</summary>
Motivation: RoPE因其在自然语言处理中的有效性受到关注，但其机制和在高维数据如图像视频中的应用尚不完全明确。作者试图数学化地解释RoPE的等变性，并探讨这种等变性是否真的是RoPE有效的核心原因。

Method: 作者首先数学分析RoPE，说明其是一维数据等变性位置嵌入的一般解；再探讨其在多维数据（M维）情形下的类比——Mixed RoPE，要求生成元可交换以保持等变性。随后作者提出Spherical RoPE，放宽生成元可交换性假设，并在实验中与已有方法对比。

Result: 实验结果显示，Spherical RoPE在学习行为上表现等同或优于等变性模型，说明严格等变性不是RoPE性能的唯一或主要来源。

Conclusion: 本文挑战了位置编码必须保持相对性和等变性的传统假设，并提出更灵活的Spherical RoPE，为视觉任务设计更快更泛化的位置编码提供了新思路。

Abstract: Rotary Positional Encodings (RoPE) have emerged as a highly effective technique for one-dimensional sequences in Natural Language Processing spurring recent progress towards generalizing RoPE to higher-dimensional data such as images and videos. The success of RoPE has been thought to be due to its positional equivariance, i.e. its status as a relative positional encoding. In this paper, we mathematically show RoPE to be one of the most general solutions for equivariant positional embedding in one-dimensional data. Moreover, we show Mixed RoPE to be the analogously general solution for M-dimensional data, if we require commutative generators -- a property necessary for RoPE's equivariance. However, we question whether strict equivariance plays a large role in RoPE's performance. We propose Spherical RoPE, a method analogous to Mixed RoPE, but assumes non-commutative generators. Empirically, we find Spherical RoPE to have the equivalent or better learning behavior compared to its equivariant analogues. This suggests that relative positional embeddings are not as important as is commonly believed, at least within computer vision. We expect this discovery to facilitate future work in positional encodings for vision that can be faster and generalize better by removing the preconception that they must be relative.

</details>


### [102] [Text-based Aerial-Ground Person Retrieval](https://arxiv.org/abs/2511.08369)
*Xinyu Zhou,Yu Wu,Jiayao Ma,Wenhao Wang,Min Cao,Mang Ye*

Main category: cs.CV

TL;DR: 提出了一个针对空地异构图像和文本检索问题的新任务——基于文本的空地协同行人检索（TAG-PR），并构建了新的数据集和方法，显著提升了异构视角下的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于文本的行人检索通常只关注地面视角，缺乏对空中视角的考虑。而实际应用中空地异构视角检索更具现实意义，但也因视角差异巨大而带来独特挑战。

Method: 贡献包括：（1）TAG-PEDES数据集：从公开数据集构建并自动生成多样化文本描述，强化模型对异构视角的鲁棒性；（2）TAG-CLIP：基于层次路由专家混合模块，联合学习视角特定与无关特征，并用视角解耦策略提升跨模态对齐能力。

Result: TAG-CLIP在新建的TAG-PEDES数据集以及现有T-PR基准上进行了实验，证实了其在处理视角异构和跨模态检索上的有效性。

Conclusion: TAG-PR任务、数据集及方法推动了文本-图像跨模态行人检索研究向更实际、复杂的场景迈进，相关资源已开源，便于后续研究发展。

Abstract: This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at https://github.com/Flame-Chasers/TAG-PR.

</details>


### [103] [RAPTR: Radar-based 3D Pose Estimation using Transformer](https://arxiv.org/abs/2511.08387)
*Sorachi Kato,Ryoma Yataka,Pu Perry Wang,Pedro Miraldo,Takuya Fujihashi,Petros Boufounos*

Main category: cs.CV

TL;DR: 本文提出了一种新的雷达室内三维人体姿态估计方法RAPTR，只需较易采集的三维包围盒和二维关键点标签，实现更高精度的人体姿态重建，而无需高昂的三维关键点标注成本。


<details>
  <summary>Details</summary>
Motivation: 以往雷达姿态估计依赖难以获取的三维关节点标签，尤其在复杂室内环境（如遮挡、多人体）下成本更高。作者希望通过弱监督、降本增效的标签方式推动该领域进步。

Method: RAPTR采用两阶段姿态解码器结构。首先通过伪三维可变形注意力机制结合雷达多视角特征，初步解码三维姿态并利用三维包围盒标签设计三维模板损失来减小深度歧义，再通过二维关键点标签和三维重力损失进行二次细化。训练仅需三维包围盒和二维关键点标签。

Result: 在HIBER和MMVR两个室内雷达数据集上，RAPTR将关节点位置误差分别降低了34.3%和76.9%，显著优于现有方法。

Conclusion: RAPTR只需易获取标签便实现室内雷达三维人体姿态高精度重建，为复杂场景下的低成本人体动作感知提供了有效方案。

Abstract: Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

</details>


### [104] [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](https://arxiv.org/abs/2511.08402)
*Difei Gu,Yunhe Gao,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 提出了一种新的精细化医学影像-语言模型Anatomy-VLM，有效结合解剖学信息和临床知识，提升疾病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLM）在医学影像诊断中只能粗略整体处理图像，忽视了对诊断至关重要的细粒度细节。而临床医生通常会基于医学知识，定位和分析重要解剖结构，因此需要一种能模仿临床医生流程的方法。

Method: Anatomy-VLM分为三步：1）模型编码器先从整张医学图像中自动定位关键解剖结构；2）将这些重要区域与结构化医学知识结合，提升局部信息的上下文理解；3）多尺度的信息被编码器对齐，用于生成临床可解释的疾病预测结果。

Result: Anatomy-VLM在分布内外的多项数据集上表现优异。同时在医学影像分割等下游任务也验证了其微观结构与病理信息的捕捉能力。此外，模型支持零样本解剖学解释，具备专家级的临床解读能力。

Conclusion: Anatomy-VLM通过精细化的影像特征处理和医学知识结合，在提高诊断准确性、可解释性感知和泛化能力方面表现突出，有助于实现更智能、更可信赖的辅助诊断。

Abstract: Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.

</details>


### [105] [OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2511.08423)
*Yuncheng Guo,Junyan Ye,Chenjue Zhang,Hengrui Kang,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 提出了OmniAID，一个基于解耦专家混合架构的AI生成图像检测器，能够更好地泛化到多生成模型和多语义内容，并引入了新数据集Mirage，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AIGI检测方法往往将内容相关和内容无关特征混为一谈，且受限于过时的数据集，导致在真实多样环境下泛化能力差。

Method: 提出了解耦的专家混合架构OmniAID：为不同语义领域（如人、动物）设置路由型专门专家，再配合通用伪造专家，并采用两阶段独立训练与路由训练，明确分离内容相关和无关伪造特征。

Result: 无论在常用数据集还是新构建的Mirage数据集上，OmniAID都显著优于当前单一检测器，展示了在多种模型和多样内容上的强泛化能力。

Conclusion: OmniAID成为AIGI检测新基准，能够有效应对现代及真实世界中的复杂威胁，显著提升了泛化能力和实用性。

Abstract: A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.

</details>


### [106] [Cross-pyramid consistency regularization for semi-supervised medical image segmentation](https://arxiv.org/abs/2511.08435)
*Matus Bojko,Maros Kollar,Marek Jakab,Wanda Benesova*

Main category: cs.CV

TL;DR: 本文提出了结合跨金字塔一致性正则（CPCR）的半监督医学图像分割新方法，通过双分支金字塔网络(DBPNet)中的两个解码器实现更有效的无标签数据利用，在多个公开数据集上优于已有主流方法。


<details>
  <summary>Details</summary>
Motivation: 为医学图像分割任务中无标签数据利用不充分及现有一致性学习方法存在不足的问题，亟需提升半监督分割模型的精度与泛化能力。

Method: 构建了Dual Branch Pyramid Network (DBPNet)，包括一个编码器和两个略有不同的解码器，在多分辨率尺度上生成辅助预测。提出了Cross-Pyramid Consistency Regularization (CPCR)学习策略，将一致性学习与不确定性最小化结合，对解码器主输出施加正则，并创新性地将软标签设置推广到分支间的金字塔特征层，实现知识蒸馏。

Result: 在公开医学图像分割基准数据集上，DBPNet结合CPCR方法超越了五种主流自监督学习方法，与当前最新方法表现相当。

Conclusion: 所提DBPNet+CPCR框架能更有效利用无标签数据，提升半监督医学图像分割性能，对相关领域有实际应用价值。

Abstract: Semi-supervised learning (SSL) enables training of powerful models with the assumption of limited, carefully labelled data and a large amount of unlabeled data to support the learning. In this paper, we propose a hybrid consistency learning approach to effectively exploit unlabeled data for semi-supervised medical image segmentation by leveraging Cross-Pyramid Consistency Regularization (CPCR) between two decoders. First, we design a hybrid Dual Branch Pyramid Network (DBPNet), consisting of an encoder and two decoders that differ slightly, each producing a pyramid of perturbed auxiliary predictions across multiple resolution scales. Second, we present a learning strategy for this network named CPCR that combines existing consistency learning and uncertainty minimization approaches on the main output predictions of decoders with our novel regularization term. More specifically, in this term, we extend the soft-labeling setting to pyramid predictions across decoders to support knowledge distillation in deep hierarchical features. Experimental results show that DBPNet with CPCR outperforms five state-of-the-art self-supervised learning methods and has comparable performance with recent ones on a public benchmark dataset.

</details>


### [107] [Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification](https://arxiv.org/abs/2511.08464)
*Anh Mai Vu,Tuan L. Vo,Ngoc Lam Quang Bui,Nam Nguyen Le Binh,Akash Awasthi,Huy Quoc Vo,Thanh-Huy Nguyen,Zhu Han,Chandra Mohan,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的可解释性方法（CIG），能更好地揭示病理全景切片图像（WSI）AI诊断模型的决策依据。


<details>
  <summary>Details</summary>
Motivation: WSI在计算病理学诊断中广泛应用，但现有的归因方法（如Integrated Gradients）在高分辨率WSI上常常无法突出区分不同肿瘤亚型的关键信号，影响模型解释性与临床信任。

Method: 作者提出对比归因方法CIG，通过在logit空间计算特定类别与参考类别的特征归因差异，更准确地定位区分肿瘤与非肿瘤的区域，并引入了MIL-AIC和MIL-SIC两个指标，评估在弱监督条件下归因区域对模型预测的影响。

Result: 在三个包含不同癌症类型的数据集上（CAMELYON16、TCGA-RCC、TCGA-Lung），CIG无论在定量指标还是直观可视化效果上，都比传统方法更能突出真实肿瘤区域，与专家标注高度一致。

Conclusion: CIG显著提升了WSI病理图像诊断中模型的可解释性，为AI辅助的临床应用提供了更可信、透明的诊断依据。

Abstract: Interpretability is essential in Whole Slide Image (WSI) analysis for computational pathology, where understanding model predictions helps build trust in AI-assisted diagnostics. While Integrated Gradients (IG) and related attribution methods have shown promise, applying them directly to WSIs introduces challenges due to their high-resolution nature. These methods capture model decision patterns but may overlook class-discriminative signals that are crucial for distinguishing between tumor subtypes. In this work, we introduce Contrastive Integrated Gradients (CIG), a novel attribution method that enhances interpretability by computing contrastive gradients in logit space. First, CIG highlights class-discriminative regions by comparing feature importance relative to a reference class, offering sharper differentiation between tumor and non-tumor areas. Second, CIG satisfies the axioms of integrated attribution, ensuring consistency and theoretical soundness. Third, we propose two attribution quality metrics, MIL-AIC and MIL-SIC, which measure how predictive information and model confidence evolve with access to salient regions, particularly under weak supervision. We validate CIG across three datasets spanning distinct cancer types: CAMELYON16 (breast cancer metastasis in lymph nodes), TCGA-RCC (renal cell carcinoma), and TCGA-Lung (lung cancer). Experimental results demonstrate that CIG yields more informative attributions both quantitatively, using MIL-AIC and MIL-SIC, and qualitatively, through visualizations that align closely with ground truth tumor regions, underscoring its potential for interpretable and trustworthy WSI-based diagnostics

</details>


### [108] [Generalizable Blood Cell Detection via Unified Dataset and Faster R-CNN](https://arxiv.org/abs/2511.08465)
*Siddharth Sahay*

Main category: cs.CV

TL;DR: 本论文提出了一种用于显微图像中血液细胞自动分类与检测的完整方法，并在多个公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前血液细胞自动分析存在数据稀缺和异质性问题，限制了自动化诊断系统的准确性和泛化能力。为此，需要构建统一的数据资源并探索高效的深度学习方法。

Method: 作者首先标准化并整合了四个公开血液细胞数据集（PBC、BCCD、Chula、Sickle Cell），建立了统一数据管道。然后采用基于ResNet-50-FPN主干网络的Faster R-CNN目标检测框架，比较了随机初始化（Regimen 1）和迁移学习（Regimen 2，使用COCO数据集预训练权重）两种训练策略。

Result: 迁移学习模型相比随机初始化模型收敛更快，训练过程更稳定，最终验证损失达到0.08666，效果显著优于基线。

Conclusion: 本文的方法为血液病自动诊断系统奠定了坚实的技术基础，具备高精度和实用性，有助于推动自动化血液分析在临床中的应用。

Abstract: This paper presents a comprehensive methodology and comparative performance analysis for the automated classification and object detection of peripheral blood cells (PBCs) in microscopic images. Addressing the critical challenge of data scarcity and heterogeneity, robust data pipeline was first developed to standardize and merge four public datasets (PBC, BCCD, Chula, Sickle Cell) into a unified resource. Then employed a state-of-the-art Faster R-CNN object detection framework, leveraging a ResNet-50-FPN backbone. Comparative training rigorously evaluated a randomly initialized baseline model (Regimen 1) against a Transfer Learning Regimen (Regimen 2), initialized with weights pre-trained on the Microsoft COCO dataset. The results demonstrate that the Transfer Learning approach achieved significantly faster convergence and superior stability, culminating in a final validation loss of 0.08666, a substantial improvement over the baseline. This validated methodology establishes a robust foundation for building high-accuracy, deployable systems for automated hematological diagnosis.

</details>


### [109] [Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding](https://arxiv.org/abs/2511.08480)
*Da Li,Yuxiao Luo,Keping Bi,Jiafeng Guo,Wei Yuan,Biao Yang,Yan Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练流程——CoMa，通过在对比学习前加入一个简短的预训练阶段，让视觉-语言模型（VLM）能高效获得更优的语义嵌入表示，从而在各种多模态任务中取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽然能通过大规模对比学习提升多模态任务表现，但如何全面保留输入的语义内容，并突出下游任务判别性特征，同时兼顾效率与效果，仍是一大挑战。作者认为可以将理解能力的学习与判别能力的学习解耦，进一步优化模型表现。

Method: 作者提出了CoMa方法：在对比学习之前加入一个压缩式的预训练“热身”阶段，用少量数据就能快速让VLM学到更全面的语义理解，再进行标准的对比学习以突出判别性。

Result: 实验表明，仅需少量预训练数据，CoMa即能显著提升模型下游任务表现，在MMEB基准数据集上达到同类VLM中最优水平，实现了效率与效果的双优化。

Conclusion: CoMa方法有效提升了视觉-语言模型的嵌入能力与任务表现，并且训练高效，展示了解耦训练目标的新范式，对后续多模态表示学习研究具有重要意义。

Abstract: Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.

</details>


### [110] [Fast Multi-Organ Fine Segmentation in CT Images with Hierarchical Sparse Sampling and Residual Transformer](https://arxiv.org/abs/2511.08509)
*Xueqi Guo,Halid Ziya Yerebakan,Yoshihisa Shinagawa,Kritika Iyer,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 本文提出了一种结合分层稀疏采样和残差Transformer的快速多器官分割框架，可在保证运算效率的同时提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习的三维医学图像多器官分割方法虽然精度高，但耗时和内存消耗大。部分场景采用分类器进行感兴趣点分割，但速度和精度之间存在权衡。因此亟需开发既快又准确的多器官分割新方法。

Method: 提出分层稀疏采样策略，通过多分辨率分层采集代表性体素，减少冗余运算；引入残差Transformer分割网络，能高效融合不同层次的信息，有效提取稀疏描述符的特征，同时保持较低的算力开销。

Result: 在内部数据集（10,253例CT）和公开TotalSegmentator数据集上，新方法在定性和定量分割表现均优于现有快速分类器。在CPU上，单例分割仅需约2.24秒，处理速度极快。

Conclusion: 新方法实现了高效、准确的多器官分割，具备实时精细分割的潜力，可用于临床自动化流。

Abstract: Multi-organ segmentation of 3D medical images is fundamental with meaningful applications in various clinical automation pipelines. Although deep learning has achieved superior performance, the time and memory consumption of segmenting the entire 3D volume voxel by voxel using neural networks can be huge. Classifiers have been developed as an alternative in cases with certain points of interest, but the trade-off between speed and accuracy remains an issue. Thus, we propose a novel fast multi-organ segmentation framework with the usage of hierarchical sparse sampling and a Residual Transformer. Compared with whole-volume analysis, the hierarchical sparse sampling strategy could successfully reduce computation time while preserving a meaningful hierarchical context utilizing multiple resolution levels. The architecture of the Residual Transformer segmentation network could extract and combine information from different levels of information in the sparse descriptor while maintaining a low computational cost. In an internal data set containing 10,253 CT images and the public dataset TotalSegmentator, the proposed method successfully improved qualitative and quantitative segmentation performance compared to the current fast organ classifier, with fast speed at the level of ~2.24 seconds on CPU hardware. The potential of achieving real-time fine organ segmentation is suggested.

</details>


### [111] [CleverBirds: A Multiple-Choice Benchmark for Fine-grained Human Knowledge Tracing](https://arxiv.org/abs/2511.08512)
*Leonie Bossemeyer,Samuel Heinrich,Grant Van Horn,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: 本论文提出了CleverBirds这一大规模知识追踪基准数据集，旨在研究细粒度鸟类识别过程中人类专家认知发展的过程，并对视觉知识追踪方法进行测试和发展。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉识别在诸多专业领域非常重要，但掌握这种能力需要长期训练。当前人们对人类专家认知提升的过程了解有限，尤其难以准确追踪学习者的知识状态。为此，急需一个能够大规模分析视觉学习过程的真实数据基准。

Method: 研究者依托eBird平台，收集了超过40,000名参与者在多项鸟类细粒度识别测验中的答题数据，总计逾1,700万次选择题，覆盖1万多种鸟类。每个参与者平均答题超400道，展现了长时间的学习轨迹。研究团队将这些数据整理成CleverBirds数据集，并用于知识追踪算法的评测。

Result: CleverBirds数据集的分析显示，追踪学习者知识状态极具挑战，复杂性涉及不同的参与者子群体和题目类型。上下文信息对知识预测的帮助程度各异。CleverBirds成为细粒度视觉领域数据量最大、可学概念最多的基准集之一。

Conclusion: CleverBirds为视觉领域专家能力发展和知识追踪研究提供了重要资源，有望推动对个体和群体视觉学习过程的深入理解及相关算法的发展。

Abstract: Mastering fine-grained visual recognition, essential in many expert domains, can require that specialists undergo years of dedicated training. Modeling the progression of such expertize in humans remains challenging, and accurately inferring a human learner's knowledge state is a key step toward understanding visual learning. We introduce CleverBirds, a large-scale knowledge tracing benchmark for fine-grained bird species recognition. Collected by the citizen-science platform eBird, it offers insight into how individuals acquire expertize in complex fine-grained classification. More than 40,000 participants have engaged in the quiz, answering over 17 million multiple-choice questions spanning over 10,000 bird species, with long-range learning patterns across an average of 400 questions per participant. We release this dataset to support the development and evaluation of new methods for visual knowledge tracing. We show that tracking learners' knowledge is challenging, especially across participant subgroups and question types, with different forms of contextual information offering varying degrees of predictive benefit. CleverBirds is among the largest benchmark of its kind, offering a substantially higher number of learnable concepts. With it, we hope to enable new avenues for studying the development of visual expertize over time and across individuals.

</details>


### [112] [UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist](https://arxiv.org/abs/2511.08521)
*Zhengyang Liang,Daoan Zhang,Huichi Zhou,Rui Huang,Bobo Li,Yuechen Zhang,Shengqiong Wu,Xiaohan Wang,Jiebo Luo,Lizi Liao,Hao Fei*

Main category: cs.CV

TL;DR: 本文提出了UniVA，一个开源、全能型多智能体视频通用框架，能够将视频理解、分割、编辑与生成这些能力自然融合，实现复杂视频任务自动化和端到端工作流。作者还发布了UniVA-Bench基准，专为多步视频任务评测设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在视频生成或理解等单一任务上表现突出，但实际需求往往要求多任务、多步骤、协作性的复杂工作流，单一模型或端到端大模型难以高效应对。为此，作者设计了一个能够统一调度各类视频处理工具、多智能体协作的框架平台。

Method: UniVA框架采用Plan-and-Act双智能体架构：规划智能体负责解析用户意图并生成结构化处理方案，执行智能体调用MCU模块化工具完成分析、编辑、生成等具体任务。系统通过多层次记忆管理（全局知识、任务上下文、用户偏好）实现长时推理、上下文保持与智能体间高效通信。工作流支持多条件生成、编辑、分割及合成等任意步骤自由组合。

Result: UniVA实现了复杂、可交互、可追溯的智能视频处理流程，能够自由组合多任务、多条件流程，有效弥补现有单一模型的不足。UniVA-Bench基准进一步验证了系统在多步视频任务上的有效性和通用性。

Conclusion: UniVA与UniVA-Bench的推出，推动了交互式、多智能体、通用型视频智能系统的研究，为多模态AI视频处理提供了新范式。两者开源，有望成为下一代视频AI系统的重要基础。

Abstract: While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\rightarrow$ multi-round editing $\rightarrow$ object segmentation $\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)

</details>


### [113] [Large Sign Language Models: Toward 3D American Sign Language Translation](https://arxiv.org/abs/2511.08535)
*Sen Zhang,Xiaoxiao He,Di Liu,Zhaoyang Xia,Mingyu Zhao,Chaowei Tan,Vivian Li,Bo Liu,Dimitris N. Metaxas,Mubbasir Kapadia*

Main category: cs.CV

TL;DR: 本文提出Large Sign Language Models（LSLM），能够直接利用3D美式手语数据进行翻译，大幅提高听障人士的虚拟交流的准确性与鲁棒性。该方法不仅用于ASL翻译，还拓展了LLM对多模态、具身语言的处理能力，推动智能系统的多样化交流。


<details>
  <summary>Details</summary>
Motivation: 当前手语识别依赖于2D视频，难以捕捉复杂的空间与深度信息，影响翻译效果。为提升数字交流的无障碍性，有必要探索更丰富数据（如3D数据）和更强大的模型架构（如LLM）对手语翻译的作用。

Method: 该方法提出将LLM作为骨干网络，输入3D手语数据（含空间、动作、深度特征），以实现高精度手语识别和翻译。研究包括直接从3D特征到文本的翻译，以及基于指令的灵活翻译两种模式。

Result: 实验表明，直接利用3D数据的LSLM模型在翻译准确率与鲁棒性上均优于传统2D方法，同时其多模态处理框架为手语等复杂语言的理解提供了技术基础。

Conclusion: LSLM不仅为ASL及手语翻译提供有效解决方案，还推动了LLM向多模态、人类多样性语言理解的能力发展，为构建包容性、多模态智能系统奠定基础。

Abstract: We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.

</details>


### [114] [3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation](https://arxiv.org/abs/2511.08536)
*Yunhong He,Zhengqing Yuan,Zhengzhong Tu,Yanfang Ye,Lichao Sun*

Main category: cs.CV

TL;DR: 本文提出了3D4D框架，实现了基于WebGL和Supersplat渲染的交互式4D可视化，可高效实时探索复杂4D场景。


<details>
  <summary>Details</summary>
Motivation: 随着数据复杂度的提升，对高维（如4D）数据可视化与交互的需求不断增长，但现有工具通常局限于静态或低维表示。

Method: 作者提出了3D4D框架，将静态图像和文本通过四个核心模块转换为有机的4D场景，并结合注视渲染策略，实现自适应、实时多模态交互。

Result: 框架可实现高效的4D场景浏览，支持用户驱动的自适应探索，兼具实时性和多模态交互。

Conclusion: 3D4D为高维数据可视化和交互提供了创新高效的解决方案，提升了用户在复杂4D环境中的探索与理解能力。

Abstract: We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.

</details>


### [115] [RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses](https://arxiv.org/abs/2511.08545)
*Sriram Srinivasan,Gautam Ramachandra*

Main category: cs.CV

TL;DR: 该论文提出了一种方法，能够在相机外参有噪声的情况下，从多视角图像直接重建高质量、可编辑的3D网格，并联动优化相机姿态和隐式场景表达，使结果既兼具准确度又易于实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF等基于神经隐式体积的三维重建方法对高精度的相机位姿依赖性强，但实际获得精准外参十分困难。此外，隐式体积表达与主流的多边形网格存在格式差异，导致在三维软件和机器人领域的应用效率低下。因此需要一种鲁棒的三维重建方法，适应噪声外参，且生成格式友好的网格模型。

Method: 提出一种端到端的框架，能够在多视角图像下联动地优化相机姿态并学习隐式场景表达，最终高质量地生成可编辑的三维网格。该方法在噪声外参下精细建模物体的几何与贴图，并使生成的网格直接兼容主流3D图形或机器人工具。

Result: 在标准数据集上进行实验，所提方法能在存在姿态不确定性的情况下，依然实现准确且鲁棒的三维重建，优于现有方法，并生成效果优秀、易于应用的网格结果。

Conclusion: 该方法有效缩小了神经隐式三维重建和实际机器人应用间的差距，为多视角三维重建提供了兼顾精度与实用性的解决方案。

Abstract: Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.

</details>


### [116] [Vision Transformer Based User Equipment Positioning](https://arxiv.org/abs/2511.08549)
*Parshwa Shah,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez,Siddhartan Govindasamy*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的Vision Transformer (ViT) 架构，用于从CSI矩阵的角度延迟剖面（ADP）实现用户设备（UE）定位，提升了定位精度，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在UE定位中的应用存在两个问题：一是对所有输入给予同等关注度，二是对非序列数据（如仅有瞬时CSI）不适配。因此，亟需能针对CSI特征分配不同关注度的方法。

Method: 作者提出使用注意力机制的ViT模型，重点关注CSI矩阵的角度延迟剖面（ADP）特征，通过模型学习关键ADP信息来提升定位准确率，并在DeepMIMO和ViWi等数据集上进行实验验证。

Result: 在DeepMIMO数据集下，室内定位RMSE为0.55米，室外为13.59米；在ViWi室外阻塞场景下RMSE为3.45米。该方法比现有最优方法提升了约38%的效果，且在误差距离分布上也有明显优势。

Conclusion: 基于注意力机制的ViT架构能有效提升CSI场景下的UE定位精度，显著优于现有方法，表明该范式在瞬时CSI和非序列数据环境下具有很大应用价值。

Abstract: Recently, Deep Learning (DL) techniques have been used for User Equipment (UE) positioning. However, the key shortcomings of such models is that: i) they weigh the same attention to the entire input; ii) they are not well suited for the non-sequential data e.g., when only instantaneous Channel State Information (CSI) is available. In this context, we propose an attention-based Vision Transformer (ViT) architecture that focuses on the Angle Delay Profile (ADP) from CSI matrix. Our approach, validated on the `DeepMIMO' and `ViWi' ray-tracing datasets, achieves an Root Mean Squared Error (RMSE) of 0.55m indoors, 13.59m outdoors in DeepMIMO, and 3.45m in ViWi's outdoor blockage scenario. The proposed scheme outperforms state-of-the-art schemes by $\sim$ 38\%. It also performs substantially better than other approaches that we have considered in terms of the distribution of error distance.

</details>


### [117] [SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology](https://arxiv.org/abs/2511.08573)
*Shanaka Liyanaarachchi,Chathurya Wijethunga,Shihab Aaquil Ahamed,Akthas Absar,Ranga Rodrigo*

Main category: cs.CV

TL;DR: 本文提出了一种名为SENCA-st的新方法，通过共享编码器和邻域交叉注意机制，融合和保留空间转录组学与组织病理学图像的关键信息，并显著提升了肿瘤结构异质性和微环境区分的效果。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学与组织病理图像的联合分析方法存在互相掩盖重要信息的问题，导致空间表达区分不准或结构信息过度平滑，从而损失肿瘤相关的关键功能区信息。为此，亟需在两类数据之间更好地维持和融合不同特征。

Method: 提出SENCA-st架构，利用共享编码器和邻域交叉注意力机制融合空间转录组学与组织病理图像信息，能够突出组织结构相似但基因表达功能不同的区域。

Result: SENCA-st在肿瘤异质性与肿瘤微环境区域识别上取得了优于现有方法的性能，能更准确地检测关键功能区域。

Conclusion: SENCA-st有效结合并强化空间转录组与组织病理学图像的不同特征，在肿瘤多样性与微环境分析等临床关键环节表现突出，推动了跨模态肿瘤区分方法的进步。

Abstract: Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [118] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本文首次将RAG方法应用于台湾历史档案（如热兰遮城与台湾省议会公报）数据，系统评估了查询特征和元数据整合策略对检索和生成任务的影响。早期元数据整合提升了系统表现，但RAG在生成幻觉和处理复杂历史查询上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法大多应用于英文或现代语料，少有针对台湾历史档案与繁体中文历史文献的系统性研究。本文旨在探索RAG在台湾历史档案应用中的可行性和优化空间。

Method: 选用两个繁体中文历史数据集和相应查询集，设计实验评估不同查询特征及元数据整合方式对检索精度、生成答案和系统整体效能的影响。

Result: 实验发现，早期整合元数据能显著提升检索和答案生成的准确率，但RAG依然存在生成幻觉和多跳、时序性历史问题难以处理等问题。

Conclusion: 将RAG应用于台湾历史档案具备可行性并能通过元数据整合有效提升效能，但系统本身对复杂历史查询处理及降低幻觉生成仍需进一步研究。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [119] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 本文综述了利用大语言模型（LLM）进行科学创意生成的方法，并对这些方法如何权衡新颖性与科学合理性进行了系统梳理和归类。


<details>
  <summary>Details</summary>
Motivation: 科学创意的生成对于科学发现至关重要，而现有LLM在科学创意生成上的能力虽具潜力但存在不一致和理解不足的问题，因此需要系统性分析和总结。

Method: 作者将现有基于LLM的科学创意生成方法分为五大类：外部知识增强、基于提示的分布引导、推理时缩放、多智能体协作、参数级适应。并借助Boden的三类创造力和Rhodes的4Ps理论，对这些方法的创意产出和侧重点进行解释和归因。

Result: 通过方法类别和创造力框架的结合，以及对各类方法的系统对比，本文梳理出LLM在科学创意生成方面的研究现状，各类别方法在创意水平及创新来源上的差异。

Conclusion: 对LLM驱动的科学创意生成进行了系统归纳和梳理，并明确了未来实现可控、高效、可靠科学创意应用的关键方向。

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [120] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文提出了GRIP框架，通过特定微调任务让大语言模型（LLM）高效内部化结构化图数据知识，在多个基准测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM处理结构化数据如知识图谱面临困难，现有方法转换开销大或对齐复杂且效果一般，因此需要一种更高效、易迁移的策略。

Method: 受测试时参数内注入知识的启发，作者提出GRIP框架，通过精心设计的微调任务将图中的关系信息注入LLM，并利用轻量级LoRA参数存储相关知识，使LLM无需访问原始图就能完成多种图相关任务。

Result: GRIP在多个标准数据集上进行实验，证明其在效率和效果上均优于现有方法。

Conclusion: GRIP能有效提升LLM对结构化图数据的理解和应用能力，提供了一种高效、实用的处理图数据新途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [121] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: 该论文提出了一种无需参考摘要的新型日志摘要评价方法REFLEX，利用大型语言模型（LLM）直接评估摘要质量，比传统评价指标更有效。


<details>
  <summary>Details</summary>
Motivation: 日志摘要系统评价难度大，主要原因是缺乏高质量的参考摘要和现有指标如ROUGE、BLEU只关注表层词汇重叠，无法全面反映摘要质量。

Method: 提出REFLEX指标，采用大型语言模型作为零样本评估器，从相关性、信息量、连贯性等多个维度评价日志摘要，无需参考标准摘要或人工标注。

Result: REFLEX在多个日志摘要数据集上验证，结果显示该方法评价稳定、可解释且细粒度区分能力强，相较传统指标更能有效区分不同模型输出。

Conclusion: REFLEX为缺少参考数据的实际环境下日志摘要评估提供了一种可扩展的新方法，能够替代传统基于参考摘要的指标。

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [122] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: 本文提出了DuTerm，一种结合术语感知NMT和大模型后编辑的机器翻译新架构，在多个语对实验中取得了较优的术语处理和译文质量。


<details>
  <summary>Details</summary>
Motivation: 在术语受限的机器翻译中，传统方法很难在术语准确性和译文自然性之间取得平衡。因此，亟需更好地结合大模型与NMT的方法以兼顾术语一致性和整体译文质量。

Method: DuTerm采用两阶段架构，第一阶段是利用大规模合成数据微调的术语感知NMT，第二阶段用提示化大语言模型(LLM)对NMT输出进行后编辑和术语修正。LLM以上下文驱动方式灵活处理术语而非死板替换。

Result: 通过在WMT 2025 Terminology Shared Task 英-德、英-西、英-俄任务上评测，DuTerm中LLM的灵活术语处理显著提高了译文质量，相较于仅做死板术语约束效果更优。

Conclusion: LLM在术语受限翻译中更适合作为基于上下文的变换器进行术语修正，而非直接生成译文。灵活处理术语能更好地实现译文质量与术语准确性的平衡。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [123] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B 是一种高效的大语言模型，通过架构创新和系统优化，在受限算力下实现了较大的性能突破。它融合了分组差分注意力机制和高效训练系统，在多个领域和基准上展现出媲美更大模型的能力。


<details>
  <summary>Details</summary>
Motivation: 受限算力预算下，亟需能高效理解语言和执行指令的基础大模型。现有大模型通常消耗极多计算资源，缺乏面向实际部署场景的效率和鲁棒性。本模型旨在推动高效基础模型的发展，兼顾代表性、泛化能力和实际可用性。

Method: Motif-2-12.7B 从 Motif-2.6B 升级而来，集成了分组差分注意力（GDA），通过信号与噪声控制路径分离提升表达效率。预训练在5.5万亿Token上，涵盖语言、数学、科学和编程等多种领域，采用分阶段数据调度。训练系统使用MuonClip优化器和定制高性能内核（如融合型PolyNorm激活和并行Muon算法），提升了分布式环境下的吞吐率和内存效率。后训练采用三阶段监督微调流程，依次强化指令服从性、组合推理和语言精度。

Result: Motif-2-12.7B 在多个基准测试上展现出强劲表现，性能可与更大体量的模型竞争，尤其在受限算力条件下效率显著提升。

Conclusion: 通过有针对性的架构扩展与训练系统优化，可以在不显著增加模型规模的情况下实现高效且强健的基础大模型能力，为受限资源场景提供有力解决方案。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [124] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过分析多头自注意力（MHA）在大语言模型中对多语言能力的贡献，提升了多语言语言模型的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多头自注意力结构在许多领域效果显著，但其对多语言处理能力的具体作用尚未深入研究。因此，该工作旨在探究MHA对提升多语言大模型能力的贡献。

Method: 作者提出了一种称为LAHIS（语言注意力头重要性评分）的方法，仅需一次前向和反向传播，就能识别出对多语言能力重要的自注意力头。此外，还介绍了一种轻量级适配方法，通过学习一个软头掩码，仅需20个可调参数即可调控语言相关注意力头。

Result: 在Aya-23-8B、Llama-3.2-3B和Mistral-7B-v0.1等模型上，实验发现存在语言特定头和语言通用头。语言特定头有助于跨语言注意力迁移，指导模型关注目标语言上下文，减少生成非目标语言内容的问题。轻量级适配也有效提升了XQuAD多语言问答基准上的准确率。

Conclusion: 本工作不仅提高了多语言大语言模型的解释性，还从MHA角度增强了其多语言能力，为设计更优的多语言模型提供了新的见解和工具。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [125] [LLM Optimization Unlocks Real-Time Pairwise Reranking](https://arxiv.org/abs/2511.07555)
*Jingyu Wu,Aditya Shrivastava,Jing Zhu,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文提出了一系列优化方法，大幅降低了基于大型语言模型（LLM）对信息检索系统中文档重排序的延迟，使其更适合于实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有通过LLM进行Pairwise Reranking虽然有效，但算法复杂、计算需求高、延迟大，难以满足实时系统的需求，因此需要寻找降低延迟、提升效率的方法。

Method: 本文通过限制重排序文档数量、采用更小的模型、低精度计算、减少位置偏置（单向顺序推断）以及输出token数量限制等多项优化手段，实现对现有Pairwise Reranking Prompting（PRP）流程的加速。

Result: 优化后单次查询的平均延迟由61.36秒大幅降低至0.37秒，最高提升166倍，且Recall@k指标基本保持不变。

Conclusion: 通过合理设计和优化，LLM-based的文档重排序能够显著提升速度且性能损失可忽略，提升了其在实际延迟敏感场景中的可行性。

Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.

</details>


### [126] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 论文比较了三种荷兰语大模型与传统情感词典工具（如LIWC和Pattern）在预测低资源方言（弗拉芒语）情感正负性任务上的表现，结果发现传统方法优于大模型。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理中情感分析需求增长，虽然传统词典工具常被使用，但大模型因其上下文理解能力被寄予厚望。该研究旨在验证在荷兰语低资源变体环境下，大模型是否真的优于传统方法。

Method: 作者构建了一个包含约25000条弗拉芒语文本及其自评情感分数的数据集，使用三种荷兰语特定大模型与LIWC、Pattern工具对文本进行情感正负性预测，并比较各方法的表现。

Result: 荷兰语大模型（ChocoLlama-8B-Instruct、Reynaerde-7B-chat、GEITje-7B-ultra）在本任务中均未超过Pattern等传统词典工具，后者表现最佳。

Conclusion: 当前荷兰语大模型尚不能很好地处理日常语言中的情感细微差别，传统方法在低资源语言变体下仍具优势。未来需发展更具文化和语言针对性的评测与微调方式。

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [127] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: 作者比较了几种大型语言模型（LLM）答案评估方法，发现简单的自然语言推断（NLI）结合词汇匹配方式与GPT-4o的评估结果相当，但计算资源需求极低；并推出DIVER-QA基准数据集，便于后续研究。


<details>
  <summary>Details</summary>
Motivation: 目前LLM答案评估存在两难：传统词汇指标难捕捉语义，LLM自评分法则资源消耗极大。因此需要寻找既高效又准确的新评测方法。

Method: 使用开箱即用的NLI方法，并加入简单的词汇匹配标志，来评价长文本问答任务。同时，设计了DIVER-QA数据集，含3000个人工标注样本，覆盖五种问答数据集和五类LLM，用以对比各评价方法的人类一致性。

Result: NLI加词汇匹配评价法在长文本问答任务上达到了89.9%的准确率，与GPT-4o的评估结果持平，但参数使用量大幅降低。DIVER-QA数据集验证了这种方法和人类标注之间的高度一致性。

Conclusion: NLI为基础的高效评估方法在LLM答案自动评分领域依然极具竞争力，DIVER-QA可作为未来自动评测方法研究的开放资源。

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [128] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 论文系统性评估了六种常用的无参考事实性评估指标在长文档摘要场景下的可靠性，发现它们在面对语义等价但表述不同的摘要时得分不一致，且随着信息密度增加，表现变差。


<details>
  <summary>Details</summary>
Motivation: 摘要生成系统需要能评估生成内容的真实性，尤其是面对长文档时。原有的事实性指标主要针对短文本设计，长文档场景下其可靠性和适用性尚未被充分研究和验证。本文动机在于探究这些指标在长文档摘要中的表现瓶颈与改进空间。

Method: 作者对六种已有的，无需参考摘要的事实性指标进行系统评估，设计七种事实性保持的摘要扰动方式（如释义、简化、同义词替换等），以此检验指标的稳健性，并考查其对信息密度和检索上下文的敏感度。评测数据集覆盖科幻、法律和科学等长文档领域。

Result: 实验证明，现有短文本领域的事实性指标面临两大问题：对等义摘要打分不一致，以及对信息密度高的断言部分评分不可靠。扩大检索上下文在部分领域有助于提升指标稳定性，但没有指标能在长上下文条件下持续维持高水平事实一致性。

Conclusion: 现有无参考事实性指标尚难胜任长文档摘要的评测，未来需发展跨片段推理、多上下文感知、基于语义等价变化训练等方法来提升评测鲁棒性。所有代码和数据已开源，便于后续研究复现和扩展。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [129] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出了一种新的偏好优化方法CAPO，提升大模型多语言偏好对齐效果，显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有如DPO的偏好优化方法虽然在英语表现良好，但在多语言环境下泛化能力较差，难以处理噪音和低区分度的偏好数据。

Method: 作者提出CAPO方法，用相对奖励动态调整损失函数的尺度，根据信心程度调整对每组偏好对的学习信号，增强对多语言文本中噪音和低区分度比对的鲁棒性。

Result: 实验表明，CAPO在奖励准确率上至少比现有基线提升16%，且在多语言环境中扩大了优选与非优选回复之间的奖励差距，更好地实现了人类偏好的对齐。

Conclusion: CAPO作为一种简单有效的偏好对齐方法，大幅提高了LLMs在多语言偏好优化中的性能，是跨语言场景下优于传统DPO的新选择。

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


### [130] [Critical Confabulation: Can LLMs Hallucinate for Social Good?](https://arxiv.org/abs/2511.07722)
*Peiqi Sui,Eamon Duede,Hoyt Long,Richard Jean So*

Main category: cs.CL

TL;DR: 本文提出“批判性虚构”方法，让大模型的有限度幻觉填补历史记录因社会和政治不平等而造成的空白，通过特定任务和评测，验证大模型生成有用虚构内容的能力。


<details>
  <summary>Details</summary>
Motivation: 现实中历史档案常因社会与政治不公，存在遗漏，尤其是“被隐身的人物”；而大模型(LLM)虽会幻觉，但若合适限定，可用于补全这些空白。

Method: 借鉴“批判性编织”（critical fabulation）理论，提出用大模型生成有限度的虚构内容，具体通过开放式narrative cloze任务：让LLM补全未公开文本中的人物事件链缺失部分。用经过审查、全开放的OLMo-2模型，以及其它基线模型，在多种提示下诱导可控虚构进行对比评测。

Result: 实验表明，LLM具备基础叙事理解能力，能在设定范围内产出有意义的批判性虚构；受控、明确定义的幻觉有助于生产知识，且可减少对历史准确性的损伤。

Conclusion: 大模型经适度设计与约束，可将幻觉转化为创新的知识补全工具，填补社会历史档案中因不平等而产生的信息空白，支持被忽视主体的多元历史叙事。

Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.

</details>


### [131] [Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production](https://arxiv.org/abs/2511.07752)
*Shiva Upadhye,Richard Futrell*

Main category: cs.CL

TL;DR: 本文提出并验证了一种新的信息论可预测性测量方法，用以同时整合前后文对词语选择和表达的影响，深入剖析了语言产生中前向和后向可预测性带来的效应。


<details>
  <summary>Details</summary>
Motivation: 传统研究已较好理解前向可预测性（前文对当前词语的预测）对词语产出和理解的影响。然而，真实语料分析又发现了后向可预测性（后文对当前词语的预测）带来的效应，但其机制尚不明确。作者希望澄清后向可预测性作用机制，并提出更系统的计算方法。

Method: 作者利用两项自然语料库研究：一项重新考察了词语持续时间与可预测性的关系；一项在生成框架内建模并预测词语替换错误，分别考察词汇、语境和交流等因素影响。提出了一种融合前后文的、基于信息论的新可预测性指标，并使用更强大的语言模型进行分析。

Result: 新提出的信息论可预测性度量在两项研究中均表现出与传统后向可预测性度量一致的效应。词语替换错误的细致分析显示，不同类型的错误揭示了说话者在词汇计划中对形式、意义和语境信息的不同权衡方式。

Conclusion: 本研究揭示了前后文在词语编码与选择中的功能性作用，为理解上下文可预测性效应与句子规划机制之间的关系提供了新的理论桥梁。

Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.

</details>


### [132] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文介绍了保险领域专业大模型评测基准CUFEInse v1.0的构建与评价体系，并基于该基准对11个主流大模型进行了详细评估，揭示大模型在保险垂直领域的表现优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏针对保险领域的专门评测标准和系统工具，导致模型专业能力难以有效评价和优化。该工作致力于填补保险领域大模型评价的空白，支持学术研究和行业的模型选择与改进。

Method: 作者按“定量导向、专家驱动、多重验证”原则，设计了涵盖5个核心维度、54个子指标、14430道高质量问题的评测体系，涉及保险理论、行业理解、安全合规、智能应用、逻辑严谨性，并对11个主流大模型进行了全面对比评测。

Result: 通用大模型在保险精算和合规适应方面存在显著短板，专域模型在保险场景下表现突出但在业务适应及合规方面仍有不足。评测有效揭示了当前大模型在保险领域（如精算、核保理赔推理、合规文本生成等）面临的共同瓶颈。

Conclusion: CUFEInse基准不仅填补了保险领域的专业评测工具缺口，也为垂直领域大模型评测范式提供了参考。未来需聚焦于“领域适应+推理强化”，推动保险大模型持续发展和优化。

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [133] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 提出了一种新型可训练的多层次图记忆框架，通过结构化和优化经验信息提升大模型自主体的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大模型自主体在复杂环境中任务解决能力强，但现有经验利用方式存在遗忘、解释性差或适应性不足的问题，因此需要改进记忆机制以增强模型推理表现。

Method: 构建面向agent的多层图记忆，把原始轨迹抽象为结构化决策路径，并进一步提取可解释的战略元认知；通过强化学习优化每个元认知的权重，根据下游任务中的奖励动态调整并融入训练循环。

Result: 这套可学习的图记忆在实验中提升了agent的泛化能力、战略推理能力，并在强化训练过程中持续带来效益。

Conclusion: 面向agent的多层图记忆显著改进了大模型agent的经验利用和推理能力，为AI自主体自适应、可解释决策提供了新思路。

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [134] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: 论文提出AlignSurvey，这是一个首次系统性利用大型语言模型（LLMs）来模拟和评估完整社会调查流程的基准，包含多阶段任务、评价指标、数据集及相应模型，提升了社会调查的有效性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统社会调查存在高昂成本、固定题目、适应性差、跨文化等问题；现有利用LLMs工作的局限在于仅处理结构化问题、未覆盖完整调查流程且容易忽视边缘群体。

Method: 提出AlignSurvey基准，设计四项与社会调查逻辑紧密关联的任务，包括社会角色建模、半结构化访谈建模、态度倾向建模和问卷答复建模，并针对每项任务制定了评估指标。建立了跨国的对话与问卷数据集、专家注释集以及跨文化调查集，并用两阶段微调方式开发领域适配的SurveyLM参考模型，公开所有数据与工具。

Result: AlignSurvey为社会调查领域提供了全面的任务、评估和高多样性数据；SurveyLM系列模型实现了针对社会调查场景的有效适配，提升了在一致性和公平性上的表现。

Conclusion: AlignSurvey为利用LLMs促进社会调查流程的自动化、灵活化和公平性带来了新工具和标准，为更透明和负责任的社会调查研究奠定了基础。

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [135] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 本文提出了一种通过分析新闻公告预测印度等民主国家社会动荡（如抗议、集会）的系统，结合主题建模、word2vec和命名实体识别等技术，实现了相关事件的提前预警。


<details>
  <summary>Details</summary>
Motivation: 由于社会动荡事件（如抗议、游行等）可能对社会秩序造成影响，且通常会提前通过新闻发布，研究人员希望通过预测这些事件来帮助行政部门提前采取措施。

Method: 本文方法包括：1) 利用主题建模和word2vec筛选相关新闻文章，2) 用命名实体识别（NER）技术识别人名、地名、组织、日期等实体，3) 进行时间标准化以统一未来日期格式，4) 提出Related Entity Extraction方法以提取真正与事件相关的实体。

Result: 开发了一个能跨地理区域泛化的通用模型，有效筛选和识别社会动荡事件，并能提取与事件真正相关的实体信息。

Conclusion: 该系统能够提前从新闻公告中预测有可能引发社会动荡的活动，并精确定位相关实体，对行政管理和应急响应具有重要意义。

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [136] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出了Manifold-Correcting Causal Flow (MC^2F)方法，通过建模干净样本在嵌入流形上的分布，实现文本分类模型在抗对抗攻击和保持干净数据性能之间的平衡，取得了新的最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有提升文本分类模型鲁棒性的方法通常会导致原始（干净）数据上的性能下降。作者认为，这一难题可以通过精确建模干净数据在嵌入空间上的分布来解决，因此提出新方法进行探究。

Method: 提出MC^2F系统，由两部分组成：一是使用分层黎曼连续归一化流（SR-CNF）学习干净数据在编码器嵌入空间的密度分布，并识别分布外样本；二是利用测地净化器将对抗样本投影回流形上的最近点，从而还原出干净且语义一致的嵌入表示。

Result: 在三个数据集和多种对抗攻击下进行大量实验，结果显示MC^2F方法不仅刷新了文本分类任务下的对抗鲁棒性SOTA表现，同时完全保留甚至略微提升了对干净样本的分类精度。

Conclusion: 该方法有效解决了文本分类中抗对抗攻击与保持干净数据性能难以兼得的问题，通过流形建模和测地净化，在提升鲁棒性的同时不损失模型原有准确率，具备实际落地应用潜力。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [137] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本论文提出Logits-to-Logic框架，通过修正LLM生成过程中的logits输出，显著提升了大语言模型在结构化知识推理任务中的逻辑一致性，并在知识图谱问答多个基准上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理中表现优异，但在结构化知识推理（如知识图谱问答）场景下由于非结构化与结构化知识的表达差异，容易产生逻辑漂移（Logic Drift）现象，导致推理逻辑不一致，现有方法无法从根本上解决这一问题。

Method: 提出Logits-to-Logic框架，核心包括logits加强与logits过滤模块，直接针对自回归生成过程中的logits输出，校正LLM的逻辑缺陷。

Result: 通过大量实验验证，该方法在提升结构化知识推理中的逻辑一致性方面表现优异，并在多个知识图谱问答基准数据集上达到新SOTA表现。

Conclusion: Logits-to-Logic框架有效提升了LLM在结构化知识推理任务中的逻辑一致性，为解决LLM逻辑漂移问题提供了创新性的解决方案。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [138] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本章综述了利用社交媒体数据研究心理健康（如抑郁、焦虑、自杀念头）的方法及成果，总结了各种机器学习和自然语言处理技术的应用及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟社区和社交媒体的普及，越来越多用户在网上分享关于心理健康的问题，尤其是不愿公开身份的高污名化群体，需要新的研究方法来挖掘这些数据、理解与支持心理健康。

Method: 梳理了社交媒体数据中应用的机器学习、特征工程、自然语言处理等方法，分析了用户在网上披露心理健康状况时的语言、情感和视觉特征，并对已有研究进行了系统性综述和分类。

Result: 现有研究已能通过社交媒体数据识别心理健康风险，并分析行为特征、情感表达，有助于及时发现求助人群。探讨了数据源类型、分析方法及其成效。

Conclusion: 社交媒体数据为心理健康研究与干预提供了新机遇，可提升医疗服务及时性，促进政策制定和公众意识；未来需进一步完善分析工具，提高科研与实务结合度。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [139] [Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined](https://arxiv.org/abs/2511.07918)
*Jung-Sun Lee,Ha-Na Jo,Eunyeong Ko*

Main category: cs.CL

TL;DR: 本研究比较了人类四种不同语音模式（感知、发声、耳语、想象）下θ波段神经同步性的差异，揭示各模式下脑区连接和活动的不同特征。


<details>
  <summary>Details</summary>
Motivation: 以往研究大多聚焦于单一语音模式（如发声），缺乏对多种语音模式下θ波段同步性的系统比较。因此，作者希望通过比较多种语音产生模式，揭示更全面的语言神经机制。

Method: 研究采用了基于脑区连接指标的神经信号分析方法，对感知、发声、耳语和想象四种语音模式下的θ波段同步性及其脑区分布进行比对分析。

Result: 研究发现，发声与耳语语音模式表现出更广泛且更强的额颞区同步，表明有更活跃的运动—语音协同，感知语音则以后部和颞区同步为主，对应感知理解过程；想象语音则更多限于额区及辅助运动区，表现较为局部但内部一致的同步。

Conclusion: 不同语音模式的θ波段同步性在范围和分布上存在显著差异，发声涉及更广泛的皮层相互作用，耳语处于中间状态，语音感知则以颞顶网络为主。研究揭示各种语音过程下的共性与差异，为理解语言感知及想象背后的神经动态提供了新见解。

Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.

</details>


### [140] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: 该论文提出了WorkBench评测套件和统一的工作领域嵌入模型（UWE），以应对工作场景下具有高度复杂性和标签稀缺性的自然语言处理任务。实验表明，UWE在零样本和多任务环境下表现优异，超越了通用嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现实中工作相关的自然语言处理任务存在分布长尾、多标签、数据稀缺等难题。当前通用嵌入模型在工作场景的泛化能力尚不明确，且相关进展多集中于单一任务。

Method: 作者构建了WorkBench评测套件，涵盖六个明确定义为排序问题的工作相关任务；通过构建任务间双分图和数据增强提出了UWE（一个任务无关的bi-encoder），采用多对多InfoNCE损失，并结合无关任务交互的软后融合机制，利用token级嵌入处理多任务。

Result: UWE展示了强大的跨任务迁移和零样本排序能力，能缓存目标空间嵌入以提高推理速度，在多个宏平均指标（如MAP和RP@10）上明显优于通用嵌入模型。

Conclusion: UWE实现了不同工作相关任务间的有效迁移，并在真实场景下表现优越，验证了基于任务结构和多样本训练目标的嵌入方法对复杂多任务工作的适应性和高效性。

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [141] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出了NOTAM-Evolve框架，通过自进化和知识图谱增强，实现了更准确的航空消息(NOTAM)深度解析，显著提升了解析准确率。


<details>
  <summary>Details</summary>
Motivation: 当前航空安全高度依赖NOATM信息的正确解读，但NOATM语言晦涩，现有自动化系统理解能力有限，难以获取实际可操作情报。

Method: 作者将NOATM解释任务形式化为深层解析问题，既需与动态航空数据关联，也需基于领域规则推理。为此，设计了NOTAM-Evolve：在知识图谱增强检索模块辅助下，让大模型通过自进化闭环学习逐步提升推理能力，减少对人工标注推理数据的依赖。同时，构建了包含1万条专家标注NOATM的新数据集。

Result: 实验证明，该方法在结构化NOATM解读任务上，准确率相比初始大模型提升30.4个百分点，创下新SOTA表现。

Conclusion: NOTAM-Evolve大幅优化了NOATM自动化理解，为提升航空运营安全和自动化决策能力提供了有力工具。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [142] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文比较了现有的BERT风格模型与大语言模型（LLM）在南斯拉夫语系文本分类任务上的表现，发现LLM在零样本设置下表现优异，但在效率和可控性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着指令微调的大语言模型（LLM）在NLP领域的崛起，越来越多的文本分类任务采用零样本或少样本提示方式。但LLM在低资源语言上的分类能力尚不明确，因此需对其在南斯拉夫语系等低资源语言上的表现进行系统评估。

Method: 对多个南斯拉夫语系语言，在情感分类、主题分类和文本体裁识别三项任务上，系统比较了开源和闭源LLM与公开可用的BERT微调模型在零样本环境下的性能表现。

Result: LLM在零样本文本分类任务中，通常达到甚至超越了微调BERT模型的水平，并且在南斯拉夫语系语言上的零样本表现与英文相当。但存在输出不可控、推理速度慢、计算成本高等缺点。

Conclusion: 尽管LLM在零样本文本分类任务中表现优异，但受限于推理效率和成本，微调BERT模型对于大规模自动文本标注目前仍然是更为实用的选择。

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [143] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 该论文提出了一种自我纠错蒸馏（SCD）方法，通过检测和纠正小规模大模型在结构化数据问答中的错误，提升其问答能力，并在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型已推动结构化问答取得重大进展，但小规模模型在生成结构化查询时容易出错，急需提升其性能以便更高效部署。

Method: 作者提出了自我纠错蒸馏（SCD）方法，包含错误提示机制（EPM）和两阶段蒸馏策略：EPM用于检测推理时的错误并产生定制化出错提示，两阶段蒸馏用于将大模型生成及纠错能力迁移至小模型。

Result: 在5个基准、3类结构化数据上，小规模模型（8B）经SCD后效果优于其他蒸馏方法，部分数据集上接近GPT4表现，大模型结合EPM在大多数数据集上超越SOTA。

Conclusion: SCD方法极大增强了小规模大模型在结构化数据问答中的表现，兼顾性能与效率，对实际应用推广有重要意义。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [144] [HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing](https://arxiv.org/abs/2511.08017)
*Shihao Yang,Zhicong Lu,Yong Yang,Bo Lv,Yang Shen,Nayu Liu*

Main category: cs.CL

TL;DR: 本论文提出了HyCoRA框架，用于加强模型在多角色扮演任务中的表现，通过平衡角色独特性与共性特征的学习，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有多角色扮演模型要么采用统一参数模块，导致难以体现每个角色的独特性，要么为每个角色分配单独模块，导致难以建模角色间共性，因此需要一个能够有效联合学习人物共性与个性的框架。

Method: 提出HyCoRA（Hyper-Contrastive Role-Adaptive）学习框架，结合Hyper-network生成的轻量级角色特异模块与可训练的通用角色共享模块，并引入超对比学习机制，帮助模型区分不同角色的独特性。

Result: 在中英文基准数据集上（包括人工和GPT-4自动评测）均取得了优异的性能表现，模型能更好地捕捉和模拟不同角色的特征。

Conclusion: HyCoRA框架能有效平衡角色个性与共性的学习，提升角色扮演能力，对多角色模拟等任务具有广泛应用前景。

Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.

</details>


### [145] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 本文提出了BARD10新数据集，并系统评估了去除停用词对孟加拉文作者归属任务的影响，发现停用词对风格识别作用显著。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作者归属研究缺乏高质量、平衡的评测数据集，对停用词在风格识别中的作用也未被充分探讨。

Method: 构建了10位作者平衡的BARD10博客及评论文本集，并在BARD10和BAAD16数据集上，采用统一预处理，比较SVM、Bangla BERT、XGBoost、MLP四种模型，重点分析停用词删减影响。

Result: 传统TF-IDF+SVM在两数据集上表现最佳（BAAD16: macro-F1 0.997；BARD10: 0.921），Bangla BERT差5分；BARD10对停用词删减敏感，BAAD16较稳健；深度模型易丢失高频风格特征。

Conclusion: 1）孟加拉语停用词是重要风格特征；2）经过调优的经典机器学习模型在短文本表现好；3）BARD10为后续研究（如长文本transformer模型）提供开源基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [146] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 本文通过掩码语言模型，定量分析科幻小说中“人类—动物—机器”范畴的边界流动性，发现科幻小说在机器相关词汇上显著增加了范畴混淆，揭示了该类型在本体论结构上的独特性。


<details>
  <summary>Details</summary>
Motivation: 科幻小说常通过模糊人、动物和机器的界限，引发所谓“陌生化”效果，但此前缺乏可量化、系统性分析这一范畴滑移的工具和方法。本文旨在通过自然语言处理技术，量化科幻文本与普通小说在范畴边界稳定性上的差异，从而实证分析“陌生化”理论。

Method: 作者收集了科幻小说和一般小说两个语料库，采用RoBERTa模型进行掩码预测，通过对“人、动物、机器”等词语的掩码补全结果进行分类，并引入滞留率、替换率、熵等指标来衡量语义范畴的稳定与混淆，比较不同流派文本中的本体边界。

Result: 结果显示，科幻小说中的“机器”词汇在掩码预测时，表现出更高的范畴混淆和语义分散性；而“人类”词汇则维持较高的语义一致性，往往成为分类与替代的核心锚点。这反映出科幻小说在本体结构上对机器的界限有意扰动，而普通小说则更维持传统人本主义范畴分界。

Conclusion: 科幻小说的‘陌生化’效果可以理解为对语义与本体范畴规范的受控干扰，这种扰动可以通过掩码语言模型以概率方式检测和量化。本研究不仅拓展了数字人文的研究方法，也为理解科幻类型在语言和思维层面的特殊性提供了新证据。

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [147] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: 本文研究多模态大型语言模型（MLLM）在不同模态间组合已学技能解决新任务的能力，发现当前模型在跨模态技能组合方面存在明显差距，即使通过链式思考提示或精细调优等方法也未完全解决。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络预训练技能复杂度的提升，模型能否将已学技能有效组合以应对新任务变得重要，特别是在多模态情境下，跨模态技能组合能力尚未被充分探索。

Method: 设计三项需顺序组合两种模态依赖技能的评测任务，对多个开源MLLM在直接提示解决任务和两步级联推理（手动强制技能组合）两种主要设定下进行评估；并尝试用链式思考提示和特定的微调方法优化组合能力。

Result: 所有被评估的MLLM在跨模态技能组合上均表现出显著能力缺口，上述提升技巧虽有改善，但依旧未能完全克服该问题。

Conclusion: 当前MLLM在跨模态技能组合能力上存在明显不足，仅用链式思考提示及特定微调无法完全解决，未来需要进一步探索提升多模态技能组合的方法。

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [148] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 论文研究了(M)LLMs在数量词处理上表现不佳的具体原因，通过对比人类和模型在数量词量级、使用范围和人类近似数系统偏见等三个重要特征上的表现，发现二者在这些方面存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态/大型语言模型在数量词处理上表现较差，但具体原因尚不明确，尤其是涉及逻辑、语用和数值领域。因此作者希望深入探索人类处理数量词过程中的核心特征，并分析这些特征模型能否有效学到及与人类的异同。

Method: 论文选取了量词间的等级排序、使用范围和典型性、人类近似数系统的偏见等三个跨语言共有特征作为切入点，对比测试人类和不同(M)LLMs在相关任务中的表现，同时考虑语言和模型类型的影响。

Result: 结果显示，在有关数量词表征的多种任务中，人类和(M)LLMs在这三个核心特征上表现出明显不同，不同模型和语言也影响表现。

Conclusion: 本研究启示我们，在数量词这一语义及语用现象上，(M)LLMs和人类的表征方式有本质差异。利用跨语言对比，有助于衡量模型能力的泛化和稳定性，并为其作为语义/语用智能体的性质探讨铺平道路。

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [149] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 本文提出了一种通过学习压缩token实现大模型上下文压缩的新方法，可将处理长序列时的内存和计算需求降低2至8倍，且无明显性能损失。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时，计算和存储代价极高，限制实际应用。因此需要有效的上下文压缩方法，以支持更高效的推理。

Method: 将预训练的大语言模型进行微调，使模型学会通过特定的压缩token对输入上下文进行压缩，从而减少所需处理的token数量。

Result: 在3B参数的LLaMA模型上进行实验，该方法在短上下文和长上下文基准测试中几乎不会降低性能，并且相比其他压缩技术有更高的压缩比。

Conclusion: 所提出的压缩方法可大幅降低LLM推理的资源消耗，并在保持准确度的前提下优于现有方法，展示了其在实际大模型部署中的潜力。

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [150] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨了当前主流的语言模型架构（如Transformer）在英语以外语言上的适用性，尤其关注“位置编码”这一架构选择对结构与英语差异较大语言的影响。作者对七种不同语言的模型在不同位置编码设定下进行了实验，但没有发现位置编码与语言形态复杂度或词序灵活性之间有明确的相互作用。研究结果说明任务、语言和评测指标的选择对结论有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 目前大多数语言模型架构基于英语而设计，存在对其它结构迥异语言可能不适用的担忧。具体到“位置编码”这一机制，其对语序和形态特征大不相同的语言效果如何未有定论。此外，理论上形态复杂度和词序灵活性之间有权衡关系，这一假设是否通过架构调整表现出来是一个开放问题。

Method: 作者选取了七种类型多样的语言，并在绝对位置编码、相对位置编码以及无位置编码三种设定下预训练单语模型，然后在四项下游任务上进行评测。使用多个数据代理指标来衡量形态复杂度和词序灵活性，检测位置编码与语言结构特征的潜在互动关系。

Result: 与以往研究不同，作者未观察到位置编码方式与形态复杂度或词序灵活性之间有稳定的相互作用关系。不同设定下，模型表现没有表现出随着语言结构差异而明显变化。

Conclusion: 通过对多语言及多任务的细致检验，本文认为位置编码对语言模型跨语言表现的影响并非如理论期望那样明显。研究结果提示学界在做类似结论推断时，必须严谨考虑所选任务、语言种类及评测指标对于结果的显著影响。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [151] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 本文提出了RelPrior范式，有效提升了大语言模型（LLMs）在文档级关系抽取（DocRE）上的表现，通过引入关系为先的处理方式，减少了噪声和标签误差，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在文档级关系抽取任务上存在两大问题：1）大量无关实体对引入噪声，干扰真实关系的预测；2）对预定义关系集之外的标签一律视为预测错误，导致误判。这些问题限制了LLMs在DocRE任务中的精细理解能力。

Method: 提出了RelPrior范式。针对大量无关实体对的问题，先利用二分类关系先验过滤无关实体对；针对标签误判问题，用预定义关系先验来引导实体匹配，实现三元组抽取，而非直接关系预测，从而减少因预定义关系集不全带来的误差。

Result: 在两个主流基准数据集上进行实验，RelPrior方法实现了当前最优性能，超过了现有基于LLM的方法。

Conclusion: RelPrior有效降低了噪声和标签误差，显著提升大语言模型在DocRE任务上的表现，证明了该方法的有效性和优越性。

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [152] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型与任务专用的小型模型在梵文诗歌转散文任务上的表现，发现针对领域特定进行微调的小型模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型(LLMs)常被当作通用的NLP解决方案，但它们在低资源且形态复杂的语言（如梵文）上的表现未被充分验证。

Method: 以梵文诗歌转散文（anvaya）任务为评测，用指令微调和上下文提示LLMs，并与完全微调的ByT5-Sanskrit Seq2Seq模型对比。人类评测用于辅助验证。

Result: 领域专用的ByT5-Sanskrit微调模型在自动评价和人工评价中均显著优于各类LLM方案。同时，设计的上下文提示策略在缺乏领域数据时有应用价值。

Conclusion: 在结构复杂、资源稀缺的语言任务上，任务特定、领域微调的小模型仍能优于泛化型大模型。提示工程虽可作为备选，但不及完全微调专用模型。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [153] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: 本文分析了BabyLM和CHILDES语料库的句法属性，发现基于句法特征筛选的数据比年龄分组或全量数据更能提升模型在语言任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着儿童语言训练语料库被用于AI模型训练，研究者希望了解不同数据分布（如年龄、句法结构）如何影响语言模型的表现，特别是在模拟儿童学习顺序时。

Method: 作者对比分析了CHILDES语料库中不同年龄组的句法差异，并对BabyLM的句法属性做出评估。此外，设计不同的“课程学习”方案，包括按儿童发展和其他认知启发式方法排序数据，再比较在语言任务上的效果。

Result: 研究发现，CHILDES语料库不同年龄组间不存在明显的句法结构差异。课程学习策略部分有利于提升阅读任务表现，但最大提升来自于仅用句法可分类的子集数据，而不是整个含噪声的语料库。

Conclusion: 精心筛选和组织句法清晰的数据对于提升儿童语言模拟模型更为关键，简单按年龄分组意义不大。未来可关注如何进一步确定和利用“高质量”的训练数据。

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [154] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的系统，利用SciBERT模型自动从天文论文中抽取望远镜、仪器等关键信息，实现多任务知识提取并优于开源GPT基线。


<details>
  <summary>Details</summary>
Motivation: 天文学领域文献数量快速增长，手动提取文献信息已难以满足需求，因此亟需自动化方法来抽取论文中的实体及其上下文信息以支持后续研究。

Method: 作者提出基于SciBERT的多任务Transformer系统，对天文领域语料进行微调。系统包含三个任务：望远镜引用分类、辅助语义属性检测、仪器提及识别。训练时从训练数据中随机采样片段；推理时对测试片段采用多数投票。

Result: 所提出的系统在简单、低成本的架构下，表现显著超过开源权重的GPT作为基线模型。

Conclusion: 基于SciBERT微调的多任务知识抽取系统能够有效提升天文学文献中的细粒度信息识别，优于通用大模型，适合实际部署和应用。

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [155] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种基于嵌入的基准框架，用于检测大语言模型（LLMs）在形成性反馈场景下的性别偏见。通过对六种主流LLM的评测，发现其在性别替换时存在语义响应不对称，表明反馈存在持续的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 随着教师在教育实践中日益依赖生成式AI，亟需稳健的评测方法以验证LLM在教学反馈中的公平性，特别是在反馈内容是否存在性别歧视方面。现有基准体系针对教育场景不足，因此有必要开发新的评测框架。

Method: 使用AES 2.0语料中600篇真实学生作文，分别进行两维度性别操控：（1）在文本内通过词汇交换进行性别隐性上下文替换，（2）在任务提示中明确指定作者性别。采用六款主流LLM生成反馈，通过余弦/欧式距离量化反馈语义变化，使用置换检验评估统计显著性，并通过降维方法可视化反馈结构。

Result: 所有模型在隐性性别替换（男女对换）下都表现出更大的语义变化，且该效应在男到女方向更为明显。仅GPT和Llama系列模型对显式性别提示敏感。质性分析显示男性提示下反馈更强调自主，女性提示下更趋于控制。

Conclusion: 主流LLM在教学反馈中存在性别偏见，表现为反馈内容随性别切换而发生语义及风格变化。建议在教学AI公平性审计中引入新的基准评测流程，并为教学场景下的提示设计和模型使用提供了具体指导，以实现更公平的学习反馈。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [156] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 该论文提出了VocalBench-zh，一个用于普通话语音交互系统评测的基准套件，填补了相关评测空白，并用其分析了主流模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的发展，普通话语音交互受到广泛关注。然而，缺乏针对普通话的系统性语音到语音评测基准，阻碍了模型开发者的系统评估和用户间的公平对比。

Method: 作者构建了VocalBench-zh评测套件，细分为10个子集，涵盖12类用户角色，总计1万多个高质量实例。并通过该评测套件，对14种主流多模态语音模型进行了系统化实验评测。

Result: 实验揭示了主流模型在普通话语音交互中的普遍难题，同时反映出目前语音交互系统尚有待提升的能力短板。

Conclusion: VocalBench-zh为普通话语音交互模型的评测与比较提供了重要工具，有助于推动更优语音交互系统的研发。

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [157] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 本文提出了一种结合提示调整（Prompt Tuning）和错误纠正的新方法，用于自然语言转SQL（NL-to-SQL），基于最新的大型语言模型和RAG技术，显著提升了查询转换准确率。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言界面日益普及，将自然语言高效且准确地翻译成SQL语句成为亟需解决的关键问题。现有方法在准确性和适用性方面仍存在局限。

Method: 作者提出模仿医疗诊断的流程，设计了一个集成错误类型诊断、原因定位、修正说明和自动更正的全新框架，并结合提示微调和RAG，通过外部知识增强结果准确性和透明度。

Result: 实验证明，该框架在自然语言转SQL任务上比现有方法准确率提升了12%。

Conclusion: 该方法可有效提升数据访问的准确性和效率，对当前数据驱动环境具有重要的现实意义和应用潜力。

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [158] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: 本文提出了ParliaBench数据集和评测框架，专为英国议会演讲生成任务设立，并验证大模型微调后在政治真实性和意识形态一致性等方面的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统大模型主要针对一般文本生成任务，难以兼顾议会演讲需要的语言质量与政治、意识形态的一致性。现有评测方式也未充分衡量政治真实性，导致生成演讲缺乏专业化与可信度。

Method: 作者构建英国议会演讲数据集，并提出融合自动指标和LLM评审的新评测框架，覆盖语言、语义和政治真实性三大维度；创新性地提出了“政治光谱对齐”和“党派对齐”两项嵌入式指标以量化政治定位，并对五个大模型进行微调，生成2.8万条演讲并系统评测。

Result: 微调后的大模型在绝大部分指标上均有显著提升，而新引入的政治光谱对齐与党派对齐指标能有效区分政治维度表现，验证了评测体系的有效性。

Conclusion: 针对议会演讲生成需求，专门的数据集及评测体系能显著提升现有大模型的表现，并为相关研究和权威评测提供了重要工具和参考。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [159] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 本文探讨了视觉大语言模型（VLLMs）对科学文章表格结构理解与解释的能力，尤其着重于其无需额外处理即可推断表格层级结构的表现。采用PubTables-1M数据集，提出用于基准测试的CHiTab子集，并通过提示工程和模型微调进行实验。结果显示部分VLLMs具备此项能力，为未来将结构化数据理解融入通用VLLMs提供参考。


<details>
  <summary>Details</summary>
Motivation: 表格作为学术论文的重要数据表达方式，其层次化结构理解对信息提取和数据挖掘至关重要。尽管视觉大语言模型在多模态任务上取得突破，尚不清楚它们是否具备无需手工特定处理即可自动识别复杂表格结构的能力，因此作者旨在探索并验证这一潜力。

Method: 1）基于PubTables-1M数据集筛选出包含层级标题的复杂表格（CHiTab基准）；2）设计多种提问方式和提示工程以激发VLLMs对表格结构的理解能力；3）选用多种主流开源VLLMs进行零样本和微调实验，并与人工表现作对比分析。

Result: 实验发现，部分未经专门设计的VLLMs在无需专用处理下能较好完成表格结构推理任务，部分模型经微调后性能进一步提升。同时，人类在此任务上的表现为模型评估提供了参考基准。

Conclusion: 通用视觉大语言模型已具备理解科学复杂表格结构的潜质，但仍存在一定局限。本文为将结构化数据理解能力引入通用VLLMs的研究提供了实验支撑和未来方向建议。

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [160] [Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates](https://arxiv.org/abs/2511.08317)
*Shuaimin Li,Liyang Fan,Yufang Lin,Zeyang Li,Xian Wei,Shiwen Ni,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: ReViewGraph是一种新颖的利用LLM模拟多轮评审人和作者争论，并用图神经网络进行推理的论文评论分析框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有论文评审方法依赖文稿表面特征或直接使用LLM，容易产生幻觉、打分偏见和推理能力有限，且难以捕捉评审人-作者互动中的复杂论证过程。

Method: 提出ReViewGraph框架。首先用LLM进行多代理合作，模拟评审人与作者间的多轮辩论，明确抽取接受、拒绝、澄清、妥协等观点关系，编码成异构交互图。再用图神经网络对这些结构化辩论图进行推理，挖掘细粒度的论证动态。

Result: 在三个数据集上的大量实验表明，ReViewGraph平均相对提升15.73%，优于现有强基线。

Conclusion: 建模详细的评审人-作者辩论结构，对于提升论文评审质量具有重要意义。

Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.

</details>


### [161] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体（multi-agent）框架，用于优化大型语言模型（LLM）在对话生成中的回应质量，重点增强事实性、个性化和连贯性。通过让多个具备不同角色的智能体分别审阅和优化回应，并引入动态协作机制，有效提升了对话系统的整体表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在实际对话中，难以同时满足个性化和知识性需求，单一模型内的校正方法难以兼顾多元对话质量维度。因此有必要探索更加模块化、协作式的优化框架，以提升对话系统的实际可用性。

Method: 作者提出用多智能体框架，对每条生成回复由扮演不同角色的智能体分别负责优化事实性、个性化和连贯性，用动态协作策略自适应地协调优化顺序和信息交换，最后将各自修订结合生成最终回答。

Result: 在涉及知识和用户个性的高难度对话任务上，该多智能体方法相较于相关基线模型明显优越，证明了这种分工协作、动态调整的优化方式能更好提升LLM回答效果。

Conclusion: 协作式多智能体优化框架能有效提升LLM对话系统在事实性、个性化和连贯性等关键质量指标上的表现，特别适用于需结合知识与用户画像的复杂场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [162] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种用于决策型大模型任务（如网页购物、浏览器导航）的过程奖励模型AgentPRM，通过追踪每步决策对最终目标的贡献，实现更加高效且鲁棒的多步智能决策。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在多轮决策任务中表现不佳，通常依赖复杂的提示工程或基于专家轨迹的微调，但这些方法效率有限，且难以评估单步动作对整体目标的实际贡献。

Method: 提出AgentPRM，一种能捕捉序列决策内在依赖关系及其对最终目标贡献的过程奖励模型。为高效获取训练数据，采用基于时序差分（TD）和广义优势估计（GAE）的方法标注数据，提升样本利用率。

Result: 在多项agent任务实验中，AgentPRM算力效率比主流方法高8倍以上，且在增加推理计算量时有更强的性能提升。此外，作者还详细分析了方法有效性，并展示AgentPRM在大模型强化学习中的应用潜力。

Conclusion: AgentPRM能更好地追踪目标进展和平衡探索-利用，极大提高了智能体大模型在复杂决策任务中的表现，并为相关技术研究提供了新思路。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [163] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了DPRM模型，结合了CoT与KG双重隐式过程奖励模型，用于优化多跳问答任务的推理过程，无需额外标注，显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有多跳问答任务常用CoT引导大模型多步推理，KG减少幻觉，但过程奖励模型(尤其是隐式过程奖励模型)未能处理KG结构约束和CoT、KG路径间一致性问题，限制了模型性能。

Method: 提出DPRM (Dual Implicit Process Reward Model)，分别为CoT与KG构建两个隐式过程奖励模型（CoT-PRM与KG-PRM），通过结果信号参数化奖励，无需显式过程标注。KG-PRM还用偏好对学习KG结构约束，并设计CoT与KG过程奖励间的一致性约束，实现互证和协同优化推理路径。方法附有理论推导。

Result: 实验在多个数据集与任务上，DPRM方法对比13个基线模型均有提升，Hit@1指标最高提升16.6%。

Conclusion: DPRM能高效处理CoT与KG双路径推理中的结构与一致性约束，增强模型在多跳问答中的推理能力和表现，无需额外人工标注。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [164] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文介绍了DYNARTmo动态发音模型，该模型基于言语手势及其得分，能生成连续的发音动作轨迹，并为言语产生的分层控制提供神经生物学启发的计算框架。


<details>
  <summary>Details</summary>
Motivation: 目前言语产生的神经生物机制尚未完全揭示，缺乏能够模拟从语言层到运动执行层的连续协调控制模型。作者旨在建立一个既符合神经生物机制又能实现语言到发音全流程模拟的模型。

Method: 提出了以言语手势及其得分为基础的模型架构，并介绍了手势库、手势在时间上的协调机制，以及如何将这些手势映射为控制DYNARTmo声道的连续发音动作轨迹。

Result: 实现了DYNARTmo模型，能够在计算机上通过言语手势和手势得分生成自然连贯的发音运动，模拟了言语产生过程的层级控制。

Conclusion: DYNARTmo为言语产生机制提供了一个神经生物学启发的有效建模工具，有助于深入理解语言到发音的转换过程，对言语科学及相关的人工智能研究具有重要意义。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [165] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: TurkEmbed 是一种新型土耳其语词向量模型，在自然语言推理（NLI）和语义文本相似度（STS）任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的土耳其语嵌入模型常依赖机器翻译数据集，导致语义理解和准确性受限，需要更强大的嵌入模型支持土耳其语自然语言处理应用。

Method: TurkEmbed 利用多样化数据集和包括套娃式（matryoshka）表征学习在内的先进训练技术，以获得更健壮和准确的词嵌入。同时该方法支持在资源受限环境下高效编码。

Result: 在土耳其语STS-b-TR数据集上，使用Pearson和Spearman相关系数评价，TurkEmbed在语义文本相似度任务中取得明显提升；在All-NLI-TR与STS-b-TR基准测试上比现有最优模型Emrecan提升1-4%。

Conclusion: TurkEmbed能够更细致地理解土耳其语言，有助于推动土耳其语NLP的发展和下游应用的进步。

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [166] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: 作者提出了PCRLLM框架，让大语言模型（LLM）的推理过程能够逐步、透明且可验证，增强模型在逻辑推理时的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在逻辑推理中经常无法遵守明确的推理规则，推理过程缺乏可验证性和信任基础，特别是在黑箱环境下。因此需要提高LLM推理的可解释性和可验证性。

Method: 提出PCRLLM框架，将LLM的推理过程限制为单步推理，每一步都明确指出前提、规则和结论，并可据此进行对照验证。此外，提出了多LLM协作推理机制，以及用于生成大规模逐步推理数据的评测基准。

Result: PCRLLM能够实现链式推理的逐步验证，提升推理过程的信任度和可控性；并能促进多个LLM协作、比较、集成不同中间结果。论文还构建了一个结合自然语言与形式严谨性的推理数据基准。

Conclusion: PCRLLM为增强LLM推理的逻辑性、可解释性和可验证性提供了有效方法，有助于大规模部署时提升其可信度和系统性。

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [167] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: 该论文提出了一种新的多轮对话大模型(LLMs)对齐奖励信号：对话嵌入轨迹的几何结构（称为交谈几何），提供了与文本内容分析互补的信息。实验发现，仅用结构信号就可获得与分析全文本接近的效果，两者结合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前大模型对齐通常基于文本内容奖励信号，忽视了对话互动过程中的动态结构信息。作者希望探索对话结构本身能否成为有效奖励信号，提升模型对齐性能并保护隐私。

Method: 提出TRACE方法，通过分析多轮对话中嵌入向量的几何运动轨迹，将其结构特征作为奖励信号训练大模型对齐代理。并对比结构信号、文本内容和两者结合三种奖励方案的有效性。

Result: 仅用结构奖励信号的模型在成对准确率上达到68.20%，接近用强大LLM分析全对话的70.04%；结构+文本的混合激励信号达到80.17%，效果最佳。

Conclusion: 对话互动结构与内容同样能有效指导LLM对齐，结合两者可最优提升性能。结构奖励信号具备隐私保护优势，还能作为诊断工具分析协作成功的模式，为交互式智能体提供了全新思路。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [168] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 本文发现当前社交机器人的检测器容易受到数据集表面特征的干扰，检测准确率在面对假关联时显著下降。作者提出了利用大语言模型进行反事实数据增强的方法，有效提升了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有社交机器人检测器在标准数据集上表现良好，但在真实多样场景下由于标签不明确和误导性线索多变，鲁棒性有限。特别是“捷径学习”——模型借助数据表面伪相关特征而非真正与任务相关的因果特征——问题尚未被充分研究，因此有必要探讨其对检测器性能的影响。

Method: 作者设计了多种“捷径”情境，人为构造用户标签与表层文本特征的假关联，对多种主流社交机器人检测器进行鲁棒性实验，并提出基于大语言模型的反事实数据增强策略，从用户文本、整体数据分布和模型因果特征学习等多层面缓解捷径学习问题。

Result: 实验结果表明，各检测器在表面特征分布移位后平均准确率下降32%。引入所提出的反事实数据增强策略后，在多类型捷径场景下准确率平均提升56%。

Conclusion: 社交机器人检测模型在真实场景下易受表面特征干扰。本文方法在提升鲁棒性方面效果显著，有助于未来更健壮的社交机器人检测系统设计。

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [169] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型迁移学习方法SPEAR-MM，在迁移到金融领域时，能有效平衡通用能力和领域适应能力，大幅减少遗忘和计算成本。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要具备专业知识的大语言模型，但在迁移过程中常常丢失原有的推理和通用交互能力（灾难性遗忘），影响实际应用。

Method: 提出SPEAR-MM方法，通过事后分析评估模型各层对外部基准任务的影响，采用选择性冻结或恢复transformer层，并结合球面插值的模型合并策略，实现有针对性地保留和恢复关键能力。

Result: 在LLaMA-3.1-8B应用中，SPEAR-MM能保留91.2%的通用能力（标准方法仅69.7%），同时保有94%的领域适应收益，计算成本减少90%。

Conclusion: SPEAR-MM能为金融行业等资源受限场景下的大模型训练提供兼顾性能和效率的迁移学习方案，并支持能力保留和适应性之间的可解释调整。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [170] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文提出了S-RAG方法，用于处理需要从大量文档中汇集信息并推理的聚合型问答任务，并引入了两份新数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法主要针对每次查询仅需少量文档（如几段文字）的情形，对于需要跨大量文档进行信息整合和推理的聚合型查询支持不足，限制了RAG在更复杂任务中的应用。

Method: S-RAG方法在数据接收阶段构建整个语料库的结构化表示，在推理阶段将自然语言查询转换为对该结构化表示的正式查询。同时，作者还构建了两份新的聚合型问答数据集（HOTELS和WORLD CUP）用于系统验证。

Result: 在新提出的HOTELS和WORLD CUP数据集以及一个公开基准数据集上，S-RAG均明显优于传统RAG系统和支持长上下文的LLMs。

Conclusion: S-RAG极大提升了RAG系统在聚合型问答任务上的表现，为复杂问答应用场景提供了更有效的技术路径，并推动了相关研究的发展。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [171] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 本文针对孟加拉语手语（BdSL）句级翻译数据稀缺的问题，提出了Bangla-SGP并扩展数据集，同时测试了多种Transformer模型在句到词汇翻译上的表现。


<details>
  <summary>Details</summary>
Motivation: BdSL领域缺乏可用于句子级翻译的大规模真实标注数据，导致现有研究主要停留在单词或字母级别，不利于实际应用和模型提升。

Method: 构建并发布了Bangla-SGP平行语料库，其中包含1000个人工标注的句子-词汇对，并通过基于规则的RAG流程和提示工程扩增出3000对。研究还与专业手语者合作，确保数据高质量，并通过微调mBart50、mT5、GPT4.1-nano等模型进行翻译实验。

Result: 在Bangla-SGP和RWTH-PHOENIX-2014T基准数据集上评估了多个模型的表现，并使用BLEU得分对翻译一致性进行了量化和比较。

Conclusion: Bangla-SGP为BdSL句级机器翻译提供了首个大规模、高质量数据集，有效推动相关模型的研发和评测，扩充了低资源手语翻译领域的研究基础。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [172] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了AlphaResearch，一种自主研究智能体，能够在开放性问题中自动发现新算法，并通过真实模拟和自动化流程与人类研究者进行对比和检验。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在复杂易验证问题上取得进展，但在未知领域的算法发现依然有限，尤其缺乏创新性自主探索能力。

Method: 提出AlphaResearch智能体，采用“提出新想法—在双重研究环境中验证—优化提案”迭代流程，结合可执行验证与模拟同行评审环境；并构建AlphaResearchComp评测平台，对8个开放算法问题提供可复现实验、客观评价指标和竞争机制。

Result: AlphaResearch在8项问题对抗赛中赢得2场，与人类研究者直接对比表现出LLMs加速算法发现的可能性；其中，在“packing circles”问题上超越了现有人类和最强基线（如AlphaEvolve）表现。

Conclusion: AlphaResearch显示了LLMs自动原创算法发现的可行性，但在部分问题仍不及人类，论文对失败案例进行了详细分析，为今后改进和研究方向提供了指导。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [173] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本论文系统性地研究了通过大模型推理链（CoT）监管模型不当行为的可行性和挑战，并提出了利用LLM自动进行监控的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在复杂任务上表现越来越好，通过分析它们详细的推理链提供了借助模型自身思考过程监管其安全性的机会，比如追踪模型抄近路或讨好人类等不良行为。然而，推理链并不总能如实反映内部决策过程，且现有监控手段对推理链的敏感度也存在问题。因此，需要系统性地研究CoT的可监管性。

Method: 论文围绕两个角度展开研究：1）模型推理链对真实决策过程的忠实程度；2）基于推理链监控异常行为的可靠性。通过具体实验评估关联性，并考察不同推理链干预方法对监控效果的影响。最终提出MoME新范式，让LLM自动分析和判断其他模型的推理链并输出结构化反馈。

Result: 实验证明推理链的表达质量、监控器的可靠性和LLM的整体表现三者之间存在紧密关联。不同干预方法可显著影响监控效果；MoME方法能够有效帮助识别并说明模型推理中的异常或不当行为。

Conclusion: 链式推理为AI安全提供了新的监控路径，但其有效性依赖推理链的忠实度及监控方法的稳定性。本文首次系统分析了这些挑战并推出自动监控新范式，有助于提升未来大模型决策透明度和安全可靠性。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [174] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 该论文提出了一种从OntoNotes 5.0语料库WSJ部分构建高质量语义角色标注（SRL）数据集并适配观点角色标注（ORL）任务的方法。


<details>
  <summary>Details</summary>
Motivation: 当前观点角色标注（ORL）任务数据稀缺，缺乏高质量可复用的训练资源。SRL与ORL任务存在天然映射关系，因此有效利用SRL资源可助力ORL研究。

Method: 使用PropBank标注框架，构建可复现的抽取流程，将谓词-论元结构与原文对齐，转换句法树指针为连续文本区间，并通过严格的数据清洗确保语义准确。具体包括处理不连续论元、注释校正及统计分析。将SRL的ARG0、REL、ARG1角色映射为ORL的Holder、Expression、Target。

Result: 构建了包含97,169个谓词-论元实例的数据集，角色映射明确，进行了细致的数据质量保障和统计分析。

Conclusion: 该数据集为利用SRL提升低资源场景下ORL任务提供了高质量、可复用的数据基础，对观点挖掘研究具有重要价值。

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [175] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型（LLMs）在不同人物设定下的道德判断表现，提出了衡量模型道德易感性和鲁棒性的基准。结果显示不同模型家族和规模在道德鲁棒性和易感性上表现各异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益在社会场景中应用，理解其如何表达和调整道德判断变得尤为重要。特别是在模拟特定角色（persona role-play）时，模型的道德反应机制值得进一步探究。

Method: 研究者使用道德基础问卷（MFQ）建立了一套用于量化大语言模型道德易感性和鲁棒性的评价基准，系统地比较了不同模型家族及不同规模模型在各种人物设定下的行为分布。

Result: 分析发现，道德鲁棒性主要受模型家族影响，而模型规模影响不大。其中Claude家族最为鲁棒，其次为Gemini和GPT-4。道德易感性则显示出模型家族的温和影响，但在同一模型家族内，模型规模越大易感性越高。此外，道德鲁棒性与易感性两者之间存在正相关关系。

Conclusion: 本文提供了一套系统性框架评估大语言模型在角色设定下的道德行为，揭示了模型家族和规模对道德表现的影响特征，为后续安全或可靠模型设计提供了参考。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [176] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TaH（Think-at-Hard）的动态推理机制，只针对难预测的token进行多步思考，有效提升了大语言模型的推理能力，并减少了不必要的计算。实验显示在有限参数下TaH显著优于多种对比方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过额外多轮推理（如recurrent transformer）提升生成质量，但额外轮次固定应用于全部token，会导致一些已正确预测的简单token反而因“过度思考”出错，浪费计算并降低性能。

Method: TaH方法采用轻量神经判别器，仅对难以预测的token触发后续迭代。在多轮推理时，引入LoRA模块专注于难点token的修正优化。同时，创新性设计duo-causal attention机制，加强了不同推理轮次之间的信息交流，同时保持序列并行计算效率。

Result: 在五个具有挑战性的评测基准上，TaH模型在不增加参数量的情况下提高了LLM的推理准确率。相比所有token都迭代两轮的方法，在只让6%的token迭代二次的情况下，准确率提升8.1-11.3%。对比单步推理模型，准确率提升4.0-5.0%。允许少量参数增加后，效果进一步提升。

Conclusion: TaH动态推理机制在保持参数高效的基础上，有效克服了固定多轮方法中的过度推理问题，大幅提升了推理准确率和模型执行效率，且方法通用性强，对LLM实际部署有重要意义。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [177] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LM）是否能够准确描述其内部计算，并比较了模型自述和他述的表现。研究发现，用于解释模型自身的解释器比用于解释其它模型的效果更好，并具有一定的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的可解释性方法大多依赖于外部分析，而语言模型本身拥有对自身内部状态的特权访问。本文旨在探索能否利用这种特权提高模型解释自身行为和机制的能力。

Method: 作者基于已有的可解释性技术（如特征编码、因果结构分析、输入对输出的影响等）作为真实标签，微调语言模型以生成关于以上三个方面的自然语言解释，并测试其在新问题上的泛化能力。还对比了模型自我解释和用其它模型解释的效果差异。

Result: 经过仅几万条解释性训练样本后，微调得到的解释器模型在新问题上展现了非平凡的泛化能力。结果表明，模型用来解释自身内部机制时通常优于由其它模型解释，即便其它模型规模更大或能力更强。

Conclusion: 语言模型能够学会并可靠地解释其内部机制，这类自解释方法有望与现有可解释性技术互补，且具备良好的可扩展性。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [178] [CAVER: Curious Audiovisual Exploring Robot](https://arxiv.org/abs/2511.07619)
*Luca Macesanu,Boueny Folefack,Samik Singh,Ruchira Ray,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 该论文提出了一种名为CAVER的新型机器人，通过主动交互学习并利用物体的视听（audiovisual）特征，实现高效的材料分类以及基于音频的模仿。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在多模态感知领域面临难以关联物体外观与音响响应的问题，制约了其在材料识别、基于音频的操作模仿等应用。该研究旨在突破这一瓶颈，让机器人通过自主探索，建立更丰富的视听知识关联。

Method: 1）开发了可激发物体声音响应的新型3D打印夹爪末端执行器；2）提出结合局部、全局外观与声音特征的视听表示方法；3）设计了好奇心驱动的探索算法，使机器人优先与高不确定性的物体交互，以高效获取多样化的音频信息。

Result: 实验表明，CAVER比其它探索基线更高效地建立了丰富的视听表征，且所学表征在材料分类和仅凭音频进行人类演示模仿方面实现了显著提升。

Conclusion: CAVER能够利用主动传感—运动交互，自主高效构建物体视听知识库，为多模态机器人操作打开了新途径，提升了相关任务的性能。

Abstract: Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. https://caver-bot.github.io/

</details>


### [179] [Time-Aware Policy Learning for Adaptive and Punctual Robot Control](https://arxiv.org/abs/2511.07654)
*Yinsen Jia,Boyuan Chen*

Main category: cs.RO

TL;DR: 本论文提出了一种时间感知强化学习框架，使机器人能够将时间作为关键变量来指导行为，从而在多领域任务中获得更高效、强健和鲁棒的表现，且无需重训练。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习方法大多忽略了时间的作用，而动物和人类智能行为普遍具备时间感知。缺乏时间感知导致机器人无法自适应地平衡高效、精确和稳定等需求，限制了其实际应用能力。

Method: 作者提出“时间感知策略学习”框架，将机器人策略中显式加入剩余时间和时间比例两个时间信号，允许单一策略根据任务需求在行为速度和精度间灵活切换，并联合优化准时性和稳定性。通过强化学习算法，机器人在无需重新训练或调整奖励函数的情况下实现自适应调整。

Result: 在长时序抓取、细颗粒物倒料、复杂物体操作和多机器人协作等任务中，所提方法在效率上提升48%，在仿真到实物转移中稳健性提升8倍，操作过程噪音降低90%，且保持接近100%的成功率，显著优于基线强化学习算法。

Conclusion: 将时间视为可控行为维度而非约束，使机器人通过“时间感知策略学习”实现高效、强健和与人类节奏同步的自主能力，提升了机器人在动态复杂环境中的适应性和协作力，为未来泛化的机器人自主提供了新基础。

Abstract: Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.

</details>


### [180] [Testing and Evaluation of Underwater Vehicle Using Hardware-In-The-Loop Simulation with HoloOcean](https://arxiv.org/abs/2511.07687)
*Braden Meyers,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 该文展示了在高保真水下模拟器HoloOcean 2.0中，结合软硬件环在环（SIL/HIL）测试框架，评估自主水下机器人（AUV）性能，并将仿真与真实实验数据对比。


<details>
  <summary>Details</summary>
Motivation: 由于水下机器人在实地测试前很难全面测试，且受限于声学传感器和操作空间，亟需一种仿真环境弥补实际测试的不足。

Method: 采用HoloOcean 2.0高保真水下仿真器，通过新的ROS2接口，搭建了CougUV鱼雷型AUV的SIL和HIL仿真-实物闭环测试平台，实现仿真传感数据与控制命令环路。

Result: 成功对CougUV AUV进行了软硬件环在环仿真验证，仿真流程可与真实水下实验结果作对比。

Conclusion: 文中方案为水下机器人实地部署前提供了可靠的仿真评估手段，有效弥补了物理环境测试的局限性。

Abstract: Testing marine robotics systems in controlled environments before field tests is challenging, especially when acoustic-based sensors and control surfaces only function properly underwater. Deploying robots in indoor tanks and pools often faces space constraints that complicate testing of control, navigation, and perception algorithms at scale. Recent developments of high-fidelity underwater simulation tools have the potential to address these problems. We demonstrate the utility of the recently released HoloOcean 2.0 simulator with improved dynamics for torpedo AUV vehicles and a new ROS 2 interface. We have successfully demonstrated a Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) setup for testing and evaluating a CougUV torpedo autonomous underwater vehicle (AUV) that was built and developed in our lab. With this HIL and SIL setup, simulations are run in HoloOcean using a ROS 2 bridge such that simulated sensor data is sent to the CougUV (mimicking sensor drivers) and control surface commands are sent back to the simulation, where vehicle dynamics and sensor data are calculated. We compare our simulated results to real-world field trial results.

</details>


### [181] [RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph](https://arxiv.org/abs/2511.07717)
*Yifan Liu,Fangneng Zhan,Wanhua Li,Haowen Sun,Katerina Fragkiadaki,Hanspeter Pfister*

Main category: cs.RO

TL;DR: 提出了一种新的机器人位姿估计方法RoboTAG，无需依赖大量标注数据，在不同类型机器人上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统单目RGB图像机器人定位方法依赖2D视觉骨干和大量有标签数据，难以适应实际应用中标注稀缺和仿真到真实的差距问题，同时忽视了3D先验知识。

Method: 提出RoboTAG，将3D分支与2D分支结合在图结构中，节点代表系统状态，边描述其依赖或对齐关系，通过图结构中的闭环实现无监督自一致性训练，使得可直接利用野外无标签图片进行训练。

Result: 在不同类型机器人实验中验证了该方法的有效性，证明了RoboTAG可显著提升在数据短缺场景下的泛化能力。

Conclusion: RoboTAG提供了一条降低机器人视觉感知对标注数据依赖的新途径，有助于缓解机器人领域数据瓶颈，为实际部署和推广提供了可能。

Abstract: Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.

</details>


### [182] [A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation](https://arxiv.org/abs/2511.07720)
*Yuxuan Zhao,Yuanchen Tang,Jindi Zhang,Hongyu Yu*

Main category: cs.RO

TL;DR: 本论文提出一种兼容多种遥操作设备和机械臂控制器的数据采集与遥操作管道，用于提高机器人操作任务的数据质量，以支持更智能的机器人学习。


<details>
  <summary>Details</summary>
Motivation: 随着机器人学习的进步，实现类人智能的机器人系统需要高质量、多样化的操作技能数据，特别是在采用大模型（如大语言模型）进行具身智能训练时。遥操作作为数据来源，其设备和控制策略对数据质量影响显著，因此有必要系统分析并优化数据采集流程。

Method: 作者开发了一套统一的遥操作管道，能够支持不同类型的遥操作设备和控制器（如基于位置的逆解、力矩的逆动力学、基于优化的顺应控制等）。其中创新性地构建了带动态零空间和阻抗跟踪的最优QP控制器，实现了高效的合规姿态追踪和奇异性规避。控制器会根据当前关节可操作性自适应调整参数权重，保证运动轨迹平滑、准确并减少奇异性发生。

Result: 通过系统实验，作者量化分析了不同遥操作接口与控制器组合下的遥操作轨迹数据质量，指标包括跟踪误差、奇异性出现频率、关节轨迹平滑度等，验证了提出管道和控制方法的有效性。

Conclusion: 论文提出的通用遥操作采集管道和最优控制方法能够有效提升用于机器人学习的数据质量，为大模型训练的多样化、高质量操作数据奠定基础。

Abstract: Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.

</details>


### [183] [LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models](https://arxiv.org/abs/2511.07727)
*Xiaohan Zhang,Yan Ding,Yohei Hayamizu,Zainab Altaweel,Yifeng Zhu,Yuke Zhu,Peter Stone,Chris Paxton,Shiqi Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种结合任务规划（task planning）与运动规划（motion planning）的移动操作（MoMa）方法，让机器人能够在现实和模拟环境下完成多物体的复杂物品重排任务。例如，根据“摆放餐具”这类模糊目标合理安排物品位置。该方法融合了大语言模型带来的常识知识与计算机视觉选址策略，兼具高效与适应性。实验表明，机器人在实际环境下的完成率为84.4%，但离人类熟练服务员还有差距。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人任务规划与运动规划各自为政，难以同时实现高层目标和底层可行性，尤其在多物体的灵活操作和不完全目标描述下更是困难。现实中的移动操作任务需求机器人在高层目标（如餐桌布置）的指导下决策如何和在哪里移动与放置多类物品，因此需要结合更强的常识推理与环境感知能力。

Method: 1) 利用大语言模型（LLM）获取任务级别常识（如餐具如何合理摆放）；2) 通过计算机视觉学习针对移动操作选择最佳底座位置（即机器人移动与操作时的起始点/朝向）；3) 将常识推理、底座选取与物体重排的任务-运动规划整合入统一的MoMa TAMP框架，实现对新情景的自适应。

Result: 在现实和仿真环境中进行了多次长时物品重排实验。机器人在实际环境下完成率为84.4%，效率和成功率显著提升。主观评价显示与经验丰富的人类服务员尚有差距。

Conclusion: 该方法在多物品移动操作任务下表现优异，尤其能适应新情景与需求不明确的目标。尽管机器人表现接近高效，但实际操作精准度和人类相比仍待提高。

Abstract: Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.
  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.

</details>


### [184] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: 本文提出了一种能够利用无动作标注视频进行机器人控制学习的新方法ViPRA。该方法通过视频预测和少量有标注数据，学习并映射潜在动作到机器人动作，实现出色的机器人控制性能。


<details>
  <summary>Details</summary>
Motivation: 大量视频包含了丰富的物理交互信息，但因缺少动作标注，难以直接用于机器人学习。目前方法对标注依赖大、泛化能力有限，需要一种能利用无标注视频预训练并以少量数据快速适应具体机器人的方法。

Method: ViPRA框架采用“预训练-微调”策略，首先用视频-语言模型预测未来视觉观测及运动潜在动作，并通过感知损失和光流一致性保证潜在动作物理真实性。下游控制时，用分块流匹配解码器仅需100-200条演示，将潜在动作解码为机器人动作序列，实现高频连续控制。

Result: 在SIMPLER基准上，比强基线方法提升16%，在现实场景中多项机械操作任务性能提升13%。能在多机器人形态上泛化，控制频率可达22 Hz。

Conclusion: ViPRA能以极少标注数据利用大量无标注视频学习机器人控制，效果优于现有方法，降低人工成本，提高泛化和控制平滑性。代码和模型将公开，有望推动机器人学习发展。

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [185] [Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space](https://arxiv.org/abs/2511.07750)
*Durgakant Pushp,Weizhe Chen,Zheng Chen,Chaomin Luo,Jason M. Gregory,Lantao Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Pareto-Optimal Visual Navigation的视觉导航方法，结合了数据驱动的语义理解、帕累托最优决策和视觉伺服，实现了轻量级且适应性强的实时导航。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法中，无地图的反应式方法在复杂环境下表现不佳；基于地图的方法需要大量的地图制作工作；而基于学习的方法通常依赖大规模数据集且泛化能力有限。

Method: 本方法在图像空间内工作，融合了语义感知、帕累托最优决策流程与视觉伺服，形成了一套轻量级、实时的视觉导航框架。

Result: 实验结果表明，该框架能够在无需重度地图或大数据集的前提下，有效地在复杂真实环境中实现导航。

Conclusion: Pareto-Optimal Visual Navigation为复杂环境下的自主导航提供了一种高效且实用的新途径，兼具实时性、适应性和轻量化，突破了传统方法的若干局限。

Abstract: Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.

</details>


### [186] [High-Altitude Balloon Station-Keeping with First Order Model Predictive Control](https://arxiv.org/abs/2511.07761)
*Myles Pasetsky,Jiawei Lin,Bradley Guo,Sarah Dean*

Main category: cs.RO

TL;DR: 本文提出了一种基于一阶模型预测控制（FOMPC）的方法，用于高空气球的定点控制，并证明该方法优于当前主流的无模型强化学习策略。


<details>
  <summary>Details</summary>
Motivation: 高空气球具有非线性、欠驱动的动态特性及风场观测不全，现有多依赖无模型RL方法进行控制，普遍认为模型驱动方法在复杂系统和风预报不确定下不实用，作者对此假设提出质疑。

Method: 提出First-Order Model Predictive Control（FOMPC），将风和气球动力学作为可微分函数融入JAX框架，实现基于梯度的轨迹优化，从而进行在线规划；并通过消融实验对建模和控制的多种假设进行评测。

Result: FOMPC无需离线训练，在线规划每步计算量较大，但定点控制效果比最先进的RL策略提升24%（以TWR为评价指标）；在简化模型和不同配置下均表现良好。

Conclusion: 在高空气球定点控制问题中，基于模型的预测控制不仅可行，还优于无模型强化学习，在线规划是有效的解决途径。

Abstract: High-altitude balloons (HABs) are common in scientific research due to their wide range of applications and low cost. Because of their nonlinear, underactuated dynamics and the partial observability of wind fields, prior work has largely relied on model-free reinforcement learning (RL) methods to design near-optimal control schemes for station-keeping. These methods often compare only against hand-crafted heuristics, dismissing model-based approaches as impractical given the system complexity and uncertain wind forecasts. We revisit this assumption about the efficacy of model-based control for station-keeping by developing First-Order Model Predictive Control (FOMPC). By implementing the wind and balloon dynamics as differentiable functions in JAX, we enable gradient-based trajectory optimization for online planning. FOMPC outperforms a state-of-the-art RL policy, achieving a 24% improvement in time-within-radius (TWR) without requiring offline training, though at the cost of greater online computation per control step. Through systematic ablations of modeling assumptions and control factors, we show that online planning is effective across many configurations, including under simplified wind and dynamics models.

</details>


### [187] [Benchmarking Resilience and Sensitivity of Polyurethane-Based Vision-Based Tactile Sensors](https://arxiv.org/abs/2511.07797)
*Benjamin Davis,Hannah Stuart*

Main category: cs.RO

TL;DR: 本文提出将聚氨酯橡胶应用于视觉触觉传感器（VBTSs）以提升耐用性，并通过一系列标准化测试对比其与传统硅胶的性能。结果显示，聚氨酯材料在承受高负载和磨损方面优于硅胶，同时在实际应用中比如瓶盖操作任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有VBTSs普遍采用硅胶作为触觉表层，虽具有高灵敏度，但耐用性差，易在长期加载和磨损中退化。作者希望探索能否用高耐用性的聚氨酯橡胶替代硅胶，在保持可用灵敏度的前提下提升传感器的物理耐久性。

Method: 设计并实施了标准化的基准评测流程，分别测试了硅胶及聚氨酯胶在正常加载、剪切加载、磨损等情况下的耐用性表现。同时，引入了无需模型依赖的力和空间分辨率灵敏度测试，直接评价材料本体的传感能力。最后，通过瓶盖拧紧/松开的实际演示任务，验证两种材料在复杂操作中的表现差异。

Result: 实验表明，聚氨酯橡胶在耐高负载和抗磨损方面显著优于硅胶，灵敏度略低但在多数应用中仍然足够。在瓶盖拧紧/松开演示中，聚氨酯表现出更佳的耐久和效果。

Conclusion: 聚氨酯橡胶作为视觉触觉传感表层材料能显著提升VBTSs的耐用性，在力学负载和磨损严苛环境下优于硅胶。对于需要高寿命的实际机器人应用，聚氨酯是一种具有优势的选择。

Abstract: Vision-based tactile sensors (VBTSs) are a promising technology for robots, providing them with dense signals that can be translated into an understanding of normal and shear load, contact region, texture classification, and more. However, existing VBTS tactile surfaces make use of silicone gels, which provide high sensitivity but easily deteriorate from loading and surface wear. We propose that polyurethane rubber, used for high-load applications like shoe soles, rubber wheels, and industrial gaskets, may provide improved physical gel resilience, potentially at the cost of sensitivity. To compare the resilience and sensitivity of silicone and polyurethane VBTS gels, we propose a series of standard evaluation benchmarking protocols. Our resilience tests assess sensor durability across normal loading, shear loading, and abrasion. For sensitivity, we introduce model-free assessments of force and spatial sensitivity to directly measure the physical capabilities of each gel without effects introduced from data and model quality. Finally, we include a bottle cap loosening and tightening demonstration as an example where polyurethane gels provide an advantage over their silicone counterparts.

</details>


### [188] [Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution](https://arxiv.org/abs/2511.07811)
*Sagar Gupta,Thanh Vinh Nguyen,Thieu Long Phan,Vidul Attri,Archit Gupta,Niroshinie Fernando,Kevin Lee,Seng W. Loke,Ronny Kutadinata,Benjamin Champion,Akansel Cosgun*

Main category: cs.RO

TL;DR: 本文提出了一种结合去中心化路径规划与中心化冲突解决的多机器人协调框架。每个机器人自主规划路径，并将信息共享到中心节点，由中心系统检测潜在冲突并控制通行顺序。仿真实验和实际机器人测试均验证了此方法能提升任务成功率并减少死锁。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人路径规划方案面临死锁和效率问题，尤其是在全中心化方法中，路径规划受限且计算负担大。作者希望通过将分布式自主性和集中式冲突管理结合，实现高效且鲁棒的多机器人协作。

Method: 每台机器人独立规划路径，将其传递给中心节点。中心系统监控并检测冲突，仅对有冲突的机器人实施交通灯式的暂停指令，让其他机器人在冲突点外等待，通过简化集中管理只在必要时介入，减少死锁发生。

Result: 仿真显示该方法提升了机器人达成目标的成功率，减少了死锁情况。此外，作者在两台四足机器人和Duckiebots小车的实际测试中也证明了该系统的有效性。

Conclusion: 结合去中心化路径自主规划与中心化冲突缓解的框架在提升多机器人系统稳定性和效率方面效果显著，比传统集中式方案更加灵活和高效。

Abstract: We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.

</details>


### [189] [SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control](https://arxiv.org/abs/2511.07820)
*Zhengyi Luo,Ye Yuan,Tingwu Wang,Chenran Li,Sirui Chen,Fernando Castañeda,Zi-Ang Cao,Jiefeng Li,David Minor,Qingwei Ben,Xingye Da,Runyu Ding,Cyrus Hogg,Lina Song,Edy Lim,Eugene Jeong,Tairan He,Haoru Xue,Wenli Xiao,Zi Wang,Simon Yuen,Jan Kautz,Yan Chang,Umar Iqbal,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文提出通过大规模扩展模型容量、数据量和算力，成功训练出具备泛化能力的人形机器人通用控制器，实现自然且稳健的全身运动。


<details>
  <summary>Details</summary>
Motivation: 尽管大型基础模型在图像、语言等领域取得显著进展，但人形机器人控制尚未受益于类似的规模扩展。现有神经网络控制器规模有限，目标行为单一，训练资源较少，无法胜任复杂多样的人类动作。

Method: 作者将动作追踪作为可扩展的人形控制任务，以丰富的动作捕捉数据进行密集监督，无需手工奖励设计。通过三方面扩展：网络规模（从1.2M到42M参数）、数据量（1亿帧，700小时动作数据）、计算资源（9000 GPU小时），构建基础动作追踪模型。此外，通过实时通用运动规划器和统一token空间，实现多种输入接口的统一控制。

Result: 实验表明，随着算力和数据多样性提升，控制性能逐步上升，学到的运动表征能泛化到新颖动作。模型不仅规模效应明显，还能支持VR、视频、视觉-语言-动作多模态的运动输入，具备实际应用价值。

Conclusion: 大规模动作追踪为人形机器人控制奠定了实用基础。扩展模型、数据和算力是提升人形控制能力的有效途径，拓展了通用控制器的实用边界。

Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.

</details>


### [190] [Occlusion-Aware Ground Target Search by a UAV in an Urban Environment](https://arxiv.org/abs/2511.07822)
*Collin Hague,Artur Wolek*

Main category: cs.RO

TL;DR: 本文提出了一种基于概率可见性体积（VV）和迭代加深A*算法的无人机动态路径规划方法，提升了城市环境下对动态目标的搜索效率。


<details>
  <summary>Details</summary>
Motivation: 城市环境中建筑物等障碍物经常遮挡无人机（UAV）对兴趣点（POI）的视线，对POI进行有效、可靠的搜索和跟踪变得极具挑战性。现有方法在面对复杂遮挡及传感器误报时表现不佳，因此亟需更高效的路径规划策略。

Method: 将无人机建模为可变速Dubins轨迹飞行器，装备视距传感器。引入概率可见性体积（VV）来动态描述POI在城市环境下的可感知区域。利用迭代加深A*算法规划路径，通过乐观估算未来概率提升对POI的观测概率，并采用最大池化策略进行变时步规划以平衡长短期目标。

Result: 通过蒙特卡洛仿真，在高误报概率和城市复杂环境下，与传统路径规划方法对比，本文方法表现出更优的搜索效率和目标观测概率。

Conclusion: 基于概率VV和迭代加深A*算法的无人机路径规划显著提升了城市复杂环境下对动态目标的搜索表现，特别适应高误报率的传感器场景。

Abstract: This paper considers the problem of searching for a point of interest (POI) moving along an urban road network with an uncrewed aerial vehicle (UAV). The UAV is modeled as a variable-speed Dubins vehicle with a line-of-sight sensor in an urban environment that may occlude the sensor's view of the POI. A search strategy is proposed that exploits a probabilistic visibility volume (VV) to plan its future motion with iterative deepening $A^\ast$. The probabilistic VV is a time-varying three-dimensional representation of the sensing constraints for a particular distribution of the POI's state. To find the path most likely to view the POI, the planner uses a heuristic to optimistically estimate the probability of viewing the POI over a time horizon. The probabilistic VV is max-pooled to create a variable-timestep planner that reduces the search space and balances long-term and short-term planning. The proposed path planning method is compared to prior work with a Monte-Carlo simulation and is shown to outperform the baseline methods in cluttered environments when the UAV's sensor has a higher false alarm probability.

</details>


### [191] [A Comprehensive Experimental Characterization of Mechanical Layer Jamming Systems](https://arxiv.org/abs/2511.07882)
*Jessica Gumowski,Krishna Manaswi Digumarti,David Howard*

Main category: cs.RO

TL;DR: 本论文研究了一种模仿自然生物（如头足类和厚皮类动物）通过调节结构刚度来提高柔性机器人操作能力的方法。作者提出了一种基于机械层挤压（layer jamming）的多材料结构，并分析了设计参数对结构性能的影响。


<details>
  <summary>Details</summary>
Motivation: 动物自然界中通过调节身体或附肢刚度实现高灵活性，对柔性机器人设计具有重要启发。因此，研究如何通过人工结构实现刚度可调，有助于提升软体机器人的功能和应用范围。

Method: 本文采用了两层多材料结构，并在其界面设计了齿状突起，通过机械层挤压来调节结构刚度。作者对齿形几何等关键设计因素进行系统实验，包括弯曲和扭转加载测试，并测量了分离所需的层间力。

Result: 所提出的结构在弯曲实验中刚度提升可达5倍，扭转载荷下提升3.2倍。同时，首次测量并报告了分离两层所需的力，对实际应用具有参考价值。

Conclusion: 机械层挤压结构能有效实现刚度大范围调节，结构中齿形等参数对性能影响显著。本文结果为机械层挤压结构的优化设计与工程应用提供了重要理论和实验参考。

Abstract: Organisms in nature, such as Cephalopods and Pachyderms, exploit stiffness modulation to achieve amazing dexterity in the control of their appendages. In this paper, we explore the phenomenon of layer jamming, which is a popular stiffness modulation mechanism that provides an equivalent capability for soft robots. More specifically, we focus on mechanical layer jamming, which we realise through two-layer multi material structure with tooth-like protrusions. We identify key design parameters for mechanical layer jamming systems, including the ability to modulate stiffness, and perform a variety of comprehensive tests placing the specimens under bending and torsional loads to understand the influence of our selected design parameters (mainly tooth geometry) on the performance of the jammed structures. We note the ability of these structures to produce a peak change in stiffness of 5 times in bending and 3.2 times in torsion. We also measure the force required to separate the two jammed layers, an often ignored parameter in the study of jamming-induced stiffness change. This study aims to shed light on the principled design of mechanical layer jammed systems and guide researchers in the selection of appropriate designs for their specific application domains.

</details>


### [192] [EquiMus: Energy-Equivalent Dynamic Modeling and Simulation of Musculoskeletal Robots Driven by Linear Elastic Actuators](https://arxiv.org/abs/2511.07887)
*Yinglei Zhu,Xuguang Dong,Qiyao Wang,Qi Shao,Fugui Xie,Xinjun Liu,Huichan Zhao*

Main category: cs.RO

TL;DR: 本文提出EquiMus框架，实现对刚-软混合机器人动力学的高效建模与仿真。


<details>
  <summary>Details</summary>
Motivation: 由于刚-软混合机器人具有复杂的构型和物理特性，现有建模与仿真方法难以精准、有效地描述其动力学，尤其是在大规模、复杂运动模式下，亟需新的动力学建模与仿真工具。

Method: 提出了一种能量等效的动力学建模框架EquiMus，用于刚-软混合结构、生物肌肉骨骼机器人的动力学建模，并集成在MuJoCo仿真平台中。该方法考虑了连杆质量的连续分布、运动环路与多种运动模式。方法通过仿真与真实仿生机器人腿实验进行了验证。

Result: 仿真与实物实验表明EquiMus建模方法在机腿运动行为上具有较高精度，且在控制器设计和基于学习的控制中表现出良好适用性。

Conclusion: EquiMus框架为刚-软混合机器人动力学建模与下游控制任务提供了有效工具，可显著提升相关机器人的开发与应用效率。

Abstract: Dynamic modeling and control are critical for unleashing soft robots' potential, yet remain challenging due to their complex constitutive behaviors and real-world operating conditions. Bio-inspired musculoskeletal robots, which integrate rigid skeletons with soft actuators, combine high load-bearing capacity with inherent flexibility. Although actuation dynamics have been studied through experimental methods and surrogate models, accurate and effective modeling and simulation remain a significant challenge, especially for large-scale hybrid rigid--soft robots with continuously distributed mass, kinematic loops, and diverse motion modes. To address these challenges, we propose EquiMus, an energy-equivalent dynamic modeling framework and MuJoCo-based simulation for musculoskeletal rigid--soft hybrid robots with linear elastic actuators. The equivalence and effectiveness of the proposed approach are validated and examined through both simulations and real-world experiments on a bionic robotic leg. EquiMus further demonstrates its utility for downstream tasks, including controller design and learning-based control strategies.

</details>


### [193] [Dual-MPC Footstep Planning for Robust Quadruped Locomotion](https://arxiv.org/abs/2511.07921)
*Byeong-Il Ham,Hyun-Bin Kim,Jeonguk Kang,Keun Ha Choi,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型预测控制（MPC）的步态规划策略，通过优化落足点，能更好地调节四足机器人身体朝向，减少不必要的身体旋转，进而提升运动稳定性。


<details>
  <summary>Details</summary>
Motivation: 以往的步态规划更多关注线速度而忽略了角速度，只用地面反作用力（GRF）来调节角动量，导致身体控制不充分，运动稳定性不足。本文的目标是将角速度也纳入步态规划，通过更精细的控制优化机器人躯干的朝向和稳定性。

Method: 方法基于MPC，将角速度引入足端规划，把角动量调控问题转化为足端位置与GRF的双输入协同优化。步态规划与GRF的MPC控制器互相反馈，各自利用对方的解进行迭代更新，优化落足点与GRF，从而减少身体晃动、延长支撑和摆动周期。

Result: 在四足机器人上进行了实验证明，该方法能有效减少身体振荡，显著延长支撑和摆动相位，并在不同地形上表现出更强的运动鲁棒性。

Conclusion: 提出的MPC步态规划方法能通过协同优化足端位置和GRF，实现对机器人身体角运动的更好调节，提高了机器人的运动性能和适应复杂地形的能力。

Abstract: In this paper, we propose a footstep planning strategy based on model predictive control (MPC) that enables robust regulation of body orientation against undesired body rotations by optimizing footstep placement. Model-based locomotion approaches typically adopt heuristic methods or planning based on the linear inverted pendulum model. These methods account for linear velocity in footstep planning, while excluding angular velocity, which leads to angular momentum being handled exclusively via ground reaction force (GRF). Footstep planning based on MPC that takes angular velocity into account recasts the angular momentum control problem as a dual-input approach that coordinates GRFs and footstep placement, instead of optimizing GRFs alone, thereby improving tracking performance. A mutual-feedback loop couples the footstep planner and the GRF MPC, with each using the other's solution to iteratively update footsteps and GRFs. The use of optimal solutions reduces body oscillation and enables extended stance and swing phases. The method is validated on a quadruped robot, demonstrating robust locomotion with reduced oscillations, longer stance and swing phases across various terrains.

</details>


### [194] [Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments](https://arxiv.org/abs/2511.07927)
*Okan Arif Guvenkaya,Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 本文提出了一种结合切线法路径规划与轨迹外推的新算法，实现无人地面车辆（UGV）在动态障碍物密集环境中的局部路径规划与避障。通过多场景仿真验证，算法能有效生成无碰撞路径，引导机器人安全通过。


<details>
  <summary>Details</summary>
Motivation: 动态障碍物密集的环境对UGV路径规划与实时避障提出了较高要求，现有方法难以兼顾实时性与安全性，因此亟需新的高效算法。

Method: 首先为UGV计算一条全局路径并给出若干中间路点。针对动态障碍物（其轨迹为多项式、速度和初位置随机），结合切线路径规划和轨迹外推方法，实时调整局部路径，避免与障碍物碰撞。

Result: 在多种动态障碍物随机运动的仿真场景下，所提算法能够动态生成安全、无碰撞的路径，有效引导UGV从起点到终点。

Conclusion: 所提出的局部路径规划算法在动态复杂环境下表现出良好的避障性能与鲁棒性，可为实际无人车导航提供技术支撑。

Abstract: Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.

</details>


### [195] [USV Obstacles Detection and Tracking in Marine Environments](https://arxiv.org/abs/2511.07950)
*Yara AlaaEldin,Enrico Simetti,Francesca Odone*

Main category: cs.RO

TL;DR: 该论文提出并评估了一种用于无人水面艇（USV）在海洋环境下障碍物检测和跟踪的系统，包括基于摄像头与LiDAR融合、仅用LiDAR、以及融合两者的混合方法，并在真实数据集上进行了实验分析。


<details>
  <summary>Details</summary>
Motivation: 在海洋复杂环境中实现USV的安全自主航行，依赖于高效可靠的障碍物检测与跟踪系统，但由于水面反光、遮挡和多变条件，这一任务极具挑战性。

Method: 论文首先评估现有的障碍物检测与跟踪系统在新近公开的海洋数据集上的性能，然后将系统各模块集成到ROS平台，并在MIT海洋数据集上利用同步的LiDAR和摄像头实时数据进行测试，同时比较摄像头与LiDAR融合、仅用LiDAR、以及融合二者优点的混合方法，并进行详细实验分析。

Result: 实验证明，融合摄像头和LiDAR的传感器融合方法与仅用LiDAR方法各有优缺点，而混合方法能有效结合两者优势，为USV周围环境建立了更为准确的信息化障碍物地图。

Conclusion: 提出的方法经过实验证明可显著提升USV在复杂海洋环境下的障碍物检测和跟踪能力，为实现更安全的自主航行提供支持。

Abstract: Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.

</details>


### [196] [Effective Game-Theoretic Motion Planning via Nested Search](https://arxiv.org/abs/2511.08001)
*Avishav Engle,Andrey Zhitnikov,Oren Salzman,Omer Ben-Porat,Kiril Solovey*

Main category: cs.RO

TL;DR: 本文提出了一种更高效、可扩展且理论证明正确的新方法GTNS，用于在常规动态系统中计算Nash均衡，并在自动驾驶等场景中实现了高效求解。


<details>
  <summary>Details</summary>
Motivation: 当前基于博弈论的机器人决策方法存在两大主要瓶颈：一是优化法依赖于简化动力学且易陷入局部极值点，二是基于收益矩阵的方法因需要枚举所有轨迹，难以扩展至复杂系统。因此，亟需一种既能处理复杂动态系统、又具备可扩展性与精度保证的博弈均衡解法。

Method: 作者提出了Game-Theoretic Nested Search（GTNS）算法，通过在全体智能体的动作空间内高效搜索，结合内层低维空间的约束筛选，排除不满足纳什均衡的轨迹，并支持用户通过指定全局目标偏好选择不同均衡点。该方法适用于一般动态系统且有理论正确性保证。

Result: GTNS算法在多种自动驾驶和赛车场景下进行了实验，能在秒级时间内在普通硬件上求解出博弈均衡，验证了方法的效率和可扩展性。

Conclusion: GTNS克服了传统博弈算法的局限，实现了在复杂动态系统中高效稳定地计算纳什均衡，并允许针对具体需求灵活选择均衡点，有助于推动机器人在多智能体复杂环境中的真实部署。

Abstract: To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.

</details>


### [197] [A Two-Layer Electrostatic Film Actuator with High Actuation Stress and Integrated Brake](https://arxiv.org/abs/2511.08005)
*Huacen Wang,Hongqiang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种带有集成刹车的新型双层静电薄膜驱动器，相比传统三相结构，极大提升了输出性能与承载能力，实现了更高效的驱动与刹车功能。


<details>
  <summary>Details</summary>
Motivation: 传统马达驱动的机器人存在质量大、控制复杂和需额外刹车装置等问题，限制了轻量化及小型化机器人的应用。现有静电薄膜驱动器在空气中的驱动力和刹车能力有限，尤其三相结构需要进一步改进。

Method: 设计并制造了双层静电薄膜驱动器，通过将电极交错分布在上下两层，实现了更小的电极间距，采用集成的静电附着机制来实现刹车。实验演示包括拉力对比、负载搬运、单自由度机械臂与双模式抓取器。

Result: 新型驱动器实现了约241 N/m²的驱动力，相较传统空气三相驱动器提升约90.5%。集成刹车设计实现负载保持，展示了更强的驱动与刹车能力。

Conclusion: 双层静电薄膜驱动器在驱动力、刹车能力和结构紧凑性上表现优异，适用于轻量化、小型化机器人系统，对未来软体机器人等领域有重要推动作用。

Abstract: Robotic systems driven by conventional motors often suffer from challenges such as large mass, complex control algorithms, and the need for additional braking mechanisms, which limit their applications in lightweight and compact robotic platforms. Electrostatic film actuators offer several advantages, including thinness, flexibility, lightweight construction, and high open-loop positioning accuracy. However, the actuation stress exhibited by conventional actuators in air still needs improvement, particularly for the widely used three-phase electrode design. To enhance the output performance of actuators, this paper presents a two-layer electrostatic film actuator with an integrated brake. By alternately distributing electrodes on both the top and bottom layers, a smaller effective electrode pitch is achieved under the same fabrication constraints, resulting in an actuation stress of approximately 241~N/m$^2$, representing a 90.5\% improvement over previous three-phase actuators operating in air. Furthermore, its integrated electrostatic adhesion mechanism enables load retention under braking mode. Several demonstrations, including a tug-of-war between a conventional single-layer actuator and the proposed two-layer actuator, a payload operation, a one-degree-of-freedom robotic arm, and a dual-mode gripper, were conducted to validate the actuator's advantageous capabilities in both actuation and braking modes.

</details>


### [198] [AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles](https://arxiv.org/abs/2511.08016)
*Adrian Schönnagel,Michael Dubé,Christoph Steup,Felix Keppler,Sanaz Mostaghim*

Main category: cs.RO

TL;DR: 本文提出了一种利用去中心化群体智能的创新方法，针对重型铰接车辆（HAVs）中的刀架现象和相互碰撞问题，实现有效规避。通过模拟实验证明方法极大提升了安全性和任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有群体机器人研究多针对形状简化的机器人，而实际应用中如物流、采矿、机场行李运输等领域涉及到的重型铰接车辆由于长度和复杂运动学带来独特挑战，现有文献几乎未有涉及。

Method: 提出了一种完全基于反应、去中心化的群体智能策略，专门针对重型铰接车辆的自动化，重点解决刀架现象，并兼顾多车之间的碰撞规避。

Result: 单车实验中99.8%有效规避刀架，86.7%和83.4%的车辆分别完成第一次和第二次目标。双车交互实验中，刀架规避率98.9%，目标完成率分别为79.4%和65.1%，而99.7%的车辆未发生相互碰撞。

Conclusion: 所提出的去中心化反应式控制策略在避免HAV刀架和碰撞方面表现优异，展示出高度的安全性和有效性，为重型铰接车辆群体自动化作业奠定了方法基础。

Abstract: This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.
  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.

</details>


### [199] [Model Predictive Control via Probabilistic Inference: A Tutorial](https://arxiv.org/abs/2511.08019)
*Kohei Honda*

Main category: cs.RO

TL;DR: 本文介绍了以概率推断为基础的模型预测控制（MPC）方法，为复杂机器人系统提供了一种新的优化控制工具，重点讲解了MPPI类算法的理论基础和实际实现方法。


<details>
  <summary>Details</summary>
Motivation: 传统的数值优化方法在处理机器人的非线性或不可微动力学时效率低下或不可行，需要更通用、灵活的控制方法。

Method: 通过将最优控制问题重新表述为概率推断问题，采用采样方法（如MPPI）估算控制分布，从而适应任意复杂的代价函数和动力学。系统推导了最优控制分布，并结合MPPI算法举例，讨论了先验设计、变分分布、参数调优以及理论分析。

Result: 以MPPI为代表的概率推断型MPC方法能够无需梯度，灵活高效地处理复杂系统，支持对任意动力学和代价函数的优化控制需求。

Conclusion: 概率推断型MPC为机器人优化控制提供了强大理论和实践支持，是理解和拓展复杂系统MPC的一种系统性新途径。

Abstract: Model Predictive Control (MPC) is a fundamental framework for optimizing robot behavior over a finite future horizon. While conventional numerical optimization methods can efficiently handle simple dynamics and cost structures, they often become intractable for the nonlinear or non-differentiable systems commonly encountered in robotics. This article provides a tutorial on probabilistic inference-based MPC, presenting a unified theoretical foundation and a comprehensive overview of representative methods. Probabilistic inference-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have gained significant attention by reinterpreting optimal control as a problem of probabilistic inference. Rather than relying on gradient-based numerical optimization, these methods estimate optimal control distributions through sampling-based techniques, accommodating arbitrary cost functions and dynamics. We first derive the optimal control distribution from the standard optimal control problem, elucidating its probabilistic interpretation and key characteristics. The widely used MPPI algorithm is then derived as a practical example, followed by discussions on prior and variational distribution design, tuning principles, and theoretical aspects. This article aims to serve as a systematic guide for researchers and practitioners seeking to understand, implement, and extend these methods in robotics and beyond.

</details>


### [200] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 研究探讨通过ReAct框架显式引入多视角信息，提升大语言模型（LLM）在多智能体协作系统中的视角理解与协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多智能体交互中，缺乏对物理与认知视角的理解和推理能力，难以处理多观察者场景下的主观性与参考歧义问题。

Method: 采用ReAct框架，将推理与行动结合，扩展了经典Director任务，设计了七种逐步增加视角难度的场景，通过显式视角提示及主动视觉探索策略，比较不同状态描述和提示方式下模型的表现。

Result: 结果显示，引入显式视角提示及主动探索策略，能显著提升模型的理解精准度和协作效果。

Conclusion: 结合主动感知与视角推理机制，有助于提升LLM在机器人和多智能体系统中的应用能力，为后续自适应、情境感知AI研究奠定基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


### [201] [Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.08214)
*Yi Huang,Zhan Qu,Lihui Jiang,Bingbing Liu,Hongbo Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为PGS（Perception-Guided Self-Supervision）的感知引导自监督训练范式，有效缓解了端到端自动驾驶系统在闭环场景中因模仿学习而产生的因果混淆问题，并在Bench2Drive基准上取得显著领先。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统多依赖模仿学习，并使用专家驾驶数据训练。然而，这些方法在闭环测试时常因依赖带有噪声的专家轨迹导致因果关系混淆，最终影响性能。本文动机是解决这种因果混淆的问题，提高自动驾驶系统的实际泛化和可靠性。

Method: 作者提出感知引导自监督（PGS）方法，即用感知模块输出（如车道中心线、周围车辆预测运动等）作为决策模块的主监督信号，实现输入输出与感知结果对齐。在自监督中引入正负监督信号，引导自主车辆轨迹调整，减少对专家噪声数据的依赖，从而建模更好的因果关系。

Result: 基于标准端到端架构，PGS方法在高难度闭环Bench2Drive基准上获得78.08的Driving Score和48.64%的平均成功率，显著优于现有先进方法，包括那些结构和推理更复杂的方法。

Conclusion: PGS框架通过引入感知驱动的自监督，有效缓解了因果混淆和专家数据噪声带来的问题，提升了自动驾驶系统在闭环实际场景下的泛化与鲁棒性。该方法简单高效，为自动驾驶技术提供了新的发展方向。

Abstract: End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.
  To address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.
  Equipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.

</details>


### [202] [Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems](https://arxiv.org/abs/2511.08231)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: 本文提出了一种基于多保真度残差物理信息神经过程（MFR-PINP）的实时数据驱动状态估计方法，并在机器人系统上验证了其实时估计能力。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在实时非线性状态估计领域的广泛应用，对于模型预测的可靠误差界变得尤为重要，特别是在安全关键应用场景下。传统低保真模型常常与实际复杂动力学存在偏差，因此需要新的方法提升估计精度并量化模型不确定性。

Method: 作者提出使用MFR-PINP方法，通过学习简单低保真模型与高保真真实动力学间的残差，以提升动态模型的精度。同时，结合split conformal（SC）预测框架，在训练和推理阶段为模型提供稳健不确定性界限，开发了面向混合在线学习场景的实现方案。

Result: 实验结果显示，在与主流卡尔曼滤波变种（无迹卡尔曼滤波器、深度卡尔曼滤波器）对比时，该方法在状态估计实时性和精度方面表现优异。

Conclusion: MFR-PINP模型能有效提升实时状态估计的准确性，同时提供不确定性保证，是用于实时状态估计任务的有竞争力的新选择。

Abstract: Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.

</details>


### [203] [X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention](https://arxiv.org/abs/2511.08277)
*Dehan Shen,Changhao Chen*

Main category: cs.RO

TL;DR: 提出了X-IONet，一种仅依赖单个IMU、可同时用于行人和四足机器人惯性里程计的新方法。通过平台分类和专用专家网络，显著提升了在两种平台上的轨迹估计精度。


<details>
  <summary>Details</summary>
Motivation: 以往基于学习的方法在行人导航上效果显著，但直接应用于四足机器人时，因运动模式差异大，效果明显下降。因此需要设计能跨平台使用、兼具鲁棒性的方法。

Method: X-IONet使用单一IMU作为输入。通过规则专家模块识别当前运动平台类型，并将IMU序列分流到平台特定的专家网络。其网络采用双阶段注意力结构（同时建模长序列依赖与多轴相关性），输出位移和不确定性，再结合扩展卡尔曼滤波实现鲁棒状态估计。

Result: 在公开行人数据集与自采集的四足机器人数据集上验证，X-IONet在行人数据上的ATE和RTE分别下降14.3%、11.4%；在四足机器人数据上分别下降52.8%、41.3%，均达到了SOTA水平。

Conclusion: X-IONet显著提升了人类与腿式机器人平台上的惯性导航精度和鲁棒性，展现了跨平台惯性里程计方法的有效性。

Abstract: Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.

</details>


### [204] [Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot](https://arxiv.org/abs/2511.08299)
*Zhiang Liu,Yang Liu,Yongchun Fang,Xian Guo*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的学习框架，使得四足机器人能够自主习得多样的全方位步态，而无需依赖预定义的动作序列，有效提升了动作的灵活性和多样性。


<details>
  <summary>Details</summary>
Motivation: 受蝾螈等生物性四足动物骨骼结构启发，现有机器人未能充分利用其形态优势，传统控制方法多依赖于预设步态或关节轨迹，导致动作单一且缺乏灵活性。该研究旨在突破现有控制方法的局限，实现更丰富和灵活的运动能力。

Method: 提出了一套基于强化学习的新框架，机器人通过各身体部位的相位变量独立控制，利用相位覆盖奖励鼓励探索腿部相位空间。同时，通过数据增强引入形态对称性以提升采样效率，并保证学习到的行为在动作和任务层面上的对称性。

Result: 通过大量实验，机器人获得了22种具有动态性和对称性的全向步态，显示了新框架在自适应和多样 gait 生成方面的有效性。

Conclusion: 本研究证明了所提出的学习框架能够让四足机器人实现多样、高效且动态平衡的多方向行走，解决了传统方法下步态缺乏多样性和适应性的难题。

Abstract: Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.

</details>


### [205] [A CODECO Case Study and Initial Validation for Edge Orchestration of Autonomous Mobile Robots](https://arxiv.org/abs/2511.08354)
*H. Zhu,T. Samizadeh,R. C. Sofia*

Main category: cs.RO

TL;DR: 本文通过案例研究，比较了CODECO编排与标准Kubernetes在智能制造自主移动机器人（AMR）上的表现，结果表明CODECO能降低CPU消耗并带来更稳定的通信，付出的代价是较小的内存开销和生命周期延迟。


<details>
  <summary>Details</summary>
Motivation: Kubernetes在移动、资源受限的机器人环境中存在网络不稳定、资源异构和计算资源有限等不适配问题。作者旨在探讨更适合AMR边缘-云环境的软件编排方式。

Method: 作者选用智能制造场景下的AMR进行案例实践，对比在KinD环境下CODECO和Kubernetes在pod部署/删除时间、CPU/内存使用、Pod间数据速率等多个指标上的表现。

Result: 实验证明，CODECO在CPU消耗上更低、通信更稳定；但内存消耗增长10-15%，且因安全覆盖网络初始化导致pod生命周期略有延迟。

Conclusion: CODECO更适合资源受限、网络不稳定的移动机器人场景，能有效提升系统的资源和通信效率。在一定的内存和延迟代价下，CODECO可以作为Kubernetes在AMR上的替代或补充方案。

Abstract: Autonomous Mobile Robots (AMRs) increasingly adopt containerized micro-services across the Edge-Cloud continuum. While Kubernetes is the de-facto orchestrator for such systems, its assumptions of stable networks, homogeneous resources, and ample compute capacity do not fully hold in mobile, resource-constrained robotic environments.
  This paper describes a case study on smart-manufacturing AMRs and performs an initial comparison between CODECO orchestration and standard Kubernetes using a controlled KinD environment. Metrics include pod deployment and deletion times, CPU and memory usage, and inter-pod data rates. The observed results indicate that CODECO offers reduced CPU consumption and more stable communication patterns, at the cost of modest memory overhead (10-15%) and slightly increased pod lifecycle latency due to secure overlay initialization.

</details>


### [206] [Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm](https://arxiv.org/abs/2511.08377)
*Michael Bowman,Xiaoli Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种用于远程操作意图推断的新框架 Psychic，能够通过检测操作员运动轨迹中的突发跳变，结合连续和非连续动力学，有效提升意图推断的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的远程操作意图推断方法往往忽视了轨迹中的细微运动变化，这些细微变化可能强烈指示意图的突变。作者希望解决：能否检测操作员轨迹的突变、如何利用这种信息推断目标状态、以及如何融合连续和不连续动力学推断整体运动意图。

Method: 提出了一种Jump-Drift-Diffusion随机微分方程模拟操作员的突变及连续运动，结合Kramers-Moyal系数检测轨迹中的跳变，并通过统计异常检测确定目标转换。之后用Sparse Identification of Nonlinear Dynamics (SINDy)模型融合跳变和连续信息，结合目标转换作为控制输入推断操作员运动行为。

Result: Psychic框架能够有效地对操作员目标进行早期检测、发现未定义目标，并给出概率性可达性集合。在对600条远程操作轨迹的回顾性实验中，Psychic相比负对数似然模型有更好的表现，能在线下和线上学习环境中提高推断效果。

Conclusion: 通过融合轨迹中的突变与连续动态，Psychic实现了更精确且更具鲁棒性的意图推断，适合于无结构环境下的远程操作任务，并作为开源工具包上线，具备实际应用价值。

Abstract: Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.

</details>


### [207] [Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface](https://arxiv.org/abs/2511.08454)
*Tianyu Jia,Xingchen Yang,Ciaran McGeady,Yifeng Li,Jinzhi Lin,Kit San Ho,Feiyu Pan,Linhong Ji,Chong Li,Dario Farina*

Main category: cs.RO

TL;DR: 该论文提出了一种通过触觉编码的脑机接口（BCI），能够在不干扰自然动作的前提下，控制额外的运动自由度。


<details>
  <summary>Details</summary>
Motivation: 传统BCI对于扩展人体运动自由度时存在难以在不影响自然动作的情况下集成多自由度控制的问题，因此需要新的方法实现直观、可靠的超常控制。

Method: 作者设计了一种新颖的触觉诱发P300范式BCI，结合多日实验，分别在单任务和双任务环境下评估其控制超常自由度及对自然运动的影响，并在实际运动增强任务中进行了测试。

Result: 经过训练，该界面能够实时可靠解码四个超常自由度，3天训练效果有显著提升。在单/双任务条件下表现接近，对自然动作无负面影响，能支持两只机器臂协同自然双手完成任务。

Conclusion: 此研究提出的基于感觉输入的BCI范式可以扩展人体运动自由度而不损害自然动作，为运动增强和神经交互提供了新方向。

Abstract: Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.

</details>


### [208] [A Supervised Autonomous Resection and Retraction Framework for Transurethral Enucleation of the Prostatic Median Lobe](https://arxiv.org/abs/2511.08490)
*Mariana Smith,Tanner Watts,Susheela Sharma Stern,Brendan Burkhart,Hao Li,Alejandro O. Chara,Nithesh Kumar,James Ferguson,Ayberk Acar,Jesse F. d'Almeida,Lauren Branscombe,Lauren Shepard,Ahmed Ghazi,Ipek Oguz,Jie Ying Wu,Robert J. Webster,Axel Krieger,Alan Kuntz*

Main category: cs.RO

TL;DR: 本研究提出了一套结合基于模型的切除规划器和基于学习的牵引网络的半自动前列腺切除系统，实现了高效和精准的前列腺中叶切除。


<details>
  <summary>Details</summary>
Motivation: 前列腺疾病微创手术需要高度灵活的小型机器人工具以提高手术精度和安全性，但当前机器人自动化程度有限，需要研发能够自动规划和操作的机器人系统。

Method: 采用双臂经尿道同心管机器人（Virtuoso），其中切除规划器依据CT分割数据自动生成三阶段切除流程的器械轨迹；推力变分自编码牵引网络（PushCVAE）基于外科医生示范训练，根据手术阶段自动生成牵引动作。整个流程在仿真的水凝胶前列腺模型上以三级（受监督）半自动模式执行。

Result: 实验结果显示，该系统对中叶目标切除体积的切除率达97.1%。

Conclusion: 本工作验证了结合自动规划与学习型牵引的半自动机器人系统在经尿道手术的可行性，并为实现全自动微创前列腺切除奠定了基础。

Abstract: Concentric tube robots (CTRs) offer dexterous motion at millimeter scales, enabling minimally invasive procedures through natural orifices. This work presents a coordinated model-based resection planner and learning-based retraction network that work together to enable semi-autonomous tissue resection using a dual-arm transurethral concentric tube robot (the Virtuoso). The resection planner operates directly on segmented CT volumes of prostate phantoms, automatically generating tool trajectories for a three-phase median lobe resection workflow: left/median trough resection, right/median trough resection, and median blunt dissection. The retraction network, PushCVAE, trained on surgeon demonstrations, generates retractions according to the procedural phase. The procedure is executed under Level-3 (supervised) autonomy on a prostate phantom composed of hydrogel materials that replicate the mechanical and cutting properties of tissue. As a feasibility study, we demonstrate that our combined autonomous system achieves a 97.1% resection of the targeted volume of the median lobe. Our study establishes a foundation for image-guided autonomy in transurethral robotic surgery and represents a first step toward fully automated minimally-invasive prostate enucleation.

</details>


### [209] [Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1](https://arxiv.org/abs/2511.08502)
*Ruya Karagulle,Cristian-Ioan Vasile,Necmiye Ozay*

Main category: cs.RO

TL;DR: 该论文提出用加权信号时序逻辑（WSTL）从人类偏好、排序和示范中学习安全、最优且高效的自主系统行为，通过结构化剪枝和对数变换将复杂的学习问题转化为带有安全保障的混合整数线性规划，并在机器人导航和F1数据中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自主系统在安全关键领域需要结合人类反馈调整行为，但现有方法难以保证行为安全。因此，亟需一种既能学习人类偏好，又能有安全保证的高效方法。

Method: 提出基于WSTL的偏好学习方法，面对多线性约束，通过结构化剪枝和对数变换，将问题化简为混合整数线性规划，从而提升计算效率并保持安全性。

Result: 在机器人导航任务和F1真实世界数据集上实验表明，所提方法能够捕捉细致的人类偏好，有效建模复杂任务目标，并严格保证安全。

Conclusion: 该方法兼具安全性、最优性和高效性，能在需求高安全标准的自主系统中，可靠地从人类偏好和示范中学习行为策略。

Abstract: Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.

</details>


### [210] [SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment](https://arxiv.org/abs/2511.08583)
*Rong Xue,Jiageng Mao,Mingtong Zhang,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了SeFA（Selective Flow Alignment）框架，实现了高效且准确的视觉-运动策略学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习中，生成的动作容易与当前视觉观测对应的真实动作偏离，导致误差累积和任务执行不稳定。现有基于rectified flow的方法难以很好地解决这一核心挑战。

Method: SeFA通过一种选择性流对齐策略，利用专家演示 selectively 校正生成动作，保持与视觉观测的一致性，并引入一致性校正机制，在不影响一步推理效率的前提下，确保生成动作始终与观测对应。同时，该方法保留了动作多模态性。

Result: 在多个仿真与真实世界操作任务实验中，SeFA策略准确性和鲁棒性优于最新diffusion-based与flow-based基线方法，推理时延降低超过98%。

Conclusion: SeFA有效统一了高效rectified flow推理和与观测一致的动作生成，为实时视觉-运动策略学习提供了可扩展、可靠的解决方案。

Abstract: Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.

</details>
