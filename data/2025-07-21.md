<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: 本论文综述了无人机航拍领域中开词汇目标检测（OVOD）的最新进展，总结了现有方法、数据集、挑战以及未来前景。


<details>
  <summary>Details</summary>
Motivation: 传统的无人机航拍目标检测方法只针对预先定义好的类别，无法识别新类别，极大限制了应用范围。随着跨模态文本图像对齐技术如CLIP的发展，实现了通过自然语言识别新对象的开词汇检测，有望提升无人机场景理解的智能和自主性。

Method: 文章首先分析了开词汇目标检测（OVOD）与无人机视觉特性的结合，然后构建了针对航拍图像的OVOD方法的系统化分类体系，并全面整理了相关数据集。通过结构化综述方法，批判性分析了该领域的关键难点和未解问题。

Result: 系统归纳了现有航拍OVOD方法、数据集、面临的挑战，并对未来研究方向和实际应用前景进行了展望，总结形成了该领域的路线图和参考资料。

Conclusion: 本综述为无人机航拍开词汇目标检测领域的发展提供了清晰的研究脉络和宝贵的参考资料，有助于推动该领域的创新。

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [2] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种用于低光照图像增强的新型深度学习框架EDNIG，该方法在提升图像质量的同时保持了较低的模型复杂度，并在定量和可视效果上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的低光照图像增强方法在提升图像亮度和细节保留方面存在局限，且部分方法模型复杂度较高，难以应用于实际场景。因此，亟需一种既能有效增强低光照图像、又具有较低复杂度的方法。

Method: EDNIG基于U-Net架构，融合了由亮通道先验（BCP）推导的照明图作为引导，使网络关注欠曝区域，提升增强效果。同时，加入空间金字塔池化（SPP）模块提取多尺度特征，提高对不同光照条件的适应能力。此外，采用Swish激活函数优化训练中的梯度传播，并在GAN框架下结合对抗损失、像素损失和感知损失进行联合优化。

Result: 实验表明，EDNIG在多项定量指标和视觉质量上均优于主流方法，并且模型复杂度更低，更适合实际应用场景。

Conclusion: EDNIG框架不仅能有效提升低光照图像的质量，还兼顾了模型复杂度和实际应用需求，是低光照增强领域具有竞争力的新方法。

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [3] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: 尽管视觉语言模型（VLMs）在复杂视觉任务中表现出色，但在非局部视觉推理（需要跨越多区域的信息整合）的能力上却大幅落后于人类，主流模型在新测试中的表现接近随机水平。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在诸如视觉问答（VQA）、图表理解等复杂任务上取得显著进步，但有研究表明它们在简单的感知测试中仍然有短板。作者希望进一步探究并量化这些模型在更高级的视觉推理，尤其是涉及整合多个图像区域信息的“非局部视觉推理”上的实际能力。

Method: 作者设计了一套针对非局部视觉推理的评测框架，涵盖三种形式：1）比较感知：要求模型在工作记忆中同时保存并对比两幅图像；2）扫视式搜索：模型需通过离散跳跃查找目标，模拟人眼扫视行为；3）平滑搜索：要求沿连续轮廓平滑搜索。测试了市面主流旗舰VLMs如Gemini 2.5 Pro、Claude Vision 3.7、GPT-o4-mini等。

Result: 即使是主流/先进的VLMs，在这些非局部视觉推理测试中表现极差，在两类人类看来非常简单的任务上，模型准确率几乎与随机猜测持平。

Conclusion: 目前VLMs虽然在视觉感知分辨率上取得进步，但在人类常见的视觉推理任务——尤其是涉及多区域证据整合的任务上，缺乏核心推理能力，与人类还有显著差距。

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [4] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: 本研究评估了视觉-语言模型（VLMs）在空间推理任务中的表现，比较了不同的链式思维（CoT）提示策略以及通过强化学习提升空间推理能力。结果显示结构化多阶段提示和强化学习优化优于常规方法。


<details>
  <summary>Details</summary>
Motivation: 空间推理能力对于视觉-语言模型理解复杂场景至关重要。目前主流CoT提示方式效果有限，且模型对测试时语言表述变化表现不佳，亟需提升其泛化能力和鲁棒性。

Method: 首先测试各种CoT提示策略，发现简单的CoT反而降低性能。随后引入结合场景图的结构化多阶段CoT提示。进一步，采用Group Relative Policy Optimization（GRPO）进行强化学习微调，并与有监督微调（SFT）在不同数据集与条件下做对比评估。

Result: 结构化多阶段CoT提示显著提升空间推理准确率。GRPO在Pass@1准确率和分布外（OOD）测试中均优于SFT，展现更好的泛化与稳健性。SFT容易过拟合测试时的语言表述，导致表现下降，而GRPO能适应不同表述。

Conclusion: 结构化提示和强化学习微调有助于提升VLMs的空间推理能力及泛化能力，为进一步优化多模态智能体提供有效路径。

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [5] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: 本论文提出了一种无需3D人工标注、能够在开放词表环境下进行3D目标检测的新方法，利用2D视觉语言模型辅助实现3D检测。


<details>
  <summary>Details</summary>
Motivation: 传统3D目标检测数据集受限于类别数量和高昂的人工标注，难以适应开放世界的多样性需求。而2D视觉语言模型具备丰富的语义理解能力，可以通过自然语言实现开放词表检测，但这些能力尚未被充分用于3D任务。

Method: 本方法以2D视觉语言检测器产生基于文本的候选区域，并通过SAM工具分割，再利用相机几何和激光雷达或单目伪深度，将这些片段投影回3D空间。为推断3D包围盒，设计了基于DBSCAN聚类与旋转卡尺的几何扩充策略，整个流程无需额外训练和3D人工标注。为测试模型稳健性，还构建了带有雾效、仅用RGB的Pseudo-nuScenes数据集。

Result: 实验结果显示，无论采用激光雷达还是纯RGB-D输入，本方法在3D定位精度上都具有竞争力，且完全无需训练和人工3D标注，并支持开放词表。

Conclusion: 2D基础模型在大规模3D感知领域具备巨大的潜力，该方法为实现更高效、无监督和开放词表的3D目标检测开辟了新思路，同时已开源相关代码和资源。

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [6] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本论文提出了一个能够处理多达12种模态（如图像、文本、音频、点云等）的多模态多任务神经网络及其训练算法，并在25个数据集上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态、多任务深度学习方法大多局限于少量模态，无法同时兼容多种类型数据，且多模态协同训练效率和效果存在挑战，因此有必要设计统一、高效、可扩展的多模态多任务解决方案。

Method: 方法包括：为每种模态设计专用tokenizer，经统一的transformer共享架构和跨模态注意力机制，将不同模态数据投射到统一嵌入空间；而针对不同任务，在网络尾部设有模态/任务特定的输出头；提出新的预训练策略（采用迭代模态切换初始化网络）和一种新训练算法（部分采用多模态联合训练，部分采用两两模态训练间权衡）。

Result: 在12种模态的25个公开数据集上进行大规模评测，所提出的方法取得了新的业界最好表现（SOTA），证明了架构和训练策略的有效性。

Conclusion: 综合实验表明，该方法能灵活高效地处理多模态、多任务场景，网络架构和训练方案均能带来优越性能，为大规模多模态深度学习提供了新思路。

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [7] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端深度学习框架，将光学动作捕捉与Transformer模型结合，提升医疗康复效果，实现更高鲁棒性的数据重建与异常检测。


<details>
  <summary>Details</summary>
Motivation: 针对康复过程中由于遮挡和环境因素导致的动作捕捉数据噪声与缺失问题，以及异常动作难以及时发现的问题，传统方法难以满足远程康复与自动化需求。

Method: 设计了基于Transformer的时序建模框架，对光学动作捕捉数据进行去噪、补全，并进行实时异常检测；在中风与骨科康复数据集上进行了实验验证。

Result: 框架在数据重建（去噪和补全）与异常检测方面表现优异，优于现有方法。

Conclusion: 该方法为远程康复提供了具备优秀重建与实时异常检测能力的解决方案，具有成本效益高、易于扩展和减少线下监管的潜力。

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [8] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种结合视觉Transformer（ViT）和图神经网络（GNN）的创新框架，用于乳腺癌的早期检测，准确率达84.2%，优于传统方法，并提供可解释的注意力热力图辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因之一，而早期检测对于提高存活率至关重要。传统的检测方法效果有限，因此亟需更高效、精准、解释性强的自动检测技术。

Method: 采用视觉Transformer（ViT）提取图像全局特征，结合图神经网络（GNN）建模乳腺影像中结构性关系，并在CBIS-DDSM数据集上进行训练和评估。同时利用注意力热力图提升模型的可解释性，为临床医生提供决策依据。

Result: 该框架在CBIS-DDSM数据集上的准确率达到84.2%，明显优于传统检测方法，并通过注意力热力图展示模型的判别依据。

Conclusion: ViT与GNN的结合能够显著提升乳腺癌检测的准确率，并具备较强的可解释性，对临床乳腺癌检测及辅助诊断具有重要应用价值。

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [9] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: 本文针对自动驾驶场景提出了Butter，一种提升分层特征表达的目标检测框架，既提升了检测精度又兼顾了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有如YOLO和DETR类的主流目标检测算法，在处理多尺度特征一致性与检测精度、效率平衡时面临困难，特别是在动态环境下准确识别行人、车辆和交通标志等目标。

Method: 提出了两项核心创新：(1) 频率自适应特征一致性增强（FAFCE）组件，通过自适应频率滤波提升多尺度特征结构及边界精度；(2) 递进式分层特征融合网络（PHFFNet）模块，分层融合多级特征，缓解语义鸿沟并增强分层特征学习。

Result: 在BDD100K、KITTI和Cityscapes三大数据集上的实验表明，Butter在特征表达和目标检测精度上均优于现有方法，同时减少了模型复杂度。

Conclusion: Butter通过高效的分层特征细化与融合，为自动驾驶目标检测实现了准确性、实用性及计算效率的平衡，将推动相关领域技术进步。

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [10] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute是一种基于大语言模型的智能路由系统，能够动态选择用于多模态视频检索的最佳模态，实现了高效且成本较低的视频内容查询。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频检索往往依赖冗余的文本描述，处理成本高，并且容易遗漏如 OCR 场景文字等关键信息，尤其在自动语音识别（ASR）无法覆盖的情况下。该领域亟需一种节省算力资源又不牺牲检索效果的方法。

Method: 提出ModaRoute系统，借助GPT-4.1分析查询意图，智能选择ASR、OCR和视觉三种索引的组合，按需路由查询，平均每个查询只使用1.78种模态，而不是传统的全模态（3种）检索。

Result: ModaRoute将计算开销降低了41%，同时在Recall@5指标上达到60.9%；与仅依赖文本描述（75.9% Recall@5，但成本高，且34%视频存在OCR未被捕捉信息）相比取得更高性价比。

Conclusion: 智能路由系统ModaRoute能有效缩减多模态检索系统的算力和基础设施消耗，同时保证较强的效果，为实际大规模应用部署提供了可行解决方案。

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [11] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: 本文综述了工业缺陷检测领域，从传统的封闭集方法到近年来兴起的开放集方法，系统总结了2D与3D场景下的检测技术进展。


<details>
  <summary>Details</summary>
Motivation: 伴随制造业对高精度、自动化和规模化的更高要求，传统的检测方法面对实际工业应用显得不足。深度学习和计算机视觉的进步极大提升了缺陷检测能力，特别是开放集检测框架的兴起，解决了对大量缺陷标注依赖的问题。当前缺乏对该领域最新进展的系统梳理与总结，亟需全面回顾现有技术与未来趋势。

Method: 本文系统分析现有2D与3D下的闭集与开集缺陷检测技术发展，提炼实际应用中的关键挑战，梳理当前技术演变，重点突出开放集方法的崛起，并总结其在实际场景中的优势和不足。

Result: 对比各类检测框架后，总结了2D/3D闭集与开集方法近年来的研究进展，揭示了开放集检测在减少标注、识别新型缺陷中的独特价值和强大潜力，同时也剖析了现实挑战和不足。

Conclusion: 本文为工业缺陷检测提供了系统、前沿的综述和趋势展望，帮助研究者和工程师理解该领域的最新发展和未来方向。

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [12] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: 本文评估了在卫星影像机器学习任务中，融合多种地理空间数据（如气候、地形等）对模型性能的影响，发现多模态输入显著提升了模型效果。


<details>
  <summary>Details</summary>
Motivation: 目前大多数卫星影像机器学习模型仅依赖光学影像等单一输入，忽视了其它可获取的地理数据。论文旨在探究将多种数据（如传感器、地形等）与光学影像联合输入，对模型性能的影响及其实际价值。

Method: 作者在已有SatML基准任务（涵盖分类、回归和分割）上，扩充了新的地理数据层，形成多模态输入的数据集。然后对比多模态与单一光学影像输入下的模型表现，并考察不同数据融合策略的效果。

Result: 实验结果显示，额外地理输入能显著提升模型性能，尤其是在标注数据有限或跨地理区域泛化时提升更大。此外，手工设定的数据融合策略表现优于学习型融合方法。

Conclusion: 多模态输入可以有效提升SatML模型的数据利用率和泛化能力，特别适用于数据稀缺、分布转移等实际问题，同时简单融合方法表现优异，对未来研究有启示作用。

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [13] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 本文提出了一种新的最小化生成模型中不良概念的方法，能有效抹除特定内容而不影响整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型尽管能生成高质量图像，但依赖大量无标签数据，带来安全和版权风险。现有内容抹除方法存在过度修改，导致模型泛用性下降，亟需更精细、影响更小的抹除技术。

Method: 作者提出以生成结果分布距离为基础的最小化抹除目标，并推导出可微分优化损失，端到端联合回传。方法还引入了神经元mask以增强抹除稳健性，减少对模型微调依赖，并进行了理论和实证分析。

Result: 实验表明，该方法能在不损害模型整体表现的前提下，有效且稳健地抹除指定内容，并在流配对（flow-matching）等先进生成模型中获得良好效果。

Conclusion: 本文方法实现了低影响、高稳健性的模型概念抹除，有助于推动生成模型的安全与合规应用，为负责任的生成式AI提供基础。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [14] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于二值占用数据的3D语义占用预测方法，通过分解为二值和语义占用两个模块，实现成本更低的数据利用，同时提升了预测效果。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测依赖大量带有语义标签的LiDAR数据，这种数据获取成本高昂。相比之下，二值占用数据（仅区分占用/空闲），易于获得且成本低，但尚未被充分利用。作者希望探索二值数据的价值，以降低3D占用预测的门槛。

Method: 作者提出了一个新颖的二值占用驱动框架，通过将3D占用预测任务划分为二值占用和语义占用两个模块，分别处理二值和语义信息。同时，将大规模二值占用数据用于预训练和基于学习的自动标注，提高模型性能和数据利用效率。

Result: 实验结果显示，该框架在预训练和自动标注任务上，都优于以往方法。大规模二值数据的引入有效提升了3D语义占用预测的准确性。

Conclusion: 论文证明了廉价获取的二值占用数据在提升3D语义占用预测方面的潜力。新方法不仅降低了对高成本带标注LiDAR数据的依赖，也在多个任务上取得了最佳表现。

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [15] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的行人轨迹预测模型InSyn和一种新颖的训练策略SSOS，显著提升了在人群密集场景下的预测准确率，尤其降低了序列初步误差。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法主要基于相对位置建模行人间相互作用，忽视了如成对行走或冲突等特定交互模式，导致在拥挤环境中预测准确性有限。

Method: 1. 设计了Interaction-Synchronization Network（InSyn），利用Transformer结构，显式建模同步行走、冲突等多样化的交互模式，并有效捕捉方向敏感的社交行为。2. 提出了Seq-Start of Seq（SSOS）训练策略，针对时序预测中序列初步（初始步）误差较高的问题，优化模型训练过程。

Result: 在ETH和UCY数据集上的实验表明：提出的模型在各项指标上均明显优于最新基线方法，在高密度场景中优势更加突出。SSOS训练策略使初始步预测误差降低了约6.58%。

Conclusion: InSyn模型和SSOS训练策略不仅提升了行人轨迹预测准确性，特别适合复杂、动态的真实社会场景，并有望扩展至更多需要建模复杂交互时序数据的任务。

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [16] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出MADI框架，通过双重掩码和推理时能力扩展，显著提升扩散模型的可编辑性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在文本到图像生成领域表现优异，但在有条件视觉编辑和组合控制领域效果有限。作者受自监督学习和上下文生成建模的发展启发，试图解决这一限制。

Method: 1. 提出Masking-Augmented gaussian Diffusion（MAgD）训练策略，结合标准去噪和掩码重构，使模型学习到区分性和结构化视觉表达。2. 推理时引入Pause Tokens暂停标记，作为特殊占位符增强推理能力，并用更丰富的训练提示词进一步增强表现。

Result: 实验证明，MADI框架大幅提升了扩散模型在编辑性、组合性和可控性方面的表现，且训练时采用更丰富的提示词对MAgD特别有效。

Conclusion: MADI框架显著提高了扩散模型的可编辑性和泛化能力，为其成为通用上下文生成扩散架构奠定了基础。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [17] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文公开了一个包含多模态信号的驾驶员疲劳检测数据集，数据涵盖面部、行为和生物特征指标，支持疲劳状态连续建模。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是交通安全的重要隐患，现有检测数据多单一且标签离散，无法充分反映疲劳状态的渐变过程，影响研究及智能检测方法的发展。

Method: 收集3D面部深度视频、红外视频、后方视频、心率、皮肤电、血氧、皮肤温度、加速度、方向盘握力及驾驶模拟器行为数据。19位被试在清醒与疲劳两种状态下，每4分钟自评疲劳程度，单次采集持续40分钟，共1400分钟数据。

Result: 建立了一个时长充足、模态丰富、标签精细的驾驶员疲劳多模态公开数据集，覆盖了生理、行为、驾驶表现多方面信号及疲劳状态的连续演变过程。

Conclusion: 新数据集为疲劳检测研究提供了多源、真实且连续标注的数据支持，有望推进相关识别算法和实验验证的发展。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [18] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff是一种基于扩散模型的新方法，可直接从CT/MRI构建平滑且适用于CFD分析的主动脉3D模型，减少了人工和大量标注数据需求。


<details>
  <summary>Details</summary>
Motivation: 当前主动脉三维重建方法依赖大量标注数据和手动处理，且生成的模型在几何连续性和CFD（计算流体力学）兼容性方面存在缺陷，限制了其在临床和科研中的应用。

Method: 提出了AortaDiff框架：首先通过体积引导条件扩散模型（CDM）迭代生成以医学影像为条件的主动脉中心线，然后自动提取每个中心线点对应的血管轮廓，最后将所有轮廓拟合生成连续平滑的3D表面。这一流程无需大量标注数据，能自动化完成主动脉重建。

Result: AortaDiff在训练数据有限的条件下也能有效执行，包括正常和异常（如动脉瘤、缩窄）主动脉重建。实验表明其能生成高几何保真度、可用于CFD分析的3D网格。

Conclusion: AortaDiff能端到端自动生成高质量、CFD兼容的主动脉三维模型，对比传统方法更实用，适用于心血管领域研究及可视化。

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [19] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 本文提出了COREVQA基准，用以评估视觉-语言模型（VLMs）在视觉蕴含推理任务中的能力。该数据集包含5608对图像与真假陈述，图片主要来自拥挤场景。结果显示，主流VLMs在该任务上的准确率均低于80%，凸显了模型在此类推理任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前许多视觉-语言模型评测数据集主要关注视觉问答（VQA），但很少测试模型进行视觉蕴含推理（根据图片接受或否定某陈述）的能力。作者希望通过更具挑战性的测试，发现VLMs在拥挤场景中的推理弱点。

Method: 提出了COREVQA数据集，包括5608对图片和合成的真假陈述，其中图片均来自CrowdHuman数据集，主要为拥挤场景。模型被要求对每对图片和陈述判断其真假，以检验模型的视觉蕴含推理能力。

Result: 实验结果表明，即使是性能最好的VLMs，其准确率也未能超过80%；其余模型的准确率在39.98%到69.95%之间，大幅低于传统VQA任务的表现。

Conclusion: 视-语言模型在拥挤场景下，面对视觉蕴含推理任务仍有明显短板。当前数据集和评测方式不能全面反映模型在复杂推理任务中的真实能力，未来需发展更具有挑战性的基准与模型。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [20] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: 提出了一种新型的生成式AI图像水印方法IConMark，通过嵌入可解释语义信息，有效提升水印的鲁棒性与可读性，优于现有水印方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI与合成媒体的兴起，真假图像辨别变得关键。传统水印技术容易受对抗攻击失效，因此需要更鲁棒且可解释的水印方法。

Method: IConMark方法在生成AI图片时嵌入可解释概念，赋予图片有语义且易于人工辨识的水印，而非简单叠加噪声或扰动。同时，提出将IConMark与现有水印技术（StegaStamp与TrustMark）结合，形成混合方案IConMark+SS与IConMark+TM，进一步增强鲁棒性。

Result: 测试结果显示，IConMark及其变体在多个数据集上，水印检测的AUROC值分别比最佳基线高出10.8%、14.5%及15.9%，且图像质量得以保持，具有更强的抗攻击能力和可读性。

Conclusion: IConMark及其混合方案在保真度、鲁棒性和可读性方面均优于传统水印方法，为AI生成图像的可解释水印与真实性验证提供了新思路。

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [21] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模型深度学习的AI系统，用于肩部X光骨折检测，准确率高，可用于临床快速筛查。


<details>
  <summary>Details</summary>
Motivation: 肩部骨折在急诊及高负荷临床环境下常被漏诊，约有10%的病例被放射科医生遗漏，因此迫切需要自动化工具提升发现率，减少诊断延误。

Method: 作者构建了一个包含10,000张标注肩部X光片的数据集，采用Faster R-CNN（ResNet50-FPN、ResNeXt）、EfficientDet、RF-DETR等架构，通过Soft-NMS、WBF和NMW等集成技术在边界框与分类层面提升检测效果。

Result: NMW集成模型实现了95.5%的准确率和0.9610的F1分数，在所有关键指标上优于单一模型，并展现出较高的召回率和定位精度。

Conclusion: 集成式AI在肩部骨折自动检测中表现优异，具有临床应用价值，适合用于快速筛查和分诊；目前模型局限于二分类检测，未来可拓展至更详细的骨科分型。

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [22] [Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2507.13857)
*Max van den Hoven,Kishaan Jeeveswaran,Pieter Piscaer,Thijs Wensveen,Elahe Arani,Bahram Zonooz*

Main category: cs.CV

TL;DR: 本文提出了一种无需昂贵传感器或标注深度数据的单目3D车道线检测方法Depth3DLane，并在OpenLane基准集上取得了有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D车道检测方法依赖于多模态昂贵传感器或全监督深度网络，需要难以大规模收集的真实深度数据，同时还假设已知摄像头参数，限制了应用场景。

Method: 提出了Depth3DLane：利用自监督单目深度估计网络生成场景点云，通过鸟瞰视角通路提取空间结构信息，前视图通路提取语义信息，再通过3D车道锚点从两个通路采样特征，推断车道几何关系。此外，框架还可逐帧预测摄像头参数，并引入理论驱动拟合过程以提高每段车道的稳定性。

Result: 在OpenLane基准集上取得了有竞争力的表现。更重要的是，用学习得到的摄像头参数代替真实参数，能够使方法应用于无法进行相机标定的众包等场景。

Conclusion: Depth3DLane无需昂贵传感器和标注深度、无需已知摄像头参数，在3D车道检测上性能优良，拓展了单目3D车道检测的应用场景。

Abstract: Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.

</details>


### [23] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 本研究将古老的CORONA卫星灰度影像与现有深度学习模型相结合，用于自动识别在过去50年间已发生巨大变化的环境中考古遗址，并显著提升了检测精度和发现新遗址的能力。


<details>
  <summary>Details</summary>
Motivation: 由于中东地区考古遗址在过去半个世纪因人类活动大量被破坏或消失，传统方法已难以发现这些遗迹。作者希望借助老旧卫星影像和AI技术，提升遗址识别能力，发现消失或未被发现的遗迹，为遗产保护和研究提供新工具。

Method: 将基于Bing影像训练的卷积神经网络模型，使用1960年代的CORONA卫星影像对伊拉克巴格达西部Abu Ghraib区进行再训练。对比评估模型识别精度，并通过实地验证模型自动发现的新遗址。

Result: 模型的分割精度（IoU）超过85%，考古遗址识别准确率达到90%。同时，模型还发现了4处使用传统考古手段未能识别，但后经实地验证确为考古遗址的新地点。

Conclusion: 将历史卫星影像与AI深度学习结合，显著提高了消失遗址的自动识别能力，为研究和保护容易因人类活动而消失的考古遗迹提供了突破性工具，对考古和文化遗产研究意义重大。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [24] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的因果时空Transformer（CaSTFormer）模型，用于提高驾驶意图预测的准确性和解释性。通过结合独特的特征对齐、因果提取和特征融合机制，模型在公开数据集上达到了当前最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前人机协同驾驶系统在驾驶意图预测中面临两个主要难题：一是复杂的时空依赖关系难以建模，二是人类驾驶行为的变化不可预测，导致现有方法表现不足。作者希望通过更好地捕捉时空与因果关系，提高预测准确率和模型可解释性。

Method: 提出CaSTFormer模型，包括三大关键技术：（1）RSF机制精确对齐内部（驾驶员）与外部（环境）特征的时间信息；（2）CPE模块去除伪相关，挖掘真实因果关系；（3）FSN网络自适应融合净化后的特征，生成一致的时空推断结果。模型在Brain4Cars数据集上进行验证。

Result: CaSTFormer模型在Brain4Cars公开数据集上取得了最先进的性能，并能有效捕捉驾驶行为与环境的复杂时空因果关系，提升了驾驶意图预测的准确率和透明度。

Conclusion: 通过显式建模因果时空关系，CaSTFormer大幅提升了驾驶意图预测的准确性和可解释性，为实现更安全、高效的人机协同自动驾驶提供了重要技术基础。

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [25] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本文提出PhyWorldBench，一个用于评测视频生成模型物理规律遵循能力的综合基准，并测试了12种主流模型，发现目前模型在物理一致性上仍面临诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管当前视频生成模型在生成高质量、拟真视频方面取得进展，但它们在准确模拟物理现象方面仍存在重大挑战。为了系统评估和推动其物理一致性，作者提出了新的评测基准。

Method: 作者构建了PhyWorldBench基准，涵盖从基础物理规律（如物体运动、能量守恒）到复杂交互（刚体碰撞、人类/动物动作）等多个层次。同时引入“反物理”场景，通过故意违背物理的提示词测试模型能否遵从指令并保持逻辑自洽。除了大规模人工评测，还设计了基于多模态大模型（MLLM）的零样本物理评测方法。对12个主流文生视频模型进行系统测试、对比和分析。

Result: 发现当前主流模型在物理一致性上仍存在明显短板，特别是在处理复杂物理交互以及应对‘反物理’需求时表现不佳。通过对1,050个精心设计的提示词系统测试，揭示了模型真实物理规律遵循性的关键难点。

Conclusion: 提出的基准有效揭示和量化了文生视频模型在物理规律遵循方面的能力与不足，并对如何设计更好满足物理一致性的视频生成提示词提供了实践性建议，为后续模型优化指明了方向。

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [26] [Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation](https://arxiv.org/abs/2507.13486)
*Debao Huang,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文提出了一种面向摄影测量点云生成中不确定性量化的新框架，能更准确地为每个点提供置信度评价，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 摄影测量点云的精度受场景影响较大，不同于激光雷达，而目前针对多视图立体匹配(MVS)阶段的不确定性估计仍缺乏有效、标准化的方法。

Method: 提出在摄影测量两步（SfM+MVS）中，为每个点建立误差协方差矩阵，重点针对MVS阶段，利用可靠的多视点（n>=6）及MVS的匹配代价特征等，提出新型自校准回归方法进行不确定性估计；方法为自监督，无需外部标注。

Result: 在多个公开航拍和无人机影像数据集上验证，该方法能在高置信包络率下避免过度估计不确定性，优于已有方法。

Conclusion: 本文方法能够在多种场景下实现稳健、可认证的不确定性量化，为摄影测量点云质量控制提供有力支持，具有广泛适用性。

Abstract: Uncertainty quantification of the photogrammetry process is essential for
providing per-point accuracy credentials of the point clouds. Unlike airborne
LiDAR, which typically delivers consistent accuracy across various scenes, the
accuracy of photogrammetric point clouds is highly scene-dependent, since it
relies on algorithm-generated measurements (i.e., stereo or multi-view stereo).
Generally, errors of the photogrammetric point clouds propagate through a
two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),
followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM
stage has been well studied using the first-order statistics of the
reprojection error function, that in the MVS stage remains largely unsolved and
non-standardized, primarily due to its non-differentiable and multi-modal
nature (i.e., from pixel values to geometry). In this paper, we present an
uncertainty quantification framework closing this gap by associating an error
covariance matrix per point accounting for this two-step photogrammetry
process. Specifically, to estimate the uncertainty in the MVS stage, we propose
a novel, self-calibrating method by taking reliable n-view points (n>=6)
per-view to regress the disparity uncertainty using highly relevant cues (such
as matching cost values) from the MVS stage. Compared to existing approaches,
our method uses self-contained, reliable 3D points extracted directly from the
MVS process, with the benefit of being self-supervised and naturally adhering
to error propagation path of the photogrammetry process, thereby providing a
robust and certifiable uncertainty quantification across diverse scenes. We
evaluate the framework using a variety of publicly available airborne and UAV
imagery datasets. Results demonstrate that our method outperforms existing
approaches by achieving high bounding rates without overestimating uncertainty.

</details>


### [27] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: 本文提出了一种全无监督的3D卷积自编码器模型，用于从Sentinel-2卫星影像时序数据中自动检测甜菜田地的胁迫（非健康）状况。


<details>
  <summary>Details</summary>
Motivation: 传统农业胁迫检测依赖监督学习方法，需要大量人工标注数据，具有较低的泛化能力和高成本。本文旨在开发一种无需标签、可迁移性强的自动检测方法，以提升农田胁迫检测的效率和适用性。

Method: 采用3D卷积自编码器从Sentinel-2卫星图像序列中学习特征，并结合针对影像获取日期的专属时序编码，增强对甜菜生长动态的捕捉。最后将所学表示用于聚类任务，实现田块健康与胁迫的区分。

Result: 所提出方法可直接应用于不同年份的数据，在无监督条件下实现有效的甜菜胁迫检测，为农业遥感分析提供了实用工具。

Conclusion: 本方法实现了无需人工标注的甜菜田胁迫检测，具有很强的应用前景和迁移能力，为广大农业实际应用提供了便捷和高效的解决方案。

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [28] [SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM](https://arxiv.org/abs/2507.13527)
*Levi Harris,Md Jayed Hossain,Mufan Qiu,Ruichen Zhang,Pingchuan Ma,Tianlong Chen,Jiaqi Gu,Seth Ariel Tongay,Umberto Celano*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的SparseC-AFM方法，能够利用稀疏AFM扫描数据高效还原二维材料的电导率分布图，大幅提升数据获取速度，准确性高，向2D材料工业化检测迈出了重要一步。


<details>
  <summary>Details</summary>
Motivation: 二维材料在纳米电子学中的大量应用需要高效的电性能测试方法，但传统的高分辨率AFM（如C-AFM）检测速度慢，难以满足工业级检测需求。

Method: 作者提出并实现了一种SparseC-AFM深度学习模型，能够从稀疏C-AFM扫描数据重建高质量的导电图谱。方法在不同扫描模式、基底材料及实验条件下都表现出较强的适应性和准确性。作者对比了传统高像素密度C-AFM（数据采集耗时约15分钟）和SparseC-AFM（采集时间小于5分钟）的性能。

Result: SparseC-AFM可实现较传统方法超过11倍的数据获取加速。在MoS2等二维材料参数提取（如覆盖率、缺陷密度、晶岛边界及裂纹识别）任务中，SparseC-AFM与全分辨率数据的分析结果高度一致。

Conclusion: SparseC-AFM显著提升了二维材料电性表征的效率和自动化水平，为将AI辅助表征技术推广到工业级大规模生产提供了可行方案。

Abstract: The increasing use of two-dimensional (2D) materials in nanoelectronics
demands robust metrology techniques for electrical characterization, especially
for large-scale production. While atomic force microscopy (AFM) techniques like
conductive AFM (C-AFM) offer high accuracy, they suffer from slow data
acquisition speeds due to the raster scanning process. To address this, we
introduce SparseC-AFM, a deep learning model that rapidly and accurately
reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM
scans. Our approach is robust across various scanning modes, substrates, and
experimental conditions. We report a comparison between (a) classic flow
implementation, where a high pixel density C-AFM image (e.g., 15 minutes to
collect) is manually parsed to extract relevant material parameters, and (b)
our SparseC-AFM method, which achieves the same operation using data that
requires substantially less acquisition time (e.g., under 5 minutes).
SparseC-AFM enables efficient extraction of critical material parameters in
MoS$_2$, including film coverage, defect density, and identification of
crystalline island boundaries, edges, and cracks. We achieve over 11x reduction
in acquisition time compared to manual extraction from a full-resolution C-AFM
image. Moreover, we demonstrate that our model-predicted samples exhibit
remarkably similar electrical properties to full-resolution data gathered using
classic-flow scanning. This work represents a significant step toward
translating AI-assisted 2D material characterization from laboratory research
to industrial fabrication. Code and model weights are available at
github.com/UNITES-Lab/sparse-cafm.

</details>


### [29] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgärtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Weiß*

Main category: cs.CV

TL;DR: 本文提出了一种针对嵌入在三维空间中的有向三角网格上法向量的二阶总广义变差（TGV）的新型数学表述，并在网格去噪实验中对其有效性进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的TGV方法主要针对标量数据，对于流形值（如法向量）数据缺乏合适的高阶正则化手段。而在几何处理任务（如网格去噪）中，直接对法向量进行高阶正则化能够更好地保留几何结构，因此需要一个适用于流形值数据的TGV新表述。

Method: 作者将经典的Raviart-Thomas有限元空间扩展到切向量场（即切向Raviart-Thomas有限元空间），以适应流形（单位球面）值函数的特殊需求，建立了一种新的法向量TGV正则化模型。该方法在离散三角网格上实现，对比实验中与现有方法的效果进行了比较。

Result: 新提出的正则化器在三维三角网格的去噪任务中表现良好。据实验结果，与主流的去噪方法相比，本方法在噪声抑制和细节保留方面取得了更优的表现。

Conclusion: 针对三角网格上流形值（法向量）数据，提出的二阶TGV正则化模型结合了高阶正则化和流形几何的优点，在几何处理任务中具有实际应用价值，为后续更多流形值数据的正则化方法提供了思路和工具。

Abstract: We propose a novel formulation for the second-order total generalized
variation (TGV) of the normal vector on an oriented, triangular mesh embedded
in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued
function, taking values on the unit sphere. Our formulation extends previous
discrete TGV models for piecewise constant scalar data that utilize a
Raviart-Thomas function space. To exctend this formulation to the manifold
setting, a tailor-made tangential Raviart-Thomas type finite element space is
constructed in this work. The new regularizer is compared to existing methods
in mesh denoising experiments.

</details>


### [30] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA提出了一种自适应局部块级注意力机制，在减少视频生成计算量的同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的视频生成模型虽然表现优秀，但“全注意力”机制计算量随输入长度与分辨率成平方级增长，限制了其在高分辨率、长视频上的推广应用。

Method: 提出Neighborhood Adaptive Block-Level Attention（NABLA）机制，通过块状的注意力结构与自适应稀疏阈值策略，降低计算复杂度，并能够直接集成到PyTorch的Flex Attention中，无需特殊算子实现。

Result: NABLA在不显著损失CLIP分数、VBench分数和人工评估分数等生成质量指标的情况下，实现了最高2.7倍的训练与推理加速。

Conclusion: NABLA为高效视频生成模型提供了一种无需定制算子、易集成且高效的注意力机制方法，为实际应用中大规模、高分辨率视频生成带来新途径。

Abstract: Recent progress in transformer-based architectures has demonstrated
remarkable success in video generation tasks. However, the quadratic complexity
of full attention mechanisms remains a critical bottleneck, particularly for
high-resolution and long-duration video sequences. In this paper, we propose
NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that
dynamically adapts to sparsity patterns in video diffusion transformers (DiTs).
By leveraging block-wise attention with adaptive sparsity-driven threshold,
NABLA reduces computational overhead while preserving generative quality. Our
method does not require custom low-level operator design and can be seamlessly
integrated with PyTorch's Flex Attention operator. Experiments demonstrate that
NABLA achieves up to 2.7x faster training and inference compared to baseline
almost without compromising quantitative metrics (CLIP score, VBench score,
human evaluation score) and visual quality drop. The code and model weights are
available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [31] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: 本文提出用LoRA优化的合成回放方法提升视觉-语言模型的持续学习能力，通过在Stable Diffusion模型中注入低秩适配器，更好地适应新任务的领域特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成回放的持续学习方法利用Stable Diffusion生成样本，但生成器难以捕捉现实任务的细粒度语义和领域差异，导致使用这些样本回放时出现偏差，从而影响模型对先前知识的保留。

Method: 本方法在冻结的Stable Diffusion模型中插入任务专用的LoRA适配器，使其能够高效学习新任务的独特视觉和语义分布。通过两阶段基于置信度的样本筛选，先用代表性强的真实样本微调LoRA，再用置信度高的合成样本做知识蒸馏，将适配后的生成器集成到现有回放流程中。

Result: 在多领域任务增量学习（MTIL）基准上，本方法优于以往合成回放方案，兼顾模型的可塑性、稳定性和零样本能力。

Conclusion: 利用LoRA对生成器进行任务适配能有效提升VLM的持续学习稳健性，对实际领域适应性有很大帮助。

Abstract: Continual learning for vision-language models has achieved remarkable
performance through synthetic replay, where samples are generated using Stable
Diffusion to regularize during finetuning and retain knowledge. However,
real-world downstream applications often exhibit domain-specific nuances and
fine-grained semantics not captured by generators, causing synthetic-replay
methods to produce misaligned samples that misguide finetuning and undermine
retention of prior knowledge. In this work, we propose a LoRA-enhanced
synthetic-replay framework that injects task-specific low-rank adapters into a
frozen Stable Diffusion model, efficiently capturing each new task's unique
visual and semantic patterns. Specifically, we introduce a two-stage,
confidence-based sample selection: we first rank real task data by
post-finetuning VLM confidence to focus LoRA finetuning on the most
representative examples, then generate synthetic samples and again select them
by confidence for distillation. Our approach integrates seamlessly with
existing replay pipelines-simply swap in the adapted generator to boost replay
fidelity. Extensive experiments on the Multi-domain Task Incremental Learning
(MTIL) benchmark show that our method outperforms previous synthetic-replay
techniques, achieving an optimal balance among plasticity, stability, and
zero-shot capability. These results demonstrate the effectiveness of generator
adaptation via LoRA for robust continual learning in VLMs.

</details>


### [32] [NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision](https://arxiv.org/abs/2507.13595)
*Tengkai Wang,Weihao Li,Ruikai Cui,Shi Qiu,Nick Barnes*

Main category: cs.CV

TL;DR: 本文提出了一种新的神经隐式表面重建方法NoiseSDF2NoiseSDF，可以直接从带有大量噪声的点云中学习到干净的隐式表面（SDF表征），极大提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云的隐式表面重建在噪声干扰下表现不佳，尤其是使用低质量扫描设备采集的数据。因此，亟需一种方法可以从噪声点云中恢复出准确的隐式曲面。

Method: 方法借鉴了图像领域的Noise2Noise思想，将其推广到了3D神经场。具体地，通过最小化两个含噪声SDF之间的MSE损失，网络在无干净标签监督下实现隐式去噪和表面精细化。

Result: 在ShapeNet、ABC、Famous和Real等基准测试上，本方法能显著提升噪声点云的表面重建质量。

Conclusion: NoiseSDF2NoiseSDF为点云到隐式曲面的去噪重建提供了一种有效框架，即便原始数据噪声较大，也能得到更高质量的重建表面。

Abstract: Reconstructing accurate implicit surface representations from point clouds
remains a challenging task, particularly when data is captured using
low-quality scanning devices. These point clouds often contain substantial
noise, leading to inaccurate surface reconstructions. Inspired by the
Noise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel
method designed to extend this concept to 3D neural fields. Our approach
enables learning clean neural SDFs directly from noisy point clouds through
noisy supervision by minimizing the MSE loss between noisy SDF representations,
allowing the network to implicitly denoise and refine surface estimations. We
evaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the
ShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that
our framework significantly improves surface reconstruction quality from noisy
inputs.

</details>


### [33] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于扩散模型（DM）的无监督图像去模糊方法，通过学习空间变化的纹理先验，有效提升了真实世界图像去模糊的效果。


<details>
  <summary>Details</summary>
Motivation: 当前获取大规模真实模糊-清晰图像对十分困难和昂贵，用非配对数据学习盲图像去模糊更实际。但现有方法主要依赖对抗学习，无法很好地应对真实世界复杂多变的模糊类型。

Method: 作者提出了一种新颖的扩散模型（DM）框架，包含两个核心组件：1) 纹理先验编码器（TPE），引入记忆机制表征并监督纹理学习；2) 纹理传递变换层（TTformer），通过自适应滤波的多头自注意力机制高效去除空间变化模糊。同时，采用小波域对抗损失保留高频纹理细节。整个方法利用无配对数据训练。

Result: 在广泛的基准测试中，所提方法实现了优于现有最先进无监督去模糊方法的性能，恢复了更多纹理细节。

Conclusion: 本工作为无监督图像去模糊提供了有效的新思路，证明了纹理先验和扩散模型对真实复杂模糊恢复的优越性。

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is
difficult and expensive, learning blind image deblurring from unpaired data is
a more practical and promising solution. Unfortunately, dominant approaches
rely heavily on adversarial learning to bridge the gap from blurry domains to
sharp domains, ignoring the complex and unpredictable nature of real-world blur
patterns. In this paper, we propose a novel diffusion model (DM)-based
framework, dubbed \ours, for image deblurring by learning spatially varying
texture prior from unpaired data. In particular, \ours performs DM to generate
the prior knowledge that aids in recovering the textures of blurry images. To
implement this, we propose a Texture Prior Encoder (TPE) that introduces a
memory mechanism to represent the image textures and provides supervision for
DM training. To fully exploit the generated texture priors, we present the
Texture Transfer Transformer layer (TTformer), in which a novel
Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes
spatially varying blurring through adaptive filtering. Furthermore, we
implement a wavelet-based adversarial loss to preserve high-frequency texture
details. Extensive evaluations show that \ours provides a promising
unsupervised deblurring solution and outperforms SOTA methods in widely-used
benchmarks.

</details>


### [34] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本论文提出了一种结合扩散模型的新型Burst图像超分辨率方法，显著减少运行时间，同时保持高质量的超分辨率结果。


<details>
  <summary>Details</summary>
Motivation: 现有Burst超分辨率（SR）方法虽然能提升超分辨率效果，但多采用确定性训练，导致生成图像模糊、感知质量较差。因此，需要一种能产生更加清晰和高保真SR图像的方法。

Method: 作者基于扩散模型设计了新的Burst超分辨率算法，并引入高阶ODE的随机采样器与知识蒸馏的一步扩散法以提升模型效率。

Result: 所提出方法在实验中将运行时间降至基线方法的1.6%，而在图像失真和感知质量等SR性能指标上仍能保持相当水准。

Conclusion: 新方法不仅极大提升了Burst图像SR任务的推理速度，还能保持甚至提升视觉质量，为低分辨率Burst图像的高质量重建提供了有效解决方案。

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super
Resolution (SR) image compared to a single LR image, prior burst SR methods are
trained in a deterministic manner, which produces a blurry SR image. Since such
blurry images are perceptually degraded, we aim to reconstruct sharp and
high-fidelity SR images by a diffusion model. Our method improves the
efficiency of the diffusion model with a stochastic sampler with a high-order
ODE as well as one-step diffusion using knowledge distillation. Our
experimental results demonstrate that our method can reduce the runtime to 1.6
% of its baseline while maintaining the SR quality measured based on image
distortion and perceptual quality.

</details>


### [35] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: 提出了一种名为CoTasks的新框架，通过引入细粒度的链式推理任务，提升了视频大语言模型（VideoLLMs）在视频理解中的推理能力，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前VideoLLMs虽然进展迅速，但在基于对象的视频理解和链式推理方面仍显不足，现有模型多依赖于高层次视频-文本数据，对分步组合式推理缺乏支持，因此需要新的方法提升模型细粒度理解与推理能力。

Method: 提出CoTasks框架，将复杂视频问题分解为四类基础任务：帧定位、实体追踪、空间和时间关系提取，并将中间推理步骤以链式思考（Chain-of-Thought）形式融入输入，增强模型的对象级时空推理能力。

Result: 在NeXT-QA基准测试中，LLaVA-video-7B模型GPT-4评测得分提升3.3分，Qwen2.5-VL-3B提升17.4分，推理、时序和描述等子项获得大幅改进。

Conclusion: CoTasks作为结构化链式推理监督框架，能显著提升视频大语言模型的组合式视频推理能力。

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [36] [Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation](https://arxiv.org/abs/2507.13628)
*Masahiro Ogawa,Qi An,Atsushi Yamashita*

Main category: cs.CV

TL;DR: 本文提出了一种新方法FoELS，融合了光流和纹理信息，有效区分移动相机视角下的动态与静态物体，比传统依赖光流的方法更适用于结构复杂、存在相机运动的场景，在DAVIS 2016数据集和真实交通视频上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有分割动态和静态物体的方法主要依赖光流，但在复杂结构或带有相机运动的场景中难以准确检测移动物体，因此需要一种更鲁棒的新方法解决这一难题。

Method: 作者提出了FoELS方法：1) 从光流计算扩展焦点（FoE）；2) 利用FoE计算中的异常值生成初步运动概率；3) 该概率与基于分割的先验信息融合，得到最终的运动物体概率，实现对动态与静态物体的区分。

Result: FoELS在复杂结构场景、相机旋转、平行运动等具有挑战性的设置下表现良好，在DAVIS 2016基准和真实交通视频实验中取得了最优效果。

Conclusion: FoELS通过融合光流和纹理分割信息，有效克服了仅依赖光流的局限性，实现了更精确的动态物体检测，为三维重建、自动导航等应用带来了改进。

Abstract: Separating moving and static objects from a moving camera viewpoint is
essential for 3D reconstruction, autonomous navigation, and scene understanding
in robotics. Existing approaches often rely primarily on optical flow, which
struggles to detect moving objects in complex, structured scenes involving
camera motion. To address this limitation, we propose Focus of Expansion
Likelihood and Segmentation (FoELS), a method based on the core idea of
integrating both optical flow and texture information. FoELS computes the focus
of expansion (FoE) from optical flow and derives an initial motion likelihood
from the outliers of the FoE computation. This likelihood is then fused with a
segmentation-based prior to estimate the final moving probability. The method
effectively handles challenges including complex structured scenes, rotational
camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016
dataset and real-world traffic videos demonstrate its effectiveness and
state-of-the-art performance.

</details>


### [37] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本文提出EPSilon，一种高效的混合人形三维头像生成方法，通过创新的空点采样略过策略，大幅降低计算成本，实现更快的推理和训练速度，同时保持高质量生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的人形化身生成方法细节不足，因此兴起了与SMPL网格结合的混合表示方法。尽管混合模型提升了逼真度，但其基于SMPL蒙皮权重的变形在每个采样点上计算量巨大，导致推理速度极慢，需要解决空采样点对推理效率的负面影响。

Method: EPSilon通过提出两种空点跳过方式：（1）空射线省略（ERO）——剔除仅穿越空空间的射线；（2）空区间省略（EIO）——缩小射线上仅由衣物或网格占据的采样区间，从而提高采样效率。该方案实现了对关键区域的重点采样，同时支持单阶段NeRF架构，无需层次采样。

Result: EPSilon方法在保持生成质量的同时，将实际采样点数量降至原有方法的3.9%，推理速度提升20倍，训练收敛速度提升4倍。

Conclusion: EPSilon创新性地通过高效空点略过策略，显著提升了混合NeRF-SMPL人形头像生成的推理和训练效率，并对未来相关领域的高效三维建模具有借鉴意义。

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [38] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一个大规模RGB-Event行人重识别数据集EvReID，并提出了基于行人属性的对比学习方法TriPro-ReID，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的行人重识别方法由于缺乏大规模真实数据集，限制了其识别性能评估及泛化能力验证。作者为解决数据稀缺这一核心问题，推动该领域发展。

Method: 1) 构建大规模、多场景、涵盖多种光照和季节的RGB-Event数据集EvReID，包括118,988对图片和1200个行人身份；2) 系统性评价15种主流行人重识别算法；3) 提出以行人属性为引导的对比学习方法TriPro-ReID，联合利用RGB与事件流、多层语义特征强化特征学习。

Result: 在EvReID和MARS数据集上进行了充分实验，结果显示TriPro-ReID框架在多项指标上表现优异，充分验证方法有效性和数据集的实用价值。

Conclusion: 新提出的数据集和方法为事件相机领域的行人重识别提供了有力支持和基准，有望推动相关研究发展。相关数据及源代码将公开，为学术界提供资源和参考。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [39] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: PW-FNet是一种新颖的高效图像复原模型，基于Pyramid Wavelet-Fourier结构，兼具出色的复原质量和优越的计算效率，广泛适用于多种天气和场景下的图像增强。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的方法在图像复原领域取得了较大进展，但其高系统复杂度导致实际应用、实时处理受限，尤其是在真实场景部署时。因此，研究如何降低复杂度并提升效率成为当前的核心需求。

Method: 提出Pyramid Wavelet-Fourier Network (PW-FNet)。其核心方法包括：1）块间采用金字塔小波的多输入多输出结构，实现多尺度、多频带分解；2）块内运用傅里叶变换替代自注意力机制，降低计算量，同时保留全局建模能力。

Result: PW-FNet在图像去雨、去雨滴、超分辨率、去模糊、去雾、去雪、低光增强等任务上，均在复原质量和效率上超越了现有最优方法，显著减少了参数量、计算和推理时间。

Conclusion: PW-FNet不仅能显著提升图像复原的质量，还具备更优的实时处理能力，为实际部署提供有效方案，证明了Wavelet-Fourier处理在高效图像复原中的巨大潜力。

Abstract: Natural image quality is often degraded by adverse weather conditions,
significantly impairing the performance of downstream tasks. Image restoration
has emerged as a core solution to this challenge and has been widely discussed
in the literature. Although recent transformer-based approaches have made
remarkable progress in image restoration, their increasing system complexity
poses significant challenges for real-time processing, particularly in
real-world deployment scenarios. To this end, most existing methods attempt to
simplify the self-attention mechanism, such as by channel self-attention or
state space model. However, these methods primarily focus on network
architecture while neglecting the inherent characteristics of image restoration
itself. In this context, we explore a pyramid Wavelet-Fourier iterative
pipeline to demonstrate the potential of Wavelet-Fourier processing for image
restoration. Inspired by the above findings, we propose a novel and efficient
restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).
Specifically, PW-FNet features two key design principles: 1) at the inter-block
level, integrates a pyramid wavelet-based multi-input multi-output structure to
achieve multi-scale and multi-frequency bands decomposition; and 2) at the
intra-block level, incorporates Fourier transforms as an efficient alternative
to self-attention mechanisms, effectively reducing computational complexity
while preserving global modeling capability. Extensive experiments on tasks
such as image deraining, raindrop removal, image super-resolution, motion
deblurring, image dehazing, image desnowing and underwater/low-light
enhancement demonstrate that PW-FNet not only surpasses state-of-the-art
methods in restoration quality but also achieves superior efficiency, with
significantly reduced parameter size, computational cost and inference time.

</details>


### [40] [MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training](https://arxiv.org/abs/2507.13673)
*Yuechen Xie,Haobo Jiang,Jian Yang,Yigong Zhang,Jin Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种创新的基于Masked Autoencoder（MAE）的预训练框架MaskHOI，用于提升单目RGB图像中3D手-物体交互姿态估计的准确性，在多个基准上显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目RGB图像下的3D手-物体交互姿态估计由于图像的固有几何不确定性和交互过程中的严重遮挡问题而极具挑战性。现有方法难以恢复复杂的手部细节和应对遮挡。

Method: 提出MaskHOI框架，利用MAE的遮挡重建策略促进特征编码器学习几何感知和遮挡鲁棒的表征。具体包括区域特异性掩码分配（对手部区域分配较低掩码率以平衡特征学习难度）、骨骼引导的掩码指导（优先遮挡关键手部区域以模拟实际遮挡）、以及引入基于Masked SDF的多模态学习（通过自遮挡3D有符号距离场预测，使特征编码器具备3D全局结构感知能力）。

Result: 大量实验表明，所提出的方法在多个3D手-物体交互姿态估计基准上均显著优于当前主流方法。

Conclusion: MaskHOI能够有效提升手部复杂结构还原和遮挡鲁棒性，为单目图像下的3D手-物体交互建模提供了新的高效解决方案。

Abstract: In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of
hands and objects from monocular RGB input remains highly challenging due to
the inherent geometric ambiguity of RGB images and the severe mutual occlusions
that occur during interaction.To address these challenges, we propose MaskHOI,
a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI
pose estimation. Our core idea is to leverage the masking-then-reconstruction
strategy of MAE to encourage the feature encoder to infer missing spatial and
structural information, thereby facilitating geometric-aware and
occlusion-robust representation learning. Specifically, based on our
observation that human hands exhibit far greater geometric complexity than
rigid objects, conventional uniform masking fails to effectively guide the
reconstruction of fine-grained hand structures. To overcome this limitation, we
introduce a Region-specific Mask Ratio Allocation, primarily comprising the
region-specific masking assignment and the skeleton-driven hand masking
guidance. The former adaptively assigns lower masking ratios to hand regions
than to rigid objects, balancing their feature learning difficulty, while the
latter prioritizes masking critical hand parts (e.g., fingertips or entire
fingers) to realistically simulate occlusion patterns in real-world
interactions. Furthermore, to enhance the geometric awareness of the pretrained
encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven
multimodal learning mechanism. Through the self-masking 3D SDF prediction, the
learned encoder is able to perceive the global geometric structure of hands and
objects beyond the 2D image plane, overcoming the inherent limitations of
monocular input and alleviating self-occlusion issues. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches.

</details>


### [41] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 本文提出HeCoFuse框架，有效应对V2X协作感知中不同车辆与路侧设备传感器异构带来的特征融合与感知稳定性难题，在多个配置下实验均优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中车辆与基础设施因成本与布设不同传感器（如摄像头、激光雷达），导致协作感知系统面临特征融合困难及感知可靠性下降，急需统一高效的感知解决方案。

Method: 提出HeCoFuse统一框架，采用分层融合机制融合异构传感器（摄像头C、激光雷达L）特征，并结合通道与空间注意力以解决跨模态失配和表征质量不均问题。此外，自适应空间分辨率调整模块可权衡计算量与融合效果，引入协作学习策略动态调整融合类型来增强鲁棒性。

Result: 在TUMTraf-V2X真实数据集上，HeCoFuse在满配（LC+LC）下3D mAP达43.22%，较CoopDet3D基线提升1.17%，并在L+LC情景下达到更高的43.38%，在九种异构配置下3D mAP维持在21.74%~43.38%。该方法在CVPR2025 DriveX挑战赛中获得冠军。

Conclusion: HeCoFuse在异构传感器协作感知场景下展现出卓越的性能与鲁棒性，刷新了TUMTraf-V2X基准结果，为实际多车多感知融合部署提供了有效解决方案。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [42] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯核的视觉运动测量新方法，能够在无需繁琐参数调整的情况下实现高精度、子像素级结构运动测量。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动测量技术在子像素级精度下，要么精度不足，要么对参数设置依赖较大，导致应用受限。为满足结构健康监测对高精度低成本测量的需求，需要开发更简单易用且高精度的视觉测量方法。

Method: 提出了一种基于高斯核追踪的运动测量方法，通过追踪图像序列中高斯核的位置来提取运动量。引入了运动一致性和超分辨率约束，提升了测量的准确性和鲁棒性，避免对参数个性化调节的需求。

Result: 经过数值仿真和实验验证，该方法能在无需依据样本定制参数的前提下，持续获得高精度运动测量结果。

Conclusion: 新方法在无需大量手动调整参数的情况下，显著提高了视觉测量的准确性和应用可靠性，为大规模结构健康监测提供了有力工具。

Abstract: The growing demand for structural health monitoring has driven increasing
interest in high-precision motion measurement, as structural information
derived from extracted motions can effectively reflect the current condition of
the structure. Among various motion measurement techniques, vision-based
methods stand out due to their low cost, easy installation, and large-scale
measurement. However, when it comes to sub-pixel-level motion measurement,
current vision-based methods either lack sufficient accuracy or require
extensive manual parameter tuning (e.g., pyramid layers, target pixels, and
filter parameters) to reach good precision. To address this issue, we developed
a novel Gaussian kernel-based motion measurement method, which can extract the
motion between different frames via tracking the location of Gaussian kernels.
The motion consistency, which fits practical structural conditions, and a
super-resolution constraint, are introduced to increase accuracy and robustness
of our method. Numerical and experimental validations show that it can
consistently reach high accuracy without customized parameter setup for
different test samples.

</details>


### [43] [GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms](https://arxiv.org/abs/2507.13706)
*Ángel F. García-Fernández,Jinhao Gu,Lennart Svensson,Yuxuan Xia,Jan Krejčí,Oliver Kost,Ondřej Straka*

Main category: cs.CV

TL;DR: 本文提出了两种适用于多目标跟踪（MOT）算法性能评估的拟度量方法，分别用于比对目标集和轨迹集，并在仿真实验中对多种Bayesian MOT算法进行了评测。


<details>
  <summary>Details</summary>
Motivation: 现有评价MOT算法的GOSPA类度量在实际应用中存在如无法灵活区分漏检与误检惩罚、位置损失需对称等局限性，作者希望解决这些不足，提供更灵活、实用的评估工具。

Method: 基于GOSPA与T-GOSPA指标扩展，提出了两种新的拟度量。其一度量目标集间差异，其二度量轨迹集间差异，二者都可灵活设定漏检与误检的不同惩罚，并允许位置损失非对称；T-GOSPA拟度量还引入了轨迹切换惩罚。最后利用T-GOSPA拟度量对多种Bayesian MOT算法进行了仿真评估。

Result: 实验结果表明，提出的拟度量方法能有效评估不同Bayesian MOT算法的性能，展现出更细致、灵活的评测能力。

Conclusion: 新提出的拟度量在漏检、误检成本非对称和灵活惩罚设置等方面优于传统GOSPA类度量，可为MOT相关应用中的性能评估提供更有针对性和实用性的量化工具。

Abstract: This paper introduces two quasi-metrics for performance assessment of
multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an
extension of the generalised optimal subpattern assignment (GOSPA) metric and
measures the discrepancy between sets of objects. The other quasi-metric is an
extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy
between sets of trajectories. Similar to the GOSPA-based metrics, these
quasi-metrics include costs for localisation error for properly detected
objects, the number of false objects and the number of missed objects. The
T-GOSPA quasi-metric also includes a track switching cost. Differently from the
GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of
penalising missed and false objects with different costs, and the localisation
costs are not required to be symmetric. These properties can be useful in MOT
evaluation in certain applications. The performance of several Bayesian MOT
algorithms is assessed with the T-GOSPA quasi-metric via simulations.

</details>


### [44] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: 该论文提出了一种无需再训练的方法PoemTale Diffusion，针对诗歌文本到图像生成任务。在诗句往往充满复杂、抽象、多重含义的情境下，通过多阶段提示优化和一致性自注意力机制，提升模型对诗意的理解和表达效果。同时，作者还发布了P4I诗歌图像数据集，并通过人工及定量评价验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的扩散模型在处理通俗描述时效果好，但处理富有创意、抽象、诗意等复杂文本时表现欠佳。传统方法信息损失严重，难以准确展现诗歌中的多层含义。作者希望解决诗歌类文本转图像时信息丢失和理解不足的问题。

Method: 1. 提出PoemTale Diffusion，无需额外训练，通过引入一个多阶段的提示优化循环在大语言模型中反复处理诗歌文本，提高诗意文本的语义可解释性。2. 修改主流扩散模型的自注意力结构，采用一致性自注意力机制，从同一诗歌多次生成一致性图像，再集成这些图像更好地表达诗意。3. 构建并发布P4I数据集（1111首诗），由专家团队参与质性评价。

Result: 通过人类专家和定量评估，证明了PoemTale Diffusion方法能够更好地保留和展现诗歌文本转图片过程中的丰富信息和多层含义，图像生成的质量和诗意捕捉能力均优于现有方法。

Conclusion: 该方法无需额外训练，有效提升了诗歌文本到图像的生成质量，并开辟了诗歌文本处理的新方向。P4I数据集的发布也有助于推动该领域的研究。

Abstract: Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.

</details>


### [45] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本论文关注视觉问答（VQA）中，用户提问常有歧义，传统做法多通过重述问题处理，而忽视了与用户互动澄清的重要性。作者提出了ClearVQA基准，用于评估模型通过互动消除歧义的能力。


<details>
  <summary>Details</summary>
Motivation: 当前VQA研究大多关注让模型直接回答问题，对于用户提问时的歧义，仅靠重述问题来处理，忽略了用户与模型之间可以通过互动来消除歧义的真实需求。另外，缺乏评测模型在互动澄清歧义方面能力的基准数据集。

Method: 作者提出了ClearVQA基准，涵盖三类常见歧义和多样化的VQA场景，用于系统地评测视觉语言模型在互动澄清歧义方面的能力。并指出现有VLM更倾向于回答问题而非主动提问、澄清歧义。

Result: 引入了ClearVQA基准，为未来研究提供了标准化测试平台，填补了该领域评测的空白。

Conclusion: 通过提出新的基准数据集，激励模型提升与用户互动澄清歧义的能力，为后续视觉-语言模型交互性研究奠定基础。

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [46] [Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction](https://arxiv.org/abs/2507.13719)
*Daniele Pannone,Alessia Castronovo,Maurizio Mancini,Gian Luca Foresti,Claudio Piciarelli,Rossana Gabrieli,Muhammad Yasir Bilal,Danilo Avola*

Main category: cs.CV

TL;DR: 本文提出了一套创新的增强现实（AR）流程，专为博物馆环境设计，实现艺术品识别及单张图片的高质量3D建模。


<details>
  <summary>Details</summary>
Motivation: 博物馆希望以沉浸式AR体验提升访客互动，但艺术品复杂的纹理和轮廓使3D重建难度增大，亟需高效精准的建模方法。

Method: 结合两种深度估计模型（GLPN负责全局结构，Depth-Anything侧重局部细节），输出优化深度图，再生成点云和网格，最终用于增强现实展示；流程中融入最先进神经网络和计算机视觉技术。

Result: 实验显示系统在3D重建精度和视觉真实感方面显著优于现有方法，成功应对艺术品多样形状与纹理的挑战。

Conclusion: 该方案为博物馆提供了一种高鲁棒性和沉浸感的AR数字展示工具，有效提升参观者体验，具有较大应用前景。

Abstract: This paper presents an innovative augmented reality pipeline tailored for
museum environments, aimed at recognizing artworks and generating accurate 3D
models from single images. By integrating two complementary pre-trained depth
estimation models, i.e., GLPN for capturing global scene structure and
Depth-Anything for detailed local reconstruction, the proposed approach
produces optimized depth maps that effectively represent complex artistic
features. These maps are then converted into high-quality point clouds and
meshes, enabling the creation of immersive AR experiences. The methodology
leverages state-of-the-art neural network architectures and advanced computer
vision techniques to overcome challenges posed by irregular contours and
variable textures in artworks. Experimental results demonstrate significant
improvements in reconstruction accuracy and visual realism, making the system a
highly robust tool for museums seeking to enhance visitor engagement through
interactive digital content.

</details>


### [47] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: 本文分析了StyleGAN生成器的内部机制，探索其架构、训练方式和潜在向量对生成图像的影响，并对未来AI生成图像技术的风险与挑战提出了警示。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像（如虚假人脸）的威胁日益增长，学术界与社会关注如何理解和监管这类技术。本文旨在深入剖析StyleGAN的生成机制，理解其内部参数与输出特征的关系，为后续防范技术滥用提供理论基础。

Method: 研究基于PyTorch训练StyleGAN模型，详细考察了生成器的主要架构元素（例如均衡化学习率），并通过权重剪枝实验评估模型在压缩后的表现。此外，深入探讨了潜在向量的全局与局部调整如何改变生成脸部的具体特征。

Result: 实验发现，StyleGAN部分权重被剪枝后，对生成图像影响不大，能有效减少计算资源消耗。潜在向量整体变化影响图像色调，单个维度微调则可精准控制面部特征，这种灵活可控性揭示了StyleGAN强大的特征编辑能力。

Conclusion: 研究不仅展示了StyleGAN内部机制的可解析性和生成能力，也凸显了其被恶意利用生成假身份的伦理和社会风险。未来需警惕此类AI工具被用于数字欺诈和网络犯罪，相关监管和检测手段亟需发展。

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [48] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: 本文提出了一个自动化、高保真图像编辑三元组挖掘流水线，用于生成高质量训练数据，推动基于自然语言指令的图像编辑助手的发展，并开源了大规模NHR-Edit数据集和Bagel-NHR-Edit模型，实验上优于所有公开对比。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言指令的图像编辑系统，需要依赖大量高质量的训练三元组（原图-指令-编辑图），但人工采集难度很高，因为每次编辑需精准符合指令、风格一致且美观。缺乏高效的自动评估标准，阻碍了自动化数据采集的规模化。研究目的是通过自动化手段大幅降低数据采集和标注的人工负担，推动该领域发展。

Method: 作者设计了一个自动、模块化的流水线，综合利用公开生成模型和专门定制的Gemini验证器，无需人工参与、分割、或地面真实模型。流水线可自动评估编辑图片与指令的契合度及美学质量；同时通过反演与组合自举扩充了数据规模约2.2倍。最终生成高保真的三元组数据，用于训练和评测。

Result: 1）提出的流水线实现完全自动化，显著扩大（2.2倍）质量可靠的三元组数量；2）开源NHR-Edit数据集（358k组），在最大规模跨数据集评测中，全面超越所有公开数据集；3）发布Bagel-NHR-Edit模型，实验指标达到最新水平。

Conclusion: 本文自动化管道有效降低了数据采集和标注难度，提高了数据质量和规模，使得大模型训练更普惠、低门槛。开放的数据集和模型资源推动了自然语言图像编辑领域的研究进步。

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [49] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Diffusion-FSCIL的新方法，利用大规模预训练的文本到图像扩散模型作为冻结主干网络，有效提升了极少样本增量学习（FSCIL）的表现。


<details>
  <summary>Details</summary>
Motivation: FSCIL任务因训练样本极少而难以兼顾减缓灾难性遗忘和学习新知识，现有方法往往在保持旧类知识和适应新类时存在瓶颈。作者希望利用大规模生成模型的泛化能力和多尺度表示解决FSCIL难题。

Method: 方法采用冻结的文本到图像扩散模型提取多尺度、多样的扩散特征，作为潜在重放以提升表示能力，并辅以特征蒸馏减少生成偏差，整个框架仅需极少可训练参数实现高效学习。

Result: 在CUB-200、miniImageNet和CIFAR-100等主流多类增量数据集上，Diffusion-FSCIL优于以往方法，该方法能更好地保护旧类别表现并高效适应新类别。

Conclusion: 利用大规模扩散生成模型及多尺度特征，FSCIL可以通过冻结主干和高效特征提取实现更优的增量学习效果，为极少样本增量学习任务提供了新思路。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [50] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练的Encapsulated Video Synthesizer (EVS)方法，通过结合文本到图像（T2I）和文本到视频（T2V）模型，有效提升了生成视频的成像质量与动作流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型虽然能根据文本生成视频，但难以在高画质和良好动作表现间取得平衡，且许多方法借用T2I模型时会引入帧间不一致的问题（如闪烁和伪影）。因此，急需一种兼顾帧画质与运动连贯性的新方法。

Method: EVS方法将T2I和T2V模型以新颖方式组合：利用强大的扩散式T2I模型对低质量视频帧（视为分布外样本）进行噪声-去噪优化，提升视觉保真度；同时通过T2V模型保证动作一致性，通过将T2V的时间先验“封装”并植入T2I生成流程，兼得两者优点。

Result: 实验表明，EVS在视频成像质量和运动流畅性方面优于现有方法，并大幅提升推理速度（加速1.6-4.5倍）。

Conclusion: EVS方法有效结合T2I与T2V模型，突破了以往在视频生成过程中的质量与运动表现困局，实现了高质高效的视频合成。

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [51] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的光谱先验（SDP）与注入模块（SPIM），改善了深度学习方法在高光谱图像重建中的细节恢复能力。通过实验验证,方法在主流模型上优于现有方案约0.5dB。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以准确重建高光谱图像中的高频细节，影响重建质量，亟需有效改进。

Method: 设计并通过扩散模型学习光谱扩散先验（SDP），并提出光谱先验注入模块（SPIM），将该先验动态融入HSI重建网络，用以增强细节重建能力。

Result: 在MST和BISRNet两种典型HSI方法上进行实验，结果显示新方法提升了0.5dB，重建效果更佳。

Conclusion: 引入扩散模型学习的光谱先验及其注入模块，有效提升了HSI重建任务中的细节恢复和整体性能。

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [52] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: 本文提出了一种结合排列熵（PE）、方向梯度直方图（HOG）和局部二值模式（LBP）的特征工程方法，实现了无需深度学习结构的高效图像分类。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在图像分类中表现优异，但其解释性差且计算资源消耗大。因此，重拾传统可解释且高效的特征工程方法对于资源受限或需高解释性的场景尤为重要。

Method: 作者将本应用于时间序列分析的排列熵扩展到二维图像，通过多尺度、多方向计算（包括行、列、对角线、反对角线及局部块），提取空间复杂性与秩序特征。为增强区分能力，进一步融合了HOG和LBP特征，最终构造780维手工特征，利用SVM与网格搜索进行分类。

Result: 该方法在Fashion-MNIST、KMNIST、EMNIST和CIFAR-10等数据集上进行了实验，表现出与深度模型媲美或有竞争力的分类性能。

Conclusion: 融合熵特征与传统图像描述子的手工特征方法在可解释性、计算效率以及性能上均表现良好，展示了熵类特征在图像分类任务中的潜力，并为可解释的轻量级图像分类提供了新思路。

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [53] [SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering](https://arxiv.org/abs/2507.13779)
*Durgesh Singh,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 本文提出在半监督学习（SSL）和无监督领域自适应（UDA）中，显式引入可微聚类模块，利用有标签数据增强聚类效果，从而提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSL和UDA方法多通过隐式方法强制聚类假设（数据同簇同类）。但很少有方法显式利用可微聚类机制，并结合有标签数据以增强监督信号。

Method: 创新在于设计一个可微的聚类模块，同时将有标签数据的类别信息融入聚类中心的计算。该模块可与主模型端到端联合训练，用于SSL和UDA场景。既可单独用作主干模型，也可作为正则项辅助其他方法。

Result: 大量实验表明，该方法不论在较少标记样本情况下，或配合现有方法作为正则化器时，均能显著提升SSL与UDA的表现。

Conclusion: 显式整合可微聚类和标签信息，为SSL和UDA提供了简单高效的框架，对低监督场景尤其有效。

Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)
enhance the model performance by exploiting information from labeled and
unlabeled data. The clustering assumption has proven advantageous for learning
with limited supervision and states that data points belonging to the same
cluster in a high-dimensional space should be assigned to the same category.
Recent works have utilized different training mechanisms to implicitly enforce
this assumption for the SSL and UDA. In this work, we take a different approach
by explicitly involving a differentiable clustering module which is extended to
leverage the supervised data to compute its centroids. We demonstrate the
effectiveness of our straightforward end-to-end training strategy for SSL and
UDA over extensive experiments and highlight its benefits, especially in low
supervision regimes, both as a standalone model and as a regularizer for
existing approaches.

</details>


### [54] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: 本文提出了一种结合几何先验与深度神经算子的新方法LoFNO，能从磁共振流场影像中提升时空分辨率并直接预测血管壁切应力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑动脉瘤破裂的预测和治疗依赖于准确的血流动力学分析，但现实中的磁共振流成像受限于分辨率低和信噪比差，影响诊断精度。因此急需方法在保证易操作的前提下大幅提升影像数据的时空分辨率和诊断相关指标预测能力。

Method: 作者提出Localized Fourier Neural Operator（LoFNO）架构，利用拉普拉斯特征向量作为几何先验，增强对复杂血管形态的建模能力，并结合EDSR超分辨率网络实现稳健的上采样。方法能够对磁共振采集到的低分辨流场数据去噪并提升时空分辨率，同时直接预测壁切应力。最终与插值及其它深度学习方法对比。

Result: LoFNO能有效提升流速和壁切应力的预测精度，无论是空间恢复还是时间恢复都优于传统插值方法和先进深度学习模型。对不规则与未见过的血管形态也表现出良好泛化能力。

Conclusion: LoFNO为从临床影像数据提取高质量血流动力学信息提供了新途径，有望提升脑血管相关疾病（尤其是动脉瘤）的诊断精度与治疗决策。

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [55] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的盲人脸修复方法DynFaceRestore，通过动态选择扩散采样起始步长和区域引导强度，在细节与结构保真间实现平衡，达到了领先的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有使用预训练扩散模型进行人脸修复的方法，由于使用固定采样步长和全局引导尺度，导致无法很好地应对未知且非均匀的退化，容易造成细节缺失或伪影，引发保真度与质量的失衡。

Method: DynFaceRestore首先将任意退化的人脸映射为高斯模糊图像，并估计对应的高斯核。然后针对每个模糊图像动态选择扩散采样的起始步长，通过闭式引导在扩散过程中保持保真。同时，提出动态引导强度调节器，对区域引导强度进行自适应调整，有助于细节丰富区域和结构轮廓的均衡增强。

Result: DynFaceRestore在多项定量和定性评估中均取得了当前最优的修复效果，证明方法在应对复杂和未知退化方面具有良好鲁棒性和有效性。

Conclusion: 通过动态调整扩散过程起点和引导强度，DynFaceRestore有效解决了人脸修复中细节与整体保真之间的矛盾，实现了高质量、鲁棒的盲人脸修复。

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial
images from unknown degraded inputs, presenting significant challenges in
preserving both identity and detail. Pre-trained diffusion models have been
increasingly used as image priors to generate fine details. Still, existing
methods often use fixed diffusion sampling timesteps and a global guidance
scale, assuming uniform degradation. This limitation and potentially imperfect
degradation kernel estimation frequently lead to under- or over-diffusion,
resulting in an imbalance between fidelity and quality. We propose
DynFaceRestore, a novel blind face restoration approach that learns to map any
blindly degraded input to Gaussian blurry images. By leveraging these blurry
images and their respective Gaussian kernels, we dynamically select the
starting timesteps for each blurry image and apply closed-form guidance during
the diffusion sampling process to maintain fidelity. Additionally, we introduce
a dynamic guidance scaling adjuster that modulates the guidance strength across
local regions, enhancing detail generation in complex areas while preserving
structural fidelity in contours. This strategy effectively balances the
trade-off between fidelity and quality. DynFaceRestore achieves
state-of-the-art performance in both quantitative and qualitative evaluations,
demonstrating robustness and effectiveness in blind face restoration.

</details>


### [56] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: 提出了一种用于单目3D语义场景补全的新方法，通过预测伪未来帧显著提升感知范围，尤其适合自动驾驶场景，获得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现实自动驾驶场景中，单摄像头由于视场受限和遮挡，难以获得完整的3D场景信息，而现有方法对此处理不足。

Method: 提出CF-SSC框架，利用时序信息和伪未来帧预测，通过结合相机位姿与深度信息建立精确3D对应，几何一致地融合过去、现在及预测未来帧，实现空间-时序关系建模。同时，避免了传统简单堆叠特征的方法。

Result: 在SemanticKITTI和SSCBench-KITTI-360这两个基准数据集上实验，取得了行业领先的3D场景补全性能，尤其在遮挡推理方面有突出表现。

Conclusion: 所提方法能显著提升单摄像头对复杂交通环境3D语义场景的完整理解能力，为自动驾驶感知奠定了更坚实基础。

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [57] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态融合方法GRAM-MAMBA，可高效、鲁棒地实现IoT感知，特别适用于资源受限环境以及传感器数据缺失的场景。


<details>
  <summary>Details</summary>
Motivation: 现有物联网多模态感知系统在部署于资源有限环境下模型复杂度高，对单一模态的对齐方法忽略多模态关系，且对传感器数据缺失不够鲁棒，导致在实际应用中效率和稳定性均受限。

Method: 1) 利用高效的Mamba模型处理传感器时序数据，降低计算复杂度；2) 通过优化的GRAM矩阵策略进行模态间成对对齐，加强多模态关系建模；3) 借鉴LoRA思想，提出自适应低秩层补偿策略以应对模态缺失，冻结模型主体，只微调相关模态和融合层，有效适配缺失模态场景。

Result: 在SPAWC2021室内定位数据集上，预训练模型误差低于主流基线，处理模态缺失时，通过微调不到0.2%参数提升24.5%结果。在USC-HAD人体动作识别数据集上获得F1 93.55%、整体准确率93.81%，均领先现有方法；更新策略下微调参数不到0.3%时F1提升23%。

Conclusion: GRAM-MAMBA能够在资源受限和模态缺失环境下有效提升多模态感知的效率和鲁棒性，适合IoT实际部署，具有良好的应用前景。

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely
deployed in smart homes, intelligent transport, industrial automation, and
healthcare. However, existing systems often face challenges: high model
complexity hinders deployment in resource-constrained environments,
unidirectional modal alignment neglects inter-modal relationships, and
robustness suffers when sensor data is missing. These issues impede efficient
and robust multimodal perception in real-world IoT settings. To overcome these
limitations, we propose GRAM-MAMBA. This framework utilizes the
linear-complexity Mamba model for efficient sensor time-series processing,
combined with an optimized GRAM matrix strategy for pairwise alignment among
modalities, addressing the shortcomings of traditional single-modality
alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive
low-rank layer compensation strategy to handle missing modalities
post-training. This strategy freezes the pre-trained model core and irrelevant
adaptive layers, fine-tuning only those related to available modalities and the
fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On
the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower
error than baselines; adapting to missing modalities yields a 24.5% performance
boost by training less than 0.2% of parameters. On the USC-HAD human activity
recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),
outperforming prior work; the update strategy increases F1 by 23% while
training less than 0.3% of parameters. These results highlight GRAM-MAMBA's
potential for achieving efficient and robust multimodal perception in
resource-constrained environments.

</details>


### [58] [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/abs/2507.13812)
*Yingying Zhang,Lixiang Ru,Kang Wu,Lei Yu,Lei Liang,Yansheng Li,Jingdong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种统一的多模态遥感基础模型SkySense V2，使用单一transformer骨干网络处理多种遥感数据模态，并通过定制的自监督学习策略进行预训练，显著提升了多任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态遥感模型通常需要为每种数据模态训练独立的骨干网络，增加了冗余并降低了参数利用率；此外，主流的自监督预训练方法未考虑遥感图像的特殊语义特征，导致性能受限。

Method: 提出使用单一transformer骨干网络统一处理多模态遥感数据，设计了创新的自适应patch合并模块和可学习的模态提示token，以适应不同分辨率及特征多样性不足的问题，并引入专家混合（MoE）模块提升模型性能。预训练采用针对遥感数据特点设计的自监督学习策略。

Result: 在16个数据集、7项任务上进行了广泛评测，SkySense V2相较于之前的SkySense，平均提升1.8分，展现出较强的泛化能力。

Conclusion: SkySense V2通过统一骨干网络和针对遥感数据特点设计的预训练方法，有效提升了多模态遥感模型的效率和表现，为遥感领域多任务处理提供了新思路。

Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly
advanced various Earth observation tasks, such as urban planning, environmental
monitoring, and natural disaster management. However, most existing approaches
generally require the training of separate backbone networks for each data
modality, leading to redundancy and inefficient parameter utilization.
Moreover, prevalent pre-training methods typically apply self-supervised
learning (SSL) techniques from natural images without adequately accommodating
the characteristics of remote sensing (RS) images, such as the complicated
semantic distribution within a single RS image. In this work, we present
SkySense V2, a unified MM-RSFM that employs a single transformer backbone to
handle multiple modalities. This backbone is pre-trained with a novel SSL
strategy tailored to the distinct traits of RS data. In particular, SkySense V2
incorporates an innovative adaptive patch merging module and learnable modality
prompt tokens to address challenges related to varying resolutions and limited
feature diversity across modalities. In additional, we incorporate the mixture
of experts (MoE) module to further enhance the performance of the foundation
model. SkySense V2 demonstrates impressive generalization abilities through an
extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense
by an average of 1.8 points.

</details>


### [59] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视频问答框架，通过多模型协作和结构化思维链整合，显著提升在复杂现实场景下的推理深度与鲁棒性，在CVRR-ES数据集上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频多模态模型在复杂场景推理时存在上下文理解弱、时序建模有限、对模糊或组合性问题泛化能力差等问题。为了解决这些不足，作者希望增强模型在真实世界复杂视频问答场景中的推理能力与鲁棒性。

Method: 作者提出了一种“提示-回应”集成机制，调用多个异构视频-语言模型，以结构化的思维链处理不同推理路径，并通过一个外部大型语言模型进行评选与融合，选取最可靠的回答作为最终输出。该方法无需重新训练原有模型，具有轻量级和可扩展性。

Result: 大量实验证明，该方法在所有评估指标上均明显优于现有基线方法，尤其是在泛化能力和鲁棒性方面表现突出。

Conclusion: 本文提出的方法为多模态推理领域提供了一套轻量且易扩展的新方案，不依赖模型再训练，为未来视频多模态大模型的发展奠定了坚实基础。

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [60] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 本文提出在城市建筑物分割任务中，将量子卷积（Quanvolutional）预处理方法与Attention U-Net相结合，有效提升了分割效果并大幅减少模型参数量。


<details>
  <summary>Details</summary>
Motivation: 在城市规划、灾后响应和人口制图等领域，精确分割城市建筑物至关重要，然而由于高分辨率遥感图像体量大、结构复杂，常规方法面临性能与效率双重挑战。

Method: 本研究以突尼斯城市为例，采用Sentinel-1 SAR雷达影像，利用量子卷积（Quanvolution）提取更具判别力的特征，并将其作为Attention U-Net分割模型的输入，以增强模型对建筑物细节的捕捉能力。

Result: 初步实验表明，Quanvolution + Attention U-Net方法在保持与传统Attention U-Net相当分割精度的同时，大幅减少了网络参数量，计算效率显著提升。

Conclusion: 该研究验证了量子辅助深度学习方法在提升大规模城市遥感分割效率与精度方面的潜力，为相关应用推广提供了新思路。

Abstract: Building segmentation in urban areas is essential in fields such as urban
planning, disaster response, and population mapping. Yet accurately segmenting
buildings in dense urban regions presents challenges due to the large size and
high resolution of satellite images. This study investigates the use of a
Quanvolutional pre-processing to enhance the capability of the Attention U-Net
model in the building segmentation. Specifically, this paper focuses on the
urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)
imagery. In this work, Quanvolution was used to extract more informative
feature maps that capture essential structural details in radar imagery,
proving beneficial for accurate building segmentation. Preliminary results
indicate that proposed methodology achieves comparable test accuracy to the
standard Attention U-Net model while significantly reducing network parameters.
This result aligns with findings from previous works, confirming that
Quanvolution not only maintains model accuracy but also increases computational
efficiency. These promising outcomes highlight the potential of
quantum-assisted Deep Learning frameworks for large-scale building segmentation
in urban environments.

</details>


### [61] [PositionIC: Unified Position and Identity Consistency for Image Customization](https://arxiv.org/abs/2507.13861)
*Junjie Hu,Tianyang Han,Kai Ma,Jialin Gao,Hao Dou,Song Yang,Xianhua He,Jianhui Zhang,Junfeng Luo,Xiaoming Wei,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为PositionIC的新框架，实现了多主体图像定制中位置与身份的高一致性和精确空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有图片定制方法虽在保真度上有较大进展，但对于精细的实体级空间控制仍然不足，阻碍了实际应用。其根本原因是缺乏将身份与精确位置数据绑定的大规模数据集。

Method: 提出PositionIC统一框架，通过一个可扩展的合成流程，采用双向生成范式，避免主体漂移并保持语义一致性。同时设计了轻量级位置调制层，实现不同主体空间嵌入的解耦，从而达到独立、精确定位的目标并保持视觉质量。

Result: 大量实验证明，该方法能实现精准的空间控制，并在图像定制任务中保持高度一致性和保真度。

Conclusion: PositionIC为多实体、开放世界场景下可控且高保真的图像定制开辟新途径，并将公开以推动后续研究。

Abstract: Recent subject-driven image customization has achieved significant
advancements in fidelity, yet fine-grained entity-level spatial control remains
elusive, hindering the broader real-world application. This limitation is
mainly attributed to scalable datasets that bind identity with precise
positional cues are absent. To this end, we introduce PositionIC, a unified
framework that enforces position and identity consistency for multi-subject
customization. We construct a scalable synthesis pipeline that employs a
bidirectional generation paradigm to eliminate subject drift and maintain
semantic coherence. On top of these data, we design a lightweight positional
modulation layer that decouples spatial embeddings among subjects, enabling
independent, accurate placement while preserving visual fidelity. Extensive
experiments demonstrate that our approach can achieve precise spatial control
while maintaining high consistency in image customization task. PositionIC
paves the way for controllable, high-fidelity image customization in
open-world, multi-entity scenarios and will be released to foster further
research.

</details>


### [62] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 本文分析了视觉-语言模型（VLMs）在其内在知识与外部信息冲突时的决策机制，并提出了揭示这种冲突新机制的数据集与方法。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs在多样化知识需求的复杂任务中的普及，它们频繁遇到模型自身知识与外界信息冲突的情况。这种冲突会导致模型幻想与不可靠回答，但相关内在机制尚不清楚。作者希望探索VLMs如何解决这类跨模态知识冲突。

Method: 作者构建了一个数据集，内容为有意与VLM内在常识相矛盾的多模态反事实查询。通过logit检测定位模型中少量主导冲突的Attention head，并实验性地修改这些head以影响模型输出偏向内在知识或视觉输入。还将该方法与梯度归因法做了对比分析。

Result: 通过logit检测，作者能发现和控制主导冲突的Attention head。对这些head的定向操作可以有效地把模型的输出导向内在知识或视觉信息。此外，相关Attention head对图像区域的关注能力比基于梯度的方法更精确地定位了导致视觉主导输出的图像区域。

Conclusion: VLM的跨模态冲突决策主要受少量Attention head的控制，对这些head可实现模型输出指导，并能提升视觉归因精度。这为理解和控制VLM知识冲突机制提供了新思路。

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [63] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 本文提出一种融合实时视觉数据和海图信息的方法，通过将海图数据叠加在视频流上，并用Transformer网络实现浮标的精确匹配和定位，提升了海洋视觉系统的定位和关联准确率。


<details>
  <summary>Details</summary>
Motivation: 当前海洋视觉感知仅依赖视觉或地图信息，存在定位不准或对象难关联的问题，因此需要结合二者提升在复杂海洋环境下的感知能力。

Method: 提出一种端到端的Transformer神经网络，输入图像后预测浮标的位置边界框和置信度，将图像中的浮标检测结果直接与海图中的浮标坐标匹配。此外，还与基于射线投射和带距离预测的YOLOv7方法进行了对比实验。

Result: 在真实海洋场景数据集上，本文方法在对象定位和跨域关联两方面的准确率显著优于对比基线方法。

Conclusion: 将Transformer与海图数据融合的方法有效提升了海洋环境中的导航物体感知能力，对动态、复杂环境下的增强现实导航和自动驾驶船舶有较大应用价值。

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [64] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本论文提出PCR-GS，一种无需COLMAP辅助的3D高斯溅射（3D-GS）技术，通过相机位姿协同正则化实现更好的三维重建效果，尤其在复杂相机轨迹下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有COLMAP-free的3D-GS方法在重建过程中往往难以应对伴随激烈旋转和平移的复杂相机轨迹，导致相机位姿估计效果差，进而影响三维重建质量。解决复杂场景下的相机位姿估计及三维建模问题，成为提升3D-GS实用性的关键。

Method: 作者提出PCR-GS技术，从两个角度对相机位姿进行正则化：（1）特征重投影正则化——利用DINO提取不同视角间鲁棒的特征，并对其语义信息对齐以提升位姿估计精度；（2）基于小波的频域正则化——利用高频细节差异进一步优化相机旋转矩阵。

Result: 在多个真实场景的实验中，PCR-GS在相机轨迹剧烈变化的情况下也能显著提升三维重建质量和位姿估计的准确性，相较现有无COLMAP方法取得更优表现。

Conclusion: PCR-GS方法有效缓解了现有无COLMAP 3D-GS方法在应对复杂相机运动时的局限性，实现了更高质量的三维建模和更精确的相机位姿估计，对无监督三维重建领域具有重要推动作用。

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [65] [Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection](https://arxiv.org/abs/2507.13899)
*Yujian Mo,Yan Wu,Junqiao Zhao,Jijun Wang,Yinghao Hu,Jun Yan*

Main category: cs.CV

TL;DR: 本文提出结合视觉基础模型DepthAnything预测的深度先验与原始LiDAR点云属性，通过新颖的特征提取和融合模块，有效提升了基于LiDAR的3D目标检测精度。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云自身的表达能力有限，特别是其反射率属性区分度弱，导致3D目标检测存在性能瓶颈。视觉基础模型可从单目RGB图像中提供稠密且可靠的几何先验，因此探索如何利用这些先验来增强LiDAR点云特征，提升检测能力具有实际意义。

Method: 首先利用DepthAnything模型生成的深度先验，融合到LiDAR点云的原始特征中。然后设计了点级特征提取模块用于增强点特征表达。整体特征经过Dual-Path RoI提取框架处理：包含基于体素的全局语义通路与基于点的细粒度结构通路。双向门控RoI特征融合模块用于平衡融合全局与局部目标特征。

Result: 在KITTI基准上大量实验表明，该方法能持续提升3D目标检测的准确率。

Conclusion: 结合视觉基础模型先验能够有效补充和提升LiDAR点云表达，推动基于LiDAR的3D目标检测进一步发展。

Abstract: Recent advances in foundation models have opened up new possibilities for
enhancing 3D perception. In particular, DepthAnything offers dense and reliable
geometric priors from monocular RGB images, which can complement sparse LiDAR
data in autonomous driving scenarios. However, such priors remain underutilized
in LiDAR-based 3D object detection. In this paper, we address the limited
expressiveness of raw LiDAR point features, especially the weak discriminative
capability of the reflectance attribute, by introducing depth priors predicted
by DepthAnything. These priors are fused with the original LiDAR attributes to
enrich each point's representation. To leverage the enhanced point features, we
propose a point-wise feature extraction module. Then, a Dual-Path RoI feature
extraction framework is employed, comprising a voxel-based branch for global
semantic context and a point-based branch for fine-grained structural details.
To effectively integrate the complementary RoI features, we introduce a
bidirectional gated RoI feature fusion module that balances global and local
cues. Extensive experiments on the KITTI benchmark show that our method
consistently improves detection accuracy, demonstrating the value of
incorporating visual foundation model priors into LiDAR-based 3D object
detection.

</details>


### [66] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF是一种可泛化的神经渲染方法，可以在任意时间与视角合成新的场景视图，支持少量输入视图，且无需针对每一场景单独优化。


<details>
  <summary>Details</summary>
Motivation: 当前神经辐射场（NeRF）虽能合成新颖视角，但在建模随时间变化的3D场景（如昼夜变换）方面研究有限，且缺乏专用数据集。此外，现实中收集多视角数据代价高昂，每次新场景都需重新训练效率低下。

Method: 结合多视角立体重建、神经辐射场及特征解耦，将不同数据集中的时序和空间特征分离，构建能在任意时间生成内容隐式辐射场表示的方法，并通过体积渲染合成指定时间的新颖视图。方法支持few-shot泛化，无需每个场景单独优化。

Result: 实验表明，TimeNeRF能在少样本下实现新颖视点合成，无需每个场景重新优化。特别地，能真实还原不同时间下场景平滑过渡（如从黎明到黄昏的自然变化），且新合成视图具备较强的真实性。

Conclusion: TimeNeRF有效地拓展了NeRF在时序三维场景建模的应用范围，实现泛化与高效生成，为虚拟世界中的沉浸式环境构建、昼夜等自然变化模拟等相关应用提供了有力方法支撑。

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [67] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: 本文提出了DiViD，一种基于视频扩散模型的视频静态-动态因子无监督解耦方法，比现有方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE和GAN的视频解耦方法存在信息泄漏和重建模糊等问题。如何有效地无监督分离视频的静态外观和动态动作仍是难点。

Method: DiViD引入端到端视频扩散框架，使用序列编码器将首帧全局静态信息和每帧动态信息分离，采用条件DDPM解码器，包含三大归纳偏置：共享噪声时程保障时间一致性、时变KL瓶颈（前期收紧静态、后期丰富动态）、跨注意力确保静态信息全局、动态信息局部并加正交正则防止泄漏。

Result: 在真实数据集上，DiViD用交换准确度和交叉泄漏等指标全面优于现有序列解耦方法，提升静态保真和动态迁移能力，降低静态-动态泄漏。

Conclusion: DiViD实现了更为明确和有效的无监督视频静态-动态因子分离，可有效提升生成视频的质量和因子解耦能力，代表了当前该领域的先进水平。

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


### [68] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: 本文提出了一种通用的预测框架，通过在冻结的视觉模型特征空间中进行预测，实现了多类型任务下对未来的有效推断。研究发现视觉模型的感知能力与时序预测性能密切相关，对未来理解具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在实际场景下，智能系统需要具备对未来的预测能力，以便更好地规划和决策。然而，不同任务与抽象层次下的预测整合较为困难，缺乏统一和高效的通用方法。本文旨在探究视觉模型表征与通用预测能力之间的关系，并提出新的方法提升其预测性能。

Method: 作者提出了一套通用预测框架：在多种冻结视觉模型（backbone）基础上，利用潜空间扩散模型(latent diffusion models)对未来特征进行预测，并通过丰富的任务特定解读器进行解码。此外，为了适应多任务评估，设计了新的分布式评价指标，并在九个模型、四类任务中进行了广泛实验。

Result: 实验结果表明，视觉模型的感知（表征）能力与其在多任务短时预测上的表现密切正相关。该方法在不同抽象层级（如像素、深度、轨迹、物体运动）下均取得良好预测效果，展现了方法的通用性与优越性。

Conclusion: 结合强感知表征和生成式建模的方法对于时序视频理解和预测具有显著优势。该工作为未来发展更泛化的时序推理与预测系统奠定了基础。

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [69] [Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset](https://arxiv.org/abs/2507.13981)
*Sara Abdulaziz,Giacomo D'Amicantonio,Egor Bondarev*

Main category: cs.CV

TL;DR: 本文提出了一个用于评估视觉隐私保护方法的全面框架，并介绍了公开的人体中心数据集HR-VISPR。该框架可以在隐私、效用和实用性三维度下对11种隐私保护技术进行客观评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的监控系统的发展，个人敏感数据的隐私问题引发关注。现有研究缺乏系统性、可衡量的隐私保护评估方法，亟需建立一个客观的评价标准来指导隐私保护设计。

Method: 作者提出一个三维（隐私、效用、实用性）评价框架，并发布了包含生物和非生物特征标签的HR-VISPR数据集，用于训练可解释的隐私度量；系统性地评估了11种传统到深度学习的隐私保护方法。

Result: 该框架能够细致区分不同隐私保护方法在隐私、效用和实用性上的表现，与人类视觉感知一致，揭示了保护隐私与实用性的权衡。

Conclusion: 本文框架和HR-VISPR数据集为视觉隐私保护的客观评价提供了工具，促进了跨场景应用，并推动了隐私保护方法的标准化发展。

Abstract: Recent advances in AI-powered surveillance have intensified concerns over the
collection and processing of sensitive personal data. In response, research has
increasingly focused on privacy-by-design solutions, raising the need for
objective techniques to evaluate privacy protection. This paper presents a
comprehensive framework for evaluating visual privacy-protection methods across
three dimensions: privacy, utility, and practicality. In addition, it
introduces HR-VISPR, a publicly available human-centric dataset with biometric,
soft-biometric, and non-biometric labels to train an interpretable privacy
metric. We evaluate 11 privacy protection methods, ranging from conventional
techniques to advanced deep-learning methods, through the proposed framework.
The framework differentiates privacy levels in alignment with human visual
perception, while highlighting trade-offs between privacy, utility, and
practicality. This study, along with the HR-VISPR dataset, serves as an
insightful tool and offers a structured evaluation framework applicable across
diverse contexts.

</details>


### [70] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CSD-VAR，实现了单张图片中的内容与风格的高效分离，并通过新数据集验证了其在内容保持和风格迁移上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 当前内容-风格分离(content-style decomposition, CSD)方法多基于扩散模型，且现有方法在分离效果及适用范围上有局限。而视觉自回归建模(VAR)作为一种新兴的生成范式，其多尺度预测特性有望提升内容与风格的分离质量，因此作者希望借助VAR探索更优的内容与风格解耦方法并提升生成灵活性。

Method: 作者提出CSD-VAR方法，核心包括三点创新：(1) 引入尺度感知的交替优化策略，将内容与风格的表征分别对齐到不同尺度以强化分离效果；(2) 采用基于奇异值分解(SVD)的方法校正，减少内容信息泄漏到风格表征中；(3) 构建增强型Key-Value记忆机制以提升内容信息的保持能力。同时，作者还构建了CSD-100数据集，用于更好地评测CSD任务。

Result: 实验表明，CSD-VAR在内容保持和风格迁移的效果上均优于现有方法，在提出的CSD-100数据集和已有评测任务上实现了更高的性能。

Conclusion: CSD-VAR方法有效提升了单张图片内容与风格的分离质量，也证明VAR在CSD任务中具有优势，为提升生成模型的创意表达能力提供了新途径。

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [71] [DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation](https://arxiv.org/abs/2507.13985)
*Haoran Li,Yuli Tian,Kun Lan,Yong Liao,Lin Wang,Pan Hui,Peng Yuan Zhou*

Main category: cs.CV

TL;DR: DreamScene提出了一套能从文本或对话自动生成高质量、可编辑3D场景的方法。该系统包含场景规划、结构化布局、物体几何生成与优化、全局一致性相机采样，以及细致编辑功能。其实验显示在质量、一致性与灵活性均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景的方法难以自动化生成、保持3D一致性和细粒度控制，限制了游戏、影视、设计等应用场景的发展。因此，亟需一种高质量且支持编辑的自动化3D场景生成方案。

Method: DreamScene框架包括：1）用GPT-4对输入文本进行场景语义与空间约束解析，构建混合图；2）图结构下实现无碰撞布局；3）基于布局通过多步采样与重建优化生成3D物体几何；4）采用渐进式相机采样以确保全球一致性，适配多种场景；5）支持物体移动、外观变化及四维动态等精细编辑能力。

Result: 实验证明DreamScene在场景质量、一致性和灵活性方面均优于现有主流方法，并能适用于开放域内容创建。论文还提供了代码和演示。

Conclusion: DreamScene为文本驱动的3D场景自动生成与编辑提供了高效、灵活和易用的新范式，有效推动了开放域3D内容生成与应用。

Abstract: Generating 3D scenes from natural language holds great promise for
applications in gaming, film, and design. However, existing methods struggle
with automation, 3D consistency, and fine-grained control. We present
DreamScene, an end-to-end framework for high-quality and editable 3D scene
generation from text or dialogue. DreamScene begins with a scene planning
module, where a GPT-4 agent infers object semantics and spatial constraints to
construct a hybrid graph. A graph-based placement algorithm then produces a
structured, collision-free layout. Based on this layout, Formation Pattern
Sampling (FPS) generates object geometry using multi-timestep sampling and
reconstructive optimization, enabling fast and realistic synthesis. To ensure
global consistent, DreamScene employs a progressive camera sampling strategy
tailored to both indoor and outdoor settings. Finally, the system supports
fine-grained scene editing, including object movement, appearance changes, and
4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior
methods in quality, consistency, and flexibility, offering a practical solution
for open-domain 3D content creation. Code and demos are available at
https://dreamscene-project.github.io.

</details>


### [72] [Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations](https://arxiv.org/abs/2507.14010)
*Yong Feng,Xiaolei Zhang,Shijin Feng,Yong Zhao,Yihan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的两步法，实现隧道裂缝的高效分类和分割，并通过实验验证其在准确率和效率上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 隧道裂缝是隧道结构安全的重要指标，自动、高效且准确地检测和分割裂缝对于隧道健康监测和养护至关重要。现有方法在精准性和效率方面存在不足，因此亟需更优的解决方案。

Method: 方法分为两步：第一步利用DenseNet-169对隧道图像进行裂缝分类，第二步用DeepLabV3+对筛选出的含裂缝图像进行分割。并结合视觉解释技术对模型内部逻辑进行可解释性分析。

Result: 分类模型的准确率达到92.23%，处理速度39.80FPS，均优于其它CNN和Transformer模型；分割模型IoU为57.01%，F1分数为67.44%，也超过了主流方法。

Conclusion: 该两步深度学习方法显著提升了隧道裂缝检测的准确率和效率，同时通过可视化解释增强了模型可解释性，为隧道健康状况的快速、定量化评估提供了技术基础。

Abstract: Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming
to classify and segment tunnel cracks with enhanced accuracy and efficiency,
this study proposes a two-step deep learning-based method. An automatic tunnel
image classification model is developed using the DenseNet-169 in the first
step. The proposed crack segmentation model in the second step is based on the
DeepLabV3+, whose internal logic is evaluated via a score-weighted visual
explanation technique. Proposed method combines tunnel image classification and
segmentation together, so that the selected images containing cracks from the
first step are segmented in the second step to improve the detection accuracy
and efficiency. The superior performances of the two-step method are validated
by experiments. The results show that the accuracy and frames per second (FPS)
of the tunnel crack classification model are 92.23% and 39.80, respectively,
which are higher than other convolutional neural networks (CNN) based and
Transformer based models. Also, the intersection over union (IoU) and F1 score
of the tunnel crack segmentation model are 57.01% and 67.44%, respectively,
outperforming other state-of-the-art models. Moreover, the provided visual
explanations in this study are conducive to understanding the "black box" of
deep learning-based models. The developed two-stage deep learning-based method
integrating visual explanations provides a basis for fast and accurate
quantitative assessment of tunnel health status.

</details>


### [73] [Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model](https://arxiv.org/abs/2507.14013)
*Ji-Yan Wu,Zheng Yong Poh,Anoop C. Patil,Bongsoo Park,Giovanni Volpe,Daisuke Urano*

Main category: cs.CV

TL;DR: 该研究提出了一种结合多光谱成像与改进YOLOv5模型的深度学习方法，实现了植物叶片养分缺乏症状的高精度分割，检测效果显著优于传统YOLOv5。


<details>
  <summary>Details</summary>
Motivation: 在精准农业中，及时准确发现植物叶片的养分缺乏对施肥、病害与胁迫管理至关重要，然而传统方法对细微、分布广泛的病征检测效果有限。

Method: 设计了一种基于多光谱九通道输入的YOLOv5改进模型，集成transformer注意力头和自注意力机制，可更好捕捉空间分布细致的养分缺乏症状。通过在控制养分胁迫条件下生长的植物实验对模型进行评估，并与标准YOLOv5进行对比。

Result: 改进模型在Dice分数和IoU（交并比）上相比传统YOLOv5提升约12%，对检测如叶片黄化（chlorosis）和色素积累等复杂症状表现出色。

Conclusion: 多光谱成像与光谱-空间特征联合学习在植物表型与精准农业领域具有广阔应用前景，本方法为提高叶片异常检测准确性提供了技术支持。

Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for
precision agriculture, enabling early intervention in fertilization, disease,
and stress management. This study presents a deep learning framework for leaf
anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model
with a transformer-based attention head. The model is tailored for processing
nine-channel multispectral input and uses self-attention mechanisms to better
capture subtle, spatially-distributed symptoms. The plants in the experiments
were grown under controlled nutrient stress conditions for evaluation. We carry
out extensive experiments to benchmark the proposed model against the baseline
YOLOv5. Extensive experiments show that the proposed model significantly
outperforms the baseline YOLOv5, with an average Dice score and IoU
(Intersection over Union) improvement of about 12%. In particular, this model
is effective in detecting challenging symptoms like chlorosis and pigment
accumulation. These results highlight the promise of combining multi-spectral
imaging with spectral-spatial feature learning for advancing plant phenotyping
and precision agriculture.

</details>


### [74] [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](https://arxiv.org/abs/2507.14024)
*Jiarong Ye,Sharon X. Huang*

Main category: cs.CV

TL;DR: 本文提出了一套情感驱动的图像编辑系统，包括大规模情感标注数据集、视觉-语言模型和无训练编辑框架，实现了更精准的情感操控且保持内容完整性，在多个创意领域具备应用潜力。


<details>
  <summary>Details</summary>
Motivation: 情感与视觉内容的关联在创意产业有巨大潜力，但由于情感抽象且多样，精确操控情感驱动图像编辑极具挑战。

Method: 1）构建超过800万张、带有详细情感分层标注的图片数据集（MoodArchive）；2）基于此数据集微调视觉-语言模型MoodifyCLIP，实现情感与视觉属性映射；3）提出无需训练的编辑模型Moodifier，利用视觉-语言模型与多模态大模型实现情感精准转换并保持内容不变。

Result: 在角色表情、时尚、珠宝、家居等多领域测试，Moodifier在情感准确性和内容保持方面均优于现有方法，实验效果突出，编辑结果具备情境适应性。

Conclusion: 该集成系统有效将抽象情感与具体视觉变换联系，为情感化内容创作提供新工具，并计划开源数据集、模型与代码，推动实际应用。

Abstract: Bridging emotions and visual content for emotion-driven image editing holds
great potential in creative industries, yet precise manipulation remains
challenging due to the abstract nature of emotions and their varied
manifestations across different contexts. We tackle this challenge with an
integrated approach consisting of three complementary components. First, we
introduce MoodArchive, an 8M+ image dataset with detailed hierarchical
emotional annotations generated by LLaVA and partially validated by human
evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned
on MoodArchive to translate abstract emotions into specific visual attributes.
Third, we propose Moodifier, a training-free editing model leveraging
MoodifyCLIP and multimodal large language models (MLLMs) to enable precise
emotional transformations while preserving content integrity. Our system works
across diverse domains such as character expressions, fashion design, jewelry,
and home d\'ecor, enabling creators to quickly visualize emotional variations
while preserving identity and structure. Extensive experimental evaluations
show that Moodifier outperforms existing methods in both emotional accuracy and
content preservation, providing contextually appropriate edits. By linking
abstract emotions to concrete visual changes, our solution unlocks new
possibilities for emotional content creation in real-world applications. We
will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier
code and demo publicly available upon acceptance.

</details>


### [75] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: 提出了一种超轻量级、基于量子辅助的深度学习框架（QuantEIT），用于高效、准确地重建电阻抗断层成像（EIT）图像，并在参数量极低的情况下优于传统和现有方法。


<details>
  <summary>Details</summary>
Motivation: EIT虽有床旁监测等优势，但反问题高度不适定，传统或现有深度学习方法依赖大量参数和复杂结构，导致效率和可扩展性受限，需要高效、低参数量的新方法。

Method: 提出了量子辅助网络（QA-Net），利用并行2比特量子电路产生有表现力的隐空间特征作为隐式非线性先验，结合单一线性层实现电导率重建。该体系结构极大减少模型复杂度与参数量，且无需标注数据、无需训练（无监督、训练数据无关）。

Result: 通过大量仿真和真实2D、3D肺部EIT实验，QuantEIT表现出比传统和现有方法更高或相当的重建精度，且仅用0.2%的参数量，对噪声更具鲁棒性。

Conclusion: QuantEIT首次将量子电路用于EIT成像重建，实现超低参数、无监督、训练数据无关的高效EIT图像恢复，并显著优于现有方法，具有广阔的应用前景。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [76] [Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)
*Qiankun Ma,Ziyao Zhang,Chi Su,Jie Chen,Zhen Song,Hairong Zheng,Wen Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖且高效的 Vision Mamba 模型的 token 压缩方法 MTR，不需要额外训练或调参，能大幅降低计算量且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 目前 Vision Mamba 虽然高效，但其 token 压缩技术鲜有研究，直接借用 ViTs 的方法会导致性能严重下降，因此迫切需要找到适合 Mamba 架构的有效 token 压缩方案，以推动其在更广泛场景下应用。

Method: 作者提出了针对 Mamba 结构特点的结构感知重要性分数来衡量 token 重要性，并据此设计出无需训练的 Mamba Token Reduction (MTR) 框架，可无缝集成至各类 Mamba 模型，实现训练前即插即用的 token 压缩。

Result: 在多种任务和多种 Mamba 不同主干网络上，实验显示该方法可显著减少计算量（如在 Vim-B 主干上减少约 40% 的 FLOPs），而在 ImageNet 上性能仅下降 1.6%，无需额外训练。

Conclusion: MTR 框架有效提升了 Vision Mamba 的计算效率，兼顾性能与通用性，有望推动其在实际应用中的更广泛落地。

Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)
due to its ability to efficiently capture long-range dependencies with linear
computational complexity. While token reduction, an effective compression
technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision
Mamba's efficiency is essential for enabling broader applications. However, we
find that directly applying existing token reduction techniques for ViTs to
Vision Mamba leads to significant performance degradation. This is primarily
because Mamba is a sequence model without attention mechanisms, whereas most
token reduction techniques for ViTs rely on attention mechanisms for importance
measurement and overlook the order of compressed tokens. In this paper, we
investigate a Mamba structure-aware importance score to evaluate token
importance in a simple and effective manner. Building on this score, we further
propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction
framework. Without the need for training or additional tuning parameters, our
method can be seamlessly integrated as a plug-and-play component across various
Mamba models. Extensive experiments demonstrate that our approach significantly
reduces computational workload while minimizing performance impact across
various tasks and multiple backbones. Notably, MTR reduces FLOPs by
approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet
performance without retraining.

</details>


### [77] [Foundation Models as Class-Incremental Learners for Dermatological Image Classification](https://arxiv.org/abs/2507.14050)
*Mohamed Elkhayat,Mohamed Mahmoud,Jamil Fayyad,Nourhan Bayasi*

Main category: cs.CV

TL;DR: 本论文评估了在皮肤病分类中，使用大规模预训练的frozen foundation models（FM）进行类增量学习（CIL），提出在冻结骨干的前提下，仅用轻量级MLP增量训练的方案，取得了SOTA性能，且无遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 传统CIL方法在引入新类别时常面临遗忘问题。随着基础模型（FM）的大规模预训练及其在视觉任务中强大的迁移能力，探索其在皮肤病学CIL场景下的潜力及最优用法显得尤为重要。

Method: 方法为冻结在大规模皮肤病数据集上预训练的FM，仅针对每一任务增量地训练轻量MLP分类头；此外也探讨了零训练情况下，用嵌入特征均值原型进行最近均值分类。

Result: 该方法在皮肤病分类CIL任务中取得了SOTA性能，优于传统的正则化、回放与结构调整方法；原型分类器同样达到了有竞争力的表现。

Conclusion: 冻结的基础模型为皮肤病学增量学习提供了强大支持，无显著遗忘，具备推广应用于实际医学场景的潜力。

Abstract: Class-Incremental Learning (CIL) aims to learn new classes over time without
forgetting previously acquired knowledge. The emergence of foundation models
(FM) pretrained on large datasets presents new opportunities for CIL by
offering rich, transferable representations. However, their potential for
enabling incremental learning in dermatology remains largely unexplored. In
this paper, we systematically evaluate frozen FMs pretrained on large-scale
skin lesion datasets for CIL in dermatological disease classification. We
propose a simple yet effective approach where the backbone remains frozen, and
a lightweight MLP is trained incrementally for each task. This setup achieves
state-of-the-art performance without forgetting, outperforming regularization,
replay, and architecture based methods. To further explore the capabilities of
frozen FMs, we examine zero training scenarios using nearest mean classifiers
with prototypes derived from their embeddings. Through extensive ablation
studies, we demonstrate that this prototype based variant can also achieve
competitive results. Our findings highlight the strength of frozen FMs for
continual learning in dermatology and support their broader adoption in real
world medical applications. Our code and datasets are available here.

</details>


### [78] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的视觉-文本对齐水印方案（VLA-Mark），能在保护版权的同时，不影响多模态一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本水印方法会破坏视觉与文本之间的对齐，导致语义关键信息丢失。因此，亟需一种能兼顾语义一致性和可检测性的视觉-文本模型水印技术。

Method: VLA-Mark 框架通过多尺度的视觉-文本对齐度量（如局部块相似性、全局语义一致性、上下文注意模式），动态调节水印强度和语义保真度，并在生成不确定性较低阶段优先保护视觉定位。无需模型重训练即可注入可检测水印。

Result: 实验表明，VLA-Mark 相对传统方法 PPL 降低 7.4%，BLEU 提高 26.6%，水印检测准确率（AUC）达到 98.8%。面对同义改写等攻击时，抗攻击能力高达 96.1%，文本-视觉一致性表现优异。

Conclusion: 该方法在保护多模态内容一致性的同时，实现高质量水印嵌入与强抗攻击能力，为多模态水印技术设定了新标准。

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [79] [Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection](https://arxiv.org/abs/2507.14083)
*Sara Abdulaziz,Egor Bondarev*

Main category: cs.CV

TL;DR: 本文系统分析了四种常见的人体匿名化技术（模糊、遮挡、加密和虚拟替代）对视频异常检测性能的影响，表明隐私保护和检测效能之间存在复杂权衡。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习助力的视频异常检测性能提升，对敏感人类数据的采集导致隐私问题愈发突出，因此该研究旨在明确不同匿名化方法会如何影响异常检测性能，并探索隐私保护与检测效用之间的平衡。

Method: 选用UCF-Crime数据集，对其应用四种匿名化方法（模糊、遮挡、加密、虚拟替身），基准评测MGFN、UR-DMU、BN-WVAD和PEL4VAD四种异常检测算法在匿名化视频下的表现，并系统分析不同算法对不同匿名化方式的响应差异。同时，还与新兴的隐私友好型技术进行了对比。

Result: 实验表明，视频数据经过匿名化处理后，异常检测仍然可行，不同算法对不同匿名方式的适应性差异明显。有时特定匿名处理（如加密、遮挡）还能使部分模型AUC提升，因为所引入的“噪声”与算法结构特性相吻合。此外，分析指出传统匿名化手段与现代隐私设计方案之间在隐私保护和效用弹性上存在权衡。

Conclusion: 本文为视频异常检测与人类隐私保护的平衡提供了定量基准和分析参考，揭示不同算法对隐私技术的敏感性，并强调在保护隐私的同时要兼顾检测性能，促进后续相关方法的设计优化。

Abstract: Advancements in deep learning have improved anomaly detection in surveillance
videos, yet they raise urgent privacy concerns due to the collection of
sensitive human data. In this paper, we present a comprehensive analysis of
anomaly detection performance under four human anonymization techniques,
including blurring, masking, encryption, and avatar replacement, applied to the
UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,
BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method
responds to different obfuscation techniques. Experimental results demonstrate
that anomaly detection remains viable under anonymized data and is dependent on
the algorithmic design and the learning strategy. For instance, under certain
anonymization patterns, such as encryption and masking, some models
inadvertently achieve higher AUC performance compared to raw data, due to the
strong responsiveness of their algorithmic components to these noise patterns.
These results highlight the algorithm-specific sensitivities to anonymization
and emphasize the trade-off between preserving privacy and maintaining
detection utility. Furthermore, we compare these conventional anonymization
techniques with the emerging privacy-by-design solutions, highlighting an often
overlooked trade-off between robust privacy protection and utility flexibility.
Through comprehensive experiments and analyses, this study provides a
compelling benchmark and insights into balancing human privacy with the demands
of anomaly detection.

</details>


### [80] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: 本文评估了一款全自动深度学习软件在多中心脊柱侧弯Cobb角度测量中的表现，结果显示该AI工具能够达到与专家放射科医师相当的准确度，有望提升临床工作效率。


<details>
  <summary>Details</summary>
Motivation: 青少年脊柱侧弯的诊断与治疗强烈依赖Cobb角度的精确测量，但传统手动测量耗时且存在评估者之间的主观差异，因此亟需高效且一致性强的自动化测量方法。

Method: 研究采用回顾性多中心设计，收集103例来自十家医院的脊柱正位X光片，通过两位肌骨放射科医师独立手工测量获得基准结果，然后比较AI系统与人工测量的一致性（使用Bland-Altman分析、MAE、RMSE、Pearson相关系数、Cohen kappa等统计指标）。

Result: AI系统与两位放射科医师的平均绝对误差(MAE)分别为3.89°和3.90°，相关性系数分别为0.906和0.880，四级严重度kappa分别为0.51和0.64，表现接近专家水平，与医师间一致性相近。

Conclusion: 该深度学习AI测量工具能够稳定地复现专家级的Cobb角度测量及分级结果，有应用于临床工作流程自动化、提升侧弯筛查与报告效率的潜力。

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [81] [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs](https://arxiv.org/abs/2507.14095)
*Yung-Hong Sun,Ting-Hung Lin,Jiangang Chen,Hongrui Jiang,Yu Hen Hu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的多视角多目标关联方法C-DOG，可在无需视觉特征情况下，实现不同视角下目标点的鲁棒匹配，提升3D重建的鲁棒性和效果。


<details>
  <summary>Details</summary>
Motivation: 传统的多视角目标关联方法依赖于目标外观特征或基础几何约束，但在目标外观高度相似或数据受噪声污染时效果较差。因此，亟需一种不依赖视觉特征并能容忍噪声、适用于复杂实际场景的新方法。

Method: 提出了C-DOG框架，将二维检测结果转为图结构节点，利用epipolar一致性赋边权，通过delta-neighbor-overlap聚类组团，并引入IQR统计和3D反投影误差筛选不一致观测，实现目标的鲁棒关联。该方法无需训练，可作为检测到3D重建之间的中间模块。

Result: 在合成基准数据集上进行了大量实验，结果显示C-DOG在高目标密度、无视觉特征、摄像机视角重叠有限等恶劣条件下，相较几何基线方法表现更优，且具有强鲁棒性。

Conclusion: C-DOG实现了无需视觉特征的多视角多目标鲁棒关联，能有效提升大规模3D重建的可用性和稳定性，适用于实际复杂环境。

Abstract: Multi-view multi-object association is a fundamental step in 3D
reconstruction pipelines, enabling consistent grouping of object instances
across multiple camera views. Existing methods often rely on appearance
features or geometric constraints such as epipolar consistency. However, these
approaches can fail when objects are visually indistinguishable or observations
are corrupted by noise. We propose C-DOG, a training-free framework that serves
as an intermediate module bridging object detection (or pose estimation) and 3D
reconstruction, without relying on visual features. It combines connected
delta-overlap graph modeling with epipolar geometry to robustly associate
detections across views. Each 2D observation is represented as a graph node,
with edges weighted by epipolar consistency. A delta-neighbor-overlap
clustering step identifies strongly consistent groups while tolerating noise
and partial connectivity. To further improve robustness, we incorporate
Interquartile Range (IQR)-based filtering and a 3D back-projection error
criterion to eliminate inconsistent observations. Extensive experiments on
synthetic benchmarks demonstrate that C-DOG outperforms geometry-based
baselines and remains robust under challenging conditions, including high
object density, without visual features, and limited camera overlap, making it
well-suited for scalable 3D reconstruction in real-world scenarios.

</details>


### [82] [Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning](https://arxiv.org/abs/2507.14137)
*Shashanka Venkataramanan,Valentinos Pariza,Mohammadreza Salehi,Lukas Knobel,Spyros Gidaris,Elias Ramzi,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: Franca是第一个完全开源（数据、代码、权重）的视觉基础模型，不仅性能媲美甚至超越了一些主流的专有模型，并在多项下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当今强大的视觉基础模型大都是专有产品，缺乏开源透明性和可复现性，且自监督聚类方法存在语义聚类上的模糊和偏见等局限。本研究希望通过设计开源且高效的模型和新方法促进行业透明和进步。

Method: 采用受Web-SSL启发的透明训练流程（基于公开数据ImageNet-21K与ReLAION-2B），在聚类时引入多头嵌套Matryoshka结构的聚类投影器，能有效细化聚类且无额外参数开销。同时提出新颖的位置解纠缠技术，消除稠密特征中的位置偏差，提升语义表示能力。

Result: 与DINOv2、CLIP等先进专有模型相比，Franca在多个评估基准上获得一致性提升，特征空间更干净实用，兼具性能与内存效率。

Conclusion: Franca刷新了开源视觉模型新标准，鼓励更透明与可复现的发展，同时为广泛AI社区提供高质量基础模型的开源方案。

Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source
(data, code, weights) vision foundation model that matches and in many cases
surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,
CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training
pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and
a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in
SSL clustering methods. While modern models rely on assigning image features to
large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to
account for the inherent ambiguity in clustering semantics. To address this, we
introduce a parameter-efficient, multi-head clustering projector based on
nested Matryoshka representations. This design progressively refines features
into increasingly fine-grained clusters without increasing the model size,
enabling both performance and memory efficiency. Additionally, we propose a
novel positional disentanglement strategy that explicitly removes positional
biases from dense representations, thereby improving the encoding of semantic
content. This leads to consistent gains on several downstream benchmarks,
demonstrating the utility of cleaner feature spaces. Our contributions
establish a new standard for transparent, high-performance vision models and
open a path toward more reproducible and generalizable foundation models for
the broader AI community. The code and model checkpoints are available at
https://github.com/valeoai/Franca.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [83] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: 本研究提出一种利用自适应语言提示（ALP）和多模态大语言模型（如GPT-4o、Gemini 1.5 Pro）识别钓鱼网站的方法，显著提升检测准确率，F1分数达0.93。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击威胁严重，传统检测技术难以应对复杂与不断演化的欺诈手法，因此亟需更智能、适应性强的检测方法。

Method: 作者提出了一种自适应语言提示（ALP）策略，结合文本、视觉和URL等多模态特征，指导LLM通过分析语言模式、紧迫感暗示及操控性用语，有结构地识别钓鱼内容。使用GPT-4o、Gemini 1.5 Pro等先进多模态LLM进行实验验证。

Result: 实验表明，ALP能够有效提升大语言模型在钓鱼检测任务中的表现，取得了0.93的F1分数，优于传统检测方法。

Conclusion: ALP与多模态LLM结合为钓鱼检测提供了更强、可解释且自适应的解决方案，有望推动语言学驱动的安全检测系统发展。

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [84] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: 情感识别领域中缺乏高质量多样化数据集，作者提出PersonaGen框架，利用大模型和分阶段的虚拟人格条件生成富有情感的文本，并验证其在多项指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于情感表达具有高度主观性，受个体特质、社会文化和语境影响，收集具有代表性的大规模真实情感数据在伦理和实际操作上都很困难。为解决数据稀缺、提升情感识别模型性能的问题，作者试图构造替代真实情感数据的高质量合成数据。

Method: 提出PersonaGen框架，利用大语言模型，通过多阶段的人格条件（结合人口属性、社会文化背景和情境）生成更具个性和情感多样性的文本。对生成数据从语义多样性、人类相似性、真实感和下游分类效果等方面进行了全面评估。

Result: 实验表明，PersonaGen在生成多样、连贯且判别性强的情感表达方面，显著优于其他基线方法。在真实性、类别多样性、人类相似性等多项指标上均有提升。

Conclusion: PersonaGen能够高效生成高质量的情感文本，可作为真实情感数据集的有力补充甚至替代方案，为情感识别等相关任务数据增强或数据集构建提供新路径。

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [85] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: 本文提出了SAFT方法，将图结构信息有效融入大语言模型（LLMs），在AMR到文本生成任务上取得新SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 当前许多方法在处理结构化输入如AMR（Abstract Meaning Representations）时，常常无视或丢弃其图结构信息，导致生成性能受限。且已有能利用结构信息的方法普遍需要对模型架构做出改变，难以兼容标准预训练大模型。因此，作者希望提出一种无需改动架构、能充分利用图结构的通用方法。

Method: 提出SAFT（结构感知微调）方法：1）对AMR图结构实施变换，2）根据磁拉普拉斯算子计算方向敏感的位置编码，3）将该编码投射到LLM的嵌入空间，实现结构信息高效注入，且无需改变基础模型架构。

Result: SAFT在AMR 3.0到文本生成任务上较基准方法BLEU分数提升3.5，效果随图结构复杂度提升而显著提升，表现出对复杂结构的强泛化能力。

Conclusion: SAFT证明结构感知表示能显著提升大语言模型对结构化数据的处理能力，尤其适用于图结构输入，为语言模型与结构化数据的结合提供了通用且高效的新途径。

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [86] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文图的新方法，结合NLP技术和图异常检测算法，用于检测虚假新闻。通过对Kaggle与疫情相关的数据集进行处理，将新闻转换为图结构后利用MDL-GBAD算法检测异常模式，有效分辨真假新闻。


<details>
  <summary>Details</summary>
Motivation: 假新闻在数字时代传播迅速，带来严峻挑战。作者希望通过更精准的方法提高虚假新闻检测的准确性，弥补传统方法在模式识别上的不足。

Method: 作者采用NLP方法将新闻文本转化为上下文图结构，并结合Kaggle及新冠相关新闻数据集，利用最小描述长度（MDL）下的基于图的异常检测（GBAD）算法，进行图模式挖掘，发现异常（可疑）新闻。

Result: 方法能够识别数据集中规律性模式，并检测出与这些规范模式偏离的异常新闻，从而实现对虚假新闻的有效辨别。

Conclusion: 基于图的方法能有效揭示复杂的新闻模式关系，相较传统方法在虚假新闻检测方面具有更强的能力。该策略有助于缓解当前假新闻带来的社会问题。

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [87] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出了PARAM-1，这是一个以印度语言多样性为核心设计的2.9B参数解码器式大语言模型，在印地语和英语高质量数据上从零训练，重点提升对印度地区多语言、多方言及混合语言现象的支持及公平性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的发展以英语为主，导致包括印度在内的多语言地区在结构上被严重低估，无法很好地支持本地语言和复杂的社会语言现象。

Method: PARAM-1采用了解决多语言平衡的三大原则：1）在数据集分配上确保印度语言25%的代表性；2）使用适应印度形态结构的SentencePiece分词器保证分词公平性；3）用符合本地文化的评测基准（如IndicQA、代码混合推理和社会语言鲁棒性任务）对模型表现进行评估。

Result: PARAM-1在泛用性任务和针对印度多语言及混合语言任务上均表现突出，作为印度本地化应用的强有力基线，为未来同类模型提供了参考。

Conclusion: 将多样性、公平性在预训练阶段嵌入模型，PARAM-1为多语言地区特别是印度提供了面向公平基础模型开发的设计范式，既通用又本地化，能够弥补现有模型在多语言社会中的弱点。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [88] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: 本文通过对客户评论的观点单元进行主题建模，提升了主题模型的表现，不仅能提取更具解释性的主题，还能与情感打分结合，对业务指标具有更强的关联分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模在客户评论分析时，难以兼顾主题的可解释性和相关情感信息，缺乏对业务指标直接影响的深入洞察。为解决此痛点，本文尝试将主题建模流程重构到观点单元（带情感分数的精细语句）层面，提升其分析效率与商业价值。

Method: 采用大型语言模型自动抽取客户评论中的观点单元，每个单元包含一段相关文本和相应情感分数；在此基础上进行主题建模，获得主题-情感耦合信息。将提取的主题和情感与业务指标如星级评分相关联，分析客户关注点与业务结果的关系。

Result: 系统实现展示了在各类用例下对比传统主题建模和分类方法拥有明显优势。实验证明该方法能更好地生成连贯且可解释的主题，同时提升星级预测的准确率。

Conclusion: 基于观点单元的主题建模显著提升了客户评论分析效果，不仅提高了主题抽取的解释性，还能结合情感信息更精准预测业务情况，为企业提供了更有洞察力的分析工具。

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [89] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: 本文提出了Babel框架，可以在不依赖风格平行语料的前提下，提高神经机器翻译（NMT）的风格保留能力。Babel通过风格检测与风格调整组件，实现了更高的风格一致性，并作为后处理模块集成至现有NMT系统。实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管NMT极大促进了跨语言交流，但现有方法在风格保留上仍有较大挑战。多数风格保留方法依赖风格平行语料，实际受限，因此需探索无需平行数据的风格增强方法。

Method: Babel框架包含两个核心部分：1) 基于上下文嵌入的风格检测器，用于识别源文与译文间的风格差异；2) 基于扩散模型的风格应用器，修正风格不一致同时保持语义完整。Babel作为后处理模块集成至NMT系统，无需修改基础架构，也无需风格平行数据。

Result: 在法律、文学、科技写作、医学和教育五大领域实验，Babel能以88.21%的精确率识别风格不一致，并将风格保留能力提升了150%，同时语义相似度高达0.92。人工评价也证实了Babel调整后的译文更好地保留了源文风格且流畅达意。

Conclusion: Babel框架仅依赖单语语料就能显著提升NMT的风格一致性与自适应性，且实现简单，适合实际系统集成，推动了高质量风格保留翻译的发展。

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [90] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: 本论文提出了一种利用稀疏自编码器（SAE）特征，在无需显式语言提示或微调的情况下，实现对多语言大模型生成语言的可控引导方法。通过在推理时仅修改一个特征，即可高效地实现语言切换。


<details>
  <summary>Details</summary>
Motivation: 当前大型多语言模型难以在零样本情况下完全控制输出语言，尚缺乏轻量、直观、无需训练或复杂输入的新方法。因此，探索基于模型内部稀疏特征的控制方法，具有重大意义。

Method: 使用预训练SAE，识别大模型Gemma-2B和Gemma-9B在英语与其他四种语言（中文、日语、西班牙语、法语）之间最具差异的特征。仅在一个Transformer层上修改单个SAE特征，通过调整其激活值实现语言切换效果。

Result: 通过该方法，多语言模型在FastText分类下实现了最高达90%的成功语言转换率，且语义保持度良好（据LaBSE相似性）。分析发现，语言引导主要发生在中间及后段Transformer层，且某些与语言敏感特征高度关联的注意力头具有放大作用。

Conclusion: SAE特征微调是一种高效、可解释的多语言生成控制手段，无需训练或复杂工程操作，展示了在多语言可控生成领域的巨大潜力。

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [91] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALIGNed-LLM的新方法，通过将知识图谱嵌入高效融合到大型语言模型（如GPT-4、Gemini等）的潜在空间来提升其事实性，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然在自然语言处理任务中表现卓越，但容易产生虚假或错误答案（即“幻觉”）。为了增强语言模型的事实可靠性，把结构化、权威的信息源——知识图谱融合进模型成为一个重要方向。

Method: 该方法借鉴LLaVA模型思路，采用预训练的知识图谱嵌入（如TransE模型生成的表示）与可训练投影层，将实体表示和文本嵌入对齐，促使语言模型更好地区分相似实体，从而提升事实基础。

Result: 在多个主流问答数据集及不同规模的语言模型上测试后，ALIGNed-LLM在事实性和结果准确率上都有显著提升。

Conclusion: 通过高效地融合知识图谱嵌入，ALIGNed-LLM能有效提高大语言模型事实性并减少幻觉，在实际的高精度领域（如金融）也证明了其实用性和优越性。

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [92] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Paper Summary Attack（PSA）的新型越狱攻击，通过模仿权威学术论文结构，将有害内容作为payload嵌入摘要模板，对主流大语言模型进行攻击，取得极高的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型具备一定的“信任权威”倾向，容易对学术论文格式的内容放松警惕，这可能成为模型安全的新漏洞。

Method: 作者设计了初步分析，验证LLMs对权威来源易放松警惕。随后开发了PSA方法，从安全相关的学术论文中自动提取结构和内容，构建带有恶意问题的摘要prompt，对模型进行越狱实验。

Result: PSA在包括Claude3.5-Sonnet和Deepseek-R1等SOTA模型上，获得高达97%-98%的攻击成功率。还发现不同模型和版本之间在面对同类攻击时的脆弱性表现存在显著差异。

Conclusion: 通过学术论文格式伪装攻击可显著提升越狱攻击成功率，模型对权威文本的信任是未来安全防护的潜在薄弱环节。该现象为对抗性攻击和安全对齐研究带来新的启示。

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [93] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本论文系统分析了大语言模型（LLMs）价值观测方法的鲁棒性和表达能力，发现现有探查方法在不同输入下结果差异较大，且其探查到的价值观与具体行为的一致性较弱。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型的价值观测评对提升不同群体用户体验很重要，但现有探查策略容易受输入扰动影响，且实际揭示价值观的有效性尚未充分验证，因此需要更系统的比较和检验。

Method: 作者系统性地对比了三种主流的价值探查策略，在不同问题和选项配置下评估鲁棒性；还设计了两个任务来考察模型价值观对人口统计学背景的敏感性及其与模型行为的关联性。

Result: 所有探查方法在输入扰动下表现出较大波动；人口统计背景信息对模型生成文本影响甚微；探查所得价值观与模型在相关场景下的行为偏好相关性较弱。

Conclusion: 需要对大语言模型价值观探查方法进行更严格和细致的检验，并且在实际应用中要充分认识现有探查技术的局限性。

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [94] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: 本文提出了一种数学方法，将词汇项表示为函数（如小波）后，可以在同一函数空间中忠实地表示任意句法对象，并用二阶Renyi熵构建了交换但非结合的半环结构，进而展示该结构与神经计算实现句法核心结构的理论可行性。


<details>
  <summary>Details</summary>
Motivation: 语言句法结构的神经计算实现是认知科学和计算语言学中的核心问题。本文作者希望以严密的数学工具，探索句法核心操作（如Merge）在神经系统中实现的理论可能性，推动对语言计算机制神经基础的理解。

Method: 作者假设词汇项可以用函数（如小波函数）在某函数空间中表示。接着在该空间上通过二阶Renyi熵建立了交换但非结合的半环（semiring）结构。利用operad代数框架，建模语言运算（如Merge）为作用于函数的电路，并通过Hopf代数和Markov链形式实现语法操作在该空间上的运算。同时，用正弦波频率同步作为具体例子，实现Merge操作。

Result: 作者证明了在所构建的数学系统中，可以将语言中的任意语法对象（如短语、句法结构）忠实地表示和合成，并完整描述了Merge操作如何映射为这一代数结构中的具体运算，还发现了Merge与算术后继函数间的对应关系。

Conclusion: 本文不仅证明了句法核心结构的神经计算实现理论上是可行的，还揭示了Merge在数学与神经计算中的普适性，为理解语言句法的神经基础及其与算术计算的联系提供了新思路。

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [95] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的计算框架，用于分析和简化对话流图，从而更好地刻画和理解大规模、松散结构对话（quasi-patterned conversations）的结构与语义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）系统的广泛应用，了解和建模人机对话的结构变得尤为重要。现有方法难以有效捕捉这些松散组织的对话流的结构与语义特征。

Method: 作者提出了新的计算框架，能够为松散结构的对话构建对话图，并提出Filter & Reconnect方法对其进行简化。该方法旨在最大程度地减少噪声，同时保持语义和结构的完整性。

Result: 通过与现有方法的对比分析，结合大型语言模型和新的图简化方法后，语义度量S提升了2.06倍，同时实现了0 δ-双曲性，呈现出类似树的结构，提高了对话建模的清晰度。

Conclusion: 该方法为分析大规模对话数据集提供了有效的工具，对自动化系统的监测、对话管理和用户行为分析等实际领域具有重要应用价值。

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [96] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: 本研究通过结合自动语音识别系统（ASR）生成的停顿特征和语义连贯性指标，用机器学习方法有效预测精神分裂症相关思维障碍（FTD）的严重程度，显示集成模型优于仅基于语义的预测。


<details>
  <summary>Details</summary>
Motivation: FTD是精神分裂症谱系障碍的一个核心特征，但传统的临床评估方法耗时且难以规模化，亟需客观且可扩展的自动化评估工具。

Method: 研究利用自动语音识别（ASR）技术，结合语音停顿相关的时间信息与语义连贯性指标，通过支持向量回归（SVR）模型，在三个不同语音数据集上预测临床FTD评分，并评估特征集成的效果。

Result: 仅使用停顿特征即可稳健预测FTD严重程度；与语义特征结合后，预测性能进一步提升，最佳模型在FTD严重个案识别上达到AUC 83.71%。这种优势在不同的数据类型与交流场景下均较为稳定。

Conclusion: 结合语音的时序特征与语义指标能显著提升思维障碍的自动化评估精度，为精神病语音分析提供了改进方向和可扩展的新方法。

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [97] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: 本文提出了Balalaika数据集，这是一个包含2000多小时高质量俄语录音及全面文本注释的新型语音数据集，并证明基于该数据集训练的模型在语音合成与增强任务上明显优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 俄语语音合成面临元音弱化、辅音清音化、重音变化、同形异义词歧义及语调不自然等独特挑战，现有数据集难以满足高质量俄语语音建模需求。

Method: 构建超过2000小时高质量俄语录音库，添加详细文本注释（包括标点及重音标记），设计完备的数据加工与注释流程，并通过模型训练与现有数据集进行对比评测。

Result: 基于Balalaika训练的模型在俄语语音合成与增强任务上均显著优于使用其他数据集的模型。

Conclusion: Balalaika数据集通过高质量音频和丰富注释极大推动了俄语语音处理技术的发展，为后续相关研究提供了重要基础。

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [98] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本文通过多个语言学特征对人类与大语言模型（LLMs）生成的文本进行分析，发现人类文本在语法结构上更简单、语义更丰富，而新的模型生成的文本在风格多样性上逐渐趋同。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的进步，机器生成文本与人类文本越来越难以区分。现有研究多着重于分类判别，这项研究希望通过细致的语言特征分析，揭示人机文本间在不同语言层面的表现差异。

Method: 作者选取来自8个领域、由11个不同LLMs生成的文本及人类文本，计算依存长度、情感等多项语言特征，结合不同采样策略、重复控制和发布时间，系统性进行统计分析，并利用风格嵌入进一步检验风格多样性。

Result: 人类文本通常句法更简单、语义更具多样性；无论人类还是机器文本，均随领域变化展现风格多样性，但人类变化更广。随模型更新，LLM生成文本在风格上开始表现同质化。

Conclusion: 通过多层次语言特征分析识别人类与LLM文本差异，对理解LLM文本特征和风险有参考价值，提示未来模型可能趋向风格同质化。

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [99] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-X是一个7B参数的开放源代码大模型家族，在多语言翻译任务上表现卓越，超越了许多更大的开源模型，并接近闭源主流水平。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译对于大语言模型来说极具挑战，尤其是在处理复杂语法结构和不自然翻译时。现有主流模型往往闭源且资源要求高，缺乏高效开放的多语言翻译解决方案。

Method: 构建了Seed-X系列开放大模型，包括基础、指令和推理模型。基础模型在涵盖28种语言的高质量单语和双语数据上预训练，指令模型通过链式思维推理（CoT）微调，并用强化学习进一步优化泛化能力。

Result: Seed-X模型在28种语言上，自动和人工评估均超过其他更大参数的开源模型，且翻译表现接近Gemini-2.5、GPT-4o等主流闭源模型。

Conclusion: Seed-X证明了高质量、多语言数据和创新训练方式可大幅提升小参数大模型的多语言翻译能力，推动翻译领域向开源方向发展，模型参数已开放。

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [100] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 本文提出了一种名为CU-ICU的新方法，可以高效定制无监督指令微调的大语言模型用于ICU领域任务，显著提升了预测准确性和可解释性，同时模型更新参数极少。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型应用于专业领域（如医疗）时面临领域适应性与标注数据稀缺问题，需要更高效、低资源消耗的模型定制方法。

Method: 作者提出基于T5架构的CU-ICU方法，采用稀疏微调策略，将少样本提示（few-shot prompting）与选择性参数更新结合，只需极少数参数调整，实现高效领域适应。

Result: CU-ICU在ICU关键任务（早期脓毒症检测、死亡率预测、临床笔记生成）上均优于标准微调方法，脓毒症检测准确率提升至15%，生成解释性文本能力提升20%，且最优情况下仅需更新1%以下参数。

Conclusion: CU-ICU是一种可扩展、低开销的技术，能够为真实ICU场景提供高准确度和可解释性的临床决策支持。

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [101] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为KiC（Keyword-inspired Cascade）的新级联方法，在兼顾生成文本质量和成本的前提下，有效提升了大语言模型推理的性价比。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然性能优越，但高质量推理通常依赖昂贵的API调用，带来较高的推理成本。现有的级联方法通过先用低成本模型，必要时再升级到高性能模型，但在自由文本生成下难以用精确匹配来挑选可靠答案和评估输出的整体可靠性。本文有意解决这些效率与质量之间的矛盾。

Method: KiC方法会先用更弱模型生成多个候选答案，通过语义对齐方式（借助关键词）确定其中代表性最强的答案，并判断其他输出与该答案的语义一致度。若一致度足够高，则接受弱模型的结果，否则升级调用更强模型。

Result: 在三个自由文本生成基准任务上，KiC方法能达到GPT-4准确率的97.53%，平均还能节省28.81%的API开销；在某些基准上表现甚至超过了GPT-4。

Conclusion: KiC兼顾文本生成任务的准确性和成本，能大幅降低高性能LLM的实际应用门槛，是大语言模型高效推理的有力工具。

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [102] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: 本文提出了LoopServe框架，通过自适应的双阶段推理加速技术，有效提升大型语言模型在多轮对话中的推理速度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着对话历史增长，当前大型语言模型在多轮对话应用中面临着计算资源和内存效率的挑战，现有方法往往难以应对真实多轮对话中的动态环境。

Method: LoopServe框架包含两项主要创新：1）在预填充阶段，动态在线稀疏化注意力矩阵，只选择最重要部分参与计算；2）在解码阶段，采取递进式的键值缓存压缩策略，自适应保留和维护最近生成输出token对应的高相关性缓存。同时，论文还提出了包含11个数据集的新多轮对话基准测试集，更贴合真实对话场景。

Result: 实验证明，LoopServe在多种长上下文多轮对话任务中，相较于以往的加速方法展现出更优的效果和显著的推理加速。

Conclusion: LoopServe能够有效应对大模型长上下文多轮对话的加速需求，在实际应用中具有较高的实用价值和推广意义。

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [103] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLM）在群体推荐系统（GRS）中的推荐和解释性能，并与传统的社会选择聚合策略进行对比。结果显示，LLM给出的推荐类似于加性功利主义（ADD）聚合，但其解释常涉及更多未明确定义的标准，且解释存在不一致和模糊性。


<details>
  <summary>Details</summary>
Motivation: LLM正越来越多地被用于群体推荐系统作为联合决策者及解释生成器。其透明度与可解释性是被关注的核心动机，但目前其解释与实际决策过程是否一致尚不清楚，因此需要系统性评估LLM的推荐和解释质量。

Method: 本研究通过将LLM生成的推荐及解释与传统社会选择聚合策略对比，包括ADD聚合等。同时考察了组内结构（是否意见一致）及分项数量变化对结果的影响，分析LLM在群体推荐情境下的表现。

Result: LLM的推荐结果往往接近于ADD聚合法。其解释中多次提到平均分等准则，但这些解释与ADD并不完全一致。组内结构对结果影响不大。LLM常引用一些模糊或未定义的标准如用户/物品相似性、多样性或流行度阈值。使用的解释标准会随群体评分项目数量变化而变化，且在大型项目集下聚合方法可能效率降低。

Conclusion: LLM虽能生成与传统聚合法接近的推荐，但解释存在不一致和歧义，容易削弱系统的透明度和可解释性。这对实际将LLM用于群体推荐系统及评估标准聚合策略都有重要启示。

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [104] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: 本文利用机器学习方法，分析法国上诉法院儿童监护权判决，发现不同法官的个人判决模式会显著影响案件结果。


<details>
  <summary>Details</summary>
Motivation: 动机在于法律现实主义与形式主义的争论：传统观点认为法官仅机械适用法律，而现实主义认为法官个人因素影响判决。本研究质疑法官中立、法律统一适用的假设。

Method: 作者构建数据集，涵盖10,306个案件中18,937份监护权判决，利用严格去标识化处理保护隐私。方法上，将数据分为按法官个体训练（专家模型）和全体判决数据训练（通用模型）；预测流程采用大语言模型提取结构化特征，然后输入RF、XGB、SVC等机器学习模型进行预测。

Result: 专家模型（基于个人法官过往判决）预测准确率（最高F1值达92.85%）显著高于通用模型（F1为82.63%，后者样本量多20-100倍）。专家模型捕捉到稳定、且不可迁移到其他法官的个体判决模式。

Conclusion: 法官个人身份对法律判决确有可测量影响，支持法律现实主义观点，否定了法官完全中立、统一适用法律的假设。所有数据与代码将公开。

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [105] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
*Maluna Menke,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本文探讨如何减少大型语言模型（LLMs）中对LGBTQIA+群体的偏见，评估了两种高效微调技术（LoRA和soft-prompt tuning）的效果。实验发现LoRA能大幅提升公平性，建议更广泛地采用社区主导的微调方法。


<details>
  <summary>Details</summary>
Motivation: LLMs由于训练语料中固有的性别和性少数身份偏见，输出内容往往边缘化LGBTQIA+用户，因此迫切需要减缓这类偏见，提高模型的包容性和公平性。

Method: 针对三个开源LLM，作者用WinoQueer基准测量性别与性取向方向的偏见，对比了LoRA（低秩变换，参数增加不足0.1%）和soft-prompt tuning（10个虚拟token）这两种小参数量微调方法，通过在QueerNews语料上微调，衡量其消除偏见的效果。

Result: 微调前偏见分最高达98（中立为50）。采用LoRA后，偏见分数降低至多50分，并将中立率从0%提升到36%。soft-prompt tuning仅有轻微改善。

Conclusion: LoRA方案以极低的计算/参数量带来显著公平性提升，建议广泛采用社区主导的高效微调，并丰富语料库与评测体系以持续提升LLMs对性别/性少数群体的包容性。

Abstract: Large Language Models (LLMs) frequently reproduce the gender- and
sexual-identity prejudices embedded in their training corpora, leading to
outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of
great importance. To achieve this, we evaluate two parameter-efficient
fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt
tuning - as lightweight alternatives to full-model fine-tuning for mitigating
such biases. Using the WinoQueer benchmark, we quantify bias in three
open-source LLMs and observe baseline bias scores reaching up to 98 (out of
100) across a range of queer identities defined by gender and/or sexual
orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%
additional parameters) on a curated QueerNews corpus reduces those scores by up
to 50 points and raises neutrality from virtually 0% to as much as 36%.
Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements.
These findings show that LoRA can deliver meaningful fairness gains with
minimal computation. We advocate broader adoption of community-informed PEFT,
the creation of larger queer-authored corpora, and richer evaluation suites
beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [106] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
*Palash Nandi,Maithili Joshi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文探讨了多模态视觉语言模型（VLM）在不同提示（prompt）设计下对不当内容生成的易感性，并展示了小的提示变动即可绕过安全防护，导致模型输出有害内容。


<details>
  <summary>Details</summary>
Motivation: 随着VLM应用日益广泛，其安全性问题受到关注，尤其是模型对于不同提示输入的敏感性可能被恶意利用产生有害内容。作者希望系统分析哪些提示设计要素最容易被利用来实现模型“越狱”。

Method: 论文分析了提示中三个关键要素对VLM越狱成功率的影响：（a）详细视觉信息的纳入，（b）对抗性示例的存在，（c）积极表述开头语。作者对单模态（仅文本/仅图像）和多模态输入进行了系统实验，并设计了一种在模型内部层次间利用跳跃连接提升越狱成功率的框架。

Result: 结果显示：VLM在单模态下可以较好地区分良性与有害输入，但在多模态环境下能力明显下降。三种提示要素各自都能独立引发越狱；只需三个上下文示例即可显著提升生成不当内容的概率。提出的跳跃连接方法能大幅提升越狱成功率，包括在输入为良性图片时。幽默或无害的meme与有害图片同样易被用以引发不当内容。

Conclusion: VLM的越狱不仅受文本设计影响，还与视觉信息密切关联，且模型在多模态下存在更复杂且难以察觉的安全漏洞。应对VLM安全性进行更深入研究，并加强防范措施。

Abstract: Language models are highly sensitive to prompt formulations - small changes
in input can drastically alter their output. This raises a critical question:
To what extent can prompt sensitivity be exploited to generate inapt content?
In this paper, we investigate how discrete components of prompt design
influence the generation of inappropriate content in Visual Language Models
(VLMs). Specifically, we analyze the impact of three key factors on successful
jailbreaks: (a) the inclusion of detailed visual information, (b) the presence
of adversarial examples, and (c) the use of positively framed beginning
phrases. Our findings reveal that while a VLM can reliably distinguish between
benign and harmful inputs in unimodal settings (text-only or image-only), this
ability significantly degrades in multimodal contexts. Each of the three
factors is independently capable of triggering a jailbreak, and we show that
even a small number of in-context examples (as few as three) can push the model
toward generating inappropriate outputs. Furthermore, we propose a framework
that utilizes a skip-connection between two internal layers of the VLM, which
substantially increases jailbreak success rates, even when using benign images.
Finally, we demonstrate that memes, often perceived as humorous or harmless,
can be as effective as toxic visuals in eliciting harmful content, underscoring
the subtle and complex vulnerabilities of VLMs.

</details>


### [107] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Xuemeng Song,Tian Gan,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出了针对短文本聚类任务的GSDMM与其改进版GSDMM+方法，以解决短文本稀疏、高维等难题，并通过大量实验证明了方法的有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体兴起，短文本数据（如推特等）的大量出现使短文本聚类变得尤为重要，但其稀疏性、高维性和大规模特点增加了聚类难度；而现有主流方法各有局限。因此，亟需高效、有效的新方法提升聚类质量。

Method: 作者提出了GSDMM（基于Dirichlet多项式混合模型的Collapsed Gibbs采样）来处理短文本聚类，强化了对词代表性的识别。随后，进一步提出GSDMM+，通过减少初始化噪音、基于熵自适应调整词权重以及针对性的簇合并策略，实现了更精细且与真实分布更一致的聚类效果。

Result: 通过与经典及最新聚类方法的广泛对比实验，GSDMM及GSDMM+在效果和运行效率上均展现出显著优势。

Conclusion: GSDMM及其改进版本GSDMM+显著提升了短文本聚类的表现，可有效应对短文本的稀疏与高维挑战，且在实际应用中具有较高的效率和准确性。

Abstract: Short text clustering has become increasingly important with the popularity
of social media like Twitter, Google+, and Facebook. Existing methods can be
broadly categorized into two paradigms: topic model-based approaches and deep
representation learning-based approaches. This task is inherently challenging
due to the sparse, large-scale, and high-dimensional characteristics of the
short text data. Furthermore, the computational intensity required by
representation learning significantly increases the running time. To address
these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet
Multinomial Mixture model (GSDMM), which effectively handles the sparsity and
high dimensionality of short texts while identifying representative words for
each cluster. Based on several aspects of GSDMM that warrant further
refinement, we propose an improved approach, GSDMM+, designed to further
optimize its performance. GSDMM+ reduces initialization noise and adaptively
adjusts word weights based on entropy, achieving fine-grained clustering that
reveals more topic-related information. Additionally, strategic cluster merging
is employed to refine clustering granularity, better aligning the predicted
distribution with the true category distribution. We conduct extensive
experiments, comparing our methods with both classical and state-of-the-art
approaches. The experimental results demonstrate the efficiency and
effectiveness of our methods. The source code for our model is publicly
available at https://github.com/chehaoa/VEMC.

</details>


### [108] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: 本文提出了两种将科学论文的主要观点以问答（QA）形式提取的方法，并通过专家评价证明知识图谱（KG）驱动的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 学者需要快速理解和筛选文献的核心内容，现有手段尚不能高效自动化实现科学论文主要观点的总结与获取。

Method: 方法一：基于LLM选取关键段落，自动生成问题并排序，再生成答案；方法二：搭建知识图谱，利用精调的实体关系抽取模型抽取三元组，并通过TF-IDF类指标选出关键三元组，再将其转换为问答对。两种方法均经专家基于预设标准评价问答质量。

Result: 评测结果显示，KG方法能更有效地捕捉和表达论文核心观点。实体关系抽取模型在科学语料上的精调对于获得高质量三元组至关重要。

Conclusion: 知识图谱驱动的问答生成法在论文要点提取中优于仅文本内容法，精调模型是提升性能的关键。

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [109] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 本研究分析中文心理咨询对话中抑郁和焦虑心理状态与语言表达之间的关系，主要关注一人称代词和负面情绪词的使用。结果发现负面情绪词的使用频率与抑郁、焦虑程度显著相关，但一人称代词与心理状态无显著关系，强调了文化与对话情境对心理健康交流中语言模式的影响。


<details>
  <summary>Details</summary>
Motivation: 过去西方研究发现，一人称代词频率和负面情绪词与心理健康状态（如抑郁、焦虑）有关，但主要基于英语背景，中文语境下的相关结论尚不一致。研究者希望在中文心理咨询情境中检验这些语言心理标记是否成立，同时探讨文化差异的影响。

Method: 研究收集了735次在线中文心理咨询会话，利用LIWC软件量化语言特征，并采用广义线性混合效应模型分析负面情绪词和一人称代词的使用频率与抑郁、焦虑严重程度的关系。

Result: 负面情绪词的使用频率与来访者的抑郁和焦虑状态存在显著正相关。而一人称代词的使用频率与心理状态无显著关系，这与以往英文文献的结论不同。

Conclusion: 中文心理咨询对话中的语言心理标记与西方研究结果存在差异，突显了文化背景和对话环境对心理健康交流中语言使用的复杂影响。研究结果有助于了解华语群体心理咨询语境下的语言特征，对实际心理治疗实践具有参考价值。

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [110] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
*Eitan Wagner,Renana Keydar,Omri Abend*

Main category: cs.CL

TL;DR: 本文提出了一个概率框架，量化侦探小说中故事的公平性与惊喜感，并用此分析了大模型自动生成小说的优缺点。


<details>
  <summary>Details</summary>
Motivation: 侦探小说需要在符合读者预期和带来惊喜之间取得平衡，文中希望通过数学化、指标化的方式定量分析和衡量‘公平性’（fair play）这一传统叙事标准。

Method: 建立了用于侦探小说的概率框架，提出并形式化定义了‘公平性’，并据此设计衡量公平性和惊喜感的指标。随后，运用该指标体系评价了大语言模型（LLM）生成的侦探小说。

Result: 实验证明，大模型生成的侦探小说虽然有一定的不可预测性，但往往未能妥善平衡故事的意外性和公平性，从而导致故事质量较差。

Conclusion: 量化‘公平性’与‘惊喜感’可以揭示侦探小说生成的核心难点，为未来算法改进指明方向。现有大模型在生成这类型故事方面还存在显著不足。

Abstract: Effective storytelling relies on a delicate balance between meeting the
reader's prior expectations and introducing unexpected developments. In the
domain of detective fiction, this tension is known as fair play, which includes
the implicit agreement between the writer and the reader as to the range of
possible resolutions the mystery story may have. In this work, we present a
probabilistic framework for detective fiction that allows us to define desired
qualities. Using this framework, we formally define fair play and design
appropriate metrics for it. Stemming from these definitions is an inherent
tension between the coherence of the story, which measures how much it ``makes
sense'', and the surprise it induces. We validate the framework by applying it
to LLM-generated detective stories. This domain is appealing since we have an
abundance of data, we can sample from the distribution generating the story,
and the story-writing capabilities of LLMs are interesting in their own right.
Results show that while LLM-generated stories may be unpredictable, they
generally fail to balance the trade-off between surprise and fair play, which
greatly contributes to their poor quality.

</details>


### [111] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
*Nicolò Brunello,Davide Rigamonti,Andrea Sassella,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: 本文提出了一款名为InTraVisTo的可视化工具，用于追踪和分析Transformer大模型内部每一层在生成token过程中的计算细节。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）能力和复杂度提升，其实际输出与预期行为常常不一致，影响了LLM的可控性和部署，因此需要更好地理解和分析模型内部机制。

Method: 作者开发了InTraVisTo工具，能够解码和可视化Transformer模型各层的token embedding，同时通过Sankey图展现不同层和组件间的信息流，帮助研究人员追踪各计算步骤。

Result: InTraVisTo可以直观展示模型内在状态和信息流，使研究人员能够观察并分析模型在生成每个token过程中的推理流程和内部规律。

Conclusion: InTraVisTo有助于加深对LLM内部推理机制的理解，为提升大模型的可靠性、可控性和实用性提供了新的分析手段。

Abstract: The reasoning capabilities of Large Language Models (LLMs) have increased
greatly over the last few years, as have their size and complexity.
Nonetheless, the use of LLMs in production remains challenging due to their
unpredictable nature and discrepancies that can exist between their desired
behavior and their actual model output. In this paper, we introduce a new tool,
InTraVisTo (Inside Transformer Visualisation Tool), designed to enable
researchers to investigate and trace the computational process that generates
each token in a Transformer-based LLM. InTraVisTo provides a visualization of
both the internal state of the Transformer model (by decoding token embeddings
at each layer of the model) and the information flow between the various
components across the different layers of the model (using a Sankey diagram).
With InTraVisTo, we aim to help researchers and practitioners better understand
the computations being performed within the Transformer model and thus to shed
some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [112] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
*Maciej Jalocha,Johan Hausted Schmidt,William Michelseen*

Main category: cs.CL

TL;DR: 本文探讨了网络安全命名实体识别领域内标签标准化的问题，并提出了统一标签和新模型架构来提升数据集通用性，但实验结果显示改进有限。


<details>
  <summary>Details</summary>
Motivation: 网络安全NER领域缺乏标准化标签，导致不同数据集难以结合使用，限制了数据资源的利用和模型通用性的提升。

Method: 作者对四个网络安全相关数据集进行了粗粒度的标签统一，用BiLSTM模型做了跨数据集的训练和评估，并分析主要预测误差原因。随后，提出了多头和基于图的迁移模型（含BERT-base-NER）进一步尝试性能提升。

Result: 结果表明，统一标签后训练的模型在跨数据集泛化能力较弱。多头模型在联合训练下性能仅有微小提升，而基于图的迁移模型（依赖BERT-base-NER）相较于BERT-base-NER本身并无显著进步。

Conclusion: 尽管对标签进行了统一并引入新模型，现有方法在提升网络安全NER数据集间的泛化能力上收效甚微，领域内仍需继续探索高效的泛化和标签标准化解决方案。

Abstract: The field of cybersecurity NER lacks standardized labels, making it
challenging to combine datasets. We investigate label unification across four
cybersecurity datasets to increase data resource usability. We perform a
coarse-grained label unification and conduct pairwise cross-dataset evaluations
using BiLSTM models. Qualitative analysis of predictions reveals errors,
limitations, and dataset differences. To address unification limitations, we
propose alternative architectures including a multihead model and a graph-based
transfer model. Results show that models trained on unified datasets generalize
poorly across datasets. The multihead model with weight sharing provides only
marginal improvements over unified training, while our graph-based transfer
model built on BERT-base-NER shows no significant performance gains compared
BERT-base-NER.

</details>


### [113] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
*Carlos Mena,Pol Serra,Jacobo Romero,Abir Messaoudi,Jose Giraldo,Carme Armentano-Oller,Rodolfo Zevallos,Ivan Meza,Javier Hernando*

Main category: cs.CL

TL;DR: 论文针对加泰罗尼亚语-西班牙语代码切换的自动语音识别（ASR）问题，通过生成合成数据、拼接单语音频及利用真实数据等方式提升Whisper模型性能，显著改善了代码切换语音的转写效果。


<details>
  <summary>Details</summary>
Motivation: 代码切换在多语社会普遍存在，但由于缺乏专门的数据集，现有ASR模型在实际代码切换语音上的表现有限，急需设计更有效的解决策略，尤其针对如加泰罗尼亚语-西班牙语这样真实生活中频繁出现的代码切换场景。

Method: 作者提出三种策略改进ASR：1) 生成合成代码切换数据；2) 拼接单语音频以模拟代码切换；3) 利用真实代码切换数据并引入语言标签。所有数据均用于对OpenAI Whisper模型微调，并实现开源。

Result: 实验显示，将一定量合成代码切换数据与主导语言标签相结合训练，能获得最佳的语音转写效果。

Conclusion: 结合合成数据与语言标签显著提升了代码切换ASR的识别性能，为多语言社会中的ASR应用提供了实用且有效的解决方案。

Abstract: Code-switching (CS), the alternating use of two or more languages, challenges
automatic speech recognition (ASR) due to scarce training data and linguistic
similarities. The lack of dedicated CS datasets limits ASR performance, as most
models rely on monolingual or mixed-language corpora that fail to reflect
real-world CS patterns. This issue is critical in multilingual societies where
CS occurs in informal and formal settings. A key example is Catalan-Spanish CS,
widely used in media and parliamentary speeches. In this work, we improve ASR
for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic
CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data
with language tokens. We extract CS data from Catalan speech corpora and
fine-tune OpenAI's Whisper models, making them available on Hugging Face.
Results show that combining a modest amount of synthetic CS data with the
dominant language token yields the best transcription performance.

</details>


### [114] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: 本研究提出利用大语言模型（LLM）从情景判断测试（SJT）开放式回答中提取与能力相关的特征，为个人与职业技能的自动化测评铺垫基础。


<details>
  <summary>Details</summary>
Motivation: 随着学术项目愈发重视个人与专业技能，需开发规模化评估和发展这些技能的系统，而传统依赖人工评分的开放式SJT难以扩展，且现有NLP评分系统在效度上存在问题。

Method: 文章提出使用大语言模型（LLM）对SJT回答中提取与能力构念相关的特征，并以Casper SJT为实验对象，验证其自动评分方法的有效性。

Result: 研究结果表明，基于LLM的构念特征提取方法能够有效提升SJT自动评分的效度和标准化水平。

Conclusion: 本研究为未来基于大模型的个人与专业技能自动测评奠定了理论与技术基础，有助于推动SJT的规模化应用。

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [115] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: 本文利用transformer模型，自动对文本的政治倾向和政治相关性进行分类，并通过新构建和整合数据集提升跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前相关任务的数据和模型方案各自为政，缺乏泛化能力，尤其是在处理领域外文本时表现较差，因此需要更全面的数据集与更强的模型。

Method: 作者整合并扩展了12个政治倾向相关数据集，并通过扩展18个现有数据集新建了用于政治相关性分类的数据集。同时采用“留一入”和“留一出”方法进行模型评测，并训练新的泛化能力更强的模型。

Result: 实验表明，经过更大规模、更多样化数据集训练的新模型在跨数据集任务上表现出更好的泛化能力。

Conclusion: 构建多样化的大规模数据集并针对性训练模型可以显著提升文本政治倾向和政治性自动分类模型的泛化能力，优于现有割裂、单一数据集下的方案。

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [116] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: 本论文通过对19种大语言模型在707个政治议题上的说服能力实验，发现后训练和提示工程的改进大幅提升了AI说服力，但这种提升通常伴随着事实准确性的下降。


<details>
  <summary>Details</summary>
Motivation: 当前社会日益担心对话型AI可能对人类信仰产生前所未有的影响，本研究旨在科学评估AI说服力的真正来源，并检验其与事实准确性的关系。

Method: 作者进行了三项大规模实验，涵盖76,977名参与者。测试了19种大语言模型（部分为说服性后训练），涉及707个政治议题，产生了466,769条模型观点，并逐一核查其事实准确性。

Result: 结果表明，后训练与提示工程最多可分别提升说服力51%与27%，远超个性化或模型规模扩展。同时，当说服力提升时，事实准确率却出现系统性下降。

Conclusion: 当前与可预见的AI说服力主要依赖后训练与提示等使用方法，模型本身扩大或个性化作用有限。但提升说服力时需警惕事实失真的风险，需关注AI使用方法带来的社会影响。

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [117] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
*Jan Trienes,Anastasiia Derzhanskaia,Roland Schwarzkopf,Markus Mühling,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: 本文介绍了Marcel，这是一个为准学生提供招生相关问题解答的开源轻量级对话代理系统，通过检索增强生成技术提供快速、个性化、可验证的答案，并在实际环境中进行部署和评估。


<details>
  <summary>Details</summary>
Motivation: 高校招生期间，大学收到大量学生咨询，人工回复造成教职工工作量大且响应速度慢。为此，亟需自动化、精准且易部署的智能问答系统，以提升招生服务质量和效率。

Method: 系统采用检索增强生成（RAG）技术，将用户提问与大学权威信息资源相结合，通过FAQ检索器将问题映射到知识库条目，使管理员可干预检索过程，并提升了与传统检索方法相比的准确率和相关性。同时，系统设计注重轻量化和易部署，适用于软硬件资源有限的高校环境。

Result: 论文详细描述了系统架构，并对各个组件进行了技术评估。实际部署后，系统能够有效为准学生提供信息服务，减轻了教职工负担，并验证了FAQ检索器在信息匹配上的优越性。

Conclusion: Marcel系统可以在高校招生服务中实现高效自动化的信息解答，具有部署门槛低、回复速度快和答案可溯源等优势。FAQ检索结合RAG机制使系统更加实用和精准，适合在资源有限的学术环境中推广应用。

Abstract: We present Marcel, a lightweight and open-source conversational agent
designed to support prospective students with admission-related inquiries. The
system aims to provide fast and personalized responses, while reducing workload
of university staff. We employ retrieval-augmented generation to ground answers
in university resources and to provide users with verifiable, contextually
relevant information. To improve retrieval quality, we introduce an FAQ
retriever that maps user questions to knowledge-base entries, allowing
administrators to steer retrieval, and improving over standard dense/hybrid
retrieval strategies. The system is engineered for easy deployment in
resource-constrained academic settings. We detail the system architecture,
provide a technical evaluation of its components, and report insights from a
real-world deployment.

</details>


### [118] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在多项选择题问答中的“首因效应”偏见，并提出通过重新排序选项以提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在处理自然语言任务时存在类似人类的偏见，特别是对选项顺序的敏感度（“首因效应”），导致模型在多项选择题任务中的表现易受选项顺序影响。理解这种偏见的成因及其影响，有助于优化模型应用场景。

Method: 首先分析了LLM经微调后在多项选择题中首因效应的加剧。接着提出一种无须知晓正确答案的策略：根据各选项与查询的语义相似度对选项重新排序。随后通过实验证明了该策略的有效性。

Result: 实验证明，基于语义相似度重新排序选项可以显著提升LLM在多项选择题上的准确率。同时，也观察到微调过程增强了模型的人类偏见。

Conclusion: 偏见既是挑战也是机遇。合理利用模型的偏见，如首因效应，可以提升任务表现，从而为偏见感知的模型设计及相关NLP应用带来新启发。

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [119] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: 本论文提出将知识图谱（KG）中的基本概念转化为推理任务，结合课程微调，提升语言模型深度领域专才，尤其以医学领域为例显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽能通用泛化，但在深度领域专才方面受限，主要原因是传统的从大规模语料的“自上而下”训练方式难以习得领域抽象。该研究旨在通过“自下而上”模式，借助知识图谱的组合结构，构建模型能不断整合与深化领域知识。

Method: 提出一种新颖的任务生成流程，从知识图谱的基本单元合成推理任务，并生成“思维轨迹”。以医学知识图谱为例，打磨2.4万个推理任务集，对QwQ-32B模型进行微调，形成QwQ-Med-3。提出ICD-Bench评测集，覆盖15个医学领域量化模型推理能力。

Result: QwQ-Med-3在ICD-Bench上显著超越了最新推理模型，特别在难度最大子集上展现出更大优势。在医学问答任务上，微调后模型能有效迁移并提升原始模型性能。

Conclusion: 通过以知识图谱为基础的任务驱动微调策略，可以促使大模型获得领域推理专才，构建高效的领域超智能体。论文预期未来AGI可由多个可组合的专才体系统筹实现，而不单纯依赖“一体化且全能”的大模型。

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


### [120] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
*Lilit Grigoryan,Nikolay Karpov,Enas Albasiri,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文提出并开源了两个基于FastConformer架构的阿拉伯语自动语音识别（ASR）模型，分别覆盖现代标准阿拉伯语（MSA）和首次覆盖MSA与古典阿拉伯语（CA）的统一模型，均在相关数据集上取得了最新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语作为世界上使用最广泛的语言之一，其ASR系统开发受语言复杂性影响较大，公开模型稀缺，且现有研究过于集中于现代标准阿拉伯语（MSA），对语言内部变体如古典阿拉伯语（CA）关注不足。

Method: 提出一种面向阿拉伯语语音与文本处理的通用方法，并基于FastConformer架构分别训练了MSA专用模型和MSA+CA统一模型。

Result: MSA模型在相关数据集上实现了SOTA性能，统一模型在CA（含变音符号）上取得最高准确率，同时在MSA上也保持较强表现。

Conclusion: 两种新模型为阿拉伯语ASR领域（特别是统一MSA与CA识别）提供了强有力的工具。通过开放源码和训练流程，有助于促进可复现性与领域发展。

Abstract: Despite Arabic being one of the most widely spoken languages, the development
of Arabic Automatic Speech Recognition (ASR) systems faces significant
challenges due to the language's complexity, and only a limited number of
public Arabic ASR models exist. While much of the focus has been on Modern
Standard Arabic (MSA), there is considerably less attention given to the
variations within the language. This paper introduces a universal methodology
for Arabic speech and text processing designed to address unique challenges of
the language. Using this methodology, we train two novel models based on the
FastConformer architecture: one designed specifically for MSA and the other,
the first unified public model for both MSA and Classical Arabic (CA). The MSA
model sets a new benchmark with state-of-the-art (SOTA) performance on related
datasets, while the unified model achieves SOTA accuracy with diacritics for CA
while maintaining strong performance for MSA. To promote reproducibility, we
open-source the models and their training recipes.

</details>


### [121] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: 提出了一种新框架RHYTHM，利用大语言模型提升人类移动轨迹预测与推理的效率和准确性，并显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法往往难以有效捕捉人类移动中的时空依赖，且面临计算开销大的问题。为此，作者希望利用大语言模型的能力改进这一任务。

Method: RHYTHM框架将人类移动轨迹按照天分段为离散token，并采用分层注意力机制捕捉日和周的时序依赖；通过冻结的大语言模型（LLM）预训练prompt embedding增强token表达能力，同时保证较低的计算负担。

Result: 在三个真实数据集上，RHYTHM相比主流方法提升了2.4%的准确率，周末提升5%，训练时间减少24.6%。

Conclusion: RHYTHM能高效且精准地进行人类移动轨迹推理，验证了大语言模型在时空建模和推理任务中的潜力。

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [122] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: 本文提出了CPC-CMS框架，用于基于多评价指标加权选择文档级情感分析的最佳分类模型。以社交媒体数据为例，ALBERT表现最优（若不考虑效率），但效率纳入考量时表现各异。该方法可推广至其他分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类模型评估主要依靠单一或少量指标，难以兼顾模型的全面性能表现，缺乏结合专家知识与多维指标科学选型的方法。本文旨在解决如何公正有效地选取文档级情感分析中的最佳分类模型。

Method: 本文构建了CPC-CMS（认知成对比较分类模型选择）框架，基于专家打分确定精度、查准率、查全率、F1分数、特异性、MCC、Kappa、效率等多指标权重，整合多种主流分类模型（如NB、SVM、RF、XGBoost、LSTM、ALBERT），通过加权决策矩阵实现模型优选。

Result: 在三个社交媒体数据集上实证表明：若不考虑效率因素，ALBERT模型综合表现最佳；若计入效率（耗时），则无单一模型始终占优。

Conclusion: CPC-CMS框架能够根据多重指标加权，有效支持文档级情感分析及其它分类应用中的模型优选，可根据实际需求灵活应用。

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [123] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
*Israt Jahan,Md Tahmid Rahman Laskar,Chun Peng,Jimmy Huang*

Main category: cs.CL

TL;DR: 本文系统评估了多种大语言模型（LLMs）在生物医学不同文本与图像任务中的性价比和适用性。结论指出没有一个模型能在所有任务中都表现最佳，不同模型各有千秋。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在生物医学领域的广泛应用，如何高效选择适合特定任务的LLM成为亟需解决的问题，尤其是在考虑成本、推理速度与隐私的情况下。

Method: 对多种闭源与开源LLM进行了评测，涵盖生物医学文本分类、生成、问答及多模态图像处理任务。通过实验比较其表现。

Result: 实验发现，不同LLM在不同任务上表现突出，部分开源模型在某些任务上不仅表现不输闭源模型，还具备更快推理和更好的隐私。

Conclusion: 选择LLM需针对具体任务权衡性能、推理速度和隐私等因素，开源模型在一些场景下为更优选择。

Abstract: This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.

</details>


### [124] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
*Lautaro Estienne,Gabriel Ben Zenou,Nona Naderi,Jackie Cheung,Pablo Piantanida*

Main category: cs.CL

TL;DR: 本文提出了协作型理性言语行为（CRSA）模型，以提升AI在多回合协作对话中的推理与合作能力，有效提升了对话系统的涉情感和社交性。


<details>
  <summary>Details</summary>
Motivation: 现有理性言语行为（RSA）框架虽然在语用推理上具有有效性，但难以扩展至多回合、协作性强的对话场景，尤其在双方均有私有信息的情境下，推理与合作存在较大挑战。

Method: 作者提出了CRSA模型，采用信息论的增益函数（基于率失真理论）对RSA进行了扩展，使其能建模多回合对话中双方基于个体私有信息进行语句生成和推理的过程，在框架上吸收了原RSA关于增益最大化的思想。

Result: 在指称游戏和医疗领域的医患模板对话实验中，CRSA模型比现有基线方法表现出更一致、更易解释和更具协作性的行为。

Conclusion: CRSA的提出为更实用、更具社会意识的智能对话体代理奠定了基础，有望推动AI在协作型多回合对话领域的应用与发展。

Abstract: As AI systems take on collaborative roles, they must reason about shared
goals and beliefs-not just generate fluent language. The Rational Speech Act
(RSA) framework offers a principled approach to pragmatic reasoning, but
existing extensions face challenges in scaling to multi-turn, collaborative
scenarios. In this paper, we introduce Collaborative Rational Speech Act
(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn
dialog by optimizing a gain function adapted from rate-distortion theory. This
gain is an extension of the gain model that is maximized in the original RSA
model but takes into account the scenario in which both agents in a
conversation have private information and produce utterances conditioned on the
dialog. We demonstrate the effectiveness of CRSA on referential games and
template-based doctor-patient dialogs in the medical domain. Empirical results
show that CRSA yields more consistent, interpretable, and collaborative
behavior than existing baselines-paving the way for more pragmatic and socially
aware language agents.

</details>


### [125] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: 本文提出了DENSE系统，通过梳理和整合电子健康记录（EHR）中的不同类型医疗文档，利用大模型自动生成高质量的阶段性病程记录（progress notes），弥补病程记录在常用大规模EHR数据集中的缺失。


<details>
  <summary>Details</summary>
Motivation: 病程记录对于反映患者状态变化和治疗决策极为关键，但在主流EHR数据中极为稀缺（如MIMIC-III中仅8.56%住院包含病程记录），影响临床叙事的连续性与后续任务（如预测、总结等）的表现。

Method: 提出DENSE系统，采用细粒度文档分类和时序对齐机制，将同一患者不同时期、不同类型的临床文档进行结构化排序输入，结合面向临床的检索策略筛选当前和既往就诊中时序和语义相关信息，最终用大语言模型生成具有时序逻辑且内容连贯的病程记录。

Result: 在包含多个就诊且病程记录完整的患者子集上评估，生成的病程记录实现了高水平的时序连贯性（时序对齐比1.089），超过原始文档连续性。

Conclusion: DENSE通过修复医疗记录碎片化带来的叙事断裂，提升了EHR的内容价值，为临床笔记生成、总结、预测建模以及临床决策支持提供了新工具，在实际医疗环境中具有可扩展性。

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [126] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型将专业生物医学文献转化为易懂文本的能力，通过举办PLABA评测赛全面评估模型表现。结果显示部分模型在准确性和完整性方面可媲美人工，却在简洁性和易读性上存在明显差距。现有自动评测指标与人工判断缺乏一致性，说明评测工具需进一步完善。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献对患者和普通公众理解难度极大，将其转换为平易近人的语言具有重要的实际意义。大语言模型在该任务中显现出潜力，但其不可预测性和医学文本的敏感性带来了高风险，因此需要系统性严格评估。

Method: 研究团队通过在两届Text Retrieval Conference举办PLABA评测赛，设定两个任务：1）对摘要进行完整的句级重写；2）识别并替换晦涩术语。Task1引入四套专业参考答案进行自动化评测，Task1和Task2均由生物医学专家进行人工详细评估。

Result: 来自12国12支团队提交了多种模型（从浅层感知机直至大预训练模型）。Task1中，顶尖模型在事实准确性和完整性上接近人工表现，但不及人工在简洁和易读性上。自动评测指标与人工评判相关性较差。Task2中，模型难以精准识别并替换难词，但LLM在生成新表达时人工评判表现较好，仍有简洁性短板。

Conclusion: 大型语言模型在促进生物医学文献平民化方面展现出前景，但暴露出简洁性不足和评测手段不健全等问题，需继续优化相关自动化评估工具。

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [127] [Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms](https://arxiv.org/abs/2507.13455)
*Dean Chen,Armin Pomeroy,Brandon T. Peterson,Will Flanagan,He Kai Lim,Alexandra Stavrakis,Nelson F. SooHoo,Jonathan B. Hopkins,Tyler R. Clites*

Main category: cs.RO

TL;DR: 本文提出了一种集成多自由度（multi-DOF）运动限位、且具有超载保护的新型顺从机构硬限位设计方法。该方法通过优化接触表面几何形状，扩大工作空间并确保结构处于弹性范围，从而解决顺从机构在复杂不确定载荷下易疲劳失效的问题。验证结果表明，该方法能有效避免屈服和屈曲，有助于顺从机构在实际高精度场景中应用。


<details>
  <summary>Details</summary>
Motivation: 顺从机构因可无接触导向且精度高而在工程上有巨大潜力，但易受疲劳和力学失效限制，尤其在载荷复杂且失效代价高的应用场景下更甚。因此，提升顺从机构在不确定、多维度载荷下的安全性成为亟需解决的问题。

Method: 作者提出了一套系统化的设计合成方法，将多自由度运动限位集成至一对紧凑硬限位表面，通过理论分析与几何优化，最大化机构多自由度工作空间，同时保证结构处于弹性变形区间。并将该方法应用于正交植入用笼形铰链结构，进行了数值及实验验证。

Result: 所设计硬限位机构通过理论、仿真和实验均证实，能显著提升顺从机构在多自由度和复杂载荷下的抗疲劳、抗屈服及抗屈曲能力，实现了可靠的超载保护。

Conclusion: 本文设计方法为复杂载荷下顺从机构硬限位保护提供理论与实践基础，有望促进顺从机构在实际高风险、精密应用场景中的进一步推广应用。

Abstract: Compliant mechanisms have significant potential in precision applications due
to their ability to guide motion without contact. However, an inherent
vulnerability to fatigue and mechanical failure has hindered the translation of
compliant mechanisms to real-world applications. This is particularly
challenging in service environments where loading is complex and uncertain, and
the cost of failure is high. In such cases, mechanical hard stops are critical
to prevent yielding and buckling. Conventional hard-stop designs, which rely on
stacking single-DOF limits, must be overly restrictive in multi-DOF space to
guarantee safety in the presence of unknown loads. In this study, we present a
systematic design synthesis method to guarantee overload protection in
compliant mechanisms by integrating coupled multi-DOF motion limits within a
single pair of compact hard-stop surfaces. Specifically, we introduce a
theoretical and practical framework for optimizing the contact surface geometry
to maximize the mechanisms multi-DOF working space while still ensuring that
the mechanism remains within its elastic regime. We apply this synthesis method
to a case study of a caged-hinge mechanism for orthopaedic implants, and
provide numerical and experimental validation that the derived design offers
reliable protection against fatigue, yielding, and buckling. This work
establishes a foundation for precision hard-stop design in compliant systems
operating under uncertain loads, which is a crucial step toward enabling the
application of compliant mechanisms in real-world systems.

</details>


### [128] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 本文介绍了ERR@HRI 2.0挑战赛，旨在推动通过多模态数据检测LLM驱动对话机器人在人机对话中的错误，提升人机交互的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型驱动的对话机器人实现了更自然的人机交流，但依旧容易出现意图误解、中断用户或完全不响应等问题。及时检测并处理这些错误对于避免对话中断、维持任务流畅及用户信任至关重要。

Method: ERR@HRI 2.0挑战赛提供了16小时的人机互动多模态数据集，包括面部、语音和头部运动特征，并对系统视角下的机器人错误和用户意图进行了标注。鼓励参赛团队利用这些多模态数据，通过机器学习模型开展机器人错误检测的研究。

Result: 该挑战为研究者提供了标准化的多模态数据评测平台，并设定了包括准确率和误报率等多项性能指标，推动了检测模型的开发与评测。

Conclusion: 本挑战赛为利用社会信号分析提升人机交互中对话机器人错误检测能力迈出了关键一步，有助于推动智能机器人系统的实用化与可靠性提升。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [129] [SCOPE for Hexapod Gait Generation](https://arxiv.org/abs/2507.13539)
*Jim O'Connor,Jay B. Nash,Derin Gezgin,Gary B. Parker*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于离散余弦变换（DCT）的政策进化方法SCOPE，大幅降低输入空间维度，有效提升六足机器人步态学习效率，并在实验中实现了20%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 进化算法在六足机器人步态控制中有效，但随着输入复杂度提升，参数量成指数级增加导致效果急剧下降。亟需一种方法在保留关键信息的同时显著压缩输入空间。

Method: 提出Sparse Cosine Optimized Policy Evolution（SCOPE）方法，利用DCT对输入矩阵进行特征提取和降维，通过截断DCT系数矩阵保留高能量特征，实现输入特征稀疏化和压缩；然后由进化算法学习基于压缩输入的控制策略。

Result: SCOPE在六足机器人步态学习任务中，将时序位姿输入从2700维降为54维，压缩98%，使参考算法性能提升20%。能够灵活压缩输入形状，并在统计上显著提升学习性能。

Conclusion: SCOPE显著降低了复杂输入对进化算法效率的影响，可大幅度压缩输入维度同时提升步态学习效果，为复杂输入环境下的机器人控制提供了新的方法。

Abstract: Evolutionary methods have previously been shown to be an effective learning
method for walking gaits on hexapod robots. However, the ability of these
algorithms to evolve an effective policy rapidly degrades as the input space
becomes more complex. This degradation is due to the exponential growth of the
solution space, resulting from an increasing parameter count to handle a more
complex input. In order to address this challenge, we introduce Sparse Cosine
Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine
Transform (DCT) to learn directly from the feature coefficients of an input
matrix. By truncating the coefficient matrix returned by the DCT, we can reduce
the dimensionality of an input while retaining the highest energy features of
the original input. We demonstrate the effectiveness of this method by using
SCOPE to learn the gait of a hexapod robot. The hexapod controller is given a
matrix input containing time-series information of previous poses, which are
then transformed to gait parameters by an evolved policy. In this task, the
addition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.
SCOPE achieves this result by reducing the total input size of the time-series
pose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of
compressing an input to any output shape, provided that each output dimension
is no greater than the corresponding input dimension. This paper demonstrates
that SCOPE is capable of significantly compressing the size of an input to an
evolved controller, resulting in a statistically significant gain in efficacy.

</details>


### [130] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 本论文对GELLO远程操控系统进行了力反馈与力信息扩展，并证明这些扩展提升了灵巧操作任务的表现与用户体验。


<details>
  <summary>Details</summary>
Motivation: 原有的GELLO系统只支持关节位置控制，无法提供环境交互时的力觉反馈，限制了仿人操作任务的表现和操作真实感。

Method: 论文在GELLO系统中加入力反馈功能，让用户操作机器人时能感受到环境的反作用力。同时，在数据收集和模仿学习模型训练过程中也融入了力信息。通过对比不同训练方式，在仿真和实际灵巧操作任务上进行实验，并进行用户研究。

Result: 用户定性反馈显示，具备机器人经验的用户更喜欢扩展后的控制器。绝大多数任务中，包含力信息的模型表现更好，任务成功率提升。

Conclusion: 力反馈与力信息的引入显著增强了GELLO系统的操作效果和用户体验，对提升机器人灵巧操作任务具有实际价值。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [131] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: 该论文提出了一种针对无人机（UAV）在动态环境中实时轨迹规划的改进粒子群算法（PE-PSO），通过保持群体多样性和自适应参数调整，有效提升算法的实时性与多无人机协同能力。


<details>
  <summary>Details</summary>
Motivation: 传统的粒子群优化（PSO）方法虽然适合离线规划，但在实时轨迹规划中易早熟收敛且响应速度慢，难以满足无人机在动态环境中的需求。因此，亟需一种能兼顾实时性与全局寻优能力的新方法。

Method: 作者提出了PE-PSO算法，引入持久探索机制以维持群体多样性，并采用基于熵的参数自适应调整策略。同时，利用B样条曲线建模无人机轨迹以降低复杂度。为扩展至无人机群，结合了遗传算法（GA）任务分配和分布式PE-PSO，以实现并行计算和分布式控制。

Result: 在多个仿真实验中，该框架在轨迹质量、能效、避障和计算时间等方面均优于传统PSO及其他群体算法。

Conclusion: PE-PSO算法能有效提升无人机及其编队在复杂动态环境下的实时轨迹规划能力，具备良好的实际应用前景。

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [132] [Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography](https://arxiv.org/abs/2507.13650)
*Yu-Ting Lai,Yasamin Foroutani,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 本文提出了一种新型机器人系统，集成了标准的经瞳孔和眼内OCT探头，用于白内障术后囊膜清理，实现了囊膜全区域可视化和工具-组织实时距离反馈，并在体外试验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 白内障术后继发性白内障很常见，主要原因是残留晶状体物质在囊膜上增生。现有清理囊膜的手术难度较大，尤其是在薄膜上的视觉化和精细操作存在挑战。本研究旨在通过机器人辅助和更先进的影像/导航技术，提高手术安全性和有效性。

Method: 作者开发了一套机器人系统，将标准经瞳孔及眼内OCT探头集成到手术器械上，用于实时囊膜可视化和工具与组织的距离反馈。系统优势在于可自动进行囊膜全局（包括瞳孔区和赤道区）的成图，并能原位校准屈光率及光纤偏移问题。系统在眼球模型和猪眼体外实验中进行了验证。

Result: 该系统在五次眼球模型实验中，囊膜模型的均方根误差显著降低，验证了其精准性；在三次体外猪眼实验中，完成了囊膜清理且无组织损伤。

Conclusion: 所提出的机器人系统能实现准确的白内障囊膜清理，为安全、高效地预防和治疗继发性白内障提供了新方法。

Abstract: Secondary cataract is one of the most common complications of vision loss due
to the proliferation of residual lens materials that naturally grow on the lens
capsule after cataract surgery. A potential treatment is capsule cleaning, a
surgical procedure that requires enhanced visualization of the entire capsule
and tool manipulation on the thin membrane. This article presents a robotic
system capable of performing the capsule cleaning procedure by integrating a
standard transpupillary and an intraocular optical coherence tomography probe
on a surgical instrument for equatorial capsule visualization and real-time
tool-to-tissue distance feedback. Using robot precision, the developed system
enables complete capsule mapping in the pupillary and equatorial regions with
in-situ calibration of refractive index and fiber offset, which are still
current challenges in obtaining an accurate capsule model. To demonstrate
effectiveness, the capsule mapping strategy was validated through five
experimental trials on an eye phantom that showed reduced root-mean-square
errors in the constructed capsule model, while the cleaning strategy was
performed in three ex-vivo pig eyes without tissue damage.

</details>


### [133] [A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment](https://arxiv.org/abs/2507.13654)
*Haoran Wang,Yasamin Foroutani,Matthew Nepo,Mercedes Rodriguez,Ji Ma,Jean-Pierre Hubschman,Tsu-Chin Tsao,Jacob Rosen*

Main category: cs.RO

TL;DR: 本文研究了在虚拟视网膜手术环境下，Inside和Outside两种控制模式在不同缩放因子下的操作性能，发现Inside控制在较高缩放系数时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着机器人辅助手术的发展，提高手术效率和精度并降低风险成为关键。本研究旨在探索不同控制模式与缩放参数对手术表现的影响，为未来机器人辅助手术优化控制策略提供依据。

Method: 使用IRISS遥操作手术系统，结合虚拟现实显示，邀请5名有经验的外科医生和5名无经验工程师，完成常见视网膜手术模拟任务，考察不同控制模式和缩放因子的表现。

Result: 结果表明，在缩放因子为20或30时，Inside Control整体表现最好，但不同任务及其复杂度下，最佳缩放因子可能不同。

Conclusion: 优化控制模式和缩放因子有望提高手术效率和准确度，并减少未来机器人辅助手术中的风险。

Abstract: This paper examines the performance of Inside and Outside Control modes at
various scaling factors in a simulated vitreoretinal surgical setting. The
IRISS teleoperated surgical system's console (cockpit) was adapted to project a
simulated microscope view of an intraocular setup to a virtual reality (VR)
headset. Five experienced vitreoretinal surgeons and five engineers with no
surgical experience used the system to perform tasks common to vitreoretinal
surgery. Experimental results indicate that Inside Control methods at higher
scaling factors (20 or 30) achieved the best performance overall, though the
optimal scaling factor may vary by task and complexity. Optimizing control
methods and scaling factors could lead to improvements in surgical efficiency
and accuracy, as well as minimize risks in future robotic-assisted intraocular
procedures.

</details>


### [134] [Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion](https://arxiv.org/abs/2507.13662)
*Jing Cheng,Yasser G. Alqaham,Zhenyu Gan,Amit K. Sanyal*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展且自适应的腿式机器人控制框架，将迭代学习控制（ILC）与类生物肌肉记忆的力矩库相结合，实现了高效准确的运动控制。验证结果显示，该方法大幅提高了轨迹跟踪精度，并能够快速适应不同环境和任务。


<details>
  <summary>Details</summary>
Motivation: 当前腿式机器人在复杂环境下运动时，面临轨迹跟踪困难、动态扰动和未建模动力学的挑战，现有方法计算量大且泛化能力有限。作者希望通过更高效、泛化性更强的方法改善这些问题，提升机器人在真实环境中的适应能力。

Method: 提出结合ILC与生物启发力矩库（TL）的控制框架。通过混合基于物理建模的轨迹优化与实时学习，TL存储已学得的控制策略，实现不同速度、地形和重力下的快速适应，降低了在线计算需求。控制框架广泛适用于周期性与非周期性步态。

Result: 实验证明，该框架可在Cassie和A1机器人上将关节跟踪误差在数秒内减少最多85%，准确执行多种步态，包括斜坡和地形适应。与现有全身控制器相比，学习后的技能免去了执行时的在线计算，控制更新率提高30倍。

Conclusion: 将ILC与力矩记忆结合的控制框架极大地提升了腿式机器人在复杂和动态环境中的运动精度、效率及适应性，是实现腿式机器人实用化的重要进展。

Abstract: This paper presents a scalable and adaptive control framework for legged
robots that integrates Iterative Learning Control (ILC) with a biologically
inspired torque library (TL), analogous to muscle memory. The proposed method
addresses key challenges in robotic locomotion, including accurate trajectory
tracking under unmodeled dynamics and external disturbances. By leveraging the
repetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the
framework enhances accuracy and generalization across diverse locomotion
scenarios. The control architecture is data-enabled, combining a physics-based
model derived from hybrid-system trajectory optimization with real-time
learning to compensate for model uncertainties and external disturbances. A
central contribution is the development of a generalized TL that stores learned
control profiles and enables rapid adaptation to changes in speed, terrain, and
gravitational conditions-eliminating the need for repeated learning and
significantly reducing online computation. The approach is validated on the
bipedal robot Cassie and the quadrupedal robot A1 through extensive simulations
and hardware experiments. Results demonstrate that the proposed framework
reduces joint tracking errors by up to 85% within a few seconds and enables
reliable execution of both periodic and nonperiodic gaits, including slope
traversal and terrain adaptation. Compared to state-of-the-art whole-body
controllers, the learned skills eliminate the need for online computation
during execution and achieve control update rates exceeding 30x those of
existing methods. These findings highlight the effectiveness of integrating ILC
with torque memory as a highly data-efficient and practical solution for legged
locomotion in unstructured and dynamic environments.

</details>


### [135] [SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization](https://arxiv.org/abs/2507.13702)
*Junho Choi,Kihwan Ryoo,Jeewon Kim,Taeyun Kim,Eungchang Lee,Myeongwoo Jeong,Kevin Christiansen Marsim,Hyungtae Lim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉、惯性和超宽带（UWB）传感器的多机器人定位方法SaWa-ML，通过结构感知校正和自适应加权，有效减小了长时间累计的定位漂移误差。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人定位方法过度依赖单机器人里程计，难以解决长期累计误差，且对机器人间距传感数据和自身里程计特性考虑不充分。

Method: SaWa-ML方法利用UWB测距优势，先估算机器人间相对位置进行结构感知校正，同时根据传感器数据和视觉-惯性里程计的特性自适应调整位置修正权重，从而提升定位鲁棒性和准确性。

Result: 实际世界实验结果表明，SaWa-ML方法在定位性能上大幅优于现有最先进算法，显著降低长期漂移。

Conclusion: 结构感知和加权自适应的方法可高效整合多种传感器，实现更稳健准确的多机器人协同定位，对长期和复杂环境中的多机器人系统有重要意义。

Abstract: Multi-robot localization is a crucial task for implementing multi-robot
systems. Numerous researchers have proposed optimization-based multi-robot
localization methods that use camera, IMU, and UWB sensors. Nevertheless,
characteristics of individual robot odometry estimates and distance
measurements between robots used in the optimization are not sufficiently
considered. In addition, previous researches were heavily influenced by the
odometry accuracy that is estimated from individual robots. Consequently,
long-term drift error caused by error accumulation is potentially inevitable.
In this paper, we propose a novel visual-inertial-range-based multi-robot
localization method, named SaWa-ML, which enables geometric structure-aware
pose correction and weight adaptation-based robust multi-robot localization.
Our contributions are twofold: (i) we leverage UWB sensor data, whose range
error does not accumulate over time, to first estimate the relative positions
between robots and then correct the positions of each robot, thus reducing
long-term drift errors, (ii) we design adaptive weights for robot pose
correction by considering the characteristics of the sensor data and
visual-inertial odometry estimates. The proposed method has been validated in
real-world experiments, showing a substantial performance increase compared
with state-of-the-art algorithms.

</details>


### [136] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM-agent）的自动交通场景增强框架，能够用自然语言描述灵活生成高质量、符合用户意图的场景，解决了自动驾驶罕见关键场景生成中的规模与精细控制难题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统对于罕见但关键的测试场景具有极高的依赖，仅靠现实采集效率低下，现有数据驱动模型缺乏细粒度控制且易引入分布偏移，人工增强又不具备规模化能力。

Method: 提出采用基于LLM-agent的框架，通过自然语言描述自动增强真实交通场景，利用代理式设计以实现对输出的精细控制，并保证即便在小型、低成本LLM上也有高性能表现。

Result: 通过大量人工专家评价，框架在准确反映用户意图、生成高质量增强场景方面表现良好，效果可与专家手工增强相比。

Conclusion: 该方法克服了现有自动场景生成和增强方式的局限性，使自动驾驶罕见场景测试与评估更加高效、可控且实用。

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [137] [Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery](https://arxiv.org/abs/2507.13787)
*Doina Pisla,Alexandru Pusca,Andrei Caprariu,Adrian Pisla,Bogdan Gherman,Calin Vaida,Damien Chablat*

Main category: cs.RO

TL;DR: 本文提出了两种适用于机器人辅助微创胰腺手术的并联机器人架构，并通过有限元分析和工作空间评估，筛选出更合适的方案用于实验模型开发。


<details>
  <summary>Details</summary>
Motivation: 目前微创胰腺手术对机器人提出了高刚度、高适应性和精确操作的需求，因此需要设计并评估新的并联机器人架构，以满足医疗任务的特殊要求。

Method: 提出了ATHENA-1和ATHENA-2两种4自由度并联机器人架构，分别建立运动学模型和3D CAD概念模型。随后，利用有限元方法（FEM）分别对两种架构进行刚度分析，并基于工作空间定量分析来评估两种方案的实际适用性。

Result: 通过有限元仿真比较了两种架构的刚度，并通过工作空间分析评估了它们的适用范围，找出更适合医疗任务要求的架构。

Conclusion: 基于仿真和评估结果，选定了最符合设计标准的并联机器人架构，为后续手术机器人实验模型的开发奠定了基础。

Abstract: This paper focuses on the design of a parallel robot designed for robotic
assisted minimally invasive pancreatic surgery. Two alternative architectures,
called ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are
proposed. Their kinematic schemes are presented, and the conceptual 3D CAD
models are illustrated. Based on these, two Finite Element Method (FEM)
simulations were performed to determine which architecture has the higher
stiffness. A workspace quantitative analysis is performed to further assess the
usability of the two proposed parallel architectures related to the medical
tasks. The obtained results are used to select the architecture which fit the
required design criteria and will be used to develop the experimental model of
the surgical robot.

</details>


### [138] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 本文提出了一种将视觉数据与控制屏障证书（CBCs）结合的半监督安全控制方法，通过在世界模型潜空间学习屏障函数与安全控制器，实现了数据高效的安全控制策略合成。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，为视觉数据大规模标注安全关键的信息成本极高且不现实，因此需要探索无需大量人工标注即可生成安全控制器的新方法。近年来世界模型(latent space)的进展为安全控制的可扩展与高效提供了新思路。

Method: 作者提出了一个半监督框架，将控制屏障证书（CBCs）嵌入世界模型的潜空间中。该方法联合学习神经屏障函数和安全控制器，仅需少量标注数据，并利用视觉Transformer模型对潜在动态进行建模。

Result: 该方法利用有限标注数据，结合现代视觉Transformer对系统潜在动态的强大建模能力，实现了在潜空间上高效且可扩展的安全控制策略合成。

Conclusion: 通过在世界模型的潜空间中学习控制屏障证书，本文方法显著减少了对标注安全数据的需求，提高了安全控制的可行性和效率，在视觉感知引导的安全控制领域具有广泛应用前景。

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [139] [AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery](https://arxiv.org/abs/2507.13903)
*Ziliang Li,Hongming Chen,Yiyang Lin,Biyu Ye,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种基于空中机械臂的自主空投系统，有效提升了空投任务的敏捷性与精度。通过多自由度补偿与非线性模型预测控制，减小了控制误差与系统延迟的影响，在仿真和实物实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下执行空投任务时，无人机系统面临制导误差、系统延迟和控制模式切换的挑战，易导致空投精度下降，影响任务效果。

Method: 作者设计了一种具有额外执行自由度的空中机械臂系统，结合层级扰动补偿策略和嵌入平滑约束的非线性模型预测控制（NMPC），用以补偿无人机跟踪误差，并通过优化投掷轨迹，减小释放时机对空投精度的敏感性。

Result: 经仿真及实物实验验证，该系统在应对突变参数变化时可维持较优性能，且显著提升了空投的机动性和落点精度。

Conclusion: 结合空中机械臂与先进控制算法的自主空投系统可有效应对复杂环境下的空投挑战，具有良好的应用前景。

Abstract: Autonomous aerial systems play an increasingly vital role in a wide range of
applications, particularly for transport and delivery tasks in complex
environments. In airdrop missions, these platforms face the dual challenges of
abrupt control mode switching and inherent system delays along with control
errors. To address these issues, this paper presents an autonomous airdrop
system based on an aerial manipulator (AM). The introduction of additional
actuated degrees of freedom enables active compensation for UAV tracking
errors. By imposing smooth and continuous constraints on the parabolic landing
point, the proposed approach generates aerial throwing trajectories that are
less sensitive to the timing of payload release. A hierarchical disturbance
compensation strategy is incorporated into the Nonlinear Model Predictive
Control (NMPC) framework to mitigate the effects of sudden changes in system
parameters, while the predictive capabilities of NMPC are further exploited to
improve the precision of aerial throwing. Both simulation and real-world
experimental results demonstrate that the proposed system achieves greater
agility and precision in airdrop missions.

</details>


### [140] [NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning](https://arxiv.org/abs/2507.13940)
*Qingyi Chen,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经Hamilton-Jacobi可达性学习（Neural HJR）的分布式多智能体运动规划方法，解决了现有方法在可扩展性和实时性上的不足，能够在高维复杂场景实现高效安全的运动规划。


<details>
  <summary>Details</summary>
Motivation: 多智能体安全运动规划（MAMP）在机器人领域具有高度挑战性。现有分布式方法依赖预判、通信，存在一定局限；集中式方法则难以扩展和满足实时性。因此需要一种既可扩展又能实时保证安全的MAMP方案。

Method: 作者提出用神经网络逼近Hamilton-Jacobi可达性解法，并在此基础上设计了分布式的轨迹优化框架，将学习得到的可达性安全边界纳入每个智能体的决策，实现高维场景下多智能体间的碰撞回避。

Result: 实验结果表明，该方法具备较强的可扩展性和数据高效性，能够在高维（如12维双臂系统）和复杂碰撞约束下成功解决MAMP任务，相比多种当前主流方法表现更优。

Conclusion: 提出的神经HJR分布式运动规划方法兼顾了安全性、可扩展性和实时性，能够适应不同动力学系统，在复杂场景中优于现有技术，具有较大应用潜力。

Abstract: Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in
robotics. Despite substantial advancements, existing methods often face a
dilemma. Decentralized algorithms typically rely on predicting the behavior of
other agents, sharing contracts, or maintaining communication for safety, while
centralized approaches struggle with scalability and real-time decision-making.
To address these challenges, we introduce Neural Hamilton-Jacobi Reachability
Learning (HJR) for Decentralized Multi-Agent Motion Planning. Our method
provides scalable neural HJR modeling to tackle high-dimensional configuration
spaces and capture worst-case collision and safety constraints between agents.
We further propose a decentralized trajectory optimization framework that
incorporates the learned HJR solutions to solve MAMP tasks in real-time. We
demonstrate that our method is both scalable and data-efficient, enabling the
solution of MAMP problems in higher-dimensional scenarios with complex
collision constraints. Our approach generalizes across various dynamical
systems, including a 12-dimensional dual-arm setup, and outperforms a range of
state-of-the-art techniques in successfully addressing challenging MAMP tasks.
Video demonstrations are available at https://youtu.be/IZiePX0p1Mc.

</details>


### [141] [A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios](https://arxiv.org/abs/2507.13969)
*Maria Eduarda Silva de Macedo,Ana Paula Chiarelli de Souza,Roberto Silvio Ubertino Rosso Jr.,Yuri Kaszubowski Lopes*

Main category: cs.RO

TL;DR: 本文提出了一种多任务自聚集方法，使同类群体机器人仅凭视线传感器即可分成多个紧凑簇群，相较以往方法提升了紧凑性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统群体机器人实现自组织行为时，多任务自聚集会导致簇群松散或自动化不足。如何在多组机器人并存时提高群体自聚集的紧凑性和自治性亟需解决。

Method: 本文提出一种基于视线传感器的多任务自聚集行为设计，使同类机器人能自主聚合成不同的紧凑簇群。通过仿真实验，对不同组数和每组机器人数量下的表现进行了测试。

Result: 仿真结果显示，所提出的方法在不同配置下均能实现良好的可扩展性，显著提升了聚集簇群的紧凑性，并保持了高比例的机器人成功聚集。

Conclusion: 本文方法有效提升了多任务群体机器人自聚集时簇群的紧凑性和可扩展性，仅借助低成本的视线传感器即可应用，拓展了群体机器人协作的实际能力。

Abstract: The deployment of simple emergent behaviors in swarm robotics has been
well-rehearsed in the literature. A recent study has shown how self-aggregation
is possible in a multitask approach -- where multiple self-aggregation task
instances occur concurrently in the same environment. The multitask approach
poses new challenges, in special, how the dynamic of each group impacts the
performance of others. So far, the multitask self-aggregation of groups of
robots suffers from generating a circular formation -- that is not fully
compact -- or is not fully autonomous. In this paper, we present a multitask
self-aggregation where groups of homogeneous robots sort themselves into
different compact clusters, relying solely on a line-of-sight sensor. Our
multitask self-aggregation behavior was able to scale well and achieve a
compact formation. We report scalability results from a series of simulation
trials with different configurations in the number of groups and the number of
robots per group. We were able to improve the multitask self-aggregation
behavior performance in terms of the compactness of the clusters, keeping the
proportion of clustered robots found in other studies.

</details>


### [142] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: 本文提出了一种热力图引导的6自由度抓取检测方法，并将其部署在低功耗的GAP9 RISC-V芯片上，实现了低延迟、低能耗的实时抓取推理。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取任务复杂，要求在多样物体和姿态下实现精确操控，尤其在资源受限的边缘设备上难以实现高效且实时的推理。

Method: 提出一个端到端的热力图引导抓取检测框架，针对GAP9芯片进行优化，包括输入降维、模型拆分、量化等硬件感知技术。

Result: 在GraspNet-1Billion数据集上实验结果表明，该方法可实现完全在芯片上的推理，验证了低功耗MCU实现实时自主操作的可行性。

Conclusion: 本方法证实了经过优化的深度网络可以在资源有限的嵌入式设备上实现高效、低延迟的机器人抓取推理，为低功耗、实际应用场景下的自主操作提供了新方向。

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>


### [143] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: 本文提出了一种多策略改进蛇优化器（MISO），并通过大量基准测试与实际应用，验证了该算法在速度、准确性和全局搜索能力上的优越性。


<details>
  <summary>Details</summary>
Motivation: 蛇优化器（SO）虽具有多样解生成能力，但存在收敛慢、易陷入局部最优等问题。为提升其全局搜索性能与收敛速度，作者提出对其进行多策略改进。

Method: 作者提出三种改进：（1）基于正弦函数的自适应随机扰动策略，降低陷入局部最优的风险；（2）基于尺度因子和领导者的自适应Levy飞行策略，为雄性蛇领导者赋予飞行能力，增强算法跳出局部最优的能力；（3）结合精英领导与布朗运动的位置更新策略，加快收敛、提高精度。新算法（MISO）在CEC2017与CEC2022基准函数及多个工程问题（如无人机三维路径规划）上与11个流行算法进行实验对比。

Result: 实验表明，MISO在解质量和稳定性方面均优于其他竞争算法，尤其在UAV路径规划和工程设计问题中展现出较强的实际应用能力。

Conclusion: 多策略改进后的蛇优化器MISO，显著提升了全局搜索能力和收敛速度，适用于复杂实际问题，具有良好的应用前景。

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


### [144] [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
*Paweł Budzianowski,Wesley Maa,Matthew Freed,Jingxiang Mo,Winston Hsiao,Aaron Xie,Tomasz Młoduchowski,Viraj Tipnis,Benjamin Bolte*

Main category: cs.RO

TL;DR: 该论文提出了Edge VLA（EVLA），通过创新方法大幅提升视觉语言行为模型在边缘设备上的推理速度，实现接近大型模型的表现但能实时运行。


<details>
  <summary>Details</summary>
Motivation: 目前，将大型视觉语言模型（VLMs）应用于资源有限的移动机器人中存在推理速度慢、难以实时应用的问题。作者希望解决在边缘设备上高效运行VLMs的挑战，满足机器人实际部署需求。

Method: 1）去除末端执行器位置预测的自回归需求，显著加快推理速度；2）采用小型语言模型（SLMs），在大幅降低计算需求的同时获得与大型模型接近的训练效果。

Result: EVLA的训练性能与OpenVLA等大模型相当，但在推理速度上实现了7倍提升，并且显著提高了内存利用效率。

Conclusion: EVLA为在边缘设备上运行VLA模型提供了高效解决方案，实现了高性能与高效率的平衡，推动了视觉语言模型在实际机器人领域的应用。

Abstract: Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.

</details>


### [145] [Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub](https://arxiv.org/abs/2507.14059)
*Tianyuan Wang,Mark A Post,Mathieu Deremetz*

Main category: cs.RO

TL;DR: 本文介绍了STARFAB项目中移动检测模块（MIM）的设计与功能，其目的是在轨道自动化仓库环境中实现组件的自主检测、运维与可持续利用。MIM具备多种传感器和模块化工具，通过标准接口与机器人协作，提升太空硬件维护效率。


<details>
  <summary>Details</summary>
Motivation: 随着新太空经济的发展，地球轨道及更远空间中的硬件组件复用与组装变得日益重要。如何高效、自动化地对这些空间部件和基础设施进行检测和维护，是构建可循环、可持续空间工作的核心需求。STARFAB项目希望通过地面演示，验证类似空间仓库的无人值守自动检测方案的可行性。

Method: 设计并开发了用于自动检测的STARFAB移动检测模块（MIM）。MIM通过标准化接口可被机械臂机器人携带移动，配备高分辨率相机、三维轮廓仪和热成像传感器，支持增加其他模块化传感器。其内部还集成了可供机械臂使用的抓取工具与力矩扳手，实现检测与维护的一体化。

Result: 目前MIM已完成部分机械和电子设计，并装载多种无损检测传感器，支持多种在轨检测和维护操作。实验和测试工作尚在进行中。

Conclusion: MIM作为自主空间检测维护系统，已明确支持STARFAB在轨道自动化仓库中的各类操作需求，有望提升未来空间商业运营的可持续性和高效性。

Abstract: The use of autonomous robots in space is an essential part of the "New Space"
commercial ecosystem of assembly and re-use of space hardware components in
Earth orbit and beyond. The STARFAB project aims to create a ground
demonstration of an orbital automated warehouse as a hub for sustainable
commercial operations and servicing. A critical part of this fully-autonomous
robotic facility will be the capability to monitor, inspect, and assess the
condition of both the components stored in the warehouse, and the STARFAB
facility itself. This paper introduces ongoing work on the STARFAB Mobile
Inspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it
can be carried by Walking Manipulators (WM) as an independently-mobile robot,
and multiple MIMs can be stored and retrieved as needed for operations on
STARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a
thermal imaging sensor, with the capability to add other modular sensors. A
grasping tool and torque wrench are stored within the modular body for use by
an attached WM for maintenance operations. Implementation and testing is still
ongoing at the time of writing. This paper details the concept of operations
for the MIM as an on-orbit autonomous inspection and maintenance system, the
mechanical and electronic design of the MIM, and the sensors package used for
non-destructive testing.

</details>


### [146] [MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation](https://arxiv.org/abs/2507.14061)
*Nataliya Nechyporenko,Yutong Zhang,Sean Campbell,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出了一种新算法MorphIt，可自动优化机器人的结构表示，利用球体近似物体形状，在保持几何精度的同时提高计算效率，对比现有方法在多个指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 目前机器人多将物理结构视为固定参数，难以根据不同任务需求调整，导致在各种计算和精度需求下都使用同一刚性几何表示，限制了机器人的多任务能力。

Method: MorphIt算法通过自动的基于梯度优化的框架，将机器人的形态近似为一组球体。用户可通过可调节参数明确控制物理精度与计算消耗之间的权衡，无需手动建模或依赖僵化的算法流程。

Result: 实验显示，MorphIt在多个指标上优于现有基线方法（如Variational Sphere Set Approximation和Adaptive Medial-Axis Approximation），能用更少的球实现更好的模型拟合，并降低计算负担。同时，在碰撞检测、接触仿真和狭小空间导航等任务中的表现也更好。

Conclusion: MorphIt能根据具体任务动态调整机器人的几何表示，使机器人能主动利用自身结构形态作为资源，跳脱仅为参数的限制，为机器人在高精度与高效能之间平衡开辟了新方向。

Abstract: What if a robot could rethink its own morphological representation to better
meet the demands of diverse tasks? Most robotic systems today treat their
physical form as a fixed constraint rather than an adaptive resource, forcing
the same rigid geometric representation to serve applications with vastly
different computational and precision requirements. We introduce MorphIt, a
novel algorithm for approximating robot morphology using spherical primitives
that balances geometric accuracy with computational efficiency. Unlike existing
approaches that rely on either labor-intensive manual specification or
inflexible computational methods, MorphIt implements an automatic
gradient-based optimization framework with tunable parameters that provides
explicit control over the physical fidelity versus computational cost tradeoff.
Quantitative evaluations demonstrate that MorphIt outperforms baseline
approaches (Variational Sphere Set Approximation and Adaptive Medial-Axis
Approximation) across multiple metrics, achieving better mesh approximation
with fewer spheres and reduced computational overhead. Our experiments show
enhanced robot capabilities in collision detection accuracy, contact-rich
interaction simulation, and navigation through confined spaces. By dynamically
adapting geometric representations to task requirements, robots can now exploit
their physical embodiment as an active resource rather than an inflexible
parameter, opening new frontiers for manipulation in environments where
physical form must continuously balance precision with computational
tractability.

</details>


### [147] [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](https://arxiv.org/abs/2507.14099)
*Markus Buchholz,Ignacio Carlucho,Michele Grimaldi,Maria Koskinopoulou,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 本文提出了一种自适应启发式运动规划器（AHMP）框架，结合启发式运动空间（HMS）与贝叶斯网络，实现了更高效和鲁棒的自主水下操作运动规划。通过大量仿真与真实测试，验证了方法在动态海洋环境下的优势。


<details>
  <summary>Details</summary>
Motivation: 当前水下机器人运动规划方法难以充分利用既往运动经验，且面对动态、不确定水下环境下的实时适应能力有限。因此，亟需开发高效且能动态应对不确定性的水下运动规划方法。

Method: 方法核心为：在启发式运动空间（HMS）内使用概率路线图（PRM）算法，优化包含距离、不确定性、能耗与执行时间的复合代价路径。通过HMS大幅减少搜索空间提升计算效率，并用贝叶斯网络根据实时传感器与环境信息动态修正路径成功的联合概率，实现不确定性自适应规划。

Result: 通过大量仿真与实际测试，验证该框架在性能和鲁棒性方面优于现有方法，能够实现高效、实时的水下运动规划。

Conclusion: 所提出的概率式方法显著提升了自主水下机器人的运动规划能力，特别是在动态和充满不确定性的海洋环境下，实现了路径优化与实时适应。

Abstract: Autonomous motion planning is critical for efficient and safe underwater
manipulation in dynamic marine environments. Current motion planning methods
often fail to effectively utilize prior motion experiences and adapt to
real-time uncertainties inherent in underwater settings. In this paper, we
introduce an Adaptive Heuristic Motion Planner framework that integrates a
Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning
for autonomous underwater manipulation. Our approach employs the Probabilistic
Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite
cost function that accounts for distance, uncertainty, energy consumption, and
execution time. By leveraging HMS, our framework significantly reduces the
search space, thereby boosting computational performance and enabling real-time
planning capabilities. Bayesian Networks are utilized to dynamically update
uncertainty estimates based on real-time sensor data and environmental
conditions, thereby refining the joint probability of path success. Through
extensive simulations and real-world test scenarios, we showcase the advantages
of our method in terms of enhanced performance and robustness. This
probabilistic approach significantly advances the capability of autonomous
underwater robots, ensuring optimized motion planning in the face of dynamic
marine challenges.

</details>
